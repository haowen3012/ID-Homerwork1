<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Affective Computing Has Changed: The Foundation Model Disruption</title>
<!--Generated on Fri Sep 13 15:02:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.08907v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S1" title="In Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2" title="In Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Emergence in Foundation Models</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS1" title="In 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>The Vision Modality Has Changed</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS1.SSS1" title="In 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS1.SSS2" title="In 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS2" title="In 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>The Linguistic Modality Has Changed</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS2.SSS1" title="In 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS2.SSS2" title="In 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS3" title="In 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>The Speech Modality Has (Not Yet) Changed</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS3.SSS1" title="In 2.3 The Speech Modality Has (Not Yet) Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS3.SSS2" title="In 2.3 The Speech Modality Has (Not Yet) Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS4" title="In 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>The Evaluation Is Changing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S3" title="In Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Concerns and Regulations Have Changed</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S4" title="In Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Outlook and Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">[1,2,3,4,5]<span class="ltx_ERROR undefined" id="p1.1.1">\fnm</span>Björn<span class="ltx_ERROR undefined" id="p1.1.2">\sur</span>Schuller</p>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">1]<span class="ltx_text" id="p2.1.1" style="font-size:90%;">CHI – Chair of Health Informatics, MRI, Technical University of Munich, Germany</span>
2]<span class="ltx_text" id="p2.1.2" style="font-size:90%;">MDSI – Munich Data Science Institute, Germany</span>
3]<span class="ltx_text" id="p2.1.3" style="font-size:90%;">MCML – Munich Center for Machine Learning, Germany</span>
4]<span class="ltx_text" id="p2.1.4" style="font-size:90%;">GLAM – Group on Language, Audio, &amp; Music, Imperial College London, UK</span>
5]<span class="ltx_text" id="p2.1.5" style="font-size:90%;">EIHW – Chair of Embedded Intelligence for Health Care &amp; Wellbeing, University of Augsburg, Germany</span>
6]<span class="ltx_text" id="p2.1.6" style="font-size:90%;">School of Engineering, Universidad Autonoma de Madrid, Spain</span>
7]<span class="ltx_text" id="p2.1.7" style="font-size:90%;">AI R&amp;D Team, SyncPilot GmbH, Germany</span></p>
</div>
<h1 class="ltx_title ltx_title_document">Affective Computing Has Changed: The Foundation Model Disruption</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:schuller@ieee.org">schuller@ieee.org</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id1.1.id1">\fnm</span>Adria<span class="ltx_ERROR undefined" id="id2.2.id2">\sur</span>Mallol-Ragolta
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id3.1.id1">\fnm</span>Alejandro<span class="ltx_ERROR undefined" id="id4.2.id2">\sur</span>Peña Almansa
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id5.1.id1">\fnm</span>Iosif<span class="ltx_ERROR undefined" id="id6.2.id2">\sur</span>Tsangko
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id7.1.id1">\fnm</span>Mostafa M.<span class="ltx_ERROR undefined" id="id8.2.id2">\sur</span>Amin
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id9.1.id1">\fnm</span>Anastasia<span class="ltx_ERROR undefined" id="id10.2.id2">\sur</span>Semertzidou
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id11.1.id1">\fnm</span>Lukas<span class="ltx_ERROR undefined" id="id12.2.id2">\sur</span>Christ
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id13.1.id1">\fnm</span>Shahin<span class="ltx_ERROR undefined" id="id14.2.id2">\sur</span>Amiriparian
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id15.id1">The dawn of Foundation Models has on the one hand revolutionised a wide range of research problems, and, on the other hand, democratised
the access and use of AI-based tools by the general public. We even observe an incursion of these models into disciplines related to human psychology, such as the Affective Computing domain, suggesting their affective, emerging capabilities. In this work, we aim to raise awareness of the power of Foundation Models in the field of Affective Computing by synthetically generating and analysing multimodal affective data, focusing on vision, linguistics, and speech (acoustics).
We also discuss some fundamental problems, such as ethical issues and regulatory aspects, related to the use of Foundation Models in this research area.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Affective Computing, Foundation Models, Large Language Models, Large Multimodal Models, Disruption
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">“<span class="ltx_text ltx_font_italic" id="S1.p1.1.1">The world of Affective Computing has changed. I see it in the vision modality. I read it in the linguistic modality. I hear it in the speech modality. Much that once was is outdated…</span>”
This quote, which might sound slightly familiar to the J. R. R. Tolkien’s readers, aims to literary exemplify how the disruption of Foundation Models (FM) might be impacting the Affective Computing research as we knew it. Before centring the discussion on this topic, we summarise where we come from.
</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The Affective Computing research can be broadly clustered into three main categories: recognition of affect,
generation of affective content, and response to affect <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib1" title="">1</a>]</cite>. The development of systems with affective features opens a vast arsenal of use cases, ranging from digital psychology recognising depression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib2" title="">2</a>]</cite> to security applications (e. g., stress prediction in driving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib3" title="">3</a>]</cite>). However, Affective Computing applications are mostly centred in Human-Computer and Human-Robot Interaction (HCI/HRI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib6" title="">6</a>]</cite>, where the ability to interpret the human affect
is not only desirable to improve the communication <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib8" title="">8</a>]</cite>, but a necessary capability to correctly understand the message. Early work in the field of psychology indicated that human affect is communicated in a multimodal manner through physical channels such as facial expressions, language, or voice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib9" title="">9</a>]</cite>. Consequently, the Affective Computing community has paid significant attention to visual <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib11" title="">11</a>]</cite>,
linguistic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib13" title="">13</a>]</cite>, and speech (acoustic) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib15" title="">15</a>]</cite> data processing, including multimodal configurations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Of the different dimensions of human affect, the emotional states have attracted major attention for its impact in a wide range of aspects of peoples’ lives. Given such relevance, the Affective Computing community has been putting major efforts in the development of automated systems for the recognition and understanding of the human feelings. Early research in the field
of emotion recognition
relied on conventional Machine Learning (ML) pipelines, in which expert-crafted features were first extracted from the raw data such as pixels, words, or an audio signal, and then processed utilising traditional statistical methods; e. g., Support Vector Machines (SVM).
The key to this conventional approach was to try to design a suitable
set of features that captures
emotional content; i. e., the hand-crafted features. In the visual domain, the emotional content was assumed to be mainly reflected in the facial expression information, from which the features were extracted; e. g., through Principal Component Analysis (PCA), or traditional Computer Vision (CV) techniques, such as Gabor wavelets or texture filters.
In addition, prior knowledge of the Action Units (AUs) defined in the Facial Action Coding System (FACS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib18" title="">18</a>]</cite>
helped in the design of these features.
In linguistics, feature extraction typically relied on <math alttext="n" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mi id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">italic_n</annotation></semantics></math>-grams or bag-of-words
– a vector representation of text by sparse vectors that represent some form of vocabulary and some form of frequency of occurrence of the words in the current text –,
after carefully preprocessing the raw text (e. g., by stemming or stopping).
In audio, several works noticed the rich emotional information reflected in the prosodic features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib19" title="">19</a>]</cite>, which in combination with spectral features
– e. g., the Mel Frequency Cepstral Coefficients (MFCC) –, formed the basis for Speech Emotion Recognition (SER).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The success of Deep Learning (DL) in the early 2010’s entailed a disruption to the entire field of Artificial Intelligence. The development of affective systems embraced the new trend as well, which was mainly marked by the use of
representational learning via Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN) proved to be extremely useful for extracting appropriate features when large amounts of data were available.
Consequently, feature engineering took a backseat when data-driven approaches unleashed their potential.
This could be seen as the first disruption of the field: the learning of representations.
In the visual domain, end-to-end CNN-based systems accepted raw images as input, discarding the need for any feature engineering.
In linguistics, complex preprocessing steps and statistical representations lost their relevance with the success of word embedding models, which could not only be utilised as input for a DNN model, but also contained semantic meaning in their own space, including affective information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib20" title="">20</a>]</cite>.
Moreover, some speech systems began to process the raw time signal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib21" title="">21</a>]</cite> or the spectrogram representations
with CNNs to take advantage of their enhanced representational capabilities.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Therefore, the efforts were
ultimately shifted from experts crafting representations to experts choosing model architectures to learn these representations. A second, albeit less noted and exploited potential disruption came with the possibility of (neural) architecture search by reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib22" title="">22</a>]</cite> or more efficient approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib23" title="">23</a>]</cite>. This meant that in principle, once having affective (labelled) data, the representation could be learnt to then analyse affective data as well as the best architecture to do so.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The data-driven disruption meant an improvement as well for synthesis of emotional data. In the visual domain, the introduction of adversarial learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib24" title="">24</a>]</cite> paved the way for Generative Adversarial Network- (GAN) based generative models, in which the emotional state was also controlled by explicit indicators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib25" title="">25</a>]</cite> or by style transfer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib26" title="">26</a>]</cite>. Some works improved the semantic control over the output by identifying and traversing emotional directions in the GAN latent space <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib27" title="">27</a>]</cite>.
For text, for instance, RNNs could similarly produce affective language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib28" title="">28</a>]</cite>.
In speech (acoustics), where traditional approaches mostly relied on Gaussian Mixture Models (GMM) or Hidden Markov Models (HMM),
new encoder-decoder architectures based on Long Short-Term Memory (LSTM) networks were devised, in which the emotional state could be controlled by both explicit labels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib29" title="">29</a>]</cite> or style transfer from a reference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Yet, a critical issue in this era was the acquisition of such reliable, annotated data. The subjectivity of measuring inner emotion through self-assessment shifted the focus to perceived emotion. However, ‘measuring’ outer perceived emotion usually requires several labellers to reduce uncertainty, hence coming at high effort and cost.
In addition, the lack of spontaneous data sources – e. g., due to privacy restrictions – favoured the use of acted/elicited, non-spontaneous data samples <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib32" title="">32</a>]</cite>. Whilst non- or less-spontaneous data eased the problem of data availability, it came with the drawback that the analysis of real-world emotion struggles with subtlety not met in training. Further, generated data samples may reflect unrealistic emotional responses if systems are only trained on such data.
The acquisition of data from Internet sources (e. g., social media, films) allowed the collection of large “in-the-wild” databases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib34" title="">34</a>]</cite>, whose annotations were obtained through semi-automatic methods, crowdsourcing, or based on the criteria of experts in affect.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Nowadays, a third disruption is taking place in the Artificial Intelligence (AI) community. Whilst the first disruption established that (almost) no feature engineering was needed – just a powerful model and annotated data –, and the second allowed to learn also the optimal architectures, in current developments even specialised annotated affective
data for training the models may no longer be needed, as affective computing abilities start to emerge in (general large data) pre-trained models as we will discuss in the following section.
Nevertheless, curating high-quality sets of annotated data to some extent remains crucial to assess the performance of the models.
New architectures, such as the Diffuser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib35" title="">35</a>]</cite> or the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib36" title="">36</a>]</cite>, together with self-supervised learning strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib37" title="">37</a>]</cite> and inter-modality alignment techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib38" title="">38</a>]</cite>, have led to new FMs  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib42" title="">42</a>]</cite>. These models have demonstrated surprising capabilities using prompt-based instructions, to the point that they can generate realistic data samples or perform zero-shot classification. The
extent of the affective capabilities of these models, and the potential they open up, is still uncertain. Herein, we aim to shed some light on this topic, and explore how the emergence of FMs and the subsequent AI regulation are influencing the Affective Computing community.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Emergence in Foundation Models</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">One of the main characteristics of the Foundation Models (FMs) is that they are trained on a broad range of data, so that the resulting models can be utilised in a wide range of problems.
In addition, they exploit large amounts of learning parameters.
Given sufficient learning material, from a certain number of such parameters hence well trained, knowledge ‘emerges’ in the FMs, and they achieve competitive performances in tasks they have not been specifically trained for. This can, however, be difficult to predict <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib43" title="">43</a>]</cite>. In this paper, we aim to investigate the ‘emergent’ affective capabilities of FMs. Focusing on the vision (cf. <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS1" title="2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>), linguistics (cf. <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS2" title="2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>), and speech (acoustics) (cf. <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS3" title="2.3 The Speech Modality Has (Not Yet) Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.3</span></a>) modalities, we assess the capabilities of current FMs to i) generate synthetic affective samples, from which we infer the conveyed emotions with pre-trained emotion recognition classifiers<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Note that in principle, this can lead to a ‘closed loop’, as we cannot be sure whether the data used in the pre-trained emotion classifiers has not also been used in the training of the Foundation Models, but generally, it is unlikely, as high-quality affective data are rarely freely available on the Internet due to their privacy restrictions.</span></span></span>, and ii) analyse well-established datasets in the field in a zero-shot manner. To favour the comparability among the different modalities investigated, we focus on the ‘Big Six’ Ekman emotions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib44" title="">44</a>]</cite> (i. e., fear, anger, happiness, sadness, disgust, and surprise), in addition to the neutral state.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>The Vision Modality Has Changed</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Generation</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">In the visual domain, data synthesis started to obtain pseudorealistic results in the last decade thanks to the Generative Adversarial Network- (GAN) based models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib25" title="">25</a>]</cite>. Nowadays, a boost in the quality of the synthesised images has been achieved via text prompt inputs-based models, due to
i) the CLIP model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib38" title="">38</a>]</cite> and ii) the Diffuser model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib45" title="">45</a>]</cite>. The former was presented in conjunction with DALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib46" title="">46</a>]</cite>, as a model to predict how well a given caption describes an image; i. e., a text-to-image alignment. The latter
is a model that learns to reconstruct images by removing an added Gaussian noise through a Markov Chain. During inference, the model is able to generate new images from Gaussian noise, being more efficient than other generative architectures. Models such as Stable Diffusion (SD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib35" title="">35</a>]</cite> or DALL-E 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib47" title="">47</a>]</cite> integrate the CLIP and the Diffuser
models
to efficiently synthesise images with high semantic control, an important feature to generate affective samples. Further, the decision to make generative models, such as
SD, open-source has allowed the general public
to discover the potential of the technology, ultimately speeding up its advancements.
</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">We have leveraged one of the latest versions of SD <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0" title="">https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0</a></span></span></span> – i. e., Stable Diffusion XL (SDXL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib42" title="">42</a>]</cite> – to synthetically generate a face emotion dataset<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>For further information and access to the dataset, please contact the authors. </span></span></span>. This dataset is generated utilising prompts based on a fixed template with three sources of variation: i) the emotion, ii) the style (photorealistic, cartoon-painting, anime, and 3D), and iii) the demographic group. The template, along with the values explored for each attribute, are detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T1" title="Table 1 ‣ 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">1</span></a>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T2" title="Table 2 ‣ 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">2</span></a> presents a summary of the gathered dataset. Although our emotion model is based on the ‘Big Six’ Ekman emotions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib44" title="">44</a>]</cite> plus the neutral state, we employed these
basic emotions
together with a higher intensity variation, as defined in Plutchik’s model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib48" title="">48</a>]</cite>, to further emphasise the desired affective states. Also, note that the generation process spans 18 different demographic groups; determined by age, biological sex, and skin tone. Visual examples for each emotion and style are provided in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.F1" title="Figure 1 ‣ 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">1</span></a> for the demographic group <math alttext="&lt;young,woman,white&gt;" class="ltx_math_unparsed" display="inline" id="S2.SS1.SSS1.p2.1.m1.1"><semantics id="S2.SS1.SSS1.p2.1.m1.1a"><mrow id="S2.SS1.SSS1.p2.1.m1.1b"><mo id="S2.SS1.SSS1.p2.1.m1.1.1">&lt;</mo><mi id="S2.SS1.SSS1.p2.1.m1.1.2">y</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.3">o</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.4">u</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.5">n</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.6">g</mi><mo id="S2.SS1.SSS1.p2.1.m1.1.7">,</mo><mi id="S2.SS1.SSS1.p2.1.m1.1.8">w</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.9">o</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.10">m</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.11">a</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.12">n</mi><mo id="S2.SS1.SSS1.p2.1.m1.1.13">,</mo><mi id="S2.SS1.SSS1.p2.1.m1.1.14">w</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.15">h</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.16">i</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.17">t</mi><mi id="S2.SS1.SSS1.p2.1.m1.1.18">e</mi><mo id="S2.SS1.SSS1.p2.1.m1.1.19">&gt;</mo></mrow><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.1.m1.1c">&lt;young,woman,white&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p2.1.m1.1d">&lt; italic_y italic_o italic_u italic_n italic_g , italic_w italic_o italic_m italic_a italic_n , italic_w italic_h italic_i italic_t italic_e &gt;</annotation></semantics></math>. For the case of the photorealistic style, we played with the background to generate some samples in
realistic scenarios (e. g., outdoors, office, park). The generation process involved two experts, the SDXL base model and the SDXL refiner<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/" title="">https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/</a></span></span></span>, with a total of 40 steps (80 % in the base model, 20 % in the refiner) and a guidance value of 7.5. Together with the desired prompt, we utilised a negative prompt to highlight what we do not want to see in the output. Once the images were generated, we filtered them according to four principles: the presence of disfigurations or artifacts, nudity, the quality of the emotion generated, and the plausibility of the style. For the sake of this experiment, only one annotator conducted this data curation process.
</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.7.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S2.T1.8.2" style="font-size:90%;">Attributes defined in the input prompts to synthesise emotional facial images with Stable Diffusion XL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib42" title="">42</a>]</cite>.
The prompt template considers
three different sources of variation: the emotion, the style, and the demographic group. The latter is determined by three different demographic attributes: age, biological sex, and skin tone. We have also utilised a negative prompt, which includes all the styles that are not desired in the current synthesis.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.5">
<tr class="ltx_tr" id="S2.T1.5.6">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.5.6.1"><span class="ltx_text ltx_font_bold" id="S2.T1.5.6.1.1">Attribute</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.5.6.2"><span class="ltx_text ltx_font_bold" id="S2.T1.5.6.2.1">Values</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.4.5" style="padding-bottom:8.5359pt;">
<span class="ltx_text" id="S2.T1.4.4.5.1"></span><span class="ltx_text" id="S2.T1.4.4.5.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.4.4.5.2.1">
<span class="ltx_tr" id="S2.T1.4.4.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.4.4.5.2.1.1.1">Prompt</span></span>
<span class="ltx_tr" id="S2.T1.4.4.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.4.4.5.2.1.2.1">template</span></span>
</span></span><span class="ltx_text" id="S2.T1.4.4.5.3"></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.4.4" style="padding-bottom:8.5359pt;">
<span class="ltx_text" id="S2.T1.4.4.4.5"></span><span class="ltx_text" id="S2.T1.4.4.4.4">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.4.4.4.4.4">
<span class="ltx_tr" id="S2.T1.3.3.3.3.3.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.3.3.3.3.3.3.3">Face image of a <math alttext="&lt;age&gt;\,&lt;sex&gt;" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.1.1.1.m1.2"><semantics id="S2.T1.1.1.1.1.1.1.1.m1.2a"><mrow id="S2.T1.1.1.1.1.1.1.1.m1.2.2" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.cmml"><mrow id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.2.cmml"><mo fence="true" id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.2" rspace="0em" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.2.1.cmml">&lt;</mo><mrow id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.cmml"><mi id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.2" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.2.cmml">a</mi><mo id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3.cmml">g</mi><mo id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1a" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.4" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.4.cmml">e</mi></mrow><mo fence="true" id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.3" lspace="0em" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.2.1.cmml">&gt;</mo></mrow><mo id="S2.T1.1.1.1.1.1.1.1.m1.2.2.3" lspace="0.170em" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.3.cmml">⁢</mo><mrow id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.2.cmml"><mo fence="true" id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.2" rspace="0em" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.2.1.cmml">&lt;</mo><mrow id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.cmml"><mi id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.2" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.2.cmml">s</mi><mo id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.1" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.3" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.3.cmml">e</mi><mo id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.1a" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.4" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.4.cmml">x</mi></mrow><mo fence="true" id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.3" lspace="0em" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.2.1.cmml">&gt;</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.1.m1.2b"><apply id="S2.T1.1.1.1.1.1.1.1.m1.2.2.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2"><times id="S2.T1.1.1.1.1.1.1.1.m1.2.2.3.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.3"></times><apply id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.2.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.2.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.2">expectation</csymbol><apply id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1"><times id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1"></times><ci id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.2.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.2">𝑎</ci><ci id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3">𝑔</ci><ci id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.4.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.1.1.4">𝑒</ci></apply></apply><apply id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.2.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1"><csymbol cd="latexml" id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.2.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.2">expectation</csymbol><apply id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1"><times id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.1"></times><ci id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.2.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.2">𝑠</ci><ci id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.3.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.3">𝑒</ci><ci id="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.4.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.2.2.2.1.1.4">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.1.m1.2c">&lt;age&gt;\,&lt;sex&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.1.1.1.m1.2d">&lt; italic_a italic_g italic_e &gt; &lt; italic_s italic_e italic_x &gt;</annotation></semantics></math> with <math alttext="&lt;skin&gt;" class="ltx_Math" display="inline" id="S2.T1.2.2.2.2.2.2.2.m2.1"><semantics id="S2.T1.2.2.2.2.2.2.2.m2.1a"><mrow id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.2.cmml"><mo fence="true" id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.2" rspace="0em" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.2.1.cmml">&lt;</mo><mrow id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.cmml"><mi id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.2" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.2.cmml">s</mi><mo id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.1" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.3" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.3.cmml">k</mi><mo id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.1a" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.4" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.4.cmml">i</mi><mo id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.1b" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.5" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.5.cmml">n</mi></mrow><mo fence="true" id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.3" lspace="0em" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.2.2.2.m2.1b"><apply id="S2.T1.2.2.2.2.2.2.2.m2.1.1.2.cmml" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1"><csymbol cd="latexml" id="S2.T1.2.2.2.2.2.2.2.m2.1.1.2.1.cmml" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.2">expectation</csymbol><apply id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.cmml" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1"><times id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.1.cmml" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.1"></times><ci id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.2.cmml" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.2">𝑠</ci><ci id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.3.cmml" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.3">𝑘</ci><ci id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.4.cmml" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.4">𝑖</ci><ci id="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.5.cmml" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.1.1.5">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.2.2.2.m2.1c">&lt;skin&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.2.2.2.2.m2.1d">&lt; italic_s italic_k italic_i italic_n &gt;</annotation></semantics></math> skin, with a <math alttext="&lt;emotion&gt;" class="ltx_Math" display="inline" id="S2.T1.3.3.3.3.3.3.3.m3.1"><semantics id="S2.T1.3.3.3.3.3.3.3.m3.1a"><mrow id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.2.cmml"><mo fence="true" id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.2" rspace="0em" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.2.1.cmml">&lt;</mo><mrow id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.cmml"><mi id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.2" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.2.cmml">e</mi><mo id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.3" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.3.cmml">m</mi><mo id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1a" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.4" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.4.cmml">o</mi><mo id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1b" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.5" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.5.cmml">t</mi><mo id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1c" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.6" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.6.cmml">i</mi><mo id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1d" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.7" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.7.cmml">o</mi><mo id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1e" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.8" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.8.cmml">n</mi></mrow><mo fence="true" id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.3" lspace="0em" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.3.3.3.3.m3.1b"><apply id="S2.T1.3.3.3.3.3.3.3.m3.1.1.2.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1"><csymbol cd="latexml" id="S2.T1.3.3.3.3.3.3.3.m3.1.1.2.1.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.2">expectation</csymbol><apply id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1"><times id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.1"></times><ci id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.2.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.2">𝑒</ci><ci id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.3.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.3">𝑚</ci><ci id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.4.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.4">𝑜</ci><ci id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.5.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.5">𝑡</ci><ci id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.6.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.6">𝑖</ci><ci id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.7.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.7">𝑜</ci><ci id="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.8.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.1.1.8">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.3.3.3.3.m3.1c">&lt;emotion&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.3.3.3.3.3.m3.1d">&lt; italic_e italic_m italic_o italic_t italic_i italic_o italic_n &gt;</annotation></semantics></math> face,</span></span>
<span class="ltx_tr" id="S2.T1.4.4.4.4.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.4.4.4.4.4.4.1">in a <math alttext="&lt;style&gt;" class="ltx_Math" display="inline" id="S2.T1.4.4.4.4.4.4.1.m1.1"><semantics id="S2.T1.4.4.4.4.4.4.1.m1.1a"><mrow id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.2.cmml"><mo fence="true" id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.2" rspace="0em" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.cmml"><mi id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.2" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.2.cmml">s</mi><mo id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.1" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.3" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.3.cmml">t</mi><mo id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.1a" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.4" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.4.cmml">y</mi><mo id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.1b" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.5" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.5.cmml">l</mi><mo id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.1c" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.6" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.6.cmml">e</mi></mrow><mo fence="true" id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.3" lspace="0em" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.4.4.4.1.m1.1b"><apply id="S2.T1.4.4.4.4.4.4.1.m1.1.1.2.cmml" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1"><csymbol cd="latexml" id="S2.T1.4.4.4.4.4.4.1.m1.1.1.2.1.cmml" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.2">expectation</csymbol><apply id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.cmml" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1"><times id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.1.cmml" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.1"></times><ci id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.2.cmml" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.2">𝑠</ci><ci id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.3.cmml" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.3">𝑡</ci><ci id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.4.cmml" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.4">𝑦</ci><ci id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.5.cmml" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.5">𝑙</ci><ci id="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.6.cmml" xref="S2.T1.4.4.4.4.4.4.1.m1.1.1.1.1.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.4.4.4.1.m1.1c">&lt;style&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.4.4.4.4.1.m1.1d">&lt; italic_s italic_t italic_y italic_l italic_e &gt;</annotation></semantics></math> style, realistic eyes, white background, ultra quality, frontal picture,</span></span>
<span class="ltx_tr" id="S2.T1.4.4.4.4.4.5">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.4.4.4.4.4.5.1">looking at camera</span></span>
</span></span><span class="ltx_text" id="S2.T1.4.4.4.6"></span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.5">
<td class="ltx_td ltx_align_left" id="S2.T1.5.5.2" style="padding-bottom:8.5359pt;">
<span class="ltx_text" id="S2.T1.5.5.2.1"></span><span class="ltx_text" id="S2.T1.5.5.2.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.5.2.2.1">
<span class="ltx_tr" id="S2.T1.5.5.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.5.5.2.2.1.1.1">Negative</span></span>
<span class="ltx_tr" id="S2.T1.5.5.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.5.5.2.2.1.2.1">prompt</span></span>
</span></span><span class="ltx_text" id="S2.T1.5.5.2.3"></span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.5.5.1" style="padding-bottom:8.5359pt;">disfigured, unrealistic eyes, blurry, b&amp;w, <math alttext="&lt;style&gt;" class="ltx_Math" display="inline" id="S2.T1.5.5.1.m1.1"><semantics id="S2.T1.5.5.1.m1.1a"><mrow id="S2.T1.5.5.1.m1.1.1.1" xref="S2.T1.5.5.1.m1.1.1.2.cmml"><mo fence="true" id="S2.T1.5.5.1.m1.1.1.1.2" rspace="0em" xref="S2.T1.5.5.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S2.T1.5.5.1.m1.1.1.1.1" xref="S2.T1.5.5.1.m1.1.1.1.1.cmml"><mi id="S2.T1.5.5.1.m1.1.1.1.1.2" xref="S2.T1.5.5.1.m1.1.1.1.1.2.cmml">s</mi><mo id="S2.T1.5.5.1.m1.1.1.1.1.1" xref="S2.T1.5.5.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.5.5.1.m1.1.1.1.1.3" xref="S2.T1.5.5.1.m1.1.1.1.1.3.cmml">t</mi><mo id="S2.T1.5.5.1.m1.1.1.1.1.1a" xref="S2.T1.5.5.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.5.5.1.m1.1.1.1.1.4" xref="S2.T1.5.5.1.m1.1.1.1.1.4.cmml">y</mi><mo id="S2.T1.5.5.1.m1.1.1.1.1.1b" xref="S2.T1.5.5.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.5.5.1.m1.1.1.1.1.5" xref="S2.T1.5.5.1.m1.1.1.1.1.5.cmml">l</mi><mo id="S2.T1.5.5.1.m1.1.1.1.1.1c" xref="S2.T1.5.5.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.5.5.1.m1.1.1.1.1.6" xref="S2.T1.5.5.1.m1.1.1.1.1.6.cmml">e</mi></mrow><mo fence="true" id="S2.T1.5.5.1.m1.1.1.1.3" lspace="0em" xref="S2.T1.5.5.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.m1.1b"><apply id="S2.T1.5.5.1.m1.1.1.2.cmml" xref="S2.T1.5.5.1.m1.1.1.1"><csymbol cd="latexml" id="S2.T1.5.5.1.m1.1.1.2.1.cmml" xref="S2.T1.5.5.1.m1.1.1.1.2">expectation</csymbol><apply id="S2.T1.5.5.1.m1.1.1.1.1.cmml" xref="S2.T1.5.5.1.m1.1.1.1.1"><times id="S2.T1.5.5.1.m1.1.1.1.1.1.cmml" xref="S2.T1.5.5.1.m1.1.1.1.1.1"></times><ci id="S2.T1.5.5.1.m1.1.1.1.1.2.cmml" xref="S2.T1.5.5.1.m1.1.1.1.1.2">𝑠</ci><ci id="S2.T1.5.5.1.m1.1.1.1.1.3.cmml" xref="S2.T1.5.5.1.m1.1.1.1.1.3">𝑡</ci><ci id="S2.T1.5.5.1.m1.1.1.1.1.4.cmml" xref="S2.T1.5.5.1.m1.1.1.1.1.4">𝑦</ci><ci id="S2.T1.5.5.1.m1.1.1.1.1.5.cmml" xref="S2.T1.5.5.1.m1.1.1.1.1.5">𝑙</ci><ci id="S2.T1.5.5.1.m1.1.1.1.1.6.cmml" xref="S2.T1.5.5.1.m1.1.1.1.1.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.1.m1.1c">&lt;style&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.5.1.m1.1d">&lt; italic_s italic_t italic_y italic_l italic_e &gt;</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.7">
<td class="ltx_td ltx_align_left" id="S2.T1.5.7.1" style="padding-bottom:8.5359pt;">Emotion</td>
<td class="ltx_td ltx_align_left" id="S2.T1.5.7.2" style="padding-bottom:8.5359pt;">
<span class="ltx_text" id="S2.T1.5.7.2.1"></span><span class="ltx_text" id="S2.T1.5.7.2.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.7.2.2.1">
<span class="ltx_tr" id="S2.T1.5.7.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.5.7.2.2.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T1.5.7.2.2.1.1.1.1">neutral, fear and terror, anger and rage, happiness and joy,</span></span></span>
<span class="ltx_tr" id="S2.T1.5.7.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.5.7.2.2.1.2.1"><span class="ltx_text ltx_font_italic" id="S2.T1.5.7.2.2.1.2.1.1">sadness and grief, disgust and loathing, surprise and amazement</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.7.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.8">
<td class="ltx_td ltx_align_left" id="S2.T1.5.8.1" style="padding-bottom:7.11317pt;">Age</td>
<td class="ltx_td ltx_align_left" id="S2.T1.5.8.2" style="padding-bottom:7.11317pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.5.8.2.1">young, middle-aged, old</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.9">
<td class="ltx_td ltx_align_left" id="S2.T1.5.9.1" style="padding-bottom:7.11317pt;">Sex</td>
<td class="ltx_td ltx_align_left" id="S2.T1.5.9.2" style="padding-bottom:7.11317pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.5.9.2.1">man, woman</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.10">
<td class="ltx_td ltx_align_left" id="S2.T1.5.10.1" style="padding-bottom:7.11317pt;">Skin tone</td>
<td class="ltx_td ltx_align_left" id="S2.T1.5.10.2" style="padding-bottom:7.11317pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.5.10.2.1">white, brown, black</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.5.11.1" style="padding-bottom:7.11317pt;">Style</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.5.11.2" style="padding-bottom:7.11317pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.5.11.2.1">photorealistic, cartoon and painting, anime, 3D Pixar animation</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="486" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Synthetic facial images of a
white-skin,
young woman conveying the ‘Big Six’ Ekman emotions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib44" title="">44</a>]</cite>, in addition to the neutral state. All the images were generated with Stable Diffusion XL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib42" title="">42</a>]</cite>, conditioned on four different styles, namely photorealistic
(first row), cartoon-painting
(second row), anime
(third row), and 3D (fourth row).</span></figcaption>
</figure>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.35.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S2.T2.36.2" style="font-size:90%;">Summary of the face images generated with the Stable Diffusion XL model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib42" title="">42</a>]</cite>. The prompts for the generation are detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T1" title="Table 1 ‣ 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">1</span></a>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T2.33.33">
<tr class="ltx_tr" id="S2.T2.33.33.34">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.33.33.34.1" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.33.33.34.1.1">Emotion</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T2.33.33.34.2" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.33.33.34.2.1">Photorealistic</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T2.33.33.34.3" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.33.33.34.3.1">Cartoon</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T2.33.33.34.4" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.33.33.34.4.1">Anime</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T2.33.33.34.5" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.33.33.34.5.1">3D</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.4.4.4.5">Neutral</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.1.1.1.1"><math alttext="233" class="ltx_Math" display="inline" id="S2.T2.1.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.1.m1.1a"><mn id="S2.T2.1.1.1.1.m1.1.1" xref="S2.T2.1.1.1.1.m1.1.1.cmml">233</mn><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.m1.1b"><cn id="S2.T2.1.1.1.1.m1.1.1.cmml" type="integer" xref="S2.T2.1.1.1.1.m1.1.1">233</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.m1.1c">233</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.1.m1.1d">233</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.2.2.2.2"><math alttext="132" class="ltx_Math" display="inline" id="S2.T2.2.2.2.2.m1.1"><semantics id="S2.T2.2.2.2.2.m1.1a"><mn id="S2.T2.2.2.2.2.m1.1.1" xref="S2.T2.2.2.2.2.m1.1.1.cmml">132</mn><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.2.2.m1.1b"><cn id="S2.T2.2.2.2.2.m1.1.1.cmml" type="integer" xref="S2.T2.2.2.2.2.m1.1.1">132</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.2.2.m1.1c">132</annotation><annotation encoding="application/x-llamapun" id="S2.T2.2.2.2.2.m1.1d">132</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.3.3.3.3"><math alttext="131" class="ltx_Math" display="inline" id="S2.T2.3.3.3.3.m1.1"><semantics id="S2.T2.3.3.3.3.m1.1a"><mn id="S2.T2.3.3.3.3.m1.1.1" xref="S2.T2.3.3.3.3.m1.1.1.cmml">131</mn><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.3.3.m1.1b"><cn id="S2.T2.3.3.3.3.m1.1.1.cmml" type="integer" xref="S2.T2.3.3.3.3.m1.1.1">131</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.3.3.m1.1c">131</annotation><annotation encoding="application/x-llamapun" id="S2.T2.3.3.3.3.m1.1d">131</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.4.4.4.4"><math alttext="143" class="ltx_Math" display="inline" id="S2.T2.4.4.4.4.m1.1"><semantics id="S2.T2.4.4.4.4.m1.1a"><mn id="S2.T2.4.4.4.4.m1.1.1" xref="S2.T2.4.4.4.4.m1.1.1.cmml">143</mn><annotation-xml encoding="MathML-Content" id="S2.T2.4.4.4.4.m1.1b"><cn id="S2.T2.4.4.4.4.m1.1.1.cmml" type="integer" xref="S2.T2.4.4.4.4.m1.1.1">143</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.4.4.4.4.m1.1c">143</annotation><annotation encoding="application/x-llamapun" id="S2.T2.4.4.4.4.m1.1d">143</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.8">
<td class="ltx_td ltx_align_left" id="S2.T2.8.8.8.5">Fear</td>
<td class="ltx_td ltx_align_right" id="S2.T2.5.5.5.1"><math alttext="185" class="ltx_Math" display="inline" id="S2.T2.5.5.5.1.m1.1"><semantics id="S2.T2.5.5.5.1.m1.1a"><mn id="S2.T2.5.5.5.1.m1.1.1" xref="S2.T2.5.5.5.1.m1.1.1.cmml">185</mn><annotation-xml encoding="MathML-Content" id="S2.T2.5.5.5.1.m1.1b"><cn id="S2.T2.5.5.5.1.m1.1.1.cmml" type="integer" xref="S2.T2.5.5.5.1.m1.1.1">185</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.5.5.1.m1.1c">185</annotation><annotation encoding="application/x-llamapun" id="S2.T2.5.5.5.1.m1.1d">185</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.6.6.6.2"><math alttext="55" class="ltx_Math" display="inline" id="S2.T2.6.6.6.2.m1.1"><semantics id="S2.T2.6.6.6.2.m1.1a"><mn id="S2.T2.6.6.6.2.m1.1.1" xref="S2.T2.6.6.6.2.m1.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S2.T2.6.6.6.2.m1.1b"><cn id="S2.T2.6.6.6.2.m1.1.1.cmml" type="integer" xref="S2.T2.6.6.6.2.m1.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.6.6.6.2.m1.1c">55</annotation><annotation encoding="application/x-llamapun" id="S2.T2.6.6.6.2.m1.1d">55</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.7.7.7.3"><math alttext="94" class="ltx_Math" display="inline" id="S2.T2.7.7.7.3.m1.1"><semantics id="S2.T2.7.7.7.3.m1.1a"><mn id="S2.T2.7.7.7.3.m1.1.1" xref="S2.T2.7.7.7.3.m1.1.1.cmml">94</mn><annotation-xml encoding="MathML-Content" id="S2.T2.7.7.7.3.m1.1b"><cn id="S2.T2.7.7.7.3.m1.1.1.cmml" type="integer" xref="S2.T2.7.7.7.3.m1.1.1">94</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.7.7.7.3.m1.1c">94</annotation><annotation encoding="application/x-llamapun" id="S2.T2.7.7.7.3.m1.1d">94</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.8.8.8.4"><math alttext="136" class="ltx_Math" display="inline" id="S2.T2.8.8.8.4.m1.1"><semantics id="S2.T2.8.8.8.4.m1.1a"><mn id="S2.T2.8.8.8.4.m1.1.1" xref="S2.T2.8.8.8.4.m1.1.1.cmml">136</mn><annotation-xml encoding="MathML-Content" id="S2.T2.8.8.8.4.m1.1b"><cn id="S2.T2.8.8.8.4.m1.1.1.cmml" type="integer" xref="S2.T2.8.8.8.4.m1.1.1">136</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.8.8.4.m1.1c">136</annotation><annotation encoding="application/x-llamapun" id="S2.T2.8.8.8.4.m1.1d">136</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T2.12.12.12">
<td class="ltx_td ltx_align_left" id="S2.T2.12.12.12.5">Anger</td>
<td class="ltx_td ltx_align_right" id="S2.T2.9.9.9.1"><math alttext="183" class="ltx_Math" display="inline" id="S2.T2.9.9.9.1.m1.1"><semantics id="S2.T2.9.9.9.1.m1.1a"><mn id="S2.T2.9.9.9.1.m1.1.1" xref="S2.T2.9.9.9.1.m1.1.1.cmml">183</mn><annotation-xml encoding="MathML-Content" id="S2.T2.9.9.9.1.m1.1b"><cn id="S2.T2.9.9.9.1.m1.1.1.cmml" type="integer" xref="S2.T2.9.9.9.1.m1.1.1">183</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.9.9.9.1.m1.1c">183</annotation><annotation encoding="application/x-llamapun" id="S2.T2.9.9.9.1.m1.1d">183</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.10.10.10.2"><math alttext="165" class="ltx_Math" display="inline" id="S2.T2.10.10.10.2.m1.1"><semantics id="S2.T2.10.10.10.2.m1.1a"><mn id="S2.T2.10.10.10.2.m1.1.1" xref="S2.T2.10.10.10.2.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S2.T2.10.10.10.2.m1.1b"><cn id="S2.T2.10.10.10.2.m1.1.1.cmml" type="integer" xref="S2.T2.10.10.10.2.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.10.10.10.2.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="S2.T2.10.10.10.2.m1.1d">165</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.11.11.11.3"><math alttext="119" class="ltx_Math" display="inline" id="S2.T2.11.11.11.3.m1.1"><semantics id="S2.T2.11.11.11.3.m1.1a"><mn id="S2.T2.11.11.11.3.m1.1.1" xref="S2.T2.11.11.11.3.m1.1.1.cmml">119</mn><annotation-xml encoding="MathML-Content" id="S2.T2.11.11.11.3.m1.1b"><cn id="S2.T2.11.11.11.3.m1.1.1.cmml" type="integer" xref="S2.T2.11.11.11.3.m1.1.1">119</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.11.11.11.3.m1.1c">119</annotation><annotation encoding="application/x-llamapun" id="S2.T2.11.11.11.3.m1.1d">119</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.12.12.12.4"><math alttext="119" class="ltx_Math" display="inline" id="S2.T2.12.12.12.4.m1.1"><semantics id="S2.T2.12.12.12.4.m1.1a"><mn id="S2.T2.12.12.12.4.m1.1.1" xref="S2.T2.12.12.12.4.m1.1.1.cmml">119</mn><annotation-xml encoding="MathML-Content" id="S2.T2.12.12.12.4.m1.1b"><cn id="S2.T2.12.12.12.4.m1.1.1.cmml" type="integer" xref="S2.T2.12.12.12.4.m1.1.1">119</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.12.12.12.4.m1.1c">119</annotation><annotation encoding="application/x-llamapun" id="S2.T2.12.12.12.4.m1.1d">119</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T2.16.16.16">
<td class="ltx_td ltx_align_left" id="S2.T2.16.16.16.5">Happiness</td>
<td class="ltx_td ltx_align_right" id="S2.T2.13.13.13.1"><math alttext="223" class="ltx_Math" display="inline" id="S2.T2.13.13.13.1.m1.1"><semantics id="S2.T2.13.13.13.1.m1.1a"><mn id="S2.T2.13.13.13.1.m1.1.1" xref="S2.T2.13.13.13.1.m1.1.1.cmml">223</mn><annotation-xml encoding="MathML-Content" id="S2.T2.13.13.13.1.m1.1b"><cn id="S2.T2.13.13.13.1.m1.1.1.cmml" type="integer" xref="S2.T2.13.13.13.1.m1.1.1">223</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.13.13.13.1.m1.1c">223</annotation><annotation encoding="application/x-llamapun" id="S2.T2.13.13.13.1.m1.1d">223</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.14.14.14.2"><math alttext="202" class="ltx_Math" display="inline" id="S2.T2.14.14.14.2.m1.1"><semantics id="S2.T2.14.14.14.2.m1.1a"><mn id="S2.T2.14.14.14.2.m1.1.1" xref="S2.T2.14.14.14.2.m1.1.1.cmml">202</mn><annotation-xml encoding="MathML-Content" id="S2.T2.14.14.14.2.m1.1b"><cn id="S2.T2.14.14.14.2.m1.1.1.cmml" type="integer" xref="S2.T2.14.14.14.2.m1.1.1">202</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.14.14.14.2.m1.1c">202</annotation><annotation encoding="application/x-llamapun" id="S2.T2.14.14.14.2.m1.1d">202</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.15.15.15.3"><math alttext="118" class="ltx_Math" display="inline" id="S2.T2.15.15.15.3.m1.1"><semantics id="S2.T2.15.15.15.3.m1.1a"><mn id="S2.T2.15.15.15.3.m1.1.1" xref="S2.T2.15.15.15.3.m1.1.1.cmml">118</mn><annotation-xml encoding="MathML-Content" id="S2.T2.15.15.15.3.m1.1b"><cn id="S2.T2.15.15.15.3.m1.1.1.cmml" type="integer" xref="S2.T2.15.15.15.3.m1.1.1">118</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.15.15.15.3.m1.1c">118</annotation><annotation encoding="application/x-llamapun" id="S2.T2.15.15.15.3.m1.1d">118</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.16.16.16.4"><math alttext="142" class="ltx_Math" display="inline" id="S2.T2.16.16.16.4.m1.1"><semantics id="S2.T2.16.16.16.4.m1.1a"><mn id="S2.T2.16.16.16.4.m1.1.1" xref="S2.T2.16.16.16.4.m1.1.1.cmml">142</mn><annotation-xml encoding="MathML-Content" id="S2.T2.16.16.16.4.m1.1b"><cn id="S2.T2.16.16.16.4.m1.1.1.cmml" type="integer" xref="S2.T2.16.16.16.4.m1.1.1">142</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.16.16.16.4.m1.1c">142</annotation><annotation encoding="application/x-llamapun" id="S2.T2.16.16.16.4.m1.1d">142</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T2.20.20.20">
<td class="ltx_td ltx_align_left" id="S2.T2.20.20.20.5">Sadness</td>
<td class="ltx_td ltx_align_right" id="S2.T2.17.17.17.1"><math alttext="179" class="ltx_Math" display="inline" id="S2.T2.17.17.17.1.m1.1"><semantics id="S2.T2.17.17.17.1.m1.1a"><mn id="S2.T2.17.17.17.1.m1.1.1" xref="S2.T2.17.17.17.1.m1.1.1.cmml">179</mn><annotation-xml encoding="MathML-Content" id="S2.T2.17.17.17.1.m1.1b"><cn id="S2.T2.17.17.17.1.m1.1.1.cmml" type="integer" xref="S2.T2.17.17.17.1.m1.1.1">179</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.17.17.17.1.m1.1c">179</annotation><annotation encoding="application/x-llamapun" id="S2.T2.17.17.17.1.m1.1d">179</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.18.18.18.2"><math alttext="156" class="ltx_Math" display="inline" id="S2.T2.18.18.18.2.m1.1"><semantics id="S2.T2.18.18.18.2.m1.1a"><mn id="S2.T2.18.18.18.2.m1.1.1" xref="S2.T2.18.18.18.2.m1.1.1.cmml">156</mn><annotation-xml encoding="MathML-Content" id="S2.T2.18.18.18.2.m1.1b"><cn id="S2.T2.18.18.18.2.m1.1.1.cmml" type="integer" xref="S2.T2.18.18.18.2.m1.1.1">156</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.18.18.18.2.m1.1c">156</annotation><annotation encoding="application/x-llamapun" id="S2.T2.18.18.18.2.m1.1d">156</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.19.19.19.3"><math alttext="137" class="ltx_Math" display="inline" id="S2.T2.19.19.19.3.m1.1"><semantics id="S2.T2.19.19.19.3.m1.1a"><mn id="S2.T2.19.19.19.3.m1.1.1" xref="S2.T2.19.19.19.3.m1.1.1.cmml">137</mn><annotation-xml encoding="MathML-Content" id="S2.T2.19.19.19.3.m1.1b"><cn id="S2.T2.19.19.19.3.m1.1.1.cmml" type="integer" xref="S2.T2.19.19.19.3.m1.1.1">137</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.19.19.19.3.m1.1c">137</annotation><annotation encoding="application/x-llamapun" id="S2.T2.19.19.19.3.m1.1d">137</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.20.20.20.4"><math alttext="99" class="ltx_Math" display="inline" id="S2.T2.20.20.20.4.m1.1"><semantics id="S2.T2.20.20.20.4.m1.1a"><mn id="S2.T2.20.20.20.4.m1.1.1" xref="S2.T2.20.20.20.4.m1.1.1.cmml">99</mn><annotation-xml encoding="MathML-Content" id="S2.T2.20.20.20.4.m1.1b"><cn id="S2.T2.20.20.20.4.m1.1.1.cmml" type="integer" xref="S2.T2.20.20.20.4.m1.1.1">99</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.20.20.20.4.m1.1c">99</annotation><annotation encoding="application/x-llamapun" id="S2.T2.20.20.20.4.m1.1d">99</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T2.24.24.24">
<td class="ltx_td ltx_align_left" id="S2.T2.24.24.24.5">Disgust</td>
<td class="ltx_td ltx_align_right" id="S2.T2.21.21.21.1"><math alttext="173" class="ltx_Math" display="inline" id="S2.T2.21.21.21.1.m1.1"><semantics id="S2.T2.21.21.21.1.m1.1a"><mn id="S2.T2.21.21.21.1.m1.1.1" xref="S2.T2.21.21.21.1.m1.1.1.cmml">173</mn><annotation-xml encoding="MathML-Content" id="S2.T2.21.21.21.1.m1.1b"><cn id="S2.T2.21.21.21.1.m1.1.1.cmml" type="integer" xref="S2.T2.21.21.21.1.m1.1.1">173</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.21.21.21.1.m1.1c">173</annotation><annotation encoding="application/x-llamapun" id="S2.T2.21.21.21.1.m1.1d">173</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.22.22.22.2"><math alttext="90" class="ltx_Math" display="inline" id="S2.T2.22.22.22.2.m1.1"><semantics id="S2.T2.22.22.22.2.m1.1a"><mn id="S2.T2.22.22.22.2.m1.1.1" xref="S2.T2.22.22.22.2.m1.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S2.T2.22.22.22.2.m1.1b"><cn id="S2.T2.22.22.22.2.m1.1.1.cmml" type="integer" xref="S2.T2.22.22.22.2.m1.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.22.22.22.2.m1.1c">90</annotation><annotation encoding="application/x-llamapun" id="S2.T2.22.22.22.2.m1.1d">90</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.23.23.23.3"><math alttext="56" class="ltx_Math" display="inline" id="S2.T2.23.23.23.3.m1.1"><semantics id="S2.T2.23.23.23.3.m1.1a"><mn id="S2.T2.23.23.23.3.m1.1.1" xref="S2.T2.23.23.23.3.m1.1.1.cmml">56</mn><annotation-xml encoding="MathML-Content" id="S2.T2.23.23.23.3.m1.1b"><cn id="S2.T2.23.23.23.3.m1.1.1.cmml" type="integer" xref="S2.T2.23.23.23.3.m1.1.1">56</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.23.23.23.3.m1.1c">56</annotation><annotation encoding="application/x-llamapun" id="S2.T2.23.23.23.3.m1.1d">56</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.24.24.24.4"><math alttext="40" class="ltx_Math" display="inline" id="S2.T2.24.24.24.4.m1.1"><semantics id="S2.T2.24.24.24.4.m1.1a"><mn id="S2.T2.24.24.24.4.m1.1.1" xref="S2.T2.24.24.24.4.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S2.T2.24.24.24.4.m1.1b"><cn id="S2.T2.24.24.24.4.m1.1.1.cmml" type="integer" xref="S2.T2.24.24.24.4.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.24.24.24.4.m1.1c">40</annotation><annotation encoding="application/x-llamapun" id="S2.T2.24.24.24.4.m1.1d">40</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T2.28.28.28">
<td class="ltx_td ltx_align_left" id="S2.T2.28.28.28.5">Surprise</td>
<td class="ltx_td ltx_align_right" id="S2.T2.25.25.25.1"><math alttext="184" class="ltx_Math" display="inline" id="S2.T2.25.25.25.1.m1.1"><semantics id="S2.T2.25.25.25.1.m1.1a"><mn id="S2.T2.25.25.25.1.m1.1.1" xref="S2.T2.25.25.25.1.m1.1.1.cmml">184</mn><annotation-xml encoding="MathML-Content" id="S2.T2.25.25.25.1.m1.1b"><cn id="S2.T2.25.25.25.1.m1.1.1.cmml" type="integer" xref="S2.T2.25.25.25.1.m1.1.1">184</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.25.25.25.1.m1.1c">184</annotation><annotation encoding="application/x-llamapun" id="S2.T2.25.25.25.1.m1.1d">184</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.26.26.26.2"><math alttext="147" class="ltx_Math" display="inline" id="S2.T2.26.26.26.2.m1.1"><semantics id="S2.T2.26.26.26.2.m1.1a"><mn id="S2.T2.26.26.26.2.m1.1.1" xref="S2.T2.26.26.26.2.m1.1.1.cmml">147</mn><annotation-xml encoding="MathML-Content" id="S2.T2.26.26.26.2.m1.1b"><cn id="S2.T2.26.26.26.2.m1.1.1.cmml" type="integer" xref="S2.T2.26.26.26.2.m1.1.1">147</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.26.26.26.2.m1.1c">147</annotation><annotation encoding="application/x-llamapun" id="S2.T2.26.26.26.2.m1.1d">147</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.27.27.27.3"><math alttext="120" class="ltx_Math" display="inline" id="S2.T2.27.27.27.3.m1.1"><semantics id="S2.T2.27.27.27.3.m1.1a"><mn id="S2.T2.27.27.27.3.m1.1.1" xref="S2.T2.27.27.27.3.m1.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S2.T2.27.27.27.3.m1.1b"><cn id="S2.T2.27.27.27.3.m1.1.1.cmml" type="integer" xref="S2.T2.27.27.27.3.m1.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.27.27.27.3.m1.1c">120</annotation><annotation encoding="application/x-llamapun" id="S2.T2.27.27.27.3.m1.1d">120</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T2.28.28.28.4"><math alttext="139" class="ltx_Math" display="inline" id="S2.T2.28.28.28.4.m1.1"><semantics id="S2.T2.28.28.28.4.m1.1a"><mn id="S2.T2.28.28.28.4.m1.1.1" xref="S2.T2.28.28.28.4.m1.1.1.cmml">139</mn><annotation-xml encoding="MathML-Content" id="S2.T2.28.28.28.4.m1.1b"><cn id="S2.T2.28.28.28.4.m1.1.1.cmml" type="integer" xref="S2.T2.28.28.28.4.m1.1.1">139</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.28.28.28.4.m1.1c">139</annotation><annotation encoding="application/x-llamapun" id="S2.T2.28.28.28.4.m1.1d">139</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T2.33.33.33">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T2.29.29.29.1"><math alttext="\boldsymbol{\Sigma}" class="ltx_Math" display="inline" id="S2.T2.29.29.29.1.m1.1"><semantics id="S2.T2.29.29.29.1.m1.1a"><mi id="S2.T2.29.29.29.1.m1.1.1" xref="S2.T2.29.29.29.1.m1.1.1.cmml">𝚺</mi><annotation-xml encoding="MathML-Content" id="S2.T2.29.29.29.1.m1.1b"><ci id="S2.T2.29.29.29.1.m1.1.1.cmml" xref="S2.T2.29.29.29.1.m1.1.1">𝚺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.29.29.29.1.m1.1c">\boldsymbol{\Sigma}</annotation><annotation encoding="application/x-llamapun" id="S2.T2.29.29.29.1.m1.1d">bold_Σ</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T2.30.30.30.2"><math alttext="1\,360" class="ltx_Math" display="inline" id="S2.T2.30.30.30.2.m1.1"><semantics id="S2.T2.30.30.30.2.m1.1a"><mn id="S2.T2.30.30.30.2.m1.1.1" xref="S2.T2.30.30.30.2.m1.1.1.cmml">1 360</mn><annotation-xml encoding="MathML-Content" id="S2.T2.30.30.30.2.m1.1b"><cn id="S2.T2.30.30.30.2.m1.1.1.cmml" type="integer" xref="S2.T2.30.30.30.2.m1.1.1">1360</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.30.30.30.2.m1.1c">1\,360</annotation><annotation encoding="application/x-llamapun" id="S2.T2.30.30.30.2.m1.1d">1 360</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T2.31.31.31.3"><math alttext="947" class="ltx_Math" display="inline" id="S2.T2.31.31.31.3.m1.1"><semantics id="S2.T2.31.31.31.3.m1.1a"><mn id="S2.T2.31.31.31.3.m1.1.1" xref="S2.T2.31.31.31.3.m1.1.1.cmml">947</mn><annotation-xml encoding="MathML-Content" id="S2.T2.31.31.31.3.m1.1b"><cn id="S2.T2.31.31.31.3.m1.1.1.cmml" type="integer" xref="S2.T2.31.31.31.3.m1.1.1">947</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.31.31.31.3.m1.1c">947</annotation><annotation encoding="application/x-llamapun" id="S2.T2.31.31.31.3.m1.1d">947</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T2.32.32.32.4"><math alttext="775" class="ltx_Math" display="inline" id="S2.T2.32.32.32.4.m1.1"><semantics id="S2.T2.32.32.32.4.m1.1a"><mn id="S2.T2.32.32.32.4.m1.1.1" xref="S2.T2.32.32.32.4.m1.1.1.cmml">775</mn><annotation-xml encoding="MathML-Content" id="S2.T2.32.32.32.4.m1.1b"><cn id="S2.T2.32.32.32.4.m1.1.1.cmml" type="integer" xref="S2.T2.32.32.32.4.m1.1.1">775</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.32.32.32.4.m1.1c">775</annotation><annotation encoding="application/x-llamapun" id="S2.T2.32.32.32.4.m1.1d">775</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T2.33.33.33.5"><math alttext="818" class="ltx_Math" display="inline" id="S2.T2.33.33.33.5.m1.1"><semantics id="S2.T2.33.33.33.5.m1.1a"><mn id="S2.T2.33.33.33.5.m1.1.1" xref="S2.T2.33.33.33.5.m1.1.1.cmml">818</mn><annotation-xml encoding="MathML-Content" id="S2.T2.33.33.33.5.m1.1b"><cn id="S2.T2.33.33.33.5.m1.1.1.cmml" type="integer" xref="S2.T2.33.33.33.5.m1.1.1">818</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.33.33.33.5.m1.1c">818</annotation><annotation encoding="application/x-llamapun" id="S2.T2.33.33.33.5.m1.1d">818</annotation></semantics></math></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1">We
now
aim to automatically verify the affective quality of the generated facial images with Face Emotion Recognition (FER) models. We employ the manually annotated subset of the AffectNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib33" title="">33</a>]</cite> to develop these models.
The images belonging to this dataset are annotated in terms of eleven emotions. Nevertheless, we select the images corresponding to the emotions fear, anger, happiness, sadness, disgust, and surprise in addition to the neutral class. We process the selected images with OpenFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib49" title="">49</a>]</cite> to extract features related to a subset of the Action Units (AU) defined in the Facial Action Coding System (FACS). Specifically, OpenFace extracts 35 features per facial image, indicating the presence (0 or 1) and the intensity (in a scale from 0 – not present – to 5 – present with maximum intensity) of a subset of the AUs. We discard the images that OpenFace fails to process; for instance, due to the absence of a face in the image. <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T3" title="In 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> summarises the resulting data in terms of the number of images per emotion in the training and the validation partitions.</p>
</div>
<figure class="ltx_table" id="S2.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S2.T3.4.2" style="font-size:90%;">Summary of the face images selected from AffectNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib33" title="">33</a>]</cite> in the training and the validation partitions. </span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T3.1.1">
<tr class="ltx_tr" id="S2.T3.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T3.1.1.2.1" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.2.1.1">Emotions</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T3.1.1.2.2" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.2.2.1">Training</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T3.1.1.2.3" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.2.3.1">Validation</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T3.1.1.3.1">Neutral</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T3.1.1.3.2">74 873</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T3.1.1.3.3">500</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.4">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.4.1">Fear</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.4.2">6 378</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.4.3">500</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.5">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.5.1">Anger</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.5.2">24 881</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.5.3">500</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.6">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.6.1">Happiness</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.6.2">134 411</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.6.3">500</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.7">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.7.1">Sadness</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.7.2">25 458</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.7.3">500</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.8">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.8.1">Disgust</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.8.2">3 803</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.8.3">500</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.9">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.9.1">Surprise</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.9.2">14 090</td>
<td class="ltx_td ltx_align_right" id="S2.T3.1.1.9.3">500</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T3.1.1.1.1"><math alttext="\boldsymbol{\Sigma}" class="ltx_Math" display="inline" id="S2.T3.1.1.1.1.m1.1"><semantics id="S2.T3.1.1.1.1.m1.1a"><mi id="S2.T3.1.1.1.1.m1.1.1" xref="S2.T3.1.1.1.1.m1.1.1.cmml">𝚺</mi><annotation-xml encoding="MathML-Content" id="S2.T3.1.1.1.1.m1.1b"><ci id="S2.T3.1.1.1.1.m1.1.1.cmml" xref="S2.T3.1.1.1.1.m1.1.1">𝚺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.1.1.1.1.m1.1c">\boldsymbol{\Sigma}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.1.1.1.1.m1.1d">bold_Σ</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T3.1.1.1.2">283 894</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T3.1.1.1.3">3 500</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p4">
<p class="ltx_p" id="S2.SS1.SSS1.p4.1">We start our preliminary investigation by training FER models with Support Vector Classifiers (SVC), as these are considered a standard machine learning technique with excellent results in a wide range of problems.
We compare their performance when utilising a linear and a Radial Basis Function (RBF) kernel. One challenge associated with the selected dataset is the imbalanced training samples in terms of the emotional classes (cf. <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T3" title="In 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>), which impacts the performance of the trained models. To overcome this issue, we consider weighting the training data, so that the samples corresponding to the least represented classes have more importance than the samples corresponding to the most represented classes when training the models. We fine-tune our models optimising the regularisation parameter <math alttext="C\in[10^{-2},10^{-1},1,10,10^{2}]" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p4.1.m1.5"><semantics id="S2.SS1.SSS1.p4.1.m1.5a"><mrow id="S2.SS1.SSS1.p4.1.m1.5.5" xref="S2.SS1.SSS1.p4.1.m1.5.5.cmml"><mi id="S2.SS1.SSS1.p4.1.m1.5.5.5" xref="S2.SS1.SSS1.p4.1.m1.5.5.5.cmml">C</mi><mo id="S2.SS1.SSS1.p4.1.m1.5.5.4" xref="S2.SS1.SSS1.p4.1.m1.5.5.4.cmml">∈</mo><mrow id="S2.SS1.SSS1.p4.1.m1.5.5.3.3" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.4.cmml"><mo id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.4" stretchy="false" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.4.cmml">[</mo><msup id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.cmml"><mn id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.2" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.2.cmml">10</mn><mrow id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3.cmml"><mo id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3a" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3.cmml">−</mo><mn id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3.2" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3.2.cmml">2</mn></mrow></msup><mo id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.5" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.4.cmml">,</mo><msup id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.cmml"><mn id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.2" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.2.cmml">10</mn><mrow id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3.cmml"><mo id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3a" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3.cmml">−</mo><mn id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3.2" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3.2.cmml">1</mn></mrow></msup><mo id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.6" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.4.cmml">,</mo><mn id="S2.SS1.SSS1.p4.1.m1.1.1" xref="S2.SS1.SSS1.p4.1.m1.1.1.cmml">1</mn><mo id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.7" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.4.cmml">,</mo><mn id="S2.SS1.SSS1.p4.1.m1.2.2" xref="S2.SS1.SSS1.p4.1.m1.2.2.cmml">10</mn><mo id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.8" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.4.cmml">,</mo><msup id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.cmml"><mn id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.2" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.2.cmml">10</mn><mn id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.3" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.3.cmml">2</mn></msup><mo id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.9" stretchy="false" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p4.1.m1.5b"><apply id="S2.SS1.SSS1.p4.1.m1.5.5.cmml" xref="S2.SS1.SSS1.p4.1.m1.5.5"><in id="S2.SS1.SSS1.p4.1.m1.5.5.4.cmml" xref="S2.SS1.SSS1.p4.1.m1.5.5.4"></in><ci id="S2.SS1.SSS1.p4.1.m1.5.5.5.cmml" xref="S2.SS1.SSS1.p4.1.m1.5.5.5">𝐶</ci><list id="S2.SS1.SSS1.p4.1.m1.5.5.3.4.cmml" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.3"><apply id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.cmml" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.1.cmml" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1">superscript</csymbol><cn id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.2.cmml" type="integer" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.2">10</cn><apply id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3.cmml" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3"><minus id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3.1.cmml" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3"></minus><cn id="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3.2.cmml" type="integer" xref="S2.SS1.SSS1.p4.1.m1.3.3.1.1.1.3.2">2</cn></apply></apply><apply id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.cmml" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.1.cmml" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2">superscript</csymbol><cn id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.2.cmml" type="integer" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.2">10</cn><apply id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3.cmml" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3"><minus id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3.1.cmml" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3"></minus><cn id="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3.2.cmml" type="integer" xref="S2.SS1.SSS1.p4.1.m1.4.4.2.2.2.3.2">1</cn></apply></apply><cn id="S2.SS1.SSS1.p4.1.m1.1.1.cmml" type="integer" xref="S2.SS1.SSS1.p4.1.m1.1.1">1</cn><cn id="S2.SS1.SSS1.p4.1.m1.2.2.cmml" type="integer" xref="S2.SS1.SSS1.p4.1.m1.2.2">10</cn><apply id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.cmml" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.1.cmml" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3">superscript</csymbol><cn id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.2.cmml" type="integer" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.2">10</cn><cn id="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.3.cmml" type="integer" xref="S2.SS1.SSS1.p4.1.m1.5.5.3.3.3.3">2</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p4.1.m1.5c">C\in[10^{-2},10^{-1},1,10,10^{2}]</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p4.1.m1.5d">italic_C ∈ [ 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , 1 , 10 , 10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]</annotation></semantics></math>. The performance of the optimal models on the validation partition is depicted in <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T4" title="In 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p5">
<p class="ltx_p" id="S2.SS1.SSS1.p5.1">We also contrast the performance of the SVC-based FER models with a state-of-the-art Vision Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib50" title="">50</a>]</cite> for Facial Expression Recognition<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/trpakov/vit-face-expression" title="">https://huggingface.co/trpakov/vit-face-expression</a></span></span></span> (ViT – FER), trained on the Facial Expression Recognition 2013 (FER-2013) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib51" title="">51</a>]</cite>. In this case, we use the pre-trained model off-the-shelf – without fine-tuning – and evaluate its performance on the validation partition of our subset of AffectNet. The obtained results are reported in <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T4" title="In 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T4.9.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S2.T4.10.2" style="font-size:90%;">Performance summary of the trained Support Vector Classifier-based models for Face Emotion Recognition on the validation partition of the considered subset of AffectNet. We also include the performance of a state-of-the-art Vision Transformer for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the metric to assess the model performances. </span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T4.7.7">
<tr class="ltx_tr" id="S2.T4.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S2.T4.1.1.1.2" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T4.1.1.1.2.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T4.1.1.1.1" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T4.1.1.1.1.1">ACC (<math alttext="\%" class="ltx_Math" display="inline" id="S2.T4.1.1.1.1.1.m1.1"><semantics id="S2.T4.1.1.1.1.1.m1.1a"><mo id="S2.T4.1.1.1.1.1.m1.1.1" xref="S2.T4.1.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.T4.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.T4.1.1.1.1.1.m1.1.1.cmml" xref="S2.T4.1.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.1.1.1.1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S2.T4.1.1.1.1.1.m1.1d">%</annotation></semantics></math>)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T4.2.2.2">
<td class="ltx_td ltx_border_t" id="S2.T4.2.2.2.2" style="padding-bottom:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.2.2.2.3" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_italic" id="S2.T4.2.2.2.3.1">Kernel</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.2.2.2.4" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_italic" id="S2.T4.2.2.2.4.1">Weighted Samples</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.2.2.2.1" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_italic" id="S2.T4.2.2.2.1.1">Optimal <math alttext="C" class="ltx_Math" display="inline" id="S2.T4.2.2.2.1.1.m1.1"><semantics id="S2.T4.2.2.2.1.1.m1.1a"><mi id="S2.T4.2.2.2.1.1.m1.1.1" xref="S2.T4.2.2.2.1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.T4.2.2.2.1.1.m1.1b"><ci id="S2.T4.2.2.2.1.1.m1.1.1.cmml" xref="S2.T4.2.2.2.1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.2.2.2.1.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S2.T4.2.2.2.1.1.m1.1d">italic_C</annotation></semantics></math></span></td>
<td class="ltx_td ltx_border_t" id="S2.T4.2.2.2.5" style="padding-bottom:2.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S2.T4.7.7.8">
<td class="ltx_td ltx_align_left" id="S2.T4.7.7.8.1" rowspan="4"><span class="ltx_text" id="S2.T4.7.7.8.1.1">SVC</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T4.7.7.8.2">Linear</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.7.7.8.3">✗</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T4.7.7.8.4">10</td>
<td class="ltx_td ltx_align_right" id="S2.T4.7.7.8.5">27.3</td>
</tr>
<tr class="ltx_tr" id="S2.T4.4.4.4">
<td class="ltx_td ltx_align_left" id="S2.T4.4.4.4.3">Linear</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.3.3.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T4.3.3.3.1.m1.1"><semantics id="S2.T4.3.3.3.1.m1.1a"><mi id="S2.T4.3.3.3.1.m1.1.1" mathvariant="normal" xref="S2.T4.3.3.3.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T4.3.3.3.1.m1.1b"><ci id="S2.T4.3.3.3.1.m1.1.1.cmml" xref="S2.T4.3.3.3.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.3.3.3.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T4.3.3.3.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T4.4.4.4.2"><math alttext="10^{2}" class="ltx_Math" display="inline" id="S2.T4.4.4.4.2.m1.1"><semantics id="S2.T4.4.4.4.2.m1.1a"><msup id="S2.T4.4.4.4.2.m1.1.1" xref="S2.T4.4.4.4.2.m1.1.1.cmml"><mn id="S2.T4.4.4.4.2.m1.1.1.2" xref="S2.T4.4.4.4.2.m1.1.1.2.cmml">10</mn><mn id="S2.T4.4.4.4.2.m1.1.1.3" xref="S2.T4.4.4.4.2.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T4.4.4.4.2.m1.1b"><apply id="S2.T4.4.4.4.2.m1.1.1.cmml" xref="S2.T4.4.4.4.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T4.4.4.4.2.m1.1.1.1.cmml" xref="S2.T4.4.4.4.2.m1.1.1">superscript</csymbol><cn id="S2.T4.4.4.4.2.m1.1.1.2.cmml" type="integer" xref="S2.T4.4.4.4.2.m1.1.1.2">10</cn><cn id="S2.T4.4.4.4.2.m1.1.1.3.cmml" type="integer" xref="S2.T4.4.4.4.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.4.4.4.2.m1.1c">10^{2}</annotation><annotation encoding="application/x-llamapun" id="S2.T4.4.4.4.2.m1.1d">10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T4.4.4.4.4">28.7</td>
</tr>
<tr class="ltx_tr" id="S2.T4.5.5.5">
<td class="ltx_td ltx_align_left" id="S2.T4.5.5.5.2">Rbf</td>
<td class="ltx_td ltx_align_center" id="S2.T4.5.5.5.3">✗</td>
<td class="ltx_td ltx_align_right" id="S2.T4.5.5.5.1"><math alttext="10^{2}" class="ltx_Math" display="inline" id="S2.T4.5.5.5.1.m1.1"><semantics id="S2.T4.5.5.5.1.m1.1a"><msup id="S2.T4.5.5.5.1.m1.1.1" xref="S2.T4.5.5.5.1.m1.1.1.cmml"><mn id="S2.T4.5.5.5.1.m1.1.1.2" xref="S2.T4.5.5.5.1.m1.1.1.2.cmml">10</mn><mn id="S2.T4.5.5.5.1.m1.1.1.3" xref="S2.T4.5.5.5.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T4.5.5.5.1.m1.1b"><apply id="S2.T4.5.5.5.1.m1.1.1.cmml" xref="S2.T4.5.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T4.5.5.5.1.m1.1.1.1.cmml" xref="S2.T4.5.5.5.1.m1.1.1">superscript</csymbol><cn id="S2.T4.5.5.5.1.m1.1.1.2.cmml" type="integer" xref="S2.T4.5.5.5.1.m1.1.1.2">10</cn><cn id="S2.T4.5.5.5.1.m1.1.1.3.cmml" type="integer" xref="S2.T4.5.5.5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.5.5.5.1.m1.1c">10^{2}</annotation><annotation encoding="application/x-llamapun" id="S2.T4.5.5.5.1.m1.1d">10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T4.5.5.5.4">28.5</td>
</tr>
<tr class="ltx_tr" id="S2.T4.7.7.7">
<td class="ltx_td ltx_align_left" id="S2.T4.7.7.7.3">Rbf</td>
<td class="ltx_td ltx_align_center" id="S2.T4.6.6.6.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T4.6.6.6.1.m1.1"><semantics id="S2.T4.6.6.6.1.m1.1a"><mi id="S2.T4.6.6.6.1.m1.1.1" mathvariant="normal" xref="S2.T4.6.6.6.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T4.6.6.6.1.m1.1b"><ci id="S2.T4.6.6.6.1.m1.1.1.cmml" xref="S2.T4.6.6.6.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.6.6.6.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T4.6.6.6.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T4.7.7.7.2"><math alttext="10^{2}" class="ltx_Math" display="inline" id="S2.T4.7.7.7.2.m1.1"><semantics id="S2.T4.7.7.7.2.m1.1a"><msup id="S2.T4.7.7.7.2.m1.1.1" xref="S2.T4.7.7.7.2.m1.1.1.cmml"><mn id="S2.T4.7.7.7.2.m1.1.1.2" xref="S2.T4.7.7.7.2.m1.1.1.2.cmml">10</mn><mn id="S2.T4.7.7.7.2.m1.1.1.3" xref="S2.T4.7.7.7.2.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T4.7.7.7.2.m1.1b"><apply id="S2.T4.7.7.7.2.m1.1.1.cmml" xref="S2.T4.7.7.7.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T4.7.7.7.2.m1.1.1.1.cmml" xref="S2.T4.7.7.7.2.m1.1.1">superscript</csymbol><cn id="S2.T4.7.7.7.2.m1.1.1.2.cmml" type="integer" xref="S2.T4.7.7.7.2.m1.1.1.2">10</cn><cn id="S2.T4.7.7.7.2.m1.1.1.3.cmml" type="integer" xref="S2.T4.7.7.7.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.7.7.7.2.m1.1c">10^{2}</annotation><annotation encoding="application/x-llamapun" id="S2.T4.7.7.7.2.m1.1d">10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S2.T4.7.7.7.4">29.7</td>
</tr>
<tr class="ltx_tr" id="S2.T4.7.7.9">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="4" id="S2.T4.7.7.9.1">ViT – FER</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T4.7.7.9.2">43.9</td>
</tr>
<tr class="ltx_tr" id="S2.T4.7.7.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" colspan="4" id="S2.T4.7.7.10.1">Chance Level</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T4.7.7.10.2">14.3</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p6">
<p class="ltx_p" id="S2.SS1.SSS1.p6.1">The best performance on the validation partition of the AffectNet dataset (cf. <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T4" title="In 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>) is obtained with the ViT – FER model. Thus, we use this model to assess the affective quality of the generated facial images. The results obtained from the conducted experiments exemplify the breakthrough
of working with end-to-end approaches, operating on the raw images instead of on the features extracted from them. Nevertheless, it is also worth mentioning that AffectNet was collected in the wild, which may complicate the estimation of the AUs from the facial images and, in turn, worsen the performance of the SVC-based models.</p>
</div>
<figure class="ltx_table" id="S2.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T5.4.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S2.T5.5.2" style="font-size:90%;">Accuracy (ACC) and Unweighted Average Recall (UAR) scores obtained when analysing the facial images generated in the four different styles with the ViT – FER pre-trained model. </span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T5.2.2">
<tr class="ltx_tr" id="S2.T5.2.2.3">
<td class="ltx_td ltx_border_tt" id="S2.T5.2.2.3.1" style="padding-bottom:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S2.T5.2.2.3.2" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T5.2.2.3.2.1">Generated Styles</span></td>
</tr>
<tr class="ltx_tr" id="S2.T5.2.2.4">
<td class="ltx_td" id="S2.T5.2.2.4.1" style="padding-bottom:2.0pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T5.2.2.4.2" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T5.2.2.4.2.1">Photorealistic</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T5.2.2.4.3" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T5.2.2.4.3.1">Cartoon-Painting</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T5.2.2.4.4" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T5.2.2.4.4.1">Anime</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T5.2.2.4.5" style="padding-bottom:2.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T5.2.2.4.5.1">3D</span></td>
</tr>
<tr class="ltx_tr" id="S2.T5.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T5.1.1.1.1">ACC (<math alttext="\%" class="ltx_Math" display="inline" id="S2.T5.1.1.1.1.m1.1"><semantics id="S2.T5.1.1.1.1.m1.1a"><mo id="S2.T5.1.1.1.1.m1.1.1" xref="S2.T5.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.T5.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.T5.1.1.1.1.m1.1.1.cmml" xref="S2.T5.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T5.1.1.1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S2.T5.1.1.1.1.m1.1d">%</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T5.1.1.1.2">35.0</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T5.1.1.1.3">43.9</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T5.1.1.1.4">42.5</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T5.1.1.1.5">57.5</td>
</tr>
<tr class="ltx_tr" id="S2.T5.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T5.2.2.2.1">UAR (<math alttext="\%" class="ltx_Math" display="inline" id="S2.T5.2.2.2.1.m1.1"><semantics id="S2.T5.2.2.2.1.m1.1a"><mo id="S2.T5.2.2.2.1.m1.1.1" xref="S2.T5.2.2.2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.T5.2.2.2.1.m1.1b"><csymbol cd="latexml" id="S2.T5.2.2.2.1.m1.1.1.cmml" xref="S2.T5.2.2.2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T5.2.2.2.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S2.T5.2.2.2.1.m1.1d">%</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S2.T5.2.2.2.2">30.9</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S2.T5.2.2.2.3">38.3</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S2.T5.2.2.2.4">39.1</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S2.T5.2.2.2.5">49.5</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p7">
<p class="ltx_p" id="S2.SS1.SSS1.p7.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T5" title="In 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> summarises the results obtained when analysing the generated facial images with the four different styles utilising the ViT – FER pre-trained model. <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.F2" title="In 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> presents the confusion matrices computed, comparing the model inferences and the ground truth. Across styles, the worst results are those of the photorealistic style, as denoted by both accuracy – Weighted Average Recall (WAR) – and Unweighted Average Recall (UAR)<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>UAR is the sum of recall per class divided by the number of classes – this reflects imbalances and is a standard measure in the field.</span></span></span>.
In contrast, the best results are obtained with
the 3D style.
Interestingly, in all the 4 styles both the neutral and the happiness emotions
consistently obtain the best results. These classes are traditionally over-represented in face datasets (e. g., see the training set distribution of AffectNet in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T3" title="Table 3 ‣ 2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">3</span></a>), probably due to the nature of the data sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib52" title="">52</a>]</cite>. Consequently, it is expected for the generative model to be biased towards those emotions, which would also explain the difficulty to generate samples for classes like disgust or fear,
which obtain the worst results.
</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="830" id="S2.F2.sf1.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F2.sf1.3.2" style="font-size:90%;">Photorealistic</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="830" id="S2.F2.sf2.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F2.sf2.3.2" style="font-size:90%;">Cartoon-painting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="830" id="S2.F2.sf3.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S2.F2.sf3.3.2" style="font-size:90%;">Anime</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="830" id="S2.F2.sf4.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S2.F2.sf4.3.2" style="font-size:90%;">3D</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Confusion matrices obtained by analysing the facial images generated according to the four different styles with the ViT – FER pre-trained model.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Analysis</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">In order to evaluate the affective analytical capabilities of FMs
in the image domain, we explore their performance in a zero-shot emotion recognition
task under
different configurations. Our experiments are conducted on the considered validation set of AffectNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib33" title="">33</a>]</cite> (cf. <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS1.SSS1" title="2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1.1</span></a>), as it is balanced, in-the-wild, and manually annotated. We compare three different approaches relying on model-prompting. In the first two, we start by extracting AU-based features
with OpenFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib49" title="">49</a>]</cite>, and
we provide these features
in a textual format as input to a FM. Recalling from <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS1.SSS1" title="2.1.1 Generation ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1.1</span></a>, OpenFace predicts both the presence and the intensity of a subset of AUs; hence, two approaches can be derived from its predictions. On the other hand, the third approach consists in directly feeding the images within the prompt of a FM. Note that the first two approaches can be addressed with a Language Foundation Model (LFM), for which we select the LLaMA2 7B model. We utilise a Multimodal Foundation Model (MFM) for the third scenario; specifically, the LLaVA1.5 7B model <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/liuhaotian/llava-v1.5-7b" title="">https://huggingface.co/liuhaotian/llava-v1.5-7b</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib54" title="">54</a>]</cite>.
This is an
open-source MFM
with visual capabilities trained by fine-tuning Vicuna<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="">https://lmsys.org/blog/2023-03-30-vicuna/</a></span></span></span>, an already fine-tuned version of LLaMA, with GPT-4 generated data. The selected models allow them having the same
number of parameters.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1">In <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T7" title="In 2.1.2 Analysis ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a>, we present the results of the aforementioned scenarios. The prompts utilised for each scenario are detailed in <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T6" title="In 2.1.2 Analysis ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a>. We also include
in <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T7" title="In 2.1.2 Analysis ‣ 2.1 The Vision Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a> the results of the ViT – FER model on the considered validation set of AffectNet for comparability
purposes. Among the three zero-shot approaches, we obtain the best results when feeding the prompts with the raw images. Both AU-based prompt approaches exhibit accuracy results close to chance. The LLaVA model achieves an accuracy only 4 points below
the ViT – FER model, which was explicitly trained on the FER-2013 dataset to recognise emotions.
This is an interesting result, which suggests that the LLaVA1.5 model presents emergent affective capabilities, despite not being specifically trained on affective computing tasks.
</p>
</div>
<figure class="ltx_table" id="S2.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T6.18.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S2.T6.19.2" style="font-size:90%;">Prompts employed to perform zero-shot emotion recognition
with Foundation Models in different scenarios. The prompts including (i. e., first two rows) AU information are injected in LLaMA2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib55" title="">55</a>]</cite>, while the prompt including the raw image is injected to a LLaVA1.5 model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib54" title="">54</a>]</cite>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T6.16">
<tr class="ltx_tr" id="S2.T6.16.17">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T6.16.17.1"><span class="ltx_text ltx_font_bold" id="S2.T6.16.17.1.1">Approach</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T6.16.17.2"><span class="ltx_text ltx_font_bold" id="S2.T6.16.17.2.1">Prompt template</span></td>
</tr>
<tr class="ltx_tr" id="S2.T6.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T6.7.7.8" style="padding-bottom:56.9055pt;">
<span class="ltx_text" id="S2.T6.7.7.8.1"></span><span class="ltx_text" id="S2.T6.7.7.8.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T6.7.7.8.2.1">
<span class="ltx_tr" id="S2.T6.7.7.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.7.7.8.2.1.1.1">AU presence</span></span>
</span></span><span class="ltx_text" id="S2.T6.7.7.8.3"></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T6.7.7.7" style="padding-bottom:56.9055pt;">
<span class="ltx_text" id="S2.T6.7.7.7.8"></span><span class="ltx_text" id="S2.T6.7.7.7.7">
<span class="ltx_tabular ltx_align_middle" id="S2.T6.7.7.7.7.7">
<span class="ltx_tr" id="S2.T6.4.4.4.4.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.4.4.4.4.4.4.4"><math alttext="&lt;" class="ltx_Math" display="inline" id="S2.T6.1.1.1.1.1.1.1.m1.1"><semantics id="S2.T6.1.1.1.1.1.1.1.m1.1a"><mo id="S2.T6.1.1.1.1.1.1.1.m1.1.1" xref="S2.T6.1.1.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.1.1.1.1.1.1.1.m1.1b"><lt id="S2.T6.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T6.1.1.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.1.1.1.1.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.1.1.1.1.1.1.1.m1.1d">&lt;</annotation></semantics></math>s<math alttext="&gt;" class="ltx_Math" display="inline" id="S2.T6.2.2.2.2.2.2.2.m2.1"><semantics id="S2.T6.2.2.2.2.2.2.2.m2.1a"><mo id="S2.T6.2.2.2.2.2.2.2.m2.1.1" xref="S2.T6.2.2.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.2.2.2.2.2.2.2.m2.1b"><gt id="S2.T6.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S2.T6.2.2.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.2.2.2.2.2.2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.2.2.2.2.2.2.2.m2.1d">&gt;</annotation></semantics></math>[INST] <math alttext="&lt;&lt;" class="ltx_Math" display="inline" id="S2.T6.3.3.3.3.3.3.3.m3.1"><semantics id="S2.T6.3.3.3.3.3.3.3.m3.1a"><mo id="S2.T6.3.3.3.3.3.3.3.m3.1.1" xref="S2.T6.3.3.3.3.3.3.3.m3.1.1.cmml">&lt;&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.3.3.3.3.3.3.3.m3.1b"><csymbol cd="latexml" id="S2.T6.3.3.3.3.3.3.3.m3.1.1.cmml" xref="S2.T6.3.3.3.3.3.3.3.m3.1.1">much-less-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.3.3.3.3.3.3.3.m3.1c">&lt;&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.3.3.3.3.3.3.3.m3.1d">&lt; &lt;</annotation></semantics></math>SYS<math alttext="&gt;&gt;" class="ltx_Math" display="inline" id="S2.T6.4.4.4.4.4.4.4.m4.1"><semantics id="S2.T6.4.4.4.4.4.4.4.m4.1a"><mo id="S2.T6.4.4.4.4.4.4.4.m4.1.1" xref="S2.T6.4.4.4.4.4.4.4.m4.1.1.cmml">&gt;&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.4.4.4.4.4.4.4.m4.1b"><csymbol cd="latexml" id="S2.T6.4.4.4.4.4.4.4.m4.1.1.cmml" xref="S2.T6.4.4.4.4.4.4.4.m4.1.1">much-greater-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.4.4.4.4.4.4.4.m4.1c">&gt;&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.4.4.4.4.4.4.4.m4.1d">&gt; &gt;</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S2.T6.7.7.7.7.7.8">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.7.7.7.7.7.8.1">You are a highly skilled Affective Computing system with an expertise in</span></span>
<span class="ltx_tr" id="S2.T6.7.7.7.7.7.9">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.7.7.7.7.7.9.1">accurately predicting emotion classes from Action Units. I will provide you a list</span></span>
<span class="ltx_tr" id="S2.T6.7.7.7.7.7.10">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.7.7.7.7.7.10.1">of Action Units present in a face. Your task is to answer the most likely emotion</span></span>
<span class="ltx_tr" id="S2.T6.7.7.7.7.7.11">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.7.7.7.7.7.11.1">class, without any further explanation. Please, provide only one of the following</span></span>
<span class="ltx_tr" id="S2.T6.7.7.7.7.7.12">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.7.7.7.7.7.12.1">classes as answer: Neutral, Fear, Anger, Happiness, Sadness, Disgust, Surprise.</span></span>
<span class="ltx_tr" id="S2.T6.7.7.7.7.7.13">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.7.7.7.7.7.13.1">The question is, which is the most likely emotion if the following Action Units</span></span>
<span class="ltx_tr" id="S2.T6.6.6.6.6.6.6">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.6.6.6.6.6.6.2">are present<math alttext="&lt;&lt;/" class="ltx_Math" display="inline" id="S2.T6.5.5.5.5.5.5.1.m1.1"><semantics id="S2.T6.5.5.5.5.5.5.1.m1.1a"><mrow id="S2.T6.5.5.5.5.5.5.1.m1.1.1" xref="S2.T6.5.5.5.5.5.5.1.m1.1.1.cmml"><mi id="S2.T6.5.5.5.5.5.5.1.m1.1.1.2" xref="S2.T6.5.5.5.5.5.5.1.m1.1.1.2.cmml"></mi><mo id="S2.T6.5.5.5.5.5.5.1.m1.1.1.1" rspace="0em" xref="S2.T6.5.5.5.5.5.5.1.m1.1.1.1.cmml">&lt;&lt;</mo><mo id="S2.T6.5.5.5.5.5.5.1.m1.1.1.3" lspace="0em" xref="S2.T6.5.5.5.5.5.5.1.m1.1.1.3.cmml">/</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T6.5.5.5.5.5.5.1.m1.1b"><apply id="S2.T6.5.5.5.5.5.5.1.m1.1.1.cmml" xref="S2.T6.5.5.5.5.5.5.1.m1.1.1"><csymbol cd="latexml" id="S2.T6.5.5.5.5.5.5.1.m1.1.1.1.cmml" xref="S2.T6.5.5.5.5.5.5.1.m1.1.1.1">much-less-than</csymbol><csymbol cd="latexml" id="S2.T6.5.5.5.5.5.5.1.m1.1.1.2.cmml" xref="S2.T6.5.5.5.5.5.5.1.m1.1.1.2">absent</csymbol><divide id="S2.T6.5.5.5.5.5.5.1.m1.1.1.3.cmml" xref="S2.T6.5.5.5.5.5.5.1.m1.1.1.3"></divide></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.5.5.5.5.5.5.1.m1.1c">&lt;&lt;/</annotation><annotation encoding="application/x-llamapun" id="S2.T6.5.5.5.5.5.5.1.m1.1d">&lt; &lt; /</annotation></semantics></math>SYS<math alttext="&gt;&gt;" class="ltx_Math" display="inline" id="S2.T6.6.6.6.6.6.6.2.m2.1"><semantics id="S2.T6.6.6.6.6.6.6.2.m2.1a"><mo id="S2.T6.6.6.6.6.6.6.2.m2.1.1" xref="S2.T6.6.6.6.6.6.6.2.m2.1.1.cmml">&gt;&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.6.6.6.6.6.6.2.m2.1b"><csymbol cd="latexml" id="S2.T6.6.6.6.6.6.6.2.m2.1.1.cmml" xref="S2.T6.6.6.6.6.6.6.2.m2.1.1">much-greater-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.6.6.6.6.6.6.2.m2.1c">&gt;&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.6.6.6.6.6.6.2.m2.1d">&gt; &gt;</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S2.T6.7.7.7.7.7.14">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.7.7.7.7.7.14.1">“{AU}”.[/INST]</span></span>
<span class="ltx_tr" id="S2.T6.7.7.7.7.7.7">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.7.7.7.7.7.7.1"><math alttext="\#\#\#" class="ltx_Math" display="inline" id="S2.T6.7.7.7.7.7.7.1.m1.1"><semantics id="S2.T6.7.7.7.7.7.7.1.m1.1a"><mrow id="S2.T6.7.7.7.7.7.7.1.m1.1.1" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1.cmml"><mi id="S2.T6.7.7.7.7.7.7.1.m1.1.1.2" mathvariant="normal" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1.2.cmml">#</mi><mo id="S2.T6.7.7.7.7.7.7.1.m1.1.1.1" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.T6.7.7.7.7.7.7.1.m1.1.1.3" mathvariant="normal" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1.3.cmml">#</mi><mo id="S2.T6.7.7.7.7.7.7.1.m1.1.1.1a" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.T6.7.7.7.7.7.7.1.m1.1.1.4" mathvariant="normal" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1.4.cmml">#</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T6.7.7.7.7.7.7.1.m1.1b"><apply id="S2.T6.7.7.7.7.7.7.1.m1.1.1.cmml" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1"><times id="S2.T6.7.7.7.7.7.7.1.m1.1.1.1.cmml" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1.1"></times><ci id="S2.T6.7.7.7.7.7.7.1.m1.1.1.2.cmml" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1.2">#</ci><ci id="S2.T6.7.7.7.7.7.7.1.m1.1.1.3.cmml" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1.3">#</ci><ci id="S2.T6.7.7.7.7.7.7.1.m1.1.1.4.cmml" xref="S2.T6.7.7.7.7.7.7.1.m1.1.1.4">#</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.7.7.7.7.7.7.1.m1.1c">\#\#\#</annotation><annotation encoding="application/x-llamapun" id="S2.T6.7.7.7.7.7.7.1.m1.1d"># # #</annotation></semantics></math>Response:</span></span>
</span></span><span class="ltx_text" id="S2.T6.7.7.7.9"></span></td>
</tr>
<tr class="ltx_tr" id="S2.T6.14.14">
<td class="ltx_td ltx_align_left" id="S2.T6.14.14.8" style="padding-bottom:56.9055pt;">
<span class="ltx_text" id="S2.T6.14.14.8.1"></span><span class="ltx_text" id="S2.T6.14.14.8.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T6.14.14.8.2.1">
<span class="ltx_tr" id="S2.T6.14.14.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.14.14.8.2.1.1.1">AU intensity</span></span>
</span></span><span class="ltx_text" id="S2.T6.14.14.8.3"></span></td>
<td class="ltx_td ltx_align_left" id="S2.T6.14.14.7" style="padding-bottom:56.9055pt;">
<span class="ltx_text" id="S2.T6.14.14.7.8"></span><span class="ltx_text" id="S2.T6.14.14.7.7">
<span class="ltx_tabular ltx_align_middle" id="S2.T6.14.14.7.7.7">
<span class="ltx_tr" id="S2.T6.11.11.4.4.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.11.11.4.4.4.4.4"><math alttext="&lt;" class="ltx_Math" display="inline" id="S2.T6.8.8.1.1.1.1.1.m1.1"><semantics id="S2.T6.8.8.1.1.1.1.1.m1.1a"><mo id="S2.T6.8.8.1.1.1.1.1.m1.1.1" xref="S2.T6.8.8.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.8.8.1.1.1.1.1.m1.1b"><lt id="S2.T6.8.8.1.1.1.1.1.m1.1.1.cmml" xref="S2.T6.8.8.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.8.8.1.1.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.8.8.1.1.1.1.1.m1.1d">&lt;</annotation></semantics></math>s<math alttext="&gt;" class="ltx_Math" display="inline" id="S2.T6.9.9.2.2.2.2.2.m2.1"><semantics id="S2.T6.9.9.2.2.2.2.2.m2.1a"><mo id="S2.T6.9.9.2.2.2.2.2.m2.1.1" xref="S2.T6.9.9.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.9.9.2.2.2.2.2.m2.1b"><gt id="S2.T6.9.9.2.2.2.2.2.m2.1.1.cmml" xref="S2.T6.9.9.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.9.9.2.2.2.2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.9.9.2.2.2.2.2.m2.1d">&gt;</annotation></semantics></math>[INST] <math alttext="&lt;&lt;" class="ltx_Math" display="inline" id="S2.T6.10.10.3.3.3.3.3.m3.1"><semantics id="S2.T6.10.10.3.3.3.3.3.m3.1a"><mo id="S2.T6.10.10.3.3.3.3.3.m3.1.1" xref="S2.T6.10.10.3.3.3.3.3.m3.1.1.cmml">&lt;&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.10.10.3.3.3.3.3.m3.1b"><csymbol cd="latexml" id="S2.T6.10.10.3.3.3.3.3.m3.1.1.cmml" xref="S2.T6.10.10.3.3.3.3.3.m3.1.1">much-less-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.10.10.3.3.3.3.3.m3.1c">&lt;&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.10.10.3.3.3.3.3.m3.1d">&lt; &lt;</annotation></semantics></math>SYS<math alttext="&gt;&gt;" class="ltx_Math" display="inline" id="S2.T6.11.11.4.4.4.4.4.m4.1"><semantics id="S2.T6.11.11.4.4.4.4.4.m4.1a"><mo id="S2.T6.11.11.4.4.4.4.4.m4.1.1" xref="S2.T6.11.11.4.4.4.4.4.m4.1.1.cmml">&gt;&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.11.11.4.4.4.4.4.m4.1b"><csymbol cd="latexml" id="S2.T6.11.11.4.4.4.4.4.m4.1.1.cmml" xref="S2.T6.11.11.4.4.4.4.4.m4.1.1">much-greater-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.11.11.4.4.4.4.4.m4.1c">&gt;&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.11.11.4.4.4.4.4.m4.1d">&gt; &gt;</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S2.T6.14.14.7.7.7.8">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.14.14.7.7.7.8.1">You are a highly skilled Affective Computing system with an expertise in</span></span>
<span class="ltx_tr" id="S2.T6.14.14.7.7.7.9">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.14.14.7.7.7.9.1">accurately predicting emotion classes from Action Units. I will provide you a</span></span>
<span class="ltx_tr" id="S2.T6.14.14.7.7.7.10">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.14.14.7.7.7.10.1">JSON object with the intensities of the Action Units present in a face. Your</span></span>
<span class="ltx_tr" id="S2.T6.14.14.7.7.7.11">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.14.14.7.7.7.11.1">task is to answer the most likely emotion class, without any further explanation.</span></span>
<span class="ltx_tr" id="S2.T6.14.14.7.7.7.12">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.14.14.7.7.7.12.1">Please, provide only one of the following classes as answer: Neutral, Fear,</span></span>
<span class="ltx_tr" id="S2.T6.14.14.7.7.7.13">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.14.14.7.7.7.13.1">Anger, Happiness, Sadness, Disgust, Surprise.</span></span>
<span class="ltx_tr" id="S2.T6.14.14.7.7.7.14">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.14.14.7.7.7.14.1">The question is, which is the most likely emotion if the following Action Units</span></span>
<span class="ltx_tr" id="S2.T6.13.13.6.6.6.6">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.13.13.6.6.6.6.2">are present<math alttext="&lt;&lt;/" class="ltx_Math" display="inline" id="S2.T6.12.12.5.5.5.5.1.m1.1"><semantics id="S2.T6.12.12.5.5.5.5.1.m1.1a"><mrow id="S2.T6.12.12.5.5.5.5.1.m1.1.1" xref="S2.T6.12.12.5.5.5.5.1.m1.1.1.cmml"><mi id="S2.T6.12.12.5.5.5.5.1.m1.1.1.2" xref="S2.T6.12.12.5.5.5.5.1.m1.1.1.2.cmml"></mi><mo id="S2.T6.12.12.5.5.5.5.1.m1.1.1.1" rspace="0em" xref="S2.T6.12.12.5.5.5.5.1.m1.1.1.1.cmml">&lt;&lt;</mo><mo id="S2.T6.12.12.5.5.5.5.1.m1.1.1.3" lspace="0em" xref="S2.T6.12.12.5.5.5.5.1.m1.1.1.3.cmml">/</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T6.12.12.5.5.5.5.1.m1.1b"><apply id="S2.T6.12.12.5.5.5.5.1.m1.1.1.cmml" xref="S2.T6.12.12.5.5.5.5.1.m1.1.1"><csymbol cd="latexml" id="S2.T6.12.12.5.5.5.5.1.m1.1.1.1.cmml" xref="S2.T6.12.12.5.5.5.5.1.m1.1.1.1">much-less-than</csymbol><csymbol cd="latexml" id="S2.T6.12.12.5.5.5.5.1.m1.1.1.2.cmml" xref="S2.T6.12.12.5.5.5.5.1.m1.1.1.2">absent</csymbol><divide id="S2.T6.12.12.5.5.5.5.1.m1.1.1.3.cmml" xref="S2.T6.12.12.5.5.5.5.1.m1.1.1.3"></divide></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.12.12.5.5.5.5.1.m1.1c">&lt;&lt;/</annotation><annotation encoding="application/x-llamapun" id="S2.T6.12.12.5.5.5.5.1.m1.1d">&lt; &lt; /</annotation></semantics></math>SYS<math alttext="&gt;&gt;" class="ltx_Math" display="inline" id="S2.T6.13.13.6.6.6.6.2.m2.1"><semantics id="S2.T6.13.13.6.6.6.6.2.m2.1a"><mo id="S2.T6.13.13.6.6.6.6.2.m2.1.1" xref="S2.T6.13.13.6.6.6.6.2.m2.1.1.cmml">&gt;&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.13.13.6.6.6.6.2.m2.1b"><csymbol cd="latexml" id="S2.T6.13.13.6.6.6.6.2.m2.1.1.cmml" xref="S2.T6.13.13.6.6.6.6.2.m2.1.1">much-greater-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.13.13.6.6.6.6.2.m2.1c">&gt;&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.13.13.6.6.6.6.2.m2.1d">&gt; &gt;</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S2.T6.14.14.7.7.7.15">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.14.14.7.7.7.15.1">“{AU}”.[/INST]</span></span>
<span class="ltx_tr" id="S2.T6.14.14.7.7.7.7">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.14.14.7.7.7.7.1"><math alttext="\#\#\#" class="ltx_Math" display="inline" id="S2.T6.14.14.7.7.7.7.1.m1.1"><semantics id="S2.T6.14.14.7.7.7.7.1.m1.1a"><mrow id="S2.T6.14.14.7.7.7.7.1.m1.1.1" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1.cmml"><mi id="S2.T6.14.14.7.7.7.7.1.m1.1.1.2" mathvariant="normal" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1.2.cmml">#</mi><mo id="S2.T6.14.14.7.7.7.7.1.m1.1.1.1" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.T6.14.14.7.7.7.7.1.m1.1.1.3" mathvariant="normal" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1.3.cmml">#</mi><mo id="S2.T6.14.14.7.7.7.7.1.m1.1.1.1a" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.T6.14.14.7.7.7.7.1.m1.1.1.4" mathvariant="normal" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1.4.cmml">#</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T6.14.14.7.7.7.7.1.m1.1b"><apply id="S2.T6.14.14.7.7.7.7.1.m1.1.1.cmml" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1"><times id="S2.T6.14.14.7.7.7.7.1.m1.1.1.1.cmml" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1.1"></times><ci id="S2.T6.14.14.7.7.7.7.1.m1.1.1.2.cmml" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1.2">#</ci><ci id="S2.T6.14.14.7.7.7.7.1.m1.1.1.3.cmml" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1.3">#</ci><ci id="S2.T6.14.14.7.7.7.7.1.m1.1.1.4.cmml" xref="S2.T6.14.14.7.7.7.7.1.m1.1.1.4">#</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.14.14.7.7.7.7.1.m1.1c">\#\#\#</annotation><annotation encoding="application/x-llamapun" id="S2.T6.14.14.7.7.7.7.1.m1.1d"># # #</annotation></semantics></math>Response:</span></span>
</span></span><span class="ltx_text" id="S2.T6.14.14.7.9"></span></td>
</tr>
<tr class="ltx_tr" id="S2.T6.16.16">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T6.16.16.3" style="padding-bottom:8.5359pt;">
<span class="ltx_text" id="S2.T6.16.16.3.1"></span><span class="ltx_text" id="S2.T6.16.16.3.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T6.16.16.3.2.1">
<span class="ltx_tr" id="S2.T6.16.16.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.16.16.3.2.1.1.1">Image</span></span>
</span></span><span class="ltx_text" id="S2.T6.16.16.3.3"></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T6.16.16.2" style="padding-bottom:8.5359pt;">
<span class="ltx_text" id="S2.T6.16.16.2.3"></span><span class="ltx_text" id="S2.T6.16.16.2.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T6.16.16.2.2.2">
<span class="ltx_tr" id="S2.T6.16.16.2.2.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.16.16.2.2.2.2.2"><math alttext="&lt;" class="ltx_Math" display="inline" id="S2.T6.15.15.1.1.1.1.1.m1.1"><semantics id="S2.T6.15.15.1.1.1.1.1.m1.1a"><mo id="S2.T6.15.15.1.1.1.1.1.m1.1.1" xref="S2.T6.15.15.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.15.15.1.1.1.1.1.m1.1b"><lt id="S2.T6.15.15.1.1.1.1.1.m1.1.1.cmml" xref="S2.T6.15.15.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.15.15.1.1.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.15.15.1.1.1.1.1.m1.1d">&lt;</annotation></semantics></math>image<math alttext="&gt;" class="ltx_Math" display="inline" id="S2.T6.16.16.2.2.2.2.2.m2.1"><semantics id="S2.T6.16.16.2.2.2.2.2.m2.1a"><mo id="S2.T6.16.16.2.2.2.2.2.m2.1.1" xref="S2.T6.16.16.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T6.16.16.2.2.2.2.2.m2.1b"><gt id="S2.T6.16.16.2.2.2.2.2.m2.1.1.cmml" xref="S2.T6.16.16.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T6.16.16.2.2.2.2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T6.16.16.2.2.2.2.2.m2.1d">&gt;</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S2.T6.16.16.2.2.2.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.16.16.2.2.2.3.1">USER: You are provided with a face image of a person. Classify the most likely</span></span>
<span class="ltx_tr" id="S2.T6.16.16.2.2.2.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.16.16.2.2.2.4.1">emotional state depicted into one of the classes between brackets [Neutral,</span></span>
<span class="ltx_tr" id="S2.T6.16.16.2.2.2.5">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.16.16.2.2.2.5.1">Fear, Anger, Happiness, Sadness, Disgust, Surprise]</span></span>
<span class="ltx_tr" id="S2.T6.16.16.2.2.2.6">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T6.16.16.2.2.2.6.1">ASSISTANT:</span></span>
</span></span><span class="ltx_text" id="S2.T6.16.16.2.4"></span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S2.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T7.24.2.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S2.T7.2.1" style="font-size:90%;">Accuracy scores obtained with the LLaMA2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib55" title="">55</a>]</cite> and LLaVA1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib54" title="">54</a>]</cite> Foundation Models on the validation set of AffecNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib33" title="">33</a>]</cite>. We have included as well the performance obtained with the ViT – FER model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib50" title="">50</a>]</cite> trained on FER-<math alttext="2013" class="ltx_Math" display="inline" id="S2.T7.2.1.m1.1"><semantics id="S2.T7.2.1.m1.1b"><mn id="S2.T7.2.1.m1.1.1" xref="S2.T7.2.1.m1.1.1.cmml">2013</mn><annotation-xml encoding="MathML-Content" id="S2.T7.2.1.m1.1c"><cn id="S2.T7.2.1.m1.1.1.cmml" type="integer" xref="S2.T7.2.1.m1.1.1">2013</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.2.1.m1.1d">2013</annotation><annotation encoding="application/x-llamapun" id="S2.T7.2.1.m1.1e">2013</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib51" title="">51</a>]</cite> for comparison purposes.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T7.22.20">
<tr class="ltx_tr" id="S2.T7.22.20.21">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T7.22.20.21.1"><span class="ltx_text ltx_font_bold" id="S2.T7.22.20.21.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T7.22.20.21.2"><span class="ltx_text ltx_font_bold" id="S2.T7.22.20.21.2.1">Input</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T7.22.20.21.3"><span class="ltx_text ltx_font_bold" id="S2.T7.22.20.21.3.1">ACC (%)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T7.5.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T7.4.2.2.2">LLaMA<math alttext="2" class="ltx_Math" display="inline" id="S2.T7.3.1.1.1.m1.1"><semantics id="S2.T7.3.1.1.1.m1.1a"><mn id="S2.T7.3.1.1.1.m1.1.1" xref="S2.T7.3.1.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S2.T7.3.1.1.1.m1.1b"><cn id="S2.T7.3.1.1.1.m1.1.1.cmml" type="integer" xref="S2.T7.3.1.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.3.1.1.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S2.T7.3.1.1.1.m1.1d">2</annotation></semantics></math> <math alttext="7" class="ltx_Math" display="inline" id="S2.T7.4.2.2.2.m2.1"><semantics id="S2.T7.4.2.2.2.m2.1a"><mn id="S2.T7.4.2.2.2.m2.1.1" xref="S2.T7.4.2.2.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S2.T7.4.2.2.2.m2.1b"><cn id="S2.T7.4.2.2.2.m2.1.1.cmml" type="integer" xref="S2.T7.4.2.2.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.4.2.2.2.m2.1c">7</annotation><annotation encoding="application/x-llamapun" id="S2.T7.4.2.2.2.m2.1d">7</annotation></semantics></math>B</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T7.5.3.3.4">Prompt with information of AU presence</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T7.5.3.3.3"><math alttext="18.7" class="ltx_Math" display="inline" id="S2.T7.5.3.3.3.m1.1"><semantics id="S2.T7.5.3.3.3.m1.1a"><mn id="S2.T7.5.3.3.3.m1.1.1" xref="S2.T7.5.3.3.3.m1.1.1.cmml">18.7</mn><annotation-xml encoding="MathML-Content" id="S2.T7.5.3.3.3.m1.1b"><cn id="S2.T7.5.3.3.3.m1.1.1.cmml" type="float" xref="S2.T7.5.3.3.3.m1.1.1">18.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.5.3.3.3.m1.1c">18.7</annotation><annotation encoding="application/x-llamapun" id="S2.T7.5.3.3.3.m1.1d">18.7</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T7.8.6.6">
<td class="ltx_td ltx_align_left" id="S2.T7.7.5.5.2">LLaMA<math alttext="2" class="ltx_Math" display="inline" id="S2.T7.6.4.4.1.m1.1"><semantics id="S2.T7.6.4.4.1.m1.1a"><mn id="S2.T7.6.4.4.1.m1.1.1" xref="S2.T7.6.4.4.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S2.T7.6.4.4.1.m1.1b"><cn id="S2.T7.6.4.4.1.m1.1.1.cmml" type="integer" xref="S2.T7.6.4.4.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.6.4.4.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S2.T7.6.4.4.1.m1.1d">2</annotation></semantics></math> <math alttext="7" class="ltx_Math" display="inline" id="S2.T7.7.5.5.2.m2.1"><semantics id="S2.T7.7.5.5.2.m2.1a"><mn id="S2.T7.7.5.5.2.m2.1.1" xref="S2.T7.7.5.5.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S2.T7.7.5.5.2.m2.1b"><cn id="S2.T7.7.5.5.2.m2.1.1.cmml" type="integer" xref="S2.T7.7.5.5.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.7.5.5.2.m2.1c">7</annotation><annotation encoding="application/x-llamapun" id="S2.T7.7.5.5.2.m2.1d">7</annotation></semantics></math>B</td>
<td class="ltx_td ltx_align_left" id="S2.T7.8.6.6.4">Prompt with description of AU presence</td>
<td class="ltx_td ltx_align_right" id="S2.T7.8.6.6.3"><math alttext="13.4" class="ltx_Math" display="inline" id="S2.T7.8.6.6.3.m1.1"><semantics id="S2.T7.8.6.6.3.m1.1a"><mn id="S2.T7.8.6.6.3.m1.1.1" xref="S2.T7.8.6.6.3.m1.1.1.cmml">13.4</mn><annotation-xml encoding="MathML-Content" id="S2.T7.8.6.6.3.m1.1b"><cn id="S2.T7.8.6.6.3.m1.1.1.cmml" type="float" xref="S2.T7.8.6.6.3.m1.1.1">13.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.8.6.6.3.m1.1c">13.4</annotation><annotation encoding="application/x-llamapun" id="S2.T7.8.6.6.3.m1.1d">13.4</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T7.11.9.9">
<td class="ltx_td ltx_align_left" id="S2.T7.10.8.8.2">LLaMA<math alttext="2" class="ltx_Math" display="inline" id="S2.T7.9.7.7.1.m1.1"><semantics id="S2.T7.9.7.7.1.m1.1a"><mn id="S2.T7.9.7.7.1.m1.1.1" xref="S2.T7.9.7.7.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S2.T7.9.7.7.1.m1.1b"><cn id="S2.T7.9.7.7.1.m1.1.1.cmml" type="integer" xref="S2.T7.9.7.7.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.9.7.7.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S2.T7.9.7.7.1.m1.1d">2</annotation></semantics></math> <math alttext="7" class="ltx_Math" display="inline" id="S2.T7.10.8.8.2.m2.1"><semantics id="S2.T7.10.8.8.2.m2.1a"><mn id="S2.T7.10.8.8.2.m2.1.1" xref="S2.T7.10.8.8.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S2.T7.10.8.8.2.m2.1b"><cn id="S2.T7.10.8.8.2.m2.1.1.cmml" type="integer" xref="S2.T7.10.8.8.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.10.8.8.2.m2.1c">7</annotation><annotation encoding="application/x-llamapun" id="S2.T7.10.8.8.2.m2.1d">7</annotation></semantics></math>B</td>
<td class="ltx_td ltx_align_left" id="S2.T7.11.9.9.4">Prompt with information of AU intensity</td>
<td class="ltx_td ltx_align_right" id="S2.T7.11.9.9.3"><math alttext="17.9" class="ltx_Math" display="inline" id="S2.T7.11.9.9.3.m1.1"><semantics id="S2.T7.11.9.9.3.m1.1a"><mn id="S2.T7.11.9.9.3.m1.1.1" xref="S2.T7.11.9.9.3.m1.1.1.cmml">17.9</mn><annotation-xml encoding="MathML-Content" id="S2.T7.11.9.9.3.m1.1b"><cn id="S2.T7.11.9.9.3.m1.1.1.cmml" type="float" xref="S2.T7.11.9.9.3.m1.1.1">17.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.11.9.9.3.m1.1c">17.9</annotation><annotation encoding="application/x-llamapun" id="S2.T7.11.9.9.3.m1.1d">17.9</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T7.14.12.12">
<td class="ltx_td ltx_align_left" id="S2.T7.13.11.11.2">LLaMA<math alttext="2" class="ltx_Math" display="inline" id="S2.T7.12.10.10.1.m1.1"><semantics id="S2.T7.12.10.10.1.m1.1a"><mn id="S2.T7.12.10.10.1.m1.1.1" xref="S2.T7.12.10.10.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S2.T7.12.10.10.1.m1.1b"><cn id="S2.T7.12.10.10.1.m1.1.1.cmml" type="integer" xref="S2.T7.12.10.10.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.12.10.10.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S2.T7.12.10.10.1.m1.1d">2</annotation></semantics></math> <math alttext="7" class="ltx_Math" display="inline" id="S2.T7.13.11.11.2.m2.1"><semantics id="S2.T7.13.11.11.2.m2.1a"><mn id="S2.T7.13.11.11.2.m2.1.1" xref="S2.T7.13.11.11.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S2.T7.13.11.11.2.m2.1b"><cn id="S2.T7.13.11.11.2.m2.1.1.cmml" type="integer" xref="S2.T7.13.11.11.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.13.11.11.2.m2.1c">7</annotation><annotation encoding="application/x-llamapun" id="S2.T7.13.11.11.2.m2.1d">7</annotation></semantics></math>B</td>
<td class="ltx_td ltx_align_left" id="S2.T7.14.12.12.4">Prompt with description of AU intensity</td>
<td class="ltx_td ltx_align_right" id="S2.T7.14.12.12.3"><math alttext="16.8" class="ltx_Math" display="inline" id="S2.T7.14.12.12.3.m1.1"><semantics id="S2.T7.14.12.12.3.m1.1a"><mn id="S2.T7.14.12.12.3.m1.1.1" xref="S2.T7.14.12.12.3.m1.1.1.cmml">16.8</mn><annotation-xml encoding="MathML-Content" id="S2.T7.14.12.12.3.m1.1b"><cn id="S2.T7.14.12.12.3.m1.1.1.cmml" type="float" xref="S2.T7.14.12.12.3.m1.1.1">16.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.14.12.12.3.m1.1c">16.8</annotation><annotation encoding="application/x-llamapun" id="S2.T7.14.12.12.3.m1.1d">16.8</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T7.17.15.15">
<td class="ltx_td ltx_align_left" id="S2.T7.16.14.14.2">LLaVA<math alttext="1.5" class="ltx_Math" display="inline" id="S2.T7.15.13.13.1.m1.1"><semantics id="S2.T7.15.13.13.1.m1.1a"><mn id="S2.T7.15.13.13.1.m1.1.1" xref="S2.T7.15.13.13.1.m1.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S2.T7.15.13.13.1.m1.1b"><cn id="S2.T7.15.13.13.1.m1.1.1.cmml" type="float" xref="S2.T7.15.13.13.1.m1.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.15.13.13.1.m1.1c">1.5</annotation><annotation encoding="application/x-llamapun" id="S2.T7.15.13.13.1.m1.1d">1.5</annotation></semantics></math> <math alttext="7" class="ltx_Math" display="inline" id="S2.T7.16.14.14.2.m2.1"><semantics id="S2.T7.16.14.14.2.m2.1a"><mn id="S2.T7.16.14.14.2.m2.1.1" xref="S2.T7.16.14.14.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S2.T7.16.14.14.2.m2.1b"><cn id="S2.T7.16.14.14.2.m2.1.1.cmml" type="integer" xref="S2.T7.16.14.14.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.16.14.14.2.m2.1c">7</annotation><annotation encoding="application/x-llamapun" id="S2.T7.16.14.14.2.m2.1d">7</annotation></semantics></math>B</td>
<td class="ltx_td ltx_align_left" id="S2.T7.17.15.15.4">Prompt with image</td>
<td class="ltx_td ltx_align_right" id="S2.T7.17.15.15.3"><math alttext="39.3" class="ltx_Math" display="inline" id="S2.T7.17.15.15.3.m1.1"><semantics id="S2.T7.17.15.15.3.m1.1a"><mn id="S2.T7.17.15.15.3.m1.1.1" xref="S2.T7.17.15.15.3.m1.1.1.cmml">39.3</mn><annotation-xml encoding="MathML-Content" id="S2.T7.17.15.15.3.m1.1b"><cn id="S2.T7.17.15.15.3.m1.1.1.cmml" type="float" xref="S2.T7.17.15.15.3.m1.1.1">39.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.17.15.15.3.m1.1c">39.3</annotation><annotation encoding="application/x-llamapun" id="S2.T7.17.15.15.3.m1.1d">39.3</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T7.20.18.18">
<td class="ltx_td ltx_align_left" id="S2.T7.19.17.17.2">LLaVA<math alttext="1.5" class="ltx_Math" display="inline" id="S2.T7.18.16.16.1.m1.1"><semantics id="S2.T7.18.16.16.1.m1.1a"><mn id="S2.T7.18.16.16.1.m1.1.1" xref="S2.T7.18.16.16.1.m1.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S2.T7.18.16.16.1.m1.1b"><cn id="S2.T7.18.16.16.1.m1.1.1.cmml" type="float" xref="S2.T7.18.16.16.1.m1.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.18.16.16.1.m1.1c">1.5</annotation><annotation encoding="application/x-llamapun" id="S2.T7.18.16.16.1.m1.1d">1.5</annotation></semantics></math> <math alttext="7" class="ltx_Math" display="inline" id="S2.T7.19.17.17.2.m2.1"><semantics id="S2.T7.19.17.17.2.m2.1a"><mn id="S2.T7.19.17.17.2.m2.1.1" xref="S2.T7.19.17.17.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S2.T7.19.17.17.2.m2.1b"><cn id="S2.T7.19.17.17.2.m2.1.1.cmml" type="integer" xref="S2.T7.19.17.17.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.19.17.17.2.m2.1c">7</annotation><annotation encoding="application/x-llamapun" id="S2.T7.19.17.17.2.m2.1d">7</annotation></semantics></math>B</td>
<td class="ltx_td ltx_align_left" id="S2.T7.20.18.18.4">Prompt with image and AU presence</td>
<td class="ltx_td ltx_align_right" id="S2.T7.20.18.18.3"><math alttext="20.5" class="ltx_Math" display="inline" id="S2.T7.20.18.18.3.m1.1"><semantics id="S2.T7.20.18.18.3.m1.1a"><mn id="S2.T7.20.18.18.3.m1.1.1" xref="S2.T7.20.18.18.3.m1.1.1.cmml">20.5</mn><annotation-xml encoding="MathML-Content" id="S2.T7.20.18.18.3.m1.1b"><cn id="S2.T7.20.18.18.3.m1.1.1.cmml" type="float" xref="S2.T7.20.18.18.3.m1.1.1">20.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.20.18.18.3.m1.1c">20.5</annotation><annotation encoding="application/x-llamapun" id="S2.T7.20.18.18.3.m1.1d">20.5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T7.21.19.19">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T7.21.19.19.2">ViT – FER</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T7.21.19.19.3">Image</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T7.21.19.19.1"><math alttext="43.9" class="ltx_Math" display="inline" id="S2.T7.21.19.19.1.m1.1"><semantics id="S2.T7.21.19.19.1.m1.1a"><mn id="S2.T7.21.19.19.1.m1.1.1" xref="S2.T7.21.19.19.1.m1.1.1.cmml">43.9</mn><annotation-xml encoding="MathML-Content" id="S2.T7.21.19.19.1.m1.1b"><cn id="S2.T7.21.19.19.1.m1.1.1.cmml" type="float" xref="S2.T7.21.19.19.1.m1.1.1">43.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.21.19.19.1.m1.1c">43.9</annotation><annotation encoding="application/x-llamapun" id="S2.T7.21.19.19.1.m1.1d">43.9</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T7.22.20.20">
<td class="ltx_td ltx_align_left ltx_border_bb" colspan="2" id="S2.T7.22.20.20.2">Chance level</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S2.T7.22.20.20.1"><math alttext="14.3" class="ltx_Math" display="inline" id="S2.T7.22.20.20.1.m1.1"><semantics id="S2.T7.22.20.20.1.m1.1a"><mn id="S2.T7.22.20.20.1.m1.1.1" xref="S2.T7.22.20.20.1.m1.1.1.cmml">14.3</mn><annotation-xml encoding="MathML-Content" id="S2.T7.22.20.20.1.m1.1b"><cn id="S2.T7.22.20.20.1.m1.1.1.cmml" type="float" xref="S2.T7.22.20.20.1.m1.1.1">14.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T7.22.20.20.1.m1.1c">14.3</annotation><annotation encoding="application/x-llamapun" id="S2.T7.22.20.20.1.m1.1d">14.3</annotation></semantics></math></td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>The Linguistic Modality Has Changed</h3>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Generation</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The evolution of text generation experienced
an important shift in the mid-2010s with the development of neural models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib56" title="">56</a>]</cite>.
The introduction of the Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib36" title="">36</a>]</cite> has revolutionised the Natural Language Processing (NLP) field and marked the beginning of the Large Language Models (LLM) era. Since then, this architecture has been established as the default backbone for the LLMs. In text generation,
OpenAI’s work with the Generative Pre-trained Transformer (GPT) models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib39" title="">39</a>]</cite>, culminating with the recent GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib58" title="">58</a>]</cite>,
has advanced text generation, paving the way for exhaustive
research. Furthermore, developing open-source models, such as Meta’s LLaMA models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib55" title="">55</a>]</cite>, democratised the
access and
fostered the
innovation in the field.
From an Affective Computing perspective, LLMs present a novel approach to inject and transfer emotional content
in linguistic data, with recent works demonstrating their intrinsic emotional capabilities in a variety of domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib63" title="">63</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.6">In this section, we aim to investigate and leverage the affective style transfer capabilities of cutting-edge open-source LLMs.
For this experiment, we select
LLaMA2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib55" title="">55</a>]</cite> and Mistral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib64" title="">64</a>]</cite>, given their proven high performance in both language understanding and text generation tasks. Additionally, we include
the Mixtral<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1" title="">https://huggingface.co/mistralai/Mixtral-8x7B-v0.1</a></span></span></span> LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib65" title="">65</a>]</cite>, which utilises the Sparse Mixture of Experts (SMoE) method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib66" title="">66</a>]</cite>. It is important to note that the implementation of SMoE in Mixtral notably influences its size. The LLaMA2 and Mistral LLMs each comprise 7 billion parameters.
We start with the text generation
by compiling a corpus comprised of 122 human-curated neutral phrases.
The dataset encompasses various topics, ranging from mundane personal activities to formal professional interactions, thus providing a targeted platform to evaluate how well LLMs
handle affective style transfer across diverse topics.
Utilising the gathered corpus,
we task the aforementioned LLMs (i. e., LLaMA2, Mistral, and Mixtral) to generate six emotional phrases from each original neutral phrase, conveying a specific target emotion: fear, anger, happiness, sadness, disgust, and surprise.
The generation is described by the formula <math alttext="\mathcal{M}(text,emotion),text\in\mathcal{N},emotion\in\mathcal{E}," class="ltx_Math" display="inline" id="S2.SS2.SSS1.p2.1.m1.1"><semantics id="S2.SS2.SSS1.p2.1.m1.1a"><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1"><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.3.cmml"><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.cmml"><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.3.cmml"><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.4" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.4.cmml">ℳ</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.3.cmml">⁢</mo><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.3" stretchy="false" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">(</mo><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">t</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">e</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1a" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml">x</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1b" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.5" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.5.cmml">t</mi></mrow><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.4" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">e</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">m</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1a" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.4" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.4.cmml">o</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1b" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.5" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.5.cmml">t</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1c" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.6" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.6.cmml">i</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1d" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.7" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.7.cmml">o</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1e" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.8" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.8.cmml">n</mi></mrow><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.5" stretchy="false" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.2.cmml">t</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.3.cmml">e</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.1a" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.4" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.4.cmml">x</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.1b" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.5" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.5.cmml">t</mi></mrow></mrow><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.4" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.4.cmml">𝒩</mi></mrow><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.3a.cmml">,</mo><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.cmml"><mrow id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.cmml"><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.2.cmml">e</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.3.cmml">m</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1a" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.4" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.4.cmml">o</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1b" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.5" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.5.cmml">t</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1c" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.6" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.6.cmml">i</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1d" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.7" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.7.cmml">o</mi><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1e" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.8" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.8.cmml">n</mi></mrow><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.3.cmml">ℰ</mi></mrow></mrow><mo id="S2.SS2.SSS1.p2.1.m1.1.1.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.1.m1.1b"><apply id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.3a.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1"><in id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.3"></in><list id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2"><apply id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1"><times id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.3"></times><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.4">ℳ</ci><interval closure="open" id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2"><apply id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑡</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑒</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4">𝑥</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.5">𝑡</ci></apply><apply id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2"><times id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1"></times><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.2">𝑒</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.3">𝑚</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.4.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.4">𝑜</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.5.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.5">𝑡</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.6.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.6">𝑖</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.7.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.7">𝑜</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.8.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.1.2.2.2.8">𝑛</ci></apply></interval></apply><apply id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2"><times id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.1"></times><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.2">𝑡</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.3">𝑒</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.4.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.4">𝑥</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.5.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.2.2.2.5">𝑡</ci></apply></list><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.4.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.4">𝒩</ci></apply><apply id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2"><in id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.1"></in><apply id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2"><times id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.1"></times><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.2">𝑒</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.3">𝑚</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.4.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.4">𝑜</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.5.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.5">𝑡</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.6.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.6">𝑖</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.7.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.7">𝑜</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.8.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.2.8">𝑛</ci></apply><ci id="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.3">ℰ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.1.m1.1c">\mathcal{M}(text,emotion),text\in\mathcal{N},emotion\in\mathcal{E},</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p2.1.m1.1d">caligraphic_M ( italic_t italic_e italic_x italic_t , italic_e italic_m italic_o italic_t italic_i italic_o italic_n ) , italic_t italic_e italic_x italic_t ∈ caligraphic_N , italic_e italic_m italic_o italic_t italic_i italic_o italic_n ∈ caligraphic_E ,</annotation></semantics></math> where <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p2.2.m2.1"><semantics id="S2.SS2.SSS1.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p2.2.m2.1.1" xref="S2.SS2.SSS1.p2.2.m2.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.2.m2.1b"><ci id="S2.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.2.m2.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p2.2.m2.1d">caligraphic_M</annotation></semantics></math> is the LLM at hand, <math alttext="\mathcal{N}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p2.3.m3.1"><semantics id="S2.SS2.SSS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p2.3.m3.1.1" xref="S2.SS2.SSS1.p2.3.m3.1.1.cmml">𝒩</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.3.m3.1b"><ci id="S2.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.1.1">𝒩</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.3.m3.1c">\mathcal{N}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p2.3.m3.1d">caligraphic_N</annotation></semantics></math> the set of the neutral phrases, and <math alttext="\mathcal{E}=\{fear,anger,happiness,sadness,disgust,surprise\}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p2.4.m4.6"><semantics id="S2.SS2.SSS1.p2.4.m4.6a"><mrow id="S2.SS2.SSS1.p2.4.m4.6.6" xref="S2.SS2.SSS1.p2.4.m4.6.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p2.4.m4.6.6.8" xref="S2.SS2.SSS1.p2.4.m4.6.6.8.cmml">ℰ</mi><mo id="S2.SS2.SSS1.p2.4.m4.6.6.7" xref="S2.SS2.SSS1.p2.4.m4.6.6.7.cmml">=</mo><mrow id="S2.SS2.SSS1.p2.4.m4.6.6.6.6" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.7.cmml"><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.7" stretchy="false" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.7.cmml">{</mo><mrow id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.2" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.2.cmml">f</mi><mo id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.1" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.3" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.3.cmml">e</mi><mo id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.1a" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.4" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.4.cmml">a</mi><mo id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.1b" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.5" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.5.cmml">r</mi></mrow><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.8" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.7.cmml">,</mo><mrow id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.2" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.2.cmml">a</mi><mo id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.1" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.3" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.3.cmml">n</mi><mo id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.1a" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.4" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.4.cmml">g</mi><mo id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.1b" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.5" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.5.cmml">e</mi><mo id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.1c" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.6" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.6.cmml">r</mi></mrow><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.9" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.7.cmml">,</mo><mrow id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.2" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.2.cmml">h</mi><mo id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.3" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.3.cmml">a</mi><mo id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1a" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.4" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.4.cmml">p</mi><mo id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1b" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.5" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.5.cmml">p</mi><mo id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1c" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.6" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.6.cmml">i</mi><mo id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1d" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.7" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.7.cmml">n</mi><mo id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1e" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.8" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.8.cmml">e</mi><mo id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1f" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.9" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.9.cmml">s</mi><mo id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1g" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.10" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.10.cmml">s</mi></mrow><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.10" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.7.cmml">,</mo><mrow id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.2" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.2.cmml">s</mi><mo id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.3" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.3.cmml">a</mi><mo id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1a" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.4" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.4.cmml">d</mi><mo id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1b" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.5" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.5.cmml">n</mi><mo id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1c" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.6" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.6.cmml">e</mi><mo id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1d" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.7" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.7.cmml">s</mi><mo id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1e" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.8" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.8.cmml">s</mi></mrow><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.11" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.7.cmml">,</mo><mrow id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.2" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.2.cmml">d</mi><mo id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.3" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.3.cmml">i</mi><mo id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1a" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.4" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.4.cmml">s</mi><mo id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1b" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.5" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.5.cmml">g</mi><mo id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1c" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.6" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.6.cmml">u</mi><mo id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1d" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.7" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.7.cmml">s</mi><mo id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1e" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.8" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.8.cmml">t</mi></mrow><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.12" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.7.cmml">,</mo><mrow id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.2" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.2.cmml">s</mi><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.3" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.3.cmml">u</mi><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1a" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.4" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.4.cmml">r</mi><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1b" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.5" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.5.cmml">p</mi><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1c" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.6" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.6.cmml">r</mi><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1d" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.7" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.7.cmml">i</mi><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1e" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.8" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.8.cmml">s</mi><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1f" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.9" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.9.cmml">e</mi></mrow><mo id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.13" stretchy="false" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.7.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.4.m4.6b"><apply id="S2.SS2.SSS1.p2.4.m4.6.6.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6"><eq id="S2.SS2.SSS1.p2.4.m4.6.6.7.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.7"></eq><ci id="S2.SS2.SSS1.p2.4.m4.6.6.8.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.8">ℰ</ci><set id="S2.SS2.SSS1.p2.4.m4.6.6.6.7.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6"><apply id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1"><times id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.1"></times><ci id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.2">𝑓</ci><ci id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.3">𝑒</ci><ci id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.4.cmml" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.4">𝑎</ci><ci id="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.5.cmml" xref="S2.SS2.SSS1.p2.4.m4.1.1.1.1.1.5">𝑟</ci></apply><apply id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2"><times id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.1"></times><ci id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.2">𝑎</ci><ci id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.3">𝑛</ci><ci id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.4.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.4">𝑔</ci><ci id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.5.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.5">𝑒</ci><ci id="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.6.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.2.2.2.6">𝑟</ci></apply><apply id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3"><times id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.1"></times><ci id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.2">ℎ</ci><ci id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.3">𝑎</ci><ci id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.4.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.4">𝑝</ci><ci id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.5.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.5">𝑝</ci><ci id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.6.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.6">𝑖</ci><ci id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.7.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.7">𝑛</ci><ci id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.8.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.8">𝑒</ci><ci id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.9.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.9">𝑠</ci><ci id="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.10.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.3.3.3.10">𝑠</ci></apply><apply id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4"><times id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.1"></times><ci id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.2">𝑠</ci><ci id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.3">𝑎</ci><ci id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.4.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.4">𝑑</ci><ci id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.5.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.5">𝑛</ci><ci id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.6.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.6">𝑒</ci><ci id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.7.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.7">𝑠</ci><ci id="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.8.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.4.4.4.8">𝑠</ci></apply><apply id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5"><times id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.1"></times><ci id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.2">𝑑</ci><ci id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.3">𝑖</ci><ci id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.4.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.4">𝑠</ci><ci id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.5.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.5">𝑔</ci><ci id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.6.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.6">𝑢</ci><ci id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.7.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.7">𝑠</ci><ci id="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.8.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.5.5.8">𝑡</ci></apply><apply id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6"><times id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.1"></times><ci id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.2">𝑠</ci><ci id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.3">𝑢</ci><ci id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.4.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.4">𝑟</ci><ci id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.5.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.5">𝑝</ci><ci id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.6.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.6">𝑟</ci><ci id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.7.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.7">𝑖</ci><ci id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.8.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.8">𝑠</ci><ci id="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.9.cmml" xref="S2.SS2.SSS1.p2.4.m4.6.6.6.6.6.9">𝑒</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.4.m4.6c">\mathcal{E}=\{fear,anger,happiness,sadness,disgust,surprise\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p2.4.m4.6d">caligraphic_E = { italic_f italic_e italic_a italic_r , italic_a italic_n italic_g italic_e italic_r , italic_h italic_a italic_p italic_p italic_i italic_n italic_e italic_s italic_s , italic_s italic_a italic_d italic_n italic_e italic_s italic_s , italic_d italic_i italic_s italic_g italic_u italic_s italic_t , italic_s italic_u italic_r italic_p italic_r italic_i italic_s italic_e }</annotation></semantics></math> (the prompted emotion). We generate three synthetic sentences
for each combination of text and emotion, yielding a corpus of <math alttext="3*|\mathcal{N}|*|\mathcal{E}|=2\,194" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p2.5.m5.2"><semantics id="S2.SS2.SSS1.p2.5.m5.2a"><mrow id="S2.SS2.SSS1.p2.5.m5.2.3" xref="S2.SS2.SSS1.p2.5.m5.2.3.cmml"><mrow id="S2.SS2.SSS1.p2.5.m5.2.3.2" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.cmml"><mn id="S2.SS2.SSS1.p2.5.m5.2.3.2.2" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.2.cmml">3</mn><mo id="S2.SS2.SSS1.p2.5.m5.2.3.2.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.1.cmml">∗</mo><mrow id="S2.SS2.SSS1.p2.5.m5.2.3.2.3.2" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.3.1.cmml"><mo id="S2.SS2.SSS1.p2.5.m5.2.3.2.3.2.1" stretchy="false" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.3.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p2.5.m5.1.1" xref="S2.SS2.SSS1.p2.5.m5.1.1.cmml">𝒩</mi><mo id="S2.SS2.SSS1.p2.5.m5.2.3.2.3.2.2" rspace="0.055em" stretchy="false" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.3.1.1.cmml">|</mo></mrow><mo id="S2.SS2.SSS1.p2.5.m5.2.3.2.1a" rspace="0.222em" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.1.cmml">∗</mo><mrow id="S2.SS2.SSS1.p2.5.m5.2.3.2.4.2" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.4.1.cmml"><mo id="S2.SS2.SSS1.p2.5.m5.2.3.2.4.2.1" stretchy="false" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.4.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p2.5.m5.2.2" xref="S2.SS2.SSS1.p2.5.m5.2.2.cmml">ℰ</mi><mo id="S2.SS2.SSS1.p2.5.m5.2.3.2.4.2.2" stretchy="false" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.4.1.1.cmml">|</mo></mrow></mrow><mo id="S2.SS2.SSS1.p2.5.m5.2.3.1" xref="S2.SS2.SSS1.p2.5.m5.2.3.1.cmml">=</mo><mn id="S2.SS2.SSS1.p2.5.m5.2.3.3" xref="S2.SS2.SSS1.p2.5.m5.2.3.3.cmml">2 194</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.5.m5.2b"><apply id="S2.SS2.SSS1.p2.5.m5.2.3.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.3"><eq id="S2.SS2.SSS1.p2.5.m5.2.3.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.3.1"></eq><apply id="S2.SS2.SSS1.p2.5.m5.2.3.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.3.2"><times id="S2.SS2.SSS1.p2.5.m5.2.3.2.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.1"></times><cn id="S2.SS2.SSS1.p2.5.m5.2.3.2.2.cmml" type="integer" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.2">3</cn><apply id="S2.SS2.SSS1.p2.5.m5.2.3.2.3.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.3.2"><abs id="S2.SS2.SSS1.p2.5.m5.2.3.2.3.1.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.3.2.1"></abs><ci id="S2.SS2.SSS1.p2.5.m5.1.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.1.1">𝒩</ci></apply><apply id="S2.SS2.SSS1.p2.5.m5.2.3.2.4.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.4.2"><abs id="S2.SS2.SSS1.p2.5.m5.2.3.2.4.1.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.3.2.4.2.1"></abs><ci id="S2.SS2.SSS1.p2.5.m5.2.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.2">ℰ</ci></apply></apply><cn id="S2.SS2.SSS1.p2.5.m5.2.3.3.cmml" type="integer" xref="S2.SS2.SSS1.p2.5.m5.2.3.3">2194</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.5.m5.2c">3*|\mathcal{N}|*|\mathcal{E}|=2\,194</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p2.5.m5.2d">3 ∗ | caligraphic_N | ∗ | caligraphic_E | = 2 194</annotation></semantics></math> emotional phrases<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>For further information and access to the dataset, please contact the authors. </span></span></span>. The data was synthesised with settings chosen to enhance variability and creativity.
The generation parameters used a temperature of 0.9, top-<math alttext="p" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p2.6.m6.1"><semantics id="S2.SS2.SSS1.p2.6.m6.1a"><mi id="S2.SS2.SSS1.p2.6.m6.1.1" xref="S2.SS2.SSS1.p2.6.m6.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.6.m6.1b"><ci id="S2.SS2.SSS1.p2.6.m6.1.1.cmml" xref="S2.SS2.SSS1.p2.6.m6.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.6.m6.1c">p</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p2.6.m6.1d">italic_p</annotation></semantics></math> of 0.6, and a repetition penalty of 1.2.
An example of the generated sentences
is shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T8" title="In 2.2.1 Generation ‣ 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.F3" title="In 2.2.1 Generation ‣ 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> visually summarises the followed pipeline. We observe that the models tend to exaggerate the injected emotion, adopting an over-dramatic style that results in more formal, yet affectively adapted phrases. Despite these exaggerated and dramatic adaptations, the primary objective was to investigate how these models perceive and express emotions under basic setup conditions. Our findings highlight the models’ propensity to amplify emotional content, which was an anticipated aspect of this exploratory study.</p>
</div>
<figure class="ltx_table" id="S2.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T8.3.1.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S2.T8.4.2" style="font-size:90%;">Affective style transfer example towards the emotion ‘surprise’ with the neutral phrase: <span class="ltx_text ltx_font_italic" id="S2.T8.4.2.1">“The weather is clear and sunny.”</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T8.5">
<tr class="ltx_tr" id="S2.T8.5.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S2.T8.5.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T8.5.1.1.1">
<span class="ltx_p" id="S2.T8.5.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S2.T8.5.1.1.1.1.1" style="font-size:90%;">Model</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S2.T8.5.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T8.5.1.2.1">
<span class="ltx_p" id="S2.T8.5.1.2.1.1" style="width:256.1pt;"><span class="ltx_text ltx_font_bold" id="S2.T8.5.1.2.1.1.1" style="font-size:90%;">Affective Phrase</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T8.5.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T8.5.2.1" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T8.5.2.1.1">
<span class="ltx_p" id="S2.T8.5.2.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T8.5.2.1.1.1.1" style="font-size:90%;">Mixtral</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T8.5.2.2" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T8.5.2.2.1">
<span class="ltx_p" id="S2.T8.5.2.2.1.1" style="width:256.1pt;"><span class="ltx_text ltx_font_italic" id="S2.T8.5.2.2.1.1.1" style="font-size:90%;">Wow! What a surprise! The sky is astonishingly bright and clear today!</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T8.5.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T8.5.3.1" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T8.5.3.1.1">
<span class="ltx_p" id="S2.T8.5.3.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T8.5.3.1.1.1.1" style="font-size:90%;">Mistral</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T8.5.3.2" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T8.5.3.2.1">
<span class="ltx_p" id="S2.T8.5.3.2.1.1" style="width:256.1pt;"><span class="ltx_text ltx_font_italic" id="S2.T8.5.3.2.1.1.1" style="font-size:90%;">The sudden emergence of unobstructed sunlight has taken me by complete astonishment!</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T8.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S2.T8.5.4.1" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T8.5.4.1.1">
<span class="ltx_p" id="S2.T8.5.4.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T8.5.4.1.1.1.1" style="font-size:90%;">LLaMA</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S2.T8.5.4.2" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T8.5.4.2.1">
<span class="ltx_p" id="S2.T8.5.4.2.1.1" style="width:256.1pt;"><span class="ltx_text ltx_font_italic" id="S2.T8.5.4.2.1.1.1" style="font-size:90%;">It comes as quite a shock to discover that the sky has transformed itself into such crystal clarity!</span></span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S2.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T9.3.1.1" style="font-size:90%;">Table 9</span>: </span><span class="ltx_text" id="S2.T9.4.2" style="font-size:90%;">Statistics of the considered subset of the GoEmotions dataset. </span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T9.1">
<tr class="ltx_tr" id="S2.T9.1.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T9.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T9.1.2.1.1">Emotion</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T9.1.2.2"><span class="ltx_text ltx_font_bold" id="S2.T9.1.2.2.1">Training</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T9.1.2.3"><span class="ltx_text ltx_font_bold" id="S2.T9.1.2.3.1">Validation</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T9.1.2.4"><span class="ltx_text ltx_font_bold" id="S2.T9.1.2.4.1">Test</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T9.1.3.1">Neutral</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T9.1.3.2">12 823</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T9.1.3.3">1 592</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T9.1.3.4">1 606</td>
</tr>
<tr class="ltx_tr" id="S2.T9.1.4">
<td class="ltx_td ltx_align_left" id="S2.T9.1.4.1">Fear</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.4.2">515</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.4.3">66</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.4.4">77</td>
</tr>
<tr class="ltx_tr" id="S2.T9.1.5">
<td class="ltx_td ltx_align_left" id="S2.T9.1.5.1">Anger</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.5.2">3 878</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.5.3">485</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.5.4">520</td>
</tr>
<tr class="ltx_tr" id="S2.T9.1.6">
<td class="ltx_td ltx_align_left" id="S2.T9.1.6.1">Happiness</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.6.2">12 920</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.6.3">1 668</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.6.4">1 603</td>
</tr>
<tr class="ltx_tr" id="S2.T9.1.7">
<td class="ltx_td ltx_align_left" id="S2.T9.1.7.1">Sadness</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.7.2">2 121</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.7.3">241</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.7.4">259</td>
</tr>
<tr class="ltx_tr" id="S2.T9.1.8">
<td class="ltx_td ltx_align_left" id="S2.T9.1.8.1">Disgust</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.8.2">498</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.8.3">61</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.8.4">76</td>
</tr>
<tr class="ltx_tr" id="S2.T9.1.9">
<td class="ltx_td ltx_align_left" id="S2.T9.1.9.1">Surprise</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.9.2">3 553</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.9.3">435</td>
<td class="ltx_td ltx_align_right" id="S2.T9.1.9.4">449</td>
</tr>
<tr class="ltx_tr" id="S2.T9.1.1">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T9.1.1.1"><math alttext="\Sigma" class="ltx_Math" display="inline" id="S2.T9.1.1.1.m1.1"><semantics id="S2.T9.1.1.1.m1.1a"><mi id="S2.T9.1.1.1.m1.1.1" mathvariant="normal" xref="S2.T9.1.1.1.m1.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S2.T9.1.1.1.m1.1b"><ci id="S2.T9.1.1.1.m1.1.1.cmml" xref="S2.T9.1.1.1.m1.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T9.1.1.1.m1.1c">\Sigma</annotation><annotation encoding="application/x-llamapun" id="S2.T9.1.1.1.m1.1d">roman_Σ</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T9.1.1.2">36 308</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T9.1.1.3">4 548</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T9.1.1.4">4 590</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1">In order to investigate the quality of the generated sentences
by the LLMs, we implement two baseline models trained on the GoEmotions dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib67" title="">67</a>]</cite>,
a well-renowned corpus in the field and commonly utilised for benchmarking purposes due to its comprehensive labelling and categorisation of the emotions.
The GoEmotions dataset consists of English Reddit comments annotated according to 27 distinct emotions, plus the neutral state, by 3 or 5 labellers each. Due to the nature of the annotations in the GoEmotions dataset, we
begin by tailoring the data to meet the specific requirements of our experiments.
First, we select instances
from the dataset annotated with a single emotion, in order to tackle the task
as a single-label classification problem, instead of a multi-label classification one.
As previously, we adopt the ‘Big Six’ Ekman emotions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib44" title="">44</a>]</cite>, in addition to a seventh neutral state.
This restructuring of the GoEmotions taxonomy to the Ekman taxonomy is achieved by aggregating the original labels into the targeted,
broader categories <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib67" title="">67</a>]</cite>.
For example, emotions like annoyance and irritation, originally distinct, were grouped under ‘anger’ to fit the Ekman model.
<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T9" title="In 2.2.1 Generation ‣ 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">9</span></a> summarises the statistics of the considered subset of the GoEmotions dataset.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="577" id="S2.F3.g1" src="x6.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Pipeline of the affective text style transfer process for generating the affective sentences with ‘surprise’ as the prompted emotion. After that, we classify the synthesised sentences using RoBERTa, GPT-3.5, and GPT-4.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS1.p4">
<p class="ltx_p" id="S2.SS2.SSS1.p4.2">For the two baselines, we employ two different architectures: a Bi-directional Long Short-Term Memory (BiLSTM) network and a fine-tuned version of the RoBERTa-base model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib68" title="">68</a>]</cite>.
Both models are trained on the selected subset of the
GoEmotions dataset. The BiLSTM consists of two bidirectional LSTM layers with 128 units each.
It is trained with a learning rate of <math alttext="5\times 10^{-3}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p4.1.m1.1"><semantics id="S2.SS2.SSS1.p4.1.m1.1a"><mrow id="S2.SS2.SSS1.p4.1.m1.1.1" xref="S2.SS2.SSS1.p4.1.m1.1.1.cmml"><mn id="S2.SS2.SSS1.p4.1.m1.1.1.2" xref="S2.SS2.SSS1.p4.1.m1.1.1.2.cmml">5</mn><mo id="S2.SS2.SSS1.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS1.p4.1.m1.1.1.1.cmml">×</mo><msup id="S2.SS2.SSS1.p4.1.m1.1.1.3" xref="S2.SS2.SSS1.p4.1.m1.1.1.3.cmml"><mn id="S2.SS2.SSS1.p4.1.m1.1.1.3.2" xref="S2.SS2.SSS1.p4.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.SS2.SSS1.p4.1.m1.1.1.3.3" xref="S2.SS2.SSS1.p4.1.m1.1.1.3.3.cmml"><mo id="S2.SS2.SSS1.p4.1.m1.1.1.3.3a" xref="S2.SS2.SSS1.p4.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.SS2.SSS1.p4.1.m1.1.1.3.3.2" xref="S2.SS2.SSS1.p4.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p4.1.m1.1b"><apply id="S2.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p4.1.m1.1.1"><times id="S2.SS2.SSS1.p4.1.m1.1.1.1.cmml" xref="S2.SS2.SSS1.p4.1.m1.1.1.1"></times><cn id="S2.SS2.SSS1.p4.1.m1.1.1.2.cmml" type="integer" xref="S2.SS2.SSS1.p4.1.m1.1.1.2">5</cn><apply id="S2.SS2.SSS1.p4.1.m1.1.1.3.cmml" xref="S2.SS2.SSS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p4.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS1.p4.1.m1.1.1.3">superscript</csymbol><cn id="S2.SS2.SSS1.p4.1.m1.1.1.3.2.cmml" type="integer" xref="S2.SS2.SSS1.p4.1.m1.1.1.3.2">10</cn><apply id="S2.SS2.SSS1.p4.1.m1.1.1.3.3.cmml" xref="S2.SS2.SSS1.p4.1.m1.1.1.3.3"><minus id="S2.SS2.SSS1.p4.1.m1.1.1.3.3.1.cmml" xref="S2.SS2.SSS1.p4.1.m1.1.1.3.3"></minus><cn id="S2.SS2.SSS1.p4.1.m1.1.1.3.3.2.cmml" type="integer" xref="S2.SS2.SSS1.p4.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p4.1.m1.1c">5\times 10^{-3}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p4.1.m1.1d">5 × 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT</annotation></semantics></math> and a batch size of 96 for 40 epochs, while RoBERTa-base was fine-tuned at a conservative learning rate of <math alttext="5\times 10^{-5}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p4.2.m2.1"><semantics id="S2.SS2.SSS1.p4.2.m2.1a"><mrow id="S2.SS2.SSS1.p4.2.m2.1.1" xref="S2.SS2.SSS1.p4.2.m2.1.1.cmml"><mn id="S2.SS2.SSS1.p4.2.m2.1.1.2" xref="S2.SS2.SSS1.p4.2.m2.1.1.2.cmml">5</mn><mo id="S2.SS2.SSS1.p4.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS1.p4.2.m2.1.1.1.cmml">×</mo><msup id="S2.SS2.SSS1.p4.2.m2.1.1.3" xref="S2.SS2.SSS1.p4.2.m2.1.1.3.cmml"><mn id="S2.SS2.SSS1.p4.2.m2.1.1.3.2" xref="S2.SS2.SSS1.p4.2.m2.1.1.3.2.cmml">10</mn><mrow id="S2.SS2.SSS1.p4.2.m2.1.1.3.3" xref="S2.SS2.SSS1.p4.2.m2.1.1.3.3.cmml"><mo id="S2.SS2.SSS1.p4.2.m2.1.1.3.3a" xref="S2.SS2.SSS1.p4.2.m2.1.1.3.3.cmml">−</mo><mn id="S2.SS2.SSS1.p4.2.m2.1.1.3.3.2" xref="S2.SS2.SSS1.p4.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p4.2.m2.1b"><apply id="S2.SS2.SSS1.p4.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p4.2.m2.1.1"><times id="S2.SS2.SSS1.p4.2.m2.1.1.1.cmml" xref="S2.SS2.SSS1.p4.2.m2.1.1.1"></times><cn id="S2.SS2.SSS1.p4.2.m2.1.1.2.cmml" type="integer" xref="S2.SS2.SSS1.p4.2.m2.1.1.2">5</cn><apply id="S2.SS2.SSS1.p4.2.m2.1.1.3.cmml" xref="S2.SS2.SSS1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p4.2.m2.1.1.3.1.cmml" xref="S2.SS2.SSS1.p4.2.m2.1.1.3">superscript</csymbol><cn id="S2.SS2.SSS1.p4.2.m2.1.1.3.2.cmml" type="integer" xref="S2.SS2.SSS1.p4.2.m2.1.1.3.2">10</cn><apply id="S2.SS2.SSS1.p4.2.m2.1.1.3.3.cmml" xref="S2.SS2.SSS1.p4.2.m2.1.1.3.3"><minus id="S2.SS2.SSS1.p4.2.m2.1.1.3.3.1.cmml" xref="S2.SS2.SSS1.p4.2.m2.1.1.3.3"></minus><cn id="S2.SS2.SSS1.p4.2.m2.1.1.3.3.2.cmml" type="integer" xref="S2.SS2.SSS1.p4.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p4.2.m2.1c">5\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p4.2.m2.1d">5 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> and a smaller batch size of 12. The models are trained on the training partition
of the dataset, and the weights yielding the highest validation Unweighted Average Recall (UAR) are selected for each model.
The test scores for both baseline models (BiLSTM and RoBERTa) on the test partition
of the GoEmotions dataset are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T10" title="In 2.2.1 Generation ‣ 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">10</span></a>.
The results are consistent with <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib67" title="">67</a>]</cite>, but with some differences, given that we model the problem as a single-label classification, instead of the original multi-label classification task. Given the superior performance of RoBERTa, we utilise it for analysing the synthetically generated emotional sentences.
</p>
</div>
<figure class="ltx_table" id="S2.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T10.2.1.1" style="font-size:90%;">Table 10</span>: </span><span class="ltx_text" id="S2.T10.3.2" style="font-size:90%;">Performance scores of the implemented
models when inferring the emotions (Ekman’s ‘Big Six’ in addition to the neutral state) conveyed by the sentences belonging to the test set of the GoEmotions dataset.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T10.4">
<tr class="ltx_tr" id="S2.T10.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T10.4.1.1"><span class="ltx_text ltx_font_bold" id="S2.T10.4.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T10.4.1.2"><span class="ltx_text ltx_font_bold" id="S2.T10.4.1.2.1">ACC (%)</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T10.4.1.3"><span class="ltx_text ltx_font_bold" id="S2.T10.4.1.3.1">UAR(%)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T10.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T10.4.2.1">BiLSTM</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T10.4.2.2">53.53</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T10.4.2.3">51.44</td>
</tr>
<tr class="ltx_tr" id="S2.T10.4.3">
<td class="ltx_td ltx_align_left" id="S2.T10.4.3.1">RoBERTa</td>
<td class="ltx_td ltx_align_right" id="S2.T10.4.3.2"><span class="ltx_text ltx_font_bold" id="S2.T10.4.3.2.1">69.22</span></td>
<td class="ltx_td ltx_align_right" id="S2.T10.4.3.3"><span class="ltx_text ltx_font_bold" id="S2.T10.4.3.3.1">62.82</span></td>
</tr>
<tr class="ltx_tr" id="S2.T10.4.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S2.T10.4.4.1">Chance Level</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id="S2.T10.4.4.2">14.29</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id="S2.T10.4.4.3">14.29</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S2.SS2.SSS1.p5">
<p class="ltx_p" id="S2.SS2.SSS1.p5.1">To evaluate the performance of the various LLMs on the emotion injection task,
we test the generated sentences with the RoBERTa baseline model,
in addition to GPT-4 as an approximation for human evaluation, and its weaker variant GPT-3.5. GPT-4 has shown superior performance in many affective computing problems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib62" title="">62</a>]</cite>, often better than fine-tuned, specialised models, especially with problems related to
sentiment or emotions.
<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T11" title="In 2.2.2 Analysis ‣ 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a> demonstrates the prompt templates used for the GPT models, following a similar pattern like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib69" title="">69</a>]</cite>.
The versions of GPT variants used are ‘gpt-3.5-turbo-0125’<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://platform.openai.com/docs/models/gpt-3-5-turbo</span></span></span> for GPT-3.5 and ‘gpt-4-turbo-2024-04-09’<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4</span></span></span> for GPT-4.
The results of this evaluation are depicted
in <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.F4" title="In 2.2.1 Generation ‣ 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Notably, these results can be considered to reflect a better agreement
between models than with the ground truth labels, which are not human-annotated.
However, the results of GPT-4 should be the closest to the human evaluations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib62" title="">62</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="747" id="S2.F4.sf1.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F4.sf1.3.2" style="font-size:90%;">LLaMA2-based synthesis.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="747" id="S2.F4.sf2.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F4.sf2.3.2" style="font-size:90%;">Mistral-based synthesis.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="747" id="S2.F4.sf3.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S2.F4.sf3.3.2" style="font-size:90%;">Mixtral-based synthesis.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">UAR scores obtained with the RoBERTa, GPT-3.5, and GPT-4 models when recognising the emotions conveyed by the synthetic sentences generated by LLaMA2 (left), Mistral (centre), and Mixtral (right).</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS1.p6">
<p class="ltx_p" id="S2.SS2.SSS1.p6.1">GPT-4 as the most superior model achieves very high UAR scores on all six emotions.
Its inferior model GPT-3.5 achieves slightly worse results in most cases, but it experiences a performance drop in the recognition of surprise.
On the other hand, RoBERTa has a different behaviour in comparison.
It is generally much worse than GPT-4 and GPT-3.5
in most of the cases, but
it obtains
a higher score than GPT-3.5 for the surprise emotion with the LLaMA2- and the Mixtral-generated sentences.
Additionally, RoBERTa is showing very low performance for disgust, which seems to be a weakness of the model, consistent with the results on the GoEmotions dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib67" title="">67</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p7">
<p class="ltx_p" id="S2.SS2.SSS1.p7.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.F5" title="In 2.2.1 Generation ‣ 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> depicts the confusion matrices obtained with the
RoBERTa
baseline model
on the LLaMA2-, the Mistral-, and the Mixtral-generated sets. We also include its performance on the test set of GoEmotions as a reference.
A common issue with the RoBERTa model is the confusion among anger and disgust.
Analysing the confusion matrices, we also observe an interesting effect: most of the model’s mispredictions are assigned to the neutral class.
</p>
</div>
<figure class="ltx_figure" id="S2.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="830" id="S2.F5.sf1.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F5.sf1.3.2" style="font-size:90%;">LLaMA2</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="830" id="S2.F5.sf2.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F5.sf2.3.2" style="font-size:90%;">Mistral</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F5.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="830" id="S2.F5.sf3.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S2.F5.sf3.3.2" style="font-size:90%;">Mixtral</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F5.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="830" id="S2.F5.sf4.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S2.F5.sf4.3.2" style="font-size:90%;">GoEmotions</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S2.F5.3.2" style="font-size:90%;">Confusion matrices showing the performance (in %) of the fine-tuned RoBERTa baseline on the synthesised benchmarks, generated
by LLaMA2, Mistral, and Mixtral, respectively, in addition to the GoEmotions test benchmark. </span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Analysis</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">In this section, we analyse the zero-shot sentiment analysis capabilities of the following LLMs: Mistral, Mixtral, and two versions of LLaMA2 (7 billion and 13 billion parameters). We assess their zero-shot capabilities on the test partition of the GoEmotions dataset. We design a prompt that requires the selected LLMs to predict the corresponding emotion, including the neutral state (cf. <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T11" title="In 2.2.2 Analysis ‣ 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a>). Prompt engineering is crucial for influencing LLMs, as it enhances nuanced responses and ensures more accurate behaviour <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib71" title="">71</a>]</cite>. To minimise randomness and increase confidence in the predictions, we reduce the temperature setting to 0.1. This lower temperature sharpens the probability distribution, ensuring that the predicted classes reflect those that the LLMs are predicting with the highest probabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib69" title="">69</a>]</cite>. However, the LLM outputs sometimes include irrelevant or multiple emotions. To address this without intrusively altering the model’s outputs, we select the first listed emotion as the most reliable prediction. This approach aligns with the operational principle of decoder-based models, where the first valid emotion is mathematically the one with the highest confidence score, thus considered the valid class prediction.</p>
</div>
<figure class="ltx_table" id="S2.T11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T11.2.1.1" style="font-size:90%;">Table 11</span>: </span><span class="ltx_text" id="S2.T11.3.2" style="font-size:90%;">Prompts to use
LLMs for zero-shot emotion recognition, following a similar pattern as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib62" title="">62</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib69" title="">69</a>]</cite>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T11.4">
<tr class="ltx_tr" id="S2.T11.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T11.4.1.1"><span class="ltx_text ltx_font_bold" id="S2.T11.4.1.1.1">Prompt template</span></td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T11.4.2.1">You are an expert at affective computing.
Given a text by the user, analyze which emotion is</td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.3">
<td class="ltx_td ltx_align_left" id="S2.T11.4.3.1">most dominant in the given text.
Only classify one of the seven Ekman emotions, namely:</td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.4">
<td class="ltx_td ltx_align_left" id="S2.T11.4.4.1">‘neutral’, ‘fear’, ‘anger’, ‘happiness’, ‘sadness’, ‘disgust’, ‘surprise’. You are only allowed</td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.5">
<td class="ltx_td ltx_align_left" id="S2.T11.4.5.1">to answer with EXACTLY ONE word corresponding to the aforementioned seven emotions.</td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.6">
<td class="ltx_td ltx_align_left" id="S2.T11.4.6.1">In case of multiple emotions, use ONLY the ONE emotion you are most confident about.</td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.7">
<td class="ltx_td ltx_align_left" id="S2.T11.4.7.1">Use the following format:</td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.8">
<td class="ltx_td ltx_align_left" id="S2.T11.4.8.1">You are only allowed to answer with one of the following seven words:</td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.9">
<td class="ltx_td ltx_align_left" id="S2.T11.4.9.1">“neutral”, “fear”, “anger”, “happiness”, “sadness”, “disgust”, “surprise”.</td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.10">
<td class="ltx_td ltx_align_left" id="S2.T11.4.10.1">Don’t write an explanation of the answer.</td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.11">
<td class="ltx_td ltx_align_left" id="S2.T11.4.11.1">Don’t write things like “My guess is…”, or “I think …”.
Just write the emotion,</td>
</tr>
<tr class="ltx_tr" id="S2.T11.4.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T11.4.12.1">and nothing else.</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.T12" title="In 2.2.2 Analysis ‣ 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">12</span></a> summarises the comparative performance of the tested LLMs. We include the performance of the RoBERTa baseline model trained on the GoEmotions dataset (cf. <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#S2.SS2.SSS1" title="2.2.1 Generation ‣ 2.2 The Linguistic Modality Has Changed ‣ 2 Emergence in Foundation Models ‣ Affective Computing Has Changed: The Foundation Model Disruption"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2.1</span></a>) for benchmarking purposes. The first observation from the obtained results is that the UAR scores obtained by all the investigated LLMs surpass the chance level (14.3 %), underscoring the emergent affective capabilities of the LLMs tested in a zero-shot manner. Nevertheless, none of the LLMs outperforms the RoBERTa baseline model, fine-tuned on the GoEmotions dataset, which confirms the advantage of model-specific tuning. Nevertheless, it is worth highlighting that the difference in the UAR scores obtained by the best-performing GPT-3.5 and GPT-4 models in comparison to the RoBERTa baseline model is around 15 %. This is an interesting result, as these models have not been trained on the GoEmotions dataset, but still obtained a competitive
performance on the emotion recognition task, reinforcing, one more time, the emergent affective capabilities of the models.</p>
</div>
<figure class="ltx_table" id="S2.T12">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T12.2.1.1" style="font-size:90%;">Table 12</span>: </span><span class="ltx_text" id="S2.T12.3.2" style="font-size:90%;">Performance scores of the different LLMs tested on a zero-shot fashion for recognising the corresponding emotion on the sentences belonging to the test partition of the GoEmotions dataset.
</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T12.4" style="width:347.7pt;height:123.9pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.9pt,19.4pt) scale(0.76,0.76) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T12.4.1">
<tr class="ltx_tr" id="S2.T12.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T12.4.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="S2.T12.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.1.2.1">Recall (%)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T12.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.1.3.1">UAR</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T12.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.1.4.1">ACC</span></td>
</tr>
<tr class="ltx_tr" id="S2.T12.4.1.2">
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.2.1.1">Neutral</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.2.2"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.2.2.1">Fear</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.2.3"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.2.3.1">Anger</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.2.4"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.2.4.1">Happiness</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.2.5"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.2.5.1">Sadness</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.2.6"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.2.6.1">Disgust</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.2.7"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.2.7.1">Surprise</span></td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.2.8"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.2.8.1">(%)</span></td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.2.9"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.2.9.1">(%)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T12.4.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T12.4.1.3.1">LLaMA2-7B</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.3.2">4.51</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.3.3">33.77</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.3.4">42.31</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.3.5">66.60</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.3.6">39.00</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.3.7">40.79</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.3.8">58.04</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.3.9">40.72</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T12.4.1.3.10">38.78</td>
</tr>
<tr class="ltx_tr" id="S2.T12.4.1.4">
<td class="ltx_td ltx_align_left" id="S2.T12.4.1.4.1">LLaMA2-13B</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.4.2">11.89</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.4.3">40.26</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.4.4"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.4.4.1">67.31</span></td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.4.5">69.73</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.4.6">37.45</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.4.7">40.79</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.4.8">28.12</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.4.9">42.22</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.4.10">42.39</td>
</tr>
<tr class="ltx_tr" id="S2.T12.4.1.5">
<td class="ltx_td ltx_align_left" id="S2.T12.4.1.5.1">Mistral</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.5.2">57.95</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.5.3">57.14</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.5.4">47.31</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.5.5">22.08</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.5.6"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.5.6.1">58.30</span></td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.5.7">40.79</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.5.8">9.60</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.5.9">41.88</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.5.10">39.20</td>
</tr>
<tr class="ltx_tr" id="S2.T12.4.1.6">
<td class="ltx_td ltx_align_left" id="S2.T12.4.1.6.1">Mixtral</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.6.2">53.50</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.6.3">66.23</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.6.4">48.65</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.6.5">52.85</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.6.6">50.19</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.6.7">30.26</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.6.8">14.96</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.6.9">45.24</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.6.10">48.59</td>
</tr>
<tr class="ltx_tr" id="S2.T12.4.1.7">
<td class="ltx_td ltx_align_left" id="S2.T12.4.1.7.1">GPT-3.5</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.7.2">23.84</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.7.3"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.7.3.1">75.32</span></td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.7.4">56.35</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.7.5">65.54</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.7.6">40.54</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.7.7">53.95</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.7.8">18.08</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.7.9">47.66</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.7.10">43.85</td>
</tr>
<tr class="ltx_tr" id="S2.T12.4.1.8">
<td class="ltx_td ltx_align_left" id="S2.T12.4.1.8.1">GPT-4</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.8.2">38.42</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.8.3">68.83</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.8.4">42.50</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.8.5">50.28</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.8.6">36.68</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.8.7"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.8.7.1">65.79</span></td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.8.8">26.56</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.8.9">47.01</td>
<td class="ltx_td ltx_align_right" id="S2.T12.4.1.8.10">42.74</td>
</tr>
<tr class="ltx_tr" id="S2.T12.4.1.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T12.4.1.9.1">RoBERTa</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T12.4.1.9.2"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.9.2.1">71.53</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T12.4.1.9.3">74.03</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T12.4.1.9.4">54.42</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T12.4.1.9.5"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.9.5.1">76.86</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T12.4.1.9.6">56.76</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T12.4.1.9.7">44.74</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T12.4.1.9.8"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.9.8.1">61.38</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T12.4.1.9.9"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.9.9.1">62.82</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T12.4.1.9.10"><span class="ltx_text ltx_font_bold" id="S2.T12.4.1.9.10.1">69.22</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>The Speech Modality Has (Not Yet) Changed</h3>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Generation</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">Research on generating affective speech has been conducted for more than three decades <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib72" title="">72</a>]</cite>. The first
approaches were rule-based, while contemporary methods typically rely on deep learning. A detailed overview can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib72" title="">72</a>]</cite>. All of these methods are explicitly engineered to produce emotional speech. Thus, this line of research can be dubbed as a subfield of ‘traditional’ Affective Computing, while technically belonging to the Text-To-Speech (TTS) field. In recent years, similar to the developments in Natural Language Processing (NLP) and Computer Vision (CV), research on TTS has heavily
been
influenced by the success of the Foundation Model (FM) paradigm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib74" title="">74</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.1">UniAudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib73" title="">73</a>]</cite> is a Transformer-based general-purpose audio synthesis system. It is pretrained on seven
generative audio tasks, including TTS. In its pretrained state, however, it does not support affective speech synthesis. The authors demonstrate that their pretrained model serves as a basis for adaptation to different downstream tasks which opens up the possibility to equip the model with affective speech synthesis capabilities by mere finetuning. Another recent generative audio FM, PromptTTS2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib74" title="">74</a>]</cite>, synthesises speech based on text prompts that include descriptions of the voice to be generated. The controllable attributes of the synthesised speech must be defined during the training time. The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib74" title="">74</a>]</cite> do not investigate emotion as one such attribute, though their proposed framework would permit this.
Models such as UniAudio and PromptTTS2 can thus be categorised
as generative audio FMs
that could be adapted to emotional speech synthesis with moderate effort. To the best of our knowledge, no evidence for affective speech synthesis as an emergent capability of such models has been published so far. However, the demos for GPT-4o<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_href" href="https://openai.com/index/hello-gpt-4o/" title="">https://openai.com/index/hello-gpt-4o/</a></span></span></span> claim affective speech synthesis capabilities. At the moment, though, neither a technical report on GPT-4o, nor a systematic evaluation of affective speech produced by GPT-4o is available.
Considering the development towards releasing large pretrained models in the NLP and CV areas, we assume that in the near future powerful speech synthesis models will also be made publicly available and, hence, investigated more thoroughly,
allowing us to carry out according experiments to the above for vision and linguistics.
In addition, we expect a continuing trend toward truly multimodal FMs
that may not just take in but also produce natural speech with controllable prosodic properties. Several multimodal models that also produce audio output data have been proposed, for an overview see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib76" title="">76</a>]</cite>. To the best of our knowledge, none of them exhibit emotional speech synthesis capabilities. We expect that emergent affective speech synthesis will eventually be achieved via a multimodal approach. As shown in the previous sections, large pretrained vision and language models already encode affective information. Multimodal approaches leveraging such pretrained models in combination with multimodal affect-related data may learn to associate affective speech characteristics with corresponding affective states as expressed in the visual and the textual data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Analysis</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">A system capable of analysing arbitrary affective properties of speech data without any tuning must ingest both audio and text inputs. Several FM
approaches fulfilling this requirement have been proposed. However, the vast majority of them are not pretrained on speech data at all.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p2">
<p class="ltx_p" id="S2.SS3.SSS2.p2.1">Examples include AnyMAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib77" title="">77</a>]</cite>, X-InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib78" title="">78</a>]</cite>, and ModaVerse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib79" title="">79</a>]</cite>.
In only a few models, speech is part of the pretraining data. QWEN-Audio’s training data comprises several labelled speech datasets, including emotionality already. Hence, QWEN-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib80" title="">80</a>]</cite> in the proposed form is not a candidate for exploring ‘emergent’ affective
recognition capabilities. X-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib81" title="">81</a>]</cite> processes video, text, and audio inputs and is explicitly designed to process speech. The authors, however, do not report any experiments related to predicting affect in speech. As of now, the pretrained X-LLM model is not publicly available,
hence, unfortunately again, not allowing us to carry out experiments analogous to the vision and the linguistic ones.
</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p3">
<p class="ltx_p" id="S2.SS3.SSS2.p3.1">Similar to the affective speech synthesis problem, near-future multimodal FMs
can be expected to be capable of analysing affective speech in a zero-shot manner, even if not explicitly pretrained in this regard. As of now, however, we are not aware of any such model.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>The Evaluation Is Changing</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">One of the reasons to understand the impressive performance of currently available Foundation Models (FM) is that they use massive amounts of data from “the Internet” for training. Nevertheless, the indiscriminate use of data poses the following challenge to the scientific community: can we guarantee that the data we feed to these models for testing – or even for evaluation – has not been used for training?
In case of a negative answer, how fair and representative of the model capabilities can standard evaluation metrics be? Although we do not have a concrete answer yet, we hope these challenges engage the research community into looking for methods and metrics that allow a proper scientific evaluation of these emerging FMs
in the field of Affective Computing.
As is, the current state of such models in Affective Computing may resemble a shell game: many different tools and approaches are shuffled and mixed until some partially less, partially more convincing performances are obtained. Especially because it is the popular ‘Big Six’ Ekman emotions we considered herein, chances are high that the models only reverberate with what they have already seen.
Testing on more subtle models such as the dimensional approach or less considered affective states is therefore urgently needed.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Concerns and Regulations Have Changed</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In April 2021, the European Commission presented the AI Act: the first-ever legal framework worldwide to regulate the use of AI-based technologies in the European territory. The Act was endorsed by all Member States in February 2024 after being approved by the EU Council. The regulation follows a risk-based approach, so instead of regulating specific systems and applications, it defines measures and requirements based on a classification system that encapsulates varying degrees of risks posed by the AI systems. The proposed classification system spans four different levels of risk: unacceptable, high, limited, and minimal.
According to the final version of the Act<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.euaiact.com/" title="">https://www.euaiact.com/</a></span></span></span>, systems that fall into the unacceptable risk category will be prohibited.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">This regulation is of special interest for the Affective Computing community, since it singles out affective
systems in several ways. The regulation defines an emotion recognition system in its Article 3 (34) as an “AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data”. Thus, any emotion recognition system using speech or facial data, as well as other physical signals such as the electroencephalogram, falls under this definition. Article 5 enumerates different practices and systems considered as prohibited, including the placing on the market or the use of AI systems to infer emotions of a natural person in the workplace or in
education institutions. Whilst this point only prohibits emotion recognition systems in two specific contexts (workplace and education), the list of high-risk systems presented in Annex III includes “AI systems intended to be used for emotion recognition” in the category of biometric systems. Article 6 provides a nuance on the referred systems, stating that AI systems “shall not be considered as high risk if they do not pose a significant risk of harm to the health, safety, or fundamental rights of natural persons”. However, this exception shall not apply for systems performing profiling as defined in Article 4 of the General Data Protection Regulation (GDPR). Considering the definition of profiling, it is unclear whether any emotion recognition system, even in non-harmful applications such as entertainment, could be considered of limited risk, except for those based on non-identifiable data. In any case, Article 52 imposes that any system including an emotion recognition component must notify the users of the operation of such system, notwithstanding its risk.
</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">In this context, the regulation imposes several obligations that high-risk systems shall comply with, and for which the provider (i. e., a natural person/agency that develops and places on the market the AI system) is responsible. These obligations are detailed across Chapters 2-5 of Title III – more than 40 articles – and include conducting post-market surveillance of risk, informing the competent authorities about the product, or providing technical documentation of both the system and the data used in development (i. e., data governance), among others. Note that the definition of provider is quite vague from a research perspective, so it is unclear how this will affect research on Affective Computing. Article 2 (5a) specifies that the regulation does not apply to models developed for the sole purpose of scientific research, but by the time the model is made available, researchers may face some of the previously cited obligations depending on how it is used, after which they may become providers. By considering cases in which the system is made available free of charge, the regulation seems to cover open-source systems and situations in which the source code is made publicly available. However, this does not seem to be the case when two academic institutions share code in a confidential manner for academic purposes.
</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Attending now to Foundation Models (FM), which are referred in the text as “General-purpose AI models”, Article 52 requires any AI-synthesised content to be marked as such, which clearly covers generation
of emotional samples with models such as LLaMA, or SD, among others.
Besides, Article 52 includes several subparagraphs with the obligations that providers shall meet when deploying this kind of systems, such as providing documentation on how it was trained and validated, and the content used for that purpose. In addition, an FM
may be considered to pose systemic risk if it has high computational capabilities, or if it is marked as such by the Commission. In this case, providers shall comply with more obligations, including testing and mitigating foreseeable risks, data governance measures, maintaining appropriate levels of performance and interpretability. In this context, there are some concerns that current FMs
do not comply with all the measures required <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib82" title="">82</a>]</cite>,
hence accepting to slow down innovation and AI development (particularly) in Europe, to assure highest safety and ethical standards.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">Beyond, emotion recognition based on physiological data has been less investigated than other modalities, such as facial expressions or speech.
Some types of physiological signals include electroencephalogram (EEG), electrocardiogram (ECG), electromyogram (EMG), electrodermal activity (EDA) or galvanic skin response (GSR), respiration rate (RSP), and pulse rate. Emotions are complex and sometimes cannot be solely recognised by analysing speech or image data.
It is quite easy for people to control – or even hide – their real current emotional state under certain circumstances, mainly because of social pressure. For instance, people may pretend to smile and laugh while they are feeling sad or angry <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08907v1#bib.bib83" title="">83</a>]</cite>.
Ethical considerations regarding privacy and data security are of high importance when analysing physiological data for affective
purposes to avoid both: ‘(Full) Affective Mind Reading’ and ‘Affective Mind Writing’.
</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">Many further challenges and ethical concerns remain and will also become more apparent once such
technology is broadly used – hopefully, the community can provide technical and legal means of protection to ensure
we are all enjoying the positive side of Affective Computing only.
</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Outlook and Conclusions</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this paper, we analysed the affective capabilities of currently available Foundation Models (FM) exploiting the vision, the linguistics, and the speech (acoustic) modalities. While the affective generation
and analysis
capabilities of the vision- and the linguistic-based FMs are plausible, the affective generation and analysis of speech-based FMs is not yet mature enough. Nonetheless, it is reasonable to imagine a not-too-distant future where this technology achieves similar results as with the other two modalities. Despite not being currently available, we also envision physiological-based FMs to be developed and explored in the near future.
</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">One of the main outcomes of this work is the collection of two synthetically-generated affective corpora generated with FMs – one containing facial images, the other textual sentences
that will be publicly available.
The models training and the analyses reported herein were performed assuming that the synthetically generated instances conveyed the prompted emotions. Nonetheless, we acknowledge this could not always be the case. To overcome this limitation, we plan to run a data collection with human annotators to annotate the generated samples, assessing the affective capabilities of the selected FMs from a human perspective.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Funding</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This project has received funding from the DFG’s Reinhart Koselleck project No. 442218748 (AUDI0NOMOUS).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.1.1">
<span class="ltx_bibblock"><span class="ltx_ERROR undefined" id="bib.1.1.1.1">\bibcommenthead</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Picard, R. W.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Affective Computing</em>
(MIT Press, Cambridge, MA, USA,
1997).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Deshpande, M. &amp; Rao, V.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Depression Detection Using Emotion Artificial
Intelligence</em>, In Proceedings of the International Conference on
Intelligent Sustainable Systems, 858–862
(IEEE, Palladam, India,
2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Healey, J. A. &amp; Picard, R. W.

</span>
<span class="ltx_bibblock">Detecting Stress during Real-world Driving Tasks
Using Physiological Sensors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Transactions on Intelligent
Transportation Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib3.2.2">6</span>,
156–166 (2005).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Spezialetti, M., Placidi, G. &amp;
Rossi, S.

</span>
<span class="ltx_bibblock">Emotion Recognition for Human-robot Interaction:
Recent Advances and Future Perspectives.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Frontiers in Robotics and AI</em>
<span class="ltx_text ltx_font_bold" id="bib.bib4.2.2">7</span> (2020).

</span>
<span class="ltx_bibblock">Paper ID: 532279.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Liu, Z. <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et al.</em>
</span>
<span class="ltx_bibblock">A Facial Expression Emotion Recognition Based
Human-robot Interaction System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.2.1">IEEE/CAA Journal of Automatica Sinica</em>
<span class="ltx_text ltx_font_bold" id="bib.bib5.3.2">4</span>, 668–676
(2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Crumpton, J. &amp; Bethel, C. L.

</span>
<span class="ltx_bibblock">A Survey of Using Vocal Prosody to Convey Emotion in
Robot Speech.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International Journal of Social Robotics</em>
<span class="ltx_text ltx_font_bold" id="bib.bib6.2.2">8</span>, 271–285
(2016).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Tan, L. <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Speech Emotion Recognition Enhanced Traffic
Efficiency Solution for Autonomous Vehicles in a 5G-Enabled
Space–Air–Ground Integrated Intelligent Transportation System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.2.1">IEEE Transactions on Intelligent
Transportation Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib7.3.2">23</span>,
2830–2842 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Amiriparian, S. <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.2.1">Speech-Based Classification of Defensive
Communication: A Novel Dataset and Results</em>, In Proceedings of the 24th
Annual Conference of the International Speech Communication Association,
2703–2707 (ISCA,
Dublin, Ireland, 2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Mehrabian, A.

</span>
<span class="ltx_bibblock"> in <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Communication Without Words</span>
(Taylor &amp; Francis, 1968).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Li, S. &amp; Deng, W.

</span>
<span class="ltx_bibblock">Deep Facial Expression Recognition: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">IEEE Transactions on Affective Computing</em>
<span class="ltx_text ltx_font_bold" id="bib.bib10.2.2">13</span>, 1195–1215
(2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Takalkar, M., Xu, M., Wu,
Q. &amp; Chaczko, Z.

</span>
<span class="ltx_bibblock">A survey: facial micro-expression recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Multimedia Tools and Applications</em>
<span class="ltx_text ltx_font_bold" id="bib.bib11.2.2">77</span>, 19301–19325
(2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Birjali, M., Kasri, M. &amp;
Beni-Hssane, A.

</span>
<span class="ltx_bibblock">A comprehensive survey on sentiment analysis:
Approaches, challenges and trends.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Knowledge-Based Systems</em>
<span class="ltx_text ltx_font_bold" id="bib.bib12.2.2">226</span> (2021).

</span>
<span class="ltx_bibblock">Paper ID: 107134.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Poria, S., Majumder, N.,
Mihalcea, R. &amp; Hovy, E.

</span>
<span class="ltx_bibblock">Emotion Recognition in Conversation: Research
Challenges, Datasets, and Recent Advances.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE Access</em> <span class="ltx_text ltx_font_bold" id="bib.bib13.2.2">7</span>,
100943–100953 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Schuller, B. W.

</span>
<span class="ltx_bibblock">Speech Emotion Recognition: Two Decades in a
Nutshell, Benchmarks, and Ongoing Trends.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Communications of the ACM</em>
<span class="ltx_text ltx_font_bold" id="bib.bib14.2.2">61</span>, 90–99
(2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Khalil, R. A. <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Speech Emotion Recognition Using Deep Learning
Techniques: A Review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.2.1">IEEE Access</em> <span class="ltx_text ltx_font_bold" id="bib.bib15.3.2">7</span>,
117327–117345 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Wang, Y. <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">et al.</em>
</span>
<span class="ltx_bibblock">A systematic review on affective computing: emotion
models, databases, and recent advances.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.2.1">Information Fusion</em>
<span class="ltx_text ltx_font_bold" id="bib.bib16.3.2">83-84</span>, 19–52
(2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Poria, S., Cambria, E.,
Bajpai, R. &amp; Hussain, A.

</span>
<span class="ltx_bibblock">A review of affective computing: From unimodal
analysis to multimodal fusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Information Fusion</em>
<span class="ltx_text ltx_font_bold" id="bib.bib17.2.2">37</span>, 98–125
(2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Ekman, P. &amp; Friesen, W.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Facial action coding system: a technique for
the measurement of facial movement</em> (Consulting
Psychologist Press, Palo Alto, CA, 1978).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Murray, I. R. &amp; Arnott, J. L.

</span>
<span class="ltx_bibblock">Toward the simulation of emotion in synthetic
speech: A review of the literature on human vocal emotion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">The Journal of the Acoustical Society of
America</em> <span class="ltx_text ltx_font_bold" id="bib.bib19.2.2">93</span>, 1097–1108
(1993).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Caliskan, A., Bryson, J. J. &amp;
Narayanan, A.

</span>
<span class="ltx_bibblock">Semantics derived automatically from language
corpora contain human-like biases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Science</em> <span class="ltx_text ltx_font_bold" id="bib.bib20.2.2">356</span>,
183–186 (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Trigeorgis, G. <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.2.1">Adieu Features? End-to-End Speech Emotion
Recognition using a Deep Convolutional Recurrent Network</em>, In Proceedings
of the 41st International Conference on Acoustics, Speech, and Signal
Processing, 5200–5204 (IEEE,
Shanghai, China, 2016).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Zhang, Z., Han, J., Qian,
K. &amp; Schuller, B.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Evolving Learning for Analysing Mood-Related
Infant Vocalisation</em>, In Proceedings of the 19th Annual Conference of the
International Speech Communication Association, 142–146
(ISCA, Hyderabad, India,
2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Rajapakshe, T. <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">et al.</em>
</span>
<span class="ltx_bibblock">emoDARTS: Joint Optimisation of CNN &amp; Sequential
Neural Network Architectures for Superior Speech Emotion Recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.2.1">IEEE Access</em> <span class="ltx_text ltx_font_bold" id="bib.bib23.3.2">12</span>,
110492–110503 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Goodfellow, I., Pouget-Abadie, J.,
Mirza, M., Xu, B. <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.2.1">Generative Adversarial Nets</em>, In
Proceedings of the 28th Annual Conference on Neural Information Processing
Systems (NIPS, Montréal, Canada,
2014).

</span>
<span class="ltx_bibblock">9 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ding, H., Sricharan, K. &amp;
Chellappa, R.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">ExprGAN: Facial Expression Editing with
Controllable Expression Intensity</em>, In Proceedings of the 32nd Conference
on Artificial Intelligence, 6781–6788
(AAAI, Austin, TX, USA,
2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Karras, T. <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.2.1">Analyzing and Improving the Image Quality of
StyleGAN</em>, In Proceedings of the Conference on Computer Vision and Pattern
Recognition, 8110–8119 (CVF,
Virtual Conference, 2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Balakrishnan, G., Xiong, Y.,
Xia, W. &amp; Perona, P.

</span>
<span class="ltx_bibblock">Towards Causal Benchmarking of Bias in Face Analysis
Algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Deep Learning-Based Face Analytics. Advances
in Computer Vision and Pattern Recognition</em> 327–359
(2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Ghosh, S., Chollet, M.,
Laksana, E., Morency, L.-P. &amp;
Scherer, S.

</span>
<span class="ltx_bibblock">Affect-LM: A Neural Language Model for Customizable
Affective Text Generation (2017).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1704.06851" title="">https://arxiv.org/abs/1704.06851</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Zhou, K., Sisman, B.,
Rana, R., Schuller, B. W. &amp;
Li, H.

</span>
<span class="ltx_bibblock">Speech Synthesis With Mixed Emotions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">IEEE Transactions on Affective Computing</em>
<span class="ltx_text ltx_font_bold" id="bib.bib29.2.2">14</span>, 3120–3134
(2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Wang, Y. <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.2.1">Style Tokens: Unsupervised Style Modeling,
Control and Transfer in End-to-End Speech Synthesis</em>, In Proceedings of
the 35th International Conference on Machine Learning,
5180–5189 (PMLR,
Stockholm, Sweden, 2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Lucey, P., Cohn, J. F.,
Kanda, T., Saragih, J. <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.2.1">The Extended Cohn-Kanade Dataset (CK+): A
complete dataset for action unit and emotion-specified expression</em>, In
Workshop Proceedings of the Conference on Computer Vision and Pattern
Recognition, 94–101 (IEEE,
San Francisco, CA, USA, 2010).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Burkhardt, F., Paeschke, A.,
Rolfes, M., Sendlmeier, W. F. &amp;
Weiss, B.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">A database of german emotional speech</em>, In
Proceedings of the 6th Annual Conference of the International Speech
Communication Association, 1517–1520
(ISCA, Lisbon, Portugal,
2005).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Mollahosseini, A., Hasani, B. &amp;
Mahoor, M. H.

</span>
<span class="ltx_bibblock">AffectNet: A Database for Facial Expression,
Valence, and Arousal Computing in the Wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">IEEE Transactions on Affective Computing</em>
<span class="ltx_text ltx_font_bold" id="bib.bib33.2.2">10</span>, 18–31
(2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Kossaifi, J. <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">et al.</em>
</span>
<span class="ltx_bibblock">SEWA DB: A Rich Database for Audio-Visual Emotion
and Sentiment Research in the Wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.2.1">IEEE Transactions on Pattern Analysis and
Machine Intelligence</em> <span class="ltx_text ltx_font_bold" id="bib.bib34.3.2">43</span>,
1022–1040 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Rombach, R., Blattmann, A.,
Lorenz, D., Esser, P. <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.2.1">High-Resolution Image Synthesis with Latent
Diffusion Models</em>, In Proceedings of the International Conference on
Computer Vision and Pattern Recognition, 10684–10695
(CVF, New Orleans, LA, USA,
2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N.,
Parmar, N., Uszkoreit, J. <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.2.1">Attention Is All You Need</em>, In Proceedings
of the 31st Annual Conference on Neural Information Processing Systems
(NIPS, Long Beach, CA, USA,
2017).

</span>
<span class="ltx_bibblock">10 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.,
Lee, K. &amp; Toutanova, K.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding</em>, In Proceedings of the Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, 4171–4186
(ACL, Minnesota, MN, USA,
2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J. W.,
Hallacy, C., Ramesh, A. <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.2.1">Learning Transferable Visual Models from
Natural Language Supervision</em>, In Proceedings of the 38th International
Conference on Machine Learning, 8748–8763
(PMLR, Virtual Conference,
2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang,
X., Almeida, D. <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.2.1">Training language models to follow
instructions with human feedback</em>, In Proceedings of the 36th Annual
Conference on Neural Information Processing Systems,
27730–27744 (NIPS,
New Orleans, LA, USA, 2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Touvron, H. <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">et al.</em>
</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language
Models (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.13971" title="">https://arxiv.org/abs/2302.13971</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Wang, C. <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Neural Codec Language Models are Zero-shot Text to
Speech Synthesizers (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2301.02111" title="">https://arxiv.org/abs/2301.02111</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Podell, D. <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">et al.</em>
</span>
<span class="ltx_bibblock">SDXL: Improving Latent Diffusion Models for
High-resolution Image Synthesis (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.01952" title="">https://arxiv.org/abs/2307.01952</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Schaeffer, R., Miranda, B. &amp;
Koyejo, S.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Are Emergent Abilities of Large Language
Models a Mirage?</em>, In Proceedings of the 37th Annual Conference on Neural
Information Processing Systems (NIPS,
New Orleans, LA, USA, 2023).

</span>
<span class="ltx_bibblock">17 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Ekman, P. &amp; Friesen, W. V.

</span>
<span class="ltx_bibblock">Constants across cultures in the face and emotion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Journal of Personality and Social
Psychology</em> <span class="ltx_text ltx_font_bold" id="bib.bib44.2.2">17</span>, 124–129
(1971).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Ho, J., Jain, A. &amp;
Abbeel, P.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Denoising Diffusion Probabilistic Models</em>,
In Proceedings of the 34th Annual Conference on Neural Information
Processing Systems (NIPS, Virtual
Conference, 2020).

</span>
<span class="ltx_bibblock">12 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Ramesh, A. <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.2.1">Zero-Shot Text-to-Image Generation</em>, In
Proceedings of the 38th International Conference on Machine Learning,
8821–8831 (PMLR,
Virtual Conference, 2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Ramesh, A., Dhariwal, P.,
Nichol, A., Chu, C. &amp;
Chen, M.

</span>
<span class="ltx_bibblock">Hierarchical Text-Conditional Image Generation with
CLIP Latents (2022).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.06125" title="">https://arxiv.org/abs/2204.06125</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Plutchik, R.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Emotions and life: Perspectives from
psychology, biology, and evolution</em> (American
Psychological Association, 2003).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Baltrušaitis, T., Robinson, P. &amp;
Morency, L.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">OpenFace: an Open Source Facial Behavior
Analysis Toolkit</em>, In Proceedings of the Winter Conference on Applications
of Computer Vision (IEEE, Lake
Placid, NY, USA, 2016).

</span>
<span class="ltx_bibblock">10 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Dosovitskiy, A. <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.2.1">An Image Is Worth 16x16 Words: Transformers
for Image Recognition at Scale</em>, In Proceedings of the 9th International
Conference on Learning Representations (ICLR,
Virtual Conference, 2021).

</span>
<span class="ltx_bibblock">21 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Goodfellow, I. J. <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.2.1">Challenges in Representation Learning: A
Report on Three Machine Learning Contests</em>, In Proceedings of the
International Conference on Neural Information Processing,
117–124 (Springer,
Daegu, Republic of Korea, 2013).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Peña, A., Morales, A.,
Serna, I., Fierrez, J. &amp;
Lapedriza, A.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Facial Expressions as a Vulnerability in Face
Recognition</em>, In Proceedings of the International Conference on Image
Processing, 2988–2992 (IEEE,
Anchorage, AK, USA, 2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Liu, H., Li, C., Wu, Q.
&amp; Lee, Y. J.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Visual Instruction Tuning</em>, In Proceedings
of the 37th Annual Conference on Neural Information Processing Systems
(NIPS, New Orleans, LA, USA,
2023).

</span>
<span class="ltx_bibblock">25 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Liu, H., Li, C., Li, Y.
&amp; Lee, Y. J.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Improved Baselines with Visual Instruction
Tuning</em>, In Proceedings of the Conference on Computer Vision and Pattern
Recognition, 26296–26306 (CVF,
Seattle, WA, USA, 2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Touvron, H. <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat
Models (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.09288" title="">https://arxiv.org/abs/2307.09288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Mikolov, T., Karafiát, M.,
Burget, L., Černocký, J. &amp;
Khudanpur, S.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Recurrent neural network based language
model</em>, In Proceedings of the 11th Annual Conference of the International
Speech Communication Association, 1045–1048
(ISCA, 2010).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Radford, A., Narasimhan, K.,
Salimans, T., Sutskever, I. <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Improving Language Understanding by Generative
Pre-Training (2018).

</span>
<span class="ltx_bibblock">Preprint at
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/research/language-unsupervised" title="">https://openai.com/research/language-unsupervised</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Achiam, J. <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">et al.</em>
</span>
<span class="ltx_bibblock">GPT-4 Technical Report (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.08774" title="">https://arxiv.org/abs/2303.08774</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Li, C. <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Large Language Models Understand and Can be Enhanced
by Emotional Stimuli (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.11760" title="">https://arxiv.org/abs/2307.11760</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Broekens, J. <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.2.1">Fine-grained Affective Processing Capabilities
Emerging from Large Language Models</em>, In Proceedings of the 11th
International Conference on Affective Computing and Intelligent Interaction
(IEEE, Cambridge, MA, USA,
2023).

</span>
<span class="ltx_bibblock">8 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Amin, M., Cambria, E. &amp;
Schuller, B. W.

</span>
<span class="ltx_bibblock">Will Affective Computing Emerge From Foundation
Models and General Artificial Intelligence? A First Evaluation of ChatGPT.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">IEEE Intelligent Systems</em>
<span class="ltx_text ltx_font_bold" id="bib.bib61.2.2">38</span>, 15–23
(2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Amin, M. M., Mao, R.,
Cambria, E. &amp; Schuller, B. W.

</span>
<span class="ltx_bibblock">A Wide Evaluation of ChatGPT on Affective Computing
Tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">IEEE Transactions on Affective Computing</em>
(2024).

</span>
<span class="ltx_bibblock">9 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Wang, X., Li, X., Yin,
Z., Wu, Y. &amp; Liu, J.

</span>
<span class="ltx_bibblock">Emotional Intelligence of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Journal of Pacific Rim Psychology</em>
<span class="ltx_text ltx_font_bold" id="bib.bib63.2.2">17</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Jiang, A. Q. <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Mistral 7B (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.06825" title="">https://arxiv.org/abs/2310.06825</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Jiang, A. Q. <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Mixtral of Experts (2024).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.04088" title="">https://arxiv.org/abs/2401.04088</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Fedus, W., Zoph, B. &amp;
Shazeer, N.

</span>
<span class="ltx_bibblock">Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">The Journal of Machine Learning Research</em>
<span class="ltx_text ltx_font_bold" id="bib.bib66.2.2">23</span>, 1–39 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Demszky, D. <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.2.1">GoEmotions: A Dataset of Fine-Grained
Emotions</em>, In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, 4040–4054
(ACL, Virtual Conference,
2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Liu, Y., Ott, M., Goyal,
N., Du, J. <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">et al.</em>
</span>
<span class="ltx_bibblock">RoBERTa: A Robustly Optimized BERT Pretraining
Approach (2019).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1907.11692" title="">https://arxiv.org/abs/1907.11692</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Amin, M. M. &amp; Schuller, B. W.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">On Prompt Sensitivity of ChatGPT in Affective
Computing</em>, In Proceedings of the 12th International Conference on
Affective Computing and Intelligent Interaction (IEEE,
Glasgow, Scotland, 2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Zheng, L. <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">et al.</em>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.2.1">Judging LLM-as-a-Judge with MT-Bench and
Chatbot Arena</em>, In Proceedings of the 37th Annual Conference on Neural
Information Processing Systems (NIPS,
New Orleans, LA, USA, 2023).

</span>
<span class="ltx_bibblock">29 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Zhou, Y. <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Large Language Models are Human-Level Prompt
Engineers (2022).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.01910" title="">https://arxiv.org/abs/2211.01910</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Triantafyllopoulos, A. <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">et al.</em>
</span>
<span class="ltx_bibblock">An Overview of Affective Speech Synthesis and
Conversion in the Deep Learning Era.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.2.1">Proceedings of the IEEE</em>
<span class="ltx_text ltx_font_bold" id="bib.bib72.3.2">111</span>, 1355–1381
(2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Yang, D. <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">et al.</em>
</span>
<span class="ltx_bibblock">UniAudio: An Audio Foundation Model Toward Universal
Audio Generation (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.00704" title="">https://arxiv.org/abs/2310.00704</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Leng, Y. <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">et al.</em>
</span>
<span class="ltx_bibblock">PromptTTS 2: Describing and Generating Voices with
Text Prompt (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.02285" title="">https://arxiv.org/abs/2309.02285</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Wu, J., Gan, W., Chen,
Z., Wan, S. &amp; Philip, S. Y.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Multimodal Large Language Models: A Survey</em>,
In Proceedings of the International Conference on Big Data,
2247–2256 (IEEE,
Sorrento, Italy, 2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Zhang, D. <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">et al.</em>
</span>
<span class="ltx_bibblock">MM-LLMs: Recent Advances in MultiModal Large
Language Models (2024).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.13601" title="">https://arxiv.org/abs/2401.13601</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Moon, S. <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">et al.</em>
</span>
<span class="ltx_bibblock">AnyMAL: An Efficient and Scalable Any-Modality
Augmented Language Model (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.16058" title="">https://arxiv.org/abs/2309.16058</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Panagopoulou, A. <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">et al.</em>
</span>
<span class="ltx_bibblock">X-InstructBLIP: A Framework for aligning X-Modal
instruction-aware representations to LLMs and Emergent Cross-modal
Reasoning (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.18799" title="">https://arxiv.org/abs/2311.18799</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Wang, X., Zhuang, B. &amp;
Wu, Q.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">ModaVerse: Efficiently Transforming Modalities
with LLMs</em>, In Proceedings of the Conference on Computer Vision and
Pattern Recognition, 26606–26616
(CVF, Seattle, WA, USA,
2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Chu, Y. <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Qwen-Audio: Advancing Universal Audio Understanding
via Unified Large-Scale Audio-Language Models (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.07919" title="">https://arxiv.org/abs/2311.07919</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Chen, F. <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">et al.</em>
</span>
<span class="ltx_bibblock">X-LLM: Bootstrapping Advanced Large Language Models
by Treating Multi-Modalities as Foreign Languages (2023).

</span>
<span class="ltx_bibblock">Preprint at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.04160" title="">https://arxiv.org/abs/2305.04160</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Bommasani, R., Klyman, K.,
Zhang, D. &amp; Liang, P.

</span>
<span class="ltx_bibblock">Do Foundation Model Providers Comply with the EU AI
Act? (2023).

</span>
<span class="ltx_bibblock">Available in
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://crfm.stanford.edu/2023/06/15/eu-ai-act.html" title="">https://crfm.stanford.edu/2023/06/15/eu-ai-act.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Shu, L. <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">et al.</em>
</span>
<span class="ltx_bibblock">A Review of Emotion Recognition Using Physiological
Signals.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.2.1">Sensors</em> <span class="ltx_text ltx_font_bold" id="bib.bib83.3.2">18</span>
(2018).

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 13 15:02:25 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
