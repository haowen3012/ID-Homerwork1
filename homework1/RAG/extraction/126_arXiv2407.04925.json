{
    "S3.T1.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.1.1.1.1.1\" style=\"width:227.6pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1.1.1\">Prompt Template</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\" id=\"S3.T1.1.2.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.2.2.1.1.1\" style=\"width:227.6pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.2.2.1.1.1.1\">You are a fantastic Coursera course recommender. Use the following pieces of context to answer the question and recommend relevant courses to the user.\nIf the user doesn&#8217;t specify their requirements, you can just recommend some courses that are most popular in the system based on their ratings and difficulty levels. You only need to provide the course title to the user.\nAlso, please pay attention to how many courses the user wants you to recommend.\nIf you don&#8217;t know the answer, just say &#8220;I don&#8217;t know&#8221;.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.1.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.3.3.1.1.1\" style=\"width:227.6pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.3.3.1.1.1.1\">Context</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\" id=\"S3.T1.1.4.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.4.4.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.4.4.1.1.1\" style=\"width:227.6pt;\">Retrieved course data</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.1.5.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.5.5.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.5.5.1.1.1\" style=\"width:227.6pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.5.5.1.1.1.1\">User Question</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r\" id=\"S3.T1.1.6.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.6.6.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.6.6.1.1.1\" style=\"width:227.6pt;\">User&#8217;s specific question to the generator</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 1: Overview of interaction prompt structure",
        "footnotes": [],
        "references": [
            "The “cold start” problem, where systems lack user historical data, is a significant challenge in recommendation systems. Both traditional course recommender algorithums like content-based and collaborative-filtering algorithms and LLM-based system recommendation systems struggle with this issue. However, our RAG-based solution addresses this by using a ‘prompt template’ in the back-end. This template guides RAMO to generate relevant responses even when no user-specific data is available, as detailed in Table 1. The RAMO system can provide meaningful recommendations from the outset, unlike non-RAG-based recommender systems, which lack a retrieval process and prompt-based customization. The prompt to our retriever (i.e., to retrieve the relevant docs from the databases) is called the ‘prompt template’, which is shown in Table 1. The prompt to our generator is composed with three parts: 1) User Question, 2) Prompt Template, and 3) Search Results (the context of the retrieved relevant documents). We also added the uplifting adverb ‘fantastic’ to the prompt template, to elevate it with Emotional Intelligence since ChatGPT is designed to recognize patterns in language, including those associated with emotions [33]."
        ]
    },
    "S3.T2.2": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.2.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.2.3.1.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">LLM Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.2.3.1.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Output Cost</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.2.3.1.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">Token Limit</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.2.1\">GPT-3.5 Turbo</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<math alttext=\"0.50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.1.1.1.m1.1\"><semantics id=\"S3.T2.1.1.1.m1.1a\"><mn id=\"S3.T2.1.1.1.m1.1.1\" xref=\"S3.T2.1.1.1.m1.1.1.cmml\">0.50</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.1.1.1.m1.1b\"><cn id=\"S3.T2.1.1.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.T2.1.1.1.m1.1.1\">0.50</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.1.1.1.m1.1c\">0.50</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.1.1.1.m1.1d\">0.50</annotation></semantics></math> per 1M tokens</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">4,096 tokens</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.2.2.2.1\">GPT-4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<math alttext=\"30.00\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.2.2.1.m1.1\"><semantics id=\"S3.T2.2.2.1.m1.1a\"><mn id=\"S3.T2.2.2.1.m1.1.1\" xref=\"S3.T2.2.2.1.m1.1.1.cmml\">30.00</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.2.2.1.m1.1b\"><cn id=\"S3.T2.2.2.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.T2.2.2.1.m1.1.1\">30.00</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.2.2.1.m1.1c\">30.00</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.2.2.1.m1.1d\">30.00</annotation></semantics></math> per 1M tokens</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.2.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">8,192 tokens</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.4.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.4.1.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.2.4.1.1.1\">Llama-2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.4.1.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Free</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.4.1.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">4,096 tokens</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.5.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T2.2.5.2.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.2.5.2.1.1\">Llama-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T2.2.5.2.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Free</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S3.T2.2.5.2.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">8,000 tokens</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 2: Cost and token limit of models we used",
        "footnotes": [],
        "references": [
            "As shown in Table 2 below, we employed several LLMs to build our course recommender system. We provide a list of the LLM models we used, along with details on their associated costs and token limits. The token limit refers to the maximum number of tokens (a token represents about 3/4 of a word or four characters, according to Open AI [1]) that the model can process in a single input. While some models, like Llama 2 and Llama 3, are free to use on small-scale dataset, due to their open-source nature, others may incur costs based on usage or subscription plans [27]."
        ]
    }
}