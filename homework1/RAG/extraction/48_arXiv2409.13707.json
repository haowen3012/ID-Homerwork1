{
    "S4.T1.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S4.T1.1.1.1.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.4.1\">Positive</span></th>\n<th class=\"ltx_td ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.1.2.1.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.2.1.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.1.2.1\">Train</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.2.1.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.1.3.1\">Eval</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.2.1.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.1.4.1\">Class %</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.2.1.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.1.5.1\">F1</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.2.1.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.1.6.1\">P</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.2.1.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.1.7.1\">R</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.1.3.2.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">In-Domain</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.3.2.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">13964</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.3.2.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">3512</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.3.2.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">25%</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.3.2.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">0.46</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.3.2.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">0.31</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.3.2.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">0.89</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.1.4.3.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Out-of-Domain</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.4.3.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.4.3.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">1375</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.4.3.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">53%</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.4.3.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">0.65</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.4.3.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">0.54</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.1.4.3.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">0.80</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 1: Single-Turn classifier model performance on six in-domain training products and three out-of-domain evaluation products",
        "footnotes": [],
        "references": [
            "For our application, we prioritized correctly predicting single-turn cases (positive class) versus multi-turn cases (negative) emphasizing recall over precision. Table 1 presents the final fine-tuned model performance on the held-out evaluation set of the six products when using a classification threshold of 0.1. We found that performance varies widely depending on the product, ranging from F1 of 0.27 to 0.62 and recall from 0.75 to 0.98. The lower performance can be explained in part by the varying class imbalance across products (positive class proportion from 11% to 44%) as well as the products’ differing inter-annotator agreement (See Section 6). Despite this, the model still substantially outperforms random guessing of the classes. Hyper-parameters including batch size, learning rate, and dropout were determined based on a small grid search."
        ]
    },
    "S4.T2.1.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T2.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.2.1\">BertScore F1</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T2.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.3.1\">ROUGE-L F1</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.1.1\">Falcon-40B*</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.2.1.2\">0.91</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.2.1.3\">0.40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.3.2.1\">Mistral-Large-2</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.3.2.2\">0.91</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.3.2.3\">0.38</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.4.3.1\">Mixtral-8x7B-Instruct</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.4.3.2\">0.91</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.4.3.3\">0.36</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.5.4.1\">Granite-13B-Chat-v2</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.5.4.2\">0.89</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.5.4.3\">0.28</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 2: Comparison of BertScore F1 and ROUGE-L F1 for different models on query generation task. BertScore is based on roberta-large embeddings. \n*Falcon-40B scores are not comparable to the other models because the prompt used was different, and it was used to create the initial questions that were validated or edited by SMEs to create the ground truth.",
        "footnotes": [],
        "references": [
            "In order to create a concise query that could be used by the retriever, we generated a single sentence question based on the case subject and description. Our experiments over various open-source generative models (Table 2) and model availability in the client’s services led us to choose Mixtral-8x7b-Instruct as the model for query generation which reliably reproduced the ground truth queries despite being a relatively small model with no domain knowledge. Note that the results are skewed for Falcon-40B (Almazrouei et al. 2023) as Falcon-40B generated the first pass of silver ground truth queries that were then edited by subject matter experts."
        ]
    },
    "S4.T3.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.1.1.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.2.1\">0 - Completely</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.3.1\">1 - somewhat</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.4.1\">2 - Solution</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.2.2.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.1.1\">Rating</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.2.2.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.2.1\">irrelevant</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.2.2.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.3.1\">relevant/helpful</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.2.2.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.4.1\">in link</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.3.3.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Product A</th>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.1.3.3.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.1.3.3.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.1.3.3.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.4.4.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">SME</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">58% (11)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">0% (0)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">42% (8)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.5.5.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Tool</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">26% (5)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">5% (1)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">68% (13)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.6.6.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Product B</th>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.1.6.6.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.1.6.6.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.1.6.6.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.7.7.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">SME</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.7.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">20% (12)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.7.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">48% (28)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.7.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">28% (17)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.8.8.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Tool</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.8.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">48% (29)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.8.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">37% (22)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.8.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">12% (7)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 3: AB testing human evaluation of retrieved links",
        "footnotes": [],
        "references": [
            "We also conducted an AB test in which support agents of two products were provided with a link retrieved by the tool and a link provided by a subject matter expert. The source of the link was randomized as Source A or Source B so that, for example, Source A could be either our tool or an SME for any given case. The support agents were asked to rate each link as shown in Table 3 and to pick the better of the two solutions."
        ]
    },
    "S4.T4.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S4.T4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.2.1\">BertScore (roberta-large) F1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.3.1\">BertScore (deberta-xlarge-mnli) F1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.4.1\">ROUGE-L F1</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T4.1.2.1.1\">GPT-4o (2024-08-06)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.2.1.2\">0.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.2.1.3\">0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.2.1.4\">0.34</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.1.3.2.1\">Mistral-Large-2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.3.2.2\">0.86</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.3.2.3\">0.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.3.2.4\">0.37</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.1.4.3.1\">Mixtral-8x7B-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.4.3.2\">0.87</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.4.3.3\">0.64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.4.3.4\">0.41</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.1.5.4.1\">Granite-13B-Chat-v2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.5.4.2\">0.86</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.5.4.3\">0.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.5.4.4\">0.32</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 4: Comparison of BertScore F1 and ROUGE-L F1 for different models performing the answer generation task. BertScore based on roberta-large embeddings",
        "footnotes": [],
        "references": [
            "To evaluate the answers, we used the subject matter expert’s annotated ground truth answers and ground truth documents verified to contain the answer to the question. We compared the answers generated by the answer extractor using the ground truth document to the ground truth answer using BertScore (Zhang et al. 2020) and ROUGE-L F1. We evaluated different models and prompts to find the optimal combination and present the results of the models assessed in Table 4. While BertScore (roberta-large) F1 is rather low (in practice it ranges between 0.85-0.95), ROUGE-L F1, traditionally a rather strict metric, shows promising results for Mixtral-8x7b-Instruct with a score of 0.41. Mixtral-8x7b-Instruct’s outperforms of GPT-4o, included as a baseline for larger models, in all three metrics, despite having substantially less parameters. Likewise, Granite-13B-Chat-v2 is not far behind GPT-4o despite its merely 13 billion parameters compared to GPT-4o’s rumored hundreds of billions or even trillions of parameters. This suggests that the RAG approach of smaller models leveraging retrieved context is a viable solution for IT incident resolution recommendation systems.",
            "The major bottleneck in RAG systems is the retrieval component. As shown in Table 4, when given the correct context, LLMs can typically generate responses that match the ground truth answers. However, we cannot expect to generate the correct answer if given the wrong contexts which happens for around 60% of the cases (Figure 2). For comparison, Google search limited to the corresponding domains indexed by the Milvus database performed worse at 30% R@3 compared to our method at 43% R@3 (See Figure 2). This implies, as other researchers have suggested, that the retrieval component in RAG is not a solved problem by any means. (Petroni et al. 2024; Cuconasu et al. 2024)"
        ]
    },
    "S4.T5.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S4.T5.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T5.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.2.1\">Score</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T5.1.2.1.1\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.1.2.1.2\">0.68</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.1.3.2.1\">Llama-3.1-8b-Instruct</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T5.1.3.2.2\">0.70</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.1.4.3.1\">InstructLab-IT</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T5.1.4.3.2\">0.76</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 5: Comparison of analytic rubric scores for different models on answer generation task.",
        "footnotes": [],
        "references": [
            "In Table 5, we present the final score for each model as the average of human annotators’ scores across 40 question-answer pairs of which InstructLab-IT emerged as the best model. While Llama-3.1-8b-Instruct performed slightly better than GPT-4o, the improvement in results with InstructLab-IT was very noticeable over both models. This is especially significant considering the model sizes: GPT-4o (over 1 trillion parameters and 1.5 TB), Llama-3.1-8b-Instruct (8 billion parameters and 16 GB), and InstructLab-IT (7 billion parameters and 28 GB). These results signal that a smaller, domain-specific model tuned for a specific set of use cases may better meet client requirements."
        ]
    },
    "S6.T6.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S6.T6.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.1.1.1.1\">Product</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S6.T6.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.1.1.2.1\">Classifier</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S6.T6.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.1.1.3.1\">Question</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S6.T6.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.1.1.4.1\">Link</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T6.1.2.1.1\">Product 1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.1.2.1.2\">0.80 (20)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.1.2.1.3\">0.75 (16)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.1.2.1.4\">0.50 (16)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.1.3.2.1\">Product 2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.3.2.2\">0.50 (20)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.3.2.3\">0.50 (10)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.3.2.4\">0.70 (10)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.1.4.3.1\">Product 3</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.4.3.2\">0.15 (20)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.4.3.3\">1.00 (3)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.4.3.4\">1.00 (3)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.1.5.4.1\">Product 4</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.5.4.2\">0.65 (20)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.5.4.3\">0.85 (13)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.5.4.4\">0.69 (13)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.1.6.5.1\">Product 5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.6.5.2\">0.65 (20)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.6.5.3\">0.54 (13)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.6.5.4\">0.69 (13)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.1.7.6.1\">Product 6</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.7.6.2\">0.25 (20)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.7.6.3\">1.00 (4)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.7.6.4\">1.00 (4)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.8.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T6.1.8.7.1\">Total</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.1.8.7.2\">0.50 (120)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.1.8.7.3\">0.72 (59)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.1.8.7.4\">0.67 (59)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 6: Inter-Annotator Agreement: Proportion of labels that 3 annotators agreed on. Total N in parenthesis. For question and link labels, proportions only calculated based on cases for which all 3 annotators labeled as single turn and evaluated quality of corresponding question and link.",
        "footnotes": [],
        "references": [
            "Three SMEs labeled a subset of twenty cases to determine inter-annotator agreement. The results in Table 6 show that labeling cases as single vs. multi-turn is not a trivial task and for most products, SMEs disagreed widely. Of the cases in which all three annotators agreed to be single-turn, agreement on the question and link quality was better but still raises questions about the validity of the training and evaluation data. In particular, the low agreement of the provided links can be explained by the fact that more than one link can potentially solve the same question and so neither annotator is necessarily wrong. This suggests that for ground truth data, we should consider a list of correct links instead of a single ground truth link for each question. The low agreement of single-turn vs. multi-turn labels also potentially explains the lower performance of the classifier model if the model is attempting to learn from potentially conflicting information."
        ]
    }
}