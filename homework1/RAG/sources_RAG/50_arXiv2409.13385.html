<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey</title>
<!--Generated on Wed Oct  2 14:30:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.13385v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S1" title="In Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2" title="In Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1" title="In 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Semantic Compression</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1.SSS1" title="In 2.1 Semantic Compression â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Context Distillation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1.SSS2" title="In 2.1 Semantic Compression â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1.SSS3" title="In 2.1 Semantic Compression â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Efficient Attention Operations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1.SSS4" title="In 2.1 Semantic Compression â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.4 </span>Extrapolation and Interpolation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1.SSS5" title="In 2.1 Semantic Compression â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.5 </span>Context Window Extension</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS2" title="In 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Pre-Trained Language Models (PLMs)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS2.SSS1" title="In 2.2 Pre-Trained Language Models (PLMs) â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>AutoCompressors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS2.SSS2" title="In 2.2 Pre-Trained Language Models (PLMs) â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>LongNET</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS2.SSS3" title="In 2.2 Pre-Trained Language Models (PLMs) â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>In-Context Auto-Encoders</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS2.SSS4" title="In 2.2 Pre-Trained Language Models (PLMs) â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.4 </span>RECOMP</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS3" title="In 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Retrievers</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS3.SSS1" title="In 2.3 Retrievers â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>LLMChainExtractor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS3.SSS2" title="In 2.3 Retrievers â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>EmbeddingsFilter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS3.SSS3" title="In 2.3 Retrievers â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.3 </span>DocumentCompressorPipeline</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3" title="In Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Metrics and Benchmarks</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1" title="In 3 Metrics and Benchmarks â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS1" title="In 3.1 Metrics â€£ 3 Metrics and Benchmarks â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Compression Ratio</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS2" title="In 3.1 Metrics â€£ 3 Metrics and Benchmarks â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Inference Time</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS3" title="In 3.1 Metrics â€£ 3 Metrics and Benchmarks â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Context Relevance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS4" title="In 3.1 Metrics â€£ 3 Metrics and Benchmarks â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.4 </span>Groundedness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS5" title="In 3.1 Metrics â€£ 3 Metrics and Benchmarks â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.5 </span>Answer Relevance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS6" title="In 3.1 Metrics â€£ 3 Metrics and Benchmarks â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.6 </span>Others</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS2" title="In 3 Metrics and Benchmarks â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Benchmarks and Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS2.SSS1" title="In 3.2 Benchmarks and Datasets â€£ 3 Metrics and Benchmarks â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Common Benchmarks and Datasets</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S4" title="In Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Challenges and Future Directions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S4.SS1" title="In 4 Challenges and Future Directions â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>More advanced Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S4.SS2" title="In 4 Challenges and Future Directions â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Performance-Size Trade-offs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S4.SS3" title="In 4 Challenges and Future Directions â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Dynamic Contextual Compression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S4.SS4" title="In 4 Challenges and Future Directions â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Explainability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S5" title="In Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sourav Verma 
<br class="ltx_break"/>IBM Watsonx Client Engineering, India 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">sourav.verma@ibm.com</span> | <span class="ltx_text ltx_font_typewriter" id="id2.2.id2">souravv.vermaa@gmail.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">Large Language Models (LLMs) showcase remarkable abilities, yet they struggle with limitations such as hallucinations, outdated knowledge, opacity, and inexplicable reasoning. To address these challenges, Retrieval-Augmented Generation (RAG) has proven to be a viable solution, leveraging external databases to improve the consistency and coherence of generated content, especially valuable for complex, knowledge-rich tasks, and facilitates continuous improvement by leveraging domain-specific insights. By combining the intrinsic knowledge of LLMs with the vast, dynamic repositories of external databases, RAG achieves a synergistic effect.
However, RAG is not without its limitations, including a limited context window, irrelevant information, and the high processing overhead for extensive contextual data. In this comprehensive work, we explore the evolution of Contextual Compression paradigms, providing an in-depth examination of the field. Finally, we outline the current challenges and suggest potential research and development directions, paving the way for future advancements in this area.Â <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Resources are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/SrGrace/Contextual-Compression" title="">https://github.com/SrGrace/Contextual-Compression</a></span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Sourav Verma</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">IBM Watsonx Client Engineering, India</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.3.1.1">sourav.verma@ibm.com</span> | <span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.3.1.2">souravv.vermaa@gmail.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The pioneering accomplishments of large language models (LLMs) have galvanized research initiatives across both industrial and academic spheres. These LLMs showcase their capacity to converse with humans in a natural and articulate manner, excelling across various tasks such as document summarization, Q&amp;A systems, conversational AI, and coding assistants. Despite their advancements, LLMs continue to struggle with tasks that require specialized knowledge or domain-specific expertise. <cite class="ltx_cite ltx_citemacro_cite">Kandpal etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib29" title="">2023</a>)</cite>. Notably, they may produce â€œhallucinationsâ€ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib64" title="">2023</a>)</cite> when confronted with out-of-scope queries or requests that necessitate up-to-date knowledge. To address these challenges, Retrieval-Augmented Generation (RAG) leverages external knowledge bases to retrieve relevant document snippets, utilizing semantic similarity metrics to identify the most pertinent information. By tapping into external knowledge sources, RAG successfully alleviates the issue of generating inaccurate content, thereby increasing the reliability of LLMs and paving the way for their widespread adoption in real-world applications.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, RAG also has its challenges. One issue is that when retrieving relevant documents, the important information may be buried in a large amount of irrelevant text, leading to inefficient and poor responses. Another challenge is that current language models have a limited input length, which causes their performance to decline when processing lengthy documents, such as academic articles, research papers, or literary works. This constraint has fueled research into developing methods to increase the input length while maintaining the modelâ€™s accuracy and efficiency.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This paper aims to shed light on the latest advancements in contextual compression methods, with a focus on their application in retrieval-based systems. Our research involves a comprehensive review of methodologies, metrics, and benchmarks, which we systematically categorize into a novel taxonomy. Our taxonomy, as shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>, presents a structured and comprehensive framework for categorizing and analyzing Contextual Compression techniques for LLMs. Our investigation involves a comprehensive analysis of established techniques, such as semantic compression, in-context auto-encoder compressors, and auto-compressors, among others. Furthermore, our research highlights the ongoing challenges in this field and provides a roadmap for future investigations. We emphasize the need for collective efforts to create a sustainable and environmentally responsible future for LLMs.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.F1.1" style="width:433.6pt;height:273.2pt;vertical-align:-269.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-143.5pt,1.4pt) scale(0.601691617452058,0.601691617452058) ;"><span class="ltx_ERROR undefined" id="S1.F1.1.1">{forest}</span>
<p class="ltx_p" id="S1.F1.1.2">forked edges,
for tree=
grow=east,reversed=true,anchor=base west,parent anchor=east,
child anchor=west,base=left,font=,rectangle,
draw=hidden-draw,rounded corners,align=left,minimum width=4em,
edge+=darkgray, line width=1pt,s sep=3pt,inner xsep=2pt,
inner ysep=3pt,ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
,
where level=1text width=6em,font=,
where level=2text width=9em,font=,
where level=3text width=6.6em,font=,
[Contextual Compression for Large Language Models, ver
[Semantic 
<br class="ltx_break"/>Compression
[
Context Distillation
[
Learning by distilling contextÂ <cite class="ltx_cite ltx_citemacro_cite">Snell etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib54" title="">2022</a>)</cite>, GistingÂ <cite class="ltx_cite ltx_citemacro_cite">Mu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib41" title="">2024</a>)</cite>, leaf, text width=30em
]
]
[
Concept Distillation
[
Compressing Long Context for Enhancing RAG with AMR-based Concept DistillationÂ <cite class="ltx_cite ltx_citemacro_cite">Shi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib51" title="">2024</a>)</cite>, leaf, text width=30em
]
]
[
Prompting
[
Soft Prompts
[
The Power of Scale for PEPTÂ <cite class="ltx_cite ltx_citemacro_cite">Lester etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib31" title="">2021</a>)</cite>, 
<br class="ltx_break"/>OptiPromptÂ <cite class="ltx_cite ltx_citemacro_cite">Zhong etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib65" title="">2021</a>)</cite>, RecurrentgptÂ <cite class="ltx_cite ltx_citemacro_cite">Zhou etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib67" title="">2023</a>)</cite>, 
<br class="ltx_break"/>P-TuningÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib36" title="">2022</a>)</cite>, leaf, text width=21.8em
]
]
[
Prompt Compression
[
Prompt compression and contrastive conditioningÂ <cite class="ltx_cite ltx_citemacro_cite">Wingate etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib61" title="">2022</a>)</cite>, leaf, text width=21.8em
]
]
[
Task-Agnostic 
<br class="ltx_break"/>Prompt Compression
[
LLMLinguaÂ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib28" title="">2023b</a>)</cite>, LongLLMLinguaÂ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib27" title="">2023a</a>)</cite>, 
<br class="ltx_break"/>LLMLingua-2Â <cite class="ltx_cite ltx_citemacro_cite">Pan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib43" title="">2024</a>)</cite>, leaf, text width=21.8em
]
]
]
[
Efficient Attention 
<br class="ltx_break"/>Operations
[
Transformer-XLÂ <cite class="ltx_cite ltx_citemacro_cite">Dai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib14" title="">2019</a>)</cite>, LongformerÂ <cite class="ltx_cite ltx_citemacro_cite">Beltagy etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib5" title="">2020</a>)</cite>, FlashAttentionÂ <cite class="ltx_cite ltx_citemacro_cite">Dao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib15" title="">2022</a>)</cite>, 
<br class="ltx_break"/>LongLoRAÂ <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib12" title="">2023b</a>)</cite>, leaf, text width=30em
]
]
[
Extrapolation and 
<br class="ltx_break"/>Interpolation
[
Exploring length generalization in LLMsÂ <cite class="ltx_cite ltx_citemacro_cite">Anil etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib1" title="">2022</a>)</cite>, Positional Interpolation(PI)Â <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib10" title="">2023a</a>)</cite>, 
<br class="ltx_break"/>YaRNÂ <cite class="ltx_cite ltx_citemacro_cite">Peng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib44" title="">2023</a>)</cite>, leaf, text width=30em ]
]
[
Context Window 
<br class="ltx_break"/>Extension
[
Extending context window of LLMs via semantic compressionÂ <cite class="ltx_cite ltx_citemacro_cite">Fei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib20" title="">2023</a>)</cite>, leaf, text width=30em
]
]
]
[
Pre-Trained 
<br class="ltx_break"/>Language Models
[
AutoCompressors
[
Adapting LMs to compress contextsÂ <cite class="ltx_cite ltx_citemacro_cite">Chevalier etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib13" title="">2023</a>)</cite>, leaf, text width=30em
]
]
[
LongNET
[
LongNET: Scaling transformers to 1B tokensÂ <cite class="ltx_cite ltx_citemacro_cite">Ding etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib17" title="">2023</a>)</cite>, leaf, text width=30em
]
]
[
In-Context Auto-Encoders
[
In-context autoencoder for context compression in a LLMÂ <cite class="ltx_cite ltx_citemacro_cite">Ge etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib23" title="">2023</a>)</cite>, leaf, text width=30em
]
]
[
RECOMP
[
Retrieve-Compress-PrependÂ <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib62" title="">2024</a>)</cite>, leaf, text width=30em
]
]
]
[
Retrievers
[
LLMChainExtractor
[
LangChainâ€™s MethodÂ <cite class="ltx_cite ltx_citemacro_cite">Chase (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib8" title="">2017-</a>)</cite>, leaf, text width=30em
]
]
[
EmbeddingsFilter
[
LangChainâ€™s MethodÂ <cite class="ltx_cite ltx_citemacro_cite">Chase (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib8" title="">2017-</a>)</cite>, leaf, text width=30em
]
]
[
DocumentCompressorPipeline
[
LangChainâ€™s MethodÂ <cite class="ltx_cite ltx_citemacro_cite">Chase (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib8" title="">2017-</a>)</cite>, leaf, text width=30em
]
]
]
]</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Taxonomy of Contextual Compression Methods for Large Language Models.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Semantic Compression</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Semantic compression is a technique that helps identify common patterns of thought in a specific context by generalizing terms. It uses a "domain frequency dictionary" to establish the context and disambiguate multiple possible meanings of words. This approach, based on semantic networks, offers improvements over existing natural language processing techniques.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Semantic compression reduces the number of terms in a text document by replacing less frequent terms with more general terms (their hypernyms) using a semantic network and term frequency data. This compression minimizes information loss and enables efficient processing, especially in tasks involving vector space models <cite class="ltx_cite ltx_citemacro_cite">Baeza-Yates etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib4" title="">1999</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Erk and PadÃ³ (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib18" title="">2008</a>)</cite>. It also helps address linguistic <cite class="ltx_cite ltx_citemacro_cite">Sinha and Mihalcea (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib52" title="">2007</a>)</cite> challenges like polysemy and synonymy <cite class="ltx_cite ltx_citemacro_cite">Krovetz and Croft (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib30" title="">1992</a>)</cite> by replacing multiple rare terms with a single, more general concept. By using statistical analysis and frequency dictionaries, semantic compression can handle polysemic concepts more effectively and with lower error rates than other techniques.
These efforts can be summarized into five approaches: <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">Context Distillation</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.2">Prompting</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.3">Efficient Attention Operations</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.4">Extrapolation and Interpolation</span>, and <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.5">Context Window Extension</span>.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Context Distillation</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Recent studies have demonstrated that augmenting language models (LMs) with contextual information, such as task descriptions, illustrative examples, and explanatory notes <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib11" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Scheurer etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib50" title="">2022</a>)</cite>, can substantially enhance their performance capabilities. This approach can even facilitate zero-shot learning <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib58" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Victor etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib56" title="">2022</a>)</cite> and enable models to tackle complex tasks by generating sequential reasoning steps <cite class="ltx_cite ltx_citemacro_cite">Nye etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib42" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib59" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Zhou etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib66" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">While LMs perform better with context tokens, this advantage disappears when the tokens are removed. Additionally, processing context tokens requires extra computation, which can be a drawback. The context tokens can also be very long, and itâ€™s unclear how to handle them when they exceed the context window size. These limitations are similar to human cognitive limitations <cite class="ltx_cite ltx_citemacro_cite">Wason and Evans (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib57" title="">1974</a>)</cite>, such as struggling with complex tasks and having limited working memory <cite class="ltx_cite ltx_citemacro_cite">Baddeley (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib3" title="">1992</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1">Humans overcome challenges through practice, which allows them to "distill" knowledge into habits and muscle memory. For example, learning to type a phone number becomes automatic with repetition, freeing up conscious reasoning for more complex tasks <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>procedural learning vs. declarative learning - <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Procedural_knowledge" title="">https://en.wikipedia.org/wiki/Procedural_knowledge</a></span></span></span>. This process is essential for building skills and knowledge, enabling us to tackle increasingly intricate challenges.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p4">
<p class="ltx_p" id="S2.SS1.SSS1.p4.1">Researchers in NLP <cite class="ltx_cite ltx_citemacro_cite">Askell etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib2" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Snell etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib54" title="">2022</a>)</cite> are exploring techniques to fine-tune language models, such as context distillation and "Gisting". Context distillation involves generating "practice" questions, having the model reason step-by-step, and fine-tuning it to predict answers from simpler prompts. This helps the model internalize skills, like step-by-step addition (ref FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F2" title="Figure 2 â€£ 2.1.1 Context Distillation â€£ 2.1 Semantic Compression â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>). "Gisting" <cite class="ltx_cite ltx_citemacro_cite">Mu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib41" title="">2024</a>)</cite> compresses instructions into concise key-value attention prefixes, saving computational resources and generalizing well to new tasks. As depicted in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F3" title="Figure 3 â€£ 2.1.1 Context Distillation â€£ 2.1 Semantic Compression â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">3</span></a>, the approach involves learning a gist model by incorporating gist tokens during instruction tuning, enabling the model to handle prompt compression and instruction following simultaneously.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="268" id="S2.F2.g1" src="extracted/5896160/figures/context_distill.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Internalization of step-by-step reasoning via context distillation <cite class="ltx_cite ltx_citemacro_cite">Snell etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib54" title="">2022</a>)</cite></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="S2.F3.g1" src="extracted/5896160/figures/gisting.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Gisting - Each vertical rectangle here represents a stack of Transformer activations <cite class="ltx_cite ltx_citemacro_cite">Mu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib41" title="">2024</a>)</cite>
</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Prompting</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p1.1.1">Soft Prompts -</span> As depicted in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F4" title="Figure 4 â€£ 2.1.2 Prompting â€£ 2.1 Semantic Compression â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">4</span></a>, soft prompt tuning enables the adaptation of pre-trained Transformers without modifying their underlying parameters, as demonstrated in recent studies <cite class="ltx_cite ltx_citemacro_cite">Lester etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib31" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Zhong etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib65" title="">2021</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib36" title="">2022</a>)</cite>. It entails adding novel embeddings to the input sequence and fine-tuning only these new parameters while keeping the remainder of the modelâ€™s architecture frozen. This approach is categorized as a parameter-efficient fine-tuning method (PEFT) <cite class="ltx_cite ltx_citemacro_cite">Lialin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib33" title="">2023</a>)</cite>, and bears resemblance to prefix tuning, which prepends task-specific vectors to the attention states instead of the input sequence <cite class="ltx_cite ltx_citemacro_cite">Li and Liang (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib32" title="">2021</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="313" id="S2.F4.g1" src="extracted/5896160/figures/soft_prompting.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>From 11 billion for a tuned model to just 20,480 for a tuned prompt, a reduction of over 5 orders of magnitude <cite class="ltx_cite ltx_citemacro_cite">Lester etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib31" title="">2021</a>)</cite></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.8"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p2.8.1">Prompt Compression -</span> In their work, <cite class="ltx_cite ltx_citemacro_cite">Wingate etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib61" title="">2022</a>)</cite> hypothesize using a soft prompt <math alttext="sp" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.1.m1.1"><semantics id="S2.SS1.SSS2.p2.1.m1.1a"><mrow id="S2.SS1.SSS2.p2.1.m1.1.1" xref="S2.SS1.SSS2.p2.1.m1.1.1.cmml"><mi id="S2.SS1.SSS2.p2.1.m1.1.1.2" xref="S2.SS1.SSS2.p2.1.m1.1.1.2.cmml">s</mi><mo id="S2.SS1.SSS2.p2.1.m1.1.1.1" xref="S2.SS1.SSS2.p2.1.m1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.1.m1.1.1.3" xref="S2.SS1.SSS2.p2.1.m1.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.1.m1.1b"><apply id="S2.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1"><times id="S2.SS1.SSS2.p2.1.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1.1"></times><ci id="S2.SS1.SSS2.p2.1.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1.2">ğ‘ </ci><ci id="S2.SS1.SSS2.p2.1.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.1.m1.1c">sp</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.1.m1.1d">italic_s italic_p</annotation></semantics></math> to compress information from a context <math alttext="ctx" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.2.m2.1"><semantics id="S2.SS1.SSS2.p2.2.m2.1a"><mrow id="S2.SS1.SSS2.p2.2.m2.1.1" xref="S2.SS1.SSS2.p2.2.m2.1.1.cmml"><mi id="S2.SS1.SSS2.p2.2.m2.1.1.2" xref="S2.SS1.SSS2.p2.2.m2.1.1.2.cmml">c</mi><mo id="S2.SS1.SSS2.p2.2.m2.1.1.1" xref="S2.SS1.SSS2.p2.2.m2.1.1.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.2.m2.1.1.3" xref="S2.SS1.SSS2.p2.2.m2.1.1.3.cmml">t</mi><mo id="S2.SS1.SSS2.p2.2.m2.1.1.1a" xref="S2.SS1.SSS2.p2.2.m2.1.1.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.2.m2.1.1.4" xref="S2.SS1.SSS2.p2.2.m2.1.1.4.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.2.m2.1b"><apply id="S2.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1"><times id="S2.SS1.SSS2.p2.2.m2.1.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1.1"></times><ci id="S2.SS1.SSS2.p2.2.m2.1.1.2.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1.2">ğ‘</ci><ci id="S2.SS1.SSS2.p2.2.m2.1.1.3.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1.3">ğ‘¡</ci><ci id="S2.SS1.SSS2.p2.2.m2.1.1.4.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1.4">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.2.m2.1c">ctx</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.2.m2.1d">italic_c italic_t italic_x</annotation></semantics></math>. They use a pre-trained LM <math alttext="p_{\text{LM}}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.3.m3.1"><semantics id="S2.SS1.SSS2.p2.3.m3.1a"><msub id="S2.SS1.SSS2.p2.3.m3.1.1" xref="S2.SS1.SSS2.p2.3.m3.1.1.cmml"><mi id="S2.SS1.SSS2.p2.3.m3.1.1.2" xref="S2.SS1.SSS2.p2.3.m3.1.1.2.cmml">p</mi><mtext id="S2.SS1.SSS2.p2.3.m3.1.1.3" xref="S2.SS1.SSS2.p2.3.m3.1.1.3a.cmml">LM</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.3.m3.1b"><apply id="S2.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.3.m3.1.1.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p2.3.m3.1.1.2.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1.2">ğ‘</ci><ci id="S2.SS1.SSS2.p2.3.m3.1.1.3a.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1.3"><mtext id="S2.SS1.SSS2.p2.3.m3.1.1.3.cmml" mathsize="70%" xref="S2.SS1.SSS2.p2.3.m3.1.1.3">LM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.3.m3.1c">p_{\text{LM}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.3.m3.1d">italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT</annotation></semantics></math> to generate continuations <math alttext="cty\sim p_{\text{LM}}(\cdot\mid ctx)" class="ltx_math_unparsed" display="inline" id="S2.SS1.SSS2.p2.4.m4.1"><semantics id="S2.SS1.SSS2.p2.4.m4.1a"><mrow id="S2.SS1.SSS2.p2.4.m4.1b"><mi id="S2.SS1.SSS2.p2.4.m4.1.1">c</mi><mi id="S2.SS1.SSS2.p2.4.m4.1.2">t</mi><mi id="S2.SS1.SSS2.p2.4.m4.1.3">y</mi><mo id="S2.SS1.SSS2.p2.4.m4.1.4">âˆ¼</mo><msub id="S2.SS1.SSS2.p2.4.m4.1.5"><mi id="S2.SS1.SSS2.p2.4.m4.1.5.2">p</mi><mtext id="S2.SS1.SSS2.p2.4.m4.1.5.3">LM</mtext></msub><mrow id="S2.SS1.SSS2.p2.4.m4.1.6"><mo id="S2.SS1.SSS2.p2.4.m4.1.6.1" stretchy="false">(</mo><mo id="S2.SS1.SSS2.p2.4.m4.1.6.2" lspace="0em" rspace="0em">â‹…</mo><mo id="S2.SS1.SSS2.p2.4.m4.1.6.3" lspace="0em" rspace="0.167em">âˆ£</mo><mi id="S2.SS1.SSS2.p2.4.m4.1.6.4">c</mi><mi id="S2.SS1.SSS2.p2.4.m4.1.6.5">t</mi><mi id="S2.SS1.SSS2.p2.4.m4.1.6.6">x</mi><mo id="S2.SS1.SSS2.p2.4.m4.1.6.7" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.4.m4.1c">cty\sim p_{\text{LM}}(\cdot\mid ctx)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.4.m4.1d">italic_c italic_t italic_y âˆ¼ italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ( â‹… âˆ£ italic_c italic_t italic_x )</annotation></semantics></math> based on the context, and then calibrate the modelâ€™s outputs with the soft prompt <math alttext="sf" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.5.m5.1"><semantics id="S2.SS1.SSS2.p2.5.m5.1a"><mrow id="S2.SS1.SSS2.p2.5.m5.1.1" xref="S2.SS1.SSS2.p2.5.m5.1.1.cmml"><mi id="S2.SS1.SSS2.p2.5.m5.1.1.2" xref="S2.SS1.SSS2.p2.5.m5.1.1.2.cmml">s</mi><mo id="S2.SS1.SSS2.p2.5.m5.1.1.1" xref="S2.SS1.SSS2.p2.5.m5.1.1.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.5.m5.1.1.3" xref="S2.SS1.SSS2.p2.5.m5.1.1.3.cmml">f</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.5.m5.1b"><apply id="S2.SS1.SSS2.p2.5.m5.1.1.cmml" xref="S2.SS1.SSS2.p2.5.m5.1.1"><times id="S2.SS1.SSS2.p2.5.m5.1.1.1.cmml" xref="S2.SS1.SSS2.p2.5.m5.1.1.1"></times><ci id="S2.SS1.SSS2.p2.5.m5.1.1.2.cmml" xref="S2.SS1.SSS2.p2.5.m5.1.1.2">ğ‘ </ci><ci id="S2.SS1.SSS2.p2.5.m5.1.1.3.cmml" xref="S2.SS1.SSS2.p2.5.m5.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.5.m5.1c">sf</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.5.m5.1d">italic_s italic_f</annotation></semantics></math>, <math alttext="p_{\text{LM}}(cty\mid sf)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.6.m6.1"><semantics id="S2.SS1.SSS2.p2.6.m6.1a"><mrow id="S2.SS1.SSS2.p2.6.m6.1.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.cmml"><msub id="S2.SS1.SSS2.p2.6.m6.1.1.3" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.cmml"><mi id="S2.SS1.SSS2.p2.6.m6.1.1.3.2" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.2.cmml">p</mi><mtext id="S2.SS1.SSS2.p2.6.m6.1.1.3.3" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.3a.cmml">LM</mtext></msub><mo id="S2.SS1.SSS2.p2.6.m6.1.1.2" xref="S2.SS1.SSS2.p2.6.m6.1.1.2.cmml">â¢</mo><mrow id="S2.SS1.SSS2.p2.6.m6.1.1.1.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml"><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.2" stretchy="false" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml"><mrow id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.cmml"><mi id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.2" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.2.cmml">c</mi><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.3" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.3.cmml">t</mi><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1a" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.4" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.4.cmml">y</mi></mrow><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.1.cmml">âˆ£</mo><mrow id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.cmml"><mi id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.2" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.2.cmml">s</mi><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.3" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.3.cmml">f</mi></mrow></mrow><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.3" stretchy="false" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.6.m6.1b"><apply id="S2.SS1.SSS2.p2.6.m6.1.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1"><times id="S2.SS1.SSS2.p2.6.m6.1.1.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.2"></times><apply id="S2.SS1.SSS2.p2.6.m6.1.1.3.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.6.m6.1.1.3.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.3">subscript</csymbol><ci id="S2.SS1.SSS2.p2.6.m6.1.1.3.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.2">ğ‘</ci><ci id="S2.SS1.SSS2.p2.6.m6.1.1.3.3a.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.3"><mtext id="S2.SS1.SSS2.p2.6.m6.1.1.3.3.cmml" mathsize="70%" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.3">LM</mtext></ci></apply><apply id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.1">conditional</csymbol><apply id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2"><times id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1"></times><ci id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.2">ğ‘</ci><ci id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.3.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.3">ğ‘¡</ci><ci id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.4.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.4">ğ‘¦</ci></apply><apply id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3"><times id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.1"></times><ci id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.2">ğ‘ </ci><ci id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.3.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.3">ğ‘“</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.6.m6.1c">p_{\text{LM}}(cty\mid sf)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.6.m6.1d">italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ( italic_c italic_t italic_y âˆ£ italic_s italic_f )</annotation></semantics></math> to the outputs based on the context <math alttext="ctx" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.7.m7.1"><semantics id="S2.SS1.SSS2.p2.7.m7.1a"><mrow id="S2.SS1.SSS2.p2.7.m7.1.1" xref="S2.SS1.SSS2.p2.7.m7.1.1.cmml"><mi id="S2.SS1.SSS2.p2.7.m7.1.1.2" xref="S2.SS1.SSS2.p2.7.m7.1.1.2.cmml">c</mi><mo id="S2.SS1.SSS2.p2.7.m7.1.1.1" xref="S2.SS1.SSS2.p2.7.m7.1.1.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.7.m7.1.1.3" xref="S2.SS1.SSS2.p2.7.m7.1.1.3.cmml">t</mi><mo id="S2.SS1.SSS2.p2.7.m7.1.1.1a" xref="S2.SS1.SSS2.p2.7.m7.1.1.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.7.m7.1.1.4" xref="S2.SS1.SSS2.p2.7.m7.1.1.4.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.7.m7.1b"><apply id="S2.SS1.SSS2.p2.7.m7.1.1.cmml" xref="S2.SS1.SSS2.p2.7.m7.1.1"><times id="S2.SS1.SSS2.p2.7.m7.1.1.1.cmml" xref="S2.SS1.SSS2.p2.7.m7.1.1.1"></times><ci id="S2.SS1.SSS2.p2.7.m7.1.1.2.cmml" xref="S2.SS1.SSS2.p2.7.m7.1.1.2">ğ‘</ci><ci id="S2.SS1.SSS2.p2.7.m7.1.1.3.cmml" xref="S2.SS1.SSS2.p2.7.m7.1.1.3">ğ‘¡</ci><ci id="S2.SS1.SSS2.p2.7.m7.1.1.4.cmml" xref="S2.SS1.SSS2.p2.7.m7.1.1.4">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.7.m7.1c">ctx</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.7.m7.1d">italic_c italic_t italic_x</annotation></semantics></math>, <math alttext="p_{\text{LM}}(cty\mid ctx)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.8.m8.1"><semantics id="S2.SS1.SSS2.p2.8.m8.1a"><mrow id="S2.SS1.SSS2.p2.8.m8.1.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.cmml"><msub id="S2.SS1.SSS2.p2.8.m8.1.1.3" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.cmml"><mi id="S2.SS1.SSS2.p2.8.m8.1.1.3.2" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.2.cmml">p</mi><mtext id="S2.SS1.SSS2.p2.8.m8.1.1.3.3" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.3a.cmml">LM</mtext></msub><mo id="S2.SS1.SSS2.p2.8.m8.1.1.2" xref="S2.SS1.SSS2.p2.8.m8.1.1.2.cmml">â¢</mo><mrow id="S2.SS1.SSS2.p2.8.m8.1.1.1.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.cmml"><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.2" stretchy="false" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.cmml"><mrow id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.cmml"><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.2" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.2.cmml">c</mi><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.3" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.3.cmml">t</mi><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1a" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.4" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.4.cmml">y</mi></mrow><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.1.cmml">âˆ£</mo><mrow id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.cmml"><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.2" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.2.cmml">c</mi><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.3" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.3.cmml">t</mi><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1a" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.4" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.4.cmml">x</mi></mrow></mrow><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.3" stretchy="false" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.8.m8.1b"><apply id="S2.SS1.SSS2.p2.8.m8.1.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1"><times id="S2.SS1.SSS2.p2.8.m8.1.1.2.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.2"></times><apply id="S2.SS1.SSS2.p2.8.m8.1.1.3.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.8.m8.1.1.3.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.3">subscript</csymbol><ci id="S2.SS1.SSS2.p2.8.m8.1.1.3.2.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.2">ğ‘</ci><ci id="S2.SS1.SSS2.p2.8.m8.1.1.3.3a.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.3"><mtext id="S2.SS1.SSS2.p2.8.m8.1.1.3.3.cmml" mathsize="70%" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.3">LM</mtext></ci></apply><apply id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.1">conditional</csymbol><apply id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2"><times id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1"></times><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.2.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.2">ğ‘</ci><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.3.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.3">ğ‘¡</ci><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.4.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.4">ğ‘¦</ci></apply><apply id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3"><times id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1"></times><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.2.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.2">ğ‘</ci><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.3.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.3">ğ‘¡</ci><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.4.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.4">ğ‘¥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.8.m8.1c">p_{\text{LM}}(cty\mid ctx)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.8.m8.1d">italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ( italic_c italic_t italic_y âˆ£ italic_c italic_t italic_x )</annotation></semantics></math>. They find that soft prompts effectively preserve abstract knowledge and improve guided output. Nevertheless, this method necessitates distinct optimization for each novel context, lacking the ability to leverage knowledge across analogous contexts.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p2.8.2">Task-Agnostic Prompt Compression -</span> Current methods for compressing natural language prompts remove tokens or lexical units based on information entropy from a language model like LlaMa-7B. However, using information entropy as a compression metric has two limitations: 1) it only considers unidirectional context, which may miss important information, and 2) it doesnâ€™t perfectly align with the goal of prompt compression.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p3">
<p class="ltx_p" id="S2.SS1.SSS2.p3.1">To address these issues, <cite class="ltx_cite ltx_citemacro_cite">Pan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib43" title="">2024</a>)</cite> propose a data distillation approach to compress prompts while retaining essential information. They introduce an extractive text compression dataset and frame prompt compression as a token classification problem (preserve or discard) (Refer to FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F5" title="Figure 5 â€£ 2.1.2 Prompting â€£ 2.1 Semantic Compression â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a>). The key benefits are as follows:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I1.i1.p1.1.1">Comprehensive Information Capture:</span> By leveraging a Transformer encoder, the method captures essential details from the full bidirectional context.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I1.i2.p1.1.1">Reduced Latency:</span> Smaller models explicitly learn the compression objective, leading to lower latency.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I1.i3.p1.1.1">Faithfulness:</span> The compressed prompt remains faithful to the original content.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="228" id="S2.F5.g1" src="extracted/5896160/figures/llmlingua-2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Overview of LLMLingua-2 <cite class="ltx_cite ltx_citemacro_cite">Pan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib43" title="">2024</a>)</cite></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Efficient Attention Operations</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">The self-attention mechanism in LLMs leads to an inference cost that scales quadratically with sequence length, prompting the development of various methods to alleviate this complexity. For example:</p>
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I2.i1.p1.1.1">Transformer-XL <cite class="ltx_cite ltx_citemacro_cite">Dai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib14" title="">2019</a>)</cite></span> - employs a recurrent architecture that operates on segments, paired with a novel positional encoding technique.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I2.i2.p1.1.1">Longformer <cite class="ltx_cite ltx_citemacro_cite">Beltagy etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib5" title="">2020</a>)</cite></span> - introduces sparse attention, scaling linearly with sequence length.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I2.i3.p1.1.1">FlashAttention <cite class="ltx_cite ltx_citemacro_cite">Dao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib15" title="">2022</a>)</cite></span> - uses chunking and re-computation to avoid quadratic attention complexity.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S2.SS1.SSS3.p1.2">However, these methods can be expensive to train and struggle with out-of-distribution content lengths <cite class="ltx_cite ltx_citemacro_cite">Ding etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib17" title="">2023</a>)</cite>. To address this, <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS3.p1.2.1">LongLoRA <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib12" title="">2023b</a>)</cite></span> provides a computationally efficient fine-tuning method with minimal resource requirements. For further insights, refer to the study by <cite class="ltx_cite ltx_citemacro_cite">Huang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib25" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Extrapolation and Interpolation</h4>
<div class="ltx_para" id="S2.SS1.SSS4.p1">
<p class="ltx_p" id="S2.SS1.SSS4.p1.1">In the field of NLP, researchers are investigating methods to extend the capabilities of existing language models, initially trained on brief texts, to process longer sequences during inference <cite class="ltx_cite ltx_citemacro_cite">Anil etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib1" title="">2022</a>)</cite>. One approach is to alter positional embeddings, which are typically designed for shorter contexts. The Rotary Position Embeddings (RoPE) from LLaMA is a key foundation for several studies in this area. For example:</p>
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I3.i1.p1.1.1">Position Interpolation (PI) <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib11" title="">2021</a>)</cite></span> applies a linear transformation to input positional indices.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I3.i2.p1.1.1">YaRN <cite class="ltx_cite ltx_citemacro_cite">Peng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib44" title="">2023</a>)</cite></span> leverages neural tangent kernel-inspired mechanisms to scale up the context window to 64,000 and 128,000 tokens.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.5 </span>Context Window Extension</h4>
<div class="ltx_para" id="S2.SS1.SSS5.p1">
<p class="ltx_p" id="S2.SS1.SSS5.p1.1">Researchers <cite class="ltx_cite ltx_citemacro_cite">Fei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib20" title="">2023</a>)</cite> propose a semantic compression method that distills long texts into concise forms, retaining their meaning and broadening the context window (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F6" title="Figure 6 â€£ 2.1.5 Context Window Extension â€£ 2.1 Semantic Compression â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">6</span></a>). This method occurs before inputting tokens into pre-trained language models and is customizable and optimized for specific tasks. It outperforms existing methods in various tasks, including question answering, summarization, and few-shot learning, without requiring additional parameter updates or memory consumption, making it computationally efficient.</p>
</div>
<figure class="ltx_figure" id="S2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="S2.F6.g1" src="extracted/5896160/figures/semantic_compression.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>1) clustering the input text into thematic groups, represented as a graph, to facilitate topic-based analysis, 2) tuning the thematic segments using pre-trained models to preserve crucial details, and 3) reassembling the refined chunks in their original order - reducing the text length by approximately 6-8 times. Additionally, other techniques like extrapolation and interpolation can be used to further extend the length <cite class="ltx_cite ltx_citemacro_cite">Fei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib20" title="">2023</a>)</cite></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pre-Trained Language Models (PLMs)</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The development of PLMs has revolutionized the field of NLP. The first generation of PLMs, such as Skip-Gram <cite class="ltx_cite ltx_citemacro_cite">Mikolov etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib40" title="">2013b</a>)</cite>, word2vec <cite class="ltx_cite ltx_citemacro_cite">Mikolov etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib39" title="">2013a</a>)</cite>, and GloVe <cite class="ltx_cite ltx_citemacro_cite">Pennington etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib45" title="">2014</a>)</cite>, used shallow neural networks <cite class="ltx_cite ltx_citemacro_cite">Qiu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib48" title="">2020</a>)</cite> to obtain word embeddings. The second generation, including CoVe <cite class="ltx_cite ltx_citemacro_cite">McCann etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib38" title="">2017</a>)</cite>, ELMo <cite class="ltx_cite ltx_citemacro_cite">Peters etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib46" title="">2018</a>)</cite>, BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib16" title="">2018</a>)</cite>, and GPT <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib49" title="">2018</a>)</cite>, focused on learning dynamic word embeddings using transformers. The pre-training and fine-tuning approach has achieved remarkable success in various NLP tasks. Moreover, recent breakthroughs in prompt learning <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib35" title="">2023a</a>)</cite> have empowered PLMs to accomplish few-shot or zero-shot learning with minimal labeled data. Notable examples of successful PLMs include ChatGPT, GPT-4, Gemini, Claude, LlaMA-3, Mixtral, etc.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>AutoCompressors</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The authors of <cite class="ltx_cite ltx_citemacro_cite">Chevalier etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib13" title="">2023</a>)</cite> propose teaching PLMs to compress text into summary vectors <cite class="ltx_cite ltx_citemacro_cite">Lester etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib31" title="">2021</a>)</cite>, which are significantly shorter than the original text (often 1-2 orders of magnitude shorter). These vectors have a two-pronged function: 1) they allow the LM to handle long documents by extending its context window with minimal computational overhead, and 2) they accelerate inference for pre-computed and cached text.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">AutoCompressors, proposed by <cite class="ltx_cite ltx_citemacro_cite">Chevalier etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib13" title="">2023</a>)</cite>, are trained To distill key information into summary vectors, generated sequentially from extended documents (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F7" title="Figure 7 â€£ 2.2.1 AutoCompressors â€£ 2.2 Pre-Trained Language Models (PLMs) â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">7</span></a>). The approach builds upon the Recurrent Memory Transformers (RMT) architecture <cite class="ltx_cite ltx_citemacro_cite">Bulatov etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib7" title="">2022</a>)</cite>, introducing summary accumulation and training with randomly segmented inputs. This enhances long-range information retention and facilitates reasoning across multiple passages. AutoCompressors can be seeded with PLMs and fine-tuned on long sequences. They improve perplexity for long documents and demonstrate robust compression capabilities across different domains, making them valuable for various downstream applications.</p>
</div>
<figure class="ltx_figure" id="S2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="436" id="S2.F7.g1" src="extracted/5896160/figures/AutoCompressors.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>AutoCompressors recursively generate summary vectors from long documents, using them as soft prompts for subsequent segments <cite class="ltx_cite ltx_citemacro_cite">Chevalier etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib13" title="">2023</a>)</cite></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>LongNET</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Overcoming sequence length limitations in language models has several advantages, including improved interactions with human language, better capture of complex causality and reasoning, and reduced catastrophic forgetting. However, scaling up sequence length poses a challenge in balancing computational complexity and model expressivity. RNN-style models and state space models <cite class="ltx_cite ltx_citemacro_cite">Gu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib24" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Smith etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib53" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Fu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib21" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Poli etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib47" title="">2023</a>)</cite> have been proposed, but they have limitations from the perspective of parallelization and model adaptability <cite class="ltx_cite ltx_citemacro_cite">Fathi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib19" title="">2023</a>)</cite>. An alternative approach is to reduce the complexity of Transformers <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib55" title="">2017</a>)</cite>, such as using sliding windows or convolution modules for attention, or sparse attention. LongNet <cite class="ltx_cite ltx_citemacro_cite">Ding etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib17" title="">2023</a>)</cite>, a novel approach, replaces the attention mechanism with "dilated attention", which achieves linear computational complexity and logarithmic dependency between tokens. This allows LongNet to efficiently scale sequence lengths to 1 billion tokens, overcoming the constraints of computation and memory.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>In-Context Auto-Encoders</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Modeling long-range dependencies is a hurdle for Transformer-based LMs <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib55" title="">2017</a>)</cite> due to their self-attention mechanism. Previous research by <cite class="ltx_cite ltx_citemacro_cite">Beltagy etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib5" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Bulatov etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib7" title="">2022</a>)</cite>, and Ding <cite class="ltx_cite ltx_citemacro_cite">Ding etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib17" title="">2023</a>)</cite> has attempted to cope with this issue through architectural innovations, but these approaches often struggle to maintain performance in long contexts, as underscored by <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib34" title="">2024</a>)</cite>. A novel approach, "context compression", is proposed by <cite class="ltx_cite ltx_citemacro_cite">Ge etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib23" title="">2023</a>)</cite>, which recognizes that an LLM can represent the same information in varying lengths. They introduce the In-context Autoencoder (ICAE), which compresses lengthy contexts into a fixed number of memory buffers using a learnable encoder and a fixed decoder (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F8" title="Figure 8 â€£ 2.2.3 In-Context Auto-Encoders â€£ 2.2 Pre-Trained Language Models (PLMs) â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">8</span></a>). The ICAE is pre-trained using auto-encoding and language modeling objectives and fine-tuned using instruction data. The approach achieves 4x context compression while maintaining effective conditioning for the target LLM, enabling faster and more memory-efficient inference.</p>
</div>
<figure class="ltx_figure" id="S2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="327" id="S2.F8.g1" src="extracted/5896160/figures/ICAE.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Condensing an extended context into a compact memory representation, which can be leveraged by the target LLM to respond to diverse prompts. <cite class="ltx_cite ltx_citemacro_cite">Ge etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib23" title="">2023</a>)</cite></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>RECOMP</h4>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">In their work, <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib62" title="">2024</a>)</cite> introduce RECOMP, an intermediary step for Retrieval-augmented Language Models (RALMs) <cite class="ltx_cite ltx_citemacro_cite">Izacard etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib26" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Borgeaud etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib6" title="">2022</a>)</cite>. RECOMP compresses retrieved documents into concise textual summaries before integrating them during inference, reducing computational costs and alleviating the burden on LMs to process lengthy documents. The aim is to produce summaries that balance brevity and fidelity to the original evidence documents, guiding the RALM to produce targeted outputs when the summary is used as a prefix to the input (illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F9" title="Figure 9 â€£ 2.2.4 RECOMP â€£ 2.2 Pre-Trained Language Models (PLMs) â€£ 2 Methods â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">9</span></a>). To achieve this, the authors train two types of compressors:</p>
<ol class="ltx_enumerate" id="S2.I4">
<li class="ltx_item" id="S2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I4.i1.p1">
<p class="ltx_p" id="S2.I4.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I4.i1.p1.1.1">Extractive Compressor:</span> This compressor filters out irrelevant sentences, retaining only the most pertinent ones from the retrieved document set.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I4.i2.p1">
<p class="ltx_p" id="S2.I4.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I4.i2.p1.1.1">Abstractive Compressor:</span> This compressor produces a summary by fusing information from multiple retrieved documents.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S2.SS2.SSS4.p1.2">Both compressors employ a multi-document query-based summarization approach <cite class="ltx_cite ltx_citemacro_cite">Xu and Lapata (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib63" title="">2020</a>)</cite>, summarizing evidence documents concerning the input query. The authors develop training strategies that maximize performance on the target task to guarantee accurate output. Contrastive learning is employed to train the extractive compressor enabling it to select key sentences effectively, while the abstractive compressor is distilled <cite class="ltx_cite ltx_citemacro_cite">West etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib60" title="">2021</a>)</cite> from a large language model (like GPT-3 or GPT-4), achieving strong summarization performance. This approach holds promise for enhancing the efficiency and efficacy of RALMs.</p>
</div>
<figure class="ltx_figure" id="S2.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="148" id="S2.F9.g1" src="extracted/5896160/figures/recomp.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>RECOMPâ€™s document compression technique generates a summary that serves as input to a language model, facilitating correct answer generation while minimizing encoding costs. <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib62" title="">2024</a>)</cite></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Retrievers</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The retriever <cite class="ltx_cite ltx_citemacro_cite">Chase (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib8" title="">2017-</a>)</cite> is an interface that processes an unstructured query and returns a curated list of documents in response. Contextual compression aims to address the challenges of retrieval by compressing the retrieved context to only include relevant information. In this context, "compressing" encompasses both condensing the content of individual documents and eliminating irrelevant documents altogether. The Contextual Compression Retriever uses a <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">base retriever</span> and a <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.2">Document Compressor</span> to process queries. The base retriever retrieves the initial documents, which are then passed through the Document Compressor to shorten the list of documents by either reducing the contents of individual documents or excluding entire documents altogether.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>LLMChainExtractor</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">In this approach, the base retriever is wrapped with a <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p1.1.1">ContextualCompressionRetriever</span>. Additionally, an <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p1.1.2">LLMChainExtractor</span> serves as the base compressor. The <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p1.1.3">LLMChainExtractor</span> iterates over the initially retrieved documents and extracts only the relevant content for the given query. It achieves this by making an additional LLM call for each retrieved document and summarizing the relevant information</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>EmbeddingsFilter</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">Making an additional LLM call for each retrieved document can be both costly and slow. However, the <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS2.p1.1.1">EmbeddingsFilter</span> offers a more economical and faster alternative. By embedding both the documents and the query, it selectively returns only those documents that exhibit sufficiently similar embeddings to the query. This approach optimizes retrieval efficiency while maintaining relevance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>DocumentCompressorPipeline</h4>
<div class="ltx_para" id="S2.SS3.SSS3.p1">
<p class="ltx_p" id="S2.SS3.SSS3.p1.1">The DocumentCompressorPipeline allows a seamless combination of multiple compressors in a sequence. Alongside these compressors, we can incorporate <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS3.p1.1.1">BaseDocumentTransformers</span> into our pipeline. Unlike contextual compressors, these transformers donâ€™t alter the content significantly but perform specific transformations on a set of documents. For instance, <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS3.p1.1.2">TextSplitters</span> can divide documents into smaller segments, while the <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS3.p1.1.3">EmbeddingsRedundantFilter</span> identifies and filters out redundant documents based on embedding similarity. This modular approach enhances flexibility and adaptability in document processing. e.g.</p>
<ul class="ltx_itemize" id="S2.I5">
<li class="ltx_item" id="S2.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S2.I5.i1.p1">
<p class="ltx_p" id="S2.I5.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I5.i1.p1.1.1">Splitter:</span> create small chunks</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S2.I5.i2.p1">
<p class="ltx_p" id="S2.I5.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I5.i2.p1.1.1">Redundant filter:</span> remove similar docs â€” embedded</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S2.I5.i3.p1">
<p class="ltx_p" id="S2.I5.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I5.i3.p1.1.1">Relevant filter:</span> relevant to query</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Metrics and Benchmarks</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Metrics</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Evaluating language model inference efficiency involves considering various metrics that capture different performance aspects, including accuracy, zero-shot capabilities, compression ratio, and inference time. Within the framework of RAG-based solutions, the "Triad of Metrics" <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>RAG Triad (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.F10" title="Figure 10 â€£ 3.1 Metrics â€£ 3 Metrics and Benchmarks â€£ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">10</span></a>): <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/" title="">https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/</a></span></span></span> - Groundedness, Context Relevance, and Answer Relevance - are also employed for evaluation. Achieving satisfactory performance across these metrics helps ensure that the language model application is reliable and free from hallucinations.</p>
</div>
<figure class="ltx_figure" id="S3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="S3.F10.g1" src="extracted/5896160/figures/rag_triad.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>RAG-Triad</figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Compression Ratio</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The compression ratio measures the reduction in size from the original uncompressed context to the compressed context. A higher compression ratio means that the compression is more efficient, as it achieves a greater reduction in size while preserving the contextâ€™s coherence.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Inference Time</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Inference time, also known as latency, measures how long it takes for a Large Language Model (LLM) to process input data and generate responses. This metric is crucial for real-world applications that require quick handling of user queries or processing of large data volumes in real-time.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Context Relevance</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">In RAG applications, the first step is retrieval, and itâ€™s crucial to ensure that the retrieved context chunks are relevant to the input query. Irrelevant information in the context can lead to hallucinations in the LLMâ€™s answer. To evaluate context relevance, the structure of the serialized record can be analyzed.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Groundedness</h4>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">After retrieving the context, an LLM transforms it into an answer. However, LLMs can sometimes stray from the facts and generate responses that are not entirely accurate. To ensure the groundedness of the application, the response can be broken down into individual claims and verified by searching for supporting evidence within the retrieved context.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.5 </span>Answer Relevance</h4>
<div class="ltx_para" id="S3.SS1.SSS5.p1">
<p class="ltx_p" id="S3.SS1.SSS5.p1.1">Furthermore, our response must still effectively address the original question. We can assess this by evaluating the relevance of the final response to the userâ€™s input.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.6 </span>Others</h4>
<div class="ltx_para" id="S3.SS1.SSS6.p1">
<p class="ltx_p" id="S3.SS1.SSS6.p1.1">RAG evaluation also encompasses four key abilities that reflect the modelâ€™s adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib9" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib37" title="">2023b</a>)</cite>. The modelâ€™s quality scores are heavily influenced by its ability to leverage these capabilities in diverse challenges and complex scenarios:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.1">Noise Robustness:</span> This metric gauges a modelâ€™s capacity to distinguish between relevant and irrelevant documents, even when the latter are tangentially related to the question.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">Negative Rejection:</span> The metric measures a modelâ€™s capacity to recognize when the retrieved documents are insufficient to answer a question, and to withhold a response accordingly.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i3.p1.1.1">Information Integration:</span> Information integration tests a modelâ€™s proficiency in combining relevant information from multiple documents to provide well-informed answers to challenging questions.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i4.p1.1.1">Counterfactual Robustness:</span> Counterfactual robustness measures a modelâ€™s skill in identifying and ignoring flawed or misleading information in documents, regardless of its awareness of potential errors.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS1.SSS6.p1.2">In brief, context relevance and noise robustness are crucial for evaluating the retrieval process, while answer groundedness, answer relevance, negative rejection, information integration, and counterfactual robustness are vital for assessing the quality of generated text.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Benchmarks and Datasets</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The primary objective of these benchmarks and datasets is to assess the trade-offs between compressed and uncompressed contexts in terms of effectiveness, efficiency, and accuracy, covering a broad range of NLP tasks and applications.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Common Benchmarks and Datasets</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">RAGâ€™s primary function revolves around answering questions, encompassing various formats such as single-hop and multi-hop queries, multiple-choice options, and domain-specific inquiries, as well as lengthy scenarios that leverage RAGâ€™s capabilities. Moreover, RAG is constantly evolving to tackle additional tasks, including extracting relevant information, generating conversational dialogue, and searching for code snippets, documentations and even interpreting them. For more details, refer to the study by <cite class="ltx_cite ltx_citemacro_cite">Gao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib22" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Challenges and Future Directions</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>More advanced Methods</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Research on contextual compression for LLMs is still in its early stages. While previous studies have shown compressed contexts, they still lag behind uncompressed contexts in terms of performance. By exploring more advanced compression methods tailored for LLMs, we can potentially bridge this performance gap and enhance the performance of uncompressed contexts.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Performance-Size Trade-offs</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Previous research highlights the importance of balancing LLM performance with context size, considering hardware limitations and practical constraints. Despite its significance, the theoretical and empirical foundations of this trade-off remain poorly understood. Future investigations should focus on conducting exhaustive examinations to drive the creation of sophisticated compression techniques that can meet the demands of increasingly complex data sets, enabling researchers to create tailored methods that effectively navigate the design space and optimize performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Dynamic Contextual Compression</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Contemporary compression approaches still utilize manual compressors, such as retrievers, which often require an empirical methodology driven by input data or task specifications. This can be a practical hindrance to adoption, especially in scenarios like context distillation, where finding suitable student templates within computational constraints can be time-consuming and require multiple trials.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Explainability</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Compressing pre-trained language models can make them hard to understand (lacking explainability). To fix this, using explainable compression methods can help make models more interpretable, easier to evaluate, and more reliable in real-life scenarios.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This in-depth analysis explores the domain of contextual compression techniques, with a focus on their application to LLMs. Our study encompasses a broad range of compression methods, evaluation metrics, and benchmark datasets, providing a comprehensive understanding of the field. By examining the complexities of contextual compression, we identify the key challenges and opportunities that arise in this area. As research in this field continues to advance, the development of specialized methodologies tailored to the needs of LLMs is crucial for unlocking their full potential across various domains. This survey aims to serve as a valuable resource, providing a detailed overview of the current landscape and encouraging further investigation into this vital topic.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">While this survey provides a comprehensive overview of contextual compression techniques for large language models, there are several limitations to acknowledge. Firstly, the field of contextual compression is rapidly evolving, and this survey may not capture the very latest advancements in the area. Additionally, the focus on large language models may not be representative of other types of language models or AI systems, which may have different compression requirements. Furthermore, the surveyâ€™s reliance on existing evaluation metrics and benchmark datasets may not fully capture the complexities and nuances of contextual compression. Moreover, the need for advanced methodologies specifically designed for LLMs highlights the potential limitations of current approaches, which may not be scalable or effective for future LLM architectures. Finally, the surveyâ€™s scope is limited to contextual compression, and future research may uncover new challenges and opportunities at the intersection of compression and other aspects of LLMs.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">As research in contextual compression for large language models continues to advance, it is essential to consider the ethical implications of these developments. One key concern is the potential for biased or unfair compression methods, which could perpetuate existing social inequalities or create new ones. For instance, compression techniques that prioritize certain types of data or language styles over others may disadvantage certain groups or communities. Furthermore, the focus on large language models may exacerbate existing power imbalances, where only those with access to significant computational resources and data can develop and deploy these models.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">Additionally, the reliance on existing evaluation metrics and benchmark datasets may perpetuate biases and limitations in the development of compression techniques. It is crucial to ensure that these metrics and datasets are diverse, representative, and regularly updated to reflect the complexities of real-world language use.</p>
</div>
<div class="ltx_para" id="Sx2.p3">
<p class="ltx_p" id="Sx2.p3.1">The need for advanced methodologies specifically designed for LLMs also raises ethical concerns around the responsible development and deployment of these models. As LLMs become increasingly ubiquitous, it is essential to consider their potential impact on individuals, communities, and society as a whole. This includes ensuring that these models are transparent, explainable, and accountable, and that their development and deployment are guided by ethical principles and values.</p>
</div>
<div class="ltx_para" id="Sx2.p4">
<p class="ltx_p" id="Sx2.p4.1">Finally, the surveyâ€™s limited scope to contextual compression highlights the need for a more comprehensive consideration of the ethical implications of LLMs and their applications. Future research should prioritize ethical considerations and ensure that the development of compression techniques and LLMs is guided by a commitment to social responsibility, fairness, and transparency.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil etÂ al. (2022)</span>
<span class="ltx_bibblock">
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022.

</span>
<span class="ltx_bibblock">Exploring length generalization in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in Neural Information Processing Systems</em>, 35:38546â€“38556.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askell etÂ al. (2021)</span>
<span class="ltx_bibblock">
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, etÂ al. 2021.

</span>
<span class="ltx_bibblock">A general language assistant as a laboratory for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2112.00861</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baddeley (1992)</span>
<span class="ltx_bibblock">
Alan Baddeley. 1992.

</span>
<span class="ltx_bibblock">Working memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Science</em>, 255(5044):556â€“559.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baeza-Yates etÂ al. (1999)</span>
<span class="ltx_bibblock">
Ricardo Baeza-Yates, Berthier Ribeiro-Neto, etÂ al. 1999.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Modern information retrieval</em>, volume 463.

</span>
<span class="ltx_bibblock">ACM press New York.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy etÂ al. (2020)</span>
<span class="ltx_bibblock">
IzÂ Beltagy, MatthewÂ E Peters, and Arman Cohan. 2020.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2004.05150</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud etÂ al. (2022)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, GeorgeÂ Bm Van DenÂ Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International conference on machine learning</em>, pages 2206â€“2240. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulatov etÂ al. (2022)</span>
<span class="ltx_bibblock">
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. 2022.

</span>
<span class="ltx_bibblock">Recurrent memory transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>, 35:11079â€“11091.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chase (2017-)</span>
<span class="ltx_bibblock">
Harrison Chase. 2017-.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/langchain-ai/langchain" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1.1">LangChain</em></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and LeÂ Sun. 2024.

</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volumeÂ 38, pages 17754â€“17762.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a.

</span>
<span class="ltx_bibblock">Extending context window of large language models via positional interpolation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2306.15595</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and HeÂ He. 2021.

</span>
<span class="ltx_bibblock">Meta-learning via language model in-context tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2110.07814</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023b.

</span>
<span class="ltx_bibblock">Longlora: Efficient fine-tuning of long-context large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2309.12307</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chevalier etÂ al. (2023)</span>
<span class="ltx_bibblock">
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023.

</span>
<span class="ltx_bibblock">Adapting language models to compress contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2305.14788</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, QuocÂ V Le, and Ruslan Salakhutdinov. 2019.

</span>
<span class="ltx_bibblock">Transformer-xl: Attentive language models beyond a fixed-length context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:1901.02860</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2022.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with io-awareness.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in Neural Information Processing Systems</em>, 35:16344â€“16359.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jiayu Ding, Shuming Ma, LiÂ Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock">Longnet: Scaling transformers to 1,000,000,000 tokens.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2307.02486</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Erk and PadÃ³ (2008)</span>
<span class="ltx_bibblock">
Katrin Erk and Sebastian PadÃ³. 2008.

</span>
<span class="ltx_bibblock">A structured vector space model for word meaning in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2008 conference on empirical methods in natural language processing</em>, pages 897â€“906.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fathi etÂ al. (2023)</span>
<span class="ltx_bibblock">
Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. 2023.

</span>
<span class="ltx_bibblock">Block-state transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2306.09539</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei etÂ al. (2023)</span>
<span class="ltx_bibblock">
Weizhi Fei, Xueyan Niu, Pingyi Zhou, LuÂ Hou, BoÂ Bai, Lei Deng, and Wei Han. 2023.

</span>
<span class="ltx_bibblock">Extending context window of large language models via semantic compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2312.09571</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu etÂ al. (2022)</span>
<span class="ltx_bibblock">
DanielÂ Y Fu, Tri Dao, KhaledÂ K Saab, ArminÂ W Thomas, Atri Rudra, and Christopher RÃ©. 2022.

</span>
<span class="ltx_bibblock">Hungry hungry hippos: Towards language modeling with state space models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2212.14052</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, YiÂ Dai, Jiawei Sun, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2312.10997</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge etÂ al. (2023)</span>
<span class="ltx_bibblock">
Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock">In-context autoencoder for context compression in a large language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2307.06945</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Albert Gu, Karan Goel, and Christopher RÃ©. 2021.

</span>
<span class="ltx_bibblock">Efficiently modeling long sequences with structured state spaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2111.00396</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. 2023.

</span>
<span class="ltx_bibblock">Advancing transformer architecture in long-context large language models: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2311.12351</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard etÂ al. (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2208.03299</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Huiqiang Jiang, Qianhui Wu, , Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.06839" title="">LongLLMLingua: Accelerating and enhancing llms in long context scenarios via prompt compression</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">ArXiv preprint</em>, abs/2310.06839.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.825" title="">LLMLingua: Compressing prompts for accelerated inference of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 13358â€“13376. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kandpal etÂ al. (2023)</span>
<span class="ltx_bibblock">
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023.

</span>
<span class="ltx_bibblock">Large language models struggle to learn long-tail knowledge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">International Conference on Machine Learning</em>, pages 15696â€“15707. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krovetz and Croft (1992)</span>
<span class="ltx_bibblock">
Robert Krovetz and WÂ Bruce Croft. 1992.

</span>
<span class="ltx_bibblock">Lexical ambiguity and information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">ACM Transactions on Information Systems (TOIS)</em>, 10(2):115â€“141.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester etÂ al. (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.243" title="">The power of scale for parameter-efficient prompt tuning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 3045â€“3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liang (2021)</span>
<span class="ltx_bibblock">
XiangÂ Lisa Li and Percy Liang. 2021.

</span>
<span class="ltx_bibblock">Prefix-tuning: Optimizing continuous prompts for generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2101.00190</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lialin etÂ al. (2023)</span>
<span class="ltx_bibblock">
Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023.

</span>
<span class="ltx_bibblock">Scaling down to scale up: A guide to parameter-efficient fine-tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2303.15647</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2024)</span>
<span class="ltx_bibblock">
NelsonÂ F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Transactions of the Association for Computational Linguistics</em>, 12:157â€“173.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a.

</span>
<span class="ltx_bibblock">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">ACM Computing Surveys</em>, 55(9):1â€“35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-short.8" title="">P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 61â€“68, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023b)</span>
<span class="ltx_bibblock">
YiÂ Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and XuÂ Sun. 2023b.

</span>
<span class="ltx_bibblock">Recall: A benchmark for llms robustness against external counterfactual knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2311.08147</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McCann etÂ al. (2017)</span>
<span class="ltx_bibblock">
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017.

</span>
<span class="ltx_bibblock">Learned in translation: Contextualized word vectors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov etÂ al. (2013a)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a.

</span>
<span class="ltx_bibblock">Efficient estimation of word representations in vector space.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:1301.3781</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov etÂ al. (2013b)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, GregÂ S Corrado, and Jeff Dean. 2013b.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their compositionality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Advances in neural information processing systems</em>, 26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jesse Mu, Xiang Li, and Noah Goodman. 2024.

</span>
<span class="ltx_bibblock">Learning to compress prompts with gist tokens.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nye etÂ al. (2021)</span>
<span class="ltx_bibblock">
Maxwell Nye, AndersÂ Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, etÂ al. 2021.

</span>
<span class="ltx_bibblock">Show your work: Scratchpads for intermediate computation with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2112.00114</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H.Â Vicky Zhao, Lili Qiu, and Dongmei Zhang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2403.12968" title="">LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">ArXiv preprint</em>, abs/2403.12968.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023.

</span>
<span class="ltx_bibblock">Yarn: Efficient context window extension of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2309.00071</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington etÂ al. (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and ChristopherÂ D Manning. 2014.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em>, pages 1532â€“1543.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters etÂ al. (2018)</span>
<span class="ltx_bibblock">
MatthewÂ E Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018.

</span>
<span class="ltx_bibblock">Dissecting contextual word embeddings: Architecture and representation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:1808.08949</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poli etÂ al. (2023)</span>
<span class="ltx_bibblock">
Michael Poli, Stefano Massaroli, Eric Nguyen, DanielÂ Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher RÃ©. 2023.

</span>
<span class="ltx_bibblock">Hyena hierarchy: Towards larger convolutional language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">International Conference on Machine Learning</em>, pages 28043â€“28078. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020.

</span>
<span class="ltx_bibblock">Pre-trained models for natural language processing: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Science China Technological Sciences</em>, 63(10):1872â€“1897.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, etÂ al. 2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">OpenAI</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scheurer etÂ al. (2022)</span>
<span class="ltx_bibblock">
JÃ©rÃ©my Scheurer, JonÂ Ander Campos, JunÂ Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2022.

</span>
<span class="ltx_bibblock">Learning from natural language feedback.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">ACL Workshop on Learning with Natural Language Supervision</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2024)</span>
<span class="ltx_bibblock">
Kaize Shi, Xueyao Sun, Qing Li, and Guandong Xu. 2024.

</span>
<span class="ltx_bibblock">Compressing long context for enhancing rag with amr-based concept distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2405.03085</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinha and Mihalcea (2007)</span>
<span class="ltx_bibblock">
Ravi Sinha and Rada Mihalcea. 2007.

</span>
<span class="ltx_bibblock">Unsupervised graph-basedword sense disambiguation using measures of word semantic similarity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">International conference on semantic computing (ICSC 2007)</em>, pages 363â€“369. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith etÂ al. (2022)</span>
<span class="ltx_bibblock">
JimmyÂ TH Smith, Andrew Warrington, and ScottÂ W Linderman. 2022.

</span>
<span class="ltx_bibblock">Simplified state space layers for sequence modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2208.04933</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snell etÂ al. (2022)</span>
<span class="ltx_bibblock">
Charlie Snell, Dan Klein, and Ruiqi Zhong. 2022.

</span>
<span class="ltx_bibblock">Learning by distilling context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2209.15189</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Victor etÂ al. (2022)</span>
<span class="ltx_bibblock">
Sanh Victor, Webson Albert, Raffel Colin, Bach Stephen, Sutawika Lintang, Alyafeai Zaid, Chaffin Antoine, Stiegler Arnaud, Raja Arun, Dey Manan, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wason and Evans (1974)</span>
<span class="ltx_bibblock">
PeterÂ C Wason and JÂ StÂ BT Evans. 1974.

</span>
<span class="ltx_bibblock">Dual processes in reasoning?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Cognition</em>, 3(2):141â€“154.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, VincentÂ Y Zhao, Kelvin Guu, AdamsÂ Wei Yu, Brian Lester, Nan Du, AndrewÂ M Dai, and QuocÂ V Le. 2021.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2109.01652</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, EdÂ Chi, QuocÂ V Le, Denny Zhou, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Advances in neural information processing systems</em>, 35:24824â€“24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">West etÂ al. (2021)</span>
<span class="ltx_bibblock">
Peter West, Chandra Bhagavatula, Jack Hessel, JenaÂ D Hwang, Liwei Jiang, RonanÂ Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2021.

</span>
<span class="ltx_bibblock">Symbolic knowledge distillation: from general language models to commonsense models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2110.07178</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wingate etÂ al. (2022)</span>
<span class="ltx_bibblock">
David Wingate, Mohammad Shoeybi, and Taylor Sorensen. 2022.

</span>
<span class="ltx_bibblock">Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2210.03162</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=mlJLVigNHp" title="">RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Lapata (2020)</span>
<span class="ltx_bibblock">
Yumo Xu and Mirella Lapata. 2020.

</span>
<span class="ltx_bibblock">Coarse-to-fine query focused multi-document summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Proceedings of the 2020 Conference on empirical methods in natural language processing (EMNLP)</em>, pages 3632â€“3645.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, YuÂ Zhang, Yulong Chen, etÂ al. 2023.

</span>
<span class="ltx_bibblock">Sirenâ€™s song in the ai ocean: a survey on hallucination in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2309.01219</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong etÂ al. (2021)</span>
<span class="ltx_bibblock">
Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.398" title="">Factual probing is [MASK]: Learning vs. learning to recall</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 5017â€“5033, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2022)</span>
<span class="ltx_bibblock">
Denny Zhou, Nathanael SchÃ¤rli, LeÂ Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Least-to-most prompting enables complex reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2205.10625</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wangchunshu Zhou, YuchenÂ Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023.

</span>
<span class="ltx_bibblock">Recurrentgpt: Interactive generation of (arbitrarily) long text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2305.13304</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  2 14:30:45 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
