<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis</title>
<!--Generated on Fri Jun 21 14:21:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.15187v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S1" title="In UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S2" title="In UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title">Retrieval Augmented Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S3" title="In UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset: UDA</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S3.SS0.SSS0.Px1" title="In 3 Dataset: UDA ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title">Data Sources</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S3.SS0.SSS0.Px2" title="In 3 Dataset: UDA ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title">Label Collection.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S3.SS0.SSS0.Px3" title="In 3 Dataset: UDA ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title">Dataset Construction.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4" title="In UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Benchmarks and Evaluations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS0.SSS0.Px1" title="In 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS0.SSS0.Px2" title="In 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title">Experiment setups</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS1" title="In 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluating Data Parsing</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS1.SSS0.Px1" title="In 4.1 Evaluating Data Parsing ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title">Remark.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS2" title="In 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Indexing and Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS3" title="In 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>RAG vs. Long Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS4" title="In 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Evaluating Chain-of-Thought and Code Interpreters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS5" title="In 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>End-to-End Evaluations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S5" title="In UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A1" title="In UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Dataset Example</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A2" title="In UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Experimental Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A2.SS1" title="In Appendix B Experimental Details ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Prompt Templates for LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A2.SS2" title="In Appendix B Experimental Details ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Settings of Indexing and Retrieval Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A2.SS3" title="In Appendix B Experimental Details ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Long-context Policy</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yulong Hui
<br class="ltx_break"/>Tsinghua University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">huiyl22@mails.tsinghua.edu.cn
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id1.1.id1.1">\And</span></span>Yao Lu 
<br class="ltx_break"/>National University of Singapore 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">luyao@comp.nus.edu.sg</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id3.3.id3">\And</span>Huanchen Zhang
<br class="ltx_break"/>Tsinghua University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.4.id4">huanchen@tsinghua.edu.cn
<br class="ltx_break"/></span>
</span><span class="ltx_author_notes">Huanchen Zhang is also affiliated with Shanghai Qi Zhi Institute.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">The use of Retrieval-Augmented Generation (RAG) has improved Large Language Models (LLMs) in collaborating with external data, yet significant challenges exist in real-world scenarios. In areas such as academic literature and finance question answering, data are often found in raw text and tables in HTML or PDF formats, which can be lengthy and highly unstructured. In this paper, we introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that involves 2,965 real-world documents and 29,590 expert-annotated Q&amp;A pairs. We revisit popular LLM- and RAG-based solutions for document analysis and evaluate the design choices and answer qualities across multiple document domains and diverse query types. Our evaluation yields interesting findings and highlights the importance of data parsing and retrieval. We hope our benchmark can shed light and better serve real-world document analysis applications. The benchmark suite and code can be found at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/qinchuanhui/UDA-Benchmark" title="">https://github.com/qinchuanhui/UDA-Benchmark</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) have achieved remarkable success yet still face limitations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib15" title="">15</a>]</cite>. One of the key challenges is to grapple with external knowledge and previously unseen data, which is a common scenario in real-world applications such as enterprise search and data analysis. For example, a company may need to query its proprietary technique documents; a financial expert may need to extract insights from the latest corporate reports; and a research group may need to assimilate cutting-edge academic papers to guide their innovations.
To overcome this challenge, retrieval-augmented generation (RAG) that incorporates relevant content from an external data source into the LLM generation procedure, has emerged as a promising approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib29" title="">29</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">1</span></a>, a common RAG workflow involves the following procedures: 1) parse the external data and segment it into chunks; 2) embed the chunks into vectors and create indexes; 3) retrieve the most relevant chunks according to the user query, and 4) assemble the prompt with relevant chunks (i.e., context) as input for the LLM to generate the response. Recently, a multitude of advanced RAG techniques have been proposed with improved retrieval policies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib61" title="">61</a>]</cite>, context chunk compression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib54" title="">54</a>]</cite>, and pre-training strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib47" title="">47</a>]</cite>. Furthermore, as an alternative approach to RAG, recent advances in long-context LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib59" title="">59</a>]</cite> have empowered querying directly on lengthy data without chunking and retrieval.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of basic RAG processing on unstructured documents.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">According to a study by Forbes, 95% of business organizations in various domains such as finance and technology need analyzing unstructured texts and tables in raw documents like web pages and PDFs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib5" title="">5</a>]</cite>. Analyzing unstructured documents in the world poses the following challenges:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">Unstructured inputs.</em> Parsing unstructured documents into regularized text and tables is error-prone. Unlike plain text, unstructured documents often contain intricate layouts and redundant symbols. Prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib60" title="">60</a>]</cite> have incorporated vision and language models, but their effectiveness is still doubtful. Further, multi-modal data, e.g., tables, require improved indexing and retrieval strategies because classic text embeddings disregard structural information from these data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">Lengthy documents</em>, such as financial reports spanning hundreds of pages, necessitate effective embedding and retrieval mechanisms.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><em class="ltx_emph ltx_font_italic" id="S1.p6.1.1">Query answering strategies.</em> User queries span from extractive queries to complex arithmetic reasoning; each may require a different answering strategy, such as using Chain-of-Thought<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib55" title="">55</a>]</cite> or external tools like Code Interpreters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib62" title="">62</a>]</cite>. We wonder how these design choices can impact the end-to-end query answering quality.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In this paper, we propose a benchmark suite that enables the evaluation of various components of RAG-based unstructured document analysis. Specifically, we leverage the Unstructured Document Analysis (UDA) dataset to cover finance, academia, and world knowledge with a total of 2,965 documents and 29,590 expert-annotated Q&amp;A pairs. UDA enables end-to-end evaluations of diverse Q&amp;As and granular analyses of individual components within the RAG pipeline.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Unlike prior datasets and benchmarks which often assume clean
or segmented inputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib14" title="">14</a>]</cite>, we exploit the design considerations in end-to-end document analysis.
We performed extensive experimental analysis to cover different data extraction policies, retrieval and generation strategies, and a range of LLMs. We also compared RAG-based solutions with those that use LLMs with long context capabilities.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">Our analysis leads to interesting findings. First, despite that various computer-vision- or language-based parsing techniques have been proposed, conventional solutions may fail to improve the overall Q&amp;A quality due to irregular edge cases.
We also found that smaller retrieval models could perform reasonably well in certain RAG applications, while
Chain-of-Thought approaches improve the answer quality in zero-shot numerical document analysis. However, long-context LLMs often fall short in these tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">The initial findings from our experiments can be pivotal but limited. We are actively updating the benchmark suite and will incorporate more state-of-the-art RAG solutions and LLMs in our benchmark. We hope such efforts will shed light on future research and production of RAG- and LLM-based document analysis.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Retrieval Augmented Generation</h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Large Language Models (LLMs) demonstrate remarkable abilities but struggle with external knowledge and the latest unseen data. Retrieval Augmented Generation (RAG) addresses these limitations by incorporating external information to enrich LLMs’ responses, yielding more precise and credible outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib42" title="">42</a>]</cite>. Furthermore, several innovative techniques have been developed to refine the procedure beyond the basic RAG approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib15" title="">15</a>]</cite>. For instance, Self-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib2" title="">2</a>]</cite> and FLARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib22" title="">22</a>]</cite> determine the retrieved content actively according to the generation results. LLMLingua <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib20" title="">20</a>]</cite>, RECOMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib56" title="">56</a>]</cite>, and FILCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib54" title="">54</a>]</cite> focus on filtering and condensing the context input to enhance the information efficiency.
Additionally, specialized pre-training and fine-tuning techniques can also optimize the process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib47" title="">47</a>]</cite>. Despite these advancements, current approaches often overlook the complexities in real-world unstructured document analysis, such as unstructured data and table schemas, extensive document lengths, and diverse analytical queries.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS0.SSS0.Px1.p2.1.1">Prior Benchmarks for RAG</span> offer tools for assessing various dimensions of RAG. RGB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib8" title="">8</a>]</cite> benchmarks RAG on robustness and negative rejection. CRUD-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib32" title="">32</a>]</cite> introduces a Chinese news dataset for multifaceted evaluation, including text continuation, question answering, and error correction. ALCE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib14" title="">14</a>]</cite> evaluates the performance of generating cited responses. In contrast, our benchmark emphasizes the tasks of document comprehension and analysis.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS0.SSS0.Px1.p3.1.1">Prior Benchmarks for Q&amp;A</span> often inadequately represent real-world scenarios. For example, mainstream datasets like TriviaQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib23" title="">23</a>]</cite>, HotPotQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib58" title="">58</a>]</cite>, SQuAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib40" title="">40</a>]</cite>, and NaturalQuestions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib28" title="">28</a>]</cite> predominantly utilize the Wikipedia sources that have limited application scope and potentially overlap with LLM’s internal knowledge.
Moreover, datasets such as QuALITY <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib36" title="">36</a>]</cite> and NarrativeQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib25" title="">25</a>]</cite> are pure plain text, while WikiTableQuestions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib37" title="">37</a>]</cite> and SQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib17" title="">17</a>]</cite> are pure tabular data. They fail to capture the complexity of real-world analytical documents. Additionally, datasets like FinQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib11" title="">11</a>]</cite> and VisualMRC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib50" title="">50</a>]</cite> present well-structured or segmented content directly, thus sidestepping the intricacies of parsing and retrieval.
Table <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S2.T1" title="Table 1 ‣ Retrieval Augmented Generation ‣ 2 Related Work ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the features of existing datasets and highlights the uniqueness of our UDA benchmark in real-world document analysis.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary and comparison of Q&amp;A datasets. <em class="ltx_emph ltx_font_italic" id="S2.T1.3.1">Raw documents</em>: datasets in the native file format, without extraction or parsing; <em class="ltx_emph ltx_font_italic" id="S2.T1.4.2">Long content</em>: the provision of unsegmented, un-retrieved long content. </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.5" style="width:390.3pt;height:182.8pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-124.8pt,58.3pt) scale(0.609905451896096,0.609905451896096) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1">
<tr class="ltx_tr" id="S2.T1.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.1.2.1"></span> <span class="ltx_text" id="S2.T1.5.1.1.2.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.1.2.2.1">
<span class="ltx_tr" id="S2.T1.5.1.1.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.1.2.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Text + tables</span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.1.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.1.3.1"></span> <span class="ltx_text" id="S2.T1.5.1.1.3.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.1.3.2.1">
<span class="ltx_tr" id="S2.T1.5.1.1.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.1.3.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Raw documents</span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.1.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.1.4.1"></span> <span class="ltx_text" id="S2.T1.5.1.1.4.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.1.4.2.1">
<span class="ltx_tr" id="S2.T1.5.1.1.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.1.4.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Long content</span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.1.4.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.1.5.1"></span> <span class="ltx_text" id="S2.T1.5.1.1.5.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.1.5.2.1">
<span class="ltx_tr" id="S2.T1.5.1.1.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.1.5.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Diverse</span></span>
<span class="ltx_tr" id="S2.T1.5.1.1.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.1.5.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Questions</span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.1.5.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.1.6.1"></span> <span class="ltx_text" id="S2.T1.5.1.1.6.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.1.6.2.1">
<span class="ltx_tr" id="S2.T1.5.1.1.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.1.6.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Sources</span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.1.6.3"></span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.2.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.2.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.2.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.2.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">HybridQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib9" title="">9</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.2.1.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_border_t" id="S2.T1.5.1.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.5.1.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.2.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.2.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">Wikipedia</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.3">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.3.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.3.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.3.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">WiKiTableQuestion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib37" title="">37</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.3.1.3"></span></td>
<td class="ltx_td" id="S2.T1.5.1.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td" id="S2.T1.5.1.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td" id="S2.T1.5.1.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.3.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.3.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">Wikipedia</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.4">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.4.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.4.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.4.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.4.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.4.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">TriviaQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib23" title="">23</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.4.1.3"></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">Wikipedia</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.5">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.5.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.5.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.5.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.5.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.5.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">VisualMRC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib50" title="">50</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.5.1.3"></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.5.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td" id="S2.T1.5.1.5.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td" id="S2.T1.5.1.5.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.5.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.5.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">Wikipedia</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.6">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.6.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.6.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.6.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.6.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.6.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.6.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">FinQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib11" title="">11</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.6.1.3"></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.6.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td" id="S2.T1.5.1.6.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td" id="S2.T1.5.1.6.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td" id="S2.T1.5.1.6.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.6.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">Finance</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.7">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.7.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.7.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.7.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.7.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.7.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.7.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">TAT-DQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib63" title="">63</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.7.1.3"></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.7.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.7.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td" id="S2.T1.5.1.7.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.7.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.7.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">Finance</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.8">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.8.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.8.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.8.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.8.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.8.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.8.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Qasper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib12" title="">12</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.8.1.3"></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.8.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td" id="S2.T1.5.1.8.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.8.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.8.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.8.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">Papers</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.9">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.9.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.9.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.9.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.9.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.9.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.9.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">PDF-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib13" title="">13</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.9.1.3"></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.9.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.9.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.9.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td" id="S2.T1.5.1.9.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.9.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">Medicine</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.10">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.10.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.10.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.10.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.10.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.10.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.10.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">DocVQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib33" title="">33</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.10.1.3"></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.10.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.10.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td" id="S2.T1.5.1.10.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.10.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.10.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.10.6.1">Multiple</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.11">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.11.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.11.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.11.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.11.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.11.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.11.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">NarrativeQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib25" title="">25</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.11.1.3"></span></td>
<td class="ltx_td" id="S2.T1.5.1.11.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td" id="S2.T1.5.1.11.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.11.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.11.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.11.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.11.6.1">Multiple</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.12.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_text" id="S2.T1.5.1.12.1.1"></span> <span class="ltx_text" id="S2.T1.5.1.12.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.12.1.2.1">
<span class="ltx_tr" id="S2.T1.5.1.12.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.1.12.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">UDA (Ours)</span></span>
</span></span><span class="ltx_text" id="S2.T1.5.1.12.1.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.12.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.12.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.12.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.12.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.12.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.12.6.1">Multiple</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset: UDA</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.4">In this section,
we outline the construction of UDA. Each data item within UDA is logically structured as a triplet <math alttext="(D,q,a)" class="ltx_Math" display="inline" id="S3.p1.1.m1.3"><semantics id="S3.p1.1.m1.3a"><mrow id="S3.p1.1.m1.3.4.2" xref="S3.p1.1.m1.3.4.1.cmml"><mo id="S3.p1.1.m1.3.4.2.1" stretchy="false" xref="S3.p1.1.m1.3.4.1.cmml">(</mo><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">D</mi><mo id="S3.p1.1.m1.3.4.2.2" xref="S3.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.p1.1.m1.2.2" xref="S3.p1.1.m1.2.2.cmml">q</mi><mo id="S3.p1.1.m1.3.4.2.3" xref="S3.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.p1.1.m1.3.3" xref="S3.p1.1.m1.3.3.cmml">a</mi><mo id="S3.p1.1.m1.3.4.2.4" stretchy="false" xref="S3.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.3b"><vector id="S3.p1.1.m1.3.4.1.cmml" xref="S3.p1.1.m1.3.4.2"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝐷</ci><ci id="S3.p1.1.m1.2.2.cmml" xref="S3.p1.1.m1.2.2">𝑞</ci><ci id="S3.p1.1.m1.3.3.cmml" xref="S3.p1.1.m1.3.3">𝑎</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.3c">(D,q,a)</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.3d">( italic_D , italic_q , italic_a )</annotation></semantics></math>, where <math alttext="D" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_D</annotation></semantics></math> represents a complete unstructured document, <math alttext="q" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_q</annotation></semantics></math> denotes a question raised from the document, and <math alttext="a" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">italic_a</annotation></semantics></math> signifies the ground truth answer (refer to the data example in Appendix  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A1" title="Appendix A Dataset Example ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">A</span></a>). To mirror the authenticity of real-world applications, the documents are retained in their original file formats without parsing or segmentation.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>An overview of sub-datasets in UDA and their statistics</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:433.6pt;height:79.8pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-142.8pt,26.1pt) scale(0.602842155751527,0.602842155751527) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Domain</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="S3.T2.1.1.1.2.1"></span> <span class="ltx_text" id="S3.T2.1.1.1.2.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.2.2.1">
<span class="ltx_tr" id="S3.T2.1.1.1.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Sub Dataset</span></span>
</span></span><span class="ltx_text" id="S3.T2.1.1.1.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.3" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="S3.T2.1.1.1.3.1"></span> <span class="ltx_text" id="S3.T2.1.1.1.3.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.3.2.1">
<span class="ltx_tr" id="S3.T2.1.1.1.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.3.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Doc Format</span></span>
</span></span><span class="ltx_text" id="S3.T2.1.1.1.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.4" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="S3.T2.1.1.1.4.1"></span> <span class="ltx_text" id="S3.T2.1.1.1.4.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.4.2.1">
<span class="ltx_tr" id="S3.T2.1.1.1.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.4.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Doc Num</span></span>
</span></span><span class="ltx_text" id="S3.T2.1.1.1.4.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.5" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="S3.T2.1.1.1.5.1"></span> <span class="ltx_text" id="S3.T2.1.1.1.5.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.5.2.1">
<span class="ltx_tr" id="S3.T2.1.1.1.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.5.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Q&amp;A Num</span></span>
</span></span><span class="ltx_text" id="S3.T2.1.1.1.5.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.6" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="S3.T2.1.1.1.6.1"></span> <span class="ltx_text" id="S3.T2.1.1.1.6.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.6.2.1">
<span class="ltx_tr" id="S3.T2.1.1.1.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.6.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Avg #Words</span></span>
</span></span><span class="ltx_text" id="S3.T2.1.1.1.6.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.7" style="padding-top:2pt;padding-bottom:2pt;">Avg #Pages</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.8" style="padding-top:2pt;padding-bottom:2pt;">Tot Size</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.9" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="S3.T2.1.1.1.9.1"></span> <span class="ltx_text" id="S3.T2.1.1.1.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.9.2.1">
<span class="ltx_tr" id="S3.T2.1.1.1.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.9.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Q&amp;A Types</span></span>
</span></span><span class="ltx_text" id="S3.T2.1.1.1.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.1" rowspan="2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S3.T2.1.1.2.1.1">Finance</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2" style="padding-top:2pt;padding-bottom:2pt;">FinHybrid</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.3" style="padding-top:2pt;padding-bottom:2pt;">PDF</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.4" style="padding-top:2pt;padding-bottom:2pt;">788</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.5" style="padding-top:2pt;padding-bottom:2pt;">8190</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.6" style="padding-top:2pt;padding-bottom:2pt;">76.6k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.7" style="padding-top:2pt;padding-bottom:2pt;">147.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.8" style="padding-top:2pt;padding-bottom:2pt;">2.61 GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.9" style="padding-top:2pt;padding-bottom:2pt;">Arithmetic</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">TatHybrid</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.2" style="padding-top:2pt;padding-bottom:2pt;">PDF</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.3" style="padding-top:2pt;padding-bottom:2pt;">170</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.4" style="padding-top:2pt;padding-bottom:2pt;">14703</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.5" style="padding-top:2pt;padding-bottom:2pt;">77.5k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.6" style="padding-top:2pt;padding-bottom:2pt;">148.5</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.7" style="padding-top:2pt;padding-bottom:2pt;">0.58 GB</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.8" style="padding-top:2pt;padding-bottom:2pt;">Extractive, counting, arithmetic</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.1" rowspan="2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S3.T2.1.1.4.1.1"><span class="ltx_text" id="S3.T2.1.1.4.1.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.4.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.4.1.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.4.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.4.1.1.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Academic</span></span>
<span class="ltx_tr" id="S3.T2.1.1.4.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.4.1.1.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">Paper</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.4.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.2" style="padding-top:2pt;padding-bottom:2pt;">PaperTab</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.3" style="padding-top:2pt;padding-bottom:2pt;">PDF</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.4" style="padding-top:2pt;padding-bottom:2pt;">307</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.5" style="padding-top:2pt;padding-bottom:2pt;">393</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.6" style="padding-top:2pt;padding-bottom:2pt;">6.1k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.7" style="padding-top:2pt;padding-bottom:2pt;">11.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.8" style="padding-top:2pt;padding-bottom:2pt;">0.22 GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.9" style="padding-top:2pt;padding-bottom:2pt;">Extractive, yes/no, free-form</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.1" style="padding-top:2pt;padding-bottom:2pt;">PaperText</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.2" style="padding-top:2pt;padding-bottom:2pt;">PDF</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.3" style="padding-top:2pt;padding-bottom:2pt;">1087</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.4" style="padding-top:2pt;padding-bottom:2pt;">2804</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5" style="padding-top:2pt;padding-bottom:2pt;">5.9k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.6" style="padding-top:2pt;padding-bottom:2pt;">10.6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.7" style="padding-top:2pt;padding-bottom:2pt;">0.87 GB</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.8" style="padding-top:2pt;padding-bottom:2pt;">Extractive, yes/no, free-form</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.1.1.6.1" rowspan="2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S3.T2.1.1.6.1.1"><span class="ltx_text" id="S3.T2.1.1.6.1.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.6.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.6.1.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.6.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.6.1.1.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">World</span></span>
<span class="ltx_tr" id="S3.T2.1.1.6.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.6.1.1.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">Knowledge</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.6.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.6.2" style="padding-top:2pt;padding-bottom:2pt;">FetaTab</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.6.3" style="padding-top:2pt;padding-bottom:2pt;">PDF &amp; HTML</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.6.4" style="padding-top:2pt;padding-bottom:2pt;">878</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.6.5" style="padding-top:2pt;padding-bottom:2pt;">1023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.6.6" style="padding-top:2pt;padding-bottom:2pt;">6.0k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.6.7" style="padding-top:2pt;padding-bottom:2pt;">14.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.6.8" style="padding-top:2pt;padding-bottom:2pt;">0.92 GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.6.9" style="padding-top:2pt;padding-bottom:2pt;">Free-form</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.7.1" style="padding-top:2pt;padding-bottom:2pt;">NqText</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.7.2" style="padding-top:2pt;padding-bottom:2pt;">PDF &amp; HTML</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.7.3" style="padding-top:2pt;padding-bottom:2pt;">645</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.7.4" style="padding-top:2pt;padding-bottom:2pt;">2477</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.7.5" style="padding-top:2pt;padding-bottom:2pt;">6.1k</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.7.6" style="padding-top:2pt;padding-bottom:2pt;">14.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.7.7" style="padding-top:2pt;padding-bottom:2pt;">0.68 GB</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.7.8" style="padding-top:2pt;padding-bottom:2pt;">Extractive</td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Examples of different Q&amp;A types</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:433.6pt;height:141.2pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-77.6pt,25.2pt) scale(0.736334700248089,0.736334700248089) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1">
<tr class="ltx_tr" id="S3.T3.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">Q&amp;A Types</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.2.2" style="padding-top:2pt;padding-bottom:2pt;">Example Question</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.2.3" style="padding-top:2pt;padding-bottom:2pt;">Example Answer</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">Extractive</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.2" style="padding-top:2pt;padding-bottom:2pt;">Who has the longest win streak in mma?</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.3" style="padding-top:2pt;padding-bottom:2pt;">Anderson Silva</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">Yes/No</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.2" style="padding-top:2pt;padding-bottom:2pt;">Are experiments performed with any other pair of languages?</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.3" style="padding-top:2pt;padding-bottom:2pt;">No</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.5.1" style="padding-top:2pt;padding-bottom:2pt;">Free-form</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.5.2" style="padding-top:2pt;padding-bottom:2pt;">How did Hayden Panettiere fare at the 2012 and 2013 Golden Globes?</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.5.3" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="S3.T3.1.1.5.3.1"></span> <span class="ltx_text" id="S3.T3.1.1.5.3.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.5.3.2.1">
<span class="ltx_tr" id="S3.T3.1.1.5.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.3.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Hayden Panettiere received two</span></span>
<span class="ltx_tr" id="S3.T3.1.1.5.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.3.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">nominations for the Golden Globe</span></span>
<span class="ltx_tr" id="S3.T3.1.1.5.3.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.3.2.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">Award, Best Supporting Actress Series,</span></span>
<span class="ltx_tr" id="S3.T3.1.1.5.3.2.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.3.2.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">Miniseries or Television Film, for her</span></span>
<span class="ltx_tr" id="S3.T3.1.1.5.3.2.1.5">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.3.2.1.5.1" style="padding-top:2pt;padding-bottom:2pt;">work on Nashville in 2012 and 2013.</span></span>
</span></span><span class="ltx_text" id="S3.T3.1.1.5.3.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.6.1" style="padding-top:2pt;padding-bottom:2pt;">Counting</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.6.2" style="padding-top:2pt;padding-bottom:2pt;">How many regions have revenues of more than $20,000 thousand?</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.6.3" style="padding-top:2pt;padding-bottom:2pt;">2</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.2" style="padding-top:2pt;padding-bottom:2pt;">Arithmetic</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.3" style="padding-top:2pt;padding-bottom:2pt;">What was the percentage increase in cash dividend from 2015 to 2016?</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="S3.T3.1.1.1.1.2"></span> <span class="ltx_text" id="S3.T3.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.1.1.1.1">
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;"> <math alttext="(0.29-0.25)\div 0.25*100\%=16\%" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mrow id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.cmml"><mrow id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.cmml"><mrow id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.2" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.2.cmml">0.29</mn><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.1" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.3" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.3.cmml">0.25</mn></mrow><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.2" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.cmml">÷</mo><mn id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.3" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.3.cmml">0.25</mn></mrow><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.2.cmml">∗</mo><mrow id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3.cmml"><mn id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3.2" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3.2.cmml">100</mn><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3.1" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3.1.cmml">%</mo></mrow></mrow><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mn id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">16</mn><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3.1" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1"><eq id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.2"></eq><apply id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1"><times id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.2.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.2"></times><apply id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1"><divide id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.2"></divide><apply id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1"><minus id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.1"></minus><cn id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.2.cmml" type="float" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.2">0.29</cn><cn id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.3.cmml" type="float" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.3">0.25</cn></apply><cn id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.3.cmml" type="float" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.1.3">0.25</cn></apply><apply id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3"><csymbol cd="latexml" id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3.1">percent</csymbol><cn id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3.2.cmml" type="integer" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.1.3.2">100</cn></apply></apply><apply id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="latexml" id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3.1">percent</csymbol><cn id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" type="integer" xref="S3.T3.1.1.1.1.1.1.1.1.m1.1.1.3.2">16</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.1.1.1.m1.1c">(0.29-0.25)\div 0.25*100\%=16\%</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.1.1.1.1.m1.1d">( 0.29 - 0.25 ) ÷ 0.25 ∗ 100 % = 16 %</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S3.T3.1.1.1.1.3"></span></td>
</tr>
</table>
</span></div>
</figure>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Data Sources</h4>
<div class="ltx_para ltx_noindent" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">Our UDA dataset includes six sub-datasets across three pivotal domains: finance, academia, and knowledge bases, reflecting typical use cases in document analysis. As delineated in Table  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S3.T2" title="Table 2 ‣ 3 Dataset: UDA ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">2</span></a>, the dataset spans table-based and text-based (or hybrid) QA formats in each domain
to ensure that the evaluation covers different data patterns.
Moreover, UDA contains 2,965 documents with a wide range of content length and 29,590 expert-annotated Q&amp;A pairs that vary from extractive queries to arithmetic reasoning (see examples in Table  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S3.T3" title="Table 3 ‣ 3 Dataset: UDA ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">3</span></a>).
These features profoundly embody the breadth and depth of practical real-world applications.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Label Collection.</h4>
<div class="ltx_para ltx_noindent" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">We first collect the Q&amp;A labels from the open-released datasets (i.e., source datasets), which are all annotated by human participants.
Specifically, our <span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px2.p1.1.1">FinHybrid</span> is based on the financial numerical reasoning dataset FINQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib11" title="">11</a>]</cite>, which is constructed based on the public earnings reports of S&amp;P 500
companies. <span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px2.p1.1.2">TatHybrid</span> is derived from TAT-DQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib63" title="">63</a>]</cite>, whose Q&amp;A pairs are accompanied by the document snapshot of 1 to 3 pages from public financial annual reports.
Both <span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px2.p1.1.3">PaperTab</span> and <span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px2.p1.1.4">PaperText</span> are based on Qasper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib12" title="">12</a>]</cite>, a reading comprehension dataset based on NLP research papers. <span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px2.p1.1.5">FetaTab</span> is built upon FetaQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib34" title="">34</a>]</cite>, a question-answering dataset for tables from Wikipedia pages.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px2.p2.1.1">NqText</span> is derived from the widely used Q&amp;A dataset, Natural-Questions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib27" title="">27</a>]</cite>, which uses the Wikipedia pages as context.
Its questions are collected from the Google Search engine, and the answers are human-annotated.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Dataset Construction.</h4>
<div class="ltx_para ltx_noindent" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">We conduct a series of essential constructing actions after collecting the Q&amp;A labels from the source datasets. The integrity of original documents is crucial for the fidelity of document analysis.
However, most of the source datasets
only offer well-parsed and segmented partial content without the complete document.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p2.1">To address this problem, we perform a comprehensive source-document identification process. We meticulously search for, verify, and collect the complete original document files, according to the metadata or content fragments from every source dataset.
Then we embark on a rigorous matching and reorganization effort, forming complete triplet data pairs, i.e., document-question-answer.
Additionally, we categorize queries by the source of factual evidence, filter out Q&amp;As without available answers, convert token-based data patterns to natural language, unify data formats and structures across datasets, and design specific LLM-prompts for each dataset after experimental attempts.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Benchmarks and Evaluations</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We provide a systematic benchmark of various modules in a typical RAG workflow, as well as an end-to-end LLM-based evaluation. The focused items of our benchmark include:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">The effectiveness of various table-parsing approaches, including raw-text extraction, computer vision (CV)-based, CV-LLM-based and advanced multi-modal parsing (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS1" title="4.1 Evaluating Data Parsing ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">The performance of different indexing and retrieval strategies, spanning sparse retrieval, classic dense embedding, and advanced retrieval model (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS2" title="4.2 Indexing and Retrieval ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">The influence of precise retrieval on the quality of LLM interpretation (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS2" title="4.2 Indexing and Retrieval ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">The effectiveness of long-context LLMs compared to typical RAGs (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS3" title="4.3 RAG vs. Long Context ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">Comparison of different Q&amp;A strategies, such as Chain-of-Thought reasoning and the integration of external code execution (Section  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS4" title="4.4 Evaluating Chain-of-Thought and Code Interpreters ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1">End-to-end comparisons of various LLMs across diverse applications (Section  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS5" title="4.5 End-to-End Evaluations ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p>
</div>
</li>
</ul>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Metrics</h4>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">To evaluate the quality of LLM-generated answers, we apply widely accepted span-level F1-score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib40" title="">40</a>]</cite> in PaperTab, PaperText, FetaTab, and NqText datasets, where ground-truth answers are in natural language and the source datasets also utilize this metric. We treat the prediction and ground truth as bags of words and calculate the F1-score to measure their overlap.
In financial analysis, the assessment becomes more intricate due to numerical values. For the TatHybrid dataset, we adopt the numeracy-focused F1-score, introduced by Zhu, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib64" title="">64</a>]</cite>, which considers the scale and the plus-minus of numerical values. In the FinHybrid dataset, where answers are always numerical or binary, we rely on the Exact-Match metric but allow for a numerical tolerance of <math alttext="1\%" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">1</mn><mo id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1">percent</csymbol><cn id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.1.m1.1c">1\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.1.m1.1d">1 %</annotation></semantics></math>, accounting for rounding discrepancies. To assess the effectiveness of retrieval strategies, we identify the factual evidence in retrieved chunks using the relative length of the Longest Common Subsequence (LCS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib38" title="">38</a>]</cite> instead of the exact match, because extracted PDF data chunks often include extraneous symbols that can hinder exact matches while still containing crucial evidence.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Experiment setups</h4>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">In our experiments, we evaluate the performance of various decomposed RAG components, mainly utilizing two representative LLMs: 1) GPT-4
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib1" title="">1</a>]</cite>, exemplifying the large-scale powerful model, proposed by OpenAI; 2) Llama-3-8B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib51" title="">51</a>]</cite>, representing the compact yet capable model, proposed by Meta.
Furthermore, to ensure a thorough comparative analysis, we also include the end-to-end experiment encompassing a suite of additional LLMs: 3) LLama-3-70B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib31" title="">31</a>]</cite>, an open-source large-scale model; 4) Qwen-1.5-32B and Qwen-1.5-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib4" title="">4</a>]</cite>, introduced by Alibaba, notable for its 32k token context window; 5) Mixtral-8x7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib19" title="">19</a>]</cite>, a Mixture-of-Experts model innovated by MistralAI; 6)Mistral-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib18" title="">18</a>]</cite>, also from MistralAI; 7) CodeLlama-7B and CodeLlama-13B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib43" title="">43</a>]</cite>, llama models tailored for code generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p2.1">Following prior works in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib65" title="">65</a>]</cite>, we focus on zero-shot LLM generation, yet adding an extra formatting example to align the output with the desired pattern (refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A2.SS1" title="B.1 Prompt Templates for LLMs ‣ Appendix B Experimental Details ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">B.1</span></a>). For the GPT-4 model, we leverage the Azure-OpenAI API to access GPT4-Turbo-1106-Preview with the context window of 128k. Other open-source models are obtained from Huggingface, and we always use the instruct-tuned version. The inference is done on 4 NVIDIA-A100 GPUs.
To reduce the compute costs, we randomly sample 1201 documents (in PDF format) accompanied by 2503 question-answer pairs to form our evaluation set (detailed in Table  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.T4" title="Table 4 ‣ Experiment setups ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">4</span></a>). We believe it serves as a practical performance indicator for real-world scenarios.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>The structure of the sampled datasets used in the following evaluation. </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.1" style="width:325.2pt;height:40.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.6pt,6.6pt) scale(0.755678065314615,0.755678065314615) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T4.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.2" style="padding-top:2pt;padding-bottom:2pt;">Sum</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.3" style="padding-top:2pt;padding-bottom:2pt;">FinHybrid</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.4" style="padding-top:2pt;padding-bottom:2pt;">TatHybrid</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.5" style="padding-top:2pt;padding-bottom:2pt;">PaperTab</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.6" style="padding-top:2pt;padding-bottom:2pt;">PaperText</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.7" style="padding-top:2pt;padding-bottom:2pt;">FetaTab</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.8" style="padding-top:2pt;padding-bottom:2pt;">NqText</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.1" style="padding-top:2pt;padding-bottom:2pt;"># Docs</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.2" style="padding-top:2pt;padding-bottom:2pt;">1201</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.3" style="padding-top:2pt;padding-bottom:2pt;">100</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.4" style="padding-top:2pt;padding-bottom:2pt;">150</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.5" style="padding-top:2pt;padding-bottom:2pt;">307</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.6" style="padding-top:2pt;padding-bottom:2pt;">194</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.7" style="padding-top:2pt;padding-bottom:2pt;">300</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.8" style="padding-top:2pt;padding-bottom:2pt;">150</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.1" style="padding-top:2pt;padding-bottom:2pt;"># Q&amp;A pairs</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.2" style="padding-top:2pt;padding-bottom:2pt;">2503</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.3" style="padding-top:2pt;padding-bottom:2pt;">451</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.4" style="padding-top:2pt;padding-bottom:2pt;">450</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.5" style="padding-top:2pt;padding-bottom:2pt;">393</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.6" style="padding-top:2pt;padding-bottom:2pt;">480</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.7" style="padding-top:2pt;padding-bottom:2pt;">350</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.8" style="padding-top:2pt;padding-bottom:2pt;">379</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluating Data Parsing</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We evaluate various parsing methods to extract tabular information from PDF files and analyze their influence on the downstream tasks. We utilize the question set from PaperTab and the table-based questions from FinHybrid. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.F2" title="Figure 2 ‣ 4.1 Evaluating Data Parsing ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">2</span></a>, each question is paired with a PDF page that contains the clue tables; doing so prevents inaccurate retrievals. The tabular data are parsed into text and merged with the rest of the text content as the input context to the LLM.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="180" id="S4.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The procedure of the table parsing experiment</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We evaluate several existing approaches of table parsing: (1) <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Raw text extraction</span>, which employs a PDF text extractor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib39" title="">39</a>]</cite> to extract all the characters. (2) Classic Computer Vision (<span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.2">CV</span>) based approach, which often performs layout detection and OCR extraction at the same time. We follow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib53" title="">53</a>]</cite> to use Yolox <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib16" title="">16</a>]</cite>, Tesseract <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib48" title="">48</a>]</cite> and TableTransformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib49" title="">49</a>]</cite> models together. (3) <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.3">CV + LLM</span> method, which further employs an LLM to transform the outputs of (2) into Markdown tables. (4) For the advanced multi-modal approach, we employ the latest <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.4">GPT-4-Omni</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib35" title="">35</a>]</cite> to convert image-based document tables into Markdown format.
(5) The FinHybrid dataset provides the verified <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.5">well-parsed</span> tables, which serve as the parsing ground truth.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance scores (EM or F1) of LLMs using varying parsing strategies on table-based Q&amp;A tasks. Due to a lack of well-parsed table data in PaperTab dataset, there are no results.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.1" style="width:359.9pt;height:68.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.6pt,11.2pt) scale(0.751323541595772,0.751323541595772) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">LLM Name</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">Well Parsed</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">GPT-4-Omni</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">Raw Text</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">CV</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">CV + LLM</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2.1" rowspan="2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T5.1.1.2.1.1">Tabular FinHybrid (EM)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">GPT-4-Turbo</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">71.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.2.4.1">72.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T5.1.1.2.5.1" style="color:#FF0000;">68.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">61.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">52.4</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.3">
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Llama-3-8B</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">59.5</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.3.3.1">56.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T5.1.1.3.4.1" style="color:#FF0000;">51.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">44.6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">40.2</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.1.1.4.1" rowspan="2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T5.1.1.4.1.1">PaperTab (F1)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">GPT-4-Turbo</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.4.4.1">42.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.4.5" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T5.1.1.4.5.1" style="color:#FF0000;">42.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.4.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">41.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.4.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">40.6</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Llama-3-8B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">32.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.5.4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.5.4.1">35.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.5.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">33.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.5.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">32.2</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.T5" title="Table 5 ‣ 4.1 Evaluating Data Parsing ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">5</span></a> reveals that GPT-4-Omni outperforms other approaches, while surprisingly, raw text extraction also yields decent results. We found that the queried tables are relatively simple; the structural markers from the raw text, such as line-breakers and space, are often adequate for LLMs to understand the table. Classic CV methods, if not meticulously tuned, may struggle in handling non-standard table presentations, i.e., edge cases (see an example in Figure  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.F3" title="Figure 3 ‣ 4.1 Evaluating Data Parsing ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">3</span></a>).
Additionally, employing GPT-4-Omni directly for question-answering scores 69.8 and 35.4, lower than sequentially parsing and generating with GPT-4 (i.e., 72.4 and 42.4).</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="397" id="S4.F3.g1" src="x3.png" width="722"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An example of table parsing with different strategies. Raw-text-extraction preserves the informational content with structural markers; CV-based method may struggle with the irregular table presentation; GPT-4-Omni yields the highest accuracy. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">We also observe from the FinHybrid dataset that while the GPT-4 model shows a modest 5.7% improvement with well-parsed data, the much smaller Llama-3-8B offers a significant 15% enhancement, suggesting that compact models with a limited capability of parsing table layouts, may benefit more from enhanced parsing.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Remark.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">Evaluations here suggest (1) CV-based parsing methods may require adaptation for edge cases before they can be useful; (2) smaller LLMs may be impacted more by uncleaned input data.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Indexing and Retrieval</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We evaluate the performance of 5 different models under the following retrieval paradigms: 1) <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">BM-25</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib52" title="">52</a>]</cite>, a lightweight sparse retrieval method without complex neural networks, ranking
document segments based on the appearing frequency of query terms. 2) <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.2">all-MiniLM-L6</span> from SentenceTransformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib41" title="">41</a>]</cite>, a prevalent dense embedding model, mapping sentences to a 384-dimensional dense vector space. 3) <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.3">all-mpnet-base</span>, another widely utilized embedding model from SentenceTransformer, noted for its larger architecture and improved performance. 4) <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.4">text-embedding-3-large model</span>, the latest embedding model from <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.5">OpenAI</span>, with enhanced capability. These classic dense embedding models process both query and document segments into vectors, and employ cosine similarity measures to retrieve the most relevant segments.
5) <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.6">ColBERT</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib44" title="">44</a>]</cite>, an advanced retrieval model, relying on token-level embedding and fine-grained contextual late interaction.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Relative LCS scores in different retrieval strategies. We evaluate the presence of evidence in most related 1, 5, 10 and 20 chunks.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.1" style="width:433.6pt;height:73.5pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-157.8pt,26.5pt) scale(0.578694784611031,0.578694784611031) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1">
<tr class="ltx_tr" id="S4.T6.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T6.1.1.1.1" rowspan="2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T6.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T6.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">FinHybrid</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T6.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">PaperTab</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T6.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">PaperText</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T6.1.1.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">FetaTab</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T6.1.1.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">NqText</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">@1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">@5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">@10</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">@20</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">@1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">@5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">@10</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">@20</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">@1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.10" style="padding-top:2.5pt;padding-bottom:2.5pt;">@5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.11" style="padding-top:2.5pt;padding-bottom:2.5pt;">@10</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.12" style="padding-top:2.5pt;padding-bottom:2.5pt;">@20</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.13" style="padding-top:2.5pt;padding-bottom:2.5pt;">@1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.14" style="padding-top:2.5pt;padding-bottom:2.5pt;">@5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.15" style="padding-top:2.5pt;padding-bottom:2.5pt;">@10</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.16" style="padding-top:2.5pt;padding-bottom:2.5pt;">@20</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.17" style="padding-top:2.5pt;padding-bottom:2.5pt;">@1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.18" style="padding-top:2.5pt;padding-bottom:2.5pt;">@5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.19" style="padding-top:2.5pt;padding-bottom:2.5pt;">@10</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.2.20" style="padding-top:2.5pt;padding-bottom:2.5pt;">@20</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Sparse</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">BM-25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.3.3.1">65.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.3.4.1">83.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">87.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">90.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">46.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">79.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">90.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.10" style="padding-top:2.5pt;padding-bottom:2.5pt;">92.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.11" style="padding-top:2.5pt;padding-bottom:2.5pt;">47.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.12" style="padding-top:2.5pt;padding-bottom:2.5pt;">80.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.13" style="padding-top:2.5pt;padding-bottom:2.5pt;">88.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.14" style="padding-top:2.5pt;padding-bottom:2.5pt;">89.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.15" style="padding-top:2.5pt;padding-bottom:2.5pt;">68.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.16" style="padding-top:2.5pt;padding-bottom:2.5pt;">91.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.17" style="padding-top:2.5pt;padding-bottom:2.5pt;">95.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.18" style="padding-top:2.5pt;padding-bottom:2.5pt;">96.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.19" style="padding-top:2.5pt;padding-bottom:2.5pt;">42.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.20" style="padding-top:2.5pt;padding-bottom:2.5pt;">69.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.21" style="padding-top:2.5pt;padding-bottom:2.5pt;">75.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3.22" style="padding-top:2.5pt;padding-bottom:2.5pt;">80.3</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.1" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T6.1.1.4.1.1"><span class="ltx_text" id="S4.T6.1.1.4.1.1.1"></span> <span class="ltx_text" id="S4.T6.1.1.4.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.4.1.1.2.1">
<span class="ltx_tr" id="S4.T6.1.1.4.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T6.1.1.4.1.1.2.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Dense</span></span>
<span class="ltx_tr" id="S4.T6.1.1.4.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T6.1.1.4.1.1.2.1.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Embedding</span></span>
</span></span> <span class="ltx_text" id="S4.T6.1.1.4.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">all-MiniLM-L6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">71.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">78.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">84.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">51.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">81.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">90.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.10" style="padding-top:2.5pt;padding-bottom:2.5pt;">92.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.11" style="padding-top:2.5pt;padding-bottom:2.5pt;">45.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.12" style="padding-top:2.5pt;padding-bottom:2.5pt;">76.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.13" style="padding-top:2.5pt;padding-bottom:2.5pt;">85.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.14" style="padding-top:2.5pt;padding-bottom:2.5pt;">89.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.15" style="padding-top:2.5pt;padding-bottom:2.5pt;">63.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.16" style="padding-top:2.5pt;padding-bottom:2.5pt;">90.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.17" style="padding-top:2.5pt;padding-bottom:2.5pt;">94.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.18" style="padding-top:2.5pt;padding-bottom:2.5pt;">95.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.19" style="padding-top:2.5pt;padding-bottom:2.5pt;">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.20" style="padding-top:2.5pt;padding-bottom:2.5pt;">71.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.21" style="padding-top:2.5pt;padding-bottom:2.5pt;">77.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.4.22" style="padding-top:2.5pt;padding-bottom:2.5pt;">80.7</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.5">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">all-mpnet-base</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">48.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">74.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">81.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">86.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">50.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">82.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">90.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">92.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.10" style="padding-top:2.5pt;padding-bottom:2.5pt;">40.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.11" style="padding-top:2.5pt;padding-bottom:2.5pt;">75.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.12" style="padding-top:2.5pt;padding-bottom:2.5pt;">86.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.13" style="padding-top:2.5pt;padding-bottom:2.5pt;">89.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.14" style="padding-top:2.5pt;padding-bottom:2.5pt;">66.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.15" style="padding-top:2.5pt;padding-bottom:2.5pt;">91.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.16" style="padding-top:2.5pt;padding-bottom:2.5pt;">94.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.17" style="padding-top:2.5pt;padding-bottom:2.5pt;">95.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.18" style="padding-top:2.5pt;padding-bottom:2.5pt;">50.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.19" style="padding-top:2.5pt;padding-bottom:2.5pt;">73.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.20" style="padding-top:2.5pt;padding-bottom:2.5pt;">78.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.5.21" style="padding-top:2.5pt;padding-bottom:2.5pt;">81.7</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.6">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">OpenAI</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">57.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">80.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">85.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">89.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.6" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.6.6.1">55.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.7" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.6.7.1">85.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">91.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">93.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.10" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.6.10.1">52.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.11" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.6.11.1">83.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.12" style="padding-top:2.5pt;padding-bottom:2.5pt;">89.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.13" style="padding-top:2.5pt;padding-bottom:2.5pt;">90.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.14" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.6.14.1">69.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.15" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.6.15.1">92.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.16" style="padding-top:2.5pt;padding-bottom:2.5pt;">95.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.17" style="padding-top:2.5pt;padding-bottom:2.5pt;">95.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.18" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.6.18.1">50.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.19" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.6.19.1">74.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.20" style="padding-top:2.5pt;padding-bottom:2.5pt;">80.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.6.21" style="padding-top:2.5pt;padding-bottom:2.5pt;">82.3</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Advanced</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Col-BERT</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">54.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">75.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">80.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">85.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">47.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">79.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">89.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.10" style="padding-top:2.5pt;padding-bottom:2.5pt;">92.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.11" style="padding-top:2.5pt;padding-bottom:2.5pt;">48.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.12" style="padding-top:2.5pt;padding-bottom:2.5pt;">77.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.13" style="padding-top:2.5pt;padding-bottom:2.5pt;">86.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.14" style="padding-top:2.5pt;padding-bottom:2.5pt;">89.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.15" style="padding-top:2.5pt;padding-bottom:2.5pt;">67.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.16" style="padding-top:2.5pt;padding-bottom:2.5pt;">91.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.17" style="padding-top:2.5pt;padding-bottom:2.5pt;">94.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.18" style="padding-top:2.5pt;padding-bottom:2.5pt;">95.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.19" style="padding-top:2.5pt;padding-bottom:2.5pt;">47.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.20" style="padding-top:2.5pt;padding-bottom:2.5pt;">70.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.21" style="padding-top:2.5pt;padding-bottom:2.5pt;">76.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.1.1.7.22" style="padding-top:2.5pt;padding-bottom:2.5pt;">80.2</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">We use the relative length of the Longest Common Subsequence (LCS) to demonstrate the presence of human-annotated evidence in retrieved chunks (more experimental details in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A2.SS2" title="B.2 Settings of Indexing and Retrieval Experiments ‣ Appendix B Experimental Details ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">B.2</span></a>). As shown in Table  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.T6" title="Table 6 ‣ 4.2 Indexing and Retrieval ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">6</span></a>, OpenAI’s text-embedding-3-large model excels in all datasets except for FinHybrid, where the simpler BM-25 approach intriguingly outperforms. This could be attributed to the fact that financial queries often contain more precise details, such as dates or keywords; this aligns well with the direct keyword-matching of BM-25.</p>
</div>
<figure class="ltx_table" id="S4.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>End-to-end answer scores using retrieved and human-annotated context </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T7.1" style="width:368.6pt;height:93.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-66.5pt,16.7pt) scale(0.734932851735654,0.734932851735654) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T7.1.1">
<tr class="ltx_tr" id="S4.T7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">LLM Name</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.1.1.1.2" style="padding-top:2pt;padding-bottom:2pt;">Context Type</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.1.1.1.3" style="padding-top:2pt;padding-bottom:2pt;">FinHybrid</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.1.1.1.4" style="padding-top:2pt;padding-bottom:2pt;">TatHybrid</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.1.1.1.5" style="padding-top:2pt;padding-bottom:2pt;">PaperTab</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.1.1.1.6" style="padding-top:2pt;padding-bottom:2pt;">PaperText</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.1.1.1.7" style="padding-top:2pt;padding-bottom:2pt;">FetaTab</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.1.1.1.8" style="padding-top:2pt;padding-bottom:2pt;">NqText</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.2.1" rowspan="3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T7.1.1.2.1.1">Llama-3-8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.2.2" style="padding-top:2pt;padding-bottom:2pt;">OpenAI Retrieval @5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.2.3" style="padding-top:2pt;padding-bottom:2pt;">37.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.2.4" style="padding-top:2pt;padding-bottom:2pt;">22.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.2.5" style="padding-top:2pt;padding-bottom:2pt;">35.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.2.6" style="padding-top:2pt;padding-bottom:2pt;">42.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.2.7" style="padding-top:2pt;padding-bottom:2pt;">56.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.2.8" style="padding-top:2pt;padding-bottom:2pt;">31.7</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.1.3">
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">Human-annotated</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.3.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T7.1.1.3.2.1" style="color:#FF0000;">51.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.3.3" style="padding-top:2pt;padding-bottom:2pt;">35.9</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.3.4" style="padding-top:2pt;padding-bottom:2pt;">35.1</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.3.5" style="padding-top:2pt;padding-bottom:2pt;">46.4</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.3.6" style="padding-top:2pt;padding-bottom:2pt;">57.5</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.3.7" style="padding-top:2pt;padding-bottom:2pt;">31.5</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.1.4">
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">Improvement</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.4.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.4.2.1">35%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.4.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.4.3.1">60%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.4.4" style="padding-top:2pt;padding-bottom:2pt;">-1%</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.4.5" style="padding-top:2pt;padding-bottom:2pt;">10%</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.4.6" style="padding-top:2pt;padding-bottom:2pt;">2%</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.4.7" style="padding-top:2pt;padding-bottom:2pt;">-1%</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T7.1.1.5.1" rowspan="3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T7.1.1.5.1.1">GPT-4-Turbo</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.5.2" style="padding-top:2pt;padding-bottom:2pt;">OpenAI Retrieval @5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.5.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T7.1.1.5.3.1" style="color:#FF0000;">45.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.5.4" style="padding-top:2pt;padding-bottom:2pt;">43.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.5.5" style="padding-top:2pt;padding-bottom:2pt;">40.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.5.6" style="padding-top:2pt;padding-bottom:2pt;">45.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.5.7" style="padding-top:2pt;padding-bottom:2pt;">61.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.5.8" style="padding-top:2pt;padding-bottom:2pt;">37.4</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.1.6">
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.6.1" style="padding-top:2pt;padding-bottom:2pt;">Human-annotated</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.6.2" style="padding-top:2pt;padding-bottom:2pt;">69.4</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.6.3" style="padding-top:2pt;padding-bottom:2pt;">57.7</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.6.4" style="padding-top:2pt;padding-bottom:2pt;">42.0</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.6.5" style="padding-top:2pt;padding-bottom:2pt;">56.5</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.6.6" style="padding-top:2pt;padding-bottom:2pt;">59.5</td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.6.7" style="padding-top:2pt;padding-bottom:2pt;">39.0</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.1.1.7.1" style="padding-top:2pt;padding-bottom:2pt;">Improvement</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.1.1.7.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.7.2.1">51%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.1.1.7.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.7.3.1">33%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.1.1.7.4" style="padding-top:2pt;padding-bottom:2pt;">4%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.1.1.7.5" style="padding-top:2pt;padding-bottom:2pt;">23%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.1.1.7.6" style="padding-top:2pt;padding-bottom:2pt;">-3%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.1.1.7.7" style="padding-top:2pt;padding-bottom:2pt;">4%</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">We also conduct end-to-end experiments to verify the impact of the retrieval quality. We evaluate the answer quality with top-5 chunks retrieved using OpenAI embeddings or with the human-annotated evidential chunks.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.T7" title="Table 7 ‣ 4.2 Indexing and Retrieval ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">7</span></a>, providing more relevant context to LLMs improves the answers in most cases, particularly in arithmetic-reasoning tasks such as FinHybrid and TatHybrid. Interestingly, for the FinHybrid dataset, the Llama-3-8B model achieved a score of 51.0 when given accurate context, outperforming GPT-4 with retrieved chunks. However, for knowledge-based questions from the NqText and FetaTab, the answer quality remains less affected. This is because complex numerical reasoning demands more precise evidence for accurate arithmetic operations,
whereas LLMs can leverage a wide range of narrative information to derive answers to knowledge-based questions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Remark.</span> We found that the model scaling law may not hold true in retrieval scenarios. The retrieval quality does matter, particularly in arithmetic tasks, but the incremental benefit of including additional chunks diminishes. Some use cases (e.g., queries that involve exact entity matching) may prefer some specific data embedding or indexing mechanisms.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>RAG vs. Long Context</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We compare RAG-based methods with long-context LLMs, utilizing GPT-4-Turbo with a 128k context window and Qwen-1.5-7B with a 32k context window. Due to the high cost of long context inference, we conduct this experiment on a subset of 600 documents (more details in Appendix  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A2.SS3" title="B.3 Long-context Policy ‣ Appendix B Experimental Details ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">B.3</span></a>).
The results are demonstrated in Table  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.T8" title="Table 8 ‣ 4.3 RAG vs. Long Context ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Performance scores between long-context and RAG mechanism. </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T8.1" style="width:390.3pt;height:70.8pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.6pt,10.0pt) scale(0.778153328715974,0.778153328715974) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T8.1.1">
<tr class="ltx_tr" id="S4.T8.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">LLM Name</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.2" style="padding-top:2pt;padding-bottom:2pt;">Input Type</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.3" style="padding-top:2pt;padding-bottom:2pt;">FinHybrid</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.4" style="padding-top:2pt;padding-bottom:2pt;">TatHybrid</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.5" style="padding-top:2pt;padding-bottom:2pt;">PaperTab</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.6" style="padding-top:2pt;padding-bottom:2pt;">PaperText</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.7" style="padding-top:2pt;padding-bottom:2pt;">FetaTab</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.8" style="padding-top:2pt;padding-bottom:2pt;">NqText</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.2.1" rowspan="2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T8.1.1.2.1.1">Qwen-1.5-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.2.2" style="padding-top:2pt;padding-bottom:2pt;">OpenAI Retrieval @5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.2.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.2.3.1">21.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.2.4" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.2.4.1">26.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.2.5" style="padding-top:2pt;padding-bottom:2pt;">31.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.2.6" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.2.6.1">39.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.2.7" style="padding-top:2pt;padding-bottom:2pt;">58.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.2.8" style="padding-top:2pt;padding-bottom:2pt;">32.4</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.3">
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">Long Context</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.3.2" style="padding-top:2pt;padding-bottom:2pt;">3.0</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.3.3" style="padding-top:2pt;padding-bottom:2pt;">20.9</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.3.4" style="padding-top:2pt;padding-bottom:2pt;">26.3</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.3.5" style="padding-top:2pt;padding-bottom:2pt;">33.1</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.3.6" style="padding-top:2pt;padding-bottom:2pt;">58.7</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.3.7" style="padding-top:2pt;padding-bottom:2pt;">30.2</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T8.1.1.4.1" rowspan="2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T8.1.1.4.1.1">GPT-4-Turbo</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.4.2" style="padding-top:2pt;padding-bottom:2pt;">OpenAI Retrieval @5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.4.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.4.3.1">43.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.4.4" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.4.4.1">46.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.4.5" style="padding-top:2pt;padding-bottom:2pt;">43.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.4.6" style="padding-top:2pt;padding-bottom:2pt;">47.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.4.7" style="padding-top:2pt;padding-bottom:2pt;">61.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.4.8" style="padding-top:2pt;padding-bottom:2pt;">35.8</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.1.1.5.1" style="padding-top:2pt;padding-bottom:2pt;">Long Context</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.1.1.5.2" style="padding-top:2pt;padding-bottom:2pt;">37.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.1.1.5.3" style="padding-top:2pt;padding-bottom:2pt;">36.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.1.1.5.4" style="padding-top:2pt;padding-bottom:2pt;">43.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.1.1.5.5" style="padding-top:2pt;padding-bottom:2pt;">47.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.1.1.5.6" style="padding-top:2pt;padding-bottom:2pt;">63.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.1.1.5.7" style="padding-top:2pt;padding-bottom:2pt;">35.4</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Remark. </span> We notice that for free-form or knowledge-based tasks (i.e., paper-based and wiki-based Q&amp;A), RAG and long-context solutions demonstrate comparable capability.
Conversely, in tasks with more numerical reasoning (i.e., financial Q&amp;A), long-context LLMs fail to match the performance of RAG. In such use cases, the excess of verbose content might hinder long-context LLMs in pinpointing facts and performing numerical reasoning effectively (see an example in Table  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.T10" title="Table 10 ‣ 4.4 Evaluating Chain-of-Thought and Code Interpreters ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">10</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluating Chain-of-Thought and Code Interpreters</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In real-world analytical queries, the reasoning capabilities can be essential, yet LLMs face limitations in this area <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib24" title="">24</a>]</cite>. To overcome these constraints, advanced methods such as Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib55" title="">55</a>]</cite> and Code-Interpreter (CI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib62" title="">62</a>]</cite> have been introduced. CoT prompts LLMs to generate a series of intermediate reasoning steps, whereas CI lets the LLM produce executable codes and then invoke an external executor to derive the answer. In this section, we evaluate the efficacy of basic generation, the CoT approach, and CI methods using the numerical reasoning dataset FinHybrid. We use the top-5 chunks retrieved with OpenAI’s embedding as the context and benchmark GPT-4-Turbo, Llama-3-8B, and the code-tailored CodeLlama models. The CoT approaches are implemented with step-wise instructive prompts, while the CI method asks LLMs to produce Python codes if necessary (more details in Appendix  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A2.SS1" title="B.1 Prompt Templates for LLMs ‣ Appendix B Experimental Details ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">B.1</span></a>).</p>
</div>
<figure class="ltx_table" id="S4.T9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Exact-match scores of different generation strategies on the FinHybrid dataset</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T9.1" style="width:151.8pt;height:72.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.8pt,8.5pt) scale(0.810036895765066,0.810036895765066) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T9.1.1">
<tr class="ltx_tr" id="S4.T9.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T9.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">LLM Name</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T9.1.1.1.2" style="padding-top:2pt;padding-bottom:2pt;">Base</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T9.1.1.1.3" style="padding-top:2pt;padding-bottom:2pt;">CoT</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T9.1.1.1.4" style="padding-top:2pt;padding-bottom:2pt;">Code</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.1.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">Llama-3-8B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.1.1.2.2" style="padding-top:2pt;padding-bottom:2pt;">21.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.1.1.2.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.2.3.1">37.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.1.1.2.4" style="padding-top:2pt;padding-bottom:2pt;">26.4</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.1.3">
<td class="ltx_td ltx_align_center" id="S4.T9.1.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">CodeLlama-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.1.3.2" style="padding-top:2pt;padding-bottom:2pt;">5.5</td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.1.3.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.3.3.1">7.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.1.3.4" style="padding-top:2pt;padding-bottom:2pt;">5.5</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.1.4">
<td class="ltx_td ltx_align_center" id="S4.T9.1.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">CodeLlama-13B</td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.1.4.2" style="padding-top:2pt;padding-bottom:2pt;">10.6</td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.1.4.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.4.3.1">11.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.1.4.4" style="padding-top:2pt;padding-bottom:2pt;">9.1</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T9.1.1.5.1" style="padding-top:2pt;padding-bottom:2pt;">GPT-4-Turbo</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T9.1.1.5.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.5.2.1">45.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T9.1.1.5.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.5.3.1">45.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T9.1.1.5.4" style="padding-top:2pt;padding-bottom:2pt;">32.2</td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S4.T10">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Case study of long-context and generating strategy. RAG outperforms the long-context through more accurate evidence retrieval, and CoT’s superiority is attributed to the integration of explicit reasoning steps.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T10.1" style="width:433.6pt;height:122.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-81.7pt,22.9pt) scale(0.726195357368785,0.726195357368785) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T10.1.1">
<tr class="ltx_tr" id="S4.T10.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Question</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">LLM Name</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">Strategy</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">LLM’s Response</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.1.1.2.1" rowspan="2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T10.1.1.2.1.1"><span class="ltx_text" id="S4.T10.1.1.2.1.1.1"></span> <span class="ltx_text" id="S4.T10.1.1.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T10.1.1.2.1.1.2.1">
<span class="ltx_tr" id="S4.T10.1.1.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T10.1.1.2.1.1.2.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">What percentage of contractual obligations</span></span>
<span class="ltx_tr" id="S4.T10.1.1.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T10.1.1.2.1.1.2.1.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">is due to maturities of long-term debt?</span></span>
</span></span> <span class="ltx_text" id="S4.T10.1.1.2.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.1.1.2.2" rowspan="2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T10.1.1.2.2.1">Qwen-1.5-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.1.1.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">long-context</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.1.1.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">33% ✗</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.1.3">
<td class="ltx_td ltx_align_center" id="S4.T10.1.1.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T10.1.1.3.1.1">RAG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.1.1.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">690 million out of 1416 million, 49% ✓</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T10.1.1.4.1" rowspan="5" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T10.1.1.4.1.1"><span class="ltx_text" id="S4.T10.1.1.4.1.1.1"></span> <span class="ltx_text" id="S4.T10.1.1.4.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T10.1.1.4.1.1.2.1">
<span class="ltx_tr" id="S4.T10.1.1.4.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T10.1.1.4.1.1.2.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">What percent of the share-based compensation</span></span>
<span class="ltx_tr" id="S4.T10.1.1.4.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T10.1.1.4.1.1.2.1.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">expense was related to stock options?</span></span>
</span></span> <span class="ltx_text" id="S4.T10.1.1.4.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T10.1.1.4.2" rowspan="5" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T10.1.1.4.2.1">Llama-3-8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.1.1.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">basic-gen</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.1.1.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">100% ✗</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.1.5">
<td class="ltx_td ltx_align_center" id="S4.T10.1.1.5.1" style="padding-bottom:4.0pt;padding-top:2.5pt;padding-bottom:2.5pt;">code</td>
<td class="ltx_td ltx_align_center" id="S4.T10.1.1.5.2" style="padding-bottom:4.0pt;padding-top:2.5pt;padding-bottom:2.5pt;">94.4% (no code is generated, directly response) ✗</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.1.1.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T10.1.1.6.1.1">CoT</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.1.1.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_text" id="S4.T10.1.1.6.2.1"></span> <span class="ltx_text" id="S4.T10.1.1.6.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T10.1.1.6.2.2.1">
<span class="ltx_tr" id="S4.T10.1.1.6.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T10.1.1.6.2.2.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">To find the percentage, we can divide the</span></span>
<span class="ltx_tr" id="S4.T10.1.1.6.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T10.1.1.6.2.2.1.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">portion related to stock options by the total</span></span>
<span class="ltx_tr" id="S4.T10.1.1.6.2.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T10.1.1.6.2.2.1.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">share-based compensation expense</span></span>
<span class="ltx_tr" id="S4.T10.1.1.6.2.2.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T10.1.1.6.2.2.1.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">($7 million / $36 million) x 100 = 19.4% ✓</span></span>
</span></span><span class="ltx_text" id="S4.T10.1.1.6.2.3"></span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Remark. </span> As illustrated in Table  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.T9" title="Table 9 ‣ 4.4 Evaluating Chain-of-Thought and Code Interpreters ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">9</span></a>, the Chain-of-Thought (CoT) approach outperforms others across all model configurations (see an example in Table  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.T10" title="Table 10 ‣ 4.4 Evaluating Chain-of-Thought and Code Interpreters ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">10</span></a>). The Llama-3-8B model shows improvements when incorporating codes, yet CoT methods still prove superior.
CodeLlama models, however, struggle with generating viable code in this scenario with lengthy context and ambiguous code-gen instructions. GPT-4-turbo exhibits a native ability to produce step-by-step explanations, leading to equivalent performance between basic generation and CoT methods. It is worth noting that while code interpreter has been demonstrated capable in handling numerical and tabular data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib65" title="">65</a>]</cite>, its efficacy in document analysis is hindered by the unstructured tables and lengthy context.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>End-to-End Evaluations</h3>
<div class="ltx_para ltx_noindent" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Assembling the insights derived from the above analyses, we construct an end-to-end RAG pipeline. Specifically, we parse unstructured data from PDFs leveraging the raw-text extraction approach, and employ OpenAI’s text-embedding-3-large model to index and retrieve relevant data chunks. In the generation phase, we incorporate the Chain-of-Thought approach to handle arithmetic-intensive tasks.
Based on this end-to-end pipeline, we evaluate 8 LLMs spanning various model sizes and architectures.</p>
</div>
<figure class="ltx_table" id="S4.T11">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>End-to-end performance scores of different LLMs </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T11.1" style="width:359.9pt;height:131.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-42.8pt,15.6pt) scale(0.807967683428813,0.807967683428813) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T11.1.1">
<tr class="ltx_tr" id="S4.T11.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T11.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">LLM Name</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T11.1.1.1.2" style="padding-top:2pt;padding-bottom:2pt;">Avg</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T11.1.1.1.3" style="padding-top:2pt;padding-bottom:2pt;">FinHybrid</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T11.1.1.1.4" style="padding-top:2pt;padding-bottom:2pt;">TatHybrid</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T11.1.1.1.5" style="padding-top:2pt;padding-bottom:2pt;">PaperTab</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T11.1.1.1.6" style="padding-top:2pt;padding-bottom:2pt;">PaperText</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T11.1.1.1.7" style="padding-top:2pt;padding-bottom:2pt;">FetaTab</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T11.1.1.1.8" style="padding-top:2pt;padding-bottom:2pt;">NqText</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.1.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">GPT-4-turbo</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.1.1.2.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T11.1.1.2.2.1">45.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.1.1.2.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T11.1.1.2.3.1">45.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.1.1.2.4" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T11.1.1.2.4.1">43.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.1.1.2.5" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T11.1.1.2.5.1">40.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.1.1.2.6" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T11.1.1.2.6.1">45.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.1.1.2.7" style="padding-top:2pt;padding-bottom:2pt;">61.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.1.1.2.8" style="padding-top:2pt;padding-bottom:2pt;">37.4</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.1.3">
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">Llama-3-70B</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.3.2" style="padding-top:2pt;padding-bottom:2pt;">42.5</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.3.3" style="padding-top:2pt;padding-bottom:2pt;">43.5</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.3.4" style="padding-top:2pt;padding-bottom:2pt;">30.9</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.3.5" style="padding-top:2pt;padding-bottom:2pt;">38.7</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.3.6" style="padding-top:2pt;padding-bottom:2pt;">44.4</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.3.7" style="padding-top:2pt;padding-bottom:2pt;">63.3</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.3.8" style="padding-top:2pt;padding-bottom:2pt;">33.9</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.1.4">
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">GPT-3.5</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.4.2" style="padding-top:2pt;padding-bottom:2pt;">40.9</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.4.3" style="padding-top:2pt;padding-bottom:2pt;">36.6</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.4.4" style="padding-top:2pt;padding-bottom:2pt;">33.9</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.4.5" style="padding-top:2pt;padding-bottom:2pt;">35.5</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.4.6" style="padding-top:2pt;padding-bottom:2pt;">42.1</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.4.7" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T11.1.1.4.7.1">64.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.4.8" style="padding-top:2pt;padding-bottom:2pt;">33.1</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.1.5">
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.5.1" style="padding-top:2pt;padding-bottom:2pt;">Qwen-1.5-32B</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.5.2" style="padding-top:2pt;padding-bottom:2pt;">38.9</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.5.3" style="padding-top:2pt;padding-bottom:2pt;">31.3</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.5.4" style="padding-top:2pt;padding-bottom:2pt;">27.9</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.5.5" style="padding-top:2pt;padding-bottom:2pt;">31.6</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.5.6" style="padding-top:2pt;padding-bottom:2pt;">43.1</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.5.7" style="padding-top:2pt;padding-bottom:2pt;">58.4</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.5.8" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T11.1.1.5.8.1">41.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.1.6">
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.6.1" style="padding-top:2pt;padding-bottom:2pt;">Llama-3-8B</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.6.2" style="padding-top:2pt;padding-bottom:2pt;">37.7</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.6.3" style="padding-top:2pt;padding-bottom:2pt;">37.9</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.6.4" style="padding-top:2pt;padding-bottom:2pt;">22.5</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.6.5" style="padding-top:2pt;padding-bottom:2pt;">35.5</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.6.6" style="padding-top:2pt;padding-bottom:2pt;">42.3</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.6.7" style="padding-top:2pt;padding-bottom:2pt;">56.6</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.6.8" style="padding-top:2pt;padding-bottom:2pt;">31.7</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.1.7">
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.7.1" style="padding-top:2pt;padding-bottom:2pt;">Mixtral-8x7B-v0.1</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.7.2" style="padding-top:2pt;padding-bottom:2pt;">34.5</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.7.3" style="padding-top:2pt;padding-bottom:2pt;">28.4</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.7.4" style="padding-top:2pt;padding-bottom:2pt;">22.5</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.7.5" style="padding-top:2pt;padding-bottom:2pt;">35.0</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.7.6" style="padding-top:2pt;padding-bottom:2pt;">38.1</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.7.7" style="padding-top:2pt;padding-bottom:2pt;">54.1</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.7.8" style="padding-top:2pt;padding-bottom:2pt;">29.1</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.1.8">
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.8.1" style="padding-top:2pt;padding-bottom:2pt;">Qwen-1.5-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.8.2" style="padding-top:2pt;padding-bottom:2pt;">33.8</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.8.3" style="padding-top:2pt;padding-bottom:2pt;">17.0</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.8.4" style="padding-top:2pt;padding-bottom:2pt;">22.6</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.8.5" style="padding-top:2pt;padding-bottom:2pt;">28.0</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.8.6" style="padding-top:2pt;padding-bottom:2pt;">37.7</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.8.7" style="padding-top:2pt;padding-bottom:2pt;">58.6</td>
<td class="ltx_td ltx_align_center" id="S4.T11.1.1.8.8" style="padding-top:2pt;padding-bottom:2pt;">38.9</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.1.9">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T11.1.1.9.1" style="padding-top:2pt;padding-bottom:2pt;">Mistral-7B-v0.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T11.1.1.9.2" style="padding-top:2pt;padding-bottom:2pt;">26.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T11.1.1.9.3" style="padding-top:2pt;padding-bottom:2pt;">18.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T11.1.1.9.4" style="padding-top:2pt;padding-bottom:2pt;">15.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T11.1.1.9.5" style="padding-top:2pt;padding-bottom:2pt;">20.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T11.1.1.9.6" style="padding-top:2pt;padding-bottom:2pt;">22.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T11.1.1.9.7" style="padding-top:2pt;padding-bottom:2pt;">54.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T11.1.1.9.8" style="padding-top:2pt;padding-bottom:2pt;">27.3</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.p2.1.1">Remark. </span> Table  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.T11" title="Table 11 ‣ 4.5 End-to-End Evaluations ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">11</span></a> presents the results, where GPT-4 leads in overall performance. GPT-3.5 and Qwen-1.5-32B excel in FetaTab and NqText, respectively.
The overall end-to-end scores demonstrate the challenges of our benchmark and also indicate considerable room for improvement in real-world document analysis for both RAG and LLMs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we propose a novel benchmark to assess Retrieval Augmented Generation (RAG) methodologies in real-world document analysis scenarios. Our benchmark features diverse question types and encompasses thousands of unstructured documents with expert labels from financial and other domains; meanwhile, we discussed interesting findings from our initial evaluations. We hope the benchmark will shed light on future research and production in unstructured document analyses.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib1.1.1">Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.</span>
</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.2.1">arXiv preprint arXiv:2303.08774</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib2.1.1">Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H.</span>
</span>
<span class="ltx_bibblock">Self-RAG: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.2.1">The Twelfth International Conference on Learning Representations</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib3.1.1">Baek, J., Aji, A. F., and Saffari, A.</span>
</span>
<span class="ltx_bibblock">Knowledge-augmented language model prompting for zero-shot knowledge graph question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.2.1">arXiv preprint arXiv:2306.04136</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib4.1.1">Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al.</span>
</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.2.1">arXiv preprint arXiv:2309.16609</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib5.1.1">Baviskar, D., Ahirrao, S., Potdar, V., and Kotecha, K.</span>
</span>
<span class="ltx_bibblock">Efficient automated processing of the unstructured documents using artificial intelligence: A systematic literature review and future directions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.2.1">IEEE Access 9</span> (2021), 72894–72936.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib6.1.1">Brown, D.</span>
</span>
<span class="ltx_bibblock">Rank-bm25: A two line search engine, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib7.1.1">Cai, Z., Cao, M., Chen, H., Chen, K., Chen, K., Chen, X., Chen, X., Chen, Z., Chen, Z., Chu, P., et al.</span>
</span>
<span class="ltx_bibblock">Internlm2 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.2.1">arXiv preprint arXiv:2403.17297</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib8.1.1">Chen, J., Lin, H., Han, X., and Sun, L.</span>
</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.2.1">Proceedings of the AAAI Conference on Artificial Intelligence</span> (2024), vol. 38, pp. 17754–17762.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib9.1.1">Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H., and Wang, W.</span>
</span>
<span class="ltx_bibblock">Hybridqa: A dataset of multi-hop question answering over tabular and textual data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.2.1">arXiv preprint arXiv:2004.07347</span> (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib10.1.1">Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J.</span>
</span>
<span class="ltx_bibblock">LongloRA: Efficient fine-tuning of long-context large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.2.1">The Twelfth International Conference on Learning Representations</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib11.1.1">Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.-H., Routledge, B., et al.</span>
</span>
<span class="ltx_bibblock">Finqa: A dataset of numerical reasoning over financial data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.2.1">arXiv preprint arXiv:2109.00122</span> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib12.1.1">Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A., and Gardner, M.</span>
</span>
<span class="ltx_bibblock">A dataset of information-seeking questions and answers anchored in research papers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.2.1">arXiv preprint arXiv:2105.03011</span> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib13.1.1">Ding, Y., Luo, S., Chung, H., and Han, S. C.</span>
</span>
<span class="ltx_bibblock">Vqa: A new dataset for real-world vqa on pdf documents.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.2.1">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</span> (2023), Springer, pp. 585–601.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib14.1.1">Gao, T., Yen, H., Yu, J., and Chen, D.</span>
</span>
<span class="ltx_bibblock">Enabling large language models to generate text with citations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.2.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span> (Singapore, Dec. 2023), H. Bouamor, J. Pino, and K. Bali, Eds., Association for Computational Linguistics, pp. 6465–6488.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib15.1.1">Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H.</span>
</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.2.1">arXiv preprint arXiv:2312.10997</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib16.1.1">Ge, Z., Liu, S., Wang, F., Li, Z., and Sun, J.</span>
</span>
<span class="ltx_bibblock">Yolox: Exceeding yolo series in 2021.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.2.1">arXiv preprint arXiv:2107.08430</span> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib17.1.1">Iyyer, M., Yih, W.-t., and Chang, M.-W.</span>
</span>
<span class="ltx_bibblock">Search-based neural structured learning for sequential question answering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.2.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span> (2017), pp. 1821–1831.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib18.1.1">Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.</span>
</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.2.1">arXiv preprint arXiv:2310.06825</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib19.1.1">Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al.</span>
</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.2.1">arXiv preprint arXiv:2401.04088</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib20.1.1">Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L.</span>
</span>
<span class="ltx_bibblock">LLMLingua: Compressing prompts for accelerated inference of large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.2.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span> (Singapore, Dec. 2023), H. Bouamor, J. Pino, and K. Bali, Eds., Association for Computational Linguistics, pp. 13358–13376.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib21.1.1">Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L.</span>
</span>
<span class="ltx_bibblock">Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.2.1">arXiv preprint arXiv:2310.06839</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib22.1.1">Jiang, Z., Xu, F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan, J., and Neubig, G.</span>
</span>
<span class="ltx_bibblock">Active retrieval augmented generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.2.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span> (Singapore, Dec. 2023), Association for Computational Linguistics, pp. 7969–7992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib23.1.1">Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L.</span>
</span>
<span class="ltx_bibblock">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.2.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span> (Vancouver, Canada, July 2017), R. Barzilay and M.-Y. Kan, Eds., Association for Computational Linguistics, pp. 1601–1611.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib24.1.1">Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R., and McHardy, R.</span>
</span>
<span class="ltx_bibblock">Challenges and applications of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.2.1">arXiv preprint arXiv:2307.10169</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib25.1.1">Kočiský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E.</span>
</span>
<span class="ltx_bibblock">The NarrativeQA reading comprehension challenge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.2.1">Transactions of the Association for Computational Linguistics 6</span> (2018), 317–328.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib26.1.1">Kong, K., Zhang, J., Shen, Z., Srinivasan, B., Lei, C., Faloutsos, C., Rangwala, H., and Karypis, G.</span>
</span>
<span class="ltx_bibblock">Opentab: Advancing large language models as open-domain table reasoners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.2.1">The Twelfth International Conference on Learning Representations</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib27.1.1">Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al.</span>
</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.2.1">Transactions of the Association for Computational Linguistics 7</span> (2019), 453–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib28.1.1">Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S.</span>
</span>
<span class="ltx_bibblock">Natural questions: A benchmark for question answering research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.2.1">Transactions of the Association for Computational Linguistics 7</span> (2019), 452–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib29.1.1">Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al.</span>
</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.2.1">Advances in Neural Information Processing Systems 33</span> (2020), 9459–9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib30.1.1">Li, X., Liu, Z., Xiong, C., Yu, S., Gu, Y., Liu, Z., and Yu, G.</span>
</span>
<span class="ltx_bibblock">Structure-aware language model pretraining improves dense retrieval on structured data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.2.1">Findings of the Association for Computational Linguistics: ACL 2023</span> (Toronto, Canada, 2023), A. Rogers, J. Boyd-Graber, and N. Okazaki, Eds., Association for Computational Linguistics, pp. 11560–11574.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib31.1.1">Llama, M.</span>
</span>
<span class="ltx_bibblock">Meta-llama-3, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib32.1.1">Lyu, Y., Li, Z., Niu, S., Xiong, F., Tang, B., Wang, W., Wu, H., Liu, H., Xu, T., and Chen, E.</span>
</span>
<span class="ltx_bibblock">Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.2.1">arXiv preprint arXiv:2401.17043</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib33.1.1">Mathew, M., Karatzas, D., and Jawahar, C.</span>
</span>
<span class="ltx_bibblock">Docvqa: A dataset for vqa on document images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.2.1">Proceedings of the IEEE/CVF winter conference on applications of computer vision</span> (2021), pp. 2200–2209.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib34.1.1">Nan, L., Hsieh, C., Mao, Z., Lin, X. V., Verma, N., Zhang, R., Kryściński, W., Schoelkopf, H., Kong, R., Tang, X., et al.</span>
</span>
<span class="ltx_bibblock">Fetaqa: Free-form table question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.2.1">Transactions of the Association for Computational Linguistics 10</span> (2022), 35–49.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib35.1.1">OpenAI</span>.

</span>
<span class="ltx_bibblock">Gpt-4o, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib36.1.1">Pang, R. Y., Parrish, A., Joshi, N., Nangia, N., Phang, J., Chen, A., Padmakumar, V., Ma, J., Thompson, J., He, H., and Bowman, S.</span>
</span>
<span class="ltx_bibblock">QuALITY: Question answering with long input texts, yes!

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.2.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</span> (Seattle, United States, July 2022), M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz, Eds., Association for Computational Linguistics, pp. 5336–5358.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib37.1.1">Pasupat, P., and Liang, P.</span>
</span>
<span class="ltx_bibblock">Compositional semantic parsing on semi-structured tables.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.2.1">arXiv preprint arXiv:1508.00305</span> (2015).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib38.1.1">Paterson, M., and Dančík, V.</span>
</span>
<span class="ltx_bibblock">Longest common subsequences.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.2.1">International symposium on mathematical foundations of computer science</span> (1994), Springer, pp. 127–142.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib39.1.1">PyPDF</span>.

</span>
<span class="ltx_bibblock">Py-pdf: A pure-python pdf library capable of splitting, merging, cropping, and transforming the pages of pdf files, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib40.1.1">Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.</span>
</span>
<span class="ltx_bibblock">SQuAD: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib40.2.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</span> (Austin, Texas, Nov. 2016), Association for Computational Linguistics, pp. 2383–2392.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib41.1.1">Reimers, N., and Gurevych, I.</span>
</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.2.1">arXiv preprint arXiv:1908.10084</span> (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib42.1.1">Ren, R., Wang, Y., Qu, Y., Zhao, W. X., Liu, J., Tian, H., Wu, H., Wen, J.-R., and Wang, H.</span>
</span>
<span class="ltx_bibblock">Investigating the factual knowledge boundary of large language models with retrieval augmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.2.1">arXiv preprint arXiv:2307.11019</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib43.1.1">Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al.</span>
</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.2.1">arXiv preprint arXiv:2308.12950</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib44.1.1">Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., and Zaharia, M.</span>
</span>
<span class="ltx_bibblock">ColBERTv2: Effective and efficient retrieval via lightweight late interaction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.2.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</span> (Seattle, United States, July 2022), Association for Computational Linguistics, pp. 3715–3734.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib45.1.1">Sarmah, B., Mehta, D., Pasquali, S., and Zhu, T.</span>
</span>
<span class="ltx_bibblock">Towards reducing hallucination in extracting information from financial reports using large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.2.1">Proceedings of the Third International Conference on AI-ML Systems</span> (2023), pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib46.1.1">Shi, T., Li, L., Lin, Z., Yang, T., Quan, X., and Wang, Q.</span>
</span>
<span class="ltx_bibblock">Dual-feedback knowledge retrieval for task-oriented dialogue systems.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.2.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span> (Singapore, Dec. 2023), Association for Computational Linguistics, pp. 6566–6580.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib47.1.1">Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, X. V., Smith, N. A., Zettlemoyer, L., tau Yih, W., and Lewis, M.</span>
</span>
<span class="ltx_bibblock">In-context pretraining: Language modeling beyond document boundaries.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.2.1">The Twelfth International Conference on Learning Representations</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib48.1.1">Smith, R.</span>
</span>
<span class="ltx_bibblock">An overview of the tesseract ocr engine.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib48.2.1">Ninth international conference on document analysis and recognition (ICDAR 2007)</span> (2007), vol. 2, IEEE, pp. 629–633.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib49.1.1">Smock, B., Pesala, R., and Abraham, R.</span>
</span>
<span class="ltx_bibblock">PubTables-1M: Towards comprehensive table extraction from unstructured documents.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.2.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span> (June 2022), pp. 4634–4642.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib50.1.1">Tanaka, R., Nishida, K., and Yoshida, S.</span>
</span>
<span class="ltx_bibblock">Visualmrc: Machine reading comprehension on document images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib50.2.1">Proceedings of the AAAI Conference on Artificial Intelligence</span> (2021), vol. 35, pp. 13878–13888.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib51.1.1">Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.</span>
</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.2.1">arXiv preprint arXiv:2302.13971</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib52.1.1">Trotman, A., Puurula, A., and Burgess, B.</span>
</span>
<span class="ltx_bibblock">Improvements to bm25 and language models examined.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib52.2.1">Proceedings of the 19th Australasian Document Computing Symposium</span> (2014), pp. 58–65.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib53.1.1">Unstructured</span>.

</span>
<span class="ltx_bibblock">Open-source pre-processing tools for unstructured data, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib54.1.1">Wang, Z., Araki, J., Jiang, Z., Parvez, M. R., and Neubig, G.</span>
</span>
<span class="ltx_bibblock">Learning to filter context for retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.2.1">arXiv preprint arXiv:2311.08377</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib55.1.1">Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D.</span>
</span>
<span class="ltx_bibblock">Chain of thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.2.1">Advances in Neural Information Processing Systems</span> (2022), A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib56.1.1">Xu, F., Shi, W., and Choi, E.</span>
</span>
<span class="ltx_bibblock">RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib56.2.1">The Twelfth International Conference on Learning Representations</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib57.1.1">Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B.</span>
</span>
<span class="ltx_bibblock">Retrieval meets long context large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib57.2.1">The Twelfth International Conference on Learning Representations</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib58.1.1">Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D.</span>
</span>
<span class="ltx_bibblock">HotpotQA: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib58.2.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</span> (Brussels, Belgium, Oct.-Nov. 2018), Association for Computational Linguistics, pp. 2369–2380.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib59.1.1">Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G., Li, H., Zhu, J., Chen, J., Chang, J., et al.</span>
</span>
<span class="ltx_bibblock">Yi: Open foundation models by 01. ai.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib59.2.1">arXiv preprint arXiv:2403.04652</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib60.1.1">Yu, T., Wu, C.-S., Lin, X. V., Wang, B., Tan, Y. C., Yang, X., Radev, D., Socher, R., and Xiong, C.</span>
</span>
<span class="ltx_bibblock">Grappa: Grammar-augmented pre-training for table semantic parsing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib60.2.1">International Conference on Learning Representations</span> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib61.1.1">Yu, Z., Xiong, C., Yu, S., and Liu, Z.</span>
</span>
<span class="ltx_bibblock">Augmentation-adapted retriever improves generalization of language models as generic plug-in.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib61.2.1">Proceedings of ACL</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib62.1.1">Zhou, A., Wang, K., Lu, Z., Shi, W., Luo, S., Qin, Z., Lu, S., Jia, A., Song, L., Zhan, M., et al.</span>
</span>
<span class="ltx_bibblock">Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.2.1">arXiv preprint arXiv:2308.07921</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib63.1.1">Zhu, F., Lei, W., Feng, F., Wang, C., Zhang, H., and Chua, T.-S.</span>
</span>
<span class="ltx_bibblock">Towards complex document understanding by discrete reasoning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib63.2.1">Proceedings of the 30th ACM International Conference on Multimedia</span> (2022), pp. 4857–4866.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib64.1.1">Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., and Chua, T.-S.</span>
</span>
<span class="ltx_bibblock">TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib64.2.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span> (Online, Aug. 2021), Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib65.1.1">Zhu, F., Liu, Z., Feng, F., Wang, C., Li, M., and Chua, T.-S.</span>
</span>
<span class="ltx_bibblock">Tat-llm: A specialized language model for discrete reasoning over tabular and textual data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.2.1">arXiv preprint arXiv:2401.13223</span> (2024).

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset Example</h2>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="344" id="A1.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Data example of FinHybrid in UDA</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Figure  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A1.F4" title="Figure 4 ‣ Appendix A Dataset Example ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">4</span></a> presents a data item from the FinHybrid dataset, a prototypical representation common to all datasets within UDA. It comprises an original multi-page unstructured document containing tables and text, accompanied by related question-answer pairs. The datasets within UDA also feature additional explanations in different formats, such as the programmatic operation and factual evidence in FinHybrid.</p>
</div>
<figure class="ltx_table" id="A1.T12">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span>Prompt templates for LLMs in each dataset and task.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T12.1" style="width:433.6pt;height:876.5pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-80.8pt,163.3pt) scale(0.72839769357767,0.72839769357767) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T12.1.1">
<tr class="ltx_tr" id="A1.T12.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T12.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T12.1.1.1.2" style="padding-top:2pt;padding-bottom:2pt;">Prompt Template</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T12.1.1.2.1" rowspan="4" style="padding-bottom:18.0pt;padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="A1.T12.1.1.2.1.1">PaperTab and PaperText</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.2.2" style="padding-bottom:18.0pt;padding-top:2pt;padding-bottom:2pt;">System</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.2.3" style="padding-bottom:18.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.2.3.1"></span><span class="ltx_text" id="A1.T12.1.1.2.3.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.2.3.2.1">
<span class="ltx_tr" id="A1.T12.1.1.2.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.2.3.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">You are a scientific researcher, given a section of an academic paper, please</span></span>
<span class="ltx_tr" id="A1.T12.1.1.2.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.2.3.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">answer the question according to the context of the paper. The final answer output</span></span>
<span class="ltx_tr" id="A1.T12.1.1.2.3.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.2.3.2.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">should be in the format of "The answer is: &lt;answer&gt;", and the &lt;answer&gt; should</span></span>
<span class="ltx_tr" id="A1.T12.1.1.2.3.2.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.2.3.2.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">be concise with no explanation.</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.2.3.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.3">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.3.1" style="padding-bottom:7.0pt;padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.3.2" style="padding-bottom:7.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.3.2.1"></span><span class="ltx_text" id="A1.T12.1.1.3.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.3.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.3.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.3.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: … ### Question: Which Indian languages do they experiment</span></span>
<span class="ltx_tr" id="A1.T12.1.1.3.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.3.2.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">with? ### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.3.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.4">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.4.1" style="padding-bottom:6.0pt;padding-top:2pt;padding-bottom:2pt;">Assistant</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.4.2" style="padding-bottom:6.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.4.2.1"></span><span class="ltx_text" id="A1.T12.1.1.4.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.4.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.4.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.4.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">The answer is: Hindi, English, Kannada, Telugu, Assamese, Bengali</span></span>
<span class="ltx_tr" id="A1.T12.1.1.4.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.4.2.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">and Malayalam.</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.4.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.5">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.5.1" style="padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.5.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.5.2.1"></span><span class="ltx_text" id="A1.T12.1.1.5.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.5.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.5.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.5.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: <span class="ltx_text" id="A1.T12.1.1.5.2.2.1.1.1.1" style="color:#0000FF;">{context}</span> ### Question: <span class="ltx_text" id="A1.T12.1.1.5.2.2.1.1.1.2" style="color:#0000FF;">{question}</span> ### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.5.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T12.1.1.6.1" rowspan="4" style="padding-bottom:12.0pt;padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="A1.T12.1.1.6.1.1">FetaTab</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.6.2" style="padding-bottom:12.0pt;padding-top:2pt;padding-bottom:2pt;">System</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.6.3" style="padding-bottom:12.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.6.3.1"></span><span class="ltx_text" id="A1.T12.1.1.6.3.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.6.3.2.1">
<span class="ltx_tr" id="A1.T12.1.1.6.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.6.3.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Given a section of a document, plese answer the question according to the</span></span>
<span class="ltx_tr" id="A1.T12.1.1.6.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.6.3.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">context. The final answer output should be in the format of "The answer is:</span></span>
<span class="ltx_tr" id="A1.T12.1.1.6.3.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.6.3.2.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">&lt;answer&gt;", and the &lt;answer&gt; should be a natural sentence.</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.6.3.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.7">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.7.1" style="padding-bottom:7.0pt;padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.7.2" style="padding-bottom:7.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.7.2.1"></span><span class="ltx_text" id="A1.T12.1.1.7.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.7.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.7.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.7.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: … ### Question: When and in what play did Platt appear at the</span></span>
<span class="ltx_tr" id="A1.T12.1.1.7.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.7.2.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">Music Box Theatre? with? ### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.7.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.8">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.8.1" style="padding-bottom:6.0pt;padding-top:2pt;padding-bottom:2pt;">Assistant</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.8.2" style="padding-bottom:6.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.8.2.1"></span><span class="ltx_text" id="A1.T12.1.1.8.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.8.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.8.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.8.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">The answer is: In 2016 and 2017, Platt played in Dear Evan Hansen on</span></span>
<span class="ltx_tr" id="A1.T12.1.1.8.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.8.2.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">Broadway at the Music Box Theatre.</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.8.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.9">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.9.1" style="padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.9.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.9.2.1"></span><span class="ltx_text" id="A1.T12.1.1.9.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.9.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.9.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.9.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: <span class="ltx_text" id="A1.T12.1.1.9.2.2.1.1.1.1" style="color:#0000FF;">{context}</span> ### Question: <span class="ltx_text" id="A1.T12.1.1.9.2.2.1.1.1.2" style="color:#0000FF;">{question}</span> ### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.9.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T12.1.1.10.1" rowspan="4" style="padding-bottom:18.0pt;padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="A1.T12.1.1.10.1.1">NqText</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.10.2" style="padding-bottom:18.0pt;padding-top:2pt;padding-bottom:2pt;">System</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.10.3" style="padding-bottom:18.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.10.3.1"></span><span class="ltx_text" id="A1.T12.1.1.10.3.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.10.3.2.1">
<span class="ltx_tr" id="A1.T12.1.1.10.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.10.3.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Given a section of a document, plese answer the question according to the</span></span>
<span class="ltx_tr" id="A1.T12.1.1.10.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.10.3.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">context. The final answer output should be in the format of "The answer is:</span></span>
<span class="ltx_tr" id="A1.T12.1.1.10.3.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.10.3.2.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">&lt;answer&gt;", and the &lt;answer&gt; should be a paragraph from the context or a</span></span>
<span class="ltx_tr" id="A1.T12.1.1.10.3.2.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.10.3.2.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">summarized short phrase.</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.10.3.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.11">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.11.1" style="padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.11.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.11.2.1"></span><span class="ltx_text" id="A1.T12.1.1.11.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.11.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.11.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.11.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: … ### Question: When will tour de france teams be announced?</span></span>
<span class="ltx_tr" id="A1.T12.1.1.11.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.11.2.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.11.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.12">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.12.1" style="padding-top:2pt;padding-bottom:2pt;">Assistant</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.12.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.12.2.1"></span><span class="ltx_text" id="A1.T12.1.1.12.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.12.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.12.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.12.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">The answer is: 6 January 2018</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.12.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.13">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.13.1" style="padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.13.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.13.2.1"></span><span class="ltx_text" id="A1.T12.1.1.13.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.13.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.13.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.13.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: <span class="ltx_text" id="A1.T12.1.1.13.2.2.1.1.1.1" style="color:#0000FF;">{context}</span> ### Question: <span class="ltx_text" id="A1.T12.1.1.13.2.2.1.1.1.2" style="color:#0000FF;">{question}</span> ### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.13.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T12.1.1.14.1" rowspan="4" style="padding-bottom:23.0pt;padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="A1.T12.1.1.14.1.1"><span class="ltx_text" id="A1.T12.1.1.14.1.1.1"></span> <span class="ltx_text" id="A1.T12.1.1.14.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.14.1.1.2.1">
<span class="ltx_tr" id="A1.T12.1.1.14.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T12.1.1.14.1.1.2.1.1.1" style="padding-bottom:4.0pt;padding-top:2pt;padding-bottom:2pt;">FinHybrid and TatHybird</span></span>
<span class="ltx_tr" id="A1.T12.1.1.14.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T12.1.1.14.1.1.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">(Chain-of-Thought)</span></span>
</span></span> <span class="ltx_text" id="A1.T12.1.1.14.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.14.2" style="padding-bottom:23.0pt;padding-top:2pt;padding-bottom:2pt;">System</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.14.3" style="padding-bottom:23.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.14.3.1"></span><span class="ltx_text" id="A1.T12.1.1.14.3.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.14.3.2.1">
<span class="ltx_tr" id="A1.T12.1.1.14.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.14.3.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">You are a financial analyzer, given a section of a company’s annual report, please</span></span>
<span class="ltx_tr" id="A1.T12.1.1.14.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.14.3.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">answer the question according to the report context. Let’s do this step by step.</span></span>
<span class="ltx_tr" id="A1.T12.1.1.14.3.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.14.3.2.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">The final answer output should be in the format of "The answer is: &lt;answer&gt;", and</span></span>
<span class="ltx_tr" id="A1.T12.1.1.14.3.2.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.14.3.2.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">the &lt;answer&gt; must be simple and short (e.g. just an accurate numerical value</span></span>
<span class="ltx_tr" id="A1.T12.1.1.14.3.2.1.5">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.14.3.2.1.5.1" style="padding-top:2pt;padding-bottom:2pt;">or phrases).</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.14.3.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.15">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.15.1" style="padding-bottom:7.0pt;padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.15.2" style="padding-bottom:7.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.15.2.1"></span><span class="ltx_text" id="A1.T12.1.1.15.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.15.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.15.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.15.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: … ### Question: What is the average price of the products?</span></span>
<span class="ltx_tr" id="A1.T12.1.1.15.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.15.2.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.15.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.16">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.16.1" style="padding-bottom:6.0pt;padding-top:2pt;padding-bottom:2pt;">Assistant</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.16.2" style="padding-bottom:6.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.16.2.1"></span><span class="ltx_text" id="A1.T12.1.1.16.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.16.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.16.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.16.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">There are 8 products with a total price value of 1000 so the average value is</span></span>
<span class="ltx_tr" id="A1.T12.1.1.16.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.16.2.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">125.00. The answer is: 125.00</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.16.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.17">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.17.1" style="padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.17.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.17.2.1"></span><span class="ltx_text" id="A1.T12.1.1.17.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.17.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.17.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.17.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: <span class="ltx_text" id="A1.T12.1.1.17.2.2.1.1.1.1" style="color:#0000FF;">{context}</span> ### Question: <span class="ltx_text" id="A1.T12.1.1.17.2.2.1.1.1.2" style="color:#0000FF;">{question}</span> ### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.17.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.18">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T12.1.1.18.1" rowspan="4" style="padding-bottom:18.0pt;padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="A1.T12.1.1.18.1.1"><span class="ltx_text" id="A1.T12.1.1.18.1.1.1"></span> <span class="ltx_text" id="A1.T12.1.1.18.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.18.1.1.2.1">
<span class="ltx_tr" id="A1.T12.1.1.18.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T12.1.1.18.1.1.2.1.1.1" style="padding-bottom:4.0pt;padding-top:2pt;padding-bottom:2pt;">FinHybrid</span></span>
<span class="ltx_tr" id="A1.T12.1.1.18.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T12.1.1.18.1.1.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">(Basic Generation)</span></span>
</span></span> <span class="ltx_text" id="A1.T12.1.1.18.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.18.2" style="padding-bottom:18.0pt;padding-top:2pt;padding-bottom:2pt;">System</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.18.3" style="padding-bottom:18.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.18.3.1"></span><span class="ltx_text" id="A1.T12.1.1.18.3.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.18.3.2.1">
<span class="ltx_tr" id="A1.T12.1.1.18.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.18.3.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">You are a financial analyzer, given a section of a company’s annual report, please</span></span>
<span class="ltx_tr" id="A1.T12.1.1.18.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.18.3.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">answer the question according to the report context. The final answer output should</span></span>
<span class="ltx_tr" id="A1.T12.1.1.18.3.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.18.3.2.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">be in the format of ’The answer is: &lt;answer&gt;’, and the &lt;answer&gt; must be simple</span></span>
<span class="ltx_tr" id="A1.T12.1.1.18.3.2.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.18.3.2.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">and short (e.g. just an accurate numerical value or phrases).</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.18.3.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.19">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.19.1" style="padding-bottom:7.0pt;padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.19.2" style="padding-bottom:7.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.19.2.1"></span><span class="ltx_text" id="A1.T12.1.1.19.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.19.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.19.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.19.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: … ### Question: What is the average price of the products?</span></span>
<span class="ltx_tr" id="A1.T12.1.1.19.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.19.2.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.19.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.20">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.20.1" style="padding-top:2pt;padding-bottom:2pt;">Assistant</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.20.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.20.2.1"></span><span class="ltx_text" id="A1.T12.1.1.20.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.20.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.20.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.20.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">The answer is: 125.00</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.20.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.21">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.21.1" style="padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.21.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.21.2.1"></span><span class="ltx_text" id="A1.T12.1.1.21.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.21.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.21.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.21.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: <span class="ltx_text" id="A1.T12.1.1.21.2.2.1.1.1.1" style="color:#0000FF;">{context}</span> ### Question: <span class="ltx_text" id="A1.T12.1.1.21.2.2.1.1.1.2" style="color:#0000FF;">{question}</span> ### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.21.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.22">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T12.1.1.22.1" rowspan="7" style="padding-bottom:28.0pt;padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="A1.T12.1.1.22.1.1"><span class="ltx_text" id="A1.T12.1.1.22.1.1.1"></span> <span class="ltx_text" id="A1.T12.1.1.22.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.22.1.1.2.1">
<span class="ltx_tr" id="A1.T12.1.1.22.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T12.1.1.22.1.1.2.1.1.1" style="padding-bottom:4.0pt;padding-top:2pt;padding-bottom:2pt;">FinHybrid</span></span>
<span class="ltx_tr" id="A1.T12.1.1.22.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T12.1.1.22.1.1.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">(Code Interpreter)</span></span>
</span></span> <span class="ltx_text" id="A1.T12.1.1.22.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.22.2" style="padding-bottom:28.0pt;padding-top:2pt;padding-bottom:2pt;">System</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.1.22.3" style="padding-bottom:28.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.22.3.1"></span><span class="ltx_text" id="A1.T12.1.1.22.3.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.22.3.2.1">
<span class="ltx_tr" id="A1.T12.1.1.22.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.22.3.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Given a section of a company’s annual report and corresponding question, please</span></span>
<span class="ltx_tr" id="A1.T12.1.1.22.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.22.3.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">generate the python codes to calculate the answer. You should firstly extract and list</span></span>
<span class="ltx_tr" id="A1.T12.1.1.22.3.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.22.3.2.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">the relevant information from the context, and then write the arithmetical python</span></span>
<span class="ltx_tr" id="A1.T12.1.1.22.3.2.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.22.3.2.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">codes in the following block format: ```python &lt;python codes&gt; ```If the answer does</span></span>
<span class="ltx_tr" id="A1.T12.1.1.22.3.2.1.5">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.22.3.2.1.5.1" style="padding-top:2pt;padding-bottom:2pt;">not require any calculation, you should directly write the answer in the format of</span></span>
<span class="ltx_tr" id="A1.T12.1.1.22.3.2.1.6">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.22.3.2.1.6.1" style="padding-top:2pt;padding-bottom:2pt;">"The answer is: &lt;answer&gt;".</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.22.3.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.23">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.23.1" style="padding-bottom:7.0pt;padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.23.2" style="padding-bottom:7.0pt;padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.23.2.1"></span><span class="ltx_text" id="A1.T12.1.1.23.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.23.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.23.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.23.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: … ### Question: What is the average price of the products?</span></span>
<span class="ltx_tr" id="A1.T12.1.1.23.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.23.2.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.23.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.24">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.24.1" style="padding-top:2pt;padding-bottom:2pt;">Assistant</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.24.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.24.2.1"></span><span class="ltx_text" id="A1.T12.1.1.24.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.24.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.24.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.24.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">The price of product 1,2,3,4 is 700, and the price of product 5,6,7,8 is 900.</span></span>
<span class="ltx_tr" id="A1.T12.1.1.24.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.24.2.2.1.2.1" style="padding-top:2pt;padding-bottom:2pt;">The python code is ```python   products = [700, 700, 700, 700, 900, 900, 900, 900] \n</span></span>
<span class="ltx_tr" id="A1.T12.1.1.24.2.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.24.2.2.1.3.1" style="padding-top:2pt;padding-bottom:2pt;">total_price = sum(products) \n average_price = total_price / len(products) \n</span></span>
<span class="ltx_tr" id="A1.T12.1.1.24.2.2.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.24.2.2.1.4.1" style="padding-top:2pt;padding-bottom:2pt;">print(average_price)```</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.24.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.25">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.25.1" style="padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.25.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.25.2.1"></span><span class="ltx_text" id="A1.T12.1.1.25.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.25.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.25.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.25.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: … ### Question: What is the net income in 2009? ### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.25.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.26">
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.26.1" style="padding-top:2pt;padding-bottom:2pt;">Assistant</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.1.26.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.26.2.1"></span><span class="ltx_text" id="A1.T12.1.1.26.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.26.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.26.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.26.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">The answer is: 1000 million</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.26.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.1.27">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T12.1.1.27.1" style="padding-top:2pt;padding-bottom:2pt;">User</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T12.1.1.27.2" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_text" id="A1.T12.1.1.27.2.1"></span><span class="ltx_text" id="A1.T12.1.1.27.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T12.1.1.27.2.2.1">
<span class="ltx_tr" id="A1.T12.1.1.27.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T12.1.1.27.2.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">### Context: <span class="ltx_text" id="A1.T12.1.1.27.2.2.1.1.1.1" style="color:#0000FF;">{context}</span> ### Question: <span class="ltx_text" id="A1.T12.1.1.27.2.2.1.1.1.2" style="color:#0000FF;">{question}</span> ### Response:</span></span>
</span></span><span class="ltx_text" id="A1.T12.1.1.27.2.3"></span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Experimental Details</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Prompt Templates for LLMs</h3>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#A1.T12" title="Table 12 ‣ Appendix A Dataset Example ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">12</span></a> presents the prompt templates for LLM. We use the Chain-of-Thought approach for FinHybrid and TatHybrid datasets and also explore basic generation and Code-Interpreter strategies for FinHybrid, as detailed in Section  <a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#S4.SS4" title="4.4 Evaluating Chain-of-Thought and Code Interpreters ‣ 4 Benchmarks and Evaluations ‣ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis"><span class="ltx_text ltx_ref_tag">4.4</span></a>. The example instruction of the prompt omits full context, just using a Q&amp;A pair to guide LLM output toward the target answer format.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Settings of Indexing and Retrieval Experiments</h3>
<div class="ltx_para ltx_noindent" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">In our indexing and retrieval experiments, we first extract raw text from documents, then segment it into 3000-character (about 500 words) chunks using the recursive-split method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15187v1#bib.bib45" title="">45</a>]</cite>, ensuring a 10% overlap to mitigate information loss. Then the index is constructed on these chunks for the retrieval task. For evaluation, human-annotated factual evidence serves as the ground truth for retrieval. We measure evidence presence by calculating the ratio of the Longest Common Subsequence (LCS) length to the evidence length.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Long-context Policy</h3>
<div class="ltx_para ltx_noindent" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1">In our long-context experiments, we employ the Qwen-1.5-7B-32k and GPT4-Turbo-128k models, which can handle extended contexts but are still insufficient for quite long documents, such as financial reports exceeding 100k words (more than 150k tokens). For such cases, the strategy falls back to retrieving the top 30 most relevant data segments to serve as the input context. Due to the high cost of long context
inference, we conduct this experiment on a subset of 600 documents, each coupled with a corresponding Q&amp;A pair.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jun 21 14:21:44 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
