<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Retrieval Augmented Generation for Domain-specific Question Answering</title>
<!--Generated on Wed May 29 16:11:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.14760v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S1" title="In Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S2" title="In Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title">LLM-based question answering systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title">Retrieval augmented question answering systems</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3" title="In Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS1" title="In 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>An overview of our framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS2" title="In 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Retriever</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS2.SSS1" title="In 3.2 Retriever ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Retriever Training Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS2.SSS2" title="In 3.2 Retriever ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Retriever Model Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS3" title="In 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Retrieval Index Creation and Database</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS4" title="In 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Preprocessing for Building the Database</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS4.SSS1" title="In 3.4 Preprocessing for Building the Database ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>QA Generation Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS4.SSS2" title="In 3.4 Preprocessing for Building the Database ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Named Entity Removal Module</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS5" title="In 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Query Augmentation via Product Identification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS6" title="In 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>LLM prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS7" title="In 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>LLM Finetuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4" title="In Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.SS1" title="In 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Retriever Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.SS2" title="In 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Query Augmentation via Product Identification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.SS3" title="In 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Quantitative Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.SS4" title="In 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Qualitative Evaluation - Comparison of the generated answers</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.SS4.SSS0.Px1" title="In 4.4 Qualitative Evaluation - Comparison of the generated answers ‣ 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title">Example 1</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.SS4.SSS0.Px2" title="In 4.4 Qualitative Evaluation - Comparison of the generated answers ‣ 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title">Example 2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.SS4.SSS0.Px3" title="In 4.4 Qualitative Evaluation - Comparison of the generated answers ‣ 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title">Example 3</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.SS4.SSS0.Px4" title="In 4.4 Qualitative Evaluation - Comparison of the generated answers ‣ 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title">Example 4</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.SS4.SSS0.Px5" title="In 4.4 Qualitative Evaluation - Comparison of the generated answers ‣ 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title">Example 5</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S5" title="In Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">호</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\copyrightclause</span>
<p class="ltx_p" id="p2.2">Copyright for this paper by its authors.
Use permitted under Creative Commons License Attribution 4.0
International (CC BY 4.0).</p>
</div>
<div class="ltx_para" id="p3">
<span class="ltx_ERROR undefined" id="p3.1">\conference</span>
<p class="ltx_p" id="p3.2">AAAI 24’, Workshop on Scientific Document Understanding,
February 20–27, 2024, Vancouver, CAN</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">[orcid=0009-0003-2479-3041,
email=sanatsha@adobe.com
]
<span class="ltx_ERROR undefined" id="p4.1.1">\fnmark</span>[1]</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">[orcid=0000-0002-7262-3579,
email=syoon@adobe.com
]
<span class="ltx_ERROR undefined" id="p5.1.1">\fnmark</span>[1]</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p" id="p6.1">[orcid=0000-0002-1119-1346,
email=franck.dernoncourt@adobe.com
]
<span class="ltx_ERROR undefined" id="p6.1.1">\fnmark</span>[1]</p>
</div>
<div class="ltx_para" id="p7">
<p class="ltx_p" id="p7.1">[orcid=0000-0002-9421-8566,
email=sultania@adobe.com
]</p>
</div>
<div class="ltx_para" id="p8">
<p class="ltx_p" id="p8.1">[email=kbagga@adobe.com
]</p>
</div>
<div class="ltx_para" id="p9">
<p class="ltx_p" id="p9.1">[orcid=0000-0003-1446-4802,
email=mengjiaoz@adobe.com
]</p>
</div>
<div class="ltx_para" id="p10">
<p class="ltx_p" id="p10.1">[email=bui@adobe.com
]</p>
</div>
<div class="ltx_para" id="p11">
<p class="ltx_p" id="p11.1">[orcid=0009-0004-1944-7600,
email=vkotte@adobe.com
]</p>
</div>
<div class="ltx_para" id="p12">
<span class="ltx_ERROR undefined" id="p12.1">\fntext</span>
<p class="ltx_p" id="p12.2">[1]These authors contributed equally.</p>
</div>
<h1 class="ltx_title ltx_title_document">Retrieval Augmented Generation for Domain-specific Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sanat Sharma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Adobe Inc
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Seunghyun Yoon
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Franck Dernoncourt
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dewang Sultania
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Karishma Bagga
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mengjiao Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Trung Bui
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Varun Kotte
</span></span>
</div>
<div class="ltx_dates">(2022)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Question answering has become an important application in the advanced development of large language models. The general pre-trained large language models for question-answering are not trained to properly understand the knowledge or terminology for a specific domain, such as finance, healthcare, education, and customer service for a product. As these systems usually have no access to the latest information and cannot adjust to customized features the product expects, we build an in-house question-answering system for our Adobe products. We propose a novel framework to compile a large question-answer database and develop the approach for retrieval-aware finetuning of a Large Language model. Our overall approach reduces hallucinations during generation while keeping in context the latest retrieval information for contextual grounding.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Question answering <span class="ltx_ERROR undefined" id="id2.id1">\sep</span>retrieval <span class="ltx_ERROR undefined" id="id3.id2">\sep</span>large language model <span class="ltx_ERROR undefined" id="id4.id3">\sep</span>in-domain understanding

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the advanced natural language processing technologies, large language models (LLMs) have demonstrated impressive capabilities in a wide range of fields, including chatbots <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">thoppilan2022lamda</span>]</cite>, language translation <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">jiao2023chatgpt</span>]</cite>,
text summarization <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2023benchmarking</span>]</cite>, and personalized robot assistance. Question-answering has become an important application for Large Language Models (LLMs), enhancing the ability of chatbots in diverse domains such as healthcare, finance, education, and customer service <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kaddour2023challenges</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, traditional question-answering systems and general Question-Answering (QA) systems based on LLMs are not trained properly to understand the knowledge and the terminology of the specific field. They will face challenges in effectively retrieving and presenting relevant information, particularly in dynamically updated databases. For example, users’ questions about Adobe products are more related to how to use and the characteristics of the products. The question-answering systems outside Adobe products are not trained to properly understand Adobe terminology. Furthermore, the applications and features change frequently and the existing systems often don’t have access to the latest information on Adobe products. Therefore, they cannot provide in-product interactions (e.g., link a menu-item present in the answer to the actual menu
item in the product). Privacy risk is another problem if the data of the product is cloud-based. Furthermore, external solutions have trouble keeping up with the latest changes in Adobe products (e.g., a feature moving from beta to production).</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we develop a new framework to find the answers to the questions that users of Adobe products ask about Adobe products (e.g., "How can I remove all the highlights in a PDF with Acrobat?"). To address these challenges, our approach builds upon a retrieval-based question-answering system. Our smart retrieval
system retrieves the most useful up-to-date relevant content. The retrieval
system is trained in a self-supervised manner based on Adobe data and user behavioral data (clicks). We then propose a novel approach for retrieval-aware finetuning of a large language model to obtain the question-answering system for Adobe products. We systematically enhance this system to improve its question-answering performance, ensuring users receive timely and accurate information.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In summary, the contributions of this work can be highlighted as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a novel framework to compile a large question-answer database.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We proposed a full-fledged, advanced, and production-ready retrieval-based question-answering system, in which the retriever is trained based on user user behavioral data (clicks).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We develop a novel approach for retrieval-aware finetuning of a Large Language model and use a query augmentation and product disambiguation pipeline to improve retrieval and generation quality on vague queries.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Our overall approach reduces hallucinations during generation while keeping in context the latest retrieval information for contextual grounding.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">LLM-based question answering systems</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Large language models have been well developed recently and have shown extraordinary performance in many fields and applications. The models with billions of parameters and trained with huge data can assist humans in simple language understanding or generation tasks. To adapt to a specific task or be aligned with human preference, these models usually need to be finetuned <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kaddour2023challenges</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">brown2020language</span>]</cite>. Bakker et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">bakker2022fine</span>]</cite> finetune the large language models to align the output of the summarization with humans. In the application in our real life, Singhal et al. evaluate PaLM <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">chowdhery2022palm</span>]</cite> with multiple prompting strategies for medical question answering and comprehension. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every dataset in the benchmark MultiMedQA. Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">yu2022legal</span>]</cite> explore the reason-based prompting mechanism with large language models on the legal question-answering task. Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhou2023lima</span>]</cite> propose that strong large language models have already acquired knowledge and capabilities. The model only needs a small amount of data for instruction tuning.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Retrieval augmented question answering systems</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Although the large language model can generate fluent or natural text, it might be unfaithful to the source content, i.e., hallucination <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">ji2023survey</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">kaddour2023challenges</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">maynez2020faithfulness</span>]</cite>. Parametric knowledge bias is one of the reasons for hallucination in large language models <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">shuster2021retrieval</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">petroni2019language</span>]</cite>. To mitigate the hallucinations, there are some works that focus on incorporating retrieval-augmented architectures into the systems. Lewis et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lewis2020retrieval</span>]</cite> proposed RAG, a general-purpose fine-tuning recipe for retrieval-augmented generation. RAG combines pre-trained parametric and non-parametric memory for better language generation. Li et al <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2023chatdoctor</span>]</cite>. proposed a medical chat model, ChatDoctor, finetuned on LLaMA using
medical domain knowledge. The model is finetuned with the medical data from real-world patient-physician conversations and introductions. With the external knowledge base, the ChatDoctor model can retrieve online and offline medical domain knowledge to answer medical questions on up-to-date medical terms and diseases, mitigating the hallucinations and errors in LLMs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section will give more information on our framework. We will first introduce our retrieval system, which retrieves the most useful up-to-date relevant content. Pretrained model confuses Photoshop with Photoshop Express, Premiere Pro with Premiere Rush, etc. We propose query augmentation via product identification to improve overall accuracy and quality of generation. We finetune the large language model based on Adobe data and our trained retriever to understand the user’s question and context and present a useful response.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>An overview of our framework</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We use several data sources including Adobe Helpx document titles and descriptions, Adobe Community questions, LLM-generated questions from Helpx documents, and YouTube video transcripts to generate a vector store using our finetuned language model. When a user asks a question, a similarity search is performed to find the closest question matches in our corpus. The most relevant answers along with the user query are passed to our finetuned LLM, which then provides a contextual response to the user. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.F1" title="Figure 1 ‣ 3.1 An overview of our framework ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overview of our proposed framework.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="332" id="S3.F1.g1" src="extracted/5629977/figure/overview.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of our proposed framework.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Retriever</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Retriever Training Dataset</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Adobe Helpx (helpx.adobe.com) gets millions of unique visitors every year looking for tutorials and learning content (Adobe Helpx articles, Adobe help videos) related to Adobe products. In order to understand user behavior, we utilize the click logs from user <span class="ltx_text ltx_font_slanted" id="S3.SS2.SSS1.p1.1.1">queries</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mo id="S3.SS2.SSS1.p1.1.m1.1.1" stretchy="false" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">→</annotation></semantics></math> <span class="ltx_text ltx_font_slanted" id="S3.SS2.SSS1.p1.1.2">helpx</span> and Adobe community content articles to generate our dataset. We use logs from January 2022 – June 2023 for our training and evaluation set. This dataset provides a big and diverse data pool that is critical for learning a good representation of user queries and our retrieval documents. Table <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.T1" title="Table 1 ‣ 3.2.1 Retriever Training Dataset ‣ 3.2 Retriever ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates a sample row from the dataset and table <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.T2" title="Table 2 ‣ 3.2.1 Retriever Training Dataset ‣ 3.2 Retriever ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the overall dataset statistics.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>A sample row from the retriever training dataset.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1">
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.1">
<span class="ltx_p" id="S3.T1.1.1.1.1.1" style="width:56.4pt;">Query</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.2.1">
<span class="ltx_p" id="S3.T1.1.1.2.1.1" style="width:151.8pt;">Document</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.3.1">
<span class="ltx_p" id="S3.T1.1.1.3.1.1" style="width:108.4pt;">Doc Input</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.4.1">
<span class="ltx_p" id="S3.T1.1.1.4.1.1" style="width:65.0pt;">Log click ratio (relevance)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T1.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.1">
<span class="ltx_p" id="S3.T1.1.2.1.1.1" style="width:56.4pt;">Change color of text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T1.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.2.1">
<span class="ltx_p" id="S3.T1.1.2.2.1.1" style="width:151.8pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://helpx.adobe.com/indesign/using/editing-text.html" title="">https://helpx.adobe.com/indesign/using/editing-text.html</a></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T1.1.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.3.1">
<span class="ltx_p" id="S3.T1.1.2.3.1.1" style="width:108.4pt;">Learn how to edit text in Adobe Acrobat</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T1.1.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.4.1">
<span class="ltx_p" id="S3.T1.1.2.4.1.1" style="width:65.0pt;">0.24</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>The summary of the dataset.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1">
<tr class="ltx_tr" id="S3.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1">Dataset Name</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.2">Total Rows</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.3">Unique Queries</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt" id="S3.T2.1.1.4">Unique Documents</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S3.T2.1.2.1">Helpx 2022-2023 Click-data</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S3.T2.1.2.2">712792</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S3.T2.1.2.3">180,799</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t" id="S3.T2.1.2.4">22,576</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">For each document, we utilize the Document title and description to represent the crux of the document. Titles are generally 5–20 words long, while the descriptions are generally 20–50 words long, and whole documents can be multiple pages long. Each document in our dataset focuses on specific tasks and hence we found the title and description to be an apt representation of the document instead of passing the full document text.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">An example of the title and description is as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p4">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Title: How to edit images or objects in a PDF using Adobe Acrobat</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Description: Learn how to add, resize, move, or replace images and objects in a PDF using Acrobat. This tutorial showcases multiple ways of editing images in Acrobat using tools such as Flip, Drag, Rotate, Group, Arrange, and more.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p5">
<p class="ltx_p" id="S3.SS2.SSS1.p5.5">We also utilize a relevance field that is derived by using the log of the click ratio of the query-document pair, as defined in Equation <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.E1" title="In 3.2.1 Retriever Training Dataset ‣ 3.2 Retriever ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>. <math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.1.m1.1"><semantics id="S3.SS2.SSS1.p5.1.m1.1a"><msub id="S3.SS2.SSS1.p5.1.m1.1.1" xref="S3.SS2.SSS1.p5.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p5.1.m1.1.1.2" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS2.SSS1.p5.1.m1.1.1.3" xref="S3.SS2.SSS1.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.1.m1.1b"><apply id="S3.SS2.SSS1.p5.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p5.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2">𝑞</ci><ci id="S3.SS2.SSS1.p5.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.1.m1.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p5.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents the specific query. <math alttext="d_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.2.m2.1"><semantics id="S3.SS2.SSS1.p5.2.m2.1a"><msub id="S3.SS2.SSS1.p5.2.m2.1.1" xref="S3.SS2.SSS1.p5.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p5.2.m2.1.1.2" xref="S3.SS2.SSS1.p5.2.m2.1.1.2.cmml">d</mi><mi id="S3.SS2.SSS1.p5.2.m2.1.1.3" xref="S3.SS2.SSS1.p5.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.2.m2.1b"><apply id="S3.SS2.SSS1.p5.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p5.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1.2">𝑑</ci><ci id="S3.SS2.SSS1.p5.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.2.m2.1c">d_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p5.2.m2.1d">italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> represents the particular document clicked. <math alttext="\operatorname{clicks}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.3.m3.1"><semantics id="S3.SS2.SSS1.p5.3.m3.1a"><mi id="S3.SS2.SSS1.p5.3.m3.1.1" xref="S3.SS2.SSS1.p5.3.m3.1.1.cmml">clicks</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.3.m3.1b"><ci id="S3.SS2.SSS1.p5.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p5.3.m3.1.1">clicks</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.3.m3.1c">\operatorname{clicks}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p5.3.m3.1d">roman_clicks</annotation></semantics></math> represents the number of clicks for the pair and <math alttext="D_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.4.m4.1"><semantics id="S3.SS2.SSS1.p5.4.m4.1a"><msub id="S3.SS2.SSS1.p5.4.m4.1.1" xref="S3.SS2.SSS1.p5.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.p5.4.m4.1.1.2" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.cmml">D</mi><mi id="S3.SS2.SSS1.p5.4.m4.1.1.3" xref="S3.SS2.SSS1.p5.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.4.m4.1b"><apply id="S3.SS2.SSS1.p5.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p5.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1.2">𝐷</ci><ci id="S3.SS2.SSS1.p5.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.4.m4.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p5.4.m4.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents the set of documents clicked for <math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.5.m5.1"><semantics id="S3.SS2.SSS1.p5.5.m5.1a"><msub id="S3.SS2.SSS1.p5.5.m5.1.1" xref="S3.SS2.SSS1.p5.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.p5.5.m5.1.1.2" xref="S3.SS2.SSS1.p5.5.m5.1.1.2.cmml">q</mi><mi id="S3.SS2.SSS1.p5.5.m5.1.1.3" xref="S3.SS2.SSS1.p5.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.5.m5.1b"><apply id="S3.SS2.SSS1.p5.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p5.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p5.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p5.5.m5.1.1.2">𝑞</ci><ci id="S3.SS2.SSS1.p5.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.p5.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.5.m5.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p5.5.m5.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p6">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{ relevance }=\log\left(\frac{\operatorname{clicks}\left(q_{i}\rightarrow
d%
_{j}\right)}{\max\left(\operatorname{clicks}\left(q_{i}\rightarrow D_{i}\right%
)\right)}\right)" class="ltx_Math" display="block" id="S3.E1.m1.6"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.7" xref="S3.E1.m1.6.7.cmml"><mtext id="S3.E1.m1.6.7.2" xref="S3.E1.m1.6.7.2a.cmml"> relevance </mtext><mo id="S3.E1.m1.6.7.1" xref="S3.E1.m1.6.7.1.cmml">=</mo><mrow id="S3.E1.m1.6.7.3.2" xref="S3.E1.m1.6.7.3.1.cmml"><mi id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml">log</mi><mo id="S3.E1.m1.6.7.3.2a" xref="S3.E1.m1.6.7.3.1.cmml">⁡</mo><mrow id="S3.E1.m1.6.7.3.2.1" xref="S3.E1.m1.6.7.3.1.cmml"><mo id="S3.E1.m1.6.7.3.2.1.1" xref="S3.E1.m1.6.7.3.1.cmml">(</mo><mfrac id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mrow id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">clicks</mi><mo id="S3.E1.m1.2.2.2.2a" xref="S3.E1.m1.2.2.2.3.cmml">⁡</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.3.cmml"><mo id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.2.2" xref="S3.E1.m1.2.2.2.2.1.1.2.2.cmml">q</mi><mi id="S3.E1.m1.2.2.2.2.1.1.2.3" xref="S3.E1.m1.2.2.2.2.1.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.2.2.1.1.1" stretchy="false" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">→</mo><msub id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.3.2" xref="S3.E1.m1.2.2.2.2.1.1.3.2.cmml">d</mi><mi id="S3.E1.m1.2.2.2.2.1.1.3.3" xref="S3.E1.m1.2.2.2.2.1.1.3.3.cmml">j</mi></msub></mrow><mo id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.5.5.5.3" xref="S3.E1.m1.5.5.5.4.cmml"><mi id="S3.E1.m1.4.4.4.2" xref="S3.E1.m1.4.4.4.2.cmml">max</mi><mo id="S3.E1.m1.5.5.5.3a" xref="S3.E1.m1.5.5.5.4.cmml">⁡</mo><mrow id="S3.E1.m1.5.5.5.3.1" xref="S3.E1.m1.5.5.5.4.cmml"><mo id="S3.E1.m1.5.5.5.3.1.2" xref="S3.E1.m1.5.5.5.4.cmml">(</mo><mrow id="S3.E1.m1.5.5.5.3.1.1.1" xref="S3.E1.m1.5.5.5.3.1.1.2.cmml"><mi id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.3.1.cmml">clicks</mi><mo id="S3.E1.m1.5.5.5.3.1.1.1a" xref="S3.E1.m1.5.5.5.3.1.1.2.cmml">⁡</mo><mrow id="S3.E1.m1.5.5.5.3.1.1.1.1" xref="S3.E1.m1.5.5.5.3.1.1.2.cmml"><mo id="S3.E1.m1.5.5.5.3.1.1.1.1.2" xref="S3.E1.m1.5.5.5.3.1.1.2.cmml">(</mo><mrow id="S3.E1.m1.5.5.5.3.1.1.1.1.1" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.cmml"><msub id="S3.E1.m1.5.5.5.3.1.1.1.1.1.2" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.2" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.2.cmml">q</mi><mi id="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.3" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.5.5.5.3.1.1.1.1.1.1" stretchy="false" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.1.cmml">→</mo><msub id="S3.E1.m1.5.5.5.3.1.1.1.1.1.3" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.2" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.2.cmml">D</mi><mi id="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.3" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E1.m1.5.5.5.3.1.1.1.1.3" xref="S3.E1.m1.5.5.5.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.5.3.1.3" xref="S3.E1.m1.5.5.5.4.cmml">)</mo></mrow></mrow></mfrac><mo id="S3.E1.m1.6.7.3.2.1.2" xref="S3.E1.m1.6.7.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.7.cmml" xref="S3.E1.m1.6.7"><eq id="S3.E1.m1.6.7.1.cmml" xref="S3.E1.m1.6.7.1"></eq><ci id="S3.E1.m1.6.7.2a.cmml" xref="S3.E1.m1.6.7.2"><mtext id="S3.E1.m1.6.7.2.cmml" xref="S3.E1.m1.6.7.2"> relevance </mtext></ci><apply id="S3.E1.m1.6.7.3.1.cmml" xref="S3.E1.m1.6.7.3.2"><log id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"></log><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><divide id="S3.E1.m1.5.5.6.cmml" xref="S3.E1.m1.5.5"></divide><apply id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">clicks</ci><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><ci id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1">→</ci><apply id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.2">𝑞</ci><ci id="S3.E1.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.3">𝑖</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.2">𝑑</ci><ci id="S3.E1.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="S3.E1.m1.5.5.5.4.cmml" xref="S3.E1.m1.5.5.5.3"><max id="S3.E1.m1.4.4.4.2.cmml" xref="S3.E1.m1.4.4.4.2"></max><apply id="S3.E1.m1.5.5.5.3.1.1.2.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1"><ci id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.1">clicks</ci><apply id="S3.E1.m1.5.5.5.3.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1"><ci id="S3.E1.m1.5.5.5.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.1">→</ci><apply id="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.2">𝑞</ci><ci id="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.2">𝐷</ci><ci id="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">\text{ relevance }=\log\left(\frac{\operatorname{clicks}\left(q_{i}\rightarrow
d%
_{j}\right)}{\max\left(\operatorname{clicks}\left(q_{i}\rightarrow D_{i}\right%
)\right)}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.6d">relevance = roman_log ( divide start_ARG roman_clicks ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT → italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG start_ARG roman_max ( roman_clicks ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT → italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p7">
<p class="ltx_p" id="S3.SS2.SSS1.p7.1">This is important since for a query, there may be multiple documents clicked and we wish to pay more attention to higher click pairs. We take the log of the max click ratio since it allows less frequently clicked documents to also be part of the learning process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Retriever Model Training</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">In order to accurately learn a semantic representation for both queries and documents, we utilize Contrastive Learning. Contrastive Learning is a technique that allows similar samples to be modeled closely in the representation space without having explicit labels. The model is trained in a self-supervised manner, we utilize a single model to represent both the query and the document. This allows us to bring them both in the same representation space.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">We utilize transformers pretrained on sentence similarity tasks (MPnet), due to their proficiency with understanding long texts using attention. We utilize Adam optimizer and mean pooling while training. We also experimented with max pooling, along with first token pooling (similar to cls pooling), and found mean pooling to work best.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">Furthermore, we utilize our relevancy metric (the log of max click ratio in equation <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.E1" title="In 3.2.1 Retriever Training Dataset ‣ 3.2 Retriever ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>) and use it for our weighted cross entropy loss function. This allows us to weight query-document loss based on how relevant the document is for the query. This is important since it adds an inherent ranking signal inside the set of relevant documents.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p4">
<p class="ltx_p" id="S3.SS2.SSS2.p4.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.F2" title="Figure 2 ‣ 3.2.2 Retriever Model Training ‣ 3.2 Retriever ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a> showcases our training process. The query and document text are passed together in a training batch to the network and cosine similarities are computed from the generated representations. The mean squared loss is minimized based on the sentence similarity scores. The same weights are shared between the two transformer instances, hence allowing us to learn the representations of both query and document together.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="290" id="S3.F2.g1" src="extracted/5629977/figure/cosine_similarity.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The training processing for the retriever.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Retrieval Index Creation and Database</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In order to create our retrieval index, we utilize both primary sources (Adobe Helpx documents, Adobe Community questions) as well as derivate sources (generated QA pairs) to provide a rich and diverse retrieval set. We use our finetuned model to generate the representations of all of the sources. We utilize the following sources:</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">Primary Sources</p>
<ul class="ltx_itemize" id="S3.I2.i1.I1">
<li class="ltx_item" id="S3.I2.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.I1.i1.p1">
<p class="ltx_p" id="S3.I2.i1.I1.i1.p1.1">Helpx Documents (title, description) – We take the title and descriptions of helpx documents and embed them.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.I1.i2.p1">
<p class="ltx_p" id="S3.I2.i1.I1.i2.p1.1">Community Questions – Adobe has a rich community forum where Adobe users can ask questions related to the product and are helped by Adobe community experts. We utilize this valuable source of data as part of our retrieval index</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Derived Datasets</p>
<ul class="ltx_itemize" id="S3.I2.i2.I1">
<li class="ltx_item" id="S3.I2.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.I1.i1.p1">
<p class="ltx_p" id="S3.I2.i2.I1.i1.p1.1">Helpx Generated QA pairs – In order to generate higher coverage for the Helpx documents and to extract all sub-nuggets of information, we generate multiple QA pairs from each document using a QA generation module powered by a Large Language model.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.I1.i2.p1">
<p class="ltx_p" id="S3.I2.i2.I1.i2.p1.1">Adobe Care video QA pairs – Adobe Care is a YouTube channel run by Adobe that provides video tutorials to several highly requested questions as well as providing How-to guides for new features. We extract the transcript from these videos and then utilize our QA generation module to create useful question-answer pairs.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">The overall architecture for indexing is presented in the flowchart of Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.F3" title="Figure 3 ‣ 3.3 Retrieval Index Creation and Database ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="260" id="S3.F3.g1" src="extracted/5629977/figure/index_database.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An overall architecture for indexing.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Preprocessing for Building the Database</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In order to curate our derived datasets, we utilize a preprocessing approach to generate question-answer pairs utilizing a QA generation module. In addition, a named entity removal module to provide privacy safeguards for the dataset.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>QA Generation Module</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">The QA generation module is designed in order to curate our derived datasets, as discussed in section <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.SS3" title="3.3 Retrieval Index Creation and Database ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">3.3</span></a>. For the Helpx documents and the AdobeCare Video transcripts, we utilize our QA generation module to create the question-answer pairs. The QA generation module is powered by a Large Language Model, with the prompts designed for our task. A sample prompt that we utilized is as follows:</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS4.SSS1.p2.1.1">Prompt<span class="ltx_text ltx_font_medium" id="S3.SS4.SSS1.p2.1.1.1">: You are an AI assistant that helps create question-answer pairs. You start every question with QUESTION and every answer with ANSWER. Answer in detail.</span></span></p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p3">
<p class="ltx_p" id="S3.SS4.SSS1.p3.1">We see a significant boost in accuracy in doing few-shot learning compared to zero-shot generation<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">parnami2022learning</span>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">song2022comprehensive</span>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">brown2020language</span>]</cite>. We prepend a couple of sample documents as well as generated QA pairs with each call to our LLM, in order to guide the generation style and quality of the question. In our case, we prefer step-wise responses to questions, as Adobe users tend to prefer discrete steps compared to blocks of text. Table <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S3.T3" title="Table 3 ‣ 3.4.1 QA Generation Module ‣ 3.4 Preprocessing for Building the Database ‣ 3 Method ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">3</span></a> shows a sample QA pair created by the QA generation module.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>A sample QA pair created by the QA generation module.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1">
<tr class="ltx_tr" id="S3.T3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T3.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.1">
<span class="ltx_p" id="S3.T3.1.1.1.1.1" style="width:433.6pt;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.1.1.1.1">
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.1.1.1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.1.1.1.1.1.1">
<span class="ltx_p" id="S3.T3.1.1.1.1.1.1.1.1.1.1" style="width:433.6pt;">QUESTION:</span>
</span></span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.1.1.1.1.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.1.1.1.2.1.1">
<span class="ltx_p" id="S3.T3.1.1.1.1.1.1.2.1.1.1" style="width:433.6pt;">What are the steps to adjust the brightness, contrast, and color in your video clips using the Adjusting Brightness, Contrast, and Color Guided Edit in Adobe Premiere Elements?</span>
</span></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.1.1">
<span class="ltx_p" id="S3.T3.1.2.1.1.1" style="width:433.6pt;">ANSWER:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.3.1.1">
<span class="ltx_p" id="S3.T3.1.3.1.1.1" style="width:433.6pt;">1. Click Add media to import the video clip you want to enhance.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.4.1.1">
<span class="ltx_p" id="S3.T3.1.4.1.1.1" style="width:433.6pt;">2. Select Guided &gt;Adjusting Brightness+Contrast &amp; Color.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.1.1">
<span class="ltx_p" id="S3.T3.1.5.1.1.1" style="width:433.6pt;">3. To adjust your video clip, select it.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.6.1.1">
<span class="ltx_p" id="S3.T3.1.6.1.1.1" style="width:433.6pt;">4. Click the Adjust panel to adjust selected settings.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.7.1.1">
<span class="ltx_p" id="S3.T3.1.7.1.1.1" style="width:433.6pt;">5. Click Lighting to adjust the brightness and contrast.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.8">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.8.1.1">
<span class="ltx_p" id="S3.T3.1.8.1.1.1" style="width:433.6pt;">6. Click a thumbnail in the grid of the adjustments panel to preview the change in brightness.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.9">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.9.1.1">
<span class="ltx_p" id="S3.T3.1.9.1.1.1" style="width:433.6pt;">7. Click More and drag the sliders for more precise adjustment.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.10">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.10.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.10.1.1">
<span class="ltx_p" id="S3.T3.1.10.1.1.1" style="width:433.6pt;">8. Click Color in the adjustments panel to open the Color section.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.11">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T3.1.11.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.11.1.1">
<span class="ltx_p" id="S3.T3.1.11.1.1.1" style="width:433.6pt;">9. You can adjust the hue, lightness, saturation, and vibrance in the Color tab.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.12">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T3.1.12.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.12.1.1">
<span class="ltx_p" id="S3.T3.1.12.1.1.1" style="width:433.6pt;">10. Click a thumbnail in the grid to preview the change.</span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S3.SS4.SSS1.p4">
<p class="ltx_p" id="S3.SS4.SSS1.p4.1">As part of our indexing pipeline, we automate question-answer pair creation of any new articles and videos in the future, thus allowing us to have a richer retrieval context for all documents in our index.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Named Entity Removal Module</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">In the pipeline of our data-driven ecosystem, the Named Entity Removal Module occupies a critical vertex, ensuring the sanitization of the community-sourced Adobe textual corpus. This module is architected to perform de-identification by removing all classes of Personally Identifiable Information, thereby fortifying the privacy safeguards for the dataset.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1">The module employs a dual-layered Named Entity Recognition (NER) strategy:</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p3">
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1">Machine Learning-based NER: At its core, the module integrates a state-of-the-art NER model, <span class="ltx_text ltx_font_typewriter" id="S3.I3.i1.p1.1.1">en_core_web_lg</span>. This model is pre-trained on a large corpus and employs a deep learning architecture, enabling the efficient categorization of ’PERSON’-type entities with a high degree of accuracy.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1">Regular Expression-based Custom Sanitization: Augmenting the machine learning-based NER, a custom function, custom_removal, is deployed to tackle non-standard or pattern-based text segments. This is particularly aimed at recognizing and eliminating textual patterns, email formats, and signature lines. It also includes patterns for phone number sanitization conforming to various international formats.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p4">
<p class="ltx_p" id="S3.SS4.SSS2.p4.1">The module operates iteratively over the dataset, which is ingested in a JSON-based serialized format. Each entry is processed through the dual-layered NER, and the sanitized text replaces the original content inline, thus ensuring data integrity while enhancing privacy. The serialized output is designed for seamless integration into subsequent machine learning workflows, including our proprietary retrieval and ranking algorithms.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p5">
<p class="ltx_p" id="S3.SS4.SSS2.p5.1">This modularity ensures that the Named Entity Removal Module can be seamlessly plugged into various stages of our data processing pipelines, ranging from initial data ingestion to pre-model training sanitization.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Query Augmentation via Product Identification</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">One of the key challenges we see is related to product disambiguation for vague queries. Since several Adobe products might contain similar features, adding product disambiguation helps improve overall accuracy and quality of generation. Vague queries usually receive vague answers.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">We utilize a product-intent extraction model that maps all input texts to 1 or more Adobe products. We then pass this information to the retriever, allowing for better relevancy in the retrieved documents.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>LLM prompting</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">Once the retriever has collected one or several passages from the database of question-answer based on the user query, the passages are given as context to an LLM, and the LLM is prompted to answer the user query based on the passages.</p>
</div>
<div class="ltx_para" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">The prompt used for the LLM is as follows:</p>
</div>
<div class="ltx_para" id="S3.SS6.p3">
<p class="ltx_p" id="S3.SS6.p3.1">You are an assistant that helps humans use [PRODUCT NAME, e.g. Adobe Premiere Pro]. You will be given a list of question-answer pairs (some pairs might be irrelevant) and a user query. Your goal is to answer the user query using only information from the given question-answer pairs.</p>
</div>
<div class="ltx_para" id="S3.SS6.p4">
<p class="ltx_p" id="S3.SS6.p4.1">List of question-answer pairs: […]</p>
</div>
<div class="ltx_para" id="S3.SS6.p5">
<p class="ltx_p" id="S3.SS6.p5.1">User query: […]
Answer:</p>
</div>
<div class="ltx_para" id="S3.SS6.p6">
<p class="ltx_p" id="S3.SS6.p6.1">In some cases, some pairs in the list of question-answer pairs may be redundant. Therefore, before the list of question-answer pairs is given to the LLM, our system removes duplicates from the list of question-answer pairs. Specifically, we use it to compute the Levenshtein distance as well as the semantic similarity between each question/title and the list of question-answer pairs. If the Levenstein distance is below some threshold or if the semantic similarity is too high, we compute the semantic similarity between their two answers. If the two answers are too semantically similar, we remove one of them based on the source credibility (Helpx &gt; Community &gt; YouTube &gt; LLM-generated QAs) and answer length.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>LLM Finetuning</h3>
<div class="ltx_para" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.3">To train the LLM, we use grounded document, <math alttext="d^{+}" class="ltx_Math" display="inline" id="S3.SS7.p1.1.m1.1"><semantics id="S3.SS7.p1.1.m1.1a"><msup id="S3.SS7.p1.1.m1.1.1" xref="S3.SS7.p1.1.m1.1.1.cmml"><mi id="S3.SS7.p1.1.m1.1.1.2" xref="S3.SS7.p1.1.m1.1.1.2.cmml">d</mi><mo id="S3.SS7.p1.1.m1.1.1.3" xref="S3.SS7.p1.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.1.m1.1b"><apply id="S3.SS7.p1.1.m1.1.1.cmml" xref="S3.SS7.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS7.p1.1.m1.1.1.1.cmml" xref="S3.SS7.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS7.p1.1.m1.1.1.2.cmml" xref="S3.SS7.p1.1.m1.1.1.2">𝑑</ci><plus id="S3.SS7.p1.1.m1.1.1.3.cmml" xref="S3.SS7.p1.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.1.m1.1c">d^{+}</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p1.1.m1.1d">italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math>, negative document <math alttext="d^{-}" class="ltx_Math" display="inline" id="S3.SS7.p1.2.m2.1"><semantics id="S3.SS7.p1.2.m2.1a"><msup id="S3.SS7.p1.2.m2.1.1" xref="S3.SS7.p1.2.m2.1.1.cmml"><mi id="S3.SS7.p1.2.m2.1.1.2" xref="S3.SS7.p1.2.m2.1.1.2.cmml">d</mi><mo id="S3.SS7.p1.2.m2.1.1.3" xref="S3.SS7.p1.2.m2.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.2.m2.1b"><apply id="S3.SS7.p1.2.m2.1.1.cmml" xref="S3.SS7.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS7.p1.2.m2.1.1.1.cmml" xref="S3.SS7.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS7.p1.2.m2.1.1.2.cmml" xref="S3.SS7.p1.2.m2.1.1.2">𝑑</ci><minus id="S3.SS7.p1.2.m2.1.1.3.cmml" xref="S3.SS7.p1.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.2.m2.1c">d^{-}</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p1.2.m2.1d">italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT</annotation></semantics></math>, and question-answer pairs, <math alttext="(q,a)" class="ltx_Math" display="inline" id="S3.SS7.p1.3.m3.2"><semantics id="S3.SS7.p1.3.m3.2a"><mrow id="S3.SS7.p1.3.m3.2.3.2" xref="S3.SS7.p1.3.m3.2.3.1.cmml"><mo id="S3.SS7.p1.3.m3.2.3.2.1" stretchy="false" xref="S3.SS7.p1.3.m3.2.3.1.cmml">(</mo><mi id="S3.SS7.p1.3.m3.1.1" xref="S3.SS7.p1.3.m3.1.1.cmml">q</mi><mo id="S3.SS7.p1.3.m3.2.3.2.2" xref="S3.SS7.p1.3.m3.2.3.1.cmml">,</mo><mi id="S3.SS7.p1.3.m3.2.2" xref="S3.SS7.p1.3.m3.2.2.cmml">a</mi><mo id="S3.SS7.p1.3.m3.2.3.2.3" stretchy="false" xref="S3.SS7.p1.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.3.m3.2b"><interval closure="open" id="S3.SS7.p1.3.m3.2.3.1.cmml" xref="S3.SS7.p1.3.m3.2.3.2"><ci id="S3.SS7.p1.3.m3.1.1.cmml" xref="S3.SS7.p1.3.m3.1.1">𝑞</ci><ci id="S3.SS7.p1.3.m3.2.2.cmml" xref="S3.SS7.p1.3.m3.2.2">𝑎</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.3.m3.2c">(q,a)</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p1.3.m3.2d">( italic_q , italic_a )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS7.p2">
<p class="ltx_p" id="S3.SS7.p2.3"><math alttext="y=\text{LLM}_{θ}(d^{+},d^{-},q)" class="ltx_Math" display="inline" id="S3.SS7.p2.1.m1.3"><semantics id="S3.SS7.p2.1.m1.3a"><mrow id="S3.SS7.p2.1.m1.3.3" xref="S3.SS7.p2.1.m1.3.3.cmml"><mi id="S3.SS7.p2.1.m1.3.3.4" xref="S3.SS7.p2.1.m1.3.3.4.cmml">y</mi><mo id="S3.SS7.p2.1.m1.3.3.3" xref="S3.SS7.p2.1.m1.3.3.3.cmml">=</mo><mrow id="S3.SS7.p2.1.m1.3.3.2" xref="S3.SS7.p2.1.m1.3.3.2.cmml"><msub id="S3.SS7.p2.1.m1.3.3.2.4" xref="S3.SS7.p2.1.m1.3.3.2.4.cmml"><mtext id="S3.SS7.p2.1.m1.3.3.2.4.2" xref="S3.SS7.p2.1.m1.3.3.2.4.2a.cmml">LLM</mtext><mi id="S3.SS7.p2.1.m1.3.3.2.4.3" xref="S3.SS7.p2.1.m1.3.3.2.4.3.cmml">θ</mi></msub><mo id="S3.SS7.p2.1.m1.3.3.2.3" xref="S3.SS7.p2.1.m1.3.3.2.3.cmml">⁢</mo><mrow id="S3.SS7.p2.1.m1.3.3.2.2.2" xref="S3.SS7.p2.1.m1.3.3.2.2.3.cmml"><mo id="S3.SS7.p2.1.m1.3.3.2.2.2.3" stretchy="false" xref="S3.SS7.p2.1.m1.3.3.2.2.3.cmml">(</mo><msup id="S3.SS7.p2.1.m1.2.2.1.1.1.1" xref="S3.SS7.p2.1.m1.2.2.1.1.1.1.cmml"><mi id="S3.SS7.p2.1.m1.2.2.1.1.1.1.2" xref="S3.SS7.p2.1.m1.2.2.1.1.1.1.2.cmml">d</mi><mo id="S3.SS7.p2.1.m1.2.2.1.1.1.1.3" xref="S3.SS7.p2.1.m1.2.2.1.1.1.1.3.cmml">+</mo></msup><mo id="S3.SS7.p2.1.m1.3.3.2.2.2.4" xref="S3.SS7.p2.1.m1.3.3.2.2.3.cmml">,</mo><msup id="S3.SS7.p2.1.m1.3.3.2.2.2.2" xref="S3.SS7.p2.1.m1.3.3.2.2.2.2.cmml"><mi id="S3.SS7.p2.1.m1.3.3.2.2.2.2.2" xref="S3.SS7.p2.1.m1.3.3.2.2.2.2.2.cmml">d</mi><mo id="S3.SS7.p2.1.m1.3.3.2.2.2.2.3" xref="S3.SS7.p2.1.m1.3.3.2.2.2.2.3.cmml">−</mo></msup><mo id="S3.SS7.p2.1.m1.3.3.2.2.2.5" xref="S3.SS7.p2.1.m1.3.3.2.2.3.cmml">,</mo><mi id="S3.SS7.p2.1.m1.1.1" xref="S3.SS7.p2.1.m1.1.1.cmml">q</mi><mo id="S3.SS7.p2.1.m1.3.3.2.2.2.6" stretchy="false" xref="S3.SS7.p2.1.m1.3.3.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS7.p2.1.m1.3b"><apply id="S3.SS7.p2.1.m1.3.3.cmml" xref="S3.SS7.p2.1.m1.3.3"><eq id="S3.SS7.p2.1.m1.3.3.3.cmml" xref="S3.SS7.p2.1.m1.3.3.3"></eq><ci id="S3.SS7.p2.1.m1.3.3.4.cmml" xref="S3.SS7.p2.1.m1.3.3.4">𝑦</ci><apply id="S3.SS7.p2.1.m1.3.3.2.cmml" xref="S3.SS7.p2.1.m1.3.3.2"><times id="S3.SS7.p2.1.m1.3.3.2.3.cmml" xref="S3.SS7.p2.1.m1.3.3.2.3"></times><apply id="S3.SS7.p2.1.m1.3.3.2.4.cmml" xref="S3.SS7.p2.1.m1.3.3.2.4"><csymbol cd="ambiguous" id="S3.SS7.p2.1.m1.3.3.2.4.1.cmml" xref="S3.SS7.p2.1.m1.3.3.2.4">subscript</csymbol><ci id="S3.SS7.p2.1.m1.3.3.2.4.2a.cmml" xref="S3.SS7.p2.1.m1.3.3.2.4.2"><mtext id="S3.SS7.p2.1.m1.3.3.2.4.2.cmml" xref="S3.SS7.p2.1.m1.3.3.2.4.2">LLM</mtext></ci><ci id="S3.SS7.p2.1.m1.3.3.2.4.3.cmml" xref="S3.SS7.p2.1.m1.3.3.2.4.3">𝜃</ci></apply><vector id="S3.SS7.p2.1.m1.3.3.2.2.3.cmml" xref="S3.SS7.p2.1.m1.3.3.2.2.2"><apply id="S3.SS7.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS7.p2.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS7.p2.1.m1.2.2.1.1.1.1.1.cmml" xref="S3.SS7.p2.1.m1.2.2.1.1.1.1">superscript</csymbol><ci id="S3.SS7.p2.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS7.p2.1.m1.2.2.1.1.1.1.2">𝑑</ci><plus id="S3.SS7.p2.1.m1.2.2.1.1.1.1.3.cmml" xref="S3.SS7.p2.1.m1.2.2.1.1.1.1.3"></plus></apply><apply id="S3.SS7.p2.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS7.p2.1.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS7.p2.1.m1.3.3.2.2.2.2.1.cmml" xref="S3.SS7.p2.1.m1.3.3.2.2.2.2">superscript</csymbol><ci id="S3.SS7.p2.1.m1.3.3.2.2.2.2.2.cmml" xref="S3.SS7.p2.1.m1.3.3.2.2.2.2.2">𝑑</ci><minus id="S3.SS7.p2.1.m1.3.3.2.2.2.2.3.cmml" xref="S3.SS7.p2.1.m1.3.3.2.2.2.2.3"></minus></apply><ci id="S3.SS7.p2.1.m1.1.1.cmml" xref="S3.SS7.p2.1.m1.1.1">𝑞</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p2.1.m1.3c">y=\text{LLM}_{θ}(d^{+},d^{-},q)</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p2.1.m1.3d">italic_y = LLM start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT , italic_q )</annotation></semantics></math>. <math alttext="y" class="ltx_Math" display="inline" id="S3.SS7.p2.2.m2.1"><semantics id="S3.SS7.p2.2.m2.1a"><mi id="S3.SS7.p2.2.m2.1.1" xref="S3.SS7.p2.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p2.2.m2.1b"><ci id="S3.SS7.p2.2.m2.1.1.cmml" xref="S3.SS7.p2.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p2.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p2.2.m2.1d">italic_y</annotation></semantics></math> is the generated outcome, and it is compared to <math alttext="a" class="ltx_Math" display="inline" id="S3.SS7.p2.3.m3.1"><semantics id="S3.SS7.p2.3.m3.1a"><mi id="S3.SS7.p2.3.m3.1.1" xref="S3.SS7.p2.3.m3.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p2.3.m3.1b"><ci id="S3.SS7.p2.3.m3.1.1.cmml" xref="S3.SS7.p2.3.m3.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p2.3.m3.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p2.3.m3.1d">italic_a</annotation></semantics></math> to update the LLM. The following shows our template to generate the training samples.</p>
</div>
<div class="ltx_para" id="S3.SS7.p3">
<p class="ltx_p" id="S3.SS7.p3.2">The grounded document, (<math alttext="d^{+}" class="ltx_Math" display="inline" id="S3.SS7.p3.1.m1.1"><semantics id="S3.SS7.p3.1.m1.1a"><msup id="S3.SS7.p3.1.m1.1.1" xref="S3.SS7.p3.1.m1.1.1.cmml"><mi id="S3.SS7.p3.1.m1.1.1.2" xref="S3.SS7.p3.1.m1.1.1.2.cmml">d</mi><mo id="S3.SS7.p3.1.m1.1.1.3" xref="S3.SS7.p3.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS7.p3.1.m1.1b"><apply id="S3.SS7.p3.1.m1.1.1.cmml" xref="S3.SS7.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS7.p3.1.m1.1.1.1.cmml" xref="S3.SS7.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS7.p3.1.m1.1.1.2.cmml" xref="S3.SS7.p3.1.m1.1.1.2">𝑑</ci><plus id="S3.SS7.p3.1.m1.1.1.3.cmml" xref="S3.SS7.p3.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p3.1.m1.1c">d^{+}</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p3.1.m1.1d">italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math>), is the original source where <math alttext="(q,a)" class="ltx_Math" display="inline" id="S3.SS7.p3.2.m2.2"><semantics id="S3.SS7.p3.2.m2.2a"><mrow id="S3.SS7.p3.2.m2.2.3.2" xref="S3.SS7.p3.2.m2.2.3.1.cmml"><mo id="S3.SS7.p3.2.m2.2.3.2.1" stretchy="false" xref="S3.SS7.p3.2.m2.2.3.1.cmml">(</mo><mi id="S3.SS7.p3.2.m2.1.1" xref="S3.SS7.p3.2.m2.1.1.cmml">q</mi><mo id="S3.SS7.p3.2.m2.2.3.2.2" xref="S3.SS7.p3.2.m2.2.3.1.cmml">,</mo><mi id="S3.SS7.p3.2.m2.2.2" xref="S3.SS7.p3.2.m2.2.2.cmml">a</mi><mo id="S3.SS7.p3.2.m2.2.3.2.3" stretchy="false" xref="S3.SS7.p3.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS7.p3.2.m2.2b"><interval closure="open" id="S3.SS7.p3.2.m2.2.3.1.cmml" xref="S3.SS7.p3.2.m2.2.3.2"><ci id="S3.SS7.p3.2.m2.1.1.cmml" xref="S3.SS7.p3.2.m2.1.1">𝑞</ci><ci id="S3.SS7.p3.2.m2.2.2.cmml" xref="S3.SS7.p3.2.m2.2.2">𝑎</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p3.2.m2.2c">(q,a)</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p3.2.m2.2d">( italic_q , italic_a )</annotation></semantics></math> pair is generated from. To increase the robustness and generalization abilities of the finetuned LLM model, we propose the following approaches:</p>
</div>
<div class="ltx_para" id="S3.SS7.p4">
<ul class="ltx_itemize" id="S3.I4">
<li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i1.p1">
<p class="ltx_p" id="S3.I4.i1.p1.1">When the length of the answer <math alttext="(a)" class="ltx_Math" display="inline" id="S3.I4.i1.p1.1.m1.1"><semantics id="S3.I4.i1.p1.1.m1.1a"><mrow id="S3.I4.i1.p1.1.m1.1.2.2"><mo id="S3.I4.i1.p1.1.m1.1.2.2.1" stretchy="false">(</mo><mi id="S3.I4.i1.p1.1.m1.1.1" xref="S3.I4.i1.p1.1.m1.1.1.cmml">a</mi><mo id="S3.I4.i1.p1.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I4.i1.p1.1.m1.1b"><ci id="S3.I4.i1.p1.1.m1.1.1.cmml" xref="S3.I4.i1.p1.1.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i1.p1.1.m1.1c">(a)</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i1.p1.1.m1.1d">( italic_a )</annotation></semantics></math> is shorter than the threshold (e.g., 90 tokens), we filtered out these examples from the training set to guide the finetuned LLM model, generating an answer with substantial information.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i2.p1">
<p class="ltx_p" id="S3.I4.i2.p1.3">The documents are retrieved from our finetuned retriever model. We use multiple <math alttext="(d^{+})" class="ltx_Math" display="inline" id="S3.I4.i2.p1.1.m1.1"><semantics id="S3.I4.i2.p1.1.m1.1a"><mrow id="S3.I4.i2.p1.1.m1.1.1.1" xref="S3.I4.i2.p1.1.m1.1.1.1.1.cmml"><mo id="S3.I4.i2.p1.1.m1.1.1.1.2" stretchy="false" xref="S3.I4.i2.p1.1.m1.1.1.1.1.cmml">(</mo><msup id="S3.I4.i2.p1.1.m1.1.1.1.1" xref="S3.I4.i2.p1.1.m1.1.1.1.1.cmml"><mi id="S3.I4.i2.p1.1.m1.1.1.1.1.2" xref="S3.I4.i2.p1.1.m1.1.1.1.1.2.cmml">d</mi><mo id="S3.I4.i2.p1.1.m1.1.1.1.1.3" xref="S3.I4.i2.p1.1.m1.1.1.1.1.3.cmml">+</mo></msup><mo id="S3.I4.i2.p1.1.m1.1.1.1.3" stretchy="false" xref="S3.I4.i2.p1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I4.i2.p1.1.m1.1b"><apply id="S3.I4.i2.p1.1.m1.1.1.1.1.cmml" xref="S3.I4.i2.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.I4.i2.p1.1.m1.1.1.1.1.1.cmml" xref="S3.I4.i2.p1.1.m1.1.1.1">superscript</csymbol><ci id="S3.I4.i2.p1.1.m1.1.1.1.1.2.cmml" xref="S3.I4.i2.p1.1.m1.1.1.1.1.2">𝑑</ci><plus id="S3.I4.i2.p1.1.m1.1.1.1.1.3.cmml" xref="S3.I4.i2.p1.1.m1.1.1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i2.p1.1.m1.1c">(d^{+})</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i2.p1.1.m1.1d">( italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT )</annotation></semantics></math> and <math alttext="(d^{-})" class="ltx_Math" display="inline" id="S3.I4.i2.p1.2.m2.1"><semantics id="S3.I4.i2.p1.2.m2.1a"><mrow id="S3.I4.i2.p1.2.m2.1.1.1" xref="S3.I4.i2.p1.2.m2.1.1.1.1.cmml"><mo id="S3.I4.i2.p1.2.m2.1.1.1.2" stretchy="false" xref="S3.I4.i2.p1.2.m2.1.1.1.1.cmml">(</mo><msup id="S3.I4.i2.p1.2.m2.1.1.1.1" xref="S3.I4.i2.p1.2.m2.1.1.1.1.cmml"><mi id="S3.I4.i2.p1.2.m2.1.1.1.1.2" xref="S3.I4.i2.p1.2.m2.1.1.1.1.2.cmml">d</mi><mo id="S3.I4.i2.p1.2.m2.1.1.1.1.3" xref="S3.I4.i2.p1.2.m2.1.1.1.1.3.cmml">−</mo></msup><mo id="S3.I4.i2.p1.2.m2.1.1.1.3" stretchy="false" xref="S3.I4.i2.p1.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I4.i2.p1.2.m2.1b"><apply id="S3.I4.i2.p1.2.m2.1.1.1.1.cmml" xref="S3.I4.i2.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.I4.i2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.I4.i2.p1.2.m2.1.1.1">superscript</csymbol><ci id="S3.I4.i2.p1.2.m2.1.1.1.1.2.cmml" xref="S3.I4.i2.p1.2.m2.1.1.1.1.2">𝑑</ci><minus id="S3.I4.i2.p1.2.m2.1.1.1.1.3.cmml" xref="S3.I4.i2.p1.2.m2.1.1.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i2.p1.2.m2.1c">(d^{-})</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i2.p1.2.m2.1d">( italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT )</annotation></semantics></math> to increase recall of the retriever components and robustness of the finetuned LLM. Top-k <math alttext="(d^{+})" class="ltx_Math" display="inline" id="S3.I4.i2.p1.3.m3.1"><semantics id="S3.I4.i2.p1.3.m3.1a"><mrow id="S3.I4.i2.p1.3.m3.1.1.1" xref="S3.I4.i2.p1.3.m3.1.1.1.1.cmml"><mo id="S3.I4.i2.p1.3.m3.1.1.1.2" stretchy="false" xref="S3.I4.i2.p1.3.m3.1.1.1.1.cmml">(</mo><msup id="S3.I4.i2.p1.3.m3.1.1.1.1" xref="S3.I4.i2.p1.3.m3.1.1.1.1.cmml"><mi id="S3.I4.i2.p1.3.m3.1.1.1.1.2" xref="S3.I4.i2.p1.3.m3.1.1.1.1.2.cmml">d</mi><mo id="S3.I4.i2.p1.3.m3.1.1.1.1.3" xref="S3.I4.i2.p1.3.m3.1.1.1.1.3.cmml">+</mo></msup><mo id="S3.I4.i2.p1.3.m3.1.1.1.3" stretchy="false" xref="S3.I4.i2.p1.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I4.i2.p1.3.m3.1b"><apply id="S3.I4.i2.p1.3.m3.1.1.1.1.cmml" xref="S3.I4.i2.p1.3.m3.1.1.1"><csymbol cd="ambiguous" id="S3.I4.i2.p1.3.m3.1.1.1.1.1.cmml" xref="S3.I4.i2.p1.3.m3.1.1.1">superscript</csymbol><ci id="S3.I4.i2.p1.3.m3.1.1.1.1.2.cmml" xref="S3.I4.i2.p1.3.m3.1.1.1.1.2">𝑑</ci><plus id="S3.I4.i2.p1.3.m3.1.1.1.1.3.cmml" xref="S3.I4.i2.p1.3.m3.1.1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i2.p1.3.m3.1c">(d^{+})</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i2.p1.3.m3.1d">( italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT )</annotation></semantics></math>s are selected by retrieving documents that are similar to the ground-truth document in the pre-indexed embedding space.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i3.p1">
<p class="ltx_p" id="S3.I4.i3.p1.3">Negative documents, <math alttext="(d^{-})" class="ltx_Math" display="inline" id="S3.I4.i3.p1.1.m1.1"><semantics id="S3.I4.i3.p1.1.m1.1a"><mrow id="S3.I4.i3.p1.1.m1.1.1.1" xref="S3.I4.i3.p1.1.m1.1.1.1.1.cmml"><mo id="S3.I4.i3.p1.1.m1.1.1.1.2" stretchy="false" xref="S3.I4.i3.p1.1.m1.1.1.1.1.cmml">(</mo><msup id="S3.I4.i3.p1.1.m1.1.1.1.1" xref="S3.I4.i3.p1.1.m1.1.1.1.1.cmml"><mi id="S3.I4.i3.p1.1.m1.1.1.1.1.2" xref="S3.I4.i3.p1.1.m1.1.1.1.1.2.cmml">d</mi><mo id="S3.I4.i3.p1.1.m1.1.1.1.1.3" xref="S3.I4.i3.p1.1.m1.1.1.1.1.3.cmml">−</mo></msup><mo id="S3.I4.i3.p1.1.m1.1.1.1.3" stretchy="false" xref="S3.I4.i3.p1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I4.i3.p1.1.m1.1b"><apply id="S3.I4.i3.p1.1.m1.1.1.1.1.cmml" xref="S3.I4.i3.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.I4.i3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.I4.i3.p1.1.m1.1.1.1">superscript</csymbol><ci id="S3.I4.i3.p1.1.m1.1.1.1.1.2.cmml" xref="S3.I4.i3.p1.1.m1.1.1.1.1.2">𝑑</ci><minus id="S3.I4.i3.p1.1.m1.1.1.1.1.3.cmml" xref="S3.I4.i3.p1.1.m1.1.1.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i3.p1.1.m1.1c">(d^{-})</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i3.p1.1.m1.1d">( italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT )</annotation></semantics></math>s, are prepared by “random sampling from the database” and by selecting documents that are less similar to the original document (i.e., cosine similarity <math alttext="&lt;\tau_{\text{sim}}" class="ltx_Math" display="inline" id="S3.I4.i3.p1.2.m2.1"><semantics id="S3.I4.i3.p1.2.m2.1a"><mrow id="S3.I4.i3.p1.2.m2.1.1" xref="S3.I4.i3.p1.2.m2.1.1.cmml"><mi id="S3.I4.i3.p1.2.m2.1.1.2" xref="S3.I4.i3.p1.2.m2.1.1.2.cmml"></mi><mo id="S3.I4.i3.p1.2.m2.1.1.1" xref="S3.I4.i3.p1.2.m2.1.1.1.cmml">&lt;</mo><msub id="S3.I4.i3.p1.2.m2.1.1.3" xref="S3.I4.i3.p1.2.m2.1.1.3.cmml"><mi id="S3.I4.i3.p1.2.m2.1.1.3.2" xref="S3.I4.i3.p1.2.m2.1.1.3.2.cmml">τ</mi><mtext id="S3.I4.i3.p1.2.m2.1.1.3.3" xref="S3.I4.i3.p1.2.m2.1.1.3.3a.cmml">sim</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I4.i3.p1.2.m2.1b"><apply id="S3.I4.i3.p1.2.m2.1.1.cmml" xref="S3.I4.i3.p1.2.m2.1.1"><lt id="S3.I4.i3.p1.2.m2.1.1.1.cmml" xref="S3.I4.i3.p1.2.m2.1.1.1"></lt><csymbol cd="latexml" id="S3.I4.i3.p1.2.m2.1.1.2.cmml" xref="S3.I4.i3.p1.2.m2.1.1.2">absent</csymbol><apply id="S3.I4.i3.p1.2.m2.1.1.3.cmml" xref="S3.I4.i3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.I4.i3.p1.2.m2.1.1.3.1.cmml" xref="S3.I4.i3.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.I4.i3.p1.2.m2.1.1.3.2.cmml" xref="S3.I4.i3.p1.2.m2.1.1.3.2">𝜏</ci><ci id="S3.I4.i3.p1.2.m2.1.1.3.3a.cmml" xref="S3.I4.i3.p1.2.m2.1.1.3.3"><mtext id="S3.I4.i3.p1.2.m2.1.1.3.3.cmml" mathsize="70%" xref="S3.I4.i3.p1.2.m2.1.1.3.3">sim</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i3.p1.2.m2.1c">&lt;\tau_{\text{sim}}</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i3.p1.2.m2.1d">&lt; italic_τ start_POSTSUBSCRIPT sim end_POSTSUBSCRIPT</annotation></semantics></math>) and that are not extremely dis-similar to the original document (i.e., cosine similarity <math alttext="&gt;\tau_{\text{dissim}})" class="ltx_math_unparsed" display="inline" id="S3.I4.i3.p1.3.m3.1"><semantics id="S3.I4.i3.p1.3.m3.1a"><mrow id="S3.I4.i3.p1.3.m3.1b"><mo id="S3.I4.i3.p1.3.m3.1.1">&gt;</mo><msub id="S3.I4.i3.p1.3.m3.1.2"><mi id="S3.I4.i3.p1.3.m3.1.2.2">τ</mi><mtext id="S3.I4.i3.p1.3.m3.1.2.3">dissim</mtext></msub><mo id="S3.I4.i3.p1.3.m3.1.3" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="S3.I4.i3.p1.3.m3.1c">&gt;\tau_{\text{dissim}})</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i3.p1.3.m3.1d">&gt; italic_τ start_POSTSUBSCRIPT dissim end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i4.p1">
<p class="ltx_p" id="S3.I4.i4.p1.2">We also add samples that do not have the grounded documents, <math alttext="(d^{+})" class="ltx_Math" display="inline" id="S3.I4.i4.p1.1.m1.1"><semantics id="S3.I4.i4.p1.1.m1.1a"><mrow id="S3.I4.i4.p1.1.m1.1.1.1" xref="S3.I4.i4.p1.1.m1.1.1.1.1.cmml"><mo id="S3.I4.i4.p1.1.m1.1.1.1.2" stretchy="false" xref="S3.I4.i4.p1.1.m1.1.1.1.1.cmml">(</mo><msup id="S3.I4.i4.p1.1.m1.1.1.1.1" xref="S3.I4.i4.p1.1.m1.1.1.1.1.cmml"><mi id="S3.I4.i4.p1.1.m1.1.1.1.1.2" xref="S3.I4.i4.p1.1.m1.1.1.1.1.2.cmml">d</mi><mo id="S3.I4.i4.p1.1.m1.1.1.1.1.3" xref="S3.I4.i4.p1.1.m1.1.1.1.1.3.cmml">+</mo></msup><mo id="S3.I4.i4.p1.1.m1.1.1.1.3" stretchy="false" xref="S3.I4.i4.p1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I4.i4.p1.1.m1.1b"><apply id="S3.I4.i4.p1.1.m1.1.1.1.1.cmml" xref="S3.I4.i4.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.I4.i4.p1.1.m1.1.1.1.1.1.cmml" xref="S3.I4.i4.p1.1.m1.1.1.1">superscript</csymbol><ci id="S3.I4.i4.p1.1.m1.1.1.1.1.2.cmml" xref="S3.I4.i4.p1.1.m1.1.1.1.1.2">𝑑</ci><plus id="S3.I4.i4.p1.1.m1.1.1.1.1.3.cmml" xref="S3.I4.i4.p1.1.m1.1.1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i4.p1.1.m1.1c">(d^{+})</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i4.p1.1.m1.1d">( italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT )</annotation></semantics></math>, but, only have the negative documents, <math alttext="(d^{-})" class="ltx_Math" display="inline" id="S3.I4.i4.p1.2.m2.1"><semantics id="S3.I4.i4.p1.2.m2.1a"><mrow id="S3.I4.i4.p1.2.m2.1.1.1" xref="S3.I4.i4.p1.2.m2.1.1.1.1.cmml"><mo id="S3.I4.i4.p1.2.m2.1.1.1.2" stretchy="false" xref="S3.I4.i4.p1.2.m2.1.1.1.1.cmml">(</mo><msup id="S3.I4.i4.p1.2.m2.1.1.1.1" xref="S3.I4.i4.p1.2.m2.1.1.1.1.cmml"><mi id="S3.I4.i4.p1.2.m2.1.1.1.1.2" xref="S3.I4.i4.p1.2.m2.1.1.1.1.2.cmml">d</mi><mo id="S3.I4.i4.p1.2.m2.1.1.1.1.3" xref="S3.I4.i4.p1.2.m2.1.1.1.1.3.cmml">−</mo></msup><mo id="S3.I4.i4.p1.2.m2.1.1.1.3" stretchy="false" xref="S3.I4.i4.p1.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I4.i4.p1.2.m2.1b"><apply id="S3.I4.i4.p1.2.m2.1.1.1.1.cmml" xref="S3.I4.i4.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.I4.i4.p1.2.m2.1.1.1.1.1.cmml" xref="S3.I4.i4.p1.2.m2.1.1.1">superscript</csymbol><ci id="S3.I4.i4.p1.2.m2.1.1.1.1.2.cmml" xref="S3.I4.i4.p1.2.m2.1.1.1.1.2">𝑑</ci><minus id="S3.I4.i4.p1.2.m2.1.1.1.1.3.cmml" xref="S3.I4.i4.p1.2.m2.1.1.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i4.p1.2.m2.1c">(d^{-})</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i4.p1.2.m2.1d">( italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT )</annotation></semantics></math>. In this case, the answer is set to, “This question cannot be answered at the moment.” This allows the LLM to focus more on the knowledge in a document while training.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Retriever Evaluation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">For our evaluation, we utilized normalized Discounted Cumulative Gain(nDCG) as our metric for two key reasons:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Relevance Grading: nDCG takes into account not only whether a document is relevant but also the degree of relevance. We utilize our relevance numbers from our dataset to assign relevance to each “correct” document (each query might have multiple relevant documents)</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Rank Sensitivity: nDCG considers the position of relevant documents in the ranked list. It gives higher scores to relevant documents that appear at the top of the list which is important for us since we can only pass a limited context to our generator.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We reserved  7% of rows from our overall user behavioral data to use for evaluation. While computing the nDCG metrics for our finetuned model, we consider all documents in our document set rather than just documents in our evaluation batch to make the task challenging. Our evaluation data contained 47936 rows. The nDCG for both the
Pretrained Model (MPnet) and our finetuned model are shown in table <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.T4" title="Table 4 ‣ 4.1 Retriever Evaluation ‣ 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">4</span></a></p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Retriever performance evaluated on nDCG for the pretrained MPnet and our finetuned model.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1">
<tr class="ltx_tr" id="S4.T4.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.1"># of Evaluation Rows</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.2">nDCG Pretrained Model (MPnet)</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T4.1.1.3">nDCG Finetuned Model (Ours)</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.1.2.1">47,936</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.1.2.2">0.23</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.1.2.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.3.1">0.391</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">We see significant improvement in retrieval quality for both head and tail queries in our evaluation. Two major improvements include:</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">Domain Understanding – One of the key attributes gained is the ability to understand product names and place importance on them. Pretrained model confuses Photoshop with Photoshop Express, Premiere Pro with Premiere Rush, etc.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">Ability to comprehend shorter sentences. Our user behavior shows that users often type sentences &lt; 15 words to get the data they need. Our model outperforms the baseline on shorter sentences.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">The total number of items indexed in the retrieval index database is 121,547. The data statistics are listed in table <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.T5" title="Table 5 ‣ 4.1 Retriever Evaluation ‣ 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Summary of the retrieval index database.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1">
<tr class="ltx_tr" id="S4.T5.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T5.1.1.1">Dataset Sources</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T5.1.1.2">Num Rows</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T5.1.1.3">Dataset %</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.2.1">Helpx Articles</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.2.2">64,959</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T5.1.2.3">53.4%</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3">
<td class="ltx_td ltx_align_left" id="S4.T5.1.3.1">Community Questions</td>
<td class="ltx_td ltx_align_left" id="S4.T5.1.3.2">15,148</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.1.3.3">12.5%</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4">
<td class="ltx_td ltx_align_left" id="S4.T5.1.4.1">Generated Helpx QA</td>
<td class="ltx_td ltx_align_left" id="S4.T5.1.4.2">40,909</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.1.4.3">33.7%</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.1.5.1">Generated AdobeCare Video QA</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.1.5.2">531</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T5.1.5.3">0.4%</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Query Augmentation via Product Identification</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To address the ambiguous queries across various Adobe products and enhance the overall accuracy and quality of generated responses, we utilize a product-intent extraction model that maps all input texts to effectively maps all input texts to one or more relevant Adobe products. This extracted product information is passed into the retriever, thereby enhancing the relevance of retrieved documents.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.T6" title="Table 6 ‣ 4.2 Query Augmentation via Product Identification ‣ 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">6</span></a> shows the generated answers for retriever that with and without product identification. Before using the product intent model, which means without passing the product identification to the retriever, we get general responses since there are pdf templates in multiple products such as Illustrator, Indesign and Acrobat. By providing product disambiguation, we focus on Adobe Acrobat which is the most common product used with PDFs.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>The generated response with and without product identification for the retriever.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T6.1">
<tr class="ltx_tr" id="S4.T6.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T6.1.1.1.1">without identifier</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T6.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.1.1.2.1">
<span class="ltx_p" id="S4.T6.1.1.2.1.1" style="width:325.2pt;">how do I create a blank PDF</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.1.2.1.1">
<span class="ltx_p" id="S4.T6.1.2.1.1.1" style="width:325.2pt;">To create a blank PDF, select File &gt; New From Template, open the Blank Templates folder, select a template, and select New. Illustrator creates a new document based on the selected blank template.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T6.1.3.1" rowspan="7"><span class="ltx_text" id="S4.T6.1.3.1.1">with identifier</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.1.3.2.1">
<span class="ltx_p" id="S4.T6.1.3.2.1.1" style="width:325.2pt;">how to create a blank PDF</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.1.4.1.1">
<span class="ltx_p" id="S4.T6.1.4.1.1.1" style="width:325.2pt;">To create a blank PDF, you can use Adobe Acrobat. Here are the steps:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.1.5.1.1">
<span class="ltx_p" id="S4.T6.1.5.1.1.1" style="width:325.2pt;">1. Open Acrobat and choose “Tools” &gt; “Create PDF”.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.1.6.1.1">
<span class="ltx_p" id="S4.T6.1.6.1.1.1" style="width:325.2pt;">2. Click on “Blank Page”.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.1.7.1.1">
<span class="ltx_p" id="S4.T6.1.7.1.1.1" style="width:325.2pt;">3. Select the size of your blank page.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.1.8.1.1">
<span class="ltx_p" id="S4.T6.1.8.1.1.1" style="width:325.2pt;">4. Click on “Create” and the new blank PDF will open in Acrobat for editing or saving.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T6.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.1.9.1.1">
<span class="ltx_p" id="S4.T6.1.9.1.1.1" style="width:325.2pt;">I hope this helps! Let me know if you have any further questions.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Quantitative Evaluation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In addition to doing component specific evaluation, we also evaluate the full system both quantitatively and qualitatively. For our quantitative evaluation, we focus on Adobe Acrobat help questions. We utilize 137 user-asked questions and utilize Acrobat product-experts to generate gold responses for each questions. The questions include short common questions such as "<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">How to remove password from the PDF</span>", as well as longer questions such as "<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">I have a large size pdf which I a unbale to share. how do I compress it and share this compressed pdf with others</span>".</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">For the evaluation questions, we generate responses from all candidates and then utilize GPT4<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">openai2023gpt4</span>]</cite> to evaluate the similarity of the candidate-generated answer with the gold human-annotated answer on a scale of 1-5. For GPT-4, we set ’n = 20, temperature = 1, top p = 1’ to sample 20 times to estimate the token probabilities<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">liu2023geval</span>]</cite>. Finally, we take the mean of all scores for a row. The relevance scores are showcased in table <a class="ltx_ref" href="https://arxiv.org/html/2404.14760v2#S4.T7" title="Table 7 ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ Retrieval Augmented Generation for Domain-specific Question Answering"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>The relevance scores for all generation candidate models on Acrobat test set.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T7.1">
<tr class="ltx_tr" id="S4.T7.1.1">
<td class="ltx_td ltx_align_left" id="S4.T7.1.1.1">Model</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T7.1.1.2">Relevance Score (1-5)</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.1.2.1">GPT4</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T7.1.2.2">3.39</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.1.3.1">Finetuned Retriever + GPT4</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T7.1.3.2">4.24</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Qualitative Evaluation - Comparison of the generated answers</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In this section, we’ll review a few examples comparing the output of our system with OpenAI’s ChatGPT<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span> (<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com" title="">https://chat.openai.com</a>) </span></span></span>.</p>
</div>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Example 1</h5>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p1">
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I3.i1.p1.1.1">Question:</span> What is firefly?</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I3.i2.p1.1.1">Answer from ChatGPT:</span> Fireflies are a type of bioluminescent insect known for their ability to produce light. They belong to the family Lampyridae, which is part of the order Coleoptera, the beetles. Fireflies are also commonly referred to as lightning bugs in some regions, particularly in North America. […]</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p2.1">OpenAI’s ChatGPT does not answer about Adobe Firefly. Even if the question is expressed more specifically, ChatGPT does not know the answer. By contrast, the answer to our system is more accurate.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p3">
<ul class="ltx_itemize" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.1">Question:</span>
What is Adobe Firefly?</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I4.i2.p1.1.1">Answer from ChatGPT:</span></p>
</div>
<div class="ltx_para" id="S4.I4.i2.p2">
<p class="ltx_p" id="S4.I4.i2.p2.1">As of my last knowledge update in September 2021, there is no widely recognized software or product called "Adobe Firefly" from Adobe Inc. Adobe is known for its extensive range of software products, including Adobe Photoshop, Adobe Illustrator, Adobe Premiere Pro, Adobe Acrobat, and many others, which are commonly used for design, video editing, document management, and various creative and productivity tasks. […]</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p4">
<ul class="ltx_itemize" id="S4.I5">
<li class="ltx_item" id="S4.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I5.i1.p1">
<p class="ltx_p" id="S4.I5.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I5.i1.p1.1.1">Answer from our system:</span></p>
</div>
<div class="ltx_para" id="S4.I5.i1.p2">
<p class="ltx_p" id="S4.I5.i1.p2.1">Firefly is a family of creative generative AI models coming to Adobe products, with the ability to use everyday language to generate extraordinary new content. It offers unlimited creative options for digital imaging and photography, illustration, artwork, graphic design, video, marketing and social media, and 3D modeling. It can generate custom vectors, brushes, and textures from just a few words or even a sketch, and can change the mood, atmosphere, or even the weather in a video. It also offers generative image compositing and the ability to turn simple 3D compositions into photorealistic images.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Example 2</h5>
<div class="ltx_para" id="S4.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px2.p1.1">ChatGPT might fail to generate the correct answer for some questions related to the process of how to use Adobe products, while our system can generate the step-by-step answer.</p>
<ul class="ltx_itemize" id="S4.I6">
<li class="ltx_item" id="S4.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I6.i1.p1">
<p class="ltx_p" id="S4.I6.i1.p1.1">Question: How can I customize my birthday poster using Adobe Express?</p>
</div>
</li>
<li class="ltx_item" id="S4.I6.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I6.i2.p1">
<p class="ltx_p" id="S4.I6.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I6.i2.p1.1.1">Answer from ChatGPT:</span></p>
</div>
<div class="ltx_para" id="S4.I6.i2.p2">
<p class="ltx_p" id="S4.I6.i2.p2.1">As of my last knowledge update in September 2021, there isn’t a specific application called “Adobe Express.” However, Adobe offers a range of creative applications, and it’s possible that a new one has been released since then. Adobe Spark Post is a popular tool for creating posters, and it’s quite user-friendly. Here’s a general guide on how to customize a birthday poster using Adobe Spark Post or a similar Adobe application: […]</p>
</div>
</li>
<li class="ltx_item" id="S4.I6.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I6.i3.p1">
<p class="ltx_p" id="S4.I6.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I6.i3.p1.1.1">Answer from our system:</span></p>
</div>
<div class="ltx_para" id="S4.I6.i3.p2">
<p class="ltx_p" id="S4.I6.i3.p2.1">Steps:</p>
</div>
<div class="ltx_para" id="S4.I6.i3.p3">
<p class="ltx_p" id="S4.I6.i3.p3.1">1. Open Adobe Express for free on your desktop or mobile device.</p>
</div>
<div class="ltx_para" id="S4.I6.i3.p4">
<p class="ltx_p" id="S4.I6.i3.p4.1">2. Choose from the available birthday poster templates or create from a blank canvas.</p>
</div>
<div class="ltx_para" id="S4.I6.i3.p5">
<p class="ltx_p" id="S4.I6.i3.p5.1">3. Upload your own photos or add stock images and designs from the libraries.</p>
</div>
<div class="ltx_para" id="S4.I6.i3.p6">
<p class="ltx_p" id="S4.I6.i3.p6.1">4. Brand your poster by uploading your logo and using fonts and colors that match your aesthetic.</p>
</div>
<div class="ltx_para" id="S4.I6.i3.p7">
<p class="ltx_p" id="S4.I6.i3.p7.1">5. Share your poster by hitting the publish button and instantly downloading it to your device. Print it out or share it online.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Example 3</h5>
<div class="ltx_para" id="S4.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px3.p1.1">In some cases, ChatGPT knows the answer. However, it is rather verbose and it adds some information that is useless to users. In this example, ChatGPT added useless information to Acrobat DC users such as “Please note that Adobe Acrobat DC is a paid software, and you may need a subscription or license to access certain features. Ad”.
By contrast, the answer of our system is concise, correct, and does not add any useless information:</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px3.p2">
<ul class="ltx_itemize" id="S4.I7">
<li class="ltx_item" id="S4.I7.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I7.i1.p1">
<p class="ltx_p" id="S4.I7.i1.p1.1">Question: How can remove one page in a PDF in Acrobat DC?</p>
</div>
</li>
<li class="ltx_item" id="S4.I7.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I7.i2.p1">
<p class="ltx_p" id="S4.I7.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I7.i2.p1.1.1">Answer from ChatGPT:</span></p>
</div>
<div class="ltx_para" id="S4.I7.i2.p2">
<p class="ltx_p" id="S4.I7.i2.p2.1">However, it is rather verbose and all it adds some information that is useless to Acrobat DC users such as “Please note that Adobe Acrobat DC is a paid software, and you may need a subscription or license to access certain features. Ad”</p>
</div>
</li>
<li class="ltx_item" id="S4.I7.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I7.i3.p1">
<p class="ltx_p" id="S4.I7.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I7.i3.p1.1.1">Answer from our system:</span></p>
</div>
<div class="ltx_para" id="S4.I7.i3.p2">
<p class="ltx_p" id="S4.I7.i3.p2.1">Steps:</p>
</div>
<div class="ltx_para" id="S4.I7.i3.p3">
<p class="ltx_p" id="S4.I7.i3.p3.1">1. Choose the “Organize Pages” tool.</p>
</div>
<div class="ltx_para" id="S4.I7.i3.p4">
<p class="ltx_p" id="S4.I7.i3.p4.1">2. Select the page thumbnail of any page or pages you want to delete, then click the “Delete” icon to remove the page or pages from the file.</p>
</div>
<div class="ltx_para" id="S4.I7.i3.p5">
<p class="ltx_p" id="S4.I7.i3.p5.1">3. After clicking the “Delete” icon, click “OK” to apply changes to remove the pages.</p>
</div>
<div class="ltx_para" id="S4.I7.i3.p6">
<p class="ltx_p" id="S4.I7.i3.p6.1">4. Save the file.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Example 4</h5>
<div class="ltx_para" id="S4.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px4.p1.1">Another key area where retrieval is important is when pricing information is needed. Since pricing and offers change over time, retrieving the latest information is key.
ChatGPT provides a general answer to the question whereas our solution is able to provide the latest numbers.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px4.p2">
<ul class="ltx_itemize" id="S4.I8">
<li class="ltx_item" id="S4.I8.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I8.i1.p1">
<p class="ltx_p" id="S4.I8.i1.p1.1">Question: How much does Premiere Pro cost for students?</p>
</div>
</li>
<li class="ltx_item" id="S4.I8.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I8.i2.p1">
<p class="ltx_p" id="S4.I8.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I8.i2.p1.1.1">Answer from ChatGPT:</span></p>
</div>
<div class="ltx_para" id="S4.I8.i2.p2">
<p class="ltx_p" id="S4.I8.i2.p2.1">As of my last knowledge update in September 2021, Adobe offers a special pricing plan for students and teachers called Adobe Creative Cloud for Education. The cost for Adobe Premiere Pro, as part of this plan, varies depending on the subscription you choose and your location. Here are some general pricing guidelines for students:</p>
</div>
<div class="ltx_para" id="S4.I8.i2.p3">
<p class="ltx_p" id="S4.I8.i2.p3.1">Adobe Creative Cloud All Apps Plan: This plan includes access to all Adobe Creative Cloud applications, including Premiere Pro. The student/teacher pricing typically ranged from around $19.99 to $29.99 per month, depending on the specific promotional offers available at the time. Adobe occasionally offers discounts or promotions for students, so it’s a good idea to check their website for the most up-to-date pricing.</p>
</div>
<div class="ltx_para" id="S4.I8.i2.p4">
<p class="ltx_p" id="S4.I8.i2.p4.1">Premiere Pro Single App Plan: If you only need access to Premiere Pro, Adobe also offers a single app plan. As of my last update, this plan for students and teachers was priced at around $20.99 per month.</p>
</div>
</li>
<li class="ltx_item" id="S4.I8.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I8.i3.p1">
<p class="ltx_p" id="S4.I8.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I8.i3.p1.1.1">Answer from our system:</span></p>
</div>
<div class="ltx_para" id="S4.I8.i3.p2">
<p class="ltx_p" id="S4.I8.i3.p2.1">Premiere Pro is available for students as a part of Adobe’s Creative Cloud All Apps plan, which includes 20+ apps like Adobe Photoshop, Acrobat, and Illustrator. The regular price for this plan is US$54.99/month, but eligible students can get it at a discounted price of US$19.99/month for the first year and US$29.99/month after that, which is over 60% off. This plan also includes 100GB of cloud storage and access to hundreds of thousands of royalty-free assets via Adobe Stock for commercial or personal use. Students can also try Premiere Pro for free with a 7-day trial of the Creative Cloud All Apps plan. For more information, please see Adobe’s terms and conditions or compare plans and pricing.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Example 5</h5>
<div class="ltx_para" id="S4.SS4.SSS0.Px5.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px5.p1.1">This example shows that OpenAI’s ChatGPT fails to give the proper answer to the question because of missing the fact that the user wants to trim the video before adding it to the timeline.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px5.p2">
<p class="ltx_p" id="S4.SS4.SSS0.Px5.p2.1">Our system is better suited for this more expert-level knowledge and provides the correct answer.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px5.p3">
<ul class="ltx_itemize" id="S4.I9">
<li class="ltx_item" id="S4.I9.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I9.i1.p1">
<p class="ltx_p" id="S4.I9.i1.p1.1">Question: How do I trim a video before adding it in the timeline in Premiere Pro?</p>
</div>
</li>
<li class="ltx_item" id="S4.I9.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I9.i2.p1">
<p class="ltx_p" id="S4.I9.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I9.i2.p1.1.1">Answer from ChatGPT:</span></p>
</div>
<div class="ltx_para" id="S4.I9.i2.p2">
<p class="ltx_p" id="S4.I9.i2.p2.1">ChatGPT missed the fact that the user wants to trim the video before adding it in the timeline.</p>
</div>
</li>
<li class="ltx_item" id="S4.I9.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I9.i3.p1">
<p class="ltx_p" id="S4.I9.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I9.i3.p1.1.1">Answer from our system:</span> Steps:</p>
</div>
<div class="ltx_para" id="S4.I9.i3.p2">
<p class="ltx_p" id="S4.I9.i3.p2.1">1. Double-click on any of the media in the project panel to preload it into the source monitor.</p>
</div>
<div class="ltx_para" id="S4.I9.i3.p3">
<p class="ltx_p" id="S4.I9.i3.p3.1">2. Play or scrub the media.</p>
</div>
<div class="ltx_para" id="S4.I9.i3.p4">
<p class="ltx_p" id="S4.I9.i3.p4.1">3. Mark an endpoint by hitting the I key or clicking the icon.</p>
</div>
<div class="ltx_para" id="S4.I9.i3.p5">
<p class="ltx_p" id="S4.I9.i3.p5.1">4. Drag your video to where you’d like to have an outpoint, either hitting the mark out icon or pressing O.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px5.p4">
<p class="ltx_p" id="S4.SS4.SSS0.Px5.p4.1">Note that answer quality aside, our system has many other upsides over using ChatGPT: privacy, cost, updated to incorporate the latest information about Adobe products, our system can have hyperlinks in the answers or even links to panels/windows/tools/etc. within the product, and we can gather user feedback to improve our answers over time. This makes an in-house question-answering solution much preferable to just using ChatGPT.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduce a novel Question-Answering system for Adobe products, including the retriever and a generator. We use our Adobe documents, such as Helpx documents as the retriever training dataset. Furthermore, we derive a metric for relevance and use it for the weighted cross entropy loss function when training the retriever. We build our retrieval database with multiple sources of data. We remove the personal information and generate QA pairs for the database in the preprocessing. To address the challenge of disambiguation for vague queries, we add the product-intent information to the retriever. We train our QA system using grounded documents, negative documents, and question-answer pairs. We compare our QA system with the openAI ChatGPT on some questions related to Adobe products. The experiments show that while ChatGPT fails to answer the questions correctly or generates some useless information for the user, our system can generate up-to-date, concise, and correct answers.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed May 29 16:11:27 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
