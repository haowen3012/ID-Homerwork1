<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation</title>
<!--Generated on Mon Jun 17 02:31:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.05654v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S1" title="In DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S2" title="In DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S2.SS1" title="In 2 Related Work ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Retrieval-augmented Generation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S2.SS2" title="In 2 Related Work ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Evaluation of RAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S3" title="In DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Evaluate Retrieval-Augmented Generation via In-domain Scenarios</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S3.SS1" title="In 3 Evaluate Retrieval-Augmented Generation via In-domain Scenarios ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Construction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4" title="In DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4.SS1" title="In 4 Experiment ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Main settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4.SS2" title="In 4 Experiment ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4.SS3" title="In 4 Experiment ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Overall Experimental Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4.SS4" title="In 4 Experiment ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Experiments on Structural Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4.SS5" title="In 4 Experiment ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Robustness of LLMs on Noisy References</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4.SS6" title="In 4 Experiment ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Faithfulness of LLMs in External References</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S5" title="In DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S6" title="In DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitations</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuting Wang<sup class="ltx_sup" id="id12.9.id1">1</sup>, Jiongnan Liu<sup class="ltx_sup" id="id13.10.id2">1</sup>, Shiren Song<sup class="ltx_sup" id="id14.11.id3">1</sup>, Jiehan Cheng<sup class="ltx_sup" id="id15.12.id4">1</sup>, Yuqi Fu<sup class="ltx_sup" id="id16.13.id5">1</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id8.8.3"> Peidong Guo<sup class="ltx_sup" id="id8.8.3.1"><span class="ltx_text ltx_font_medium" id="id8.8.3.1.1">2</span></sup>, Kun Fang<sup class="ltx_sup" id="id8.8.3.2"><span class="ltx_text ltx_font_medium" id="id8.8.3.2.1">2</span></sup>, Yutao Zhu<sup class="ltx_sup" id="id8.8.3.3"><span class="ltx_text ltx_font_medium" id="id8.8.3.3.1">1</span></sup>, </span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id9.1.1">Zhicheng Dou<sup class="ltx_sup" id="id9.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id9.1.1.1.1">1∗</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id17.4.id1">1</sup>Gaoling School of Artificial Intelligence, Renmin University of China 
<br class="ltx_break"/><sup class="ltx_sup" id="id18.5.id2">2</sup>Baichuan Intelligent Technology
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id19.6.id3">{wangshuting, liujn, dou}@ruc.edu.cn</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id20.id1">Retrieval-Augmented Generation (RAG) offers a promising solution to address various limitations of Large Language Models (LLMs), such as hallucination and difficulties in keeping up with real-time updates. This approach is particularly critical in expert and domain-specific applications where LLMs struggle to cover expert knowledge. Therefore, evaluating RAG models in such scenarios is crucial, yet current studies often rely on general knowledge sources like Wikipedia to assess the models’ abilities in solving common-sense problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific context, college enrollment. We identified six required abilities for RAG models, including the ability in conversational RAG, analyzing structural information, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding multi-document interactions. Each ability has an associated dataset with shared corpora to evaluate the RAG models’ performance. We evaluated popular LLMs such as Llama, Baichuan, ChatGLM, and GPT models. Experimental results indicate that existing closed-book LLMs struggle with domain-specific questions, highlighting the need for RAG models to solve expert problems. Moreover, there is room for RAG models to improve their abilities in comprehending conversational history, analyzing structural information, denoising, processing multi-document interactions, and faithfulness in expert knowledge. We expect future studies could solve these problems better.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.11">
<p class="ltx_p" id="p1.11.12"><span class="ltx_text ltx_font_bold" id="p1.11.12.1">DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.11.11" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.11.11.11" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.11.11.11.11">
<span class="ltx_tr" id="p1.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.5.5"><span class="ltx_text ltx_font_bold" id="p1.5.5.5.5.5.5.5">Shuting Wang<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.1"><span class="ltx_text ltx_font_medium" id="p1.5.5.5.5.5.5.5.1.1">1</span></sup>, Jiongnan Liu<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.2"><span class="ltx_text ltx_font_medium" id="p1.5.5.5.5.5.5.5.2.1">1</span></sup>, Shiren Song<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.3"><span class="ltx_text ltx_font_medium" id="p1.5.5.5.5.5.5.5.3.1">1</span></sup>, Jiehan Cheng<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.4"><span class="ltx_text ltx_font_medium" id="p1.5.5.5.5.5.5.5.4.1">1</span></sup>, Yuqi Fu<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.5"><span class="ltx_text ltx_font_medium" id="p1.5.5.5.5.5.5.5.5.1">1</span></sup></span></span></span>
<span class="ltx_tr" id="p1.9.9.9.9.9">
<span class="ltx_td ltx_align_center" id="p1.9.9.9.9.9.4"><span class="ltx_text ltx_font_bold" id="p1.8.8.8.8.8.3.3">Peidong Guo<sup class="ltx_sup" id="p1.8.8.8.8.8.3.3.1"><span class="ltx_text ltx_font_medium" id="p1.8.8.8.8.8.3.3.1.1">2</span></sup>, Kun Fang<sup class="ltx_sup" id="p1.8.8.8.8.8.3.3.2"><span class="ltx_text ltx_font_medium" id="p1.8.8.8.8.8.3.3.2.1">2</span></sup>, Yutao Zhu<sup class="ltx_sup" id="p1.8.8.8.8.8.3.3.3"><span class="ltx_text ltx_font_medium" id="p1.8.8.8.8.8.3.3.3.1">1</span></sup>, </span> and <span class="ltx_text ltx_font_bold" id="p1.9.9.9.9.9.4.4">Zhicheng Dou<sup class="ltx_sup" id="p1.9.9.9.9.9.4.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.9.9.9.9.9.4.4.1.1">1∗</span></sup></span></span></span>
<span class="ltx_tr" id="p1.10.10.10.10.10">
<span class="ltx_td ltx_align_center" id="p1.10.10.10.10.10.1"><sup class="ltx_sup" id="p1.10.10.10.10.10.1.1">1</sup>Gaoling School of Artificial Intelligence, Renmin University of China</span></span>
<span class="ltx_tr" id="p1.11.11.11.11.11">
<span class="ltx_td ltx_align_center" id="p1.11.11.11.11.11.1"><sup class="ltx_sup" id="p1.11.11.11.11.11.1.1">2</sup>Baichuan Intelligent Technology</span></span>
<span class="ltx_tr" id="p1.11.11.11.11.12">
<span class="ltx_td ltx_align_center" id="p1.11.11.11.11.12.1"><span class="ltx_text ltx_font_typewriter" id="p1.11.11.11.11.12.1.1">{wangshuting, liujn, dou}@ruc.edu.cn</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>Corresponding author.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recently, the emergence of large language models (LLMs) has revolutionized the way we access information. These LLMs are typically trained on vast amounts of web documents using the next token prediction task, which equips them with a wide range of world knowledge and advanced capabilities in understanding and generating natural language. However, despite these impressive attributes, they still face significant challenges, including hallucinations, difficulties in keeping up with real-time updates, etc <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib4" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Retrieval-Augmented Generation (RAG), which involves retrieving external information from Information Retrieval (IR) systems to provide reliable knowledge, is a promising and widely adopted approach to overcome the above limitations.
Furthermore, when deploying LLMs in practice, such as building question-answering systems for enterprises or some expert fields, it is more vital to provide domain-specific information for LLMs <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib33" title="">2024</a>)</cite> since they are likely unequipped with this expert knowledge.
For example, consulting firm financial statements or data aggregation in the investment industry are all widely used scenarios of RAG systems. Nevertheless, due to the problem of data privacy, these corpora cannot be incorporated into the training data of LLM, hence RAG systems are needed to plug these data into the LLMs in the form of external knowledge.
Thus, evaluating the performance of RAG in domain-specific scenarios becomes imperative. However, existing studies <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib4" title="">2024</a>)</cite> predominantly rely on general knowledge sources, such as Wikipedia, as external knowledge bases to evaluate RAG models on dealing with commonsense or hot knowledge-intensive tasks <cite class="ltx_cite ltx_citemacro_cite">Kwiatkowski et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib11" title="">2019</a>); Joshi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib10" title="">2017</a>); Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib30" title="">2018</a>); Petroni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib20" title="">2021</a>)</cite>. Such a method may not fully evaluate the ability of RAG models to solve domain-specific problems.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Therefore, the use of domain-specific corpora and questions is essential to assess the ability of the LLM to effectively use external knowledge from these specific fields to solve expert problems.
In this paper, we identify six vital abilities to comprehensively evaluate RAG models, which are visualized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>, from three perspectives:</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mo id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><ci id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Understanding of user intents.</span> In traditional web information retrieval methods, such as search engines, understanding the actual user intents has always been a crucial step and studied in the literature <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib34" title="">2020</a>); Yao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib31" title="">2020</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib26" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib27" title="">b</a>); Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib35" title="">2021</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib3" title="">2022</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib25" title="">2024</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib16" title="">2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib17" title="">2022</a>); Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib5" title="">2023</a>)</cite>. Nowadays, LLMs demonstrate remarkable abilities in various natural language processing tasks. However, comprehending user information needs and providing accurate responses is a more intricate task, especially in conversational scenarios that require clarifying the current user intents based on previous interactions.
As a result, the <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">conversation ability</span> is critical to building a user-friendly RAG system.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mo id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Analysis of retrieved documents.</span> Apart from understanding user questions, the analysis of external documents plays a critical role in RAG systems.
Considering that web pages not only contain massive textual knowledge but also intricate structures, such as HTML structures, which may also contain valuable information. It is also important for LLMs to <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">comprehend the structural information</span> from the provided knowledge, hence providing accurate and reliable responses.
Furthermore, the inherent difficulty for LLMs in acquiring in-domain knowledge underscores the importance of trusting external expert knowledge to bridge gaps in their own perception. In other words, when faced with in-domain problems, it is more reliable for LLMs to answer questions based on external expert knowledge rather than relying on their own knowledge, which may be limited and prone to hallucination. Thus, assessing the <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">faithfulness of LLMs on external expert knowledge</span> is also an important task.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><ci id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Interactions between intents and documents.</span> Given the provided external documents, LLMs must not only accurately comprehend the knowledge contained within them but also identify the relevant portions that contribute to solving the user’s current problem. Typically, not all provided information is useful for solving problems, as there may be a significant amount of noise that potentially hinders the prediction of accurate results. Thus, assessing <span class="ltx_text ltx_font_italic" id="S1.p6.1.2">denoising ability</span> of RAG models is also critical. At the same time, this problem could be more distinct for time-sensitive questions, where the answers may change over time. Therefore, the RAG models’s <span class="ltx_text ltx_font_italic" id="S1.p6.1.3">ability to solve time-sensitive problems</span> is another angle to evaluate their denoising abilities. Additionally, due to the complexity of user intents, answering some questions may require interactions between multiple documents and questions, highlighting the need for LLMs to effectively navigate and integrate information from various sources. As a result, we also propose to evaluate RAG models’ <span class="ltx_text ltx_font_italic" id="S1.p6.1.4">ability to understand the interaction between multi-documents and complex questions</span>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="371" id="S1.F1.g1" src="x1.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Important abilities for RAG models.</figcaption>
</figure>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Specifically, we constructed a comprehensive dataset that evaluates the above abilities of RAG models in a domain-specific scenario, namely DomainRAG. The application scenario is the enrollment system of a university in China (with official permission).
In addition to an extractive QA dataset that assesses basic QA ability, we further annotated the following sub-datasets, each targeting a specific ability, <span class="ltx_text ltx_font_italic" id="S1.p7.1.1">i.e.</span>, conversational QA, structural QA, faithful QA, time-sensitive QA, noisy QA, and multi-document QA.
Concretely, the conversational QA dataset simulates complex and realistic scenarios where users interact with models through multiple turns to fulfill their information needs.
The structural QA is designed to test the ability of LLMs to understand and infer answers from structured information of external knowledge, and the faithful QA evaluates the faithfulness of LLMs in handling external knowledge.
The left three sub-datasets assess the capabilities of LLMs in handling the complex interaction between questions and documents.
The noisy QA involves providing external knowledge with noisy information, challenging LLMs to filter out irrelevant or misleading content. The time-sensitive QA introduces time-sensitive questions, where the answers may vary at different timestamps. Lastly, the multi-document QA requires LLMs to integrate information from multiple external documents to provide satisfactory answers to complex questions.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Possibly, some sub-datasets may also indirectly evaluate LLMs’ abilities from other perspectives. However, to decouple the assessment of each capability, we assign each sub-dataset to the category that best represents its primary focus.</span></span></span>
In experiments, we evaluated seven popular LLMs, including Llama2-7B-chat, Llama2-13B-chat, Llama2-70B-chat, Baichuan2-7B-chat, Baichuan2-33B-32k, ChatGLM2-6B-32k, and GPT-3.5-turbo-1106. Generally, we find that
(1) In domain-specific scenarios, most LLMs struggle to exactly answer the user questions without the aid of external knowledge. It highlights the importance of RAG models in such applications.
(2) Leveraging HTML content is beneficial for LLMs to generate more accurate answers. However, the ability to comprehend and analyze structural information is not yet well-developed in all LLMs. Therefore, when deploying RAG models in practice, it is crucial to choose an LLM suitable for the specific application needs.
(3) There is a large room for RAG models to improve their performance in complex scenarios involving various kinds of information sources. In conversational scenarios, RAG models need to accurately understand the user’s intent based on historical information. In multi-doc QA, RAG models must comprehend the intricate relationships between multiple documents and questions. These challenges highlight the need for further investigation of high-quality RAG models.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Retrieval-augmented Generation Models</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">To alleviate the hallucination problem of language models, Retrieval-augmented generation (RAG) strategy <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib7" title="">2023</a>); Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib36" title="">2023</a>)</cite> is proposed by providing external references to LMs to help them provide more accurate and factual answers. In particular, RAG approaches usually devise a retrieval model to collect relevant documents or passages according to user queries from the corpus. Then, these retrieved references are fed together with the user queries into the downstream language models to generate answers.
Traditional approaches <cite class="ltx_cite ltx_citemacro_cite">Guu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib8" title="">2020</a>); Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib13" title="">2020</a>)</cite> in this area mainly focus on supporting language models with limited parameters such as BERT, BART, and T5.
Recently, with the development of Large language models, more and more researchers have paid attention to improving the RAG performance based on large language models by considering the retrieval frequency <cite class="ltx_cite ltx_citemacro_cite">Lazaridou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib12" title="">2022</a>); Ram et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib21" title="">2023</a>); Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib9" title="">2023</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib18" title="">2023</a>)</cite>, designing delicate CoTs <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib32" title="">2023</a>); Trivedi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib24" title="">2023</a>)</cite>, training the retrieval models and language models together <cite class="ltx_cite ltx_citemacro_cite">Asai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib2" title="">2023</a>); Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib15" title="">2023</a>)</cite>, comprising the retrieved references to fit the input length limit <cite class="ltx_cite ltx_citemacro_cite">Arefeen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib1" title="">2023</a>); Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib29" title="">2023</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib14" title="">2024</a>)</cite>. Research <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib22" title="">2023</a>)</cite> has shown that by incorporating the retrieved documents, RAG models do respond with few mistakes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Evaluation of RAG</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Previous studies in the retrieval-augmented generation area mainly conduct experiments on open-domain QA datasets such NQ <cite class="ltx_cite ltx_citemacro_cite">Kwiatkowski et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib11" title="">2019</a>)</cite> and HotpotQA<cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib30" title="">2018</a>)</cite> using Wikipedia as the retrieval corpus. Though this general evaluation setting can somehow reflect the quality of answers generated by the RAG models, it fails to analyze the abilities of these models from different perspectives such as intent understanding and faithfulness to the references. Besides, since Wikipedia is widely used in the pre-training of language models and the information in the retrieved documents may have been learned by LLMs, it is questionable whether RAG models really utilize the references to answer questions instead of their intrinsic knowledge. Recently, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib4" title="">2024</a>)</cite> alleviated this problem by proposing a specialized RAG benchmark to analyze the four disentangled fundamental abilities of different large language models. However, it still focuses on the open-domain QA, without considering the LLM’s ability under in-domain situations. To thoroughly assess the abilities of RAG models, in this paper, we propose to leverage an in-domain document corpus collected from the enrollment websites of a Chinese university to evaluate the capabilities of RAG from multi-aspects.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluate Retrieval-Augmented Generation via In-domain Scenarios</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To avoid that the external knowledge has been studied well in pre-training or instruction tuning of LLMs, we focus on a domain-specific Chinese application scenario, <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">i.e.</span>, college enrollment. This scenario primarily involves questions related to long-tail and domain-specific information such as admission introductions, admission policies, and details of schools or departments. Therefore, it is difficult for LLMs to rely solely on their internal knowledge to answer the user’s questions. Instead, they need to heavily depend on external knowledge resources. To comprehensively evaluate the aforementioned capabilities of RAG models, we annotated seven sub-datasets, and the corresponding data construction process is demonstrated below.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Construction</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To acquire the document corpus for this scenario, we crawled web pages from the admission official website with official permission. We not only extracted their text contents but also reserved the original HTML contents to facilitate the construction of the structural QA dataset. Given the lengthy nature of web pages, we further split the text contents of each web page into passages using a sliding window of 256 length and 128 overlap. The numbers of web pages and passages are 1,686 and 14,406 respectively. Finally, we created two document corpora, a text corpus and an HTML corpus. The evaluation datasets are built by initially being generated from powerful generative models (ChatGPT or GPT-4), then being corrected manually.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mo id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Extractive QA Dataset.</span>
We first randomly sampled document passages from the text corpus. These passages were then incorporated into the prompt designed for ChatGPT, which generated question-answer (QA) pairs based on the provided passages. To ensure the in-domain nature of questions, we specifically instructed ChatGPT to generate questions that cannot be answered without providing external information. The selected passages can be directly considered as positive references for the corresponding questions in the dataset.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.4"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mo id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.4.1">Conversational QA Dataset.</span>
To build question-answering conversations, we began by choosing documents with substantial content, primarily focusing on the introduction web pages of each school. We then utilized ChatGPT to generate domain-specific questions according to each passage within the selected documents. This process resulted in a collection of question-answer pairs associated with each document. To test the conversational intent understanding ability of RAG models, we simplified each question in the QA pair list (except the first one) by removing entities that duplicate the preceding questions. The revised QA list can be regarded as a vanilla conversation. Furthermore, we derived multiple conversation samples from a given vanilla one. Specifically, at the <math alttext="t-" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">t</mi><mo id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="latexml" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">limit-from</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝑡</ci><minus id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">t-</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_t -</annotation></semantics></math>th step, the QAs from the previous <math alttext="t-1" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><mrow id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">t</mi><mo id="S3.SS1.p3.3.m3.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.cmml">−</mo><mn id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><minus id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1"></minus><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝑡</ci><cn id="S3.SS1.p3.3.m3.1.1.3.cmml" type="integer" xref="S3.SS1.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">t-1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">italic_t - 1</annotation></semantics></math> steps were considered as historical conversations, and the current <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">italic_t</annotation></semantics></math>-step QA was treated as the question and the golden answer.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mo id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Structural QA Dataset.</span>
To assess the understanding capabilities of RAG models on structural information (we focus on table structures in this paper), we first selected web pages containing table structures. Then, we offered the HTML contents of these web pages to ChatGPT and instructed it to generate QA pairs where the answers are derived from the table information. To accommodate the input length limits of ChatGPT, we preprocessed the HTML content by removing irrelevant elements, such as HTML comments, script tags, etc. For each QA pair, we provided both the HTML and corresponding pure text content of the positive document. This approach not only allows us to evaluate the models’ ability to analyze and comprehend HTML structures but also to compare the effectiveness of using HTML structural information versus pure texts for solving problems.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mo id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">Faithful QA Dataset.</span>
To test the faithfulness of LLMs to domain-specific knowledge, we first provided document contents to GPT4 and prompt it to generate QA pairs that rely solely on external expert knowledge, rather than common-sense information. This step is similar to the process used in creating the extractive QA dataset. Furthermore, to ensure the generation quality, we manually filtered out the QA pairs that could be answered using knowledge contained within LLMs themselves. Finally, we modify the answer-related information in the positive references to build anti-references and corresponding anti-answers.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overall results on the extractive, conversational, time-sensitive, and multi-doc datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Dataset</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Count</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Avg. Q Len</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Avg. A Len</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Extractive</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">90</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">25.09</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">8.17</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3">
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Conversational</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">49</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">16.65</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">35.66</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4">
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Structural</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">94</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">35.48</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">6.07</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5">
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Time-sensitive</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">65</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">21.38</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">4.67</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6">
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Multi-document</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">48</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">35.90</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">86.69</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.7.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Faithfulness</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.7.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">49</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.7.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">27.29</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.7.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">12.85 /11.80</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p6.1.m1.1"><semantics id="S3.SS1.p6.1.m1.1a"><mo id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><ci id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Noisy QA Dataset.</span>
To evaluate LLMs’ robustness to noisy information in provided references, we expanded upon the extractive QA dataset to create the noisy dataset. Concretely, for each piece of data, we randomly sampled several irrelevant passages from the text corpus to construct noisy information. During the test experiments, we varied the number of irrelevant passages selected and combined them with positive references to build external references with different noise ratios.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p7.1.m1.1"><semantics id="S3.SS1.p7.1.m1.1a"><mo id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><ci id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.1">Time-sensitive QA Dataset.</span>
Since the dataset is static, it is difficult to evaluate the abilities of RAG in answering real-time questions. Inspired by <cite class="ltx_cite ltx_citemacro_cite">Dhingra et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib6" title="">2022</a>)</cite>, we focused on generating questions that have different answers at different timestamps. To indicate the timestamp of each question, we included a “date” attribute in each data sample. It is challenging for ChatGPT to automatically generate time-sensitive question-answer pairs that require rich prior domain knowledge. Therefore, we manually design possible questions and identify answerable document passages to build answers and positive references.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p8.1.m1.1"><semantics id="S3.SS1.p8.1.m1.1a"><mo id="S3.SS1.p8.1.m1.1.1" xref="S3.SS1.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.1.m1.1b"><ci id="S3.SS1.p8.1.m1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S3.SS1.p8.1.1">Multi-doc QA Dataset.</span>
To address complex questions that can not be fully answered by simply extracting information at the entity level, it becomes necessary to aggregate information from multiple relevant documents. To build a dataset that evaluates such ability, we follow a specific approach. First,
we identified a set of relevant documents that share similar topics or themes, such as introductions to relevant institutes or majors. These documents serve as the basis for generating the dataset. Next, we provide the text contents of these relevant documents to GPT4, which generates questions that require answers derived from multiple document contents.</p>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.1">The statistical information of our datasets is demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S3.T1" title="Table 1 ‣ 3.1 Data Construction ‣ 3 Evaluate Retrieval-Augmented Generation via In-domain Scenarios ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>. The noisy dataset is derived from the extractive dataset, its average lengths of queries and answers are the same as the extractive. There are two items of average answer length (Avg. A Len) of the faithful dataset where the first is the golden answer, the second is the anti-answer.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The anonymous link of our dataset is provided here:
<a class="ltx_ref ltx_href" href="https://github.com/ShootingWong/DomainRAG" title="">https://github.com/ShootingWong/DomainRAG</a>.</span></span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Main settings</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We first conducted experiments using the following external knowledge settings,</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">(1) Close-book: No external domain-specific knowledge was provided to assess whether LLMs could solve these expert problems themselves.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">(2) Golden reference: We provided human-annotated positive references for LLMs to explore the upper bounds of their abilities.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">(3) Retrieved reference: Simulating real-world applications of RAG models, we provided them with retrieved documents. We chose BM25 <cite class="ltx_cite ltx_citemacro_cite">Robertson and Zaragoza (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib23" title="">2009</a>)</cite> and BGE-base-zh-v1.5 <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib28" title="">2023</a>)</cite> as two classical retrievers to represent sparse and dense retrieval.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">(4) Noisy reference. To test the robustness of LLMs on noisy external knowledge, we provided different levels of irrelevant references blended with golden references. We also investigated the impact of the position of golden references within the noisy references on RAG models’ performances.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">(5) Structural reference. In the experiments on the structural QA dataset, we provided two versions of golden references, <span class="ltx_text ltx_font_italic" id="S4.SS1.p6.1.1">i.e.</span>, HTML and pure texts, for LLMs to evaluate the abilities to analyze HTML structures and compare the effect of structural information versus pure texts for RAG tasks.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">6) Anti-reference. In the faithful QA dataset, we provided both golden and anti-references for LLMs in the same question to compare the faithfulness of LLMs in utilizing expert external knowledge.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1">We selected six commonly used LLMs, including Llama2-7B-chat, Llama2-13B-chat, Llama2-70B-chat, Baichuan2-7B-chat, Baichuan2-33B-32k, ChatGLM2-6B-32k, and gpt-3.5-turbo-1106 to compare their abilities comprehensively. Note that we chose the Baichuan2-33B-32k version with a built-in general retrieval system to further assess the effectiveness of general knowledge sources in our domain-specific scenario.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To evaluate model performance, we chose four widely used metrics: Two versions of exact-match, where the one assesses whether the ground truth answers are contained by predictions (EM), the one assesses whether the predictions are strictly the same as the answers (EMS); F1 is used to evaluate models in the perspective of term-matching; Rouge-L and GPT-4 evaluation (GE) are used to assess the performance of long-form answers, <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">i.e.</span>, conversational and multi-doc datasets.
For the GE metric, we prompt GPT to score whether the prediction is consistent with the answer from the three perspectives: factual consistency, redundancy, and deficiency. The predicted score should range from 0 to 5 and we normalized it to <math alttext="[0,1]" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.2"><semantics id="S4.SS2.p1.1.m1.2a"><mrow id="S4.SS2.p1.1.m1.2.3.2" xref="S4.SS2.p1.1.m1.2.3.1.cmml"><mo id="S4.SS2.p1.1.m1.2.3.2.1" stretchy="false" xref="S4.SS2.p1.1.m1.2.3.1.cmml">[</mo><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">0</mn><mo id="S4.SS2.p1.1.m1.2.3.2.2" xref="S4.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml">1</mn><mo id="S4.SS2.p1.1.m1.2.3.2.3" stretchy="false" xref="S4.SS2.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.2b"><interval closure="closed" id="S4.SS2.p1.1.m1.2.3.1.cmml" xref="S4.SS2.p1.1.m1.2.3.2"><cn id="S4.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1">0</cn><cn id="S4.SS2.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.2d">[ 0 , 1 ]</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Overall results on the extractive, conversational, time-sensitive, and multi-doc datasets. The overall best result is indicated in bold, and the best result under each setting is identified with <sup class="ltx_sup" id="S4.T2.79.1">∗</sup>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.77">
<tr class="ltx_tr" id="S4.T2.77.76">
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.77.76.1" rowspan="2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S4.T2.77.76.1.1">Settings</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T2.77.76.2" rowspan="2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S4.T2.77.76.2.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T2.77.76.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Extractive</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.77.76.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Conversational</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T2.77.76.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Time-sensitive</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.77.76.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Multi-doc</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.77">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">EM</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">EMS</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">F1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Rouge-L</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Rouge-L</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">GE</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">EM</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">EMS</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">F1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Rouge-L</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.77.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Rouge-L</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.77.77.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">GE</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.78">
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.77.78.1" rowspan="7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S4.T2.77.78.1.1"><span class="ltx_text" id="S4.T2.77.78.1.1.1"></span> <span class="ltx_text" id="S4.T2.77.78.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.77.78.1.1.2.1">
<span class="ltx_tr" id="S4.T2.77.78.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.78.1.1.2.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Close</span></span>
<span class="ltx_tr" id="S4.T2.77.78.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.78.1.1.2.1.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Book</span></span>
</span></span> <span class="ltx_text" id="S4.T2.77.78.1.1.3"></span></span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.77.78.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-7B-chat</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1269</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1952</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0863</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1444</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1429</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1272</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1454</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0706</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.78.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2370</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.77.78.14" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2750</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.79">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.79.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-13B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1307</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2171</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1018</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1273</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1878</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1959</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1375</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0411</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.79.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2341</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.79.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2624</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.80">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.80.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-70B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1520</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2263</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1096</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1479</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2122</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1118</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1141</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0426</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.80.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2536</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.80.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2542</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.3.1.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">GPT-3.5-turbo-1106</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1929</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0111</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3759</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2102</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2429<sup class="ltx_sup" id="S4.T2.3.1.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2245</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0631</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0154</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2544</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1177</td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.1.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2802</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.1.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3292</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.81">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.81.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-7B</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1548</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0556</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3531</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1911</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2108</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2041</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1118</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0164</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1620</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0925</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.81.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2397</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.81.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2584</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.82">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.82.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">ChatGLM2-6B-32K</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1471</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1843</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0781</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1592</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2082</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1426</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0154</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1580</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0880</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.82.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2258</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.82.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3208</td>
</tr>
<tr class="ltx_tr" id="S4.T2.14.12">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.14.12.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-33B-32k</td>
<td class="ltx_td ltx_align_left" id="S4.T2.4.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2443<sup class="ltx_sup" id="S4.T2.4.2.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.5.3.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1333<sup class="ltx_sup" id="S4.T2.5.3.2.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.4.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4320<sup class="ltx_sup" id="S4.T2.6.4.3.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.7.5.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2828<sup class="ltx_sup" id="S4.T2.7.5.4.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.14.12.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1906</td>
<td class="ltx_td ltx_align_left" id="S4.T2.8.6.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3143<sup class="ltx_sup" id="S4.T2.8.6.5.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.9.7.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2154<sup class="ltx_sup" id="S4.T2.9.7.6.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.10.8.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0769<sup class="ltx_sup" id="S4.T2.10.8.7.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.11.9.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2794<sup class="ltx_sup" id="S4.T2.11.9.8.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.12.10.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1722<sup class="ltx_sup" id="S4.T2.12.10.9.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.13.11.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2843<sup class="ltx_sup" id="S4.T2.13.11.10.1">∗</sup>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.14.12.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3334<sup class="ltx_sup" id="S4.T2.14.12.11.1">∗</sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.83">
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.77.83.1" rowspan="7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S4.T2.77.83.1.1"><span class="ltx_text" id="S4.T2.77.83.1.1.1"></span> <span class="ltx_text" id="S4.T2.77.83.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.77.83.1.1.2.1">
<span class="ltx_tr" id="S4.T2.77.83.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.83.1.1.2.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Golden</span></span>
<span class="ltx_tr" id="S4.T2.77.83.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.83.1.1.2.1.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Reference</span></span>
</span></span> <span class="ltx_text" id="S4.T2.77.83.1.1.3"></span></span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.77.83.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-7B-chat</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7986</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0111</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3948</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3503</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4460</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5388</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7405</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0154</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4166</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3701</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.83.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2626</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.77.83.14" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3750</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.84">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.84.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-13B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8106</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5322</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4745</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5004</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6858</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7867</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5161</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4497</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.84.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2950</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.84.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4666</td>
</tr>
<tr class="ltx_tr" id="S4.T2.15.13">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.15.13.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-70B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8880</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2556</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6612</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6219</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5762</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7918</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8846</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4923</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7364</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7056</td>
<td class="ltx_td ltx_align_left" id="S4.T2.15.13.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3179</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.15.13.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4958<sup class="ltx_sup" id="S4.T2.15.13.1.1">∗</sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.16.14">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.16.14.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">GPT-3.5-turbo-1106</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.16.14.1.1">0.9233<sup class="ltx_sup" id="S4.T2.16.14.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.16.14.1.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3667</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8213</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8065</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5963</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7633</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8785</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6308</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8811</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8711</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.14.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3901</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.16.14.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4917</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.85">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.85.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-7B</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7794</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4583</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7718</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7044</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5231</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7061</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7923</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5846</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7923</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7450</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.85.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3392</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.85.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4375</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.86">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.86.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">ChatGLM2-6B-32K</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8503</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0556</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4845</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4528</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5797</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7674</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7123</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4175</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3915</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.86.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3357</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.86.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4124</td>
</tr>
<tr class="ltx_tr" id="S4.T2.27.25">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.27.25.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-33B-32k</td>
<td class="ltx_td ltx_align_left" id="S4.T2.27.25.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8667</td>
<td class="ltx_td ltx_align_left" id="S4.T2.17.15.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.17.15.1.1">0.5778<sup class="ltx_sup" id="S4.T2.17.15.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.17.15.1.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.18.16.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.18.16.2.1">0.8885<sup class="ltx_sup" id="S4.T2.18.16.2.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.18.16.2.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.19.17.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.19.17.3.1">0.8674<sup class="ltx_sup" id="S4.T2.19.17.3.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.19.17.3.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.20.18.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.20.18.4.1">0.6632<sup class="ltx_sup" id="S4.T2.20.18.4.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.20.18.4.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.21.19.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.21.19.5.1">0.8326<sup class="ltx_sup" id="S4.T2.21.19.5.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.21.19.5.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.22.20.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.22.20.6.1">0.9154<sup class="ltx_sup" id="S4.T2.22.20.6.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.22.20.6.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.23.21.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.23.21.7.1">0.7846<sup class="ltx_sup" id="S4.T2.23.21.7.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.23.21.7.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.24.22.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0<span class="ltx_text ltx_font_bold" id="S4.T2.24.22.8.1">.9503<sup class="ltx_sup" id="S4.T2.24.22.8.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.24.22.8.1.1.1">∗</span></sup></span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.25.23.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.25.23.9.1">0.9459<sup class="ltx_sup" id="S4.T2.25.23.9.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.25.23.9.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.26.24.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.26.24.10.1">0.3936<sup class="ltx_sup" id="S4.T2.26.24.10.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.26.24.10.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.27.25.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.27.25.11.1">0.4958<sup class="ltx_sup" id="S4.T2.27.25.11.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.27.25.11.1.1.1">∗</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.87">
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.77.87.1" rowspan="7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S4.T2.77.87.1.1"><span class="ltx_text" id="S4.T2.77.87.1.1.1"></span> <span class="ltx_text" id="S4.T2.77.87.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.77.87.1.1.2.1">
<span class="ltx_tr" id="S4.T2.77.87.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.87.1.1.2.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">BM25</span></span>
<span class="ltx_tr" id="S4.T2.77.87.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.87.1.1.2.1.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">TOP1</span></span>
</span></span> <span class="ltx_text" id="S4.T2.77.87.1.1.3"></span></span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.77.87.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-7B-chat</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6638</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0111</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3647</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2942</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2764</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1429</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4246</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2618</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2156</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.87.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2371</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.77.87.14" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3042</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.88">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.88.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-13B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6988</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4621</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3847</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3125</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3674</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4087</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0308</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2911</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2488</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.88.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2587</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.88.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2708</td>
</tr>
<tr class="ltx_tr" id="S4.T2.29.27">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.29.27.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-70B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.29.27.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7184</td>
<td class="ltx_td ltx_align_left" id="S4.T2.29.27.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2111</td>
<td class="ltx_td ltx_align_left" id="S4.T2.29.27.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5778</td>
<td class="ltx_td ltx_align_left" id="S4.T2.29.27.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5029</td>
<td class="ltx_td ltx_align_left" id="S4.T2.29.27.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3467</td>
<td class="ltx_td ltx_align_left" id="S4.T2.28.26.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4654<sup class="ltx_sup" id="S4.T2.28.26.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.29.27.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4497</td>
<td class="ltx_td ltx_align_left" id="S4.T2.29.27.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.29.27.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4422</td>
<td class="ltx_td ltx_align_left" id="S4.T2.29.27.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3721</td>
<td class="ltx_td ltx_align_left" id="S4.T2.29.27.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3039</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.29.27.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3458<sup class="ltx_sup" id="S4.T2.29.27.2.1">∗</sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.32.30">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.32.30.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">GPT-3.5-turbo-1106</td>
<td class="ltx_td ltx_align_left" id="S4.T2.30.28.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7749<sup class="ltx_sup" id="S4.T2.30.28.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.32.30.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3222</td>
<td class="ltx_td ltx_align_left" id="S4.T2.32.30.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7222</td>
<td class="ltx_td ltx_align_left" id="S4.T2.32.30.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6588</td>
<td class="ltx_td ltx_align_left" id="S4.T2.32.30.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3798</td>
<td class="ltx_td ltx_align_left" id="S4.T2.32.30.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7552</td>
<td class="ltx_td ltx_align_left" id="S4.T2.31.29.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4800<sup class="ltx_sup" id="S4.T2.31.29.2.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.32.30.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3385</td>
<td class="ltx_td ltx_align_left" id="S4.T2.32.30.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5273<sup class="ltx_sup" id="S4.T2.32.30.3.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.32.30.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4812</td>
<td class="ltx_td ltx_align_left" id="S4.T2.32.30.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2647</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.32.30.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2958</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.89">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.89.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-7B</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6562</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4028</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6916</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5980</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3271</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4215</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3472</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5111</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4456</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.89.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2819</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.89.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2500</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.90">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.90.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">ChatGLM2-6B-32K</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7029</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0111</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3954</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3384</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3261</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4612</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4369</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0615</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2498</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2057</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.90.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2683</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.90.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3208</td>
</tr>
<tr class="ltx_tr" id="S4.T2.39.37">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.39.37.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-33B-32k</td>
<td class="ltx_td ltx_align_left" id="S4.T2.39.37.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7267</td>
<td class="ltx_td ltx_align_left" id="S4.T2.33.31.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5111<sup class="ltx_sup" id="S4.T2.33.31.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.34.32.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7717<sup class="ltx_sup" id="S4.T2.34.32.2.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.35.33.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7045<sup class="ltx_sup" id="S4.T2.35.33.3.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.36.34.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4092<sup class="ltx_sup" id="S4.T2.36.34.4.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.39.37.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.39.37.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4579</td>
<td class="ltx_td ltx_align_left" id="S4.T2.37.35.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4236<sup class="ltx_sup" id="S4.T2.37.35.5.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.39.37.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5272</td>
<td class="ltx_td ltx_align_left" id="S4.T2.38.36.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5140<sup class="ltx_sup" id="S4.T2.38.36.6.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.39.37.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3074<sup class="ltx_sup" id="S4.T2.39.37.7.1">∗</sup>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.39.37.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2958</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.91">
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.77.91.1" rowspan="7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S4.T2.77.91.1.1"><span class="ltx_text" id="S4.T2.77.91.1.1.1"></span> <span class="ltx_text" id="S4.T2.77.91.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.77.91.1.1.2.1">
<span class="ltx_tr" id="S4.T2.77.91.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.91.1.1.2.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">BM25</span></span>
<span class="ltx_tr" id="S4.T2.77.91.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.91.1.1.2.1.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">TOP3</span></span>
</span></span> <span class="ltx_text" id="S4.T2.77.91.1.1.3"></span></span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.77.91.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-7B-chat</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7188</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2881</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2373</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2419</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2734</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5390</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1794</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1463</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.91.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2219</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.77.91.14" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3376</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.92">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.92.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-13B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7548</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3203</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2616</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2652</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4734</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5795</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2146</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1769</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.92.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2454</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.92.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3584</td>
</tr>
<tr class="ltx_tr" id="S4.T2.40.38">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.40.38.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-70B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7298</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0111</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4038</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3409</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3074</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4490</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5672</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0462</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3450</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2853</td>
<td class="ltx_td ltx_align_left" id="S4.T2.40.38.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2651</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.40.38.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3792<sup class="ltx_sup" id="S4.T2.40.38.1.1">∗</sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.42.40">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.42.40.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">GPT-3.5-turbo-1106</td>
<td class="ltx_td ltx_align_left" id="S4.T2.41.39.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7835<sup class="ltx_sup" id="S4.T2.41.39.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.42.40.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3222</td>
<td class="ltx_td ltx_align_left" id="S4.T2.42.40.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7463</td>
<td class="ltx_td ltx_align_left" id="S4.T2.42.40.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6951</td>
<td class="ltx_td ltx_align_left" id="S4.T2.42.40.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4200</td>
<td class="ltx_td ltx_align_left" id="S4.T2.42.40.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5306</td>
<td class="ltx_td ltx_align_left" id="S4.T2.42.40.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6400</td>
<td class="ltx_td ltx_align_left" id="S4.T2.42.40.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5077</td>
<td class="ltx_td ltx_align_left" id="S4.T2.42.40.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6805</td>
<td class="ltx_td ltx_align_left" id="S4.T2.42.40.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6397</td>
<td class="ltx_td ltx_align_left" id="S4.T2.42.40.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3419<sup class="ltx_sup" id="S4.T2.42.40.2.1">∗</sup>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.42.40.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3542</td>
</tr>
<tr class="ltx_tr" id="S4.T2.43.41">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.43.41.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-7B</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7145</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4333</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6814</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6106</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3281</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5756<sup class="ltx_sup" id="S4.T2.43.41.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5067</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3646</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5412</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4824</td>
<td class="ltx_td ltx_align_left" id="S4.T2.43.41.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2918</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.43.41.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3876</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.93">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.93.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">ChatGLM2-6B-32K</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7245</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0333</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4868</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4352</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3758</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4530</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4959</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1231</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3575</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3110</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.93.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2806</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.93.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3708</td>
</tr>
<tr class="ltx_tr" id="S4.T2.52.50">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.52.50.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-33B-32k</td>
<td class="ltx_td ltx_align_left" id="S4.T2.52.50.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7767</td>
<td class="ltx_td ltx_align_left" id="S4.T2.44.42.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5593<sup class="ltx_sup" id="S4.T2.44.42.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.45.43.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.8014<sup class="ltx_sup" id="S4.T2.45.43.2.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.46.44.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7568<sup class="ltx_sup" id="S4.T2.46.44.3.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.47.45.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4352<sup class="ltx_sup" id="S4.T2.47.45.4.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.48.46.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5756<sup class="ltx_sup" id="S4.T2.48.46.5.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.49.47.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6841<sup class="ltx_sup" id="S4.T2.49.47.6.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.50.48.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6369<sup class="ltx_sup" id="S4.T2.50.48.7.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.51.49.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7284<sup class="ltx_sup" id="S4.T2.51.49.8.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.52.50.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7226<sup class="ltx_sup" id="S4.T2.52.50.9.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.52.50.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3406</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.52.50.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3584</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.94">
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.77.94.1" rowspan="7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S4.T2.77.94.1.1"><span class="ltx_text" id="S4.T2.77.94.1.1.1"></span> <span class="ltx_text" id="S4.T2.77.94.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.77.94.1.1.2.1">
<span class="ltx_tr" id="S4.T2.77.94.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.94.1.1.2.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Dense</span></span>
<span class="ltx_tr" id="S4.T2.77.94.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.94.1.1.2.1.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">TOP1</span></span>
</span></span> <span class="ltx_text" id="S4.T2.77.94.1.1.3"></span></span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.77.94.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-7B-chat</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5048</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3160</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2210</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2182</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2898</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3564</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2093</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1383</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.94.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2489</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.77.94.14" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3458</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.95">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.95.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-13B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5651</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0111</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4210</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3088</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2860</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3020</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3867</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3062</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2351</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.95.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2728</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.95.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3584</td>
</tr>
<tr class="ltx_tr" id="S4.T2.55.53">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.55.53.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-70B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.53.51.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5807<sup class="ltx_sup" id="S4.T2.53.51.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.55.53.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1444</td>
<td class="ltx_td ltx_align_left" id="S4.T2.55.53.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5086</td>
<td class="ltx_td ltx_align_left" id="S4.T2.55.53.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3967</td>
<td class="ltx_td ltx_align_left" id="S4.T2.55.53.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3010</td>
<td class="ltx_td ltx_align_left" id="S4.T2.55.53.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3674</td>
<td class="ltx_td ltx_align_left" id="S4.T2.54.52.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4026<sup class="ltx_sup" id="S4.T2.54.52.2.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.55.53.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1846</td>
<td class="ltx_td ltx_align_left" id="S4.T2.55.53.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3706</td>
<td class="ltx_td ltx_align_left" id="S4.T2.55.53.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2978</td>
<td class="ltx_td ltx_align_left" id="S4.T2.55.53.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2896</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.55.53.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3792<sup class="ltx_sup" id="S4.T2.55.53.3.1">∗</sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.59.57">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.59.57.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">GPT-3.5-turbo-1106</td>
<td class="ltx_td ltx_align_left" id="S4.T2.59.57.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5467</td>
<td class="ltx_td ltx_align_left" id="S4.T2.59.57.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2235</td>
<td class="ltx_td ltx_align_left" id="S4.T2.59.57.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5915</td>
<td class="ltx_td ltx_align_left" id="S4.T2.59.57.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4831</td>
<td class="ltx_td ltx_align_left" id="S4.T2.59.57.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3180</td>
<td class="ltx_td ltx_align_left" id="S4.T2.59.57.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3632</td>
<td class="ltx_td ltx_align_left" id="S4.T2.59.57.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3979</td>
<td class="ltx_td ltx_align_left" id="S4.T2.59.57.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2769</td>
<td class="ltx_td ltx_align_left" id="S4.T2.56.54.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4874<sup class="ltx_sup" id="S4.T2.56.54.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.57.55.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4031<sup class="ltx_sup" id="S4.T2.57.55.2.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.58.56.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3327<sup class="ltx_sup" id="S4.T2.58.56.3.1">∗</sup>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.59.57.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3792<sup class="ltx_sup" id="S4.T2.59.57.4.1">∗</sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.96">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.96.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-7B</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4499</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2444</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5220</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4129</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2771</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3633</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2733</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2579</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3539</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3129</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.96.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2790</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.96.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3708</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.97">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.97.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">ChatGLM2-6B-32K</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5204</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0222</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3310</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2406</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2666</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3062</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3410</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0154</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1857</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1242</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.97.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2666</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.97.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3624</td>
</tr>
<tr class="ltx_tr" id="S4.T2.65.63">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.65.63.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-33B-32k</td>
<td class="ltx_td ltx_align_left" id="S4.T2.65.63.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5573</td>
<td class="ltx_td ltx_align_left" id="S4.T2.60.58.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3333<sup class="ltx_sup" id="S4.T2.60.58.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.61.59.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6467<sup class="ltx_sup" id="S4.T2.61.59.2.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.62.60.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5192<sup class="ltx_sup" id="S4.T2.62.60.3.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.63.61.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3772<sup class="ltx_sup" id="S4.T2.63.61.4.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.64.62.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4858<sup class="ltx_sup" id="S4.T2.64.62.5.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.65.63.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3831</td>
<td class="ltx_td ltx_align_left" id="S4.T2.65.63.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3215<sup class="ltx_sup" id="S4.T2.65.63.6.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.65.63.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4631</td>
<td class="ltx_td ltx_align_left" id="S4.T2.65.63.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3943</td>
<td class="ltx_td ltx_align_left" id="S4.T2.65.63.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2970</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.65.63.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3208</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.98">
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.77.98.1" rowspan="7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S4.T2.77.98.1.1"><span class="ltx_text" id="S4.T2.77.98.1.1.1"></span> <span class="ltx_text" id="S4.T2.77.98.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.77.98.1.1.2.1">
<span class="ltx_tr" id="S4.T2.77.98.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.98.1.1.2.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Dense</span></span>
<span class="ltx_tr" id="S4.T2.77.98.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.77.98.1.1.2.1.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">TOP3</span></span>
</span></span> <span class="ltx_text" id="S4.T2.77.98.1.1.3"></span></span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.77.98.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-7B-chat</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5630</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2601</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2022</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2357</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3388</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4328</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1436</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1112</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.77.98.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2221</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.77.98.14" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3458</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.99">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.99.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-13B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6111</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2763</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2178</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2678</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4286</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4077</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1462</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1197</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.99.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2621</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.99.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4458</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.100">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.100.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Llama2-70B-chat</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6921</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0111</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3920</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3265</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2775</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4490</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4359</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3480</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2749</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.100.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2549</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.100.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4250</td>
</tr>
<tr class="ltx_tr" id="S4.T2.67.65">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.67.65.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">GPT-3.5-turbo-1106</td>
<td class="ltx_td ltx_align_left" id="S4.T2.66.64.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6930<sup class="ltx_sup" id="S4.T2.66.64.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.67.65.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3111</td>
<td class="ltx_td ltx_align_left" id="S4.T2.67.65.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7096</td>
<td class="ltx_td ltx_align_left" id="S4.T2.67.65.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6364</td>
<td class="ltx_td ltx_align_left" id="S4.T2.67.65.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4028</td>
<td class="ltx_td ltx_align_left" id="S4.T2.67.65.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4898</td>
<td class="ltx_td ltx_align_left" id="S4.T2.67.65.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4021</td>
<td class="ltx_td ltx_align_left" id="S4.T2.67.65.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2769</td>
<td class="ltx_td ltx_align_left" id="S4.T2.67.65.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4814</td>
<td class="ltx_td ltx_align_left" id="S4.T2.67.65.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4112</td>
<td class="ltx_td ltx_align_left" id="S4.T2.67.65.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3580<sup class="ltx_sup" id="S4.T2.67.65.2.1">∗</sup>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.67.65.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4334</td>
</tr>
<tr class="ltx_tr" id="S4.T2.68.66">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.68.66.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-7B</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5917</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3000</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5805</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4938</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3327</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4612</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3636</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2359</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4255</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3635</td>
<td class="ltx_td ltx_align_left" id="S4.T2.68.66.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2916</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.68.66.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4876<sup class="ltx_sup" id="S4.T2.68.66.1.1">∗</sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.101">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.77.101.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">ChatGLM2-6B-32K</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6635</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.0333</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4530</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3922</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3390</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4694</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4308</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.1385</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3295</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2868</td>
<td class="ltx_td ltx_align_left" id="S4.T2.77.101.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.2837</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.77.101.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3500</td>
</tr>
<tr class="ltx_tr" id="S4.T2.77.75">
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.77.75.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baichuan2-33B-32k</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.77.75.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6769</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.69.67.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4222<sup class="ltx_sup" id="S4.T2.69.67.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.70.68.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.7299<sup class="ltx_sup" id="S4.T2.70.68.2.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.71.69.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.6533<sup class="ltx_sup" id="S4.T2.71.69.3.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.72.70.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4696<sup class="ltx_sup" id="S4.T2.72.70.4.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.73.71.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5755<sup class="ltx_sup" id="S4.T2.73.71.5.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.74.72.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4841<sup class="ltx_sup" id="S4.T2.74.72.6.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.75.73.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3692<sup class="ltx_sup" id="S4.T2.75.73.7.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.76.74.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.5633<sup class="ltx_sup" id="S4.T2.76.74.8.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.77.75.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4839<sup class="ltx_sup" id="S4.T2.77.75.9.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.77.75.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.3384</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T2.77.75.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">0.4000</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Overall Experimental Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The overall experimental results of the first three settings are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4.T2" title="Table 2 ‣ 4.2 Evaluation Metrics ‣ 4 Experiment ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>, and we analyze the following conclusions.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">(1) In domain-specific scenarios, the knowledge contained within LLMs themselves may hard to tackle the user’s expert problems.
The experimental results in the "Close Book" block confirm the poor performance of LLMs when faced with in-domain questions that go beyond their internal knowledge
Additionally,
the retrieval settings in the last four blocks demonstrate that external expert knowledge can provide more reliable information for LLMs in expert scenarios.
Even when equipped with a built-in retrieval system like Baichuan2-33B-32k, the close-book results are significantly inferior to those obtained from retrieval settings. This finding reinforces the importance of domain-specific corpora over general knowledge sources.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">(2) The BM25 retriever shows better generalization than the dense retriever. Interestingly, when using BM25, the results are generally better than the dense retriever.
One possible explanation is that our application scenarios are long-tail, meaning they contain specialized knowledge that may not have been adequately covered during the pre-training of the dense retriever.
However, BM25 is a spare retrieve with strong generalization ability. Therefore, the BM25 is also a good choice for RAG models in domain-specific application scenarios, especially if applications are more cost-sensitive.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">(3) Dealing with long-form QA problems remains challenging for RAG models and warrants further investigation in the future. We notice that for conversational and multi-doc datasets, the improvement of retrieval-augmented results is not as significant as in other datasets. Especially for the multi-doc dataset, its results of providing golden references are still limited. These phenomena imply that accurately understanding the user’s current intents in conversational scenarios requires RAG models to possess strong abilities in analyzing complex relationships among historical information, the current query, and external references. Moreover, analyzing the interactions between multiple external documents is also a critical ability for RAG models and needs to be investigated further.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="537" id="S4.F2.g1" src="x2.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The experiments on the structural QA dataset.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="492" id="S4.F3.g1" src="x3.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The experiments on the faithful QA dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Experiments on Structural Dataset</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To evaluate the effectiveness of structural information for RAG models and analyze their abilities to comprehend knowledge in HTML format, we conducted the corresponding experiments on our structural QA dataset. It is worth noting that the whole HTML content of a web page is redundant and may contain some useless information about the web layout. Therefore, we proactively filtered out the information irrelevant to the valuable content of web pages. Nevertheless, the processed contents still exceed the maximum length of some LLMs, <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">e.g.</span>, Llama. For simplicity, we directly truncated the provided information for LLMs that cannot handle lengthy texts. We expect that there are more elaborate techniques to tackle this problem. We provided two versions of web page content: one is pure text and the other one is HTML content for comparing the performance of RAG models on the different formats of external knowledge. The experimental results are presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4.F2" title="Figure 2 ‣ 4.3 Overall Experimental Results ‣ 4 Experiment ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Obviously, for Llama models and Baichuan2-7B, the predicted results from pure texts are more accurate than ones from HTML content. However, for Baichuan2-33B-32k, ChatGLM2-6B-32k, and GPT-3.5-turbo-1106, providing HTML content leads to better performance than providing pure texts. According to this phenomenon, we can draw the following conclusions:
(1) Structural data contains valuable information beyond pure texts. Strong LLMs, such as GPT-3.5-turbo-1106, demonstrate better results when using HTML-format external knowledge, suggesting that the structural information of web pages complements textual content and helps LLMs understand web content and address user queries effectively.
(2) Baichuan2-33B-32k, ChatGLM2-6B-32k, and GPT-3.5-turbo-1106 have stronger abilities in understanding and analyzing HTML contents than Llama-family models and Baichuan2-7B. The reason may be that these models have been pre-trained on data in the HTML format, which enables them to better comprehend the corresponding information. In the future, with more diverse formats of external knowledge, such ability is more and more important for LLMs to provide better experiences for users.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Robustness of LLMs on Noisy References</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">To assess the robustness of LLMs on noised references, we mixed the positive references with different amounts of noisy references, including 4, 9, 14, 19, and 24.
Additionally, the position of the positive reference was varied, <span class="ltx_text ltx_font_italic" id="S4.SS5.p1.1.1">i.e.</span>the first, the middle, and the last positions, to assess the impact of the reference order on RAG models.
The experiments were performed on LLMs capable of processing long texts, <span class="ltx_text ltx_font_italic" id="S4.SS5.p1.1.2">i.e.</span>Baichuan2-33B and GPT-3.5-turbo-1106, considering the potential issue of overlength when dealing with extensive external knowledge.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.3">The results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4.F4" title="Figure 4 ‣ 4.6 Faithfulness of LLMs in External References ‣ 4 Experiment ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a> indicate that both different positions of golden references and amounts of noise have a significant influence on the performance of RAG models. There are some interesting findings: (1) Lost in the middle is a common phenomenon. Placing positive references in the middle position of noisy references often leads to a significant decline in model performance. This phenomenon has also been indicated in recent studies <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#bib.bib19" title="">2024b</a>)</cite>, highlighting the importance of not only the quality of the provided knowledge but also its order. (2) More noise often leads to worse results. It is evident that the lines with high <math alttext="NC" class="ltx_Math" display="inline" id="S4.SS5.p2.1.m1.1"><semantics id="S4.SS5.p2.1.m1.1a"><mrow id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml"><mi id="S4.SS5.p2.1.m1.1.1.2" xref="S4.SS5.p2.1.m1.1.1.2.cmml">N</mi><mo id="S4.SS5.p2.1.m1.1.1.1" xref="S4.SS5.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS5.p2.1.m1.1.1.3" xref="S4.SS5.p2.1.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><apply id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1"><times id="S4.SS5.p2.1.m1.1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1.1"></times><ci id="S4.SS5.p2.1.m1.1.1.2.cmml" xref="S4.SS5.p2.1.m1.1.1.2">𝑁</ci><ci id="S4.SS5.p2.1.m1.1.1.3.cmml" xref="S4.SS5.p2.1.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">NC</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.1.m1.1d">italic_N italic_C</annotation></semantics></math>-values are generally below the line with low <math alttext="NC" class="ltx_Math" display="inline" id="S4.SS5.p2.2.m2.1"><semantics id="S4.SS5.p2.2.m2.1a"><mrow id="S4.SS5.p2.2.m2.1.1" xref="S4.SS5.p2.2.m2.1.1.cmml"><mi id="S4.SS5.p2.2.m2.1.1.2" xref="S4.SS5.p2.2.m2.1.1.2.cmml">N</mi><mo id="S4.SS5.p2.2.m2.1.1.1" xref="S4.SS5.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS5.p2.2.m2.1.1.3" xref="S4.SS5.p2.2.m2.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><apply id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1"><times id="S4.SS5.p2.2.m2.1.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1.1"></times><ci id="S4.SS5.p2.2.m2.1.1.2.cmml" xref="S4.SS5.p2.2.m2.1.1.2">𝑁</ci><ci id="S4.SS5.p2.2.m2.1.1.3.cmml" xref="S4.SS5.p2.2.m2.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">NC</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.2.m2.1d">italic_N italic_C</annotation></semantics></math>-values (“NC” is short for “Noise Count”). It is reasonable since excessive irrelevant knowledge may disturb the LLMs’ cognition, thus negatively affecting the generated results. Therefore, a high-quality IR model is also critical for RAG tasks. (3) Noise is not always bad. The results of “No Noise” are not always the best compared to those obtained from noisy references. The reason may be that
compared to the “No Noise” setting where only one document is provided, the noisy references contain <math alttext="NC+1" class="ltx_Math" display="inline" id="S4.SS5.p2.3.m3.1"><semantics id="S4.SS5.p2.3.m3.1a"><mrow id="S4.SS5.p2.3.m3.1.1" xref="S4.SS5.p2.3.m3.1.1.cmml"><mrow id="S4.SS5.p2.3.m3.1.1.2" xref="S4.SS5.p2.3.m3.1.1.2.cmml"><mi id="S4.SS5.p2.3.m3.1.1.2.2" xref="S4.SS5.p2.3.m3.1.1.2.2.cmml">N</mi><mo id="S4.SS5.p2.3.m3.1.1.2.1" xref="S4.SS5.p2.3.m3.1.1.2.1.cmml">⁢</mo><mi id="S4.SS5.p2.3.m3.1.1.2.3" xref="S4.SS5.p2.3.m3.1.1.2.3.cmml">C</mi></mrow><mo id="S4.SS5.p2.3.m3.1.1.1" xref="S4.SS5.p2.3.m3.1.1.1.cmml">+</mo><mn id="S4.SS5.p2.3.m3.1.1.3" xref="S4.SS5.p2.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.3.m3.1b"><apply id="S4.SS5.p2.3.m3.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1"><plus id="S4.SS5.p2.3.m3.1.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1.1"></plus><apply id="S4.SS5.p2.3.m3.1.1.2.cmml" xref="S4.SS5.p2.3.m3.1.1.2"><times id="S4.SS5.p2.3.m3.1.1.2.1.cmml" xref="S4.SS5.p2.3.m3.1.1.2.1"></times><ci id="S4.SS5.p2.3.m3.1.1.2.2.cmml" xref="S4.SS5.p2.3.m3.1.1.2.2">𝑁</ci><ci id="S4.SS5.p2.3.m3.1.1.2.3.cmml" xref="S4.SS5.p2.3.m3.1.1.2.3">𝐶</ci></apply><cn id="S4.SS5.p2.3.m3.1.1.3.cmml" type="integer" xref="S4.SS5.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.3.m3.1c">NC+1</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.3.m3.1d">italic_N italic_C + 1</annotation></semantics></math> external documents.
The increased amount of provided knowledge may emphasize the confidence of LLMs in external knowledge, making them more inclined to rely on it when solving problems. To verify this assumption, we conducted an experiment, where the golden references were repeated to match the number of noisy references. This experiment partially supports this assumption as the repeated references outperformed all other settings in most situations. This observation provides some insights for future studies of RAG that repeating provided references may be beneficial for motivating LLMs to provide better results.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Faithfulness of LLMs in External References</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">To assess the faithfulness of LLMs in external knowledge in out-of-domain applications, we provided the anti-references for LLMs to test whether they could generate anti-answers for these expert questions according to the external information. We compare the results with two different settings, in one we provided golden references and tested the performance of generating golden answers, other one is the close book setting. The comparison results are demonstrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.05654v2#S4.F3" title="Figure 3 ‣ 4.3 Overall Experimental Results ‣ 4 Experiment ‣ DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="661" id="S4.F4.g1" src="x4.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Experiments in different noise ratio settings.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.1">We found that in the close book setting, LLMs significantly underperform the settings with external knowledge, further confirming the importance of external knowledge for this scenario. Additionally, whether or not external knowledge is provided, LLMs often tend to generate golden answers instead of anti-answers. This suggests that (1) LLMs still contain a certain of prior knowledge about the in-domain information. (2) There is some room for LLMs to improve their faithfulness in the right external knowledge.
It is reasonable that LLMs could answer some questions according to their own knowledge, hence may impact their confidence in external information. Nevertheless, their own knowledge is static and may be out-of-date while knowledge in some in-domain scenarios, such as enrollment plans, will quickly change over time. These situations put forward a strong requirement for LLMs to distinguish and trust external knowledge correctly.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We built a comprehensive dataset, DomainRAG, to assess some crucial abilities of RAG models in a domain-specific scenario, college enrollment. We crawled the corresponding webpages from the website and two types of corpora, HTML corpus and pure text corpus were built. Then, we created corresponding sub-datasets to assess the following abilities, <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">i.e.</span>conversational RAG, structural information analysis, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding of multi-document interactions. Our experiments confirm the role of RAG models in domain-specific scenarios where LLMs cannot solve expert questions well. Furthermore,
RAG models still have room for improvement in comprehending users’ conversational history, analyzing structural knowledge, denoising references, managing multi-document interactions, and preserving fidelity to expert knowledge. We expect future research to make advancements in addressing these challenges more effectively.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we identified six critical capabilities of RAG models and developed a comprehensive dataset, namely DomainRAG, to evaluate these capabilities in a domain-specific application scenario. We acknowledge the following limitations of our current study that present opportunities for future investigations.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">First, though we chose several popular LLMs to assess their abilities in leveraging external knowledge to solve domain-specific questions, there exists some more sophisticated frameworks designed for enhancing the performance of RAG systems. Due to the complexity and diversity of implementation processes, we did not include them in our current research and evaluate their performances. Secondly, the application scenario is single. While we selected a in-domain and long-tail application scenario, its unicity may also introduce some biases to experimental results. In the future, it is valuable to explore more model structures and application scenarios to evaluate the capabilities of RAG systems more comprehensively and reliably.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arefeen et al. (2023)</span>
<span class="ltx_bibblock">
Md. Adnan Arefeen, Biplob Debnath, and Srimat Chakradhar. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.00841" title="">Leancontext: Cost-efficient domain-specific question answering using llms</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">CoRR</em>, abs/2309.00841.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.11511" title="">Self-rag: Learning to retrieve, generate, and critique through self-reflection</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">CoRR</em>, abs/2310.11511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Haonan Chen, Zhicheng Dou, Yutao Zhu, Zhao Cao, Xiaohua Cheng, and Ji-Rong Wen. 2022.

</span>
<span class="ltx_bibblock">Enhancing user behavior sequence modeling by generative tasks for session search.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CIKM</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/AAAI.V38I16.29728" title="">Benchmarking large language models in retrieval-augmented generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada</em>, pages 17754–17762. AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Shitong Dai, Jiongnan Liu, Zhicheng Dou, Haonan Wang, Lin Liu, Bo Long, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3580305.3599287" title="">Contrastive learning for user sequence representation in personalized product search</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, KDD ’23, page 380–389, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhingra et al. (2022)</span>
<span class="ltx_bibblock">
Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00459" title="">Time-aware language models as temporal knowledge bases</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Transactions of the Association for Computational Linguistics</em>, 10:257–273.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2312.10997" title="">Retrieval-augmented generation for large language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/2312.10997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2002.08909" title="">REALM: retrieval-augmented language model pre-training</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">CoRR</em>, abs/2002.08909.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.495" title="">Active retrieval augmented generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 7969–7992. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1147" title="">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00276" title="">Natural questions: A benchmark for question answering research</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Transactions of the Association for Computational Linguistics</em>, 7:452–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazaridou et al. (2022)</span>
<span class="ltx_bibblock">
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2203.05115" title="">Internet-augmented language models through few-shot prompting for open-domain question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em>, abs/2203.05115.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="">Retrieval-augmented generation for knowledge-intensive NLP tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, and Ge Yu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.16058" title="">Say more with less: Understanding prompt learning behaviors through gist compression</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Preprint</em>, arXiv:2402.16058.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.01352" title="">RA-DIT: retrieval-augmented dual instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, abs/2310.01352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024a)</span>
<span class="ltx_bibblock">
Jiongnan Liu, Zhicheng Dou, Jian-Yun Nie, and Ji-Rong Wen. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TKDE.2023.3291006" title="">Integrated personalized and diversified search based on search logs</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, 36(2):694–707.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Jiongnan Liu, Zhicheng Dou, Qiannan Zhu, and Ji-Rong Wen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3485447.3511964" title="">A category-aware multi-interest model for personalized product search</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the ACM Web Conference 2022</em>, WWW ’22, page 360–368, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2306.05212" title="">RETA-LLM: A retrieval-augmented large language model toolkit</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CoRR</em>, abs/2306.05212.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024b)</span>
<span class="ltx_bibblock">
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00638" title="">Lost in the Middle: How Language Models Use Long Contexts</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Transactions of the Association for Computational Linguistics</em>, 12:157–173.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al. (2021)</span>
<span class="ltx_bibblock">
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.200" title="">KILT: a benchmark for knowledge intensive language tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 2523–2544, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et al. (2023)</span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2302.00083" title="">In-context retrieval-augmented language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">CoRR</em>, abs/2302.00083.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2023)</span>
<span class="ltx_bibblock">
Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.11019" title="">Investigating the factual knowledge boundary of large language models with retrieval augmentation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">CoRR</em>, abs/2307.11019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza (2009)</span>
<span class="ltx_bibblock">
Stephen Robertson and Hugo Zaragoza. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1561/1500000019" title="">The probabilistic relevance framework: Bm25 and beyond</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Found. Trends Inf. Retr.</em>, 3(4).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al. (2023)</span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.ACL-LONG.557" title="">Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 10014–10037. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Shuting Wang, Zhicheng Dou, Jiongnan Liu, Qiannan Zhu, and Ji-Rong Wen. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3631989" title="">Personalized and diversified: Ranking search results in an integrated way</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">ACM Trans. Inf. Syst.</em>, 42(3).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Shuting Wang, Zhicheng Dou, Jing Yao, Yujia Zhou, and Ji-Rong Wen. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3543507.3583488" title="">Incorporating explicit subtopics in personalized search</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the ACM Web Conference 2023</em>, WWW ’23, page 3364–3374, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Shuting Wang, Zhicheng Dou, and Yutao Zhu. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3539597.3570390" title="">Heterogeneous graph-based context-aware document ranking</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining</em>, WSDM ’23, page 724–732, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2023)</span>
<span class="ltx_bibblock">
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.07597" title="">C-pack: Packaged resources to advance general chinese embedding</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Preprint</em>, arXiv:2309.07597.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.04408" title="">RECOMP: improving retrieval-augmented lms with compression and selective augmentation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">CoRR</em>, abs/2310.04408.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1259" title="">HotpotQA: A dataset for diverse, explainable multi-hop question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2369–2380, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2020)</span>
<span class="ltx_bibblock">
Jing Yao, Zhicheng Dou, and Ji-Rong Wen. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3397271.3401153" title="">Employing personal word embeddings for personalized search</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, page 1359–1368, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.09210" title="">Chain-of-note: Enhancing robustness in retrieval-augmented language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">CoRR</em>, abs/2311.09210.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2403.10131" title="">Raft: Adapting language model to domain specific rag</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Preprint</em>, arXiv:2403.10131.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2020)</span>
<span class="ltx_bibblock">
Yujia Zhou, Zhicheng Dou, and Ji-Rong Wen. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3397271.3401175" title="">Encoding history with context-aware representation learning for personalized search</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, page 1111–1120, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2021)</span>
<span class="ltx_bibblock">
Yutao Zhu, Jian-Yun Nie, Zhicheng Dou, Zhengyi Ma, Xinyu Zhang, Pan Du, Xiaochen Zuo, and Hao Jiang. 2021.

</span>
<span class="ltx_bibblock">Contrastive learning of user behavior sequence for context-aware document ranking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">CIKM ’21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, QLD, Australia, November 1-5, 2021</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2308.07107" title="">Large language models for information retrieval: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">CoRR</em>, abs/2308.07107.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jun 17 02:31:28 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
