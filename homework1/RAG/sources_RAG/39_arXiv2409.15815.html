<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support</title>
<!--Generated on Tue Sep 24 07:21:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.15815v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S1" title="In AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S2" title="In AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Previous Literature</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S2.SS1" title="In 2 Previous Literature ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Large Language Models (LLMs)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S2.SS2" title="In 2 Previous Literature ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Biomedical LLMs and RAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3" title="In AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3.SS1" title="In 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Collections</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3.SS2" title="In 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Vector DB Building and Retrievers</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3.SS3" title="In 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Prompting and Backbone LLM</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3.SS4" title="In 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Translation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3.SS5" title="In 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Visual Interface</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3.SS6" title="In 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>AsthmaBot’s Inference Process</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S4" title="In 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S4.SS1" title="In 4 Results ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation Procedure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S4.SS2" title="In 4 Results ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S4.SS3" title="In 4 Results ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Evaluation Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S5" title="In 4.3 Evaluation Results ‣ 4 Results ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Impact and Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S6" title="In 5 Impact and Future Work ‣ 4.3 Evaluation Results ‣ 4 Results ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\cormark</span>
<p class="ltx_p" id="p1.2">[1]</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\credit</span>
<p class="ltx_p" id="p2.2">Conceptualization, Software, Formal analysis, Investigation, Methodology, Writing - original draft</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">1]organization=TICLab, College of Engineering &amp; Architecture, International University of Rabat,
<span class="ltx_ERROR undefined" id="p3.1.1">\city</span>=Rabat,
country=Morocco
</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">2]organization=Faculty of Engineering,University of Leeds,
country=United Kingdom
</p>
</div>
<div class="ltx_para" id="p5">
<span class="ltx_ERROR undefined" id="p5.1">\credit</span>
<p class="ltx_p" id="p5.2">Conceptualization, Formal analysis, Project administration, Supervision, Validation, Writing - review &amp; editing</p>
</div>
<h1 class="ltx_title ltx_title_document">
AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adil Bahaj
</span><span class="ltx_author_notes">adil.bahaj@uir.ac.ma
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mounir Ghogho
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">[
</span>mounir.ghogho@uir.ac.ma</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Asthma rates have risen globally, driven by environmental and lifestyle factors. Access to immediate medical care is limited, particularly in developing countries, necessitating automated support systems. Large Language Models like ChatGPT (Chat Generative Pre-trained Transformer) and Gemini have advanced natural language processing in general and question answering in particular, however, they are prone to producing factually incorrect responses (i.e. hallucinations). Retrieval-augmented generation systems, integrating curated documents, can improve large language models’ performance and reduce the incidence of hallucination. We introduce AsthmaBot, a multi-lingual, multi-modal retrieval-augmented generation system for asthma support. Evaluation of an asthma-related frequently asked questions dataset shows AsthmaBot’s efficacy. AsthmaBot has an added interactive and intuitive interface that integrates different data modalities (text, images, videos) to make it accessible to the larger public. AsthmaBot is available online via <a class="ltx_ref ltx_url ltx_font_typewriter" href="asthmabot.datanets.org" title="">asthmabot.datanets.org</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Large language models <span class="ltx_ERROR undefined" id="id2.id1">\sep</span>Retrieval augmented generation

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the past few decades, asthma rates have been on the rise globally, attributed not just to genetic factors but primarily to the influence of numerous environmental and lifestyle risk factors <cite class="ltx_cite ltx_citemacro_cite">Nunes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib21" title="">2017</a>)</cite>. Asthma claims thousands of lives every year mainly due to lack of access to immediate and adequate medical care <cite class="ltx_cite ltx_citemacro_cite">Mwangi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib20" title="">2020</a>)</cite>. However, a significant number of asthma-related fatalities are preventable by home remedies, exercise, treatments, and action plans which can help reduce the symptoms of asthma patients either by avoiding triggers or employing relieving remedies <cite class="ltx_cite ltx_citemacro_cite">Mwangi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib20" title="">2020</a>); Murphy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib19" title="">2021</a>)</cite>. This shows the importance of easy and fast access to information in reducing the ramifications of asthma attacks. However, having an around-the-clock service by medical providers can be prohibitive in many ways, especially in developing countries, which motivates the need for an interactive automated medical support system for asthma patients.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Large language models (LLMs) have garnered considerable attention in recent years due to their generative abilities <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib1" title="">2020</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib3" title="">2021</a>); Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib27" title="">2021</a>)</cite>. Models such as ChatGPT <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib22" title="">2022</a>)</cite>, Gemini <cite class="ltx_cite ltx_citemacro_cite">Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib24" title="">2023</a>)</cite>, and Llama2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib25" title="">2023</a>)</cite> have paved the way for a new era of artificial intelligence (AI) where humans can interact with models in a mutually inclusive way. These models revolutionized multiple aspects of natural language processing (NLP) tasks (information retrieval, question answering, summarization, sentiment analysis etc) <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib22" title="">2022</a>); Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib24" title="">2023</a>); Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib25" title="">2023</a>)</cite>. Recently, a plethora of works showed the proficiency of LLMs in question answering. Although their performance is encouraging they were found to generate plausibly sounding but factually incorrect responses, commonly known as hallucinations. In addition, LLMs can only remember the data that they trained on hindering the recency of their knowledge <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib12" title="">2023</a>)</cite>. These limitations can negatively affect critical domains such as healthcare.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address these challenges new works <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib16" title="">2020</a>); Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib9" title="">2023</a>)</cite> supply LLMs with pertinent documents sourced from current and reliable collections. This technique is called retrieval augmented generation (RAG) and was first introduced in <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib16" title="">2020</a>)</cite>. RAG systems generally contain three main components: document collections (corpora), retrieval algorithms (retrievers), and backbone LLMs <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib9" title="">2023</a>)</cite>. However, existing RAG systems only produce textual information and are not multi-modal in addition LLMs suffer from language biases, which limits the quality of information that they generate in languages other than English.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we introduce AsthmaBot, an automated asthma medical support multi-lingual, multi-modal RAG system. It is designed to provide asthma patients with answers to their questions based on a recent and curated list of documents, videos and images that are relevant to their queries. We evaluated AsthmaBot on a list of asthma-related question-answer pairs based on frequently asked questions (FAQs) lists from multiple online sources. Although this work focuses on asthma our methodology can be applied to various domains (e.g. law, other diseases and medical concepts etc). In fact, the same system can be used for any set of text, image and video data without any additional steps. Our experiments show an increased performance relative to a no RAG baseline in different languages on different modalities. This work makes contributions on the application and methodology levels the following summarizes our contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Application:</p>
<ul class="ltx_itemize" id="S1.I1.i1.I1">
<li class="ltx_item" id="S1.I1.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.I1.i1.p1.1">Creating an automated asthma medical support system which contains text and media (images &amp; videos).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i1.I1.i2.p1.1">Evaluating our system on asthma question answering.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i1.I1.i3.p1.1">Making the system accessible to the wider community via a web-based interface.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Methodology:</p>
<ul class="ltx_itemize" id="S1.I1.i2.I1">
<li class="ltx_item" id="S1.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i2.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i2.I1.i1.p1.1">Design and implementation of a multi-modal and multi-lingual RAG system.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i2.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.I1.i2.p1.1">Proposing an automated evaluation process for a multi-modal, multi-language setting based on frequently asked questions (FAQs) and question-answer pairs.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i2.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i2.I1.i3.p1.1">Empirically Proving the linguistic biases of the used LLM, which motivates the translation of queries to English.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.I1.i4.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i2.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i2.I1.i4.p1.1">Proposing a visual interface that integrates the different modalities (text, images, and videos) and their sources in each AsthmaBot response.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Previous Literature</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Large Language Models (LLMs)</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Traditionally, language models (LM) were developed to model the sequential nature of text <cite class="ltx_cite ltx_citemacro_cite">Merity et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib17" title="">2018</a>)</cite>. These language models evolved from simple LSTM-based models <cite class="ltx_cite ltx_citemacro_cite">Merity et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib17" title="">2018</a>); Graves and Graves (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib10" title="">2012</a>)</cite> to transformer-based models such as BERT <cite class="ltx_cite ltx_citemacro_cite">Kenton and Toutanova (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib13" title="">2019</a>)</cite>. In general, these models were pre-trained in a general-purpose unsupervised task (e.g. next token prediction, next sentence prediction) and then fine-tuned to a specific task using supervised training. More recently, the surge in computational resources and increased efficiency of training techniques have allowed for a surmountable increase in transformer models’ parameters, which coupled with a significant amount of available data on the web have allowed for the creation of LLMs <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib22" title="">2022</a>); Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib24" title="">2023</a>)</cite>. LLMs came with a new set of abilities that the traditional models never exhibited, like the emergence property, which allows LLMs to do tasks that they were not trained on. However, LLMs suffer from hallucinations. Hallucinations describe model outputs that are linguistically coherent but nonfactual <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib12" title="">2023</a>)</cite>. To reduce the effects of these hallucinations retrieval augmented generation was proposed <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib16" title="">2020</a>); Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib9" title="">2023</a>)</cite>. In this setting an LLM is provided with a query relevant context that is extracted from factual sources to augment the limited knowledge of the LLM and increase the likelihood of having a factual answer. RAG systems have the following components: document collections (corpora), retrieval algorithms (retrievers), and backbone LLMs <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib9" title="">2023</a>)</cite>. The document collection is a collection of text documents from various sources that can be general or domain-specific. Retrieval algorithms are tasked with retrieving relevant documents given a query. The backbone LLM is used to generate an answer to the query given retrieved documents augmented prompt. Additionally, the previous RAG system is based on text-only documents, recent works explored multi-modal RAG systems, which can retrieve not just text but also images <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib5" title="">2022</a>); Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib30" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Biomedical LLMs and RAG</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">NLP researchers found fertile ground for innovation in Biomedical NLP due to its knowledge-intensive nature. Medicine has benefited from the development of NLP techniques since the inception of the field <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib15" title="">2020</a>); Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib11" title="">2021</a>)</cite>. Recently, LLMs have demonstrated a considerable improvement in existing approaches to medical benchmarks in various tasks (named entity recognition, question answering, medical text summarization, medical diagnosis etc) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib6" title="">2023c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib4" title="">b</a>)</cite>. Although multiple LLMs trained on medical text have been proposed, they still suffer from the same hallucinations as the general domain LLMs. Consequently, RAG-based systems for medical question answering have been proposed <cite class="ltx_cite ltx_citemacro_cite">Xiong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib28" title="">2024</a>); Miao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib18" title="">2024</a>); Zakka et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib29" title="">2024</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib26" title="">2024</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib26" title="">2024</a>)</cite> showed that a RAG system with a 13B parameter backbone LLM can significantly outperform a 70B model (i.e. MediTron <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib6" title="">2023c</a>)</cite>). However, these RAG systems are not multi-modal or multi-lingual.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">AsthmaBot was conceptualised and designed to allow asthma patients to access relevant information interactively and adaptively. AsthmaBot backend is a multi-modal, multi-lingual retrieval augmented generation LLM, while the frontend is in the form of a chatbot (e.g. ChatGPT, Gemini etc) where questions and answers are saved so that the user can explore them.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Collections</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To enrich LLM’s responses with factual up-to-date information specific documents and media were selected. These elements satisfy certain criteria rather than just doing a web search every time a query is given by the user, which can leave room for pseudo-scientific sources. Our main data formats are PDF documents obtained by searching Google, images from Google images and videos from YouTube. Search for the elements is done via queries in the different search platforms. Queries can be general like "asthma" or more specific (e.g. FAQs). Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3.SS1" title="3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_tag">3.1</span></a> shows different queries used to obtain data. FAQs are obtained from the gina website<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://ginasthma.org/about-us/faqs/</span></span></span>. These FAQs were later translated into different languages to obtain data from languages other than English. The PDF documents were downloaded manually while images from Google Images and YouTube videos were scraped automatically. After scraping the different images and videos we extracted the text from the source of the images and the video transcripts of each image and video respectively. We should note that we explored using image captioning to obtain image description but it was limited and didn’t give importance to the context in which the image was used, consequently, we opted for the text in the webpage of the image source as an alternative.</p>
</div>
<figure class="ltx_table" id="S3.SS1.tab1">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_align_middle" id="S3.SS1.tab1.1">
<tr class="ltx_tr" id="S3.SS1.tab1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.SS1.tab1.1.1.1">Data Type</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.tab1.1.1.2">Source</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.tab1.1.1.3">Query</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.tab1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.SS1.tab1.1.2.1">Documents</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.SS1.tab1.1.2.2">Google</td>
<td class="ltx_td ltx_border_tt" id="S3.SS1.tab1.1.2.3"></td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS1.tab1.2">الربو"
Images &amp; Google Images"asthma", "asthme", "الربو", FAQs
Videos  YouTube"asthma", "asthme", "الربو", FAQs</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.SS1.tab1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.SS1.tab1.4.2" style="font-size:90%;">Different queries for obtaining data from different modalities.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<section class="ltx_subsection ltx_figure_panel" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Vector DB Building and Retrievers</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">A vector database indexes text, images, videos and other data modalities based on a textual description of what they contain. This textual description is transformed into a dense vector using a language model, which facilitates the process of semantic search. We built multiple independent vector databases where each containing different modalities in different languages. Although one database that contains all the aforementioned elements can be used, it causes certain limitations. First, having multiple stores makes parallelism possible, increasing the speed of the inference process. Second, the search process can be parameterized providing more structure and control to the LLM and reducing hallucinations.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We built nine vector DBs for each modality (text, images, videos) in each language (French, Arab, English). For each modality, we obtain the indexing description by summarising document text, image source text and video transcripts for text, images and videos respectively. These descriptions are further translated into English if they are in French or Arabic. This choice was made: 1) to use high-performing embedding models in English, 2) to avoid the linguistic biases in LLMs which seem to favour English. We used the FAISS vector database <cite class="ltx_cite ltx_citemacro_cite">Douze et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib8" title="">2024</a>)</cite> to save and query the different elements. FAISS has a retriever that uses cosine similarity to retrieve passages that are close to the query.</p>
</div>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Prompting and Backbone LLM</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Prompting is a fundamental element of LLM inference as it has been shown that the quality and the design of the prompt affect the quality of the output of the LLM <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib2" title="">2023a</a>)</cite>. <a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3.SS3" title="3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a> shows the prompt that we used in AsthmaBot. The prompt contains three parameters the query, context and "history". The query refers to the user input, the context refers to the different text passages obtained by querying the different text vector DBs. The "history" refers to the chat history between the user and the LLM, composed of question-answer pairs. We used the Google Gemini LLM to infer the results of different queries.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<svg class="ltx_picture" height="212.98" id="S3.SS3.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,212.98) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 207.07 C 0 210.33 2.64 212.98 5.91 212.98 L 594.09 212.98 C 597.36 212.98 600 210.33 600 207.07 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 188.87 L 598.03 188.87 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#808080" fill-opacity="1.0"><path d="M 1.97 190.84 L 1.97 207.07 C 1.97 209.25 3.73 211.01 5.91 211.01 L 594.09 211.01 C 596.27 211.01 598.03 209.25 598.03 207.07 L 598.03 190.84 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 194.77)"><foreignobject color="#000000" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS3.p2.pic1.1.1.1.1.1.1">Prompt 2:</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.1.1.1.1.1.2">Prompt for LLM inference</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="163.28" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.1">INSTRUCTIONS:</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.2">You are an asthma medical support provider called AsthmaBot. You are designed to be as helpful as possible while providing only factual information.</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.3">You should be friendly, but not overly chatty. Context information is below.</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.4">Given the context information and chat history and no prior knowledge, answer the query.
Give a detailed answer.</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.5">Your answer should encompass the whole context.</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.6">CONTEXT:</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.7">{context}</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.8">CHAT HISTORY:</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.9">{history}</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.10">QUERY:</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.11">{question}</span>
<span class="ltx_p" id="S3.SS3.p2.pic1.2.2.2.1.1.12">ANSWER:</span>
</span></foreignobject></g></g></svg>
</div>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Translation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Instead of opting for queries in the native language, we translate the queries to English, search the vector databases, prompt the model in English, and translate the model response to the native query language. This was done because we noticed that LLMs (Gemini and ChatGPT) have biases towards the English language and can produce significantly richer responses in English than in Arabic or French. We further illustrate this in the results section, where we query the LLM using Arabic and get a significantly worse result than if we query it using the described process. We opted for the Google Translate API since it gave better results and a faster response time compared to other translators.</p>
</div>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Visual Interface</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">To ensure the accessibility of AsthmaBot to a wider user base we created an interactive interface similar to that of ChatGPT and Gemini with the added features of embedded videos, source documents and images. In addition, users can click on the images to be transferred to the source website of the image. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3.F1" title="Figure 1 ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_tag">1</span></a> shows the interface of AsthmaBot.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="598" id="S3.F1.g1" src="extracted/5875396/asthma_interface.png" width="795"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">AsthmaBot interface.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>AsthmaBot’s Inference Process</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">AsthmaBots’ inference process can be summarized as follows:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Query: the process starts with a user inputting a query.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Language detection: a language detection module detects the language of the query to choose the right vector stores.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Query translation: if the detected language is not English then the query is translated to English.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Vector store selection: AsthmaBot contains multiple vector stores: multiple text vector stores separated depending on language and content, and each one contains text chunks and their vector representation. Multiple image vector stores are separated depending on the language and indexed by the summary of the content of the image source document. Multiple video vector stores are separated depending on the language, where each video is indexed depending on a summary of its transcript. A vector store of each modality is selected.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.3">Retriever: a retriever searches the different vector stores to look for chunks of text, videos and images that are semantically similar to the query and where the similarity score is above a threshold. We have three thresholds for different text (<math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="S3.I1.i5.p1.1.m1.1"><semantics id="S3.I1.i5.p1.1.m1.1a"><msub id="S3.I1.i5.p1.1.m1.1.1" xref="S3.I1.i5.p1.1.m1.1.1.cmml"><mi id="S3.I1.i5.p1.1.m1.1.1.2" xref="S3.I1.i5.p1.1.m1.1.1.2.cmml">λ</mi><mi id="S3.I1.i5.p1.1.m1.1.1.3" xref="S3.I1.i5.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.1.m1.1b"><apply id="S3.I1.i5.p1.1.m1.1.1.cmml" xref="S3.I1.i5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i5.p1.1.m1.1.1.1.cmml" xref="S3.I1.i5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i5.p1.1.m1.1.1.2.cmml" xref="S3.I1.i5.p1.1.m1.1.1.2">𝜆</ci><ci id="S3.I1.i5.p1.1.m1.1.1.3.cmml" xref="S3.I1.i5.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.1.m1.1c">\lambda_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i5.p1.1.m1.1d">italic_λ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>), images (<math alttext="\lambda_{i}" class="ltx_Math" display="inline" id="S3.I1.i5.p1.2.m2.1"><semantics id="S3.I1.i5.p1.2.m2.1a"><msub id="S3.I1.i5.p1.2.m2.1.1" xref="S3.I1.i5.p1.2.m2.1.1.cmml"><mi id="S3.I1.i5.p1.2.m2.1.1.2" xref="S3.I1.i5.p1.2.m2.1.1.2.cmml">λ</mi><mi id="S3.I1.i5.p1.2.m2.1.1.3" xref="S3.I1.i5.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.2.m2.1b"><apply id="S3.I1.i5.p1.2.m2.1.1.cmml" xref="S3.I1.i5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i5.p1.2.m2.1.1.1.cmml" xref="S3.I1.i5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i5.p1.2.m2.1.1.2.cmml" xref="S3.I1.i5.p1.2.m2.1.1.2">𝜆</ci><ci id="S3.I1.i5.p1.2.m2.1.1.3.cmml" xref="S3.I1.i5.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.2.m2.1c">\lambda_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i5.p1.2.m2.1d">italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>) and videos (<math alttext="\lambda_{v}" class="ltx_Math" display="inline" id="S3.I1.i5.p1.3.m3.1"><semantics id="S3.I1.i5.p1.3.m3.1a"><msub id="S3.I1.i5.p1.3.m3.1.1" xref="S3.I1.i5.p1.3.m3.1.1.cmml"><mi id="S3.I1.i5.p1.3.m3.1.1.2" xref="S3.I1.i5.p1.3.m3.1.1.2.cmml">λ</mi><mi id="S3.I1.i5.p1.3.m3.1.1.3" xref="S3.I1.i5.p1.3.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.3.m3.1b"><apply id="S3.I1.i5.p1.3.m3.1.1.cmml" xref="S3.I1.i5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i5.p1.3.m3.1.1.1.cmml" xref="S3.I1.i5.p1.3.m3.1.1">subscript</csymbol><ci id="S3.I1.i5.p1.3.m3.1.1.2.cmml" xref="S3.I1.i5.p1.3.m3.1.1.2">𝜆</ci><ci id="S3.I1.i5.p1.3.m3.1.1.3.cmml" xref="S3.I1.i5.p1.3.m3.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.3.m3.1c">\lambda_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i5.p1.3.m3.1d">italic_λ start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>) respectively.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1">Prompt Generation: the retrieved text is fed to a prompt in addition to the query to augment the LLM response.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i7.p1">
<p class="ltx_p" id="S3.I1.i7.p1.1">Answer generation: the constructed prompt is fed to an LLM to generate the answer to the query.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i8.p1">
<p class="ltx_p" id="S3.I1.i8.p1.1">Answer translation: The output of the LLM is in English, consequently we have to translate it back to the detected language of the query.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS6.p1.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S3.F2" title="Figure 2 ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_tag">2</span></a> shows the process of inference in AsthmaBot. This interface is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="asthmabot.datanets.org" title="">asthmabot.datanets.org</a>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="322" id="S3.F2.g1" src="x1.png" width="573"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.4.2" style="font-size:90%;">AsthmaBot Overview<span class="ltx_text ltx_font_medium" id="S3.F2.4.2.1">: given a query AsthmaBot uses asthma-related external resources to retrieve information relevant to the query and the query language before synthesizing a multi-modal response. This framework provides a multi-modal response grounded in truth, which reduces hallucinations and provides multiple formats for the answer.</span></span></figcaption>
</figure>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Procedure</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To evaluate the efficacy of AsthmaBot at answering asthma-related questions we extracted a list of frequently asked questions (FAQs) and their corresponding answers from different sources and compared the answers generated by AsthmaBot to the answers originally given to the FAQs. In addition, to evaluate the veracity of information in the videos and images we modify the AsthmaBot context to video transcript summary and image source summary, respectively. In this way, we can automatically evaluate AsthmaBot’s multi-modal retrieval capabilities without human intervention. We employed two automated evaluation metrics: ROUGE <cite class="ltx_cite ltx_citemacro_cite">Chin-Yew (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib7" title="">2004</a>)</cite> and BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib23" title="">2002</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Data</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To evaluate AsthmaBot we collected a set of asthma-related FAQs from various sources. We collected over 400 FAQs from over 30 sources. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S4.T2" title="Table 2 ‣ 4.2 Evaluation Data ‣ 4 Results ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_tag">2</span></a> gives various examples of question-answer pairs that we collected. To evaluate AsthmaBot’s multilingual capabilities we translated the FAQs using Google Translate API.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.2">
<tr class="ltx_tr" id="S4.T2.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.1">
<span class="ltx_p" id="S4.T2.2.1.1.1.1" style="width:100.0pt;">Question</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.2.1">
<span class="ltx_p" id="S4.T2.2.1.2.1.1" style="width:250.0pt;">Answer</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.1.1">
<span class="ltx_p" id="S4.T2.2.2.1.1.1" style="width:100.0pt;">How do I know when my child is old enough not to use a spacer?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.2.1">
<span class="ltx_p" id="S4.T2.2.2.2.1.1" style="width:250.0pt;">Anyone using a metered-dose inhaler (MDI aka ’puffer’) should always use a spacer. A spacer helps more medicine reach your lungs. If you don’t want to use a spacer, you should talk to your doctor about getting a different asthma device. Dry powder inhalers do not require spacers. You should ask your pharmacist, your doctor, or your asthma educator to review your device technique every time you see them. It is harder to be good at taking your medicine than you might think. Consider using the same device for all your asthma medications so that you can become really good at one technique.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.3.1.1">
<span class="ltx_p" id="S4.T2.2.3.1.1.1" style="width:100.0pt;">Why does asthma seem worse at night?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.3.2.1">
<span class="ltx_p" id="S4.T2.2.3.2.1.1" style="width:250.0pt;">We have natural hormones (glucocorticoids) in our bodies that help keep the airways open by reducing inflammation. At nighttime when you are sleeping, these hormones are normally at lower levels, allowing more inflammation in the airways and increasing asthma symptoms. If your child has regular asthma symptoms at night, it may mean their asthma is poorly controlled and you should make an appointment to discuss this with your doctor.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.4.1.1">
<span class="ltx_p" id="S4.T2.2.4.1.1.1" style="width:100.0pt;">Will my child become resistant to asthma medicine?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.4.2.1">
<span class="ltx_p" id="S4.T2.2.4.2.1.1" style="width:250.0pt;">No. If asthma medicines become less effective, contact a healthcare provider for advice. Your health care provider may increase or change your medicines.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.5.1.1">
<span class="ltx_p" id="S4.T2.2.5.1.1.1" style="width:100.0pt;">Why does my asthma get worse when I am upset or worried about something?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.5.2.1">
<span class="ltx_p" id="S4.T2.2.5.2.1.1" style="width:250.0pt;">Not all people with asthma feel worse when they are upset or worried. Those who do may be easily stressed, or may cry or breathe too fast (hyperventilate) easily. Another reason that your asthma gets worse could be that you are not being treated properly for the inflammation you have in your airways.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.1.1">
<span class="ltx_p" id="S4.T2.2.6.1.1.1" style="width:100.0pt;">Can asthma medication help prevent asthma symptoms?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S4.T2.2.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.2.1">
<span class="ltx_p" id="S4.T2.2.6.2.1.1" style="width:250.0pt;">Yes. Asthma medications include very effective airway openers. Even more importantly, they include very effective controllers (inhaled steroids), which can prevent most asthma attacks when used regularly.</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Question-answer pair examples from the FAQs dataset.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S4.T3" title="Table 3 ‣ 4.3 Evaluation Results ‣ 4 Results ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results of querying using AsthmaBot in multiple languages (English, Arabic, French) in multiple data modalities (text, images, videos). The table shows that RAG significantly improves the performance in question answering relative to the no RAG baseline. This is true for all modalities and languages. On the other hand, we notice that there are variations in performance between languages. This can be attributed to the richness of the documents that were used in RAG.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S4.T4" title="Table 4 ‣ 4.3 Evaluation Results ‣ 4 Results ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_tag">4</span></a> shows the results of experimenting with using English-only input to the LLM and using prompts in the native language of the query. The results show that the English-only prompts perform significantly better than the native language prompt. This can be attributed to the significant amount of English prompts that the LLM was trained on. This language bias limits the richness of LLM output in languages other than English.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.2">
<tr class="ltx_tr" id="S4.T3.2.1">
<td class="ltx_td ltx_align_center ltx_border_r" colspan="2" id="S4.T3.2.1.1">Setting</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.2">ROUGE-1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.3">ROUGE-2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.4">ROUGE-L</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.5">BLEU</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.1" rowspan="4"><span class="ltx_text" id="S4.T3.2.2.1.1">English</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.2">No RAG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.3">0.2370</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.4">0.0504</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5">0.1338</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.6">0.0180</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.3.1">Text</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.2">0.2684</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.3">0.0612</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.4">0.1576</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.5">0.0367</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.4.1">Image</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.2">0.2664</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.3">0.0585</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.4">0.1547</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.5">0.0327</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.5.1">Video</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.2">0.2686</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.3">0.0570</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.4">0.1556</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.5">0.0321</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.6.1" rowspan="4"><span class="ltx_text" id="S4.T3.2.6.1.1">French</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.6.2">No RAG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.6.3">0.2810</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.6.4">0.0868</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.6.5">0.1528</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.6.6">0.0219</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.7.1">Text</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.7.2">0.2963</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.7.3">0.0952</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.7.4">0.1706</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.7.5">0.0426</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.8.1">Image</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.2">0.3108</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.3">0.1003</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.4">0.1709</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.5">0.0441</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.9.1">Video</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.2">0.3024</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.3">0.0973</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.4">0.1696</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.5">0.0408</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.10">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T3.2.10.1" rowspan="4"><span class="ltx_text" id="S4.T3.2.10.1.1">Arabic</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.10.2">No RAG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.10.3">0.0148</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.10.4">0.0056</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.10.5">0.0149</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.10.6">0.0125</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.11.1">Text</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.11.2">0.0152</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.11.3">0.0068</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.11.4">0.0149</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.11.5">0.0300</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.12.1">Image</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.12.2">0.0162</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.12.3">0.0044</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.12.4">0.0162</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.12.5">0.0282</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.13">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.2.13.1">Video</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.13.2">0.0169</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.13.3">0.0050</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.13.4">0.0167</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.13.5">0.0263</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">Performance Evaluation in different languages for different modalities.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.2">
<tr class="ltx_tr" id="S4.T4.2.1">
<td class="ltx_td ltx_align_center ltx_border_r" colspan="2" id="S4.T4.2.1.1">Setting</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.2">ROUGE-1</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.3">ROUGE-2</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4">ROUGE-L</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5">BLEU</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.1" rowspan="3"><span class="ltx_text" id="S4.T4.2.2.1.1">Arabic</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2">No RAG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.3">0.0148</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.4">0.0056</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.5">0.0148</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.6">0.0125</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.2.3.1">RAG NQ</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.3.2">0.0106</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.3.3">0.0026</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.3.4">0.0106</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.3.5">0.0176</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.2.4.1">RAG TQ</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.2">0.0152</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.3">0.0068</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.4">0.0149</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.5">0.0300</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T4.2.5.1" rowspan="3"><span class="ltx_text" id="S4.T4.2.5.1.1">French</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.5.2">No RAG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.3">0.2810</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.4">0.0868</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.5">0.1528</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.6">0.0219</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.2.6.1">RAG NQ</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.2">0.2963</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.3">0.0952</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.4">0.1706</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.5">0.0426</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.7">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.2.7.1">RAG TQ</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.2.7.2">0.3181</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.2.7.3">0.0989</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.2.7.4">0.1761</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.2.7.5">0.0394</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.4.2" style="font-size:90%;">Results of querying with native language and with English translations of the query and the prompts. We conducted the same experiment for French and Arabic. NQ refers to "native query" and TQ refers to "translated query".</span></figcaption>
</figure>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Impact and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">AsthmaBot integrates multiple features that are not included in many publicly available LLMs. We compared AsthmaBot to multiple publically available LLMs: ChatGPT<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>chat.openai.com</span></span></span>, Gemini <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>gemini.google.com</span></span></span>, Perplexity AI<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>perplexity.ai</span></span></span>, YouChat<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>you.com</span></span></span>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S5.T5" title="Table 5 ‣ 5 Impact and Future Work ‣ 4.3 Evaluation Results ‣ 4 Results ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_tag">5</span></a> compares the retrieval capabilities of different LLMs and AsthmaBot. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#S5.T6" title="Table 6 ‣ 5 Impact and Future Work ‣ 4.3 Evaluation Results ‣ 4 Results ‣ 3.6 AsthmaBot’s Inference Process ‣ 3.5 Visual Interface ‣ 3.4 Translation ‣ 3.3 Prompting and Backbone LLM ‣ 3.2 Vector DB Building and Retrievers ‣ 3.1 Data Collections ‣ 3 Methodology ‣ AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support"><span class="ltx_text ltx_ref_tag">6</span></a> compares different LLMs generative capabilities. Although these models are made for general-purpose use, there is an increasing need for more domain-specific models with more constraints to reduce hallucination and language bias and AsthmaBot is another step in this direction for biomedical applications.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T5.2">
<tr class="ltx_tr" id="S5.T5.2.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.2.1.1">LLM</td>
<td class="ltx_td ltx_align_center" colspan="2" id="S5.T5.2.1.2">Document Retrieval</td>
<td class="ltx_td ltx_align_center" colspan="2" id="S5.T5.2.1.3">Image Retrieval</td>
<td class="ltx_td ltx_align_center" colspan="2" id="S5.T5.2.1.4">Video Retrieval</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.2">
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T5.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.2.2">ML</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.2.3">SL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.2.4">ML</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.2.5">SL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.2.6">ML</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.2.7">SL</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.3.1">ChatGPT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.2">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.3">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.4">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.5">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.6">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.7">No</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.2.4.1">Gemini</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.2">Yes</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.3">Yes</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.4">No</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.5">Yes</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.6">No</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.7">No</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.2.5.1">Perplexity AI</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.2">No</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.3">Yes</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.4">No</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.5">Yes</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.6">No</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.7">Yes</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.2.6.1">YouChat</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.2">No</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.3">Yes</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.4">No</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.5">No</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.6">No</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.7">No</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.7">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.2.7.1">AsthmaBot</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.7.2">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.7.3">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.7.4">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.7.5">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.7.6">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.7.7">Yes</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.3.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S5.T5.4.2" style="font-size:90%;">Comparison of retrieval capabilities of different publicly available LLMs and AsthmaBot. "ML" refers to multi-lingual and "SL" refers to simple language.</span></figcaption>
</figure>
<figure class="ltx_table" id="S5.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T6.2">
<tr class="ltx_tr" id="S5.T6.2.1">
<td class="ltx_td ltx_border_r" id="S5.T6.2.1.1"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.1.2">Language answer consistency</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.1.3">Answer specificity</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.1.4">Hallucinations</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.1">ChatGPT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.2">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.3">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4">Yes</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.2.3.1">Gemini</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.3.2">No</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.3.3">No</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.3.4">Yes</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.2.4.1">Perplexity AI</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.4.2">No</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.4.3">No</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.4.4">No</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.2.5.1">YouChat</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.5.2">Yes</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.5.3">No</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.5.4">No</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.2.6.1">AsthmaBot</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.2.6.2">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.2.6.3">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.2.6.4">No</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T6.3.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S5.T6.4.2" style="font-size:90%;">Comparison of the outputs of different LLMs and AsthmaBot.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">There are multiple improvements that we think can further enhance the quality of AsthmaBot responses. The following are a few:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Improving data curation: AsthmaBot RAG video and image modules are less curated than the text module that we took from educational resources. Consequently, improving having expert-curated videos and images can further improve the trustworthiness of AsthmaBot.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Visual summary (infographics) generation: since AsthmaBot will be mainly used by patients with varying educational levels, visual summaries of LLM results can improve accessibility significantly especially for children.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">Automatic prompting: AsthmaBot can benefit from the new research in learnable prompts <cite class="ltx_cite ltx_citemacro_cite">Khattab et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15815v1#bib.bib14" title="">2023</a>)</cite>. This is particularly important since AsthmaBot since its primarily destined for asthma patients. Consequently, fine-tuning prompts to FAQs can improve it.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">Extending to other diseases: Although AsthmaBot focuses mainly on asthma it can be customised to fit other diseases by adding more documents, images and videos and changing the prompt.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">Evaluation: In this work, we attempted a fully automated evaluation protocol, which may not convey the full extent of evaluation like factuality and adherence to contextual information. On the other hand, these aspects can only be evaluated manually for now, which is prohibitive in our case.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we have presented AsthmaBot, a multi-lingual, multi-modal Retrieval-Augmented Generation (RAG) system designed to provide automated support for asthma patients. Through the integration of curated documents, videos, and images, AsthmaBot offers personalized responses to asthma-related queries, empowering patients with valuable information. Our evaluation, based on diverse asthma-related FAQs, demonstrates AsthmaBot’s enhanced performance compared to a non-RAG baseline, highlighting its effectiveness in providing relevant information. Moreover, AsthmaBot tackles the language biases inherent in Large Language Models (LLMs) by offering multi-lingual support. The visual interface of AsthmaBot further enhances user experience, presenting information in a comprehensive and accessible manner. Although this work focuses on applying the system to asthma it can be applied to any domain containing text, video and image information (e.g. law, entertainment, other medical conditions and concepts etc).</p>
</div>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 33, 1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023a)</span>
<span class="ltx_bibblock">
Chen, B., Zhang, Z., Langrené, N., Zhu, S., 2023a.

</span>
<span class="ltx_bibblock">Unleashing the potential of prompt engineering in large language models: a comprehensive review.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.14735 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al., 2021.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2107.03374 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023b)</span>
<span class="ltx_bibblock">
Chen, Q., Sun, H., Liu, H., Jiang, Y., Ran, T., Jin, X., Xiao, X., Lin, Z., Chen, H., Niu, Z., 2023b.

</span>
<span class="ltx_bibblock">An extensive benchmark study on biomedical text generation and mining with chatgpt.

</span>
<span class="ltx_bibblock">Bioinformatics 39, btad557.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Chen, W., Hu, H., Chen, X., Verga, P., Cohen, W., 2022.

</span>
<span class="ltx_bibblock">Murag: Multimodal retrieval-augmented generator for open question answering over images and text, in: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5558–5570.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023c)</span>
<span class="ltx_bibblock">
Chen, Z., Cano, A.H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., Pagliardini, M., Fan, S., Köpf, A., Mohtashami, A., et al., 2023c.

</span>
<span class="ltx_bibblock">Meditron-70b: Scaling medical pretraining for large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2311.16079 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chin-Yew (2004)</span>
<span class="ltx_bibblock">
Chin-Yew, L., 2004.

</span>
<span class="ltx_bibblock">Rouge: A package for automatic evaluation of summaries, in: Proceedings of the Workshop on Text Summarization Branches Out, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Douze et al. (2024)</span>
<span class="ltx_bibblock">
Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.E., Lomeli, M., Hosseini, L., Jégou, H., 2024.

</span>
<span class="ltx_bibblock">The faiss library.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2401.08281 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang, H., 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2312.10997 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves and Graves (2012)</span>
<span class="ltx_bibblock">
Graves, A., Graves, A., 2012.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock">Supervised sequence labelling with recurrent neural networks , 37–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2021)</span>
<span class="ltx_bibblock">
Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., Poon, H., 2021.

</span>
<span class="ltx_bibblock">Domain-specific language model pretraining for biomedical natural language processing.

</span>
<span class="ltx_bibblock">ACM Transactions on Computing for Healthcare (HEALTH) 3, 1–23.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., et al., 2023.

</span>
<span class="ltx_bibblock">A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2311.05232 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenton and Toutanova (2019)</span>
<span class="ltx_bibblock">
Kenton, J.D.M.W.C., Toutanova, L.K., 2019.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding, in: Proceedings of NAACL-HLT, pp. 4171–4186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab et al. (2023)</span>
<span class="ltx_bibblock">
Khattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam, K., Haq, S., Sharma, A., Joshi, T.T., Moazam, H., Miller, H., et al., 2023.

</span>
<span class="ltx_bibblock">Dspy: Compiling declarative language model calls into self-improving pipelines, in: R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2020)</span>
<span class="ltx_bibblock">
Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J., 2020.

</span>
<span class="ltx_bibblock">Biobert: a pre-trained biomedical language representation model for biomedical text mining.

</span>
<span class="ltx_bibblock">Bioinformatics 36, 1234–1240.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., et al., 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 33, 9459–9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merity et al. (2018)</span>
<span class="ltx_bibblock">
Merity, S., Keskar, N.S., Socher, R., 2018.

</span>
<span class="ltx_bibblock">Regularizing and optimizing lstm language models, in: International Conference on Learning Representations.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao et al. (2024)</span>
<span class="ltx_bibblock">
Miao, J., Thongprayoon, C., Suppadungsuk, S., Valencia, O.A.G., Cheungpasitporn, W., 2024.

</span>
<span class="ltx_bibblock">Integrating retrieval-augmented generation with large language models in nephrology: Advancing practical applications.

</span>
<span class="ltx_bibblock">Medicina (Kaunas, Lithuania) 60, 445.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murphy et al. (2021)</span>
<span class="ltx_bibblock">
Murphy, J., McSharry, J., Hynes, L., Molloy, G.J., et al., 2021.

</span>
<span class="ltx_bibblock">A smartphone app to support adherence to inhaled corticosteroids in young adults with asthma: multi-methods feasibility study.

</span>
<span class="ltx_bibblock">JMIR Formative Research 5, e28784.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mwangi et al. (2020)</span>
<span class="ltx_bibblock">
Mwangi, A., Ndashimye, E., Karikumutima, B., Ray, S.K., 2020.

</span>
<span class="ltx_bibblock">An iot-alert system for chronic asthma patients, in: 2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), IEEE. pp. 0012–0019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nunes et al. (2017)</span>
<span class="ltx_bibblock">
Nunes, C., Pereira, A.M., Morais-Almeida, M., 2017.

</span>
<span class="ltx_bibblock">Asthma costs and social impact.

</span>
<span class="ltx_bibblock">Asthma research and practice 3, 1–11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al., 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 35, 27730–27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Papineni, K., Roukos, S., Ward, T., Zhu, W.J., 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation, in: Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2023)</span>
<span class="ltx_bibblock">
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al., 2023.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2312.11805 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al., 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.09288 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Wang, J., Yang, Z., Yao, Z., Yu, H., 2024.

</span>
<span class="ltx_bibblock">Jmlr: Joint medical llm and retrieval training for enhancing reasoning and professional question answering capability.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2402.17887 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2021)</span>
<span class="ltx_bibblock">
Wei, C., Xie, S.M., Ma, T., 2021.

</span>
<span class="ltx_bibblock">Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 34, 16158–16170.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al. (2024)</span>
<span class="ltx_bibblock">
Xiong, G., Jin, Q., Lu, Z., Zhang, A., 2024.

</span>
<span class="ltx_bibblock">Benchmarking retrieval-augmented generation for medicine.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2402.13178 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zakka et al. (2024)</span>
<span class="ltx_bibblock">
Zakka, C., Shad, R., Chaurasia, A., Dalal, A.R., Kim, J.L., Moor, M., Fong, R., Phillips, C., Alexander, K., Ashley, E., et al., 2024.

</span>
<span class="ltx_bibblock">Almanac—retrieval-augmented language models for clinical medicine.

</span>
<span class="ltx_bibblock">NEJM AI 1, AIoa2300068.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Zhao, R., Chen, H., Wang, W., Jiao, F., Qin, C., Ding, B., Guo, X., Li, M., Li, X., Joty, S., et al., 2023.

</span>
<span class="ltx_bibblock">Retrieving multimodal information for augmented generation: A survey, in: The 2023 Conference on Empirical Methods in Natural Language Processing.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 24 07:21:21 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
