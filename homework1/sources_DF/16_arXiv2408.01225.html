<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion</title>
<!--Generated on Fri Aug  2 12:28:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.01225v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S1" title="In Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">INTRODUCTION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S2" title="In Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">RELATED WORK</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S2.SS1" title="In II RELATED WORK ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Immersive Robot Telepresence and Teleoperation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S2.SS2" title="In II RELATED WORK ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">3D Representations for Robot Teleoperation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S3" title="In Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Reality Fusion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S3.SS1" title="In III Reality Fusion ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Design Goals</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S3.SS2" title="In III Reality Fusion ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">3D Gaussian Splatting</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S3.SS3" title="In III Reality Fusion ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Stereoscopic 3D Projection in World Space</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4" title="In Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">FRAMEWORK IMPLEMENTATION</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.SS1" title="In IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">The Telepresence Robot</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.SS2" title="In IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Unity 3DGS VR Renderer</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.SS3" title="In IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Unity Turtlebot Control Module</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.SS4" title="In IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Overall System Performance</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S5" title="In Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">USER STUDY EXPERIMENT</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S5.SS1" title="In V USER STUDY EXPERIMENT ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Conditions</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S5.SS2" title="In V USER STUDY EXPERIMENT ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Participants</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S5.SS3" title="In V USER STUDY EXPERIMENT ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Tasks</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S5.SS4" title="In V USER STUDY EXPERIMENT ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Materials</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S5.SS5" title="In V USER STUDY EXPERIMENT ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span> </span><span class="ltx_text ltx_font_italic">Measures</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S5.SS6" title="In V USER STUDY EXPERIMENT ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-F</span> </span><span class="ltx_text ltx_font_italic">Hypothesis</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S6" title="In Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">RESULTS and DISCUSSION</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S6.SS1" title="In VI RESULTS and DISCUSSION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Improved Performance with Reality Fusion (H1)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S6.SS2" title="In VI RESULTS and DISCUSSION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Exocentric and Egocentric Comparison (H2)</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S7" title="In Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">CONCLUSION and FUTURE WORK</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ke Li<sup class="ltx_sup" id="id8.8.id1"><span class="ltx_text ltx_font_italic" id="id8.8.id1.1">1,2</span></sup>, Reinhard Bacher<sup class="ltx_sup" id="id9.9.id2"><span class="ltx_text ltx_font_italic" id="id9.9.id2.1">1</span></sup>, Susanne Schmidt<sup class="ltx_sup" id="id10.10.id3"><span class="ltx_text ltx_font_italic" id="id10.10.id3.1">2</span></sup>, Wim Leemans<sup class="ltx_sup" id="id11.11.id4"><span class="ltx_text ltx_font_italic" id="id11.11.id4.1">1</span></sup>, Frank Steinicke<sup class="ltx_sup" id="id12.12.id5"><span class="ltx_text ltx_font_italic" id="id12.12.id5.1">2</span></sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id13.13.id1"><span class="ltx_text ltx_font_italic" id="id13.13.id1.1">1</span></sup> Ke Li, Reinhard Bacher, and Wim Leemans are with the accelerator division at the Deutsche Elektronen Synchrotron DESY, Notkestraße 85, 22607 Hamburg, Germany. Contact email: <span class="ltx_text ltx_font_typewriter" id="id14.14.id2" style="font-size:90%;">ke.li1@desy.de</span><sup class="ltx_sup" id="id15.15.id1"><span class="ltx_text ltx_font_italic" id="id15.15.id1.1">2</span></sup> Ke Li, Susanne Schmidt, and Frank Steinicke are with the Human-Computer Interaction group at Hamburg University, Vogt-Kölln-Straße 30, 22527 Hamburg, Germany.This work was supported by DASHH (HELMHOLTZ Graduate School for the Structure of Matter) with the Grant-No. HIDSS-0002.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id16.id1">We introduce <em class="ltx_emph ltx_font_italic" id="id16.id1.1">Reality Fusion</em>, a novel robot teleoperation system that localizes, streams, projects, and merges a typical onboard depth sensor with a photorealistic, high resolution, high framerate, and wide FoV rendering of the complex remote environment represented as 3D Gaussian splats (3DGS). Our framework enables robust egocentric and exocentric robot teleoperation in immersive VR, with the 3DGS effectively extending spatial information of a depth sensor with limited FoV and balancing the trade-off between data streaming costs and data visual quality. We evaluated our framework through a user study with 24 participants, which revealed that <em class="ltx_emph ltx_font_italic" id="id16.id1.2">Reality Fusion</em> leads to significantly better user performance, situation awareness, and user preferences. To support further research and development, we provide an open-source implementation with an easy-to-replicate custom-made telepresence robot, a high-performance virtual reality 3DGS renderer, and an immersive robot control package. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/uhhhci/RealityFusion" title="">https://github.com/uhhhci/RealityFusion</a></span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">INTRODUCTION</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, the rapid development of extended reality (XR) technology has presented enormous opportunities for robot teleoperation and telepresence systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib21" title="">21</a>]</cite>. The possibility to display 3D spatial cues about the robot’s environment to the operators through an immersive head-mounted display (HMD) enables a remote “telepresence” experience that has the potential to significantly improve the operator’s task performance. However, building a robust real-time immersive robot teleoperation system is still faced with many technical challenges. On one hand, real-time 3D data capturing and streaming to the HMD is crucial in providing operators with accurate situational awareness of the robot’s environment. On the other hand, spatial data streaming and processing presents a trade-off between visual quality and processing latency. While telepresence systems using omnidirectional camera <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib23" title="">23</a>]</cite> or multi-camera setups <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib7" title="">7</a>]</cite> can provide operators with a high level of immersion and detailed information of the robot’s environment, the latency in streaming, processing, and rendering these 3D data introduce the undesirable effect of VR motion sickness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib23" title="">23</a>]</cite> and a delay in robot control and intervention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib16" title="">16</a>]</cite>. Due to the limitation of robots’ payload and the limited computational resources of an embedded system, many real-world mobile platforms can only provide operators with real-time visual feedback through low-cost sensor setups such as a single 2D video camera or a single stereo depth camera. These sensors often capture visual information with a restricted field of view (FoV), which hinders users from observing the remote environment from obscured angles and therefore restricts them from establishing a concrete mental model of the robot’s surroundings.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="631" id="S1.F1.g1" src="x1.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;"> Real-time point points from the robot’s onboard RGBD sensor are localized, streamed, and projected onto a photorealistic scene of the remote environment represented as 3DGS in immersive VR. </span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this work, we propose <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">Reality Fusion</em>, a novel immersive robot teleoperation framework that implements a multi-modal data fusion method to address the issue of spatial data streaming trade-offs. As Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S1.F1" title="Figure 1 ‣ I INTRODUCTION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates, our framework first introduces a system that can render high resolution, high framerate, wide FoV, and photorealistic 3D scenes of complex environments that are represented as 3D Gaussian splats (3DGS)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib10" title="">10</a>]</cite> in VR. Using this type of photorealistic scene representation as the digital replica of the robot’s remote environment, the VR system enables offline visualization of complex scenarios that are typically difficult to accurately model with conventional 3D meshes or point clouds.
To enable operators to view the real-time status of the remote environment, we introduce a data fusion method, where real-time point cloud from the onboard RGBD sensor is localized, streamed, projected, and merged with the 3DGS environment, with the 3DGS rendering of the remote environment effectively extending the FoV of the RGBD point cloud. Such a teleoperation system design allows us to implement a lightweight, flexible, and cost-efficient immersive telepresence mobile robot with only a single RGBD sensor and a small embedded system.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We systematically benchmarked our framework through a controlled user study experiment, in which 24 participants performed a mobile robot navigation task through a real-world maze. Furthermore, we compare the efficiency of egocentric and exocentric teleoperation, from which we discuss the advantages and trade-offs of these two teleoperation modes to derive further design insights for immersive telepresence systems using <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">reality fusion</em>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">RELATED WORK</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Immersive Robot Telepresence and Teleoperation</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">According to Adamides et.al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib2" title="">2</a>]</cite>, one central aspect of an efficient teleoperation system is its capability to provide operators with a high level of situation awareness of the robot’s surroundings. With the rapid advancement of VR technology, abundant research demonstrated that teleoperation through an immersive HMD can significantly enhance users’ situation awareness and task performance compared to traditional 2D display <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">A key challenge in building an immersive robot teleoperation system is to provide operators with high-quality and low-latency volumetric visual feedback of the robot’s environment. Ferland et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib5" title="">5</a>]</cite>’s teleoperation system can perform a stereoscopic projection of a binocular camera into the user’s world space, allowing for 6 Degrees of Freedom (DoF) exocentric immersive teleoperation in VR. However, operators’ situation awareness can be largely restricted by the narrow FoV of the onboard camera. Although several previous works investigated using a multi-camera setup <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib16" title="">16</a>]</cite> or streaming videos from an omnidirectional camera <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib23" title="">23</a>]</cite>, these solutions introduce a significant increase in data streaming latency, leading to undesirable effect of VR motion sickness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib23" title="">23</a>]</cite> and delay in robot control and intervention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib16" title="">16</a>]</cite>. Another approach suggests displaying real-time mesh reconstruction results to the operator using dynamic SLAM algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib15" title="">15</a>]</cite>. However, 3D reconstruction with dynamic SLAM can introduce temporal delays of several seconds, making it unsuitable for applications requiring immediate intervention by the operator. Tefera et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib18" title="">18</a>]</cite> propose reducing live point cloud streaming bandwidth through a foveated point cloud segmentation and streaming framework, but this could introduce visual degradation and aliasing effects during third-person teleoperation, possibly reducing user flexibility and control options.
Various prior robot teleoperation systems use a cockpit-like design, integrating multiple sensor data sources—including 2D videos and 3D point clouds—into immersive user interfaces within a simulated control-room environment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib3" title="">3</a>]</cite>. These systems, similar to our approach, enable monitoring of a robot’s movements from both egocentric and exocentric perspectives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib3" title="">3</a>]</cite>. However, cockpit-like teleoperation frameworks typically use a large, immersive 3D space to display a wide range of visual information, which can vary in latency and spatial dimensions. This can lead to confusing interface designs and cognitive overload for users.
Our work extends previous teleoperation systems by combining limited online volumetric data with a high-quality offline photorealistic 3D representation of the environment. The offline representation serves as a contextual guideline to enhance operators’ immersion, while the online data provides primary visual feedback for monitoring the robot’s surroundings. This creates a more robust data streaming solution and a more coherent visualization style for real-time remote control.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">3D Representations for Robot Teleoperation</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Another key challenge in immersive robot teleoperation is creating robust 3D representations to visualize the remote environment. Currently, most immersive robot teleoperation systems use conventional explicit representations such as point clouds and meshes to visualize the remote environment. However, 3D meshes for robot teleoperation are typically created with dynamic SLAM algorithms for real-time feedback <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib15" title="">15</a>]</cite>, which can yield inaccurate results with complex geometries, making them unsuitable to represent complex real-world conditions such as an industrial facility. Although point cloud is also a popular 3D representation in immersive robot teleoperation, such discrete representations introduce holes and occlusions, degrading the visual quality of the 3D representation and restricting users’ understanding of the modeled environments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The latest breakthrough in photorealistic scene rendering proposes a radiance field representation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib12" title="">12</a>]</cite>. By storing such radiance fields in the form of 3D Gaussians to encode the directional optical property of a continuous volume space, radiance field rendering through 3D Gaussian splatting enables robust and accurate replication and real-time visualization of the appearance of our complex realities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib10" title="">10</a>]</cite>. Despite the recency of its development, several works already investigated its applicability in robot localization and mapping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib22" title="">22</a>]</cite>. However, due to the early stage of research of 3DGS for robotics, existing 3DGS SLAM methods result in degraded visual quality or an increase in rendering frame timing, making them unsuitable for real-time immersive VR applications. Therefore, our framework focuses on developing an efficient integration of the current 3DGS method to an immersive robot teleoperation system for rendering 3DGS models offline rather than reconstructing dynamic SLAM 3DGS visual feedback online. Nonetheless, this is, to our best knowledge, the first usable robot teleoperation system based on immersive photorealistic rendering in VR.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Reality Fusion</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We define the term “<em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">Reality Fusion</em>” as the merging of two photorealistic 3D representations of a real-world environment with the purpose of data augmentation by combining the complementary features of different types of spatial data and naturally integrating them into a coherent spatial user interface (UI). In this work, we develop a <em class="ltx_emph ltx_font_italic" id="S3.p1.1.2">reality fusion</em> method that can combine real-time 3D projection of a stereo camera and the 3DGS of the real world to create a fully immersive telepresence experience for robot operators. This section presents the design goal and theoretical background related to such a <em class="ltx_emph ltx_font_italic" id="S3.p1.1.3">reality fusion</em> method.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Design Goals</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our framework specifically targets a typical application scenario of a robot teleoperation system for remote industrial facility inspection, where human access to the facility is limited due to various hazards or operation constraints, however, the environment of the facility is static and is unlikely to undergo immediate large structural change. A typical example of such an environment is a particle accelerator tunnel such as the large hadron collider (LHC) at the European Organization for Nuclear Research (CERN), as articulated in the work of Szczurek et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib16" title="">16</a>]</cite>. Building an effective and robust immersive teleoperation system for mobile robot navigation for such an application needs to consider the following design goals:</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.1.1.1">G1</span></span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The mobile robot needs to be flexible and lightweight, capable of visiting areas such as narrow gaps between facility components.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.1.1.1">G2</span></span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">The operator needs to have high situation awareness and spatial orientation of the robot’s environment for task planning and navigation control.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.1.1.1">G3</span></span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">The operator needs to receive real-time visual feedback on the robot’s current status with minimum latency to perform timely intervention and robot control.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.1.1.1">G4</span></span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">The implementation of the VR application needs to enable intuitive control of the robot without introducing undesirable effects such as motion sickness.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">3D Gaussian Splatting</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To fulfill the requirement of <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">G2</span>, our framework adapts 3DGS models for photorealistic scene rendering in immersive VR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib10" title="">10</a>]</cite>.
According to Kerbl et al, given an initial set of sparse points and camera poses estimated from a set of 2D images of a scene, the opacity and color of a real-world 3D volume can be represented as a set of 3D Gaussians optimized through Gaussian density control and gradient descent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib10" title="">10</a>]</cite>. Each 3D Gaussian represents a part of the 3D volume with its position, rotation, scale, covariance matrix, as well as spherical harmonic coefficients which encode the view-dependent radiance field values for photorealistic rendering. The point blending method of 3D Gaussians (a.k.a splatting) can overlay a pixel by blending a summation of radiance contribution from each point to achieve state-of-the-art photorealistic view synthesis and rendering performance. As a result, a VR 3DGS system can rapidly duplicate and visualize the appearance of complex realities, enabling robot operators to be fully immersed in the remote environment.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Stereoscopic 3D Projection in World Space</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.3">Rendering a 3DGS model can only provide offline passive visual feedback of the environment. To fulfill the requirement of <span class="ltx_text ltx_font_bold" id="S3.SS3.p1.3.1">G3</span>, we integrate stereoscopic 3D projection of a depth camera to provide operators with real-time spatial feedback.
Given a stereo camera, depth information (<math alttext="d" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_d</annotation></semantics></math>) can be estimated based on the correspondence established from the two camera views. Given a homogeneous pixel vector with depth estimation <math alttext="P_{d}=[x_{d},y_{d},d,1]" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.4"><semantics id="S3.SS3.p1.2.m2.4a"><mrow id="S3.SS3.p1.2.m2.4.4" xref="S3.SS3.p1.2.m2.4.4.cmml"><msub id="S3.SS3.p1.2.m2.4.4.4" xref="S3.SS3.p1.2.m2.4.4.4.cmml"><mi id="S3.SS3.p1.2.m2.4.4.4.2" xref="S3.SS3.p1.2.m2.4.4.4.2.cmml">P</mi><mi id="S3.SS3.p1.2.m2.4.4.4.3" xref="S3.SS3.p1.2.m2.4.4.4.3.cmml">d</mi></msub><mo id="S3.SS3.p1.2.m2.4.4.3" xref="S3.SS3.p1.2.m2.4.4.3.cmml">=</mo><mrow id="S3.SS3.p1.2.m2.4.4.2.2" xref="S3.SS3.p1.2.m2.4.4.2.3.cmml"><mo id="S3.SS3.p1.2.m2.4.4.2.2.3" stretchy="false" xref="S3.SS3.p1.2.m2.4.4.2.3.cmml">[</mo><msub id="S3.SS3.p1.2.m2.3.3.1.1.1" xref="S3.SS3.p1.2.m2.3.3.1.1.1.cmml"><mi id="S3.SS3.p1.2.m2.3.3.1.1.1.2" xref="S3.SS3.p1.2.m2.3.3.1.1.1.2.cmml">x</mi><mi id="S3.SS3.p1.2.m2.3.3.1.1.1.3" xref="S3.SS3.p1.2.m2.3.3.1.1.1.3.cmml">d</mi></msub><mo id="S3.SS3.p1.2.m2.4.4.2.2.4" xref="S3.SS3.p1.2.m2.4.4.2.3.cmml">,</mo><msub id="S3.SS3.p1.2.m2.4.4.2.2.2" xref="S3.SS3.p1.2.m2.4.4.2.2.2.cmml"><mi id="S3.SS3.p1.2.m2.4.4.2.2.2.2" xref="S3.SS3.p1.2.m2.4.4.2.2.2.2.cmml">y</mi><mi id="S3.SS3.p1.2.m2.4.4.2.2.2.3" xref="S3.SS3.p1.2.m2.4.4.2.2.2.3.cmml">d</mi></msub><mo id="S3.SS3.p1.2.m2.4.4.2.2.5" xref="S3.SS3.p1.2.m2.4.4.2.3.cmml">,</mo><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">d</mi><mo id="S3.SS3.p1.2.m2.4.4.2.2.6" xref="S3.SS3.p1.2.m2.4.4.2.3.cmml">,</mo><mn id="S3.SS3.p1.2.m2.2.2" xref="S3.SS3.p1.2.m2.2.2.cmml">1</mn><mo id="S3.SS3.p1.2.m2.4.4.2.2.7" stretchy="false" xref="S3.SS3.p1.2.m2.4.4.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.4b"><apply id="S3.SS3.p1.2.m2.4.4.cmml" xref="S3.SS3.p1.2.m2.4.4"><eq id="S3.SS3.p1.2.m2.4.4.3.cmml" xref="S3.SS3.p1.2.m2.4.4.3"></eq><apply id="S3.SS3.p1.2.m2.4.4.4.cmml" xref="S3.SS3.p1.2.m2.4.4.4"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.4.4.4.1.cmml" xref="S3.SS3.p1.2.m2.4.4.4">subscript</csymbol><ci id="S3.SS3.p1.2.m2.4.4.4.2.cmml" xref="S3.SS3.p1.2.m2.4.4.4.2">𝑃</ci><ci id="S3.SS3.p1.2.m2.4.4.4.3.cmml" xref="S3.SS3.p1.2.m2.4.4.4.3">𝑑</ci></apply><list id="S3.SS3.p1.2.m2.4.4.2.3.cmml" xref="S3.SS3.p1.2.m2.4.4.2.2"><apply id="S3.SS3.p1.2.m2.3.3.1.1.1.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.3.3.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.3.3.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1.1.2">𝑥</ci><ci id="S3.SS3.p1.2.m2.3.3.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1.1.3">𝑑</ci></apply><apply id="S3.SS3.p1.2.m2.4.4.2.2.2.cmml" xref="S3.SS3.p1.2.m2.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.4.4.2.2.2.1.cmml" xref="S3.SS3.p1.2.m2.4.4.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.2.m2.4.4.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.4.4.2.2.2.2">𝑦</ci><ci id="S3.SS3.p1.2.m2.4.4.2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.4.4.2.2.2.3">𝑑</ci></apply><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑑</ci><cn id="S3.SS3.p1.2.m2.2.2.cmml" type="integer" xref="S3.SS3.p1.2.m2.2.2">1</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.4c">P_{d}=[x_{d},y_{d},d,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.4d">italic_P start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_d , 1 ]</annotation></semantics></math>, it’s 3D coordinate <math alttext="P_{C}=[x_{c},y_{c},z_{c},w]" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.4"><semantics id="S3.SS3.p1.3.m3.4a"><mrow id="S3.SS3.p1.3.m3.4.4" xref="S3.SS3.p1.3.m3.4.4.cmml"><msub id="S3.SS3.p1.3.m3.4.4.5" xref="S3.SS3.p1.3.m3.4.4.5.cmml"><mi id="S3.SS3.p1.3.m3.4.4.5.2" xref="S3.SS3.p1.3.m3.4.4.5.2.cmml">P</mi><mi id="S3.SS3.p1.3.m3.4.4.5.3" xref="S3.SS3.p1.3.m3.4.4.5.3.cmml">C</mi></msub><mo id="S3.SS3.p1.3.m3.4.4.4" xref="S3.SS3.p1.3.m3.4.4.4.cmml">=</mo><mrow id="S3.SS3.p1.3.m3.4.4.3.3" xref="S3.SS3.p1.3.m3.4.4.3.4.cmml"><mo id="S3.SS3.p1.3.m3.4.4.3.3.4" stretchy="false" xref="S3.SS3.p1.3.m3.4.4.3.4.cmml">[</mo><msub id="S3.SS3.p1.3.m3.2.2.1.1.1" xref="S3.SS3.p1.3.m3.2.2.1.1.1.cmml"><mi id="S3.SS3.p1.3.m3.2.2.1.1.1.2" xref="S3.SS3.p1.3.m3.2.2.1.1.1.2.cmml">x</mi><mi id="S3.SS3.p1.3.m3.2.2.1.1.1.3" xref="S3.SS3.p1.3.m3.2.2.1.1.1.3.cmml">c</mi></msub><mo id="S3.SS3.p1.3.m3.4.4.3.3.5" xref="S3.SS3.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.SS3.p1.3.m3.3.3.2.2.2" xref="S3.SS3.p1.3.m3.3.3.2.2.2.cmml"><mi id="S3.SS3.p1.3.m3.3.3.2.2.2.2" xref="S3.SS3.p1.3.m3.3.3.2.2.2.2.cmml">y</mi><mi id="S3.SS3.p1.3.m3.3.3.2.2.2.3" xref="S3.SS3.p1.3.m3.3.3.2.2.2.3.cmml">c</mi></msub><mo id="S3.SS3.p1.3.m3.4.4.3.3.6" xref="S3.SS3.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.SS3.p1.3.m3.4.4.3.3.3" xref="S3.SS3.p1.3.m3.4.4.3.3.3.cmml"><mi id="S3.SS3.p1.3.m3.4.4.3.3.3.2" xref="S3.SS3.p1.3.m3.4.4.3.3.3.2.cmml">z</mi><mi id="S3.SS3.p1.3.m3.4.4.3.3.3.3" xref="S3.SS3.p1.3.m3.4.4.3.3.3.3.cmml">c</mi></msub><mo id="S3.SS3.p1.3.m3.4.4.3.3.7" xref="S3.SS3.p1.3.m3.4.4.3.4.cmml">,</mo><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">w</mi><mo id="S3.SS3.p1.3.m3.4.4.3.3.8" stretchy="false" xref="S3.SS3.p1.3.m3.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.4b"><apply id="S3.SS3.p1.3.m3.4.4.cmml" xref="S3.SS3.p1.3.m3.4.4"><eq id="S3.SS3.p1.3.m3.4.4.4.cmml" xref="S3.SS3.p1.3.m3.4.4.4"></eq><apply id="S3.SS3.p1.3.m3.4.4.5.cmml" xref="S3.SS3.p1.3.m3.4.4.5"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.4.4.5.1.cmml" xref="S3.SS3.p1.3.m3.4.4.5">subscript</csymbol><ci id="S3.SS3.p1.3.m3.4.4.5.2.cmml" xref="S3.SS3.p1.3.m3.4.4.5.2">𝑃</ci><ci id="S3.SS3.p1.3.m3.4.4.5.3.cmml" xref="S3.SS3.p1.3.m3.4.4.5.3">𝐶</ci></apply><list id="S3.SS3.p1.3.m3.4.4.3.4.cmml" xref="S3.SS3.p1.3.m3.4.4.3.3"><apply id="S3.SS3.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS3.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS3.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS3.p1.3.m3.2.2.1.1.1.2">𝑥</ci><ci id="S3.SS3.p1.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS3.p1.3.m3.2.2.1.1.1.3">𝑐</ci></apply><apply id="S3.SS3.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS3.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.3.3.2.2.2.1.cmml" xref="S3.SS3.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.3.m3.3.3.2.2.2.2.cmml" xref="S3.SS3.p1.3.m3.3.3.2.2.2.2">𝑦</ci><ci id="S3.SS3.p1.3.m3.3.3.2.2.2.3.cmml" xref="S3.SS3.p1.3.m3.3.3.2.2.2.3">𝑐</ci></apply><apply id="S3.SS3.p1.3.m3.4.4.3.3.3.cmml" xref="S3.SS3.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.4.4.3.3.3.1.cmml" xref="S3.SS3.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S3.SS3.p1.3.m3.4.4.3.3.3.2.cmml" xref="S3.SS3.p1.3.m3.4.4.3.3.3.2">𝑧</ci><ci id="S3.SS3.p1.3.m3.4.4.3.3.3.3.cmml" xref="S3.SS3.p1.3.m3.4.4.3.3.3.3">𝑐</ci></apply><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑤</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.4c">P_{C}=[x_{c},y_{c},z_{c},w]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.4d">italic_P start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , italic_w ]</annotation></semantics></math> can be calculated using stereoscopic 3D projection transformation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib5" title="">5</a>]</cite>:</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}x_{c}\\
y_{c}\\
z_{c}\\
w\\
\end{bmatrix}=\begin{bmatrix}a&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;0\\
0&amp;0&amp;0&amp;f\\
0&amp;0&amp;-\frac{1}{b}&amp;0\\
\end{bmatrix}\begin{bmatrix}x_{d}\\
y_{d}\\
d\\
1\\
\end{bmatrix}" class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.4" xref="S3.E1.m1.3.4.cmml"><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.2.1.cmml">[</mo><mtable displaystyle="true" id="S3.E1.m1.1.1.1.1" rowspacing="0pt" xref="S3.E1.m1.1.1.1.1.cmml"><mtr id="S3.E1.m1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1b" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">c</mi></msub></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1c" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1d" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.2.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1.2" xref="S3.E1.m1.1.1.1.1.2.1.1.2.cmml">y</mi><mi id="S3.E1.m1.1.1.1.1.2.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.1.3.cmml">c</mi></msub></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1e" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1f" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.3.1.1" xref="S3.E1.m1.1.1.1.1.3.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3.1.1.2" xref="S3.E1.m1.1.1.1.1.3.1.1.2.cmml">z</mi><mi id="S3.E1.m1.1.1.1.1.3.1.1.3" xref="S3.E1.m1.1.1.1.1.3.1.1.3.cmml">c</mi></msub></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1g" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1h" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.4.1.1" xref="S3.E1.m1.1.1.1.1.4.1.1.cmml">w</mi></mtd></mtr></mtable><mo id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.2.1.cmml">]</mo></mrow><mo id="S3.E1.m1.3.4.1" xref="S3.E1.m1.3.4.1.cmml">=</mo><mrow id="S3.E1.m1.3.4.2" xref="S3.E1.m1.3.4.2.cmml"><mrow id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.2.cmml"><mo id="S3.E1.m1.2.2.3.1" xref="S3.E1.m1.2.2.2.1.cmml">[</mo><mtable columnspacing="5pt" displaystyle="true" id="S3.E1.m1.2.2.1.1" rowspacing="0pt" xref="S3.E1.m1.2.2.1.1.cmml"><mtr id="S3.E1.m1.2.2.1.1a" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1b" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml">a</mi></mtd><mtd id="S3.E1.m1.2.2.1.1c" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.1.2.1" xref="S3.E1.m1.2.2.1.1.1.2.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.2.2.1.1d" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.1.3.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.2.2.1.1e" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.1.4.1" xref="S3.E1.m1.2.2.1.1.1.4.1.cmml">0</mn></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1f" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1g" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.2.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.2.2.1.1h" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.2.2.1" xref="S3.E1.m1.2.2.1.1.2.2.1.cmml">1</mn></mtd><mtd id="S3.E1.m1.2.2.1.1i" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.2.3.1" xref="S3.E1.m1.2.2.1.1.2.3.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.2.2.1.1j" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.2.4.1" xref="S3.E1.m1.2.2.1.1.2.4.1.cmml">0</mn></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1k" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1l" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.3.1.1" xref="S3.E1.m1.2.2.1.1.3.1.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.2.2.1.1m" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.3.2.1" xref="S3.E1.m1.2.2.1.1.3.2.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.2.2.1.1n" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.3.3.1" xref="S3.E1.m1.2.2.1.1.3.3.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.2.2.1.1o" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.3.4.1" xref="S3.E1.m1.2.2.1.1.3.4.1.cmml">f</mi></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1p" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1q" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.4.1.1" xref="S3.E1.m1.2.2.1.1.4.1.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.2.2.1.1r" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.4.2.1" xref="S3.E1.m1.2.2.1.1.4.2.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.2.2.1.1s" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.4.3.1" xref="S3.E1.m1.2.2.1.1.4.3.1.cmml"><mo id="S3.E1.m1.2.2.1.1.4.3.1a" xref="S3.E1.m1.2.2.1.1.4.3.1.cmml">−</mo><mstyle displaystyle="false" id="S3.E1.m1.2.2.1.1.4.3.1.2" xref="S3.E1.m1.2.2.1.1.4.3.1.2.cmml"><mfrac id="S3.E1.m1.2.2.1.1.4.3.1.2a" xref="S3.E1.m1.2.2.1.1.4.3.1.2.cmml"><mn id="S3.E1.m1.2.2.1.1.4.3.1.2.2" xref="S3.E1.m1.2.2.1.1.4.3.1.2.2.cmml">1</mn><mi id="S3.E1.m1.2.2.1.1.4.3.1.2.3" xref="S3.E1.m1.2.2.1.1.4.3.1.2.3.cmml">b</mi></mfrac></mstyle></mrow></mtd><mtd id="S3.E1.m1.2.2.1.1t" xref="S3.E1.m1.2.2.1.1.cmml"><mn id="S3.E1.m1.2.2.1.1.4.4.1" xref="S3.E1.m1.2.2.1.1.4.4.1.cmml">0</mn></mtd></mtr></mtable><mo id="S3.E1.m1.2.2.3.2" xref="S3.E1.m1.2.2.2.1.cmml">]</mo></mrow><mo id="S3.E1.m1.3.4.2.1" xref="S3.E1.m1.3.4.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.2.cmml"><mo id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.2.1.cmml">[</mo><mtable displaystyle="true" id="S3.E1.m1.3.3.1.1" rowspacing="0pt" xref="S3.E1.m1.3.3.1.1.cmml"><mtr id="S3.E1.m1.3.3.1.1a" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1b" xref="S3.E1.m1.3.3.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.3.cmml">d</mi></msub></mtd></mtr><mtr id="S3.E1.m1.3.3.1.1c" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1d" xref="S3.E1.m1.3.3.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.2.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.2.1.1.2" xref="S3.E1.m1.3.3.1.1.2.1.1.2.cmml">y</mi><mi id="S3.E1.m1.3.3.1.1.2.1.1.3" xref="S3.E1.m1.3.3.1.1.2.1.1.3.cmml">d</mi></msub></mtd></mtr><mtr id="S3.E1.m1.3.3.1.1e" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1f" xref="S3.E1.m1.3.3.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.3.1.1" xref="S3.E1.m1.3.3.1.1.3.1.1.cmml">d</mi></mtd></mtr><mtr id="S3.E1.m1.3.3.1.1g" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1h" xref="S3.E1.m1.3.3.1.1.cmml"><mn id="S3.E1.m1.3.3.1.1.4.1.1" xref="S3.E1.m1.3.3.1.1.4.1.1.cmml">1</mn></mtd></mtr></mtable><mo id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.4.cmml" xref="S3.E1.m1.3.4"><eq id="S3.E1.m1.3.4.1.cmml" xref="S3.E1.m1.3.4.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="latexml" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><matrixrow id="S3.E1.m1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">𝑐</ci></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.2.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2">𝑦</ci><ci id="S3.E1.m1.1.1.1.1.2.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3">𝑐</ci></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1c.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.3.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.2">𝑧</ci><ci id="S3.E1.m1.1.1.1.1.3.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.3">𝑐</ci></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1d.cmml" xref="S3.E1.m1.1.1.1.1"><ci id="S3.E1.m1.1.1.1.1.4.1.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1">𝑤</ci></matrixrow></matrix></apply><apply id="S3.E1.m1.3.4.2.cmml" xref="S3.E1.m1.3.4.2"><times id="S3.E1.m1.3.4.2.1.cmml" xref="S3.E1.m1.3.4.2.1"></times><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.3"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.3.1">matrix</csymbol><matrix id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><matrixrow id="S3.E1.m1.2.2.1.1a.cmml" xref="S3.E1.m1.2.2.1.1"><ci id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1">𝑎</ci><cn id="S3.E1.m1.2.2.1.1.1.2.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.1.2.1">0</cn><cn id="S3.E1.m1.2.2.1.1.1.3.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.1.3.1">0</cn><cn id="S3.E1.m1.2.2.1.1.1.4.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.1.4.1">0</cn></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1b.cmml" xref="S3.E1.m1.2.2.1.1"><cn id="S3.E1.m1.2.2.1.1.2.1.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.2.1.1">0</cn><cn id="S3.E1.m1.2.2.1.1.2.2.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.2.2.1">1</cn><cn id="S3.E1.m1.2.2.1.1.2.3.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.2.3.1">0</cn><cn id="S3.E1.m1.2.2.1.1.2.4.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.2.4.1">0</cn></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1c.cmml" xref="S3.E1.m1.2.2.1.1"><cn id="S3.E1.m1.2.2.1.1.3.1.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.3.1.1">0</cn><cn id="S3.E1.m1.2.2.1.1.3.2.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.3.2.1">0</cn><cn id="S3.E1.m1.2.2.1.1.3.3.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.3.3.1">0</cn><ci id="S3.E1.m1.2.2.1.1.3.4.1.cmml" xref="S3.E1.m1.2.2.1.1.3.4.1">𝑓</ci></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1d.cmml" xref="S3.E1.m1.2.2.1.1"><cn id="S3.E1.m1.2.2.1.1.4.1.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.4.1.1">0</cn><cn id="S3.E1.m1.2.2.1.1.4.2.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.4.2.1">0</cn><apply id="S3.E1.m1.2.2.1.1.4.3.1.cmml" xref="S3.E1.m1.2.2.1.1.4.3.1"><minus id="S3.E1.m1.2.2.1.1.4.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.4.3.1"></minus><apply id="S3.E1.m1.2.2.1.1.4.3.1.2.cmml" xref="S3.E1.m1.2.2.1.1.4.3.1.2"><divide id="S3.E1.m1.2.2.1.1.4.3.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.4.3.1.2"></divide><cn id="S3.E1.m1.2.2.1.1.4.3.1.2.2.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.4.3.1.2.2">1</cn><ci id="S3.E1.m1.2.2.1.1.4.3.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.4.3.1.2.3">𝑏</ci></apply></apply><cn id="S3.E1.m1.2.2.1.1.4.4.1.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.4.4.1">0</cn></matrixrow></matrix></apply><apply id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.3"><csymbol cd="latexml" id="S3.E1.m1.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.1">matrix</csymbol><matrix id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1"><matrixrow id="S3.E1.m1.3.3.1.1a.cmml" xref="S3.E1.m1.3.3.1.1"><apply id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3">𝑑</ci></apply></matrixrow><matrixrow id="S3.E1.m1.3.3.1.1b.cmml" xref="S3.E1.m1.3.3.1.1"><apply id="S3.E1.m1.3.3.1.1.2.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.2">𝑦</ci><ci id="S3.E1.m1.3.3.1.1.2.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.3">𝑑</ci></apply></matrixrow><matrixrow id="S3.E1.m1.3.3.1.1c.cmml" xref="S3.E1.m1.3.3.1.1"><ci id="S3.E1.m1.3.3.1.1.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1.3.1.1">𝑑</ci></matrixrow><matrixrow id="S3.E1.m1.3.3.1.1d.cmml" xref="S3.E1.m1.3.3.1.1"><cn id="S3.E1.m1.3.3.1.1.4.1.1.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.4.1.1">1</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\begin{bmatrix}x_{c}\\
y_{c}\\
z_{c}\\
w\\
\end{bmatrix}=\begin{bmatrix}a&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;0\\
0&amp;0&amp;0&amp;f\\
0&amp;0&amp;-\frac{1}{b}&amp;0\\
\end{bmatrix}\begin{bmatrix}x_{d}\\
y_{d}\\
d\\
1\\
\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">[ start_ARG start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_w end_CELL end_ROW end_ARG ] = [ start_ARG start_ROW start_CELL italic_a end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL italic_f end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL - divide start_ARG 1 end_ARG start_ARG italic_b end_ARG end_CELL start_CELL 0 end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_d end_CELL end_ROW start_ROW start_CELL 1 end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.5">Here, <math alttext="b" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">b</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_b</annotation></semantics></math> is the stereo camera baseline, <math alttext="a" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_a</annotation></semantics></math> is the camera aspect ratio, and <math alttext="f" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.1"><semantics id="S3.SS3.p3.3.m3.1a"><mi id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.1d">italic_f</annotation></semantics></math> is the camera focal length, all of which could be obtained through camera calibration. Then, <math alttext="P_{C}" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m4.1"><semantics id="S3.SS3.p3.4.m4.1a"><msub id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.2" xref="S3.SS3.p3.4.m4.1.1.2.cmml">P</mi><mi id="S3.SS3.p3.4.m4.1.1.3" xref="S3.SS3.p3.4.m4.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2">𝑃</ci><ci id="S3.SS3.p3.4.m4.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">P_{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m4.1d">italic_P start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT</annotation></semantics></math> can be converted to homogeneous 3D coordinate <math alttext="P_{h}" class="ltx_Math" display="inline" id="S3.SS3.p3.5.m5.1"><semantics id="S3.SS3.p3.5.m5.1a"><msub id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml"><mi id="S3.SS3.p3.5.m5.1.1.2" xref="S3.SS3.p3.5.m5.1.1.2.cmml">P</mi><mi id="S3.SS3.p3.5.m5.1.1.3" xref="S3.SS3.p3.5.m5.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><apply id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m5.1.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p3.5.m5.1.1.2.cmml" xref="S3.SS3.p3.5.m5.1.1.2">𝑃</ci><ci id="S3.SS3.p3.5.m5.1.1.3.cmml" xref="S3.SS3.p3.5.m5.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">P_{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.5.m5.1d">italic_P start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> through perspective division.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.4">To calculate the point cloud’s 3D coordinate in world space <math alttext="P_{W}=[x,y,z,1]" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.4"><semantics id="S3.SS3.p4.1.m1.4a"><mrow id="S3.SS3.p4.1.m1.4.5" xref="S3.SS3.p4.1.m1.4.5.cmml"><msub id="S3.SS3.p4.1.m1.4.5.2" xref="S3.SS3.p4.1.m1.4.5.2.cmml"><mi id="S3.SS3.p4.1.m1.4.5.2.2" xref="S3.SS3.p4.1.m1.4.5.2.2.cmml">P</mi><mi id="S3.SS3.p4.1.m1.4.5.2.3" xref="S3.SS3.p4.1.m1.4.5.2.3.cmml">W</mi></msub><mo id="S3.SS3.p4.1.m1.4.5.1" xref="S3.SS3.p4.1.m1.4.5.1.cmml">=</mo><mrow id="S3.SS3.p4.1.m1.4.5.3.2" xref="S3.SS3.p4.1.m1.4.5.3.1.cmml"><mo id="S3.SS3.p4.1.m1.4.5.3.2.1" stretchy="false" xref="S3.SS3.p4.1.m1.4.5.3.1.cmml">[</mo><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">x</mi><mo id="S3.SS3.p4.1.m1.4.5.3.2.2" xref="S3.SS3.p4.1.m1.4.5.3.1.cmml">,</mo><mi id="S3.SS3.p4.1.m1.2.2" xref="S3.SS3.p4.1.m1.2.2.cmml">y</mi><mo id="S3.SS3.p4.1.m1.4.5.3.2.3" xref="S3.SS3.p4.1.m1.4.5.3.1.cmml">,</mo><mi id="S3.SS3.p4.1.m1.3.3" xref="S3.SS3.p4.1.m1.3.3.cmml">z</mi><mo id="S3.SS3.p4.1.m1.4.5.3.2.4" xref="S3.SS3.p4.1.m1.4.5.3.1.cmml">,</mo><mn id="S3.SS3.p4.1.m1.4.4" xref="S3.SS3.p4.1.m1.4.4.cmml">1</mn><mo id="S3.SS3.p4.1.m1.4.5.3.2.5" stretchy="false" xref="S3.SS3.p4.1.m1.4.5.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.4b"><apply id="S3.SS3.p4.1.m1.4.5.cmml" xref="S3.SS3.p4.1.m1.4.5"><eq id="S3.SS3.p4.1.m1.4.5.1.cmml" xref="S3.SS3.p4.1.m1.4.5.1"></eq><apply id="S3.SS3.p4.1.m1.4.5.2.cmml" xref="S3.SS3.p4.1.m1.4.5.2"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.4.5.2.1.cmml" xref="S3.SS3.p4.1.m1.4.5.2">subscript</csymbol><ci id="S3.SS3.p4.1.m1.4.5.2.2.cmml" xref="S3.SS3.p4.1.m1.4.5.2.2">𝑃</ci><ci id="S3.SS3.p4.1.m1.4.5.2.3.cmml" xref="S3.SS3.p4.1.m1.4.5.2.3">𝑊</ci></apply><list id="S3.SS3.p4.1.m1.4.5.3.1.cmml" xref="S3.SS3.p4.1.m1.4.5.3.2"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">𝑥</ci><ci id="S3.SS3.p4.1.m1.2.2.cmml" xref="S3.SS3.p4.1.m1.2.2">𝑦</ci><ci id="S3.SS3.p4.1.m1.3.3.cmml" xref="S3.SS3.p4.1.m1.3.3">𝑧</ci><cn id="S3.SS3.p4.1.m1.4.4.cmml" type="integer" xref="S3.SS3.p4.1.m1.4.4">1</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.4c">P_{W}=[x,y,z,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.4d">italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT = [ italic_x , italic_y , italic_z , 1 ]</annotation></semantics></math>, we transform <math alttext="P_{h}" class="ltx_Math" display="inline" id="S3.SS3.p4.2.m2.1"><semantics id="S3.SS3.p4.2.m2.1a"><msub id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mi id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml">P</mi><mi id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2">𝑃</ci><ci id="S3.SS3.p4.2.m2.1.1.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">P_{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.2.m2.1d">italic_P start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> through a transformation matrix obtained using the view projection matrix of the virtual camera <math alttext="M_{VP}" class="ltx_Math" display="inline" id="S3.SS3.p4.3.m3.1"><semantics id="S3.SS3.p4.3.m3.1a"><msub id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml"><mi id="S3.SS3.p4.3.m3.1.1.2" xref="S3.SS3.p4.3.m3.1.1.2.cmml">M</mi><mrow id="S3.SS3.p4.3.m3.1.1.3" xref="S3.SS3.p4.3.m3.1.1.3.cmml"><mi id="S3.SS3.p4.3.m3.1.1.3.2" xref="S3.SS3.p4.3.m3.1.1.3.2.cmml">V</mi><mo id="S3.SS3.p4.3.m3.1.1.3.1" xref="S3.SS3.p4.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p4.3.m3.1.1.3.3" xref="S3.SS3.p4.3.m3.1.1.3.3.cmml">P</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><apply id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p4.3.m3.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2">𝑀</ci><apply id="S3.SS3.p4.3.m3.1.1.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3"><times id="S3.SS3.p4.3.m3.1.1.3.1.cmml" xref="S3.SS3.p4.3.m3.1.1.3.1"></times><ci id="S3.SS3.p4.3.m3.1.1.3.2.cmml" xref="S3.SS3.p4.3.m3.1.1.3.2">𝑉</ci><ci id="S3.SS3.p4.3.m3.1.1.3.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">M_{VP}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.3.m3.1d">italic_M start_POSTSUBSCRIPT italic_V italic_P end_POSTSUBSCRIPT</annotation></semantics></math> and transformation matrix of the stereo camera <math alttext="M_{camera}" class="ltx_Math" display="inline" id="S3.SS3.p4.4.m4.1"><semantics id="S3.SS3.p4.4.m4.1a"><msub id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml"><mi id="S3.SS3.p4.4.m4.1.1.2" xref="S3.SS3.p4.4.m4.1.1.2.cmml">M</mi><mrow id="S3.SS3.p4.4.m4.1.1.3" xref="S3.SS3.p4.4.m4.1.1.3.cmml"><mi id="S3.SS3.p4.4.m4.1.1.3.2" xref="S3.SS3.p4.4.m4.1.1.3.2.cmml">c</mi><mo id="S3.SS3.p4.4.m4.1.1.3.1" xref="S3.SS3.p4.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p4.4.m4.1.1.3.3" xref="S3.SS3.p4.4.m4.1.1.3.3.cmml">a</mi><mo id="S3.SS3.p4.4.m4.1.1.3.1a" xref="S3.SS3.p4.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p4.4.m4.1.1.3.4" xref="S3.SS3.p4.4.m4.1.1.3.4.cmml">m</mi><mo id="S3.SS3.p4.4.m4.1.1.3.1b" xref="S3.SS3.p4.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p4.4.m4.1.1.3.5" xref="S3.SS3.p4.4.m4.1.1.3.5.cmml">e</mi><mo id="S3.SS3.p4.4.m4.1.1.3.1c" xref="S3.SS3.p4.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p4.4.m4.1.1.3.6" xref="S3.SS3.p4.4.m4.1.1.3.6.cmml">r</mi><mo id="S3.SS3.p4.4.m4.1.1.3.1d" xref="S3.SS3.p4.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p4.4.m4.1.1.3.7" xref="S3.SS3.p4.4.m4.1.1.3.7.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><apply id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.4.m4.1.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p4.4.m4.1.1.2.cmml" xref="S3.SS3.p4.4.m4.1.1.2">𝑀</ci><apply id="S3.SS3.p4.4.m4.1.1.3.cmml" xref="S3.SS3.p4.4.m4.1.1.3"><times id="S3.SS3.p4.4.m4.1.1.3.1.cmml" xref="S3.SS3.p4.4.m4.1.1.3.1"></times><ci id="S3.SS3.p4.4.m4.1.1.3.2.cmml" xref="S3.SS3.p4.4.m4.1.1.3.2">𝑐</ci><ci id="S3.SS3.p4.4.m4.1.1.3.3.cmml" xref="S3.SS3.p4.4.m4.1.1.3.3">𝑎</ci><ci id="S3.SS3.p4.4.m4.1.1.3.4.cmml" xref="S3.SS3.p4.4.m4.1.1.3.4">𝑚</ci><ci id="S3.SS3.p4.4.m4.1.1.3.5.cmml" xref="S3.SS3.p4.4.m4.1.1.3.5">𝑒</ci><ci id="S3.SS3.p4.4.m4.1.1.3.6.cmml" xref="S3.SS3.p4.4.m4.1.1.3.6">𝑟</ci><ci id="S3.SS3.p4.4.m4.1.1.3.7.cmml" xref="S3.SS3.p4.4.m4.1.1.3.7">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">M_{camera}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.4.m4.1d">italic_M start_POSTSUBSCRIPT italic_c italic_a italic_m italic_e italic_r italic_a end_POSTSUBSCRIPT</annotation></semantics></math> which describes the tracked camera’s position and orientation in global world space:</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P_{W}=M_{VP}\times T_{offset}\times M_{camera}\times P_{h}." class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.cmml">P</mi><mi id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">W</mi></msub><mo id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><msub id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.cmml">M</mi><mrow id="S3.E2.m1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.3.2" xref="S3.E2.m1.1.1.1.1.3.2.3.2.cmml">V</mi><mo id="S3.E2.m1.1.1.1.1.3.2.3.1" xref="S3.E2.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.2.3.3" xref="S3.E2.m1.1.1.1.1.3.2.3.3.cmml">P</mi></mrow></msub><mo id="S3.E2.m1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.1.1.1.1.3.1.cmml">×</mo><msub id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">T</mi><mrow id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.3.2.cmml">o</mi><mo id="S3.E2.m1.1.1.1.1.3.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.3.cmml">f</mi><mo id="S3.E2.m1.1.1.1.1.3.3.3.1a" xref="S3.E2.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.3.3.4" xref="S3.E2.m1.1.1.1.1.3.3.3.4.cmml">f</mi><mo id="S3.E2.m1.1.1.1.1.3.3.3.1b" xref="S3.E2.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.3.3.5" xref="S3.E2.m1.1.1.1.1.3.3.3.5.cmml">s</mi><mo id="S3.E2.m1.1.1.1.1.3.3.3.1c" xref="S3.E2.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.3.3.6" xref="S3.E2.m1.1.1.1.1.3.3.3.6.cmml">e</mi><mo id="S3.E2.m1.1.1.1.1.3.3.3.1d" xref="S3.E2.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.3.3.7" xref="S3.E2.m1.1.1.1.1.3.3.3.7.cmml">t</mi></mrow></msub><mo id="S3.E2.m1.1.1.1.1.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.1.1.1.1.3.1.cmml">×</mo><msub id="S3.E2.m1.1.1.1.1.3.4" xref="S3.E2.m1.1.1.1.1.3.4.cmml"><mi id="S3.E2.m1.1.1.1.1.3.4.2" xref="S3.E2.m1.1.1.1.1.3.4.2.cmml">M</mi><mrow id="S3.E2.m1.1.1.1.1.3.4.3" xref="S3.E2.m1.1.1.1.1.3.4.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.4.3.2" xref="S3.E2.m1.1.1.1.1.3.4.3.2.cmml">c</mi><mo id="S3.E2.m1.1.1.1.1.3.4.3.1" xref="S3.E2.m1.1.1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.4.3.3" xref="S3.E2.m1.1.1.1.1.3.4.3.3.cmml">a</mi><mo id="S3.E2.m1.1.1.1.1.3.4.3.1a" xref="S3.E2.m1.1.1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.4.3.4" xref="S3.E2.m1.1.1.1.1.3.4.3.4.cmml">m</mi><mo id="S3.E2.m1.1.1.1.1.3.4.3.1b" xref="S3.E2.m1.1.1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.4.3.5" xref="S3.E2.m1.1.1.1.1.3.4.3.5.cmml">e</mi><mo id="S3.E2.m1.1.1.1.1.3.4.3.1c" xref="S3.E2.m1.1.1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.4.3.6" xref="S3.E2.m1.1.1.1.1.3.4.3.6.cmml">r</mi><mo id="S3.E2.m1.1.1.1.1.3.4.3.1d" xref="S3.E2.m1.1.1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.3.4.3.7" xref="S3.E2.m1.1.1.1.1.3.4.3.7.cmml">a</mi></mrow></msub><mo id="S3.E2.m1.1.1.1.1.3.1b" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.1.1.1.1.3.1.cmml">×</mo><msub id="S3.E2.m1.1.1.1.1.3.5" xref="S3.E2.m1.1.1.1.1.3.5.cmml"><mi id="S3.E2.m1.1.1.1.1.3.5.2" xref="S3.E2.m1.1.1.1.1.3.5.2.cmml">P</mi><mi id="S3.E2.m1.1.1.1.1.3.5.3" xref="S3.E2.m1.1.1.1.1.3.5.3.cmml">h</mi></msub></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" lspace="0em" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"></eq><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2">𝑃</ci><ci id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3">𝑊</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1"></times><apply id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2">𝑀</ci><apply id="S3.E2.m1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3"><times id="S3.E2.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.1"></times><ci id="S3.E2.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.2">𝑉</ci><ci id="S3.E2.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.3">𝑃</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">𝑇</ci><apply id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3"><times id="S3.E2.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.1"></times><ci id="S3.E2.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.2">𝑜</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.3">𝑓</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.4.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.4">𝑓</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.5.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.5">𝑠</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.6.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.6">𝑒</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.7.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.7">𝑡</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.4.1.cmml" xref="S3.E2.m1.1.1.1.1.3.4">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.4.2.cmml" xref="S3.E2.m1.1.1.1.1.3.4.2">𝑀</ci><apply id="S3.E2.m1.1.1.1.1.3.4.3.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3"><times id="S3.E2.m1.1.1.1.1.3.4.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3.1"></times><ci id="S3.E2.m1.1.1.1.1.3.4.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.1.3.4.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3.3">𝑎</ci><ci id="S3.E2.m1.1.1.1.1.3.4.3.4.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3.4">𝑚</ci><ci id="S3.E2.m1.1.1.1.1.3.4.3.5.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3.5">𝑒</ci><ci id="S3.E2.m1.1.1.1.1.3.4.3.6.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3.6">𝑟</ci><ci id="S3.E2.m1.1.1.1.1.3.4.3.7.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3.7">𝑎</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.5.cmml" xref="S3.E2.m1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.5.1.cmml" xref="S3.E2.m1.1.1.1.1.3.5">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.5.2.cmml" xref="S3.E2.m1.1.1.1.1.3.5.2">𝑃</ci><ci id="S3.E2.m1.1.1.1.1.3.5.3.cmml" xref="S3.E2.m1.1.1.1.1.3.5.3">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">P_{W}=M_{VP}\times T_{offset}\times M_{camera}\times P_{h}.</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT = italic_M start_POSTSUBSCRIPT italic_V italic_P end_POSTSUBSCRIPT × italic_T start_POSTSUBSCRIPT italic_o italic_f italic_f italic_s italic_e italic_t end_POSTSUBSCRIPT × italic_M start_POSTSUBSCRIPT italic_c italic_a italic_m italic_e italic_r italic_a end_POSTSUBSCRIPT × italic_P start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">As the robot’s tracked position has a different center of origin than the actual position of the camera, an additional translation matrix <math alttext="T_{offset}" class="ltx_Math" display="inline" id="S3.SS3.p6.1.m1.1"><semantics id="S3.SS3.p6.1.m1.1a"><msub id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mi id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2.cmml">T</mi><mrow id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3.cmml"><mi id="S3.SS3.p6.1.m1.1.1.3.2" xref="S3.SS3.p6.1.m1.1.1.3.2.cmml">o</mi><mo id="S3.SS3.p6.1.m1.1.1.3.1" xref="S3.SS3.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p6.1.m1.1.1.3.3" xref="S3.SS3.p6.1.m1.1.1.3.3.cmml">f</mi><mo id="S3.SS3.p6.1.m1.1.1.3.1a" xref="S3.SS3.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p6.1.m1.1.1.3.4" xref="S3.SS3.p6.1.m1.1.1.3.4.cmml">f</mi><mo id="S3.SS3.p6.1.m1.1.1.3.1b" xref="S3.SS3.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p6.1.m1.1.1.3.5" xref="S3.SS3.p6.1.m1.1.1.3.5.cmml">s</mi><mo id="S3.SS3.p6.1.m1.1.1.3.1c" xref="S3.SS3.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p6.1.m1.1.1.3.6" xref="S3.SS3.p6.1.m1.1.1.3.6.cmml">e</mi><mo id="S3.SS3.p6.1.m1.1.1.3.1d" xref="S3.SS3.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p6.1.m1.1.1.3.7" xref="S3.SS3.p6.1.m1.1.1.3.7.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.1.m1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p6.1.m1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.2">𝑇</ci><apply id="S3.SS3.p6.1.m1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3"><times id="S3.SS3.p6.1.m1.1.1.3.1.cmml" xref="S3.SS3.p6.1.m1.1.1.3.1"></times><ci id="S3.SS3.p6.1.m1.1.1.3.2.cmml" xref="S3.SS3.p6.1.m1.1.1.3.2">𝑜</ci><ci id="S3.SS3.p6.1.m1.1.1.3.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3.3">𝑓</ci><ci id="S3.SS3.p6.1.m1.1.1.3.4.cmml" xref="S3.SS3.p6.1.m1.1.1.3.4">𝑓</ci><ci id="S3.SS3.p6.1.m1.1.1.3.5.cmml" xref="S3.SS3.p6.1.m1.1.1.3.5">𝑠</ci><ci id="S3.SS3.p6.1.m1.1.1.3.6.cmml" xref="S3.SS3.p6.1.m1.1.1.3.6">𝑒</ci><ci id="S3.SS3.p6.1.m1.1.1.3.7.cmml" xref="S3.SS3.p6.1.m1.1.1.3.7">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">T_{offset}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p6.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_o italic_f italic_f italic_s italic_e italic_t end_POSTSUBSCRIPT</annotation></semantics></math> obtained through manual calibration is applied to compensate for the translation offset.</p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1">As <em class="ltx_emph ltx_font_italic" id="S3.SS3.p7.1.1">reality fusion</em> only depends on a single depth camera for receiving real-time visual feedback, it reduces the overall required payload on the robot (<span class="ltx_text ltx_font_bold" id="S3.SS3.p7.1.2">G1</span>). In addition, the fusion of both a 3DGS model and real-time point clouds creates a coherent and natural visual appearance with low streaming and rendering latency (<span class="ltx_text ltx_font_bold" id="S3.SS3.p7.1.3">G4</span>). Moreover, both types of volumetric data enable 6DoF changes of perspective and robust exocentric robot control, making it easy for users to adjust their viewpoints and increasing the flexibility of robot planning and navigation control tasks (<span class="ltx_text ltx_font_bold" id="S3.SS3.p7.1.4">G4</span>).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">FRAMEWORK IMPLEMENTATION</span>
</h2>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="207" id="S4.F2.g1" src="extracted/5770358/system_overview.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">System overview of our immersive teleoperation framework which includes: 1. the operator equipped with a VR-HMD and sends command to the remote robot via VR controller inputs; 2. a Unity application which manages robot control logic, communicates with remote ROS endpoint, and perform data fusion and graphics rendering; 3. a custom-built telepresence mobile robot equipped with a SBC and a RGBD sensor. </span></figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.F2" title="Figure 2 ‣ IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag">2</span></a>, our framework consists of three main components: (i) the robot operator equipped with a VR HMD and VR controllers for interaction with the spatial UI; (ii) a Unity application that handles the robot control logic, communicates with remote ROS endpoint, and perform high-performance graphics rendering through native CUDA and C++ plugins; and (iii) a telepresence robot with an onboard visual sensor providing real-time feedback to the remote operator. This section presents relevant details of our framework and implementations.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">The Telepresence Robot</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.2.1">Overview</span> We designed and developed a compact, lightweight, and modulized telepresence mobile robot that can be easily reassembled and replicated from commercially available hardwares. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.F2" title="Figure 2 ‣ IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag">2</span></a>, the robot is modified from the open-source Turtlebot 3 burger robot platform. Our custom-made robot has three core hardware components: an OpenCR board for low-level control of the robot’s motion, a single ZED Mini stereo camera as the spatial vision sensor, and a single-board computer (SBC) as the robot’s central computing unit. As with the original Turtlebot burger, the robot has two differential wheel drives with a maximum linear speed of <math alttext="0.22m/s" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mrow id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2.2" xref="S4.SS1.p1.1.m1.1.1.2.2.cmml">0.22</mn><mo id="S4.SS1.p1.1.m1.1.1.2.1" xref="S4.SS1.p1.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.SS1.p1.1.m1.1.1.2.3" xref="S4.SS1.p1.1.m1.1.1.2.3.cmml">m</mi></mrow><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">/</mo><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><divide id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></divide><apply id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2"><times id="S4.SS1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS1.p1.1.m1.1.1.2.1"></times><cn id="S4.SS1.p1.1.m1.1.1.2.2.cmml" type="float" xref="S4.SS1.p1.1.m1.1.1.2.2">0.22</cn><ci id="S4.SS1.p1.1.m1.1.1.2.3.cmml" xref="S4.SS1.p1.1.m1.1.1.2.3">𝑚</ci></apply><ci id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">0.22m/s</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">0.22 italic_m / italic_s</annotation></semantics></math> and a maximum angular speed of <math alttext="2.84rad/s" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mrow id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml"><mn id="S4.SS1.p1.2.m2.1.1.2.2" xref="S4.SS1.p1.2.m2.1.1.2.2.cmml">2.84</mn><mo id="S4.SS1.p1.2.m2.1.1.2.1" xref="S4.SS1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS1.p1.2.m2.1.1.2.3" xref="S4.SS1.p1.2.m2.1.1.2.3.cmml">r</mi><mo id="S4.SS1.p1.2.m2.1.1.2.1a" xref="S4.SS1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS1.p1.2.m2.1.1.2.4" xref="S4.SS1.p1.2.m2.1.1.2.4.cmml">a</mi><mo id="S4.SS1.p1.2.m2.1.1.2.1b" xref="S4.SS1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS1.p1.2.m2.1.1.2.5" xref="S4.SS1.p1.2.m2.1.1.2.5.cmml">d</mi></mrow><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">/</mo><mi id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><divide id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></divide><apply id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2"><times id="S4.SS1.p1.2.m2.1.1.2.1.cmml" xref="S4.SS1.p1.2.m2.1.1.2.1"></times><cn id="S4.SS1.p1.2.m2.1.1.2.2.cmml" type="float" xref="S4.SS1.p1.2.m2.1.1.2.2">2.84</cn><ci id="S4.SS1.p1.2.m2.1.1.2.3.cmml" xref="S4.SS1.p1.2.m2.1.1.2.3">𝑟</ci><ci id="S4.SS1.p1.2.m2.1.1.2.4.cmml" xref="S4.SS1.p1.2.m2.1.1.2.4">𝑎</ci><ci id="S4.SS1.p1.2.m2.1.1.2.5.cmml" xref="S4.SS1.p1.2.m2.1.1.2.5">𝑑</ci></apply><ci id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">2.84rad/s</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">2.84 italic_r italic_a italic_d / italic_s</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">The Single-board Computer</span> The single-board computer (SBC) is an edge AI device based on an Nvidia TX2 NX, a high-performance embedded system with an accelerated 256-core NVIDIA Pascal GPU with CUDA version 10.2. The ZED Box runs on Ubuntu 18.04.6 LTS, Jetpack 4.6, ZED SDK version 3.7.3, and ROS Melodic. The SBC is powered by a TalentCell 72W 100WH power bank.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.2.1">High-Resolution Stereo Camera and Video Streaming </span> A ZED Mini stereoscopic camera is mounted facing the forward direction of the robot. Stereoscopic videos are streamed at HD720 resolution with a vertical FoV of 54° and a horizontal FoV of 85°. The video stream is wirelessly sent via a local 5G network to the VR device through a user datagram protocol (UDP) with the ZED SDK, where streaming latency is minimized through highly optimized GPU video encoding and decoding processes. From a similar video streaming configuration, we can estimate the motion-to-photon latency of such a setup to be <math alttext="153.47\pm 33.33ms" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mn id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">153.47</mn><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">±</mo><mrow id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml"><mn id="S4.SS1.p3.1.m1.1.1.3.2" xref="S4.SS1.p3.1.m1.1.1.3.2.cmml">33.33</mn><mo id="S4.SS1.p3.1.m1.1.1.3.1" xref="S4.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p3.1.m1.1.1.3.3" xref="S4.SS1.p3.1.m1.1.1.3.3.cmml">m</mi><mo id="S4.SS1.p3.1.m1.1.1.3.1a" xref="S4.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p3.1.m1.1.1.3.4" xref="S4.SS1.p3.1.m1.1.1.3.4.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.SS1.p3.1.m1.1.1.2.cmml" type="float" xref="S4.SS1.p3.1.m1.1.1.2">153.47</cn><apply id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3"><times id="S4.SS1.p3.1.m1.1.1.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.3.1"></times><cn id="S4.SS1.p3.1.m1.1.1.3.2.cmml" type="float" xref="S4.SS1.p3.1.m1.1.1.3.2">33.33</cn><ci id="S4.SS1.p3.1.m1.1.1.3.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3.3">𝑚</ci><ci id="S4.SS1.p3.1.m1.1.1.3.4.cmml" xref="S4.SS1.p3.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">153.47\pm 33.33ms</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">153.47 ± 33.33 italic_m italic_s</annotation></semantics></math> at 30 frames per second (fps) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib11" title="">11</a>]</cite> within a local 5G network. In addition, the ZED Mini camera also has a built-in 6DoF inertial measurement unit (IMU) to obtain <math alttext="M_{camera}" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><msub id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mi id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml">M</mi><mrow id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3.cmml"><mi id="S4.SS1.p3.2.m2.1.1.3.2" xref="S4.SS1.p3.2.m2.1.1.3.2.cmml">c</mi><mo id="S4.SS1.p3.2.m2.1.1.3.1" xref="S4.SS1.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p3.2.m2.1.1.3.3" xref="S4.SS1.p3.2.m2.1.1.3.3.cmml">a</mi><mo id="S4.SS1.p3.2.m2.1.1.3.1a" xref="S4.SS1.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p3.2.m2.1.1.3.4" xref="S4.SS1.p3.2.m2.1.1.3.4.cmml">m</mi><mo id="S4.SS1.p3.2.m2.1.1.3.1b" xref="S4.SS1.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p3.2.m2.1.1.3.5" xref="S4.SS1.p3.2.m2.1.1.3.5.cmml">e</mi><mo id="S4.SS1.p3.2.m2.1.1.3.1c" xref="S4.SS1.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p3.2.m2.1.1.3.6" xref="S4.SS1.p3.2.m2.1.1.3.6.cmml">r</mi><mo id="S4.SS1.p3.2.m2.1.1.3.1d" xref="S4.SS1.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p3.2.m2.1.1.3.7" xref="S4.SS1.p3.2.m2.1.1.3.7.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">𝑀</ci><apply id="S4.SS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3"><times id="S4.SS1.p3.2.m2.1.1.3.1.cmml" xref="S4.SS1.p3.2.m2.1.1.3.1"></times><ci id="S4.SS1.p3.2.m2.1.1.3.2.cmml" xref="S4.SS1.p3.2.m2.1.1.3.2">𝑐</ci><ci id="S4.SS1.p3.2.m2.1.1.3.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3.3">𝑎</ci><ci id="S4.SS1.p3.2.m2.1.1.3.4.cmml" xref="S4.SS1.p3.2.m2.1.1.3.4">𝑚</ci><ci id="S4.SS1.p3.2.m2.1.1.3.5.cmml" xref="S4.SS1.p3.2.m2.1.1.3.5">𝑒</ci><ci id="S4.SS1.p3.2.m2.1.1.3.6.cmml" xref="S4.SS1.p3.2.m2.1.1.3.6">𝑟</ci><ci id="S4.SS1.p3.2.m2.1.1.3.7.cmml" xref="S4.SS1.p3.2.m2.1.1.3.7">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">M_{camera}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_M start_POSTSUBSCRIPT italic_c italic_a italic_m italic_e italic_r italic_a end_POSTSUBSCRIPT</annotation></semantics></math> for accurate motion tracking.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Unity 3DGS VR Renderer</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Overview</span> Efficient 3DGS rendering relies on a sorting process that can rapidly re-order each Gaussian primitive based on the update of the camera poses and their clipping planes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib10" title="">10</a>]</cite>. Therefore, we developed a custom Unity VR renderer through Unity’s native render plugin to utilize Kerbl’s original CUDA kernels for parallel sorting and tiled-based rendering. As a result, compared to the currently available 3DGS Unity integration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib13" title="">13</a>]</cite>, our renderer can provide performance equivalent to the original CUDA implementation and is better optimized for immersive VR rendering.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Native Renderer Architect</span>
As our custom renderer does not include the 3D Gaussian points as built-in Unity game assets, we directly displayed the final rendered images as screen quad objects in the HMD. Our 3DGS Unity renderer takes the user’s tracked head pose, converts it into the camera view-projection matrix, combines it with other user-defined values such as resolution, FoV, geometric transformation, and updates these parameters in the native CUDA/C++ renderer at each frame. An effective VR plugin should also correctly synchronize the user’s head movement and the rendered images to avoid undesirable temporal aliasing effects such as scene jittering, which occurs in a recent attempt for native CUDA Unity-3DGS integration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib4" title="">4</a>]</cite>. Our rendering pipeline prevents this problem by triggering native render events inside the <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">onPreCull</span> Unity camera event to ensure that all native rendering jobs are completed before displaying the final render texture to users.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Spatial Registration</span> To correctly project 3D Gaussians in Unity, we converted the coordinate systems of 3DGS from COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib14" title="">14</a>]</cite> to Unity. For easy registration of the 3D Gaussians with the Unity world space, we define a reference object in the real world with known scale, rotation, and position whose one-to-one digital copy is available in Unity. Through the reference object, we performed manual calibration to obtain a relative transformation matrix to register the 3DGS model. In addition, we also assume the reference object is the robot’s initial position when using the OpenCR’s odometry sensor for markless tracking and pose estimation of the robot.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="261" id="S4.F3.g1" src="extracted/5770358/study_overview2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">Illustration of our user study experiment design, with C1 showing the exocentric stereo projection condition, C2 showing the exocentric reality fusion condition, C3 showing the egocentric reality fusion condition. The second row illustrates the real-world maze and the four planned teleoperation trajectories. </span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Unity Turtlebot Control Module</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Overview</span>  We developed a control module in Unity for handling user inputs, managing robot motion, communicating with the remote ROS master, and visualizing real-time robot states. Users can control the robot’s movement using the joysticks of the VR controllers with the robot’s linear and angular speed linearly mapped to the joystick inputs. The communication module for sending and receiving ROS messages is based on the Unity-TCP-Connector package. Messages with low bandwidth consumption such as odometry and IMU data are sent via the ROS-TCP connector. However, multi-media data such as videos and point clouds are transmitted via UDP for faster processing. To visualize real-time poses of the robot, a digital twin of the robot is rendered whose transform is updated at every frame based on the pose estimation obtained from OpenCR odometry.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Exocentric Control</span>  As Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.F3" title="Figure 3 ‣ IV-B Unity 3DGS VR Renderer ‣ IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag">3</span></a> C2 demonstrates, our framework allows robust control of the robot from third-person (exocentric) perspective. In the exocentric control mode, an operator can observe the robot’s current state in the world from any desired perspective. Users can use the joystick inputs of the VR controllers to translate their positions in the virtual world. While pressing the trigger button of the VR controllers, users can switch the joystick inputs to control the robot’s movement instead.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Egocentric Control</span>  As Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.F3" title="Figure 3 ‣ IV-B Unity 3DGS VR Renderer ‣ IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag">3</span></a> C3 demonstrates, our framework also provides an egocentric robot control mode, where the operator sees the environment from the robot’s perspective. In egocentric control mode, the user’s head position in the virtual environment automatically follows the robot’s tracked movement. In addition, we place the user’s virtual head position right behind the virtual robot state indicator rather than at the position of the stereo camera so that operator is aware of the robot’s position relative to its environment. Notice that in egocentric control mode, we only render half of the digital twin of the robot state indicator, such that the rendering does not obscure the operator’s fovea vision of the real-time stereo projection.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">Other Materials and Software</span>  Our framework was developed on Unity version 2019.4.29f1 based on OpenGL graphics API and uses the OpenVR desktop runtime and steamVR runtime. In addition, we use the mixed reality toolkit (MRTK) version 2.8 to develop spatial UIs and to manage user inputs in VR.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Overall System Performance</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.8">Our framework can achieve real-time performance on both high-end and moderate workstations. On moderate hardware such as the Alienware m17 R2 laptop with an RTX 2080 graphics card, the overall performance of the entire system, including video decoding, stereo correspondence estimation, stereo projection, point cloud rendering, robot control, and 3DGS rendering runs at <math alttext="40-45fps" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mn id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">40</mn><mo id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">−</mo><mrow id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml"><mn id="S4.SS4.p1.1.m1.1.1.3.2" xref="S4.SS4.p1.1.m1.1.1.3.2.cmml">45</mn><mo id="S4.SS4.p1.1.m1.1.1.3.1" xref="S4.SS4.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p1.1.m1.1.1.3.3" xref="S4.SS4.p1.1.m1.1.1.3.3.cmml">f</mi><mo id="S4.SS4.p1.1.m1.1.1.3.1a" xref="S4.SS4.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p1.1.m1.1.1.3.4" xref="S4.SS4.p1.1.m1.1.1.3.4.cmml">p</mi><mo id="S4.SS4.p1.1.m1.1.1.3.1b" xref="S4.SS4.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p1.1.m1.1.1.3.5" xref="S4.SS4.p1.1.m1.1.1.3.5.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><minus id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1"></minus><cn id="S4.SS4.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS4.p1.1.m1.1.1.2">40</cn><apply id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3"><times id="S4.SS4.p1.1.m1.1.1.3.1.cmml" xref="S4.SS4.p1.1.m1.1.1.3.1"></times><cn id="S4.SS4.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS4.p1.1.m1.1.1.3.2">45</cn><ci id="S4.SS4.p1.1.m1.1.1.3.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3.3">𝑓</ci><ci id="S4.SS4.p1.1.m1.1.1.3.4.cmml" xref="S4.SS4.p1.1.m1.1.1.3.4">𝑝</ci><ci id="S4.SS4.p1.1.m1.1.1.3.5.cmml" xref="S4.SS4.p1.1.m1.1.1.3.5">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">40-45fps</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">40 - 45 italic_f italic_p italic_s</annotation></semantics></math> for an industrial facility environment with <math alttext="622,335" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.2"><semantics id="S4.SS4.p1.2.m2.2a"><mrow id="S4.SS4.p1.2.m2.2.3.2" xref="S4.SS4.p1.2.m2.2.3.1.cmml"><mn id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">622</mn><mo id="S4.SS4.p1.2.m2.2.3.2.1" xref="S4.SS4.p1.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS4.p1.2.m2.2.2" xref="S4.SS4.p1.2.m2.2.2.cmml">335</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.2b"><list id="S4.SS4.p1.2.m2.2.3.1.cmml" xref="S4.SS4.p1.2.m2.2.3.2"><cn id="S4.SS4.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS4.p1.2.m2.1.1">622</cn><cn id="S4.SS4.p1.2.m2.2.2.cmml" type="integer" xref="S4.SS4.p1.2.m2.2.2">335</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.2c">622,335</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.2d">622 , 335</annotation></semantics></math> Gaussians at <math alttext="900\times 960" class="ltx_Math" display="inline" id="S4.SS4.p1.3.m3.1"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mn id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">900</mn><mo id="S4.SS4.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS4.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml">960</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><times id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1"></times><cn id="S4.SS4.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS4.p1.3.m3.1.1.2">900</cn><cn id="S4.SS4.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS4.p1.3.m3.1.1.3">960</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">900\times 960</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.3.m3.1d">900 × 960</annotation></semantics></math> per eye resolution (<math alttext="50\%" class="ltx_Math" display="inline" id="S4.SS4.p1.4.m4.1"><semantics id="S4.SS4.p1.4.m4.1a"><mrow id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mn id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">50</mn><mo id="S4.SS4.p1.4.m4.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1">percent</csymbol><cn id="S4.SS4.p1.4.m4.1.1.2.cmml" type="integer" xref="S4.SS4.p1.4.m4.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">50\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.4.m4.1d">50 %</annotation></semantics></math> resolution of a Meta Quest Pro headset with 106° horizontal FoV, and 96° vertical FoV). On a high-end device such as the Nvidia RTX 3090 GPU, the overall performance is <math alttext="30-35" class="ltx_Math" display="inline" id="S4.SS4.p1.5.m5.1"><semantics id="S4.SS4.p1.5.m5.1a"><mrow id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml"><mn id="S4.SS4.p1.5.m5.1.1.2" xref="S4.SS4.p1.5.m5.1.1.2.cmml">30</mn><mo id="S4.SS4.p1.5.m5.1.1.1" xref="S4.SS4.p1.5.m5.1.1.1.cmml">−</mo><mn id="S4.SS4.p1.5.m5.1.1.3" xref="S4.SS4.p1.5.m5.1.1.3.cmml">35</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.1b"><apply id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1"><minus id="S4.SS4.p1.5.m5.1.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1.1"></minus><cn id="S4.SS4.p1.5.m5.1.1.2.cmml" type="integer" xref="S4.SS4.p1.5.m5.1.1.2">30</cn><cn id="S4.SS4.p1.5.m5.1.1.3.cmml" type="integer" xref="S4.SS4.p1.5.m5.1.1.3">35</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.1c">30-35</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.5.m5.1d">30 - 35</annotation></semantics></math> fps for a room-scale environment with <math alttext="727,019" class="ltx_Math" display="inline" id="S4.SS4.p1.6.m6.2"><semantics id="S4.SS4.p1.6.m6.2a"><mrow id="S4.SS4.p1.6.m6.2.3.2" xref="S4.SS4.p1.6.m6.2.3.1.cmml"><mn id="S4.SS4.p1.6.m6.1.1" xref="S4.SS4.p1.6.m6.1.1.cmml">727</mn><mo id="S4.SS4.p1.6.m6.2.3.2.1" xref="S4.SS4.p1.6.m6.2.3.1.cmml">,</mo><mn id="S4.SS4.p1.6.m6.2.2" xref="S4.SS4.p1.6.m6.2.2.cmml">019</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m6.2b"><list id="S4.SS4.p1.6.m6.2.3.1.cmml" xref="S4.SS4.p1.6.m6.2.3.2"><cn id="S4.SS4.p1.6.m6.1.1.cmml" type="integer" xref="S4.SS4.p1.6.m6.1.1">727</cn><cn id="S4.SS4.p1.6.m6.2.2.cmml" type="integer" xref="S4.SS4.p1.6.m6.2.2">019</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m6.2c">727,019</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.6.m6.2d">727 , 019</annotation></semantics></math> Gaussians at <math alttext="1536\times 1440" class="ltx_Math" display="inline" id="S4.SS4.p1.7.m7.1"><semantics id="S4.SS4.p1.7.m7.1a"><mrow id="S4.SS4.p1.7.m7.1.1" xref="S4.SS4.p1.7.m7.1.1.cmml"><mn id="S4.SS4.p1.7.m7.1.1.2" xref="S4.SS4.p1.7.m7.1.1.2.cmml">1536</mn><mo id="S4.SS4.p1.7.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS4.p1.7.m7.1.1.1.cmml">×</mo><mn id="S4.SS4.p1.7.m7.1.1.3" xref="S4.SS4.p1.7.m7.1.1.3.cmml">1440</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.7.m7.1b"><apply id="S4.SS4.p1.7.m7.1.1.cmml" xref="S4.SS4.p1.7.m7.1.1"><times id="S4.SS4.p1.7.m7.1.1.1.cmml" xref="S4.SS4.p1.7.m7.1.1.1"></times><cn id="S4.SS4.p1.7.m7.1.1.2.cmml" type="integer" xref="S4.SS4.p1.7.m7.1.1.2">1536</cn><cn id="S4.SS4.p1.7.m7.1.1.3.cmml" type="integer" xref="S4.SS4.p1.7.m7.1.1.3">1440</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.7.m7.1c">1536\times 1440</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.7.m7.1d">1536 × 1440</annotation></semantics></math> per eye resolution (<math alttext="80\%" class="ltx_Math" display="inline" id="S4.SS4.p1.8.m8.1"><semantics id="S4.SS4.p1.8.m8.1a"><mrow id="S4.SS4.p1.8.m8.1.1" xref="S4.SS4.p1.8.m8.1.1.cmml"><mn id="S4.SS4.p1.8.m8.1.1.2" xref="S4.SS4.p1.8.m8.1.1.2.cmml">80</mn><mo id="S4.SS4.p1.8.m8.1.1.1" xref="S4.SS4.p1.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.8.m8.1b"><apply id="S4.SS4.p1.8.m8.1.1.cmml" xref="S4.SS4.p1.8.m8.1.1"><csymbol cd="latexml" id="S4.SS4.p1.8.m8.1.1.1.cmml" xref="S4.SS4.p1.8.m8.1.1.1">percent</csymbol><cn id="S4.SS4.p1.8.m8.1.1.2.cmml" type="integer" xref="S4.SS4.p1.8.m8.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.8.m8.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.8.m8.1d">80 %</annotation></semantics></math> of the maximum resolution of a Meat Quest Pro headset with 106° horizontal FoV, and 96° vertical FoV).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">USER STUDY EXPERIMENT</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present a user study experiment that benchmarks user performance and evaluates the effectiveness of our framework. The following research questions guided our user study and experiment design:</p>
</div>
<div class="ltx_para" id="S5.p2">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">RQ1</span>: What is the effect of reality fusion on the operator’s cognitive load, task performance, and situation awareness?</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">RQ2</span>: For reality fusion, how does the teleoperation perspective change users’ performance and perception?</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Conditions</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We used a within-subject experiment design through which participants need to complete a mobile robot navigation task through the following three types of immersive robot teleoperation UIs in VR. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.F3" title="Figure 3 ‣ IV-B Unity 3DGS VR Renderer ‣ IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag">3</span></a> C1-C3 presents application screenshots of the three different conditions.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<ol class="ltx_enumerate" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I2.i1.1.1.1">C1</span></span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i1.p1.1.1">Exocentric Stereo Projection Only <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib5" title="">5</a>]</cite></span>: users see only the real-time stereo projection while navigating the robot and their position in the virtual world in exocentric control mode. </p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I2.i2.1.1.1">C2</span></span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i2.p1.1.1">Exocentric Reality Fusion</span>: users see both the real-time stereo projection and 3DGS rendering while navigating the robot and their position in the virtual world in exocentric control mode.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I2.i3.1.1.1">C3</span></span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i3.p1.1.1">Egocentric Reality Fusion</span>: users see both the real-time stereo projection and the 3DGS rendering while navigating the robot in egocentric control mode.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S5.SS1.p2.1">C1 is a reference VR system as proposed by Freland et al. as our comparison baseline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib5" title="">5</a>]</cite>. As abundant previous research already revealed the superiority of robot teleoperation systems with VR HMD compared to conventional 2D displays and videos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib15" title="">15</a>]</cite>, this study focuses on comparison across different immersive VR robot teleoperation designs only. The ordering of the three conditions for each participant is counter-balanced using a balanced Latin Square method to compensate for carry-over effects.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Participants</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We invited 24 participants (10 female and 14 male) to make sure that each condition and route can be equally balanced. Two participants were between 18-24 years old, 17 were between 25-34 years old, 3 were between 35-44, 1 was between 45-54, and 1 was 65 years old or higher. All were students, researchers, or scientists in HCI, computer science, physics, or robotics. 10 participants use VR systems regularly (at least once a month), and 8 rarely use them ( once or less than once a year). 10 participants never operated a robot before, 11 rarely operated a robot before, and 3 participants worked with a robot regularly. All participants had normal or correct to normal vision.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Tasks</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.10">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.F3" title="Figure 3 ‣ IV-B Unity 3DGS VR Renderer ‣ IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag">3</span></a>, we designed a <math alttext="2.2m\times 2.2m" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mrow id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml"><mrow id="S5.SS3.p1.1.m1.1.1.2.2" xref="S5.SS3.p1.1.m1.1.1.2.2.cmml"><mn id="S5.SS3.p1.1.m1.1.1.2.2.2" xref="S5.SS3.p1.1.m1.1.1.2.2.2.cmml">2.2</mn><mo id="S5.SS3.p1.1.m1.1.1.2.2.1" xref="S5.SS3.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS3.p1.1.m1.1.1.2.2.3" xref="S5.SS3.p1.1.m1.1.1.2.2.3.cmml">m</mi></mrow><mo id="S5.SS3.p1.1.m1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S5.SS3.p1.1.m1.1.1.2.1.cmml">×</mo><mn id="S5.SS3.p1.1.m1.1.1.2.3" xref="S5.SS3.p1.1.m1.1.1.2.3.cmml">2.2</mn></mrow><mo id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><times id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1"></times><apply id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2"><times id="S5.SS3.p1.1.m1.1.1.2.1.cmml" xref="S5.SS3.p1.1.m1.1.1.2.1"></times><apply id="S5.SS3.p1.1.m1.1.1.2.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2.2"><times id="S5.SS3.p1.1.m1.1.1.2.2.1.cmml" xref="S5.SS3.p1.1.m1.1.1.2.2.1"></times><cn id="S5.SS3.p1.1.m1.1.1.2.2.2.cmml" type="float" xref="S5.SS3.p1.1.m1.1.1.2.2.2">2.2</cn><ci id="S5.SS3.p1.1.m1.1.1.2.2.3.cmml" xref="S5.SS3.p1.1.m1.1.1.2.2.3">𝑚</ci></apply><cn id="S5.SS3.p1.1.m1.1.1.2.3.cmml" type="float" xref="S5.SS3.p1.1.m1.1.1.2.3">2.2</cn></apply><ci id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">2.2m\times 2.2m</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">2.2 italic_m × 2.2 italic_m</annotation></semantics></math> maze with four symmetrical different entrance points. Inside the maze, there are three <math alttext="0.6m\times 0.15m" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mrow id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml"><mrow id="S5.SS3.p1.2.m2.1.1.2.2" xref="S5.SS3.p1.2.m2.1.1.2.2.cmml"><mn id="S5.SS3.p1.2.m2.1.1.2.2.2" xref="S5.SS3.p1.2.m2.1.1.2.2.2.cmml">0.6</mn><mo id="S5.SS3.p1.2.m2.1.1.2.2.1" xref="S5.SS3.p1.2.m2.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS3.p1.2.m2.1.1.2.2.3" xref="S5.SS3.p1.2.m2.1.1.2.2.3.cmml">m</mi></mrow><mo id="S5.SS3.p1.2.m2.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S5.SS3.p1.2.m2.1.1.2.1.cmml">×</mo><mn id="S5.SS3.p1.2.m2.1.1.2.3" xref="S5.SS3.p1.2.m2.1.1.2.3.cmml">0.15</mn></mrow><mo id="S5.SS3.p1.2.m2.1.1.1" xref="S5.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p1.2.m2.1.1.3" xref="S5.SS3.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><times id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1"></times><apply id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2"><times id="S5.SS3.p1.2.m2.1.1.2.1.cmml" xref="S5.SS3.p1.2.m2.1.1.2.1"></times><apply id="S5.SS3.p1.2.m2.1.1.2.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2.2"><times id="S5.SS3.p1.2.m2.1.1.2.2.1.cmml" xref="S5.SS3.p1.2.m2.1.1.2.2.1"></times><cn id="S5.SS3.p1.2.m2.1.1.2.2.2.cmml" type="float" xref="S5.SS3.p1.2.m2.1.1.2.2.2">0.6</cn><ci id="S5.SS3.p1.2.m2.1.1.2.2.3.cmml" xref="S5.SS3.p1.2.m2.1.1.2.2.3">𝑚</ci></apply><cn id="S5.SS3.p1.2.m2.1.1.2.3.cmml" type="float" xref="S5.SS3.p1.2.m2.1.1.2.3">0.15</cn></apply><ci id="S5.SS3.p1.2.m2.1.1.3.cmml" xref="S5.SS3.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">0.6m\times 0.15m</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">0.6 italic_m × 0.15 italic_m</annotation></semantics></math> obstacle areas which form two <math alttext="0.6m\times 0.875m" class="ltx_Math" display="inline" id="S5.SS3.p1.3.m3.1"><semantics id="S5.SS3.p1.3.m3.1a"><mrow id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mrow id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml"><mrow id="S5.SS3.p1.3.m3.1.1.2.2" xref="S5.SS3.p1.3.m3.1.1.2.2.cmml"><mn id="S5.SS3.p1.3.m3.1.1.2.2.2" xref="S5.SS3.p1.3.m3.1.1.2.2.2.cmml">0.6</mn><mo id="S5.SS3.p1.3.m3.1.1.2.2.1" xref="S5.SS3.p1.3.m3.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS3.p1.3.m3.1.1.2.2.3" xref="S5.SS3.p1.3.m3.1.1.2.2.3.cmml">m</mi></mrow><mo id="S5.SS3.p1.3.m3.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S5.SS3.p1.3.m3.1.1.2.1.cmml">×</mo><mn id="S5.SS3.p1.3.m3.1.1.2.3" xref="S5.SS3.p1.3.m3.1.1.2.3.cmml">0.875</mn></mrow><mo id="S5.SS3.p1.3.m3.1.1.1" xref="S5.SS3.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p1.3.m3.1.1.3" xref="S5.SS3.p1.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><times id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1.1"></times><apply id="S5.SS3.p1.3.m3.1.1.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2"><times id="S5.SS3.p1.3.m3.1.1.2.1.cmml" xref="S5.SS3.p1.3.m3.1.1.2.1"></times><apply id="S5.SS3.p1.3.m3.1.1.2.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2.2"><times id="S5.SS3.p1.3.m3.1.1.2.2.1.cmml" xref="S5.SS3.p1.3.m3.1.1.2.2.1"></times><cn id="S5.SS3.p1.3.m3.1.1.2.2.2.cmml" type="float" xref="S5.SS3.p1.3.m3.1.1.2.2.2">0.6</cn><ci id="S5.SS3.p1.3.m3.1.1.2.2.3.cmml" xref="S5.SS3.p1.3.m3.1.1.2.2.3">𝑚</ci></apply><cn id="S5.SS3.p1.3.m3.1.1.2.3.cmml" type="float" xref="S5.SS3.p1.3.m3.1.1.2.3">0.875</cn></apply><ci id="S5.SS3.p1.3.m3.1.1.3.cmml" xref="S5.SS3.p1.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">0.6m\times 0.875m</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.3.m3.1d">0.6 italic_m × 0.875 italic_m</annotation></semantics></math> pathways. We designed four different trajectories through which users can navigate the robot from one entrance of the maze to the target exit. Each trajectory consists of three subpaths and the level of difficulty for navigation of these subpaths is the same according to the steering law which can predict the amount of time (<math alttext="T" class="ltx_Math" display="inline" id="S5.SS3.p1.4.m4.1"><semantics id="S5.SS3.p1.4.m4.1a"><mi id="S5.SS3.p1.4.m4.1.1" xref="S5.SS3.p1.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.4.m4.1b"><ci id="S5.SS3.p1.4.m4.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.4.m4.1d">italic_T</annotation></semantics></math>) users need to navigate through a 2D tunnel given the width of the tunnel <math alttext="W" class="ltx_Math" display="inline" id="S5.SS3.p1.5.m5.1"><semantics id="S5.SS3.p1.5.m5.1a"><mi id="S5.SS3.p1.5.m5.1.1" xref="S5.SS3.p1.5.m5.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.5.m5.1b"><ci id="S5.SS3.p1.5.m5.1.1.cmml" xref="S5.SS3.p1.5.m5.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.5.m5.1c">W</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.5.m5.1d">italic_W</annotation></semantics></math> and the length of the tunnel <math alttext="A" class="ltx_Math" display="inline" id="S5.SS3.p1.6.m6.1"><semantics id="S5.SS3.p1.6.m6.1a"><mi id="S5.SS3.p1.6.m6.1.1" xref="S5.SS3.p1.6.m6.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.6.m6.1b"><ci id="S5.SS3.p1.6.m6.1.1.cmml" xref="S5.SS3.p1.6.m6.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.6.m6.1c">A</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.6.m6.1d">italic_A</annotation></semantics></math>: <math alttext="T=a+b\frac{A}{W}" class="ltx_Math" display="inline" id="S5.SS3.p1.7.m7.1"><semantics id="S5.SS3.p1.7.m7.1a"><mrow id="S5.SS3.p1.7.m7.1.1" xref="S5.SS3.p1.7.m7.1.1.cmml"><mi id="S5.SS3.p1.7.m7.1.1.2" xref="S5.SS3.p1.7.m7.1.1.2.cmml">T</mi><mo id="S5.SS3.p1.7.m7.1.1.1" xref="S5.SS3.p1.7.m7.1.1.1.cmml">=</mo><mrow id="S5.SS3.p1.7.m7.1.1.3" xref="S5.SS3.p1.7.m7.1.1.3.cmml"><mi id="S5.SS3.p1.7.m7.1.1.3.2" xref="S5.SS3.p1.7.m7.1.1.3.2.cmml">a</mi><mo id="S5.SS3.p1.7.m7.1.1.3.1" xref="S5.SS3.p1.7.m7.1.1.3.1.cmml">+</mo><mrow id="S5.SS3.p1.7.m7.1.1.3.3" xref="S5.SS3.p1.7.m7.1.1.3.3.cmml"><mi id="S5.SS3.p1.7.m7.1.1.3.3.2" xref="S5.SS3.p1.7.m7.1.1.3.3.2.cmml">b</mi><mo id="S5.SS3.p1.7.m7.1.1.3.3.1" xref="S5.SS3.p1.7.m7.1.1.3.3.1.cmml">⁢</mo><mfrac id="S5.SS3.p1.7.m7.1.1.3.3.3" xref="S5.SS3.p1.7.m7.1.1.3.3.3.cmml"><mi id="S5.SS3.p1.7.m7.1.1.3.3.3.2" xref="S5.SS3.p1.7.m7.1.1.3.3.3.2.cmml">A</mi><mi id="S5.SS3.p1.7.m7.1.1.3.3.3.3" xref="S5.SS3.p1.7.m7.1.1.3.3.3.3.cmml">W</mi></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.7.m7.1b"><apply id="S5.SS3.p1.7.m7.1.1.cmml" xref="S5.SS3.p1.7.m7.1.1"><eq id="S5.SS3.p1.7.m7.1.1.1.cmml" xref="S5.SS3.p1.7.m7.1.1.1"></eq><ci id="S5.SS3.p1.7.m7.1.1.2.cmml" xref="S5.SS3.p1.7.m7.1.1.2">𝑇</ci><apply id="S5.SS3.p1.7.m7.1.1.3.cmml" xref="S5.SS3.p1.7.m7.1.1.3"><plus id="S5.SS3.p1.7.m7.1.1.3.1.cmml" xref="S5.SS3.p1.7.m7.1.1.3.1"></plus><ci id="S5.SS3.p1.7.m7.1.1.3.2.cmml" xref="S5.SS3.p1.7.m7.1.1.3.2">𝑎</ci><apply id="S5.SS3.p1.7.m7.1.1.3.3.cmml" xref="S5.SS3.p1.7.m7.1.1.3.3"><times id="S5.SS3.p1.7.m7.1.1.3.3.1.cmml" xref="S5.SS3.p1.7.m7.1.1.3.3.1"></times><ci id="S5.SS3.p1.7.m7.1.1.3.3.2.cmml" xref="S5.SS3.p1.7.m7.1.1.3.3.2">𝑏</ci><apply id="S5.SS3.p1.7.m7.1.1.3.3.3.cmml" xref="S5.SS3.p1.7.m7.1.1.3.3.3"><divide id="S5.SS3.p1.7.m7.1.1.3.3.3.1.cmml" xref="S5.SS3.p1.7.m7.1.1.3.3.3"></divide><ci id="S5.SS3.p1.7.m7.1.1.3.3.3.2.cmml" xref="S5.SS3.p1.7.m7.1.1.3.3.3.2">𝐴</ci><ci id="S5.SS3.p1.7.m7.1.1.3.3.3.3.cmml" xref="S5.SS3.p1.7.m7.1.1.3.3.3.3">𝑊</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.7.m7.1c">T=a+b\frac{A}{W}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.7.m7.1d">italic_T = italic_a + italic_b divide start_ARG italic_A end_ARG start_ARG italic_W end_ARG</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib1" title="">1</a>]</cite>. As Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.F3" title="Figure 3 ‣ IV-B Unity 3DGS VR Renderer ‣ IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag">3</span></a> C1-C3 presents, a goal state indicator (rendered as blue) is presented to inform the participants of the target robot position they need to navigate the robot to. Participants need to sequentially navigate the robot to reach all three goal states (<math alttext="T1,T2,T3" class="ltx_Math" display="inline" id="S5.SS3.p1.8.m8.3"><semantics id="S5.SS3.p1.8.m8.3a"><mrow id="S5.SS3.p1.8.m8.3.3.3" xref="S5.SS3.p1.8.m8.3.3.4.cmml"><mrow id="S5.SS3.p1.8.m8.1.1.1.1" xref="S5.SS3.p1.8.m8.1.1.1.1.cmml"><mi id="S5.SS3.p1.8.m8.1.1.1.1.2" xref="S5.SS3.p1.8.m8.1.1.1.1.2.cmml">T</mi><mo id="S5.SS3.p1.8.m8.1.1.1.1.1" xref="S5.SS3.p1.8.m8.1.1.1.1.1.cmml">⁢</mo><mn id="S5.SS3.p1.8.m8.1.1.1.1.3" xref="S5.SS3.p1.8.m8.1.1.1.1.3.cmml">1</mn></mrow><mo id="S5.SS3.p1.8.m8.3.3.3.4" xref="S5.SS3.p1.8.m8.3.3.4.cmml">,</mo><mrow id="S5.SS3.p1.8.m8.2.2.2.2" xref="S5.SS3.p1.8.m8.2.2.2.2.cmml"><mi id="S5.SS3.p1.8.m8.2.2.2.2.2" xref="S5.SS3.p1.8.m8.2.2.2.2.2.cmml">T</mi><mo id="S5.SS3.p1.8.m8.2.2.2.2.1" xref="S5.SS3.p1.8.m8.2.2.2.2.1.cmml">⁢</mo><mn id="S5.SS3.p1.8.m8.2.2.2.2.3" xref="S5.SS3.p1.8.m8.2.2.2.2.3.cmml">2</mn></mrow><mo id="S5.SS3.p1.8.m8.3.3.3.5" xref="S5.SS3.p1.8.m8.3.3.4.cmml">,</mo><mrow id="S5.SS3.p1.8.m8.3.3.3.3" xref="S5.SS3.p1.8.m8.3.3.3.3.cmml"><mi id="S5.SS3.p1.8.m8.3.3.3.3.2" xref="S5.SS3.p1.8.m8.3.3.3.3.2.cmml">T</mi><mo id="S5.SS3.p1.8.m8.3.3.3.3.1" xref="S5.SS3.p1.8.m8.3.3.3.3.1.cmml">⁢</mo><mn id="S5.SS3.p1.8.m8.3.3.3.3.3" xref="S5.SS3.p1.8.m8.3.3.3.3.3.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.8.m8.3b"><list id="S5.SS3.p1.8.m8.3.3.4.cmml" xref="S5.SS3.p1.8.m8.3.3.3"><apply id="S5.SS3.p1.8.m8.1.1.1.1.cmml" xref="S5.SS3.p1.8.m8.1.1.1.1"><times id="S5.SS3.p1.8.m8.1.1.1.1.1.cmml" xref="S5.SS3.p1.8.m8.1.1.1.1.1"></times><ci id="S5.SS3.p1.8.m8.1.1.1.1.2.cmml" xref="S5.SS3.p1.8.m8.1.1.1.1.2">𝑇</ci><cn id="S5.SS3.p1.8.m8.1.1.1.1.3.cmml" type="integer" xref="S5.SS3.p1.8.m8.1.1.1.1.3">1</cn></apply><apply id="S5.SS3.p1.8.m8.2.2.2.2.cmml" xref="S5.SS3.p1.8.m8.2.2.2.2"><times id="S5.SS3.p1.8.m8.2.2.2.2.1.cmml" xref="S5.SS3.p1.8.m8.2.2.2.2.1"></times><ci id="S5.SS3.p1.8.m8.2.2.2.2.2.cmml" xref="S5.SS3.p1.8.m8.2.2.2.2.2">𝑇</ci><cn id="S5.SS3.p1.8.m8.2.2.2.2.3.cmml" type="integer" xref="S5.SS3.p1.8.m8.2.2.2.2.3">2</cn></apply><apply id="S5.SS3.p1.8.m8.3.3.3.3.cmml" xref="S5.SS3.p1.8.m8.3.3.3.3"><times id="S5.SS3.p1.8.m8.3.3.3.3.1.cmml" xref="S5.SS3.p1.8.m8.3.3.3.3.1"></times><ci id="S5.SS3.p1.8.m8.3.3.3.3.2.cmml" xref="S5.SS3.p1.8.m8.3.3.3.3.2">𝑇</ci><cn id="S5.SS3.p1.8.m8.3.3.3.3.3.cmml" type="integer" xref="S5.SS3.p1.8.m8.3.3.3.3.3">3</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.8.m8.3c">T1,T2,T3</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.8.m8.3d">italic_T 1 , italic_T 2 , italic_T 3</annotation></semantics></math>) in the designated trajectory for each condition. For each different condition, participants are assigned a different navigation trajectory. The maximum speed of the robot was adjusted to <math alttext="0.05m/s" class="ltx_Math" display="inline" id="S5.SS3.p1.9.m9.1"><semantics id="S5.SS3.p1.9.m9.1a"><mrow id="S5.SS3.p1.9.m9.1.1" xref="S5.SS3.p1.9.m9.1.1.cmml"><mrow id="S5.SS3.p1.9.m9.1.1.2" xref="S5.SS3.p1.9.m9.1.1.2.cmml"><mn id="S5.SS3.p1.9.m9.1.1.2.2" xref="S5.SS3.p1.9.m9.1.1.2.2.cmml">0.05</mn><mo id="S5.SS3.p1.9.m9.1.1.2.1" xref="S5.SS3.p1.9.m9.1.1.2.1.cmml">⁢</mo><mi id="S5.SS3.p1.9.m9.1.1.2.3" xref="S5.SS3.p1.9.m9.1.1.2.3.cmml">m</mi></mrow><mo id="S5.SS3.p1.9.m9.1.1.1" xref="S5.SS3.p1.9.m9.1.1.1.cmml">/</mo><mi id="S5.SS3.p1.9.m9.1.1.3" xref="S5.SS3.p1.9.m9.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.9.m9.1b"><apply id="S5.SS3.p1.9.m9.1.1.cmml" xref="S5.SS3.p1.9.m9.1.1"><divide id="S5.SS3.p1.9.m9.1.1.1.cmml" xref="S5.SS3.p1.9.m9.1.1.1"></divide><apply id="S5.SS3.p1.9.m9.1.1.2.cmml" xref="S5.SS3.p1.9.m9.1.1.2"><times id="S5.SS3.p1.9.m9.1.1.2.1.cmml" xref="S5.SS3.p1.9.m9.1.1.2.1"></times><cn id="S5.SS3.p1.9.m9.1.1.2.2.cmml" type="float" xref="S5.SS3.p1.9.m9.1.1.2.2">0.05</cn><ci id="S5.SS3.p1.9.m9.1.1.2.3.cmml" xref="S5.SS3.p1.9.m9.1.1.2.3">𝑚</ci></apply><ci id="S5.SS3.p1.9.m9.1.1.3.cmml" xref="S5.SS3.p1.9.m9.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.9.m9.1c">0.05m/s</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.9.m9.1d">0.05 italic_m / italic_s</annotation></semantics></math> linearly and <math alttext="0.5rad/s" class="ltx_Math" display="inline" id="S5.SS3.p1.10.m10.1"><semantics id="S5.SS3.p1.10.m10.1a"><mrow id="S5.SS3.p1.10.m10.1.1" xref="S5.SS3.p1.10.m10.1.1.cmml"><mrow id="S5.SS3.p1.10.m10.1.1.2" xref="S5.SS3.p1.10.m10.1.1.2.cmml"><mn id="S5.SS3.p1.10.m10.1.1.2.2" xref="S5.SS3.p1.10.m10.1.1.2.2.cmml">0.5</mn><mo id="S5.SS3.p1.10.m10.1.1.2.1" xref="S5.SS3.p1.10.m10.1.1.2.1.cmml">⁢</mo><mi id="S5.SS3.p1.10.m10.1.1.2.3" xref="S5.SS3.p1.10.m10.1.1.2.3.cmml">r</mi><mo id="S5.SS3.p1.10.m10.1.1.2.1a" xref="S5.SS3.p1.10.m10.1.1.2.1.cmml">⁢</mo><mi id="S5.SS3.p1.10.m10.1.1.2.4" xref="S5.SS3.p1.10.m10.1.1.2.4.cmml">a</mi><mo id="S5.SS3.p1.10.m10.1.1.2.1b" xref="S5.SS3.p1.10.m10.1.1.2.1.cmml">⁢</mo><mi id="S5.SS3.p1.10.m10.1.1.2.5" xref="S5.SS3.p1.10.m10.1.1.2.5.cmml">d</mi></mrow><mo id="S5.SS3.p1.10.m10.1.1.1" xref="S5.SS3.p1.10.m10.1.1.1.cmml">/</mo><mi id="S5.SS3.p1.10.m10.1.1.3" xref="S5.SS3.p1.10.m10.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.10.m10.1b"><apply id="S5.SS3.p1.10.m10.1.1.cmml" xref="S5.SS3.p1.10.m10.1.1"><divide id="S5.SS3.p1.10.m10.1.1.1.cmml" xref="S5.SS3.p1.10.m10.1.1.1"></divide><apply id="S5.SS3.p1.10.m10.1.1.2.cmml" xref="S5.SS3.p1.10.m10.1.1.2"><times id="S5.SS3.p1.10.m10.1.1.2.1.cmml" xref="S5.SS3.p1.10.m10.1.1.2.1"></times><cn id="S5.SS3.p1.10.m10.1.1.2.2.cmml" type="float" xref="S5.SS3.p1.10.m10.1.1.2.2">0.5</cn><ci id="S5.SS3.p1.10.m10.1.1.2.3.cmml" xref="S5.SS3.p1.10.m10.1.1.2.3">𝑟</ci><ci id="S5.SS3.p1.10.m10.1.1.2.4.cmml" xref="S5.SS3.p1.10.m10.1.1.2.4">𝑎</ci><ci id="S5.SS3.p1.10.m10.1.1.2.5.cmml" xref="S5.SS3.p1.10.m10.1.1.2.5">𝑑</ci></apply><ci id="S5.SS3.p1.10.m10.1.1.3.cmml" xref="S5.SS3.p1.10.m10.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.10.m10.1c">0.5rad/s</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.10.m10.1d">0.5 italic_r italic_a italic_d / italic_s</annotation></semantics></math> angularly to ensure teleoperation safety.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">Materials</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.5">The experiment was performed on a Meta Quest Pro headset and a Windows PC with a Nvidia 3090 GPU. The 3DGS model of the real-world maze was reconstructed from 69 images with <math alttext="3990\times 2985" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.1"><semantics id="S5.SS4.p1.1.m1.1a"><mrow id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mn id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml">3990</mn><mo id="S5.SS4.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3.cmml">2985</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><times id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1"></times><cn id="S5.SS4.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS4.p1.1.m1.1.1.2">3990</cn><cn id="S5.SS4.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS4.p1.1.m1.1.1.3">2985</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">3990\times 2985</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.1.m1.1d">3990 × 2985</annotation></semantics></math> resolution. We generated the 3DGS model following the original model training pipeline developed by Kerbl et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib10" title="">10</a>]</cite>. The training output was post-processed using the Unity 3DGS 2D editing toolkit developed by Pranckevičius <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib13" title="">13</a>]</cite>, where we removed outliners and erroneous results to improve the overall visual appearance. The post-processed 3DGS model has <math alttext="727,019" class="ltx_Math" display="inline" id="S5.SS4.p1.2.m2.2"><semantics id="S5.SS4.p1.2.m2.2a"><mrow id="S5.SS4.p1.2.m2.2.3.2" xref="S5.SS4.p1.2.m2.2.3.1.cmml"><mn id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml">727</mn><mo id="S5.SS4.p1.2.m2.2.3.2.1" xref="S5.SS4.p1.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS4.p1.2.m2.2.2" xref="S5.SS4.p1.2.m2.2.2.cmml">019</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.2b"><list id="S5.SS4.p1.2.m2.2.3.1.cmml" xref="S5.SS4.p1.2.m2.2.3.2"><cn id="S5.SS4.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS4.p1.2.m2.1.1">727</cn><cn id="S5.SS4.p1.2.m2.2.2.cmml" type="integer" xref="S5.SS4.p1.2.m2.2.2">019</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.2c">727,019</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.2.m2.2d">727 , 019</annotation></semantics></math> Gaussians representing around <math alttext="3.7m\times 3.7m\times 1.5m" class="ltx_Math" display="inline" id="S5.SS4.p1.3.m3.1"><semantics id="S5.SS4.p1.3.m3.1a"><mrow id="S5.SS4.p1.3.m3.1.1" xref="S5.SS4.p1.3.m3.1.1.cmml"><mrow id="S5.SS4.p1.3.m3.1.1.2" xref="S5.SS4.p1.3.m3.1.1.2.cmml"><mrow id="S5.SS4.p1.3.m3.1.1.2.2" xref="S5.SS4.p1.3.m3.1.1.2.2.cmml"><mrow id="S5.SS4.p1.3.m3.1.1.2.2.2" xref="S5.SS4.p1.3.m3.1.1.2.2.2.cmml"><mrow id="S5.SS4.p1.3.m3.1.1.2.2.2.2" xref="S5.SS4.p1.3.m3.1.1.2.2.2.2.cmml"><mn id="S5.SS4.p1.3.m3.1.1.2.2.2.2.2" xref="S5.SS4.p1.3.m3.1.1.2.2.2.2.2.cmml">3.7</mn><mo id="S5.SS4.p1.3.m3.1.1.2.2.2.2.1" xref="S5.SS4.p1.3.m3.1.1.2.2.2.2.1.cmml">⁢</mo><mi id="S5.SS4.p1.3.m3.1.1.2.2.2.2.3" xref="S5.SS4.p1.3.m3.1.1.2.2.2.2.3.cmml">m</mi></mrow><mo id="S5.SS4.p1.3.m3.1.1.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="S5.SS4.p1.3.m3.1.1.2.2.2.1.cmml">×</mo><mn id="S5.SS4.p1.3.m3.1.1.2.2.2.3" xref="S5.SS4.p1.3.m3.1.1.2.2.2.3.cmml">3.7</mn></mrow><mo id="S5.SS4.p1.3.m3.1.1.2.2.1" xref="S5.SS4.p1.3.m3.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS4.p1.3.m3.1.1.2.2.3" xref="S5.SS4.p1.3.m3.1.1.2.2.3.cmml">m</mi></mrow><mo id="S5.SS4.p1.3.m3.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S5.SS4.p1.3.m3.1.1.2.1.cmml">×</mo><mn id="S5.SS4.p1.3.m3.1.1.2.3" xref="S5.SS4.p1.3.m3.1.1.2.3.cmml">1.5</mn></mrow><mo id="S5.SS4.p1.3.m3.1.1.1" xref="S5.SS4.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS4.p1.3.m3.1.1.3" xref="S5.SS4.p1.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.m3.1b"><apply id="S5.SS4.p1.3.m3.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1"><times id="S5.SS4.p1.3.m3.1.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1.1"></times><apply id="S5.SS4.p1.3.m3.1.1.2.cmml" xref="S5.SS4.p1.3.m3.1.1.2"><times id="S5.SS4.p1.3.m3.1.1.2.1.cmml" xref="S5.SS4.p1.3.m3.1.1.2.1"></times><apply id="S5.SS4.p1.3.m3.1.1.2.2.cmml" xref="S5.SS4.p1.3.m3.1.1.2.2"><times id="S5.SS4.p1.3.m3.1.1.2.2.1.cmml" xref="S5.SS4.p1.3.m3.1.1.2.2.1"></times><apply id="S5.SS4.p1.3.m3.1.1.2.2.2.cmml" xref="S5.SS4.p1.3.m3.1.1.2.2.2"><times id="S5.SS4.p1.3.m3.1.1.2.2.2.1.cmml" xref="S5.SS4.p1.3.m3.1.1.2.2.2.1"></times><apply id="S5.SS4.p1.3.m3.1.1.2.2.2.2.cmml" xref="S5.SS4.p1.3.m3.1.1.2.2.2.2"><times id="S5.SS4.p1.3.m3.1.1.2.2.2.2.1.cmml" xref="S5.SS4.p1.3.m3.1.1.2.2.2.2.1"></times><cn id="S5.SS4.p1.3.m3.1.1.2.2.2.2.2.cmml" type="float" xref="S5.SS4.p1.3.m3.1.1.2.2.2.2.2">3.7</cn><ci id="S5.SS4.p1.3.m3.1.1.2.2.2.2.3.cmml" xref="S5.SS4.p1.3.m3.1.1.2.2.2.2.3">𝑚</ci></apply><cn id="S5.SS4.p1.3.m3.1.1.2.2.2.3.cmml" type="float" xref="S5.SS4.p1.3.m3.1.1.2.2.2.3">3.7</cn></apply><ci id="S5.SS4.p1.3.m3.1.1.2.2.3.cmml" xref="S5.SS4.p1.3.m3.1.1.2.2.3">𝑚</ci></apply><cn id="S5.SS4.p1.3.m3.1.1.2.3.cmml" type="float" xref="S5.SS4.p1.3.m3.1.1.2.3">1.5</cn></apply><ci id="S5.SS4.p1.3.m3.1.1.3.cmml" xref="S5.SS4.p1.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.m3.1c">3.7m\times 3.7m\times 1.5m</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.3.m3.1d">3.7 italic_m × 3.7 italic_m × 1.5 italic_m</annotation></semantics></math> real-world volume which covers the entire maze and its surroundings. The robot teleoperation application runs at <math alttext="30-35fps" class="ltx_Math" display="inline" id="S5.SS4.p1.4.m4.1"><semantics id="S5.SS4.p1.4.m4.1a"><mrow id="S5.SS4.p1.4.m4.1.1" xref="S5.SS4.p1.4.m4.1.1.cmml"><mn id="S5.SS4.p1.4.m4.1.1.2" xref="S5.SS4.p1.4.m4.1.1.2.cmml">30</mn><mo id="S5.SS4.p1.4.m4.1.1.1" xref="S5.SS4.p1.4.m4.1.1.1.cmml">−</mo><mrow id="S5.SS4.p1.4.m4.1.1.3" xref="S5.SS4.p1.4.m4.1.1.3.cmml"><mn id="S5.SS4.p1.4.m4.1.1.3.2" xref="S5.SS4.p1.4.m4.1.1.3.2.cmml">35</mn><mo id="S5.SS4.p1.4.m4.1.1.3.1" xref="S5.SS4.p1.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S5.SS4.p1.4.m4.1.1.3.3" xref="S5.SS4.p1.4.m4.1.1.3.3.cmml">f</mi><mo id="S5.SS4.p1.4.m4.1.1.3.1a" xref="S5.SS4.p1.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S5.SS4.p1.4.m4.1.1.3.4" xref="S5.SS4.p1.4.m4.1.1.3.4.cmml">p</mi><mo id="S5.SS4.p1.4.m4.1.1.3.1b" xref="S5.SS4.p1.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S5.SS4.p1.4.m4.1.1.3.5" xref="S5.SS4.p1.4.m4.1.1.3.5.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.m4.1b"><apply id="S5.SS4.p1.4.m4.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1"><minus id="S5.SS4.p1.4.m4.1.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1.1"></minus><cn id="S5.SS4.p1.4.m4.1.1.2.cmml" type="integer" xref="S5.SS4.p1.4.m4.1.1.2">30</cn><apply id="S5.SS4.p1.4.m4.1.1.3.cmml" xref="S5.SS4.p1.4.m4.1.1.3"><times id="S5.SS4.p1.4.m4.1.1.3.1.cmml" xref="S5.SS4.p1.4.m4.1.1.3.1"></times><cn id="S5.SS4.p1.4.m4.1.1.3.2.cmml" type="integer" xref="S5.SS4.p1.4.m4.1.1.3.2">35</cn><ci id="S5.SS4.p1.4.m4.1.1.3.3.cmml" xref="S5.SS4.p1.4.m4.1.1.3.3">𝑓</ci><ci id="S5.SS4.p1.4.m4.1.1.3.4.cmml" xref="S5.SS4.p1.4.m4.1.1.3.4">𝑝</ci><ci id="S5.SS4.p1.4.m4.1.1.3.5.cmml" xref="S5.SS4.p1.4.m4.1.1.3.5">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.m4.1c">30-35fps</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.4.m4.1d">30 - 35 italic_f italic_p italic_s</annotation></semantics></math> at <math alttext="1536\times 1440" class="ltx_Math" display="inline" id="S5.SS4.p1.5.m5.1"><semantics id="S5.SS4.p1.5.m5.1a"><mrow id="S5.SS4.p1.5.m5.1.1" xref="S5.SS4.p1.5.m5.1.1.cmml"><mn id="S5.SS4.p1.5.m5.1.1.2" xref="S5.SS4.p1.5.m5.1.1.2.cmml">1536</mn><mo id="S5.SS4.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS4.p1.5.m5.1.1.1.cmml">×</mo><mn id="S5.SS4.p1.5.m5.1.1.3" xref="S5.SS4.p1.5.m5.1.1.3.cmml">1440</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.5.m5.1b"><apply id="S5.SS4.p1.5.m5.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1"><times id="S5.SS4.p1.5.m5.1.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1.1"></times><cn id="S5.SS4.p1.5.m5.1.1.2.cmml" type="integer" xref="S5.SS4.p1.5.m5.1.1.2">1536</cn><cn id="S5.SS4.p1.5.m5.1.1.3.cmml" type="integer" xref="S5.SS4.p1.5.m5.1.1.3">1440</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.5.m5.1c">1536\times 1440</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.5.m5.1d">1536 × 1440</annotation></semantics></math> per eye resolution as recorded in <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S4.SS4" title="IV-D Overall System Performance ‣ IV FRAMEWORK IMPLEMENTATION ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="539" id="S5.F4.g1" src="x2.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.3.2" style="font-size:90%;">Mean mental demand, physical demand, frustration, task performance, situational awareness, and SSQ score per condition. Vertical bars represent the standard deviation. Any significant differences were labeled with their corresponding p values between conditions.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS5.5.1.1">V-E</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS5.6.2">Measures</span>
</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p1.1.1">Task Performance</span> To objectively compare users’ performance in different conditions, we record the total elapsed time for users to complete each condition, starting from the moment when the first goal state indicator was displayed in the HMD until the robot successfully reaches the last goal.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p2.1.1">Perceived Workload</span> For evaluating users’ subjective perceived task loads, we use the standard NASA-TLX questionnaire, which measures various aspects of workload <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p3.1.1">Situation Awareness</span> For evaluating users’ situation awareness of the remote environment, we used the Situation Awareness Rating Technique (SART) questionnaire <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS5.p4">
<p class="ltx_p" id="S5.SS5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p4.1.1">Cybersickness</span> To measure the amount of induced cybersickness, we use a standard Simulator Sickness Questionnaire (SSQ). The questionnaire was completed before the user study and immediately after each VR exposure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#bib.bib9" title="">9</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS6.5.1.1">V-F</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS6.6.2">Hypothesis</span>
</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">We formulate the following hypothesis concerning previously described measures and conditions:</p>
</div>
<div class="ltx_para" id="S5.SS6.p2">
<ol class="ltx_enumerate" id="S5.I3">
<li class="ltx_item" id="S5.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I3.i1.1.1.1">(H1)</span></span>
<div class="ltx_para" id="S5.I3.i1.p1">
<p class="ltx_p" id="S5.I3.i1.p1.1">Reality Fusion (C2, C3) leads to less perceived cognitive workload while improving the operator’s situation awareness and overall performance.</p>
</div>
</li>
<li class="ltx_item" id="S5.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I3.i2.1.1.1">(H2)</span></span>
<div class="ltx_para" id="S5.I3.i2.p1">
<p class="ltx_p" id="S5.I3.i2.p1.1">Egocentric teleoperation (C3) results in better user performance and lower task load than exocentric teleoperation (C2). However, due to non-self-induced motion, C3 leads to higher motion sickness than C2.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">RESULTS and DISCUSSION</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we present a summary of statistically significant results and discuss their design implications.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.4">As Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S5.F4" title="Figure 4 ‣ V-D Materials ‣ V USER STUDY EXPERIMENT ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag">4</span></a> plots, Wilcoxon signed-rank tests with Bonferroni-Holm adjustment shows that C1 (Point Cloud Only) results in significantly higher mental demand (<math alttext="Z=-2.584,p=.029,r=-.527" class="ltx_Math" display="inline" id="S6.p2.1.m1.2"><semantics id="S6.p2.1.m1.2a"><mrow id="S6.p2.1.m1.2.2.2" xref="S6.p2.1.m1.2.2.3.cmml"><mrow id="S6.p2.1.m1.1.1.1.1" xref="S6.p2.1.m1.1.1.1.1.cmml"><mi id="S6.p2.1.m1.1.1.1.1.2" xref="S6.p2.1.m1.1.1.1.1.2.cmml">Z</mi><mo id="S6.p2.1.m1.1.1.1.1.1" xref="S6.p2.1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S6.p2.1.m1.1.1.1.1.3" xref="S6.p2.1.m1.1.1.1.1.3.cmml"><mo id="S6.p2.1.m1.1.1.1.1.3a" xref="S6.p2.1.m1.1.1.1.1.3.cmml">−</mo><mn id="S6.p2.1.m1.1.1.1.1.3.2" xref="S6.p2.1.m1.1.1.1.1.3.2.cmml">2.584</mn></mrow></mrow><mo id="S6.p2.1.m1.2.2.2.3" xref="S6.p2.1.m1.2.2.3a.cmml">,</mo><mrow id="S6.p2.1.m1.2.2.2.2.2" xref="S6.p2.1.m1.2.2.2.2.3.cmml"><mrow id="S6.p2.1.m1.2.2.2.2.1.1" xref="S6.p2.1.m1.2.2.2.2.1.1.cmml"><mi id="S6.p2.1.m1.2.2.2.2.1.1.2" xref="S6.p2.1.m1.2.2.2.2.1.1.2.cmml">p</mi><mo id="S6.p2.1.m1.2.2.2.2.1.1.1" xref="S6.p2.1.m1.2.2.2.2.1.1.1.cmml">=</mo><mn id="S6.p2.1.m1.2.2.2.2.1.1.3" xref="S6.p2.1.m1.2.2.2.2.1.1.3.cmml">.029</mn></mrow><mo id="S6.p2.1.m1.2.2.2.2.2.3" xref="S6.p2.1.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S6.p2.1.m1.2.2.2.2.2.2" xref="S6.p2.1.m1.2.2.2.2.2.2.cmml"><mi id="S6.p2.1.m1.2.2.2.2.2.2.2" xref="S6.p2.1.m1.2.2.2.2.2.2.2.cmml">r</mi><mo id="S6.p2.1.m1.2.2.2.2.2.2.1" xref="S6.p2.1.m1.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S6.p2.1.m1.2.2.2.2.2.2.3" xref="S6.p2.1.m1.2.2.2.2.2.2.3.cmml"><mo id="S6.p2.1.m1.2.2.2.2.2.2.3a" xref="S6.p2.1.m1.2.2.2.2.2.2.3.cmml">−</mo><mn id="S6.p2.1.m1.2.2.2.2.2.2.3.2" xref="S6.p2.1.m1.2.2.2.2.2.2.3.2.cmml">.527</mn></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.2b"><apply id="S6.p2.1.m1.2.2.3.cmml" xref="S6.p2.1.m1.2.2.2"><csymbol cd="ambiguous" id="S6.p2.1.m1.2.2.3a.cmml" xref="S6.p2.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p2.1.m1.1.1.1.1.cmml" xref="S6.p2.1.m1.1.1.1.1"><eq id="S6.p2.1.m1.1.1.1.1.1.cmml" xref="S6.p2.1.m1.1.1.1.1.1"></eq><ci id="S6.p2.1.m1.1.1.1.1.2.cmml" xref="S6.p2.1.m1.1.1.1.1.2">𝑍</ci><apply id="S6.p2.1.m1.1.1.1.1.3.cmml" xref="S6.p2.1.m1.1.1.1.1.3"><minus id="S6.p2.1.m1.1.1.1.1.3.1.cmml" xref="S6.p2.1.m1.1.1.1.1.3"></minus><cn id="S6.p2.1.m1.1.1.1.1.3.2.cmml" type="float" xref="S6.p2.1.m1.1.1.1.1.3.2">2.584</cn></apply></apply><apply id="S6.p2.1.m1.2.2.2.2.3.cmml" xref="S6.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S6.p2.1.m1.2.2.2.2.3a.cmml" xref="S6.p2.1.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p2.1.m1.2.2.2.2.1.1.cmml" xref="S6.p2.1.m1.2.2.2.2.1.1"><eq id="S6.p2.1.m1.2.2.2.2.1.1.1.cmml" xref="S6.p2.1.m1.2.2.2.2.1.1.1"></eq><ci id="S6.p2.1.m1.2.2.2.2.1.1.2.cmml" xref="S6.p2.1.m1.2.2.2.2.1.1.2">𝑝</ci><cn id="S6.p2.1.m1.2.2.2.2.1.1.3.cmml" type="float" xref="S6.p2.1.m1.2.2.2.2.1.1.3">.029</cn></apply><apply id="S6.p2.1.m1.2.2.2.2.2.2.cmml" xref="S6.p2.1.m1.2.2.2.2.2.2"><eq id="S6.p2.1.m1.2.2.2.2.2.2.1.cmml" xref="S6.p2.1.m1.2.2.2.2.2.2.1"></eq><ci id="S6.p2.1.m1.2.2.2.2.2.2.2.cmml" xref="S6.p2.1.m1.2.2.2.2.2.2.2">𝑟</ci><apply id="S6.p2.1.m1.2.2.2.2.2.2.3.cmml" xref="S6.p2.1.m1.2.2.2.2.2.2.3"><minus id="S6.p2.1.m1.2.2.2.2.2.2.3.1.cmml" xref="S6.p2.1.m1.2.2.2.2.2.2.3"></minus><cn id="S6.p2.1.m1.2.2.2.2.2.2.3.2.cmml" type="float" xref="S6.p2.1.m1.2.2.2.2.2.2.3.2">.527</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.2c">Z=-2.584,p=.029,r=-.527</annotation><annotation encoding="application/x-llamapun" id="S6.p2.1.m1.2d">italic_Z = - 2.584 , italic_p = .029 , italic_r = - .527</annotation></semantics></math>) and a higher level of frustration (<math alttext="Z=-3.162,p=.005,r=-.645" class="ltx_Math" display="inline" id="S6.p2.2.m2.2"><semantics id="S6.p2.2.m2.2a"><mrow id="S6.p2.2.m2.2.2.2" xref="S6.p2.2.m2.2.2.3.cmml"><mrow id="S6.p2.2.m2.1.1.1.1" xref="S6.p2.2.m2.1.1.1.1.cmml"><mi id="S6.p2.2.m2.1.1.1.1.2" xref="S6.p2.2.m2.1.1.1.1.2.cmml">Z</mi><mo id="S6.p2.2.m2.1.1.1.1.1" xref="S6.p2.2.m2.1.1.1.1.1.cmml">=</mo><mrow id="S6.p2.2.m2.1.1.1.1.3" xref="S6.p2.2.m2.1.1.1.1.3.cmml"><mo id="S6.p2.2.m2.1.1.1.1.3a" xref="S6.p2.2.m2.1.1.1.1.3.cmml">−</mo><mn id="S6.p2.2.m2.1.1.1.1.3.2" xref="S6.p2.2.m2.1.1.1.1.3.2.cmml">3.162</mn></mrow></mrow><mo id="S6.p2.2.m2.2.2.2.3" xref="S6.p2.2.m2.2.2.3a.cmml">,</mo><mrow id="S6.p2.2.m2.2.2.2.2.2" xref="S6.p2.2.m2.2.2.2.2.3.cmml"><mrow id="S6.p2.2.m2.2.2.2.2.1.1" xref="S6.p2.2.m2.2.2.2.2.1.1.cmml"><mi id="S6.p2.2.m2.2.2.2.2.1.1.2" xref="S6.p2.2.m2.2.2.2.2.1.1.2.cmml">p</mi><mo id="S6.p2.2.m2.2.2.2.2.1.1.1" xref="S6.p2.2.m2.2.2.2.2.1.1.1.cmml">=</mo><mn id="S6.p2.2.m2.2.2.2.2.1.1.3" xref="S6.p2.2.m2.2.2.2.2.1.1.3.cmml">.005</mn></mrow><mo id="S6.p2.2.m2.2.2.2.2.2.3" xref="S6.p2.2.m2.2.2.2.2.3a.cmml">,</mo><mrow id="S6.p2.2.m2.2.2.2.2.2.2" xref="S6.p2.2.m2.2.2.2.2.2.2.cmml"><mi id="S6.p2.2.m2.2.2.2.2.2.2.2" xref="S6.p2.2.m2.2.2.2.2.2.2.2.cmml">r</mi><mo id="S6.p2.2.m2.2.2.2.2.2.2.1" xref="S6.p2.2.m2.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S6.p2.2.m2.2.2.2.2.2.2.3" xref="S6.p2.2.m2.2.2.2.2.2.2.3.cmml"><mo id="S6.p2.2.m2.2.2.2.2.2.2.3a" xref="S6.p2.2.m2.2.2.2.2.2.2.3.cmml">−</mo><mn id="S6.p2.2.m2.2.2.2.2.2.2.3.2" xref="S6.p2.2.m2.2.2.2.2.2.2.3.2.cmml">.645</mn></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.2b"><apply id="S6.p2.2.m2.2.2.3.cmml" xref="S6.p2.2.m2.2.2.2"><csymbol cd="ambiguous" id="S6.p2.2.m2.2.2.3a.cmml" xref="S6.p2.2.m2.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p2.2.m2.1.1.1.1.cmml" xref="S6.p2.2.m2.1.1.1.1"><eq id="S6.p2.2.m2.1.1.1.1.1.cmml" xref="S6.p2.2.m2.1.1.1.1.1"></eq><ci id="S6.p2.2.m2.1.1.1.1.2.cmml" xref="S6.p2.2.m2.1.1.1.1.2">𝑍</ci><apply id="S6.p2.2.m2.1.1.1.1.3.cmml" xref="S6.p2.2.m2.1.1.1.1.3"><minus id="S6.p2.2.m2.1.1.1.1.3.1.cmml" xref="S6.p2.2.m2.1.1.1.1.3"></minus><cn id="S6.p2.2.m2.1.1.1.1.3.2.cmml" type="float" xref="S6.p2.2.m2.1.1.1.1.3.2">3.162</cn></apply></apply><apply id="S6.p2.2.m2.2.2.2.2.3.cmml" xref="S6.p2.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S6.p2.2.m2.2.2.2.2.3a.cmml" xref="S6.p2.2.m2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p2.2.m2.2.2.2.2.1.1.cmml" xref="S6.p2.2.m2.2.2.2.2.1.1"><eq id="S6.p2.2.m2.2.2.2.2.1.1.1.cmml" xref="S6.p2.2.m2.2.2.2.2.1.1.1"></eq><ci id="S6.p2.2.m2.2.2.2.2.1.1.2.cmml" xref="S6.p2.2.m2.2.2.2.2.1.1.2">𝑝</ci><cn id="S6.p2.2.m2.2.2.2.2.1.1.3.cmml" type="float" xref="S6.p2.2.m2.2.2.2.2.1.1.3">.005</cn></apply><apply id="S6.p2.2.m2.2.2.2.2.2.2.cmml" xref="S6.p2.2.m2.2.2.2.2.2.2"><eq id="S6.p2.2.m2.2.2.2.2.2.2.1.cmml" xref="S6.p2.2.m2.2.2.2.2.2.2.1"></eq><ci id="S6.p2.2.m2.2.2.2.2.2.2.2.cmml" xref="S6.p2.2.m2.2.2.2.2.2.2.2">𝑟</ci><apply id="S6.p2.2.m2.2.2.2.2.2.2.3.cmml" xref="S6.p2.2.m2.2.2.2.2.2.2.3"><minus id="S6.p2.2.m2.2.2.2.2.2.2.3.1.cmml" xref="S6.p2.2.m2.2.2.2.2.2.2.3"></minus><cn id="S6.p2.2.m2.2.2.2.2.2.2.3.2.cmml" type="float" xref="S6.p2.2.m2.2.2.2.2.2.2.3.2">.645</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.2c">Z=-3.162,p=.005,r=-.645</annotation><annotation encoding="application/x-llamapun" id="S6.p2.2.m2.2d">italic_Z = - 3.162 , italic_p = .005 , italic_r = - .645</annotation></semantics></math>) than C3 (Egocentric Reality Fusion). Moreover, Wilcoxon signed-rank tests with Bonferroni-Holm adjustment show that participants perform significantly worse in C1 (Exocentric Reality Fusion) than C2 (Point Cloud Only) (<math alttext="Z=-2.543,p=.033,r=-.519" class="ltx_Math" display="inline" id="S6.p2.3.m3.2"><semantics id="S6.p2.3.m3.2a"><mrow id="S6.p2.3.m3.2.2.2" xref="S6.p2.3.m3.2.2.3.cmml"><mrow id="S6.p2.3.m3.1.1.1.1" xref="S6.p2.3.m3.1.1.1.1.cmml"><mi id="S6.p2.3.m3.1.1.1.1.2" xref="S6.p2.3.m3.1.1.1.1.2.cmml">Z</mi><mo id="S6.p2.3.m3.1.1.1.1.1" xref="S6.p2.3.m3.1.1.1.1.1.cmml">=</mo><mrow id="S6.p2.3.m3.1.1.1.1.3" xref="S6.p2.3.m3.1.1.1.1.3.cmml"><mo id="S6.p2.3.m3.1.1.1.1.3a" xref="S6.p2.3.m3.1.1.1.1.3.cmml">−</mo><mn id="S6.p2.3.m3.1.1.1.1.3.2" xref="S6.p2.3.m3.1.1.1.1.3.2.cmml">2.543</mn></mrow></mrow><mo id="S6.p2.3.m3.2.2.2.3" xref="S6.p2.3.m3.2.2.3a.cmml">,</mo><mrow id="S6.p2.3.m3.2.2.2.2.2" xref="S6.p2.3.m3.2.2.2.2.3.cmml"><mrow id="S6.p2.3.m3.2.2.2.2.1.1" xref="S6.p2.3.m3.2.2.2.2.1.1.cmml"><mi id="S6.p2.3.m3.2.2.2.2.1.1.2" xref="S6.p2.3.m3.2.2.2.2.1.1.2.cmml">p</mi><mo id="S6.p2.3.m3.2.2.2.2.1.1.1" xref="S6.p2.3.m3.2.2.2.2.1.1.1.cmml">=</mo><mn id="S6.p2.3.m3.2.2.2.2.1.1.3" xref="S6.p2.3.m3.2.2.2.2.1.1.3.cmml">.033</mn></mrow><mo id="S6.p2.3.m3.2.2.2.2.2.3" xref="S6.p2.3.m3.2.2.2.2.3a.cmml">,</mo><mrow id="S6.p2.3.m3.2.2.2.2.2.2" xref="S6.p2.3.m3.2.2.2.2.2.2.cmml"><mi id="S6.p2.3.m3.2.2.2.2.2.2.2" xref="S6.p2.3.m3.2.2.2.2.2.2.2.cmml">r</mi><mo id="S6.p2.3.m3.2.2.2.2.2.2.1" xref="S6.p2.3.m3.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S6.p2.3.m3.2.2.2.2.2.2.3" xref="S6.p2.3.m3.2.2.2.2.2.2.3.cmml"><mo id="S6.p2.3.m3.2.2.2.2.2.2.3a" xref="S6.p2.3.m3.2.2.2.2.2.2.3.cmml">−</mo><mn id="S6.p2.3.m3.2.2.2.2.2.2.3.2" xref="S6.p2.3.m3.2.2.2.2.2.2.3.2.cmml">.519</mn></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.3.m3.2b"><apply id="S6.p2.3.m3.2.2.3.cmml" xref="S6.p2.3.m3.2.2.2"><csymbol cd="ambiguous" id="S6.p2.3.m3.2.2.3a.cmml" xref="S6.p2.3.m3.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p2.3.m3.1.1.1.1.cmml" xref="S6.p2.3.m3.1.1.1.1"><eq id="S6.p2.3.m3.1.1.1.1.1.cmml" xref="S6.p2.3.m3.1.1.1.1.1"></eq><ci id="S6.p2.3.m3.1.1.1.1.2.cmml" xref="S6.p2.3.m3.1.1.1.1.2">𝑍</ci><apply id="S6.p2.3.m3.1.1.1.1.3.cmml" xref="S6.p2.3.m3.1.1.1.1.3"><minus id="S6.p2.3.m3.1.1.1.1.3.1.cmml" xref="S6.p2.3.m3.1.1.1.1.3"></minus><cn id="S6.p2.3.m3.1.1.1.1.3.2.cmml" type="float" xref="S6.p2.3.m3.1.1.1.1.3.2">2.543</cn></apply></apply><apply id="S6.p2.3.m3.2.2.2.2.3.cmml" xref="S6.p2.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S6.p2.3.m3.2.2.2.2.3a.cmml" xref="S6.p2.3.m3.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p2.3.m3.2.2.2.2.1.1.cmml" xref="S6.p2.3.m3.2.2.2.2.1.1"><eq id="S6.p2.3.m3.2.2.2.2.1.1.1.cmml" xref="S6.p2.3.m3.2.2.2.2.1.1.1"></eq><ci id="S6.p2.3.m3.2.2.2.2.1.1.2.cmml" xref="S6.p2.3.m3.2.2.2.2.1.1.2">𝑝</ci><cn id="S6.p2.3.m3.2.2.2.2.1.1.3.cmml" type="float" xref="S6.p2.3.m3.2.2.2.2.1.1.3">.033</cn></apply><apply id="S6.p2.3.m3.2.2.2.2.2.2.cmml" xref="S6.p2.3.m3.2.2.2.2.2.2"><eq id="S6.p2.3.m3.2.2.2.2.2.2.1.cmml" xref="S6.p2.3.m3.2.2.2.2.2.2.1"></eq><ci id="S6.p2.3.m3.2.2.2.2.2.2.2.cmml" xref="S6.p2.3.m3.2.2.2.2.2.2.2">𝑟</ci><apply id="S6.p2.3.m3.2.2.2.2.2.2.3.cmml" xref="S6.p2.3.m3.2.2.2.2.2.2.3"><minus id="S6.p2.3.m3.2.2.2.2.2.2.3.1.cmml" xref="S6.p2.3.m3.2.2.2.2.2.2.3"></minus><cn id="S6.p2.3.m3.2.2.2.2.2.2.3.2.cmml" type="float" xref="S6.p2.3.m3.2.2.2.2.2.2.3.2">.519</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.3.m3.2c">Z=-2.543,p=.033,r=-.519</annotation><annotation encoding="application/x-llamapun" id="S6.p2.3.m3.2d">italic_Z = - 2.543 , italic_p = .033 , italic_r = - .519</annotation></semantics></math>) and C3 ( <math alttext="Z=-2.971,p=.009,r=-.606" class="ltx_Math" display="inline" id="S6.p2.4.m4.2"><semantics id="S6.p2.4.m4.2a"><mrow id="S6.p2.4.m4.2.2.2" xref="S6.p2.4.m4.2.2.3.cmml"><mrow id="S6.p2.4.m4.1.1.1.1" xref="S6.p2.4.m4.1.1.1.1.cmml"><mi id="S6.p2.4.m4.1.1.1.1.2" xref="S6.p2.4.m4.1.1.1.1.2.cmml">Z</mi><mo id="S6.p2.4.m4.1.1.1.1.1" xref="S6.p2.4.m4.1.1.1.1.1.cmml">=</mo><mrow id="S6.p2.4.m4.1.1.1.1.3" xref="S6.p2.4.m4.1.1.1.1.3.cmml"><mo id="S6.p2.4.m4.1.1.1.1.3a" xref="S6.p2.4.m4.1.1.1.1.3.cmml">−</mo><mn id="S6.p2.4.m4.1.1.1.1.3.2" xref="S6.p2.4.m4.1.1.1.1.3.2.cmml">2.971</mn></mrow></mrow><mo id="S6.p2.4.m4.2.2.2.3" xref="S6.p2.4.m4.2.2.3a.cmml">,</mo><mrow id="S6.p2.4.m4.2.2.2.2.2" xref="S6.p2.4.m4.2.2.2.2.3.cmml"><mrow id="S6.p2.4.m4.2.2.2.2.1.1" xref="S6.p2.4.m4.2.2.2.2.1.1.cmml"><mi id="S6.p2.4.m4.2.2.2.2.1.1.2" xref="S6.p2.4.m4.2.2.2.2.1.1.2.cmml">p</mi><mo id="S6.p2.4.m4.2.2.2.2.1.1.1" xref="S6.p2.4.m4.2.2.2.2.1.1.1.cmml">=</mo><mn id="S6.p2.4.m4.2.2.2.2.1.1.3" xref="S6.p2.4.m4.2.2.2.2.1.1.3.cmml">.009</mn></mrow><mo id="S6.p2.4.m4.2.2.2.2.2.3" xref="S6.p2.4.m4.2.2.2.2.3a.cmml">,</mo><mrow id="S6.p2.4.m4.2.2.2.2.2.2" xref="S6.p2.4.m4.2.2.2.2.2.2.cmml"><mi id="S6.p2.4.m4.2.2.2.2.2.2.2" xref="S6.p2.4.m4.2.2.2.2.2.2.2.cmml">r</mi><mo id="S6.p2.4.m4.2.2.2.2.2.2.1" xref="S6.p2.4.m4.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S6.p2.4.m4.2.2.2.2.2.2.3" xref="S6.p2.4.m4.2.2.2.2.2.2.3.cmml"><mo id="S6.p2.4.m4.2.2.2.2.2.2.3a" xref="S6.p2.4.m4.2.2.2.2.2.2.3.cmml">−</mo><mn id="S6.p2.4.m4.2.2.2.2.2.2.3.2" xref="S6.p2.4.m4.2.2.2.2.2.2.3.2.cmml">.606</mn></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.4.m4.2b"><apply id="S6.p2.4.m4.2.2.3.cmml" xref="S6.p2.4.m4.2.2.2"><csymbol cd="ambiguous" id="S6.p2.4.m4.2.2.3a.cmml" xref="S6.p2.4.m4.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p2.4.m4.1.1.1.1.cmml" xref="S6.p2.4.m4.1.1.1.1"><eq id="S6.p2.4.m4.1.1.1.1.1.cmml" xref="S6.p2.4.m4.1.1.1.1.1"></eq><ci id="S6.p2.4.m4.1.1.1.1.2.cmml" xref="S6.p2.4.m4.1.1.1.1.2">𝑍</ci><apply id="S6.p2.4.m4.1.1.1.1.3.cmml" xref="S6.p2.4.m4.1.1.1.1.3"><minus id="S6.p2.4.m4.1.1.1.1.3.1.cmml" xref="S6.p2.4.m4.1.1.1.1.3"></minus><cn id="S6.p2.4.m4.1.1.1.1.3.2.cmml" type="float" xref="S6.p2.4.m4.1.1.1.1.3.2">2.971</cn></apply></apply><apply id="S6.p2.4.m4.2.2.2.2.3.cmml" xref="S6.p2.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S6.p2.4.m4.2.2.2.2.3a.cmml" xref="S6.p2.4.m4.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p2.4.m4.2.2.2.2.1.1.cmml" xref="S6.p2.4.m4.2.2.2.2.1.1"><eq id="S6.p2.4.m4.2.2.2.2.1.1.1.cmml" xref="S6.p2.4.m4.2.2.2.2.1.1.1"></eq><ci id="S6.p2.4.m4.2.2.2.2.1.1.2.cmml" xref="S6.p2.4.m4.2.2.2.2.1.1.2">𝑝</ci><cn id="S6.p2.4.m4.2.2.2.2.1.1.3.cmml" type="float" xref="S6.p2.4.m4.2.2.2.2.1.1.3">.009</cn></apply><apply id="S6.p2.4.m4.2.2.2.2.2.2.cmml" xref="S6.p2.4.m4.2.2.2.2.2.2"><eq id="S6.p2.4.m4.2.2.2.2.2.2.1.cmml" xref="S6.p2.4.m4.2.2.2.2.2.2.1"></eq><ci id="S6.p2.4.m4.2.2.2.2.2.2.2.cmml" xref="S6.p2.4.m4.2.2.2.2.2.2.2">𝑟</ci><apply id="S6.p2.4.m4.2.2.2.2.2.2.3.cmml" xref="S6.p2.4.m4.2.2.2.2.2.2.3"><minus id="S6.p2.4.m4.2.2.2.2.2.2.3.1.cmml" xref="S6.p2.4.m4.2.2.2.2.2.2.3"></minus><cn id="S6.p2.4.m4.2.2.2.2.2.2.3.2.cmml" type="float" xref="S6.p2.4.m4.2.2.2.2.2.2.3.2">.606</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.4.m4.2c">Z=-2.971,p=.009,r=-.606</annotation><annotation encoding="application/x-llamapun" id="S6.p2.4.m4.2d">italic_Z = - 2.971 , italic_p = .009 , italic_r = - .606</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.5">In terms of cybersickness, Multiple Wilcoxon signed-rank tests show that C3 (Egocentric Reality Fusion) leads to significantly higher nausea (<math alttext="Z=-2.539,p=.033,r=-.518" class="ltx_Math" display="inline" id="S6.p3.1.m1.2"><semantics id="S6.p3.1.m1.2a"><mrow id="S6.p3.1.m1.2.2.2" xref="S6.p3.1.m1.2.2.3.cmml"><mrow id="S6.p3.1.m1.1.1.1.1" xref="S6.p3.1.m1.1.1.1.1.cmml"><mi id="S6.p3.1.m1.1.1.1.1.2" xref="S6.p3.1.m1.1.1.1.1.2.cmml">Z</mi><mo id="S6.p3.1.m1.1.1.1.1.1" xref="S6.p3.1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S6.p3.1.m1.1.1.1.1.3" xref="S6.p3.1.m1.1.1.1.1.3.cmml"><mo id="S6.p3.1.m1.1.1.1.1.3a" xref="S6.p3.1.m1.1.1.1.1.3.cmml">−</mo><mn id="S6.p3.1.m1.1.1.1.1.3.2" xref="S6.p3.1.m1.1.1.1.1.3.2.cmml">2.539</mn></mrow></mrow><mo id="S6.p3.1.m1.2.2.2.3" xref="S6.p3.1.m1.2.2.3a.cmml">,</mo><mrow id="S6.p3.1.m1.2.2.2.2.2" xref="S6.p3.1.m1.2.2.2.2.3.cmml"><mrow id="S6.p3.1.m1.2.2.2.2.1.1" xref="S6.p3.1.m1.2.2.2.2.1.1.cmml"><mi id="S6.p3.1.m1.2.2.2.2.1.1.2" xref="S6.p3.1.m1.2.2.2.2.1.1.2.cmml">p</mi><mo id="S6.p3.1.m1.2.2.2.2.1.1.1" xref="S6.p3.1.m1.2.2.2.2.1.1.1.cmml">=</mo><mn id="S6.p3.1.m1.2.2.2.2.1.1.3" xref="S6.p3.1.m1.2.2.2.2.1.1.3.cmml">.033</mn></mrow><mo id="S6.p3.1.m1.2.2.2.2.2.3" xref="S6.p3.1.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S6.p3.1.m1.2.2.2.2.2.2" xref="S6.p3.1.m1.2.2.2.2.2.2.cmml"><mi id="S6.p3.1.m1.2.2.2.2.2.2.2" xref="S6.p3.1.m1.2.2.2.2.2.2.2.cmml">r</mi><mo id="S6.p3.1.m1.2.2.2.2.2.2.1" xref="S6.p3.1.m1.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S6.p3.1.m1.2.2.2.2.2.2.3" xref="S6.p3.1.m1.2.2.2.2.2.2.3.cmml"><mo id="S6.p3.1.m1.2.2.2.2.2.2.3a" xref="S6.p3.1.m1.2.2.2.2.2.2.3.cmml">−</mo><mn id="S6.p3.1.m1.2.2.2.2.2.2.3.2" xref="S6.p3.1.m1.2.2.2.2.2.2.3.2.cmml">.518</mn></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.1.m1.2b"><apply id="S6.p3.1.m1.2.2.3.cmml" xref="S6.p3.1.m1.2.2.2"><csymbol cd="ambiguous" id="S6.p3.1.m1.2.2.3a.cmml" xref="S6.p3.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p3.1.m1.1.1.1.1.cmml" xref="S6.p3.1.m1.1.1.1.1"><eq id="S6.p3.1.m1.1.1.1.1.1.cmml" xref="S6.p3.1.m1.1.1.1.1.1"></eq><ci id="S6.p3.1.m1.1.1.1.1.2.cmml" xref="S6.p3.1.m1.1.1.1.1.2">𝑍</ci><apply id="S6.p3.1.m1.1.1.1.1.3.cmml" xref="S6.p3.1.m1.1.1.1.1.3"><minus id="S6.p3.1.m1.1.1.1.1.3.1.cmml" xref="S6.p3.1.m1.1.1.1.1.3"></minus><cn id="S6.p3.1.m1.1.1.1.1.3.2.cmml" type="float" xref="S6.p3.1.m1.1.1.1.1.3.2">2.539</cn></apply></apply><apply id="S6.p3.1.m1.2.2.2.2.3.cmml" xref="S6.p3.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S6.p3.1.m1.2.2.2.2.3a.cmml" xref="S6.p3.1.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p3.1.m1.2.2.2.2.1.1.cmml" xref="S6.p3.1.m1.2.2.2.2.1.1"><eq id="S6.p3.1.m1.2.2.2.2.1.1.1.cmml" xref="S6.p3.1.m1.2.2.2.2.1.1.1"></eq><ci id="S6.p3.1.m1.2.2.2.2.1.1.2.cmml" xref="S6.p3.1.m1.2.2.2.2.1.1.2">𝑝</ci><cn id="S6.p3.1.m1.2.2.2.2.1.1.3.cmml" type="float" xref="S6.p3.1.m1.2.2.2.2.1.1.3">.033</cn></apply><apply id="S6.p3.1.m1.2.2.2.2.2.2.cmml" xref="S6.p3.1.m1.2.2.2.2.2.2"><eq id="S6.p3.1.m1.2.2.2.2.2.2.1.cmml" xref="S6.p3.1.m1.2.2.2.2.2.2.1"></eq><ci id="S6.p3.1.m1.2.2.2.2.2.2.2.cmml" xref="S6.p3.1.m1.2.2.2.2.2.2.2">𝑟</ci><apply id="S6.p3.1.m1.2.2.2.2.2.2.3.cmml" xref="S6.p3.1.m1.2.2.2.2.2.2.3"><minus id="S6.p3.1.m1.2.2.2.2.2.2.3.1.cmml" xref="S6.p3.1.m1.2.2.2.2.2.2.3"></minus><cn id="S6.p3.1.m1.2.2.2.2.2.2.3.2.cmml" type="float" xref="S6.p3.1.m1.2.2.2.2.2.2.3.2">.518</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.1.m1.2c">Z=-2.539,p=.033,r=-.518</annotation><annotation encoding="application/x-llamapun" id="S6.p3.1.m1.2d">italic_Z = - 2.539 , italic_p = .033 , italic_r = - .518</annotation></semantics></math>) and overall motion sickness symptoms (<math alttext="Z=-2.633,p=.025,r=-.537" class="ltx_Math" display="inline" id="S6.p3.2.m2.2"><semantics id="S6.p3.2.m2.2a"><mrow id="S6.p3.2.m2.2.2.2" xref="S6.p3.2.m2.2.2.3.cmml"><mrow id="S6.p3.2.m2.1.1.1.1" xref="S6.p3.2.m2.1.1.1.1.cmml"><mi id="S6.p3.2.m2.1.1.1.1.2" xref="S6.p3.2.m2.1.1.1.1.2.cmml">Z</mi><mo id="S6.p3.2.m2.1.1.1.1.1" xref="S6.p3.2.m2.1.1.1.1.1.cmml">=</mo><mrow id="S6.p3.2.m2.1.1.1.1.3" xref="S6.p3.2.m2.1.1.1.1.3.cmml"><mo id="S6.p3.2.m2.1.1.1.1.3a" xref="S6.p3.2.m2.1.1.1.1.3.cmml">−</mo><mn id="S6.p3.2.m2.1.1.1.1.3.2" xref="S6.p3.2.m2.1.1.1.1.3.2.cmml">2.633</mn></mrow></mrow><mo id="S6.p3.2.m2.2.2.2.3" xref="S6.p3.2.m2.2.2.3a.cmml">,</mo><mrow id="S6.p3.2.m2.2.2.2.2.2" xref="S6.p3.2.m2.2.2.2.2.3.cmml"><mrow id="S6.p3.2.m2.2.2.2.2.1.1" xref="S6.p3.2.m2.2.2.2.2.1.1.cmml"><mi id="S6.p3.2.m2.2.2.2.2.1.1.2" xref="S6.p3.2.m2.2.2.2.2.1.1.2.cmml">p</mi><mo id="S6.p3.2.m2.2.2.2.2.1.1.1" xref="S6.p3.2.m2.2.2.2.2.1.1.1.cmml">=</mo><mn id="S6.p3.2.m2.2.2.2.2.1.1.3" xref="S6.p3.2.m2.2.2.2.2.1.1.3.cmml">.025</mn></mrow><mo id="S6.p3.2.m2.2.2.2.2.2.3" xref="S6.p3.2.m2.2.2.2.2.3a.cmml">,</mo><mrow id="S6.p3.2.m2.2.2.2.2.2.2" xref="S6.p3.2.m2.2.2.2.2.2.2.cmml"><mi id="S6.p3.2.m2.2.2.2.2.2.2.2" xref="S6.p3.2.m2.2.2.2.2.2.2.2.cmml">r</mi><mo id="S6.p3.2.m2.2.2.2.2.2.2.1" xref="S6.p3.2.m2.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S6.p3.2.m2.2.2.2.2.2.2.3" xref="S6.p3.2.m2.2.2.2.2.2.2.3.cmml"><mo id="S6.p3.2.m2.2.2.2.2.2.2.3a" xref="S6.p3.2.m2.2.2.2.2.2.2.3.cmml">−</mo><mn id="S6.p3.2.m2.2.2.2.2.2.2.3.2" xref="S6.p3.2.m2.2.2.2.2.2.2.3.2.cmml">.537</mn></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.2.m2.2b"><apply id="S6.p3.2.m2.2.2.3.cmml" xref="S6.p3.2.m2.2.2.2"><csymbol cd="ambiguous" id="S6.p3.2.m2.2.2.3a.cmml" xref="S6.p3.2.m2.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p3.2.m2.1.1.1.1.cmml" xref="S6.p3.2.m2.1.1.1.1"><eq id="S6.p3.2.m2.1.1.1.1.1.cmml" xref="S6.p3.2.m2.1.1.1.1.1"></eq><ci id="S6.p3.2.m2.1.1.1.1.2.cmml" xref="S6.p3.2.m2.1.1.1.1.2">𝑍</ci><apply id="S6.p3.2.m2.1.1.1.1.3.cmml" xref="S6.p3.2.m2.1.1.1.1.3"><minus id="S6.p3.2.m2.1.1.1.1.3.1.cmml" xref="S6.p3.2.m2.1.1.1.1.3"></minus><cn id="S6.p3.2.m2.1.1.1.1.3.2.cmml" type="float" xref="S6.p3.2.m2.1.1.1.1.3.2">2.633</cn></apply></apply><apply id="S6.p3.2.m2.2.2.2.2.3.cmml" xref="S6.p3.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S6.p3.2.m2.2.2.2.2.3a.cmml" xref="S6.p3.2.m2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p3.2.m2.2.2.2.2.1.1.cmml" xref="S6.p3.2.m2.2.2.2.2.1.1"><eq id="S6.p3.2.m2.2.2.2.2.1.1.1.cmml" xref="S6.p3.2.m2.2.2.2.2.1.1.1"></eq><ci id="S6.p3.2.m2.2.2.2.2.1.1.2.cmml" xref="S6.p3.2.m2.2.2.2.2.1.1.2">𝑝</ci><cn id="S6.p3.2.m2.2.2.2.2.1.1.3.cmml" type="float" xref="S6.p3.2.m2.2.2.2.2.1.1.3">.025</cn></apply><apply id="S6.p3.2.m2.2.2.2.2.2.2.cmml" xref="S6.p3.2.m2.2.2.2.2.2.2"><eq id="S6.p3.2.m2.2.2.2.2.2.2.1.cmml" xref="S6.p3.2.m2.2.2.2.2.2.2.1"></eq><ci id="S6.p3.2.m2.2.2.2.2.2.2.2.cmml" xref="S6.p3.2.m2.2.2.2.2.2.2.2">𝑟</ci><apply id="S6.p3.2.m2.2.2.2.2.2.2.3.cmml" xref="S6.p3.2.m2.2.2.2.2.2.2.3"><minus id="S6.p3.2.m2.2.2.2.2.2.2.3.1.cmml" xref="S6.p3.2.m2.2.2.2.2.2.2.3"></minus><cn id="S6.p3.2.m2.2.2.2.2.2.2.3.2.cmml" type="float" xref="S6.p3.2.m2.2.2.2.2.2.2.3.2">.537</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.2.m2.2c">Z=-2.633,p=.025,r=-.537</annotation><annotation encoding="application/x-llamapun" id="S6.p3.2.m2.2d">italic_Z = - 2.633 , italic_p = .025 , italic_r = - .537</annotation></semantics></math>) than C1 (Point Cloud Only). Moreover, C3 (Egocentric Reality Fusion) also leads to significantly higher overall motion sickness than C2 (Exocentric Reality Fusion) (<math alttext="Z=-2.565,p=.031,r=-.524" class="ltx_Math" display="inline" id="S6.p3.3.m3.2"><semantics id="S6.p3.3.m3.2a"><mrow id="S6.p3.3.m3.2.2.2" xref="S6.p3.3.m3.2.2.3.cmml"><mrow id="S6.p3.3.m3.1.1.1.1" xref="S6.p3.3.m3.1.1.1.1.cmml"><mi id="S6.p3.3.m3.1.1.1.1.2" xref="S6.p3.3.m3.1.1.1.1.2.cmml">Z</mi><mo id="S6.p3.3.m3.1.1.1.1.1" xref="S6.p3.3.m3.1.1.1.1.1.cmml">=</mo><mrow id="S6.p3.3.m3.1.1.1.1.3" xref="S6.p3.3.m3.1.1.1.1.3.cmml"><mo id="S6.p3.3.m3.1.1.1.1.3a" xref="S6.p3.3.m3.1.1.1.1.3.cmml">−</mo><mn id="S6.p3.3.m3.1.1.1.1.3.2" xref="S6.p3.3.m3.1.1.1.1.3.2.cmml">2.565</mn></mrow></mrow><mo id="S6.p3.3.m3.2.2.2.3" xref="S6.p3.3.m3.2.2.3a.cmml">,</mo><mrow id="S6.p3.3.m3.2.2.2.2.2" xref="S6.p3.3.m3.2.2.2.2.3.cmml"><mrow id="S6.p3.3.m3.2.2.2.2.1.1" xref="S6.p3.3.m3.2.2.2.2.1.1.cmml"><mi id="S6.p3.3.m3.2.2.2.2.1.1.2" xref="S6.p3.3.m3.2.2.2.2.1.1.2.cmml">p</mi><mo id="S6.p3.3.m3.2.2.2.2.1.1.1" xref="S6.p3.3.m3.2.2.2.2.1.1.1.cmml">=</mo><mn id="S6.p3.3.m3.2.2.2.2.1.1.3" xref="S6.p3.3.m3.2.2.2.2.1.1.3.cmml">.031</mn></mrow><mo id="S6.p3.3.m3.2.2.2.2.2.3" xref="S6.p3.3.m3.2.2.2.2.3a.cmml">,</mo><mrow id="S6.p3.3.m3.2.2.2.2.2.2" xref="S6.p3.3.m3.2.2.2.2.2.2.cmml"><mi id="S6.p3.3.m3.2.2.2.2.2.2.2" xref="S6.p3.3.m3.2.2.2.2.2.2.2.cmml">r</mi><mo id="S6.p3.3.m3.2.2.2.2.2.2.1" xref="S6.p3.3.m3.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S6.p3.3.m3.2.2.2.2.2.2.3" xref="S6.p3.3.m3.2.2.2.2.2.2.3.cmml"><mo id="S6.p3.3.m3.2.2.2.2.2.2.3a" xref="S6.p3.3.m3.2.2.2.2.2.2.3.cmml">−</mo><mn id="S6.p3.3.m3.2.2.2.2.2.2.3.2" xref="S6.p3.3.m3.2.2.2.2.2.2.3.2.cmml">.524</mn></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.3.m3.2b"><apply id="S6.p3.3.m3.2.2.3.cmml" xref="S6.p3.3.m3.2.2.2"><csymbol cd="ambiguous" id="S6.p3.3.m3.2.2.3a.cmml" xref="S6.p3.3.m3.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p3.3.m3.1.1.1.1.cmml" xref="S6.p3.3.m3.1.1.1.1"><eq id="S6.p3.3.m3.1.1.1.1.1.cmml" xref="S6.p3.3.m3.1.1.1.1.1"></eq><ci id="S6.p3.3.m3.1.1.1.1.2.cmml" xref="S6.p3.3.m3.1.1.1.1.2">𝑍</ci><apply id="S6.p3.3.m3.1.1.1.1.3.cmml" xref="S6.p3.3.m3.1.1.1.1.3"><minus id="S6.p3.3.m3.1.1.1.1.3.1.cmml" xref="S6.p3.3.m3.1.1.1.1.3"></minus><cn id="S6.p3.3.m3.1.1.1.1.3.2.cmml" type="float" xref="S6.p3.3.m3.1.1.1.1.3.2">2.565</cn></apply></apply><apply id="S6.p3.3.m3.2.2.2.2.3.cmml" xref="S6.p3.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S6.p3.3.m3.2.2.2.2.3a.cmml" xref="S6.p3.3.m3.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S6.p3.3.m3.2.2.2.2.1.1.cmml" xref="S6.p3.3.m3.2.2.2.2.1.1"><eq id="S6.p3.3.m3.2.2.2.2.1.1.1.cmml" xref="S6.p3.3.m3.2.2.2.2.1.1.1"></eq><ci id="S6.p3.3.m3.2.2.2.2.1.1.2.cmml" xref="S6.p3.3.m3.2.2.2.2.1.1.2">𝑝</ci><cn id="S6.p3.3.m3.2.2.2.2.1.1.3.cmml" type="float" xref="S6.p3.3.m3.2.2.2.2.1.1.3">.031</cn></apply><apply id="S6.p3.3.m3.2.2.2.2.2.2.cmml" xref="S6.p3.3.m3.2.2.2.2.2.2"><eq id="S6.p3.3.m3.2.2.2.2.2.2.1.cmml" xref="S6.p3.3.m3.2.2.2.2.2.2.1"></eq><ci id="S6.p3.3.m3.2.2.2.2.2.2.2.cmml" xref="S6.p3.3.m3.2.2.2.2.2.2.2">𝑟</ci><apply id="S6.p3.3.m3.2.2.2.2.2.2.3.cmml" xref="S6.p3.3.m3.2.2.2.2.2.2.3"><minus id="S6.p3.3.m3.2.2.2.2.2.2.3.1.cmml" xref="S6.p3.3.m3.2.2.2.2.2.2.3"></minus><cn id="S6.p3.3.m3.2.2.2.2.2.2.3.2.cmml" type="float" xref="S6.p3.3.m3.2.2.2.2.2.2.3.2">.524</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.3.m3.2c">Z=-2.565,p=.031,r=-.524</annotation><annotation encoding="application/x-llamapun" id="S6.p3.3.m3.2d">italic_Z = - 2.565 , italic_p = .031 , italic_r = - .524</annotation></semantics></math>). A repeated-measures ANOVA test and Post-hoc tests with adjustment show that participants obtained significantly higher situation awareness in C3 than in C1 (<math alttext="p=0.015" class="ltx_Math" display="inline" id="S6.p3.4.m4.1"><semantics id="S6.p3.4.m4.1a"><mrow id="S6.p3.4.m4.1.1" xref="S6.p3.4.m4.1.1.cmml"><mi id="S6.p3.4.m4.1.1.2" xref="S6.p3.4.m4.1.1.2.cmml">p</mi><mo id="S6.p3.4.m4.1.1.1" xref="S6.p3.4.m4.1.1.1.cmml">=</mo><mn id="S6.p3.4.m4.1.1.3" xref="S6.p3.4.m4.1.1.3.cmml">0.015</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.4.m4.1b"><apply id="S6.p3.4.m4.1.1.cmml" xref="S6.p3.4.m4.1.1"><eq id="S6.p3.4.m4.1.1.1.cmml" xref="S6.p3.4.m4.1.1.1"></eq><ci id="S6.p3.4.m4.1.1.2.cmml" xref="S6.p3.4.m4.1.1.2">𝑝</ci><cn id="S6.p3.4.m4.1.1.3.cmml" type="float" xref="S6.p3.4.m4.1.1.3">0.015</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.4.m4.1c">p=0.015</annotation><annotation encoding="application/x-llamapun" id="S6.p3.4.m4.1d">italic_p = 0.015</annotation></semantics></math>) as well as higher situation awareness in C2 than in C1 (<math alttext="p=0.003" class="ltx_Math" display="inline" id="S6.p3.5.m5.1"><semantics id="S6.p3.5.m5.1a"><mrow id="S6.p3.5.m5.1.1" xref="S6.p3.5.m5.1.1.cmml"><mi id="S6.p3.5.m5.1.1.2" xref="S6.p3.5.m5.1.1.2.cmml">p</mi><mo id="S6.p3.5.m5.1.1.1" xref="S6.p3.5.m5.1.1.1.cmml">=</mo><mn id="S6.p3.5.m5.1.1.3" xref="S6.p3.5.m5.1.1.3.cmml">0.003</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.5.m5.1b"><apply id="S6.p3.5.m5.1.1.cmml" xref="S6.p3.5.m5.1.1"><eq id="S6.p3.5.m5.1.1.1.cmml" xref="S6.p3.5.m5.1.1.1"></eq><ci id="S6.p3.5.m5.1.1.2.cmml" xref="S6.p3.5.m5.1.1.2">𝑝</ci><cn id="S6.p3.5.m5.1.1.3.cmml" type="float" xref="S6.p3.5.m5.1.1.3">0.003</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.5.m5.1c">p=0.003</annotation><annotation encoding="application/x-llamapun" id="S6.p3.5.m5.1d">italic_p = 0.003</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">In the post-study questionnaire, 12 participants indicated clear preferences for C2, 8 participants indicated clear preferences for C3, 2 indicated equal preferences for C2 and C3, and 2 did not indicate any clear preferences.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Improved Performance with Reality Fusion (H1)</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">As plotted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01225v1#S5.F4" title="Figure 4 ‣ V-D Materials ‣ V USER STUDY EXPERIMENT ‣ Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion"><span class="ltx_text ltx_ref_tag">4</span></a>, statistical analysis confirmed that reality fusion (C2, C3) results in significantly higher situation awareness and better task performance, partially confirming H1. According to participants’ qualitative feedback, reality fusion enables “<span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.1">a better overview/understanding of the whole environment</span>” (N=7), makes it easier to “<span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.2">determine the robot’s global position</span>” (N=3), and therefore makes it “<span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.3">easier to plan routes</span>” (N=4). Without reality fusion, participants were “<span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.4">not sure if there was a direct route between the robot and the target</span>” and therefore tend to “<span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.5">scan around the environment much more</span>”, resulting in worse task performance.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">However, although C2 and C3 lead to lower overall mental demand, physical demand, and frustration, a comparison between C1 and C2 alone does not reveal a significant difference in perceived task load. Therefore, this part of H1 can not be confirmed. This indicates that displaying a global 3D map may introduce extra cognitive demands to users while these extra efforts help them achieve better task performance and gain more situational awareness.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">In terms of SSQ, participants experienced only none to minor motion sickness in both C1 and C2, despite that users were exposed to more complex virtual environments in C2. This is attributed to our technical implementation, which is highly optimized for high-framerate and high-resolution rendering with low video streaming latency. Therefore, it is also safe to conclude that by using reality fusion, users can teleoperate robots in VR in real-time without discomfort.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">Exocentric and Egocentric Comparison (H2)</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In answering RQ2, we compare the results of different measures between C2 and C3. Although C3 leads to lower overall mental demand, physical demand, frustration, and better task performance than C2, the differences were not significant. Therefore, the first part of H2 can not be confirmed. Nonetheless, participants mentioned in the qualitative feedback that it is more demanding to have to control both the robot’s movement and their own movements in the virtual environment (N=4). In addition, they found the egocentric teleoperation mode (C3) more immersive and it helps them “<span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1">pay more attention to the real-time point cloud</span>” and become “<span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.2">more aware of the (robot’s) environment</span>”.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">The post-study qualitative questionnaire also revealed participants’ split preferences for the two teleoperation modes, with those who preferred C3 believing that egocentric teleoperation is more “<span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.1">natural</span>” and “<span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.2">intuitive</span>” (N=2), while others preferring moving freely (N=3) and looking for the best perspective (e.g. a top-down view) on their own.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">In terms of SSQ, as expected, participants reported stronger motion sickness symptoms in the egocentric mode due to continuous non-self-induced motion, with a significant difference between C3 and C2 in total SSQ score confirming the second part of H2. This indicates that while the egocentric teleoperation mode presents certain advantages, it might not be suitable for long-duration teleoperation tasks and could be offered as an option the user could switch to, rather than the main teleoperation mode.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">CONCLUSION and FUTURE WORK</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we presented a novel immersive robot teleoperation framework that allows natural, intuitive, and robust remote control of mobile robots in complex semi-structured environments through the reality fusion technique. Our open-source implementation includes a high-performance Unity application for high resolution, photorealistic 3DGS VR rendering, low-latency point cloud streaming, and intuitive mobile robot motion control, as well as a telepresence mobile robot system design that can be easily replicated. We thoroughly evaluated various human aspects of our framework with 24 participants and demonstrated the significant improvement of reality fusion in objective task performance as well as perceived situation awareness.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">In future work, we encourage researchers to enhance reality fusion methods by integrating dynamic SLAM capturing techniques, allowing for real-time updates of the 3DGS global environments to accommodate dynamic changes. A major limitation of the current system is its reliance on the OpenCR board’s odometry for tracking the robot’s poses, which can lead to motion drift and accumulated tracking errors. Additionally, the current system uses the camera’s built-in tracking for projecting point clouds rather than the robot’s odometry. This can result in visual misalignment and motion control errors, particularly in environments with uneven floors where the robot’s odometry tracking may fail. To improve tracking accuracy, we recommend incorporating unified adaptive tracking and control methods for both the robot and the stereoscopic camera. This can be achieved using vision markers, dynamic SLAM, or optical tracking systems in future implementations.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Accot and S. Zhai.

</span>
<span class="ltx_bibblock">Performance evaluation of input devices in trajectory-based tasks: an application of the steering law.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">International Conference on Human Factors in Computing Systems</span>, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
G. Adamides, G. Christou, C. Katsanos, M. N. Xenos, and T. Hadzilacos.

</span>
<span class="ltx_bibblock">Usability guidelines for the design of robot teleoperation: A taxonomy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">IEEE Transactions on Human-Machine Systems</span>, 45:256–262, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Allspaw, L. Heinold, and H. A. Yanco.

</span>
<span class="ltx_bibblock">Design of virtual reality for humanoid robots with inspiration from video games.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Interacción</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
CLARTE-LAB.

</span>
<span class="ltx_bibblock">Gaussian splatting vr viewer.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/clarte53/GaussianSplattingVRViewerUnity" title="">https://github.com/clarte53/GaussianSplattingVRViewerUnity</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
F. Ferland, F. Pomerleau, C. T. L. Dinh, and F. Michaud.

</span>
<span class="ltx_bibblock">Egocentric and exocentric teleoperation interface using real-time, 3d video projection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</span>, pp. 37–44, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. G. Hart and L. E. Staveland.

</span>
<span class="ltx_bibblock">Development of nasa-tlx (task load index): Results of empirical and theoretical research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Advances in psychology</span>, 52:139–183, 1988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. Hofbauer, C. B. Kuhn, G. Petrovic, and E. G. Steinbach.

</span>
<span class="ltx_bibblock">Adaptive multi-view live video streaming for teledriving using a single hardware encoder.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">2020 IEEE International Symposium on Multimedia (ISM)</span>, pp. 9–16, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. J. Jorgensen, M. Wonsick, M. Paterson, A. Watson, I. Chase, and J. S. Mehling.

</span>
<span class="ltx_bibblock">Cockpit interface for locomotion and manipulation control of the nasa valkyrie humanoid in virtual reality (vr).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">NASA New Technology Report (NTR): MSC-27278-1</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. S. Kennedy, N. E. Lane, K. S. Berbaum, and M. G. Lilienthal.

</span>
<span class="ltx_bibblock">Simulator sickness questionnaire: An enhanced method for quantifying simulator sickness.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">The international journal of aviation psychology</span>, 3(3):203–220, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. Kerbl, G. Kopanas, T. Leimkuehler, and G. Drettakis.

</span>
<span class="ltx_bibblock">3d gaussian splatting for real-time radiance field rendering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">ACM Transactions on Graphics (TOG)</span>, 42:1 – 14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. Li, S. Schmidt, R. Bacher, W. P. Leemans, and F. Steinicke.

</span>
<span class="ltx_bibblock">Mixed reality tunneling effects for stereoscopic untethered video-see-through head-mounted displays.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</span>, pp. 44–53, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng.

</span>
<span class="ltx_bibblock">Nerf: Representing scenes as neural radiance fields for view synthesis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Commun. ACM</span>, 65:99–106, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Pranckevičius.

</span>
<span class="ltx_bibblock">Gaussian splatting playground in unity.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/aras-p/UnityGaussianSplatting" title="">https://github.com/aras-p/UnityGaussianSplatting</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. L. Schönberger and J.-M. Frahm.

</span>
<span class="ltx_bibblock">Structure-from-motion revisited.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
P. Stotko, S. Krumpen, M. Schwarz, C. Lenz, S. Behnke, R. Klein, and M. Weinmann.

</span>
<span class="ltx_bibblock">A vr system for immersive teleoperation and live exploration with a mobile robot.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, pp. 3630–3637, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K. A. Szczurek, R. M. Prades, E. Matheson, J. Rodriguez-Nogueira, and M. D. Castro.

</span>
<span class="ltx_bibblock">Mixed reality human–robot interface with adaptive communications congestion control for the teleoperation of mobile redundant manipulators in hazardous environments.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">IEEE Access</span>, 10:87182–87216, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
R. M. Taylor.

</span>
<span class="ltx_bibblock">Situational awareness rating technique (sart): The development of a tool for aircrew systems design.

</span>
<span class="ltx_bibblock">2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y. T. Tefera, D. Mazzanti, S. Anastasi, D. G. Caldwell, P. Fiorini, and N. Deshpande.

</span>
<span class="ltx_bibblock">Towards gaze-contingent visualization of real-time 3d reconstructed remote scenes in mixed reality.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2023 6th International Conference on Advances in Robotics</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. E. Walker, M. Gramopadhye, B. Ikeda, J. O. Burns, and D. A. Szafir.

</span>
<span class="ltx_bibblock">The cyber-physical control room: A mixed reality interface for mobile robot teleoperation and human-robot teaming.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D. Wei, B. Huang, and Q. Li.

</span>
<span class="ltx_bibblock">Multi-view merging for robot teleoperation with virtual reality.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">IEEE Robotics and Automation Letters</span>, 6:8537–8544, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T. Williams, D. A. Szafir, T. Chakraborti, and H. B. Amor.

</span>
<span class="ltx_bibblock">Virtual, augmented, and mixed reality for human-robot interaction (vam-hri).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li.

</span>
<span class="ltx_bibblock">Gs-slam: Dense visual slam with 3d gaussian splatting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">ArXiv</span>, abs/2311.11700, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Zhang, E. Langbehn, D. Krupke, N. Katzakis, and F. Steinicke.

</span>
<span class="ltx_bibblock">Detection thresholds for rotation and translation gains in 360° video-based telepresence systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 24:1671–1680, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug  2 12:28:02 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
