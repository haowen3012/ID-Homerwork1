<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom</title>
<!--Generated on Thu Sep 21 02:18:56 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="https://browse.arxiv.org/latexml/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="https://browse.arxiv.org/latexml/styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://browse.arxiv.org/latexml/addons.js"></script>
<script src="https://browse.arxiv.org/latexml/feedbackOverlay.js"></script>
<meta content="
DIKW,  multimodality,  data fusion,  p4 medicine,  smart healthcare
" lang="en" name="keywords"/>
</head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="I Introduction ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Modalities in Smart Healthcare</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="II-A Electronic Health Records (EHRs) ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Electronic Health Records (EHRs)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="II-B Wearable Devices ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Wearable Devices</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS3" title="II-C Sensor data ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Sensor data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS4" title="II-D Medical Imaging ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Medical Imaging</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS5" title="II-E Genomic data ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-E</span> </span><span class="ltx_text ltx_font_italic">Genomic data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS6" title="II-F Environmental data ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-F</span> </span><span class="ltx_text ltx_font_italic">Environmental data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS7" title="II-G Behavioural data ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-G</span> </span><span class="ltx_text ltx_font_italic">Behavioural data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS8" title="II-H Multimodality Data ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-H</span> </span><span class="ltx_text ltx_font_italic">Multimodality Data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS9" title="II-I Datasets for Multimodal Fusion for Smart Healthcare ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-I</span> </span><span class="ltx_text ltx_font_italic">Datasets for Multimodal Fusion for Smart Healthcare</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">SOTA Techniques in Multimodal Fusion for Smart Healthcare</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="III-A Feature selection ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Feature selection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="III-B Rule-based systems ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Rule-based systems</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="III-C Machine Learning ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Machine Learning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS4" title="III-D Deep learning ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Deep learning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS5" title="III-E Natural Language Processing ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Natural Language Processing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS6" title="III-F Taxonomy of Approaches in Multimodal Fusion ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-F</span> </span><span class="ltx_text ltx_font_italic">Taxonomy of Approaches in Multimodal Fusion</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="IV Challenges in adopting Multimodal Fusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Challenges in adopting Multimodal Fusion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="IV-A Data quality and interoperability ‣ IV Challenges in adopting Multimodal Fusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Data quality and interoperability</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="IV-B Privacy and security ‣ IV Challenges in adopting Multimodal Fusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Privacy and security</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS3" title="IV-C Data processing and analysis ‣ IV Challenges in adopting Multimodal Fusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Data processing and analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS4" title="IV-D Clinical integration and adoption ‣ IV Challenges in adopting Multimodal Fusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Clinical integration and adoption</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS5" title="IV-E Ethical considerations ‣ IV Challenges in adopting Multimodal Fusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_italic">Ethical considerations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS6" title="IV-F Interpretation of results ‣ IV Challenges in adopting Multimodal Fusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span> </span><span class="ltx_text ltx_font_italic">Interpretation of results</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="V DIKW Fusion Framework with Multimodality ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">DIKW Fusion Framework with Multimodality</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S6" title="VI Future Directions of DIKW Fusion in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Future Directions of DIKW Fusion in Smart Healthcare</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS1" title="VI-A Predictive Healthcare ‣ VI Future Directions of DIKW Fusion in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Predictive Healthcare</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS2" title="VI-B Preventive Healthcare ‣ VI Future Directions of DIKW Fusion in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Preventive Healthcare</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS3" title="VI-C Personalized Healthcare ‣ VI Future Directions of DIKW Fusion in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Personalized Healthcare</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS4" title="VI-D Participatory Healthcare ‣ VI Future Directions of DIKW Fusion in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-D</span> </span><span class="ltx_text ltx_font_italic">Participatory Healthcare</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S7" title="VII Conclusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_ERROR undefined" id="id1">\usetikzlibrary</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">shapes, arrows, positioning





</p>
</div>
<h1 class="ltx_title ltx_title_document">A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thanveer Shaik, Xiaohui Tao, Lin Li, Haoran Xie, Juan D. Vel ̵́asquez
</span><span class="ltx_author_notes">Thanveer Shaik, Xiaohui Tao are with
the School of Mathematics, Physics &amp; Computing, University of Southern Queensland, Toowoomba, Queensland, Australia (e-mail: Thanveer.Shaik@usq.edu.au, Xiaohui.Tao@usq.edu.au).Lin Li is with the School of Computer and Artificial Intelligence, Wuhan University of Technology, China (e-mail: cathylilin@whut.edu.cn)Haoran Xie is with the Department of Computing and Decision Sciences, Lingnan University, Tuen Mun, Hong Kong (e-mail: hrxie@ln.edu.hk)Juan D. Vel ̵́asquez is with Industrial Engineering Department, University of Chile, Chile (e-mail: Rajendra.Acharya@usq.edu.au).</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Multimodal medical data fusion has emerged as a transformative approach in smart healthcare, enabling a comprehensive understanding of patient health and personalized treatment plans. In this paper, a journey from data to information to knowledge to wisdom (DIKW) is explored through multimodal fusion for smart healthcare. We present a comprehensive review of multimodal medical data fusion focused on the integration of various data modalities. The review explores different approaches such as feature selection, rule-based systems, machine learning, deep learning, and natural language processing, for fusing and analyzing multimodal data. This paper also highlights the challenges associated with multimodal fusion in healthcare. By synthesizing the reviewed frameworks and theories, it proposes a generic framework for multimodal medical data fusion that aligns with the DIKW model. Moreover, it discusses future directions related to the four pillars of healthcare: Predictive, Preventive, Personalized, and Participatory approaches. The components of the comprehensive survey presented in this paper form the foundation for more successful implementation of multimodal fusion in smart healthcare. Our findings can guide researchers and practitioners in leveraging the power of multimodal fusion with state-of-the-art approaches to revolutionize healthcare and improve patient outcomes.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
DIKW, multimodality, data fusion, p4 medicine, smart healthcare

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the realm of smart healthcare, where cutting-edge technologies and data-driven approaches are revolutionizing the field, the integration of multimodal data has emerged as a transformative tool to enhance decision-making and improve patient outcomes. This paper presents a comprehensive exploration of multimodal medical data fusion for smart healthcare, illustrating the journey from raw data to actionable insights through the four-level pyramid shown in Fig. <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The Data Information Knowledge Wisdom (DIKW) model is a conceptual framework that illustrates the hierarchical progression of data into wisdom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>. Through its process, raw data is transformed into meaningful information, knowledge, and ultimately wisdom, which can be used for informed decision-making and problem-solving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib2" title="">2</a>]</cite>. The DIKW model recognizes that data alone is not sufficient to drive insights and actions. Instead, data needs to be processed, organized, and contextualized to extract valuable information. This information is then synthesized and combined with existing knowledge to gain understanding, leading to the development of knowledge. Knowledge, in turn, can then be applied in practical situations to make informed decisions and solve complex problems, resulting in wisdom.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="583" id="S1.F1.g1" src="extracted/5125724/pyramid2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>DIKW Fusion conceptual model</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">At the base of the pyramid in Fig. <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">1</span></a>, we have the Data level, which encompasses diverse sources of data such as Electronic Health Records (EHRs), medical imaging, wearable devices, genomic data, sensor data, environmental data, and behavioral data. This raw data serves as the foundation for subsequent analysis and interpretation. Moving up the pyramid, we reach the Information level, where the raw data undergoes processing, organization, and structuring to derive meaningful and contextualized information. For instance, heart rate data from a wearable device can be processed to determine average resting heart rates, activity levels, and potential anomalies.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The Knowledge level, situated above Information, represents the interconnected structure of the organized data from various sources. By establishing relationships and connections between entities like patients, diseases, or medical treatments, the Knowledge level enables the identification of patterns, trends, and correlations. It facilitates a holistic understanding of the data and serves as a powerful tool for generating insights.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Finally, at the pinnacle of the pyramid, we have the Wisdom level. This is where actionable insights are derived from the Knowledge level, allowing informed decision-making, prediction of future outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib3" title="">3</a>]</cite>, and a deeper understanding of complex phenomena. These insights enable personalized treatment plans, predictions about disease progression, and the identification of risk factors.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">It’s important to point out a key feature of the model, its circular structure, shown by the arrows in Fig. <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">1</span></a>. These arrows indicate that combining different types of data assists in the progression of Data to Information, then to Knowledge and Wisdom. This cyclical nature adds flexibility to the model, allowing for constant updates and improvements in how data is processed. In this sense, reaching the level of Wisdom helps to fine-tune the steps and methods used at earlier stages, making future data collection, information gathering, and knowledge creation more effective.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">This paper further explores different approaches such as feature selection, rule-based systems, machine learning, deep learning, and natural language processing for multimodal fusion. It also addresses challenges related to data quality, privacy, security, processing, analysis, clinical integration, ethics, and interpretation of results. With its emphasis on the transformative potential of multimodal medical data fusion, this paper sets the stage for future research and advancements in the field of smart healthcare, and paves the way for improved patient care outcomes and personalized healthcare solutions.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">The following are the key contributions of this paper:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">The application and adaptation of the existing DIKW conceptual model to describe the journey of data to information to knowledge to wisdom in the context of multimodality fusion for smart healthcare.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A taxonomy that organizes state-of-the-art techniques in multimodality fusion with the DIKW conceptual model.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">A proposed generic DIKW techniques framework for smart healthcare, that not only highlights the current efforts, but also provides a vision for its future evolution.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">A review of the challenges and recommended solutions associated with multimodal fusion, informed by the existing DIKW conceptual model and the proposed framework, to guide future research directions.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">The rest of this paper is organized as follows: Section <a class="ltx_ref" href="#S2" title="II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">II</span></a> delves into the representation of data in multimodal applications. Section <a class="ltx_ref" href="#S3" title="III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">III</span></a> explores various approaches for integrating information from multiple modalities, then outlines the proposed taxonomy. Section <a class="ltx_ref" href="#S4" title="IV Challenges in adopting Multimodal Fusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">IV</span></a> examines the challenges and trends associated with multimodal fusion. Section <a class="ltx_ref" href="#S5" title="V DIKW Fusion Framework with Multimodality ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">V</span></a> presents a generic framework for multimodal fusion that aligns with the DIKW model. In section <a class="ltx_ref" href="#S6" title="VI Future Directions of DIKW Fusion in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">VI</span></a>, we outline the future directions for multimodal fusion in smart healthcare, with a particular focus on the 4Ps of healthcare. Finally, the paper concludes in Section <a class="ltx_ref" href="#S7" title="VII Conclusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">VII</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Modalities in Smart Healthcare</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">There are various data modalities in healthcare, such as EHRs, Medical Imaging, Wearable Devices, Genomic data, Sensor data, Environmental data, and Behavioral data as shown in Fig. <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">2</span></a>. These modalities contain unstructured raw data specific to their respective formats. As the data is processed, it is transformed into meaningful information through the involvement of techniques such as structuring EHRs, feature extraction from Medical Imaging, and analysis of wearable device data.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="578" id="S2.F2.g1" src="extracted/5125724/overview_multimodality.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of Multimodality Fusion for Smart Healthcare</figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Electronic Health Records (EHRs)</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">EHRs serve as a central repository of medical data for healthcare providers, the adoption of which has led to a surge in the amount of intricate patient data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib4" title="">4</a>]</cite>. These datasets, although extensive and tailored to each patient, are often fragmented and may lack organization. They encompass diverse variables like medications, laboratory values, imaging results, physiological measurements, and historical notes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib5" title="">5</a>]</cite>, which lead to increased complexity in analysis. Machine learning (ML) provides a potential solution to this complexity by enabling the exploration of intricate relationships among the diverse variables present in EHR datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">EHRs play a crucial role in multimodal fusion systems within smart healthcare, especially for multidisciplinary and life-threatening diseases like diabetes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib7" title="">7</a>]</cite>. However, managing and analyzing unstructured data collected from sensors and EHRs is challenging. Data fusion is crucial for accurate predictions, and deep learning approaches are effective for larger healthcare datasets. Healthcare datasets can be enhanced by collecting patients’ data through wearable sensors and EHRs. An ensemble ML approach is employed to develop a recommendation system for accurate prediction and timely recommendations for patients with multidisciplinary diabetes.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">To optimize multimodal fusion strategies in EHR data, Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite> proposed MUFASA, a novel approach that extends Neural Architecture Search (NAS). The authors based their model on the Transformer architecture, which has shown promise in leveraging EHR’s internal structure. Experimental results demonstrated that MUFASA outperformed Transformer, Evolved Transformer, RNN variants, and traditional NAS models on public EHR data. MUFASA architectures achieved higher top-5 recall rates compared to Transformer in predicting CCS diagnosis codes, and they outperformed unimodal NAS by customizing modelling for each modality. MUFASA also exhibited effective transfer learning to ICD-9, another EHR task. The representation of EHR data is challenging due to different modalities, such as medical codes and clinical notes, all of which have distinct characteristics.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Another challenge is the extraction of inter-modal correlations, which are often overlooked or not effectively captured by existing models. An et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite> proposed the Multimodal Attention-based fusIon Networks (MAIN) model, which aims to address two key challenges in healthcare prediction using EHR data. The MAIN model incorporates multiple independent feature extraction modules tailored to each modality, including self-attention and time-aware Transformer for medical codes, and a CNN model for clinical notes. It also introduces an inter-modal correlation extraction module composed of a low-rank multimodal fusion method and a cross-modal attention layer. The model combines the representations of each modality and their correlations to generate visit and patient representations for diagnosis prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite>, by leveraging attention mechanisms and neural networks. Overall, MAIN offers a comprehensive framework for multimodal fusion and correlation extraction in EHR-based prediction tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Wearable Devices</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Wearable devices have become increasingly prevalent in the field of smart healthcare, offering the potential to monitor and track various aspects of an individual’s health and well-being. These devices, typically worn on the body or incorporated into clothing or accessories, can collect real-time data about vital signs, physical activity, sleep patterns, and other health-related metrics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>, <a class="ltx_ref" href="#bib.bib12" title="">12</a>]</cite>. The data gathered by wearable devices can provide valuable insights into an individual’s overall health status, enabling personalized health monitoring and preventive care <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib13" title="">13</a>]</cite>. Moreover, wearable devices can facilitate remote patient monitoring, allowing healthcare professionals to track patients’ health remotely and intervene when necessary. The integration of wearable devices with smart healthcare systems enables continuous monitoring, early detection of health issues, and the ability to deliver personalized interventions and recommendations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib14" title="">14</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Sensor data</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Sensor data plays a crucial role in enabling smart healthcare by providing real-time monitoring and tracking of various physiological parameters and activities. Wearable devices, implantable sensors, and remote monitoring systems collect data such as heart rate, blood pressure, temperature, glucose levels, physical activity, sleep patterns, and so on. Sensor data in smart healthcare enables continuous monitoring of an individual’s health status, facilitating early detection and intervention for potential health issues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>. It allows healthcare providers to gather objective and accurate data, leading to more informed decision-making and personalized treatment plans. For example, sensor data can help in managing chronic conditions like diabetes or cardiovascular diseases by monitoring glucose levels or heart rate variability. Real-time sensor data can also enable remote patient monitoring, telemedicine, and telehealth services, allowing healthcare professionals to monitor patients’ conditions from a distance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>]</cite>. This is particularly beneficial for individuals with limited mobility or those residing in remote areas, by providing access to healthcare services without the need for frequent hospital visits <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">The life cycle for data from EHRs, wearable devices, and sensors in the context of smart healthcare each follow a similar trajectory, encompassing stages such as raw data acquisition, data structuring, data fusion, and ultimately, predictive modeling. This cyclical process is graphically illustrated in Fig. <a class="ltx_ref" href="#S2.F3" title="Figure 3 ‣ II-C Sensor data ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="153" id="S2.F3.g1" src="extracted/5125724/EHR_WD_flowchart.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The Lifecycle of Electronic Health Records, Wearable Devices, and Sensors in Smart Healthcare</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Medical Imaging</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Medical imaging plays a crucial role in smart healthcare by providing valuable diagnostic information and aiding in the management of various medical conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib18" title="">18</a>, <a class="ltx_ref" href="#bib.bib19" title="">19</a>, <a class="ltx_ref" href="#bib.bib20" title="">20</a>, <a class="ltx_ref" href="#bib.bib21" title="">21</a>, <a class="ltx_ref" href="#bib.bib22" title="">22</a>, <a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>. It involves the use of advanced imaging technologies to capture detailed images of the human body, allowing healthcare professionals to visualize and analyze anatomical structures, detect abnormalities, and monitor the progress of treatments. In smart healthcare, medical imaging is integrated with digital technologies and data analytics to enhance the efficiency, accuracy, and accessibility of healthcare services <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib18" title="">18</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS5.5.1.1">II-E</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS5.6.2">Genomic data</span>
</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">Genomic data plays a significant role in the realm of smart healthcare, offering valuable insights into an individual’s genetic makeup and its impact on their health. This type of data includes information about an individual’s DNA sequence, genetic variations, and gene expression patterns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>. With advancements in genomic sequencing technologies, it has become more accessible and affordable to obtain a person’s genetic information. In smart healthcare, genomic data can be utilized for various purposes, such as in the diagnosis and prediction of genetic disorders, as well as in the identification of genetic markers associated with increased disease risk or treatment response <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite>. Genomic data can also enable personalized medicine by guiding treatment decisions based on an individual’s unique genetic profile <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite>. For example, it can help determine optimal drug choices and dosages, minimizing adverse reactions and improving treatment outcomes. Integrating genomic data with other health data sources, such as EHRs and wearable devices, can provide a comprehensive overview of an individual’s health <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite>. This multimodal approach allows for practices such as more accurate assessment of disease risks, personalized prevention strategies, and targeted interventions.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS6.5.1.1">II-F</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS6.6.2">Environmental data</span>
</h3>
<div class="ltx_para" id="S2.SS6.p1">
<p class="ltx_p" id="S2.SS6.p1.1">Environmental data can significantly contribute to smart healthcare by providing insights into the impact of environmental factors on individual health and well-being. Environmental data includes information about air quality, temperature, humidity, pollution levels, noise levels, and other relevant parameters in a person’s surroundings. By incorporating environmental data into smart healthcare systems, healthcare providers can better understand the environmental conditions that may influence a person’s health outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib28" title="">28</a>]</cite>. For example, monitoring air quality can help identify areas with high pollution levels, which is particularly valuable for individuals with respiratory conditions like asthma.</p>
</div>
<div class="ltx_para" id="S2.SS6.p2">
<p class="ltx_p" id="S2.SS6.p2.1">Analyzing this environmental data in real-time allows healthcare professionals to provide personalized recommendations and interventions to mitigate the impact of poor air quality on patients’ health <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib29" title="">29</a>]</cite>. Environmental data can also aid in preventive healthcare by identifying patterns and correlations between environmental factors and specific health conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib30" title="">30</a>, <a class="ltx_ref" href="#bib.bib31" title="">31</a>]</cite>. For instance, studying the relationship between temperature and heat-related illnesses facilitates the implementation early warning systems and interventions during heat waves or extreme weather events.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Multimodal Datasets for smart healthcare</figcaption><div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.5" style="width:433.6pt;height:294.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-216.4pt,147.0pt) scale(0.500516055490232,0.500516055490232) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.5.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.1.1.1">Modality</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.1.2.1">Datatype</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.1.3.1">Dataset</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.1.4.1">No. of Instances</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.1.5.1">No. of Attributes</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.5.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.1.6.1">Task</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.5.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.1.7.1">Popularity*</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.2.2.1" rowspan="14">
<span class="ltx_text" id="S2.T1.5.1.2.2.1.1">Single Modality</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.2.2.2" rowspan="2">
<span class="ltx_text" id="S2.T1.5.1.2.2.2.1">EHR</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.2.2.3">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.2.2.3.1">
<tr class="ltx_tr" id="S2.T1.5.1.2.2.3.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.2.2.3.1.1.1">eICU Collaborative</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.2.2.3.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.2.2.3.1.2.1">Research Database <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib32" title="">32</a>]</cite>
</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.2.2.4">200,000 admissions</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.2.2.5">Varies</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.2.2.6">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.2.2.6.1">
<tr class="ltx_tr" id="S2.T1.5.1.2.2.6.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.2.2.6.1.1.1">Various tasks, mainly</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.2.2.6.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.2.2.6.1.2.1">diagnosis and prognosis</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.2.2.7">Medium</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.3.3.1">MIMIC-III <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib33" title="">33</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.3.3.2">40,000 patients</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.3.3.3">Varies</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.3.3.4">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.3.3.4.1">
<tr class="ltx_tr" id="S2.T1.5.1.3.3.4.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.3.3.4.1.1.1">Various tasks, mainly</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.3.3.4.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.3.3.4.1.2.1">diagnosis and prognosis</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.3.3.5">High</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.4.4.1" rowspan="12">
<span class="ltx_text" id="S2.T1.5.1.4.4.1.1">Imaging</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.4.4.2">MRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.4.4.3">1,370 exams</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.4.4.4">MRI data</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.4.4.5">Disease detection</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.4.4.6">Low</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.5.5.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.5.5.1.1">
<tr class="ltx_tr" id="S2.T1.5.1.5.5.1.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.5.5.1.1.1.1">RSNA Pneumonia</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.5.5.1.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.5.5.1.1.2.1">Detection Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib35" title="">35</a>]</cite>
</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.5.5.2">30,000 images</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.5.5.3">Pneumonia labels</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.5.5.4">Disease detection</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.5.5.5">Low</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.6.6.1">MURA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib36" title="">36</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.6.6.2">40,895 images</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.6.6.3">Abnormal/normal</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.6.6.4">Disease detection</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.6.6.5">Medium</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.7.7.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.7.7.1.1">
<tr class="ltx_tr" id="S2.T1.5.1.7.7.1.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.7.7.1.1.1.1">Pediatric Bone Age</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.7.7.1.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.7.7.1.1.2.1">Challenge Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib37" title="">37</a>]</cite>
</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.7.7.2">Thousands of images</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.7.7.3">Bone age</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.7.7.4">Bone age estimation</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.7.7.5">Medium</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.8.8.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.8.8.1.1">
<tr class="ltx_tr" id="S2.T1.5.1.8.8.1.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.8.8.1.1.1.1">Indiana University Chest</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.8.8.1.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.8.8.1.1.2.1">X-ray Collection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib38" title="">38</a>]</cite>
</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.8.8.2">8,000 images</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.8.8.3">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.8.8.3.1">
<tr class="ltx_tr" id="S2.T1.5.1.8.8.3.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.8.8.3.1.1.1">Chest radiograph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.8.8.3.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.8.8.3.1.2.1">DICOM images</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.8.8.4">Various tasks</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.8.8.5">Medium</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.9.9.1">FastMRI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib39" title="">39</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.9.9.2">Thousands of scans</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.9.9.3">MRI data</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.9.9.4">Image reconstruction</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.9.9.5">Medium</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.10.10.1">CheXpert <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib40" title="">40</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.10.10.2">224,316 images</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.10.10.3">14 labels per image</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.10.10.4">Disease detection</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.10.10.5">High</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.11.11.1">OASIS Brains Project <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.11.11.2">Varies with dataset</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.11.11.3">MRI and clinical data</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.11.11.4">Brain studies</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.11.11.5">High</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.12.12.1">LIDC-IDRI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib42" title="">42</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.12.12.2">Over 1,000 patients</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.12.12.3">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.12.12.3.1">
<tr class="ltx_tr" id="S2.T1.5.1.12.12.3.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.12.12.3.1.1.1">CT scans with marked-up</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.12.12.3.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.12.12.3.1.2.1">annotated lesions</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.12.12.4">Nodule detection</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.12.12.5">High</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.13.13.1">TCIA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib43" title="">43</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.13.13.2">Millions of images</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.13.13.3">Various data types</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.13.13.4">Cancer research</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.13.13.5">High</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.14.14">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.14.14.1">ChestX-ray8 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.14.14.2">108,948 images</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.14.14.3">8 labels per image</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.14.14.4">Disease detection</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.14.14.5">High</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.15.15">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.15.15.1">BraTS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib45" title="">45</a>, <a class="ltx_ref" href="#bib.bib46" title="">46</a>, <a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.15.15.2">Varies annually</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.15.15.3">MRI data</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.15.15.4">Tumor segmentation</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.15.15.5">High</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.16.16">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.5.1.16.16.1" rowspan="6">
<span class="ltx_text" id="S2.T1.5.1.16.16.1.1">Multimodality</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.16.16.2">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.16.16.2.1">
<tr class="ltx_tr" id="S2.T1.5.1.16.16.2.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.16.16.2.1.1.1">Genomics,</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.16.16.2.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.16.16.2.1.2.1">Imaging</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.16.16.3">TCGA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib48" title="">48</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.16.16.4">Thousands of patients</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.16.16.5">Genomic and clinical data</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.16.16.6">Cancer research</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.16.16.7">High</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.17.17">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.17.17.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.17.17.1.1">
<tr class="ltx_tr" id="S2.T1.5.1.17.17.1.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.17.17.1.1.1.1">Genomics,</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.17.17.1.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.17.17.1.1.2.1">Imaging, EHR</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.17.17.2">UK Biobank <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.17.17.3">500,000 individuals</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.17.17.4">Various data types</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.17.17.5">Various tasks</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.17.17.6">Medium</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.18.18">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.18.18.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1.18.18.1.1">
<tr class="ltx_tr" id="S2.T1.5.1.18.18.1.1.1">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.18.18.1.1.1.1">Imaging,</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.18.18.1.1.2">
<td class="ltx_td ltx_align_left" id="S2.T1.5.1.18.18.1.1.2.1">Genomics, EHR</td>
</tr>
</table></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.18.18.2">ADNI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.18.18.3">Thousands of patients</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.18.18.4">MRI and clinical data</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.18.18.5">Alzheimer’s research</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.18.18.6">High</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.19.19">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.19.19.1" rowspan="2">
<span class="ltx_text" id="S2.T1.5.1.19.19.1.1">Imaging, Text</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.19.19.2">ImageCLEFmed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib51" title="">51</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.19.19.3">Varies annually</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.19.19.4">Various data types</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.19.19.5">Various tasks</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.19.19.6">Low</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.20.20">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.20.20.1">Openi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib52" title="">52</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.20.20.2">4.5 million images</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.20.20.3">Various data types</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.20.20.4">Various tasks</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.20.20.5">Low</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.21.21">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.5.1.21.21.1">Various modalities</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.5.1.21.21.2">PhysioNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib53" title="">53</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.5.1.21.21.3">Various datasets</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.5.1.21.21.4">Various data types</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.5.1.21.21.5">Various tasks</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.5.1.21.21.6">High</td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_cell">
<p class="ltx_p ltx_align_center" id="S2.T1.4"><sup class="ltx_sup" id="S2.T1.4.5"><span class="ltx_text" id="S2.T1.4.5.1" style="font-size:80%;">*</span></sup><span class="ltx_text" id="S2.T1.4.4" style="font-size:80%;">Popularity is determined by the citation count in Google Scholar as of 05/06/2023. It is categorized as Low (<math alttext="\leq" class="ltx_Math" display="inline" id="S2.T1.1.1.m1.1"><semantics id="S2.T1.1.1.m1.1a"><mo id="S2.T1.1.1.m1.1.1" xref="S2.T1.1.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.m1.1b"><leq id="S2.T1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.m1.1d">≤</annotation></semantics></math>200 citations), Medium (<math alttext="&gt;" class="ltx_Math" display="inline" id="S2.T1.2.2.m2.1"><semantics id="S2.T1.2.2.m2.1a"><mo id="S2.T1.2.2.m2.1.1" xref="S2.T1.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.m2.1b"><gt id="S2.T1.2.2.m2.1.1.cmml" xref="S2.T1.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.m2.1d">&gt;</annotation></semantics></math>200 and <math alttext="&lt;" class="ltx_Math" display="inline" id="S2.T1.3.3.m3.1"><semantics id="S2.T1.3.3.m3.1a"><mo id="S2.T1.3.3.m3.1.1" xref="S2.T1.3.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.m3.1b"><lt id="S2.T1.3.3.m3.1.1.cmml" xref="S2.T1.3.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.m3.1d">&lt;</annotation></semantics></math>1000 citations), and High (<math alttext="\geq" class="ltx_Math" display="inline" id="S2.T1.4.4.m4.1"><semantics id="S2.T1.4.4.m4.1a"><mo id="S2.T1.4.4.m4.1.1" xref="S2.T1.4.4.m4.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.m4.1b"><geq id="S2.T1.4.4.m4.1.1.cmml" xref="S2.T1.4.4.m4.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.m4.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.m4.1d">≥</annotation></semantics></math>1000 citations).</span></p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS7.5.1.1">II-G</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS7.6.2">Behavioural data</span>
</h3>
<div class="ltx_para" id="S2.SS7.p1">
<p class="ltx_p" id="S2.SS7.p1.1">Behavioural data plays a crucial role in smart healthcare by providing valuable insights into individuals’ habits, lifestyles, and behaviours, which have a significant impact on their overall health and well-being. Behavioural data encompasses various aspects of human behaviour, including physical activity, sleep patterns, dietary habits, stress levels, social interactions, and adherence to medical treatments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib54" title="">54</a>]</cite>. By leveraging behavioural data, smart healthcare systems can monitor and analyze individuals’ behaviours in real time, allowing healthcare providers to comprehensively understand their patients’ daily routines and habits <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib55" title="">55</a>]</cite>. This data can help identify patterns, trends, and deviations from normal behaviour, enabling early detection of potential health issues and the implementation of timely interventions.</p>
</div>
<div class="ltx_para" id="S2.SS7.p2">
<p class="ltx_p" id="S2.SS7.p2.1">Stress levels and emotional well-being can also be monitored through behavioural data, enabling healthcare providers to identify triggers and patterns that may impact individuals’ mental health <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib56" title="">56</a>]</cite>. This data has been used to guide the development of stress management techniques, relaxation strategies, and personalised interventions to support individuals in maintaining good mental health <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib57" title="">57</a>]</cite>. Furthermore, behavioural data can facilitate patient engagement and self-management.</p>
</div>
<div class="ltx_para" id="S2.SS7.p3">
<p class="ltx_p" id="S2.SS7.p3.1">Through interactive platforms and feedback mechanisms, individuals can actively participate in monitoring their own behaviours, goal-setting, and receive personalised recommendations based on their provided data. This empowerment can lead to increased motivation and accountability in someone managing their health and well-being. However, the collection and analysis of behavioural data raises important ethical considerations, including privacy, data security, and informed consent. It is crucial to ensure that individuals’ privacy is protected, their data is securely stored, and proper consent is obtained for data collection and usage.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS8.5.1.1">II-H</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS8.6.2">Multimodality Data</span>
</h3>
<div class="ltx_para" id="S2.SS8.p1">
<p class="ltx_p" id="S2.SS8.p1.1">Multimodality data fusion in smart healthcare involves integrating information from various sources, such as EHRs, medical imaging, wearable devices, genomic data, sensor data, environmental data, and behavioural data. By combining data from different modalities, healthcare professionals can gain a comprehensive understanding of a patient’s health, leading to personalised care and informed decision-making. Each modality provides unique insights, and their fusion enhances the accuracy and completeness of the analysis. For example, in a fusion approach, EHRs provide historical medical records, medical imaging offers anatomical details, wearables capture real-time physiological data, genomics reveal genetic predispositions, sensors provide contextual information, and behavioural data reflect lifestyle choices.</p>
</div>
<div class="ltx_para" id="S2.SS8.p2">
<p class="ltx_p" id="S2.SS8.p2.1">By integrating these modalities, healthcare professionals can uncover hidden patterns, correlations, and relationships that contribute toward techniques for optimising treatment strategies, predicting disease progression, identifying risk factors, and implementing preventive measures. Multimodality data fusion is a crucial step towards a holistic approach for advancing smart healthcare and improving patient outcomes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS9.5.1.1">II-I</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS9.6.2">Datasets for Multimodal Fusion for Smart Healthcare</span>
</h3>
<div class="ltx_para" id="S2.SS9.p1">
<p class="ltx_p" id="S2.SS9.p1.1">An overview of multimodal datasets used in smart healthcare is presented in Table <a class="ltx_ref" href="#S2.T1" title="TABLE I ‣ II-F Environmental data ‣ II Modalities in Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">I</span></a>. It includes information on the modality, dataset name, number of instances, number of attributes, task, popularity, and reference count. The datasets cover various modalities such as EHRs, Genomics, Imaging, and Text. Examples of datasets include the eICU Collaborative Research Database, TCGA, UK Biobank, MRNet, RSNA Pneumonia Detection Challenge, MURA, and ChestX-ray8. These datasets are used for diagnosis, prognosis, cancer research, disease detection, and image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib58" title="">58</a>]</cite>. The popularity of the datasets depends on the reference counts, and the table provides reference counts for further exploration.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">SOTA Techniques in Multimodal Fusion for Smart Healthcare</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Multimodal medical data fusion involves combining information from multiple modalities such as medical imaging, genomic data, EHRs, wearable devices, and more. In this section, we explore state-of-the-art (SOTA) techniques for multimodal fusion across these multiple modalities within the context of smart healthcare.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Feature selection</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Feature selection focuses on identifying and selecting relevant features from raw data to transform them into meaningful information. In the context of multimodal medical data fusion for smart healthcare, recent studies have highlighted the importance of feature selection. Albahri et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite> emphasized its role in effective decision-making and improved patient care by identifying the most relevant features, reducing dimensionality, and enhancing the accuracy and interpretability of the fusion process. Similarly, Alghowinem et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib60" title="">60</a>]</cite> emphasized its impact on improving result interpretability. By systematically applying feature selection techniques, healthcare professionals can extract the most relevant information from the diverse data sources available, to facilitate a more comprehensive understanding of a patient’s health conditions and support informed decision-making in personalized healthcare settings. Feature selection ensures efficient and effective data analysis and integration, ultimately enhancing the quality of data-driven insights and improving patient outcomes in smart healthcare environments.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Before fusion, it is important to perform feature selection within each modality separately. This can be achieved using various techniques, such as statistical tests, information gain, correlation analysis, or ML algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib61" title="">61</a>]</cite>. By selecting relevant features within each modality, noise and irrelevant information can be reduced, leading to improved fusion outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib62" title="">62</a>]</cite>. Certain modalities may contain more inherent noise or provide less relevant information compared to others. In such cases, modality-specific feature selection methods can be employed to identify the most informative features within each modality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib63" title="">63</a>]</cite>. This can be done by leveraging domain knowledge, statistical analysis, or ML techniques tailored to the specific modality. After performing feature selection within each modality, the next step is to select features that are relevant across different modalities. Cross-modal feature selection methods aim to identify features that carry complementary information from multiple modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib64" title="">64</a>]</cite>. These methods can involve techniques such as correlation analysis, mutual information, or joint optimization algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib65" title="">65</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Feature selection and fusion should be performed in a coordinated manner to optimize the overall process. The selected features can be used as input for the fusion algorithm, which combines the information from different modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib66" title="">66</a>]</cite>. This integration can be achieved through techniques such as early fusion, late fusion, or hybrid fusion approaches, depending on the nature of the data and the problem at hand <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib67" title="">67</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">It is important to evaluate the performance of the feature selection and fusion methods using appropriate evaluation metrics. Evaluation can involve assessing classification accuracy, regression performance, clustering quality, or other domain-specific evaluation criteria <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib68" title="">68</a>]</cite>. Cross-validation or independent validation on separate datasets can help validate the effectiveness of the feature selection techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib69" title="">69</a>]</cite>. In healthcare applications, interpretability and explainability of the selected features and fusion results are crucial for building trust and understanding the decision-making process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib70" title="">70</a>]</cite>. Various methods can be employed to enhance interpretability, such as feature importance ranking, visualization techniques, or rule extraction algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib71" title="">71</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="182" id="S3.F4.g1" src="extracted/5125724/feature_selection.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Multimodal Fusion - Feature Selection</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">It’s worth noting that the choice of feature selection methods may vary depending on the specific data characteristics, the fusion task, and the available computational resources. Additionally, the field of multimodal medical data fusion is an active area of research, and new techniques and algorithms are continuously being developed to address its challenges. The feature selection techniques of multimodal fusion are summarised in Fig. <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ III-A Feature selection ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Rule-based systems</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Rule-based systems operate to process and interpret information using predefined rules or logical statements. By employing these rules, these systems can make inferences and derive knowledge from the available data. The defined rules capture relationships and patterns, enabling decision-making based on the processed information. In the context of multimodal medical data fusion for smart healthcare, rule-based systems play a crucial role. They provide a structured approach to decision-making and knowledge representation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib72" title="">72</a>]</cite>, offering a systematic framework for integrating information from various modalities. By employing a set of predefined rules, these systems can effectively process and integrate data from multiple sources, enabling informed decisions and providing valuable recommendations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib73" title="">73</a>]</cite>. The utilization of rule-based systems in multimodal medical data fusion enhances the overall knowledge generation process, aiding in accurate diagnoses, personalized treatment plans, and improved healthcare outcomes.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Rules in a rule-based system are typically defined using an “if-then” format, meaning that each rule consists of an antecedent (if) and a consequent (then). The antecedent represents the conditions or criteria that need to be satisfied, while the consequent specifies the action or conclusion to be taken if the conditions are met <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib74" title="">74</a>]</cite>. Rule-based systems can also contribute to feature selection in multimodal fusion. For example, rules can be designed to identify and select relevant features from different modalities based on their contribution to the decision-making process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib70" title="">70</a>]</cite>. These rules can incorporate domain knowledge or statistical analysis to determine the importance of the features.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">In multimodal medical data fusion, rules can be designed to accommodate multiple modalities. The antecedent of a rule can include conditions from different modalities, allowing the system to consider information from various sources simultaneously <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib75" title="">75</a>]</cite>. This integration can leverage the complementary nature of different modalities to enhance decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib76" title="">76</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Medical data often contains uncertainty and imprecision. Rule-based systems can leverage fuzzy logic, a mathematical framework that handles uncertainty, to model and reason with uncertain or imprecise data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib77" title="">77</a>]</cite>. Fuzzy rules allow for more flexible decision-making by assigning degrees of membership to antecedents and consequents, capturing the inherent uncertainty in medical data fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib78" title="">78</a>]</cite>. In scenarios where multiple rules are applicable, conflicts may arise. Here, rule-based systems can employ strategies for rule prioritization and conflict resolution. These strategies determine the order in which rules are applied and resolve conflicts when multiple rules have conflicting conclusions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib79" title="">79</a>]</cite>. This ensures a systematic and consistent decision-making process.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">Rule-based systems also offer transparency and interpretability by providing explicit rules that can be examined and understood by healthcare professionals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib80" title="">80</a>]</cite>. The rules provide explanations for the system’s decisions, allowing users to understand the underlying reasoning process. This transparency is crucial in building trust and facilitating collaboration between clinicians and the decision-support system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib81" title="">81</a>]</cite>. Furthermore, rule-based systems enable the incorporation of expert knowledge into the decision-making process. Domain experts contribute their expertise by defining the rules that encapsulate their knowledge and clinical guidelines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib73" title="">73</a>]</cite>. This allows the system to leverage the collective intelligence of healthcare professionals and enhance the accuracy and reliability of decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib82" title="">82</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">By monitoring a system’s performance and collecting feedback from its users, rule-based systems can be adapted or refined over time to improve decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib83" title="">83</a>]</cite>. This adaptive capability enables the system to evolve with new insights, changes in medical guidelines, or updates in the underlying data.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">Although rule-based systems provide a structured and interpretable framework for multimodal medical data fusion, it’s important to consider the limitations of these approaches, such as the challenge of capturing complex relationships or interactions between modalities, and the potential for a large number of rules to manage. Hybrid approaches that combine rule-based systems with ML techniques can offer more flexibility and scalability in handling multimodal fusion tasks. The rule-based systems discussed in this subsection are outlined in Fig. <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‣ III-B Rule-based systems ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="125" id="S3.F5.g1" src="extracted/5125724/rule_based_sys.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Multimodal Fusion - Rule-based systems</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Machine Learning</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Machine Learning (ML) encompasses the creation of algorithms capable of learning from data, enabling them to make predictions and informed decisions. By analyzing and processing vast amounts of data, ML algorithms can identify patterns, extract knowledge, and generate valuable insights to support decision-making processes. In the context of smart healthcare, ML techniques play a critical role in multimodal medical data fusion. These methods harness the capabilities of algorithms and statistical models to autonomously detect and understand patterns, relationships, and representations within diverse medical data sources. Through this automated learning process, ML contributes to overall knowledge generation within the healthcare sector, facilitating accurate diagnoses, personalized treatment plans, and improved patient outcomes.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Ensemble methods, such as Random Forests, gradient boosting, or AdaBoost, can be employed to combine the predictions or decisions of multiple ML models that have been trained on different modalities. Each modality can be processed independently using suitable algorithms, and their outputs can be fused using ensemble techniques to make a final decision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib84" title="">84</a>]</cite>. Ensemble learning helps leverage the diversity and complementary information present in different modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib85" title="">85</a>]</cite>. Its adaptive weights combination approach works by assigning weights to different modalities based on their relevance or importance for the fusion task. The weights can be learned using various techniques, such as feature selection algorithms, statistical analysis, or ML models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib86" title="">86</a>]</cite>. The modalities are then combined using weighted fusion strategies, such as weighted averaging or weighted voting, to generate a fused representation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib87" title="">87</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Bayesian networks provide a probabilistic graphical model framework for representing and reasoning about uncertainty in medical data fusion. Each modality can be treated as a node in the network, and the dependencies between modalities can be modelled using conditional probability distributions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib88" title="">88</a>]</cite>. Bayesian networks allow for principled fusion of multimodal information, enabling probabilistic inference and decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib89" title="">89</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Multiple Kernel Learning (MKL) is a technique that combines multiple kernels, which capture different types of information or relationships, into a unified representation. Each modality can be represented using a separate kernel, and MKL methods can learn the optimal combination of these kernels to maximize the performance of the fusion task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib90" title="">90</a>]</cite>. MKL allows for flexible and effective integration of information from different modalities. Feature-level fusion techniques combine features extracted from different modalities to create a unified feature representation. This can involve techniques like concatenation, feature stacking, or feature selection, based on relevance or mutual information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib91" title="">91</a>]</cite>. The fused features can then be used as input for traditional ML algorithms, such as support vector machines (SVM), logistic regression, or k-nearest neighbours (k-NN), to perform classification, regression, or clustering tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib92" title="">92</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">Canonical Correlation Analysis(CCA) is a statistical technique that aims to find linear transformations of multiple modalities to maximize their correlation. It identifies common underlying factors that explain the correlations between modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib93" title="">93</a>]</cite>. The fused representation can then be used as input for subsequent ML algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib94" title="">94</a>]</cite>. Manifold learning techniques, such as t-SNE (t-Distributed Stochastic Neighbor Embedding) or Isomap, can be used to map multimodal data into a lower-dimensional space, while preserving the underlying structures and relationships <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib95" title="">95</a>]</cite>. By projecting the multimodal data onto a common latent space, these techniques facilitate the fusion of modalities and enable visualization and analysis of the fused data.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="440" id="S3.F6.g1" src="extracted/5125724/ML_techniques.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Multimodal Fusion - Machine Learning Techniques</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">Graph-based methods offer a framework for representing and fusing multimodal medical data using graph structures. Each modality can be represented as nodes, and edges can be defined based on the relationships or correlations between modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib96" title="">96</a>]</cite>. Graph-based algorithms, such as graph convolutional networks (GCNs) or graph regularized non-negative matrix factorization (GNMF), can then be applied to capture the dependencies and interactions between modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib97" title="">97</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1">ML techniques, even without deep learning, can still be effective in multimodal medical data fusion for smart healthcare. It’s important to note that the selection of specific ML techniques for multimodal medical data fusion depends on the nature of the data, the fusion task, and the available computational resources. Careful consideration should be given to feature selection, normalization, and data preprocessing steps to ensure optimal fusion performance. Additionally, model evaluation and validation using appropriate metrics and cross-validation techniques are crucial to assess the effectiveness of the fusion approach in smart healthcare applications. In Fig. <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‣ III-C Machine Learning ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">6</span></a>, the ML techniques in multimodal fusion are presented.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Deep learning</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Deep learning, as a subset of ML, plays a crucial role at the knowledge level of the DIKW framework. It specializes in training neural networks with multiple layers to extract intricate features and representations from data. By processing vast amounts of data, deep learning models can learn complex patterns and generate valuable knowledge for decision-making purposes. In the context of smart healthcare, deep learning has emerged as a potent approach for multimodal medical data fusion. Its unique capability to automatically learn hierarchical representations from diverse and complex medical modalities makes it particularly well-suited for integrating and extracting meaningful information. With its ability to handle diverse data types and capture intricate relationships, deep learning contributes to the overall knowledge-generation process in healthcare, enabling more accurate diagnoses, personalized treatment plans, and improved patient outcomes. Here are some key aspects of deep learning in multimodal medical data fusion.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Deep learning architectures, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), or transformers, can learn representations directly from multimodal medical data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib98" title="">98</a>]</cite>. By jointly processing multiple modalities, these models capture both local and global dependencies, enabling the extraction of rich and informative features. This facilitates the fusion of modalities at various levels, ranging from low-level pixel or waveform data to high-level semantic representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib99" title="">99</a>]</cite>. Deep learning models with recurrent or temporal components, such as RNNs or long short-term memory (LSTM) networks, can handle sequential or temporal aspects of multimodal medical data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib100" title="">100</a>]</cite>. These models can capture temporal dependencies, changes over time, or dynamic patterns across modalities. This is particularly relevant for applications such as physiological signal analysis, time-series data fusion, or modelling disease progression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib101" title="">101</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">Deep learning models that have been pre-trained on large-scale datasets, such as ImageNet or natural language corpora, can be leveraged for multimodal medical data fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib102" title="">102</a>]</cite>. Transfer learning techniques allow the transfer of knowledge from pretraining to the medical domain, enabling the models to learn relevant representations from limited medical data. This approach can boost performance, especially when multimodal medical datasets are small or resource-intensive to collect <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib103" title="">103</a>]</cite>. Attention mechanisms in deep learning models provide a mechanism for focusing on salient regions or modalities within the input data. They learn to allocate attention to the most relevant features or modalities, enhancing the fusion process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib104" title="">104</a>]</cite>. Attention mechanisms can be employed within CNNs, RNNs, or transformer architectures to selectively combine or weigh the contributions of different modalities based on their importance for the task at hand <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib105" title="">105</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1">Generative models, such as generative adversarial networks (GANs) or variational autoencoders (VAEs), can be used for multimodal fusion. These models learn to generate new samples from the joint distribution of multiple modalities, capturing their underlying correlations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib106" title="">106</a>]</cite>. Generative models can aid in data augmentation, missing data imputation, or synthesis of multimodal data, facilitating improved training and fusion outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib107" title="">107</a>]</cite>. Deep fusion architectures combine multiple modalities at different stages of the network, allowing for the explicit integration of multimodal information. For example, early fusion involves combining modalities at the input level, while late fusion integrates modalities at higher layers or during decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib108" title="">108</a>]</cite>. Hybrid fusion approaches leverage both early and late fusion strategies to capture complementary information effectively. Deep fusion architectures can enhance the performance and robustness of multimodal fusion tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib109" title="">109</a>]</cite>.
</p>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">Deep learning models will benefit from the integration of clinical knowledge, domain expertise, or prior medical information. Architectures that incorporate domain-specific constraints, expert rules, or Bayesian priors can enhance the fusion process and align the models with established medical knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib110" title="">110</a>]</cite>. Integrating clinical knowledge helps improve the interpretability, reliability, and acceptance of deep learning models in smart healthcare settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib111" title="">111</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p6">
<p class="ltx_p" id="S3.SS4.p6.1">Deep learning models, although powerful, can be challenging to interpret. However, techniques such as attention visualization, saliency mapping, or gradient-based methods can provide insights into the model’s decision-making process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib112" title="">112</a>]</cite>. Interpretable deep learning architectures, such as CNNs with structured receptive fields or interpretable RNN variants, are also being explored to enhance transparency and explainability in multimodal medical data fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib113" title="">113</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p7">
<p class="ltx_p" id="S3.SS4.p7.1">Deep learning techniques offer promising avenues for multimodal medical data fusion, but challenges such as the need for large labelled datasets, interpretability, and generalization to new patient populations must be addressed. Collaboration between deep learning researchers, healthcare professionals, and data scientists is crucial to developing effective and reliable deep learning approaches for multimodal medical data fusion in smart healthcare. The deep learning techniques for multimodal fusion are outlined in Fig. <a class="ltx_ref" href="#S3.F7" title="Figure 7 ‣ III-D Deep learning ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">7</span></a></p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="669" id="S3.F7.g1" src="extracted/5125724/deep_learn_tech.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Multimodal Fusion - Deep Learning Techniques</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.5.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.6.2">Natural Language Processing</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">NLP plays a vital role in transforming textual data into structured information, uncovering insights, and facilitating informed decision-making. In the context of multimodal medical data fusion for smart healthcare, NLP is particularly valuable for processing textual information from clinical notes, reports, and records. It extracts relevant details, identifies relationships, and discovers hidden patterns within the text. By incorporating NLP into the data fusion process, healthcare professionals can gain a comprehensive understanding of patients’ health, improve diagnosis accuracy, and enhance personalized treatment planning. NLP is a powerful tool for integrating text-based information with other data modalities, enabling a holistic approach to healthcare decision-making.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">NLP techniques are employed to process and extract meaningful information from unstructured textual data. Tasks such as tokenization, sentence segmentation, part-of-speech tagging, named entity recognition, and syntactic parsing help structure and analyze clinical text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib114" title="">114</a>]</cite>. NLP enables the extraction of relevant concepts, medical terms, and relationships from textual data, facilitating their integration with other modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib115" title="">115</a>]</cite>. NLP techniques can extract structured information from clinical narratives, such as diagnoses, medications, procedures, and patient demographics. Named entity recognition and relationship extraction algorithms identify and classify relevant entities and their associations, contributing to the fusion of textual information with other modalities. This extracted information can be used for decision support, clinical coding, or cohort identification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib116" title="">116</a>]</cite>. NLP techniques, including semantic parsing, semantic role labeling, and medical concept normalization, each of which enable the understanding of clinical text in a structured manner <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib117" title="">117</a>]</cite>. This facilitates the extraction of clinical concepts, relations, and contextual knowledge, which can be fused with other modalities for comprehensive analysis, decision support, or knowledge discovery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib118" title="">118</a>]</cite>.
</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">NLP models can be trained to classify clinical text into various categories, such as disease categories, severity levels, or treatment options <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib119" title="">119</a>, <a class="ltx_ref" href="#bib.bib120" title="">120</a>]</cite>. Sentiment analysis techniques can also assess the sentiment or opinion expressed in patient feedback, social media data, or clinical notes. Text classification and sentiment analysis provide valuable insights, and can be integrated with other modalities for a comprehensive understanding of the patient’s condition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib3" title="">3</a>, <a class="ltx_ref" href="#bib.bib121" title="">121</a>]</cite>. NLP can also bridge the gap between textual and visual modalities in medical data fusion. By analyzing textual descriptions or radiology reports, NLP techniques can extract relevant information about anatomical locations, findings, or abnormalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib122" title="">122</a>]</cite>. This information can be linked to corresponding images or visual data, enabling the fusion of text and image modalities for improved diagnosis, treatment planning, or image interpretation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib123" title="">123</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.1">NLP methods aid in identifying and monitoring adverse events by analyzing textual data sources such as EHRs, patient complaints, or pharmacovigilance reports <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib124" title="">124</a>]</cite>. Sentiment analysis, information extraction, and text mining techniques can automatically detect and categorize adverse events <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib125" title="">125</a>]</cite>, enabling timely interventions and enhancing patient safety <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib126" title="">126</a>]</cite>. NLP techniques contribute to patient risk assessment by analyzing clinical narratives and extracting relevant information related to patient history, comorbidities, and lifestyle factors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib127" title="">127</a>]</cite>. By integrating this textual information with other patient data, such as vital signs, imaging results, or genetic information, multimodal medical data fusion can provide a comprehensive risk assessment strategy for personalized healthcare interventions and preventive measures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib128" title="">128</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS5.p5">
<p class="ltx_p" id="S3.SS5.p5.1">NLP techniques support the development of clinical decision support systems by extracting relevant clinical knowledge from medical literature, clinical guidelines, or research articles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib129" title="">129</a>, <a class="ltx_ref" href="#bib.bib130" title="">130</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="587" id="S3.F8.g1" src="extracted/5125724/nlp_tech.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Multimodal Fusion - NLP techniques</figcaption>
</figure>
<div class="ltx_para" id="S3.SS5.p6">
<p class="ltx_p" id="S3.SS5.p6.1">NLP techniques provide valuable capabilities in extracting, processing, and integrating textual information within multimodal medical data fusion. They improve the comprehension of clinical text, enable the incorporation of clinical knowledge, and facilitate comprehensive and effective analysis in smart healthcare applications. Fig. <a class="ltx_ref" href="#S3.F8" title="Figure 8 ‣ III-E Natural Language Processing ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">8</span></a> presents the NLP techniques that can be adopted for multimodal fusion.</p>
</div>
<figure class="ltx_figure" id="S3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="371" id="S3.F9.g1" src="extracted/5125724/taxonomy.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Taxonomy of SOTA techniques in Multimodal Fusion for Smart Healthcare</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS6.5.1.1">III-F</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS6.6.2">Taxonomy of Approaches in Multimodal Fusion</span>
</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">The taxonomy of approaches in multimodal fusion for smart healthcare encompasses feature selection, rule-based systems, ML, deep learning, and NLP, as shown in Fig. <a class="ltx_ref" href="#S3.F9" title="Figure 9 ‣ III-E Natural Language Processing ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">9</span></a>. These techniques play a crucial role in integrating and analyzing diverse data modalities to extract valuable insights and support informed decision-making in healthcare applications.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Feature selection focuses on identifying relevant features to create a concise representation of the data.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Rule-based systems utilize predefined rules to process and combine data from multiple modalities.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">ML leverages patterns and relationships in the data to predict, classify, or cluster information from different modalities.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Deep learning employs neural networks to automatically learn hierarchical representations and capture complex relationships.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">NLP techniques process and analyze textual information, enhancing the understanding of clinical data and facilitating its integration with other modalities.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">By leveraging these approaches, researchers and healthcare professionals can gain a deeper understanding of patient health, enable personalized care, and make more informed decisions in intelligent healthcare systems, ultimately advancing patient care and well-being. In Fig. <a class="ltx_ref" href="#S3.F9" title="Figure 9 ‣ III-E Natural Language Processing ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">9</span></a>, we provide a comprehensive taxonomy of multimodal fusion methods relevant to smart healthcare. The Fig. <a class="ltx_ref" href="#S3.F9" title="Figure 9 ‣ III-E Natural Language Processing ‣ III SOTA Techniques in Multimodal Fusion for Smart Healthcare ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">9</span></a> categorizes SOTA techniques, including feature selection, rule-based systems, machine learning, deep learning, and NLP, and aligns them with the different levels of the DIKW conceptual model.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Challenges in adopting Multimodal Fusion</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">There are numerous challenges in adopting multimodal data fusion approaches that influence different stages and aspects of the DIKW framework. These challenges include issues such as data quality and interoperability, privacy and security, data processing and analysis, clinical integration and adoption, ethical considerations, and interpretation of results, all of which impact the transformation of data into meaningful information, knowledge, and wisdom in healthcare.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Data quality and interoperability</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Data quality and interoperability present significant challenges in the context of multimodal fusion for smart healthcare within the DIKW framework. The integration of data from diverse sources and ensuring its quality and compatibility across different healthcare systems and modalities can be complex and time-consuming <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib131" title="">131</a>]</cite>. Insufficient data quality and a lack of interoperability can result in inaccurate analysis and hinder the effectiveness of the data fusion process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib132" title="">132</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Addressing these challenges necessitates the development and adoption of data standards and protocols. Standardizing data formats, such as HL7 for EHRs or DICOM for medical imaging, facilitates seamless integration and data exchange across healthcare systems and modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib133" title="">133</a>]</cite>. These standards establish a shared language for data representation, simplifying the processing and integration of data from various sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib134" title="">134</a>]</cite>. Establishing interoperability frameworks plays a vital role in promoting smooth data sharing and exchange among different healthcare systems and modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib135" title="">135</a>]</cite>. These frameworks provide guidelines and best practices for data integration, harmonization, and transmission, since they define the protocols, data models, and communication standards that enable efficient interoperability across diverse the data sources. Adhering to interoperability frameworks enhances data compatibility and coherence, thereby facilitating effective multimodal fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib136" title="">136</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Advancements in ML techniques offer promising avenues for addressing data integration and interoperability challenges <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib137" title="">137</a>]</cite>. ML algorithms can learn patterns and relationships in data obtained from different sources, facilitating automated data mapping, harmonization, and integration. Leveraging ML enables organizations to streamline the data fusion process, improving efficiency and accuracy in multimodal integration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib138" title="">138</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Privacy and security</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Privacy and security pose significant challenges in the integration of sensitive patient data from multiple sources within the DIKW framework. Protecting patient privacy and ensuring data security are paramount concerns in healthcare, particularly when dealing with sensitive health information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib139" title="">139</a>]</cite>. Robust privacy and security measures are necessary to safeguard patient confidentiality and prevent unauthorized access or data breaches in the integration of multimodal medical data.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">To address these challenges, implementing data encryption techniques is essential to protect patient data during transmission and storage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib140" title="">140</a>]</cite>. Encryption converts data into an unreadable form, ensuring that only authorized individuals with decryption keys can access and interpret the data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib141" title="">141</a>]</cite>. Secure storage methods, such as secure servers or cloud platforms with robust access controls, play a crucial role in safeguarding patient data from unauthorized access, loss, or theft <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib142" title="">142</a>]</cite>. In addition, adopting privacy-preserving techniques is crucial to protect patient privacy during data fusion. Differential privacy, for example, adds noise to aggregated data to prevent individual identification while preserving the utility of the fused data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib143" title="">143</a>, <a class="ltx_ref" href="#bib.bib64" title="">64</a>]</cite>. Secure multiparty computation (SMC) techniques allow collaboration on data fusion without revealing individual-level data to any party involved, ensuring privacy during the fusion process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib144" title="">144</a>]</cite>. The continuous monitoring and auditing of data access and usage are vital in detecting and preventing unauthorized activities and potential data breaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib145" title="">145</a>]</cite>. Robust auditing mechanisms and log analysis techniques enable organizations to track and investigate any suspicious or anomalous access patterns or data breaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib146" title="">146</a>]</cite>. Real-time monitoring systems can provide alerts and notifications in case of any unauthorized access attempts or potential security incidents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib147" title="">147</a>]</cite>. By implementing these privacy and security measures, healthcare organizations can ensure the protection of patient data, maintain privacy during data fusion, and mitigate the risks associated with unauthorized access or data breaches.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Data processing and analysis</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Data processing and analysis play a critical role in the DIKW framework, particularly in multimodal medical data fusion. Challenges arise in handling large volumes of data, scalability of data processing, and extracting actionable insights from the fused data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib148" title="">148</a>]</cite>. To address these challenges, ML algorithms and AI techniques are leveraged to enable efficient processing and analysis of multimodal medical data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib70" title="">70</a>, <a class="ltx_ref" href="#bib.bib149" title="">149</a>]</cite>. Supervised and unsupervised learning algorithms are utilized for classification, prediction, and pattern discovery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib150" title="">150</a>]</cite>. Deep learning models, such as CNNs and RNNs, are applied to tasks involving medical imaging and sequential data analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib151" title="">151</a>]</cite>. Reinforcement learning techniques optimize treatment plans and interventions based on patient outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib152" title="">152</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Collaboration between clinicians and data scientists are crucial in developing effective data processing and analysis solutions that align with clinical needs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib153" title="">153</a>, <a class="ltx_ref" href="#bib.bib154" title="">154</a>]</cite>. Integration of multimodal medical data fusion into Clinical Decision Support Systems (CDSS) enhances clinical decision-making and improves patient outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib155" title="">155</a>, <a class="ltx_ref" href="#bib.bib156" title="">156</a>]</cite>. Scalable data processing techniques, including distributed computing frameworks and cloud computing platforms, handle large-scale datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib157" title="">157</a>, <a class="ltx_ref" href="#bib.bib158" title="">158</a>]</cite>. Real-time data analytics enable immediate insights and proactive interventions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib159" title="">159</a>]</cite>. Effective visualization techniques can aid in interpreting and communicating analysis results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib160" title="">160</a>]</cite>. By addressing these challenges and utilizing advanced data processing and analysis techniques, healthcare organizations can unlock the full potential of multimodal medical data fusion within the DIKW framework.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Clinical integration and adoption</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Clinical integration and adoption present significant challenges in the successful implementation of multimodal fusion within the DIKW framework in healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib70" title="">70</a>]</cite>. To address these challenges, involving clinicians and other stakeholders in the development and implementation process is crucial for both ensuring successful adoption and maximizing the impact of multimodal fusion in clinical practice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib161" title="">161</a>, <a class="ltx_ref" href="#bib.bib162" title="">162</a>]</cite>. Their input and feedback contributes toward designing technologies that align with clinical workflows and meet end-users needs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib163" title="">163</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">The design of user-friendly interfaces and intuitive workflows is essential to facilitate the integration of multimodal fusion into clinical practice. Applying user-centered design principles and conducting usability testing can identify and address usability issues, enhancing user satisfaction and adoption rates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib164" title="">164</a>, <a class="ltx_ref" href="#bib.bib83" title="">83</a>]</cite>. Integrating multimodal fusion technologies with CDSS can enhance clinical decision-making processes by providing real-time recommendations and alerts based on the fused data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib155" title="">155</a>, <a class="ltx_ref" href="#bib.bib165" title="">165</a>]</cite>. Embedding these technologies into familiar clinical tools streamlines the integration process and facilitates adoption. To ensure successful adoption, it is crucial to provide adequate training and education to healthcare professionals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib166" title="">166</a>, <a class="ltx_ref" href="#bib.bib167" title="">167</a>]</cite>. Training programs should focus not only on the technical aspects, but also on the clinical relevance and potential impacts on patient care. Continual education and support help healthcare professionals remain proficient in using the technologies and stay updated on advancements.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.5.1.1">IV-E</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS5.6.2">Ethical considerations</span>
</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Ethical considerations are of utmost importance in the context of multimodal medical data fusion within the DIKW framework. Ensuring patient privacy, autonomy, and fairness is essential in utilizing patient data ethically <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib168" title="">168</a>]</cite>. Obtaining informed consent from patients is a fundamental ethical requirement in multimodal medical data fusion. Patients should be fully informed about the purpose, risks, and benefits of data fusion, and consent processes should be transparent and understandable <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib169" title="">169</a>]</cite>. Clear mechanisms for patients to withdraw their consent should also be provided.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">Defining data ownership and governance policies is crucial. Healthcare organizations should establish guidelines to determine data ownership, usage, and access <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib170" title="">170</a>]</cite>. Transparent governance mechanisms, such as data access committees, should oversee the ethical use of patient data and compliance with privacy regulations. Respecting patient privacy and ensuring data confidentiality is paramount. Robust security measures, including encryption and access controls, should be implemented <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib171" title="">171</a>]</cite>. Compliance with privacy regulations like HIPAA or GDPR should be ensured.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">Addressing potential biases is essential in multimodal fusion. Efforts should be made to mitigate biases through rigorous data collection processes, algorithmic fairness assessments, and diverse representation in data and development teams <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib172" title="">172</a>]</cite>. Regular monitoring and auditing can help identify and address biases. Ethical frameworks and guidelines should be developed and followed. These frameworks should outline principles and best practices for ethical data collection, fusion, analysis, and decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib173" title="">173</a>]</cite>. Guidelines should cover areas such as data privacy, informed consent, fairness, transparency, and accountability. Engaging the public and stakeholders in discussions about ethical considerations is crucial. Open communication channels should be established to foster trust and incorporate patient perspectives into decision-making processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib174" title="">174</a>]</cite>. By addressing ethical considerations, healthcare organizations can ensure the responsible and ethical use of patient data in multimodal medical data fusion, promoting patient privacy, fairness, and trust within the DIKW framework.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS6.5.1.1">IV-F</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS6.6.2">Interpretation of results</span>
</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">Interpreting the results of multimodal medical data fusion within the DIKW framework can be challenging due to the complexity of integrating multiple modalities and the large volumes of generated data. Effective interpretation is crucial for extracting meaningful insights and making actionable decisions in clinical settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib175" title="">175</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.1">To facilitate interpretation, the development of visual analytics tools and techniques is essential. Interactive visualizations, such as heatmaps, scatter plots, or network diagrams, can assist clinicians in identifying patterns, correlations, and outliers in the fused data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib176" title="">176</a>]</cite>. These visualizations should provide intuitive representations and enable interactive exploration at different levels of detail <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib177" title="">177</a>]</cite>. Enhancing the interpretability of fusion models and algorithms is another important aspect. Techniques like explainable AI, interpretable machine learning, or rule-based systems can provide transparent explanations for the fusion process and decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib178" title="">178</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS6.p3">
<p class="ltx_p" id="S4.SS6.p3.1">Understanding how the fusion models arrive at certain conclusions helps clinicians trust the results and make informed decisions based on the interpretation of the fused data. Clinical validation studies are crucial for assessing the clinical utility and effectiveness of the interpretation results. Real-world evaluation with healthcare professionals provides insights into the practical applicability of the interpretation and helps refine the techniques to ensure meaningful and actionable results are aligned with clinical practice. Involving domain experts, such as clinicians or medical researchers, in the interpretation process is vital. They bring valuable insights into the clinical relevance of the fused data and help interpret the results within the context of patient care <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib179" title="">179</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS6.p4">
<p class="ltx_p" id="S4.SS6.p4.1">Collaboration between data scientists and domain experts fosters mutual understanding and enables the development of interpretation techniques that meet the specific needs of healthcare professionals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib180" title="">180</a>]</cite>. Incorporating clinical guidelines and contextual information into the interpretation of fused data is crucial. Considering patient-specific factors, such as demographics, medical history, or treatment guidelines, helps provide personalized interpretations and recommendations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib181" title="">181</a>]</cite>. Aligning the interpretation with existing clinical knowledge and guidelines ensures clinically meaningful and actionable results for healthcare providers.
</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="482" id="S4.F10.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Generic Framework of Multimodal Fusion for Smart Healthcare</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">DIKW Fusion Framework with Multimodality</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Based on the data representation modalities and multimodal fusion approaches, we present a universal framework that can be applied to diverse applications in Fig. <a class="ltx_ref" href="#S4.F10" title="Figure 10 ‣ IV-F Interpretation of results ‣ IV Challenges in adopting Multimodal Fusion ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">10</span></a>. In the framework for multimodal fusion in smart healthcare, several components are identified that facilitate the progression towards wisdom. At the data level, strategies for efficient data acquisition are employed, followed by data integration and harmonization processes to create a unified dataset.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Moving to the information level, context-aware fusion incorporates contextual information to enhance the fusion process, while multi-level fusion techniques capture complex relationships and patterns. Explainable fusion models provide transparency and trust, and uncertainty modeling supports decision-making based on fused data. Privacy-preserving fusion techniques ensure responsible data handling, and validation and evaluation methods assess the performance of the fusion framework. At the knowledge level, continuous learning and adaptability mechanisms enable the framework to stay up-to-date, while ethical considerations and governance frameworks address ethical issues in healthcare fusion.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">In the landscape of multimodal data fusion for healthcare applications, the journey toward wisdom can be conceptualized as a hierarchical framework consisting of four integral stages: data fusion, information fusion, knowledge fusion, and ultimately, wisdom, as shown in Fig. <a class="ltx_ref" href="#S5.F11" title="Figure 11 ‣ V DIKW Fusion Framework with Multimodality ‣ A Survey of Multimodal Information Fusion for Smart Healthcare: Mapping the Journey from Data to Wisdom"><span class="ltx_text ltx_ref_tag">11</span></a>. In the first stage, data fusion, we employ techniques such as feature selection, ensemble learning, and graph-based methods to combine and select the most relevant features from diverse data sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib182" title="">182</a>]</cite>. This crucial step forms a solid groundwork for effectively integrating and processing multimodal data.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Moving on to the second stage, information fusion, we delve deeper into the data by utilizing advanced techniques such as deep fusion architectures, transfer learning, attention mechanisms, and sequential modeling. These sophisticated approaches enable us to uncover intricate relationships and patterns across modalities, providing a more profound understanding of the data at hand. Additionally, we employ explainability and interpretability techniques to gain valuable insights into the decision-making process of the fusion models.
</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">In the final stage, knowledge fusion, we take integration to the next level by incorporating clinical knowledge and domain expertise. Here, techniques like CDSS, adverse event detection, patient risk assessment, and clinical natural language understanding come into play. By leveraging this wealth of clinical knowledge, we can extract actionable insights and make informed decisions in the healthcare domain.</p>
</div>
<figure class="ltx_figure" id="S5.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="281" id="S5.F11.g1" src="extracted/5125724/data_to_wisdom_journey.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>DIKW Conceptual Model Journey</figcaption>
</figure>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">By following this progressive journey from data fusion to information fusion and knowledge fusion, we empower ourselves to enhance our understanding and analysis of multimodal data. This, in turn, contributes to the development of wisdom in the field of multimodal data fusion, enabling us to make more impactful advancements in smart healthcare and its applications.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Future Directions of DIKW Fusion in Smart Healthcare</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The field of multimodal medical data fusion for smart healthcare is expected to evolve in line with the 4Ps of healthcare – Predictive, Preventive, Personalized, and Participatory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib183" title="">183</a>, <a class="ltx_ref" href="#bib.bib184" title="">184</a>]</cite>. Predictive healthcare data fusion aims to anticipate health events and outcomes by combining data from various sources, while Preventive data fusion focuses on identifying risk factors and promoting healthy behaviors. Personalized fusion caters to individual-specific data for customized care, and Participatory fusion involves patients and stakeholders in the data fusion process, enhancing transparency and trust. The progress towards these goals forms the crux of our future research.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Predictive Healthcare</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">In the context of the DIKW framework and the generic framework discussed, the “Predictive” component of multimodal fusion in smart healthcare focuses on utilizing diverse data sources to anticipate health events and outcomes, thereby enabling proactive interventions and personalized healthcare strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib183" title="">183</a>]</cite>. Within the DIKW framework, the Predictive component involves leveraging multimodal fusion to generate predictive models for assessing disease likelihoods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib113" title="">113</a>]</cite>. By integrating and analyzing data from various modalities, such as genomics, medical imaging, and clinical records, healthcare professionals can identify early indicators or risk factors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib185" title="">185</a>]</cite>. ML algorithms play a key role in analyzing the combined data and uncovering patterns indicative of disease risk <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib186" title="">186</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">In the generic framework, the Predictive component of multimodal fusion aims to identify potential risk factors or biomarkers for disease prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib128" title="">128</a>]</cite>. Healthcare professionals gain insights into genetic predispositions, imaging abnormalities, and the clinical context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib187" title="">187</a>]</cite>. ML algorithms enable the development of predictive models by leveraging the combined data and considering variables such as genetic markers, imaging features, clinical parameters, lifestyle factors, and environmental exposures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib188" title="">188</a>, <a class="ltx_ref" href="#bib.bib189" title="">189</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">By applying predictive multimodal fusion within the framework, healthcare professionals can proactively identify individuals at higher risk of developing specific diseases or conditions, facilitating preventive interventions and personalized healthcare strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>, <a class="ltx_ref" href="#bib.bib190" title="">190</a>]</cite>. For instance, early identification of individuals at risk of cardiovascular disease enables targeted lifestyle modifications, medication interventions, and regular monitoring to prevent or manage the condition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>. Through the integration of multimodal data and the application of predictive analytics, the Predictive component enhances healthcare decision-making and supports proactive interventions, ultimately improving patient outcomes and healthcare delivery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib191" title="">191</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">Preventive Healthcare</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In the DIKW framework, the “Preventive” component of multimodal fusion focuses on utilizing diverse data sources to develop personalized preventive strategies for patients. By integrating and analyzing data from various sources, such as mHealth devices and EHRs, healthcare providers can gain insights into patients’ lifestyle factors and health issues, enabling them to develop targeted interventions and preventive measures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib190" title="">190</a>, <a class="ltx_ref" href="#bib.bib113" title="">113</a>]</cite>. By combining data from sources such as mHealth devices and EHRs, healthcare providers can gain a comprehensive understanding of patients’ health status and develop personalized preventive strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib192" title="">192</a>, <a class="ltx_ref" href="#bib.bib72" title="">72</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">Multimodal data fusion enables the integration and analysis of data from diverse sources, such as wearable fitness trackers or smart watches for monitoring physical activity, heart rate, and sleep patterns, and EHRs containing medical history, diagnoses, and laboratory results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib193" title="">193</a>]</cite>. By combining these data sources, healthcare providers obtain a holistic view of patients’ health and lifestyle factors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib194" title="">194</a>]</cite>. Through multimodal fusion, healthcare providers can identify lifestyle factors that may contribute to patients’ health issues. Data from mHealth devices and EHRs may indicate emergent patterns such as sedentary behavior, inadequate sleep, or poor nutrition that may impact the development or progression of certain health conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib187" title="">187</a>, <a class="ltx_ref" href="#bib.bib188" title="">188</a>, <a class="ltx_ref" href="#bib.bib189" title="">189</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">Based on the analysis of multimodal data, healthcare providers can develop personalized preventive strategies tailored to each patient’s unique needs. For instance, if data fusion analysis reveals that a patient’s sedentary behavior contributes to their health issues, healthcare providers may prescribe regular physical activity, provide educational materials on exercise, or suggest behavioral change techniques to promote a more active lifestyle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib195" title="">195</a>]</cite>. Furthermore, multimodal fusion facilitates ongoing monitoring and feedback, empowering patients to maintain their preventive strategies and make informed decisions about their health. By leveraging technology and data integration, patients can receive real-time feedback on their health behaviors, track their progress, and receive personalized recommendations to support their preventive efforts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib196" title="">196</a>, <a class="ltx_ref" href="#bib.bib197" title="">197</a>]</cite>. By incorporating the Preventive component within the DIKW and generic frameworks, multimodal fusion plays a vital role in developing personalized preventive strategies for patients, aligning with the broader goals of precision medicine and personalized healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib198" title="">198</a>, <a class="ltx_ref" href="#bib.bib184" title="">184</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.5.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.6.2">Personalized Healthcare</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">In the DIKW framework, the “Personalized” component of multimodal fusion focuses on utilizing diverse data sources to develop personalized treatment plans for patients. By integrating and analyzing data from different modalities, such as imaging and genomics, healthcare providers can gain a deeper understanding of the patient’s molecular profile and tailor treatment strategies based on their individual characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib190" title="">190</a>, <a class="ltx_ref" href="#bib.bib113" title="">113</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">Within the generic framework, the Personalized component of multimodal fusion aims to identify specific genetic mutations or variations that underpin the patient’s disease. By combining imaging data with genomics data, healthcare providers can obtain a comprehensive view of the patient’s health condition and uncover genetic markers associated with conditions such as tumor growth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib192" title="">192</a>, <a class="ltx_ref" href="#bib.bib72" title="">72</a>]</cite>. Multimodal data fusion allows for the integration and analysis of data from diverse sources, such as magnetic resonance imaging (MRI), computed tomography (CT), positron emission tomography (PET), and genomics data. These modalities provide detailed anatomical, functional, and genetic information about the patient’s body and disease state <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib194" title="">194</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">By fusing imaging and genomics data, healthcare providers can identify specific genetic mutations or variations that inform the underlying molecular mechanisms of the disease. This knowledge guides the development of personalized treatment plans tailored to the patient’s individual molecular profile <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib195" title="">195</a>]</cite>. The analysis of multimodal fusion enables healthcare providers to make informed treatment decisions, such as recommending targeted therapies for specific genetic mutations. This personalized approach ensures that patients receive the most effective treatments based on their unique genetic characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib196" title="">196</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1">Moreover, multimodal fusion facilitates ongoing monitoring of treatment response and enables treatment adjustments over time. By integrating data from imaging, genomics, and other modalities, healthcare providers can assess treatment effectiveness and make informed decisions regarding treatment modifications to optimize patient outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib197" title="">197</a>]</cite>. By incorporating the Personalized component within the DIKW and generic frameworks, multimodal fusion can play a significant role in tailoring treatment plans based on individual patient characteristics and molecular profiles. This personalized approach to patient care and treatment outcomes aligna with the broader goals of precision medicine and personalized healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib198" title="">198</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS4.5.1.1">VI-D</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS4.6.2">Participatory Healthcare</span>
</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">In the DIKW framework, the “Participatory” component of multimodal fusion in smart healthcare focuses on empowering patients to actively participate in their own healthcare journey, fostering collaboration and shared decision-making with healthcare providers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib199" title="">199</a>, <a class="ltx_ref" href="#bib.bib200" title="">200</a>]</cite>. Within the DIKW framework, the Participatory component involves leveraging multimodal fusion to provide patients with real-time feedback and insights into their health status <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib201" title="">201</a>]</cite>. By integrating data from mHealth devices and patient-reported information, patients can actively monitor and track their health, enabling them to make informed decisions about their well-being <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib202" title="">202</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">In the generic framework, the Participatory component of multimodal fusion emphasizes the active engagement of patients in their healthcare by combining data from mHealth devices and patient-reported data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib203" title="">203</a>]</cite>. Through real-time access to their health information, patients can receive personalized feedback and recommendations, and participate in discussions about their treatment plans  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib199" title="">199</a>]</cite>. This collaborative approach enables patients to actively contribute to decision-making based on their preferences, values, and personal health goals. Multimodal data fusion also enables patients to participate in larger-scale initiatives, such as contributing their data to aggregated and anonymized datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib200" title="">200</a>]</cite>. By participating in research studies, clinical trials, or public health monitoring programs, individuals can contribute to advancements in medical research, personalized interventions, and population health initiatives. By embracing the Participatory component of multimodal fusion, patients become active partners in their own healthcare, and can be further empowered to make proactive choices and actively contribute to their own well-being. This collaborative approach enhances patient-centered care and fosters a stronger partnership between patients and healthcare providers.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Multimodal medical data fusion, integrating various modalities like EHRs, medical imaging, wearable devices, genomic data, sensor data, environmental data, and behavioral data, has the potential to revolutionize smart healthcare. By leveraging approaches such as feature selection, rule-based systems, ML, deep learning, and NL, practitioners can extract valuable insights from a wealth of diverse sources, which will advance gains in knowledge and wisdom in healthcare.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">However, the challenges related to data quality, interoperability, privacy, security, data processing, clinical integration, and ethical considerations must be addressed. Future research should focus on Predictive, Preventive, Personalized, and Participatory approaches, the implementation or combination of which which can enable better anticipation of health events, identify risk factors, deliver tailored interventions, or further empower patients in their healthcare journeys. Embracing these opportunities will transform healthcare by improving patient well-being, treatment outcomes, and the overall function of the healthcare industry.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The authors gratefully acknowledge financial support ANID Fondecyt 1231122, PIA/PUENTE AFB220003, Chile.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:80%;">
R. L. Ackoff, “From data to wisdom,” </span><span class="ltx_text ltx_font_italic" id="bib.bib1.2.2" style="font-size:80%;">Journal of applied systems
analysis</span><span class="ltx_text" id="bib.bib1.3.3" style="font-size:80%;">, vol. 16, no. 1, pp. 3–9, 1989.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:80%;">
S. M. Fiore, J. Elias, E. Salas, N. W. Warner, and M. P. Letsky, “From data,
to information, to knowledge: Measuring knowledge building in the context of
collaborative cognition,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib2.2.2" style="font-size:80%;">Macrocognition metrics and scenarios</span><span class="ltx_text" id="bib.bib2.3.3" style="font-size:80%;">,
pp. 179–200, CRC Press, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:80%;">
X. Tao, T. Pham, J. Zhang, J. Yong, W. P. Goh, W. Zhang, and Y. Cai, “Mining
health knowledge graph for health risk prediction,” </span><span class="ltx_text ltx_font_italic" id="bib.bib3.2.2" style="font-size:80%;">World Wide Web</span><span class="ltx_text" id="bib.bib3.3.3" style="font-size:80%;">,
vol. 23, pp. 2341–2362, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:80%;">
J. Liang, Y. Li, Z. Zhang, D. Shen, J. Xu, X. Zheng, T. Wang, B. Tang, J. Lei,
J. Zhang, </span><span class="ltx_text ltx_font_italic" id="bib.bib4.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib4.3.3" style="font-size:80%;">, “Adoption of electronic health records (ehrs) in
china during the past 10 years: consecutive survey data analysis and
comparison of sino-american challenges and experiences,” </span><span class="ltx_text ltx_font_italic" id="bib.bib4.4.4" style="font-size:80%;">Journal of
medical Internet research</span><span class="ltx_text" id="bib.bib4.5.5" style="font-size:80%;">, vol. 23, no. 2, p. e24813, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:80%;">
Z. Zhang, E. P. Navarese, B. Zheng, Q. Meng, N. Liu, H. Ge, Q. Pan, Y. Yu, and
X. Ma, “Analytics with artificial intelligence to advance the treatment of
acute respiratory distress syndrome,” </span><span class="ltx_text ltx_font_italic" id="bib.bib5.2.2" style="font-size:80%;">Journal of Evidence-Based
Medicine</span><span class="ltx_text" id="bib.bib5.3.3" style="font-size:80%;">, vol. 13, no. 4, pp. 301–312, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:80%;">
E. Hossain, R. Rana, N. Higgins, J. Soar, P. D. Barua, A. R. Pisani, and
K. Turner, “Use of ai/ml-enabled state-of-the-art method in electronic
medical records: A systematic review,” </span><span class="ltx_text ltx_font_italic" id="bib.bib6.2.2" style="font-size:80%;">Computers in Biology and
Medicine</span><span class="ltx_text" id="bib.bib6.3.3" style="font-size:80%;">, p. 106649, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:80%;">
B. Ihnaini, M. Khan, T. A. Khan, S. Abbas, M. S. Daoud, M. Ahmad, and M. A.
Khan, “A smart healthcare recommendation system for multidisciplinary
diabetes patients with data fusion based on deep ensemble learning,” </span><span class="ltx_text ltx_font_italic" id="bib.bib7.2.2" style="font-size:80%;">Computational Intelligence and Neuroscience</span><span class="ltx_text" id="bib.bib7.3.3" style="font-size:80%;">, vol. 2021, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:80%;">
Z. Xu, D. R. So, and A. M. Dai, “Mufasa: Multimodal fusion architecture search
for electronic health records,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib8.2.2" style="font-size:80%;">Proceedings of the AAAI Conf. on
Artificial Intelligence</span><span class="ltx_text" id="bib.bib8.3.3" style="font-size:80%;">, vol. 35, pp. 10532–10540, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:80%;">
Y. An, H. Zhang, Y. Sheng, J. Wang, and X. Chen, “Main: Multimodal
</span><span class="ltx_text" id="bib.bib9.2.2" style="font-size:80%;">attention-based fusion networks for diagnosis prediction,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib9.3.3" style="font-size:80%;">2021 IEEE
Int’l Conf. on Bioinformatics and Biomedicine (BIBM)</span><span class="ltx_text" id="bib.bib9.4.4" style="font-size:80%;">, pp. 809–816, IEEE,
2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:80%;">
S. Malakar, S. D. Roy, S. Das, S. Sen, J. D. Velásquez, and R. Sarkar,
“Computer based diagnosis of some chronic diseases: A medical journey of the
last two decades,” </span><span class="ltx_text ltx_font_italic" id="bib.bib10.2.2" style="font-size:80%;">Archives of Computational Methods in Engineering</span><span class="ltx_text" id="bib.bib10.3.3" style="font-size:80%;">,
pp. 1–43, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:80%;">
A. Papa, M. Mital, P. Pisano, and M. Del Giudice, “E-health and wellbeing
monitoring using smart healthcare devices: An empirical investigation,” </span><span class="ltx_text ltx_font_italic" id="bib.bib11.2.2" style="font-size:80%;">Technological Forecasting and Social Change</span><span class="ltx_text" id="bib.bib11.3.3" style="font-size:80%;">, vol. 153, p. 119226, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:80%;">
E. Teixeira, H. Fonseca, F. Diniz-Sousa, L. Veras, G. Boppre, J. Oliveira,
D. Pinto, A. J. Alves, A. Barbosa, R. Mendes, </span><span class="ltx_text ltx_font_italic" id="bib.bib12.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib12.3.3" style="font-size:80%;">, “Wearable
devices for physical activity and healthcare monitoring in elderly people: A
critical review,” </span><span class="ltx_text ltx_font_italic" id="bib.bib12.4.4" style="font-size:80%;">Geriatrics</span><span class="ltx_text" id="bib.bib12.5.5" style="font-size:80%;">, vol. 6, no. 2, p. 38, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:80%;">
A. Sheth, U. Jaimini, and H. Y. Yip, “How will the internet of things enable
augmented personalized health?,” </span><span class="ltx_text ltx_font_italic" id="bib.bib13.2.2" style="font-size:80%;">IEEE intelligent systems</span><span class="ltx_text" id="bib.bib13.3.3" style="font-size:80%;">, vol. 33,
no. 1, pp. 89–97, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:80%;">
T. Shaik, X. Tao, N. Higgins, L. Li, R. Gururajan, X. Zhou, and U. R. Acharya,
“Remote patient monitoring using artificial intelligence: Current state,
applications, and challenges,” </span><span class="ltx_text ltx_font_italic" id="bib.bib14.2.2" style="font-size:80%;">Wiley Interdisciplinary Reviews: Data
Mining and Knowledge Discovery</span><span class="ltx_text" id="bib.bib14.3.3" style="font-size:80%;">, p. e1485, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:80%;">
X. Tao, T. B. Shaik, N. Higgins, R. Gururajan, and X. Zhou, “Remote patient
monitoring using radio frequency identification (rfid) technology and machine
learning for early detection of suicidal behaviour in mental health
facilities,” </span><span class="ltx_text ltx_font_italic" id="bib.bib15.2.2" style="font-size:80%;">Sensors</span><span class="ltx_text" id="bib.bib15.3.3" style="font-size:80%;">, vol. 21, no. 3, p. 776, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:80%;">
K. Mohammed, A. Zaidan, B. Zaidan, O. S. Albahri, M. Alsalem, A. S. Albahri,
A. Hadi, and M. Hashim, “Real-time remote-health monitoring systems: a
review on patients prioritisation for multiple-chronic diseases, taxonomy
analysis, concerns and solution procedure,” </span><span class="ltx_text ltx_font_italic" id="bib.bib16.2.2" style="font-size:80%;">Journal of medical
systems</span><span class="ltx_text" id="bib.bib16.3.3" style="font-size:80%;">, vol. 43, pp. 1–21, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:80%;">
L. A. Durán-Vega, P. C. Santana-Mancilla, R. Buenrostro-Mariscal,
J. Contreras-Castillo, L. E. Anido-Rifón, M. A. García-Ruiz, O. A.
Montesinos-López, and F. Estrada-González, “An iot system for remote
health monitoring in elderly adults through a wearable device and mobile
</span><span class="ltx_text" id="bib.bib17.2.2" style="font-size:80%;">application,” </span><span class="ltx_text ltx_font_italic" id="bib.bib17.3.3" style="font-size:80%;">Geriatrics</span><span class="ltx_text" id="bib.bib17.4.4" style="font-size:80%;">, vol. 4, no. 2, p. 34, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:80%;">
S. Tian, W. Yang, J. M. Le Grange, P. Wang, W. Huang, and Z. Ye, “Smart
healthcare: making medical care more intelligent,” </span><span class="ltx_text ltx_font_italic" id="bib.bib18.2.2" style="font-size:80%;">Global Health
Journal</span><span class="ltx_text" id="bib.bib18.3.3" style="font-size:80%;">, vol. 3, no. 3, pp. 62–65, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:80%;">
M. Senbekov, T. Saliev, Z. Bukeyeva, A. Almabayeva, M. Zhanaliyeva,
N. Aitenova, Y. Toishibekov, I. Fakhradiyev, </span><span class="ltx_text ltx_font_italic" id="bib.bib19.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib19.3.3" style="font-size:80%;">, “The recent
progress and applications of digital technologies in healthcare: a review,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib19.4.4" style="font-size:80%;">Int’l journal of telemedicine and applications</span><span class="ltx_text" id="bib.bib19.5.5" style="font-size:80%;">, vol. 2020, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:80%;">
M. S. Linet, T. L. Slovis, D. L. Miller, R. Kleinerman, C. Lee, P. Rajaraman,
and A. Berrington de Gonzalez, “Cancer risks associated with external
radiation from diagnostic imaging procedures,” </span><span class="ltx_text ltx_font_italic" id="bib.bib20.2.2" style="font-size:80%;">CA: a cancer journal for
clinicians</span><span class="ltx_text" id="bib.bib20.3.3" style="font-size:80%;">, vol. 62, no. 2, pp. 75–100, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:80%;">
X. Liu, L. Faes, A. U. Kale, S. K. Wagner, D. J. Fu, A. Bruynseels,
T. Mahendiran, G. Moraes, M. Shamdas, C. Kern, </span><span class="ltx_text ltx_font_italic" id="bib.bib21.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib21.3.3" style="font-size:80%;">, “A comparison
of deep learning performance against health-care professionals in detecting
diseases from medical imaging: a systematic review and meta-analysis,” </span><span class="ltx_text ltx_font_italic" id="bib.bib21.4.4" style="font-size:80%;">The lancet digital health</span><span class="ltx_text" id="bib.bib21.5.5" style="font-size:80%;">, vol. 1, no. 6, pp. e271–e297, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:80%;">
A. Garain, A. Basu, F. Giampaolo, J. D. Velasquez, and R. Sarkar, “Detection
of covid-19 from ct scan images: A spiking neural network-based approach,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib22.2.2" style="font-size:80%;">Neural Computing and Applications</span><span class="ltx_text" id="bib.bib22.3.3" style="font-size:80%;">, vol. 33, no. 19, pp. 12591–12604,
2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:80%;">
S. Das, S. D. Roy, S. Malakar, J. D. Velásquez, and R. Sarkar, “Bi-level
prediction model for screening covid-19 patients using chest x-ray images,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib23.2.2" style="font-size:80%;">Big Data Research</span><span class="ltx_text" id="bib.bib23.3.3" style="font-size:80%;">, vol. 25, p. 100233, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:80%;">
J. B. Awotunde, F. E. Ayo, R. G. Jimoh, R. O. Ogundokun, O. E. Matiluko, I. D.
Oladipo, and M. Abdulraheem, “Prediction and classification of diabetes
mellitus using genomic data,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib24.2.2" style="font-size:80%;">Intelligent IoT systems in
personalized health care</span><span class="ltx_text" id="bib.bib24.3.3" style="font-size:80%;">, pp. 235–292, Elsevier, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:80%;">
H. Yu, H. Yan, L. Wang, J. Li, L. Tan, W. Deng, Q. Chen, G. Yang, F. Zhang,
T. Lu, </span><span class="ltx_text ltx_font_italic" id="bib.bib25.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib25.3.3" style="font-size:80%;">, “Five novel loci associated with antipsychotic
treatment response in patients with schizophrenia: a genome-wide association
study,” </span><span class="ltx_text ltx_font_italic" id="bib.bib25.4.4" style="font-size:80%;">The Lancet Psychiatry</span><span class="ltx_text" id="bib.bib25.5.5" style="font-size:80%;">, vol. 5, no. 4, pp. 327–338, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib26.2.2" style="font-size:80%;">S. Pai and G. D. Bader, “Patient similarity networks for precision medicine,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib26.3.3" style="font-size:80%;">Journal of molecular biology</span><span class="ltx_text" id="bib.bib26.4.4" style="font-size:80%;">, vol. 430, no. 18, pp. 2924–2938, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:80%;">
J. N. Acosta, G. J. Falcone, P. Rajpurkar, and E. J. Topol, “Multimodal
biomedical ai,” </span><span class="ltx_text ltx_font_italic" id="bib.bib27.2.2" style="font-size:80%;">Nature Medicine</span><span class="ltx_text" id="bib.bib27.3.3" style="font-size:80%;">, vol. 28, no. 9, pp. 1773–1784, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:80%;">
O. Taiwo and A. E. Ezugwu, “Smart healthcare support for remote patient
monitoring during covid-19 quarantine,” </span><span class="ltx_text ltx_font_italic" id="bib.bib28.2.2" style="font-size:80%;">Informatics in medicine
unlocked</span><span class="ltx_text" id="bib.bib28.3.3" style="font-size:80%;">, vol. 20, p. 100428, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:80%;">
C. Carlsten, S. Salvi, G. W. Wong, and K. F. Chung, “Personal strategies to
minimise effects of air pollution on respiratory health: advice for
providers, patients and the public,” </span><span class="ltx_text ltx_font_italic" id="bib.bib29.2.2" style="font-size:80%;">European Respiratory Journal</span><span class="ltx_text" id="bib.bib29.3.3" style="font-size:80%;">,
vol. 55, no. 6, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:80%;">
M. Hu, J. D. Roberts, G. P. Azevedo, and D. Milner, “The role of built and
social environmental factors in covid-19 transmission: A look at america’s
capital city,” </span><span class="ltx_text ltx_font_italic" id="bib.bib30.2.2" style="font-size:80%;">Sustainable Cities and Society</span><span class="ltx_text" id="bib.bib30.3.3" style="font-size:80%;">, vol. 65, p. 102580,
2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib31.2.2" style="font-size:80%;">E. A. Alvarez, M. Garrido, D. P. Ponce, G. Pizarro, A. A. Córdova, F. Vera,
R. Ruiz, R. Fernández, J. D. Velásquez, E. Tobar, </span><span class="ltx_text ltx_font_italic" id="bib.bib31.3.3" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib31.4.4" style="font-size:80%;">, “A
software to prevent delirium in hospitalised older adults: development and
feasibility assessment,” </span><span class="ltx_text ltx_font_italic" id="bib.bib31.5.5" style="font-size:80%;">Age and Ageing</span><span class="ltx_text" id="bib.bib31.6.6" style="font-size:80%;">, vol. 49, no. 2, pp. 239–245,
2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:80%;">
T. J. Pollard, A. E. Johnson, J. D. Raffa, L. A. Celi, R. G. Mark, and
O. Badawi, “The eicu collaborative research database, a freely available
multi-center database for critical care research,” </span><span class="ltx_text ltx_font_italic" id="bib.bib32.2.2" style="font-size:80%;">Scientific data</span><span class="ltx_text" id="bib.bib32.3.3" style="font-size:80%;">,
vol. 5, no. 1, pp. 1–13, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:80%;">
A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghassemi,
B. Moody, P. Szolovits, L. Anthony Celi, and R. G. Mark, “Mimic-iii, a
freely accessible critical care database,” </span><span class="ltx_text ltx_font_italic" id="bib.bib33.2.2" style="font-size:80%;">Scientific data</span><span class="ltx_text" id="bib.bib33.3.3" style="font-size:80%;">, vol. 3,
no. 1, pp. 1–9, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:80%;">
D. Azcona, K. McGuinness, and A. F. Smeaton, “A comparative study of existing
and new deep learning methods for detecting knee injuries using the mrnet
dataset,” </span><span class="ltx_text ltx_font_italic" id="bib.bib34.2.2" style="font-size:80%;">arXiv preprint arXiv:2010.01947</span><span class="ltx_text" id="bib.bib34.3.3" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:80%;">
G. Shih, C. C. Wu, S. S. Halabi, M. D. Kohli, L. M. Prevedello, T. S. Cook,
</span><span class="ltx_text" id="bib.bib35.2.2" style="font-size:80%;">A. Sharma, J. K. Amorosa, V. Arteaga, M. Galperin-Aizenberg, </span><span class="ltx_text ltx_font_italic" id="bib.bib35.3.3" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib35.4.4" style="font-size:80%;">,
“Augmenting the national institutes of health chest radiograph dataset with
expert annotations of possible pneumonia,” </span><span class="ltx_text ltx_font_italic" id="bib.bib35.5.5" style="font-size:80%;">Radiology: Artificial
Intelligence</span><span class="ltx_text" id="bib.bib35.6.6" style="font-size:80%;">, vol. 1, no. 1, p. e180041, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:80%;">
P. Rajpurkar, J. Irvin, A. Bagul, D. Ding, T. Duan, H. Mehta, B. Yang, K. Zhu,
D. Laird, R. L. Ball, </span><span class="ltx_text ltx_font_italic" id="bib.bib36.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib36.3.3" style="font-size:80%;">, “Mura: Large dataset for abnormality
detection in musculoskeletal radiographs,” </span><span class="ltx_text ltx_font_italic" id="bib.bib36.4.4" style="font-size:80%;">arXiv preprint
arXiv:1712.06957</span><span class="ltx_text" id="bib.bib36.5.5" style="font-size:80%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:80%;">
S. S. Halabi, L. M. Prevedello, J. Kalpathy-Cramer, A. B. Mamonov, A. Bilbily,
M. Cicero, I. Pan, L. A. Pereira, R. T. Sousa, N. Abdala, </span><span class="ltx_text ltx_font_italic" id="bib.bib37.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib37.3.3" style="font-size:80%;">, “The
rsna pediatric bone age machine learning challenge,” </span><span class="ltx_text ltx_font_italic" id="bib.bib37.4.4" style="font-size:80%;">Radiology</span><span class="ltx_text" id="bib.bib37.5.5" style="font-size:80%;">,
vol. 290, no. 2, pp. 498–503, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:80%;">
D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan, L. Rodriguez,
S. Antani, G. R. Thoma, and C. J. McDonald, “Preparing a collection of
radiology examinations for distribution and retrieval,” </span><span class="ltx_text ltx_font_italic" id="bib.bib38.2.2" style="font-size:80%;">Journal of the
American Medical Informatics Association</span><span class="ltx_text" id="bib.bib38.3.3" style="font-size:80%;">, vol. 23, no. 2, pp. 304–310,
2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib39.2.2" style="font-size:80%;">J. Zbontar, F. Knoll, A. Sriram, T. Murrell, Z. Huang, M. J. Muckley,
A. Defazio, R. Stern, P. Johnson, M. Bruno, </span><span class="ltx_text ltx_font_italic" id="bib.bib39.3.3" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib39.4.4" style="font-size:80%;">, “fastmri: An open
dataset and benchmarks for accelerated mri,” </span><span class="ltx_text ltx_font_italic" id="bib.bib39.5.5" style="font-size:80%;">arXiv preprint
arXiv:1811.08839</span><span class="ltx_text" id="bib.bib39.6.6" style="font-size:80%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:80%;">
J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund,
B. Haghgoo, R. Ball, K. Shpanskaya, </span><span class="ltx_text ltx_font_italic" id="bib.bib40.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib40.3.3" style="font-size:80%;">, “Chexpert: A large chest
radiograph dataset with uncertainty labels and expert comparison,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib40.4.4" style="font-size:80%;">Proceedings of the AAAI Conf. on artificial intelligence</span><span class="ltx_text" id="bib.bib40.5.5" style="font-size:80%;">, vol. 33,
pp. 590–597, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:80%;">
D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky, J. C. Morris, and R. L.
Buckner, “Open access series of imaging studies (oasis): cross-sectional mri
data in young, middle aged, nondemented, and demented older adults,” </span><span class="ltx_text ltx_font_italic" id="bib.bib41.2.2" style="font-size:80%;">Journal of cognitive neuroscience</span><span class="ltx_text" id="bib.bib41.3.3" style="font-size:80%;">, vol. 19, no. 9, pp. 1498–1507, 2007.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:80%;">
S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer, A. P.
Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A. Hoffman, </span><span class="ltx_text ltx_font_italic" id="bib.bib42.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib42.3.3" style="font-size:80%;">,
“The lung image database consortium (lidc) and image database resource
initiative (idri): a completed reference database of lung nodules on ct
scans,” </span><span class="ltx_text ltx_font_italic" id="bib.bib42.4.4" style="font-size:80%;">Medical physics</span><span class="ltx_text" id="bib.bib42.5.5" style="font-size:80%;">, vol. 38, no. 2, pp. 915–931, 2011.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:80%;">
K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore,
S. Phillips, D. Maffitt, M. Pringle, </span><span class="ltx_text ltx_font_italic" id="bib.bib43.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib43.3.3" style="font-size:80%;">, “The cancer imaging
archive (tcia): maintaining and operating a public information repository,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib43.4.4" style="font-size:80%;">Journal of digital imaging</span><span class="ltx_text" id="bib.bib43.5.5" style="font-size:80%;">, vol. 26, pp. 1045–1057, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="font-size:80%;">
X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, “Chestx-ray8:
Hospital-scale chest x-ray database and benchmarks on weakly-supervised
classification and localization of common thorax diseases,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib44.2.2" style="font-size:80%;">Proceedings of the IEEE Conf. on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib44.3.3" style="font-size:80%;">,
pp. 2097–2106, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="font-size:80%;">
B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby,
Y. Burren, N. Porz, J. Slotboom, R. Wiest, </span><span class="ltx_text ltx_font_italic" id="bib.bib45.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib45.3.3" style="font-size:80%;">, “The multimodal
brain tumor image segmentation benchmark (brats),” </span><span class="ltx_text ltx_font_italic" id="bib.bib45.4.4" style="font-size:80%;">IEEE transactions on
medical imaging</span><span class="ltx_text" id="bib.bib45.5.5" style="font-size:80%;">, vol. 34, no. 10, pp. 1993–2024, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.1.1" style="font-size:80%;">
S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. S. Kirby, J. B.
Freymann, K. Farahani, and C. Davatzikos, “Advancing the cancer genome atlas
glioma mri collections with expert segmentation labels and radiomic
features,” </span><span class="ltx_text ltx_font_italic" id="bib.bib46.2.2" style="font-size:80%;">Scientific data</span><span class="ltx_text" id="bib.bib46.3.3" style="font-size:80%;">, vol. 4, no. 1, pp. 1–13, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.1.1" style="font-size:80%;">
S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R. T. Shinohara,
C. Berger, S. M. Ha, M. Rozycki, </span><span class="ltx_text ltx_font_italic" id="bib.bib47.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib47.3.3" style="font-size:80%;">, “Identifying the best machine
learning algorithms for brain tumor segmentation, progression assessment, and
overall survival prediction in the brats challenge,” </span><span class="ltx_text ltx_font_italic" id="bib.bib47.4.4" style="font-size:80%;">arXiv preprint
arXiv:1811.02629</span><span class="ltx_text" id="bib.bib47.5.5" style="font-size:80%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.1.1" style="font-size:80%;">
K. Tomczak, P. Czerwińska, and M. Wiznerowicz, “Review the cancer genome
atlas (tcga): an immeasurable source of knowledge,” </span><span class="ltx_text ltx_font_italic" id="bib.bib48.2.2" style="font-size:80%;">Contemporary
Oncology/Współczesna Onkologia</span><span class="ltx_text" id="bib.bib48.3.3" style="font-size:80%;">, vol. 2015, no. 1, pp. 68–77, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.1.1" style="font-size:80%;">
N. E. Allen, C. Sudlow, T. Peakman, R. Collins, and U. biobank, “Uk biobank
data: come and get it,” 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.1.1" style="font-size:80%;">
C. R. Jack Jr, M. A. Bernstein, N. C. Fox, P. Thompson, G. Alexander,
D. Harvey, B. Borowski, P. J. Britson, J. L. Whitwell, C. Ward, </span><span class="ltx_text ltx_font_italic" id="bib.bib50.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib50.3.3" style="font-size:80%;">,
“The alzheimer’s disease neuroimaging initiative (adni): Mri methods,” </span><span class="ltx_text ltx_font_italic" id="bib.bib50.4.4" style="font-size:80%;">Journal of Magnetic Resonance Imaging: An Official Journal of the Int’l
Society for Magnetic Resonance in Medicine</span><span class="ltx_text" id="bib.bib50.5.5" style="font-size:80%;">, vol. 27, no. 4, pp. 685–691,
2008.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib51.2.2" style="font-size:80%;">A. García Seco de Herrera, R. Schaer, S. Bromuri, and H. Müller, “Overview
of the ImageCLEF 2016 medical task,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib51.3.3" style="font-size:80%;">Working Notes of CLEF 2016
(Cross Language Evaluation Forum)</span><span class="ltx_text" id="bib.bib51.4.4" style="font-size:80%;">, September 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.1.1" style="font-size:80%;">
D. Demner-Fushman, S. Antani, M. Simpson, and G. R. Thoma, “Design and
development of a multimodal biomedical information retrieval system,” </span><span class="ltx_text ltx_font_italic" id="bib.bib52.2.2" style="font-size:80%;">Journal of Computing Science and Engineering</span><span class="ltx_text" id="bib.bib52.3.3" style="font-size:80%;">, vol. 6, no. 2, pp. 168–177,
2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.1.1" style="font-size:80%;">
A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov,
R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley,
“PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research
resource for complex physiologic signals,” </span><span class="ltx_text ltx_font_italic" id="bib.bib53.2.2" style="font-size:80%;">Circulation</span><span class="ltx_text" id="bib.bib53.3.3" style="font-size:80%;">, vol. 101,
no. 23, pp. e215–e220, 2000 (June 13).
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.4.1" style="font-size:80%;">Circulation Electronic Pages:
http://circ.ahajournals.org/content/101/23/e215.full PMID:1085218; doi:
10.1161/01.CIR.101.23.e215.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.1.1" style="font-size:80%;">
T. Feng, B. M. Booth, B. Baldwin-Rodríguez, F. Osorno, and S. Narayanan,
“A multimodal analysis of physical activity, sleep, and work shift in nurses
with wearable sensor data,” </span><span class="ltx_text ltx_font_italic" id="bib.bib54.2.2" style="font-size:80%;">Scientific reports</span><span class="ltx_text" id="bib.bib54.3.3" style="font-size:80%;">, vol. 11, no. 1,
p. 8693, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.1.1" style="font-size:80%;">
S. Zeadally and O. Bello, “Harnessing the power of internet of things based
connectivity to improve healthcare,” </span><span class="ltx_text ltx_font_italic" id="bib.bib55.2.2" style="font-size:80%;">Internet of Things</span><span class="ltx_text" id="bib.bib55.3.3" style="font-size:80%;">, vol. 14,
p. 100074, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.1.1" style="font-size:80%;">
K. Woodward, E. Kanjo, D. J. Brown, T. M. McGinnity, B. Inkster, D. J.
Macintyre, and A. Tsanas, “Beyond mobile apps: a survey of technologies for
mental well-being,” </span><span class="ltx_text ltx_font_italic" id="bib.bib56.2.2" style="font-size:80%;">IEEE Transactions on Affective Computing</span><span class="ltx_text" id="bib.bib56.3.3" style="font-size:80%;">, vol. 13,
no. 3, pp. 1216–1235, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.1.1" style="font-size:80%;">
S. Soklaridis, E. Lin, Y. Lalani, T. Rodak, and S. Sockalingam, “Mental health
interventions and supports during covid-19 and other medical pandemics: A
rapid systematic review of the evidence,” </span><span class="ltx_text ltx_font_italic" id="bib.bib57.2.2" style="font-size:80%;">General hospital psychiatry</span><span class="ltx_text" id="bib.bib57.3.3" style="font-size:80%;">,
vol. 66, pp. 133–146, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.1.1" style="font-size:80%;">
P. Bhowal, S. Sen, J. D. Velasquez, and R. Sarkar, “Fuzzy ensemble of deep
learning models using choquet fuzzy integral, coalition game and information
theory for breast cancer histology classification,” </span><span class="ltx_text ltx_font_italic" id="bib.bib58.2.2" style="font-size:80%;">Expert Systems with
Applications</span><span class="ltx_text" id="bib.bib58.3.3" style="font-size:80%;">, vol. 190, p. 116167, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib59.2.2" style="font-size:80%;">A. Albahri, A. M. Duhaim, M. A. Fadhel, A. Alnoor, N. S. Baqer, L. Alzubaidi,
O. Albahri, A. Alamoodi, J. Bai, A. Salhi, </span><span class="ltx_text ltx_font_italic" id="bib.bib59.3.3" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib59.4.4" style="font-size:80%;">, “A systematic
review of trustworthy and explainable artificial intelligence in healthcare:
assessment of quality, bias risk, and data fusion,” </span><span class="ltx_text ltx_font_italic" id="bib.bib59.5.5" style="font-size:80%;">Information
Fusion</span><span class="ltx_text" id="bib.bib59.6.6" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.1.1" style="font-size:80%;">
S. M. Alghowinem, T. Gedeon, R. Goecke, J. Cohn, and G. Parker,
“Interpretation of depression detection models via feature selection
methods,” </span><span class="ltx_text ltx_font_italic" id="bib.bib60.2.2" style="font-size:80%;">IEEE transactions on affective computing</span><span class="ltx_text" id="bib.bib60.3.3" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.1.1" style="font-size:80%;">
J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition using
multi-modal data and machine learning techniques: A tutorial and review,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib61.2.2" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib61.3.3" style="font-size:80%;">, vol. 59, pp. 103–126, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.1.1" style="font-size:80%;">
T. Zhang and M. Shi, “Multi-modal neuroimaging feature fusion for diagnosis of
alzheimer disease,” </span><span class="ltx_text ltx_font_italic" id="bib.bib62.2.2" style="font-size:80%;">Journal of Neuroscience Methods</span><span class="ltx_text" id="bib.bib62.3.3" style="font-size:80%;">, vol. 341,
p. 108795, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.1.1" style="font-size:80%;">
T. Zhou, M. Liu, K.-H. Thung, and D. Shen, “Latent representation learning for
alzheimer’s disease diagnosis with incomplete multi-modality neuroimaging and
genetic data,” </span><span class="ltx_text ltx_font_italic" id="bib.bib63.2.2" style="font-size:80%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib63.3.3" style="font-size:80%;">, vol. 38, no. 10,
</span><span class="ltx_text" id="bib.bib63.4.4" style="font-size:80%;">pp. 2411–2422, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.1.1" style="font-size:80%;">
D. Kim, Y.-H. Tsai, B. Zhuang, X. Yu, S. Sclaroff, K. Saenko, and
M. Chandraker, “Learning cross-modal contrastive features for video domain
adaptation,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib64.2.2" style="font-size:80%;">Proceedings of the IEEE/CVF Int’l Conf. on Computer
Vision</span><span class="ltx_text" id="bib.bib64.3.3" style="font-size:80%;">, pp. 13618–13627, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.1.1" style="font-size:80%;">
T. Hoang, T.-T. Do, T. V. Nguyen, and N.-M. Cheung, “Multimodal mutual
information maximization: A novel approach for unsupervised deep cross-modal
hashing,” </span><span class="ltx_text ltx_font_italic" id="bib.bib65.2.2" style="font-size:80%;">IEEE Transactions on Neural Networks and Learning Systems</span><span class="ltx_text" id="bib.bib65.3.3" style="font-size:80%;">,
2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.1.1" style="font-size:80%;">
S. Qiu, H. Zhao, N. Jiang, Z. Wang, L. Liu, Y. An, H. Zhao, X. Miao, R. Liu,
and G. Fortino, “Multi-sensor information fusion based on machine learning
for real applications in human activity recognition: State-of-the-art and
research challenges,” </span><span class="ltx_text ltx_font_italic" id="bib.bib66.2.2" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib66.3.3" style="font-size:80%;">, vol. 80, pp. 241–265, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.1.1" style="font-size:80%;">
M. Abdel-Basset, D. El-Shahat, I. El-Henawy, V. H. C. De Albuquerque, and
S. Mirjalili, “A new fusion of grey wolf optimizer algorithm with a
two-phase mutation for feature selection,” </span><span class="ltx_text ltx_font_italic" id="bib.bib67.2.2" style="font-size:80%;">Expert Systems with
Applications</span><span class="ltx_text" id="bib.bib67.3.3" style="font-size:80%;">, vol. 139, p. 112824, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.1.1" style="font-size:80%;">
J. Zhou, A. H. Gandomi, F. Chen, and A. Holzinger, “Evaluating the quality of
machine learning explanations: A survey on methods and metrics,” </span><span class="ltx_text ltx_font_italic" id="bib.bib68.2.2" style="font-size:80%;">Electronics</span><span class="ltx_text" id="bib.bib68.3.3" style="font-size:80%;">, vol. 10, no. 5, p. 593, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.1.1" style="font-size:80%;">
X. Hao, Y. Bao, Y. Guo, M. Yu, D. Zhang, S. L. Risacher, A. J. Saykin, X. Yao,
L. Shen, A. D. N. Initiative, </span><span class="ltx_text ltx_font_italic" id="bib.bib69.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib69.3.3" style="font-size:80%;">, “Multi-modal neuroimaging
feature selection with consistent metric constraint for diagnosis of
alzheimers disease,” </span><span class="ltx_text ltx_font_italic" id="bib.bib69.4.4" style="font-size:80%;">Medical image analysis</span><span class="ltx_text" id="bib.bib69.5.5" style="font-size:80%;">, vol. 60, p. 101625, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.1.1" style="font-size:80%;">
G. Yang, Q. Ye, and J. Xia, “Unbox the black-box for the medical explainable
ai via multi-modal and multi-centre data fusion: A mini-review, two showcases
and beyond,” </span><span class="ltx_text ltx_font_italic" id="bib.bib70.2.2" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib70.3.3" style="font-size:80%;">, vol. 77, pp. 29–52, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.1.1" style="font-size:80%;">
Y. Zhang, P. Tiňo, A. Leonardis, and K. Tang, “A survey on neural
network interpretability,” </span><span class="ltx_text ltx_font_italic" id="bib.bib71.2.2" style="font-size:80%;">IEEE Transactions on Emerging Topics in
Computational Intelligence</span><span class="ltx_text" id="bib.bib71.3.3" style="font-size:80%;">, vol. 5, no. 5, pp. 726–742, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.1.1" style="font-size:80%;">
G. Muhammad, F. Alshehri, F. Karray, A. El Saddik, M. Alsulaiman, and T. H.
Falk, “A comprehensive survey on multimodal medical signals fusion for smart
</span><span class="ltx_text" id="bib.bib72.2.2" style="font-size:80%;">healthcare systems,” </span><span class="ltx_text ltx_font_italic" id="bib.bib72.3.3" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib72.4.4" style="font-size:80%;">, vol. 76, pp. 355–375, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.1.1" style="font-size:80%;">
M. Hussain, F. A. Satti, S. I. Ali, J. Hussain, T. Ali, H.-S. Kim, K.-H. Yoon,
T. Chung, and S. Lee, “Intelligent knowledge consolidation: from data to
wisdom,” </span><span class="ltx_text ltx_font_italic" id="bib.bib73.2.2" style="font-size:80%;">Knowledge-Based Systems</span><span class="ltx_text" id="bib.bib73.3.3" style="font-size:80%;">, vol. 234, p. 107578, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.1.1" style="font-size:80%;">
T. Chen, C. Shang, P. Su, E. Keravnou-Papailiou, Y. Zhao, G. Antoniou, and
Q. Shen, “A decision tree-initialised neuro-fuzzy approach for clinical
decision support,” </span><span class="ltx_text ltx_font_italic" id="bib.bib74.2.2" style="font-size:80%;">Artificial Intelligence in Medicine</span><span class="ltx_text" id="bib.bib74.3.3" style="font-size:80%;">, vol. 111,
p. 101986, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.1.1" style="font-size:80%;">
T. K. Mohd, N. Nguyen, and A. Y. Javaid, “Multi-modal data fusion in enhancing
human-machine interaction for robotic applications: A survey,” </span><span class="ltx_text ltx_font_italic" id="bib.bib75.2.2" style="font-size:80%;">arXiv
preprint arXiv:2202.07732</span><span class="ltx_text" id="bib.bib75.3.3" style="font-size:80%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.1.1" style="font-size:80%;">
R. Yan, F. Zhang, X. Rao, Z. Lv, J. Li, L. Zhang, S. Liang, Y. Li, F. Ren,
C. Zheng, </span><span class="ltx_text ltx_font_italic" id="bib.bib76.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib76.3.3" style="font-size:80%;">, “Richer fusion network for breast cancer
classification based on multimodal data,” </span><span class="ltx_text ltx_font_italic" id="bib.bib76.4.4" style="font-size:80%;">BMC Medical Informatics and
Decision Making</span><span class="ltx_text" id="bib.bib76.5.5" style="font-size:80%;">, vol. 21, no. 1, pp. 1–15, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib77.2.2" style="font-size:80%;">A. Amirkhani, E. I. Papageorgiou, M. R. Mosavi, and K. Mohammadi, “A novel
medical decision support system based on fuzzy cognitive maps enhanced by
intuitive and learning capabilities for modeling uncertainty,” </span><span class="ltx_text ltx_font_italic" id="bib.bib77.3.3" style="font-size:80%;">Applied
Mathematics and Computation</span><span class="ltx_text" id="bib.bib77.4.4" style="font-size:80%;">, vol. 337, pp. 562–582, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.1.1" style="font-size:80%;">
A. Geramian, A. Abraham, and M. Ahmadi Nozari, “Fuzzy logic-based fmea robust
design: a quantitative approach for robustness against groupthink in
group/team decision-making,” </span><span class="ltx_text ltx_font_italic" id="bib.bib78.2.2" style="font-size:80%;">Int’l Journal of Production Research</span><span class="ltx_text" id="bib.bib78.3.3" style="font-size:80%;">,
vol. 57, no. 5, pp. 1331–1344, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.1.1" style="font-size:80%;">
A. Alharbi, A. Poujade, K. Malandrakis, I. Petrunin, D. Panagiotakopoulos, and
A. Tsourdos, “Rule-based conflict management for unmanned traffic management
scenarios,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib79.2.2" style="font-size:80%;">2020 AIAA/IEEE 39th Digital Avionics Systems Conf.
(DASC)</span><span class="ltx_text" id="bib.bib79.3.3" style="font-size:80%;">, pp. 1–10, IEEE, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.1.1" style="font-size:80%;">
K. Bahani, M. Moujabbir, and M. Ramdani, “An accurate fuzzy rule-based
classification systems for heart disease diagnosis,” </span><span class="ltx_text ltx_font_italic" id="bib.bib80.2.2" style="font-size:80%;">Scientific
African</span><span class="ltx_text" id="bib.bib80.3.3" style="font-size:80%;">, vol. 14, p. e01019, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.1.1" style="font-size:80%;">
A. M. Antoniadi, Y. Du, Y. Guendouz, L. Wei, C. Mazo, B. A. Becker, and
C. Mooney, “Current challenges and future opportunities for xai in machine
</span><span class="ltx_text" id="bib.bib81.2.2" style="font-size:80%;">learning-based clinical decision support systems: a systematic review,” </span><span class="ltx_text ltx_font_italic" id="bib.bib81.3.3" style="font-size:80%;">Applied Sciences</span><span class="ltx_text" id="bib.bib81.4.4" style="font-size:80%;">, vol. 11, no. 11, p. 5088, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.1.1" style="font-size:80%;">
W.-T. Wang and S.-Y. Wu, “Knowledge management based on information technology
in response to covid-19 crisis,” </span><span class="ltx_text ltx_font_italic" id="bib.bib82.2.2" style="font-size:80%;">Knowledge management research &amp;
practice</span><span class="ltx_text" id="bib.bib82.3.3" style="font-size:80%;">, vol. 19, no. 4, pp. 468–474, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.1.1" style="font-size:80%;">
L. Rundo, R. Pirrone, S. Vitabile, E. Sala, and O. Gambino, “Recent advances
of hci in decision-making tasks for optimized clinical workflows and
precision medicine,” </span><span class="ltx_text ltx_font_italic" id="bib.bib83.2.2" style="font-size:80%;">Journal of biomedical informatics</span><span class="ltx_text" id="bib.bib83.3.3" style="font-size:80%;">, vol. 108,
p. 103479, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.1.1" style="font-size:80%;">
S. El-Sappagh, F. Ali, T. Abuhmed, J. Singh, and J. M. Alonso, “Automatic
detection of alzheimer’s disease progression: An efficient information fusion
approach with heterogeneous ensemble classifiers,” </span><span class="ltx_text ltx_font_italic" id="bib.bib84.2.2" style="font-size:80%;">Neurocomputing</span><span class="ltx_text" id="bib.bib84.3.3" style="font-size:80%;">,
vol. 512, pp. 203–224, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.1.1" style="font-size:80%;">
K. Srinivasan, K. Raman, J. Chen, M. Bendersky, and M. Najork, “Wit:
Wikipedia-based image text dataset for multimodal multilingual machine
learning,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib85.2.2" style="font-size:80%;">Proceedings of the 44th Int’l ACM SIGIR Conf. on Research
and Development in Information Retrieval</span><span class="ltx_text" id="bib.bib85.3.3" style="font-size:80%;">, pp. 2443–2449, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.1.1" style="font-size:80%;">
M. Yan, Z. Deng, B. He, C. Zou, J. Wu, and Z. Zhu, “Emotion classification
with multichannel physiological signals using hybrid feature and adaptive
decision fusion,” </span><span class="ltx_text ltx_font_italic" id="bib.bib86.2.2" style="font-size:80%;">Biomedical Signal Processing and Control</span><span class="ltx_text" id="bib.bib86.3.3" style="font-size:80%;">, vol. 71,
p. 103235, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.1.1" style="font-size:80%;">
A. de Souza Brito, M. B. Vieira, S. M. Villela, H. Tacon, H. de Lima Chaves,
H. de Almeida Maia, D. T. Concha, and H. Pedrini, “Weighted voting of
multi-stream convolutional neural networks for video-based action recognition
using optical flow rhythms,” </span><span class="ltx_text ltx_font_italic" id="bib.bib87.2.2" style="font-size:80%;">Journal of Visual Communication and Image
Representation</span><span class="ltx_text" id="bib.bib87.3.3" style="font-size:80%;">, vol. 77, p. 103112, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.1.1" style="font-size:80%;">
J. Gaebel, H.-G. Wu, A. Oeser, M. A. Cypko, M. Stoehr, A. Dietz, T. Neumuth,
S. Franke, and S. Oeltze-Jafra, “Modeling and processing up-to-dateness of
patient information in probabilistic therapy decision support,” </span><span class="ltx_text ltx_font_italic" id="bib.bib88.2.2" style="font-size:80%;">Artificial Intelligence in Medicine</span><span class="ltx_text" id="bib.bib88.3.3" style="font-size:80%;">, vol. 104, p. 101842, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.1.1" style="font-size:80%;">
J. Chen and Y. Liu, “Multimodality data fusion for probabilistic strength
estimation of aging materials using bayesian networks,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib89.2.2" style="font-size:80%;">AIAA Scitech
2020 Forum</span><span class="ltx_text" id="bib.bib89.3.3" style="font-size:80%;">, p. 1653, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.1.1" style="font-size:80%;">
P. Cao, X. Liu, J. Yang, D. Zhao, M. Huang, and O. Zaiane, “l2, 1- l1
regularized nonlinear multi-task representation learning based cognitive
performance prediction of alzheimers disease,” </span><span class="ltx_text ltx_font_italic" id="bib.bib90.2.2" style="font-size:80%;">Pattern Recognition</span><span class="ltx_text" id="bib.bib90.3.3" style="font-size:80%;">,
vol. 79, pp. 195–215, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.1.1" style="font-size:80%;">
S. Sharma and P. K. Mandal, “A comprehensive report on machine learning-based
early detection of alzheimer’s disease using multi-modal neuroimaging data,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib91.2.2" style="font-size:80%;">ACM Computing Surveys (CSUR)</span><span class="ltx_text" id="bib.bib91.3.3" style="font-size:80%;">, vol. 55, no. 2, pp. 1–44, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.1.1" style="font-size:80%;">
K. M. M. Lopez, M. S. A. Magboo, A. Tallón-Ballesteros, and C. Chen, “A
clinical decision support tool to detect invasive ductal carcinoma in
histopathological images using support vector machines, naïve-bayes, and
k-nearest neighbor classifiers.,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib92.2.2" style="font-size:80%;">MLIS</span><span class="ltx_text" id="bib.bib92.3.3" style="font-size:80%;">, pp. 46–53, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.1.1" style="font-size:80%;">
Y. Liu, Z. Gu, T. H. Ko, and J. Liu, “Identifying key opinion leaders in
social media via modality-consistent harmonized discriminant embedding,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib93.2.2" style="font-size:80%;">IEEE Transactions on Cybernetics</span><span class="ltx_text" id="bib.bib93.3.3" style="font-size:80%;">, vol. 50, no. 2, pp. 717–728, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.1.1" style="font-size:80%;">
C. Zhang, Z. Yang, X. He, and L. Deng, “Multimodal intelligence:
Representation learning, information fusion, and applications,” </span><span class="ltx_text ltx_font_italic" id="bib.bib94.2.2" style="font-size:80%;">IEEE
Journal of Selected Topics in Signal Processing</span><span class="ltx_text" id="bib.bib94.3.3" style="font-size:80%;">, vol. 14, no. 3,
pp. 478–493, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.1.1" style="font-size:80%;">
F. Anowar, S. Sadaoui, and B. Selim, “Conceptual and empirical comparison of
dimensionality reduction algorithms (pca, kpca, lda, mds, svd, lle, isomap,
le, ica, t-sne),” </span><span class="ltx_text ltx_font_italic" id="bib.bib95.2.2" style="font-size:80%;">Computer Science Review</span><span class="ltx_text" id="bib.bib95.3.3" style="font-size:80%;">, vol. 40, p. 100378, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.1.1" style="font-size:80%;">
S. Zheng, Z. Zhu, Z. Liu, Z. Guo, Y. Liu, Y. Yang, and Y. Zhao, “Multi-modal
graph learning for disease prediction,” </span><span class="ltx_text ltx_font_italic" id="bib.bib96.2.2" style="font-size:80%;">IEEE Transactions on Medical
Imaging</span><span class="ltx_text" id="bib.bib96.3.3" style="font-size:80%;">, vol. 41, no. 9, pp. 2207–2216, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.1.1" style="font-size:80%;">
S. Yan, Y. Xiong, and D. Lin, “Spatial temporal graph convolutional networks
for skeleton-based action recognition,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib97.2.2" style="font-size:80%;">Proceedings of the AAAI
Conf. on artificial intelligence</span><span class="ltx_text" id="bib.bib97.3.3" style="font-size:80%;">, vol. 32, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.1.1" style="font-size:80%;">
M. Hügle, G. Kalweit, T. Hügle, and J. Boedecker, “A dynamic deep
neural network for multimodal clinical data analysis,” </span><span class="ltx_text ltx_font_italic" id="bib.bib98.2.2" style="font-size:80%;">Explainable AI
in Healthcare and Medicine: Building a Culture of Transparency and
Accountability</span><span class="ltx_text" id="bib.bib98.3.3" style="font-size:80%;">, pp. 79–92, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib99.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib99.2.2" style="font-size:80%;">A. Elboushaki, R. Hannane, K. Afdel, and L. Koutti, “Multid-cnn: A
multi-dimensional feature learning approach based on deep convolutional
networks for gesture recognition in rgb-d image sequences,” </span><span class="ltx_text ltx_font_italic" id="bib.bib99.3.3" style="font-size:80%;">Expert
Systems with Applications</span><span class="ltx_text" id="bib.bib99.4.4" style="font-size:80%;">, vol. 139, p. 112829, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib100.1.1" style="font-size:80%;">
K. M. Rashid and J. Louis, “Times-series data augmentation and deep learning
for construction equipment activity recognition,” </span><span class="ltx_text ltx_font_italic" id="bib.bib100.2.2" style="font-size:80%;">Advanced Engineering
Informatics</span><span class="ltx_text" id="bib.bib100.3.3" style="font-size:80%;">, vol. 42, p. 100944, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib101.1.1" style="font-size:80%;">
N. Bahador, J. Jokelainen, S. Mustola, and J. Kortelainen, “Multimodal
spatio-temporal-spectral fusion for deep learning applications in
physiological time series processing: A case study in monitoring the depth of
anesthesia,” </span><span class="ltx_text ltx_font_italic" id="bib.bib101.2.2" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib101.3.3" style="font-size:80%;">, vol. 73, pp. 125–143, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib102.1.1" style="font-size:80%;">
X. Wang, G. Chen, G. Qian, P. Gao, X.-Y. Wei, Y. Wang, Y. Tian, and W. Gao,
“Large-scale multi-modal pre-trained models: A comprehensive survey,” </span><span class="ltx_text ltx_font_italic" id="bib.bib102.2.2" style="font-size:80%;">arXiv preprint arXiv:2302.10035</span><span class="ltx_text" id="bib.bib102.3.3" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib103.1.1" style="font-size:80%;">
G. Ayana, K. Dese, and S.-w. Choe, “Transfer learning in breast cancer
diagnoses via ultrasound imaging,” </span><span class="ltx_text ltx_font_italic" id="bib.bib103.2.2" style="font-size:80%;">Cancers</span><span class="ltx_text" id="bib.bib103.3.3" style="font-size:80%;">, vol. 13, no. 4, p. 738,
2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib104.1.1" style="font-size:80%;">
A. de Santana Correia and E. L. Colombini, “Attention, please! a survey of
neural attention models in deep learning,” </span><span class="ltx_text ltx_font_italic" id="bib.bib104.2.2" style="font-size:80%;">Artificial Intelligence
Review</span><span class="ltx_text" id="bib.bib104.3.3" style="font-size:80%;">, vol. 55, no. 8, pp. 6037–6124, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib105.1.1" style="font-size:80%;">
Z. Niu, G. Zhong, and H. Yu, “A review on the attention mechanism of deep
learning,” </span><span class="ltx_text ltx_font_italic" id="bib.bib105.2.2" style="font-size:80%;">Neurocomputing</span><span class="ltx_text" id="bib.bib105.3.3" style="font-size:80%;">, vol. 452, pp. 48–62, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib106.1.1" style="font-size:80%;">
Y. Shi, B. Paige, P. Torr, </span><span class="ltx_text ltx_font_italic" id="bib.bib106.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib106.3.3" style="font-size:80%;">, “Variational mixture-of-experts
autoencoders for multi-modal deep generative models,” </span><span class="ltx_text ltx_font_italic" id="bib.bib106.4.4" style="font-size:80%;">Advances in
Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib106.5.5" style="font-size:80%;">, vol. 32, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib107.1.1" style="font-size:80%;">
C. Du, C. Du, and H. He, “Multimodal deep generative adversarial models for
scalable doubly semi-supervised learning,” </span><span class="ltx_text ltx_font_italic" id="bib.bib107.2.2" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib107.3.3" style="font-size:80%;">,
vol. 68, pp. 118–130, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib108.1.1" style="font-size:80%;">
H. R. V. Joze, A. Shaban, M. L. Iuzzolino, and K. Koishida, “Mmtm: Multimodal
transfer module for cnn fusion,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib108.2.2" style="font-size:80%;">Proceedings of the IEEE/CVF Conf.
on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib108.3.3" style="font-size:80%;">, pp. 13289–13299, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib109.1.1" style="font-size:80%;">
Y. Zhang, D. Sidibé, O. Morel, and F. Mériaudeau, “Deep multimodal
fusion for semantic image segmentation: A survey,” </span><span class="ltx_text ltx_font_italic" id="bib.bib109.2.2" style="font-size:80%;">Image and Vision
Computing</span><span class="ltx_text" id="bib.bib109.3.3" style="font-size:80%;">, vol. 105, p. 104042, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib110.1.1" style="font-size:80%;">
R. Carvalho, A. C. Morgado, C. Andrade, T. Nedelcu, A. Carreiro, and M. J. M.
Vasconcelos, “Integrating domain knowledge into deep learning for skin
lesion risk prioritization to assist teledermatology referral,” </span><span class="ltx_text ltx_font_italic" id="bib.bib110.2.2" style="font-size:80%;">Diagnostics</span><span class="ltx_text" id="bib.bib110.3.3" style="font-size:80%;">, vol. 12, no. 1, p. 36, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib111.1.1" style="font-size:80%;">
D. Jin, E. Sergeeva, W.-H. Weng, G. Chauhan, and P. Szolovits, “Explainable
deep learning in healthcare: A methodological survey from an attribution
view,” </span><span class="ltx_text ltx_font_italic" id="bib.bib111.2.2" style="font-size:80%;">WIREs Mechanisms of Disease</span><span class="ltx_text" id="bib.bib111.3.3" style="font-size:80%;">, vol. 14, no. 3, p. e1548, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib112.1.1" style="font-size:80%;">
R. Sevastjanova, F. Beck, B. Ell, C. Turkay, R. Henkin, M. Butt, D. A. Keim,
and M. El-Assady, “Going beyond visualization: Verbalization as
complementary medium to explain machine learning models,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib112.2.2" style="font-size:80%;">Workshop
on Visualization for AI Explainability at IEEE VIS</span><span class="ltx_text" id="bib.bib112.3.3" style="font-size:80%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib113.1.1" style="font-size:80%;">
K. M. Boehm, P. Khosravi, R. Vanguri, J. Gao, and S. P. Shah, “Harnessing
multimodal data integration to advance precision oncology,” </span><span class="ltx_text ltx_font_italic" id="bib.bib113.2.2" style="font-size:80%;">Nature
Reviews Cancer</span><span class="ltx_text" id="bib.bib113.3.3" style="font-size:80%;">, vol. 22, no. 2, pp. 114–126, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib114.1.1" style="font-size:80%;">
T. Shaik, X. Tao, Y. Li, C. Dann, J. McDonald, P. Redmond, and L. Galligan, “A
review of the trends and challenges in adopting natural language processing
methods for education feedback analysis,” </span><span class="ltx_text ltx_font_italic" id="bib.bib114.2.2" style="font-size:80%;">IEEE Access</span><span class="ltx_text" id="bib.bib114.3.3" style="font-size:80%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib115.1.1" style="font-size:80%;">
Z. Zeng, Y. Deng, X. Li, T. Naumann, and Y. Luo, “Natural language processing
for ehr-based computational phenotyping,” </span><span class="ltx_text ltx_font_italic" id="bib.bib115.2.2" style="font-size:80%;">IEEE/ACM transactions on
computational biology and bioinformatics</span><span class="ltx_text" id="bib.bib115.3.3" style="font-size:80%;">, vol. 16, no. 1, pp. 139–153,
2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib116.1.1" style="font-size:80%;">
P. Bhatia, B. Celikkaya, M. Khalilia, and S. Senthivel, “Comprehend medical: a
named entity recognition and relationship extraction web service,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib116.2.2" style="font-size:80%;">2019 18th IEEE Int’l Conf. On Machine Learning And Applications (ICMLA)</span><span class="ltx_text" id="bib.bib116.3.3" style="font-size:80%;">,
pp. 1844–1851, IEEE, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib117.1.1" style="font-size:80%;">
D. Demner-Fushman, N. Elhadad, and C. Friedman, “Natural language processing
for health-related texts,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib117.2.2" style="font-size:80%;">Biomedical Informatics: Computer
Applications in Health Care and Biomedicine</span><span class="ltx_text" id="bib.bib117.3.3" style="font-size:80%;">, pp. 241–272, Springer, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib118.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib118.2.2" style="font-size:80%;">E. Petrova, P. Pauwels, K. Svidt, and R. L. Jensen, “Towards data-driven
sustainable design: decision support based on knowledge discovery in
disparate building data,” </span><span class="ltx_text ltx_font_italic" id="bib.bib118.3.3" style="font-size:80%;">Architectural Engineering and Design
Management</span><span class="ltx_text" id="bib.bib118.4.4" style="font-size:80%;">, vol. 15, no. 5, pp. 334–356, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib119.1.1" style="font-size:80%;">
T. Pham, X. Tao, J. Zhang, and J. Yong, “Constructing a knowledge-based
heterogeneous information graph for medical health status classification,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib119.2.2" style="font-size:80%;">Health information science and systems</span><span class="ltx_text" id="bib.bib119.3.3" style="font-size:80%;">, vol. 8, pp. 1–14, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib120.1.1" style="font-size:80%;">
M. Tang, P. Gandhi, M. A. Kabir, C. Zou, J. Blakey, and X. Luo, “Progress
notes classification and keyword extraction using attention-based deep
learning models with bert,” </span><span class="ltx_text ltx_font_italic" id="bib.bib120.2.2" style="font-size:80%;">arXiv preprint arXiv:1910.05786</span><span class="ltx_text" id="bib.bib120.3.3" style="font-size:80%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib121.1.1" style="font-size:80%;">
N. Chintalapudi, G. Battineni, M. Di Canio, G. G. Sagaro, and F. Amenta, “Text
mining with sentiment analysis on seafarers’ medical documents,” </span><span class="ltx_text ltx_font_italic" id="bib.bib121.2.2" style="font-size:80%;">Int’l Journal of Information Management Data Insights</span><span class="ltx_text" id="bib.bib121.3.3" style="font-size:80%;">, vol. 1, no. 1,
p. 100005, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib122.1.1" style="font-size:80%;">
S. Bozkurt, E. Alkim, I. Banerjee, and D. L. Rubin, “Automated detection of
measurements and their descriptors in radiology reports using a hybrid
natural language processing algorithm,” </span><span class="ltx_text ltx_font_italic" id="bib.bib122.2.2" style="font-size:80%;">Journal of digital imaging</span><span class="ltx_text" id="bib.bib122.3.3" style="font-size:80%;">,
</span><span class="ltx_text" id="bib.bib122.4.4" style="font-size:80%;">vol. 32, pp. 544–553, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib123.1.1" style="font-size:80%;">
X. Pei, K. Zuo, Y. Li, and Z. Pang, “A review of the application of
multi-modal deep learning in medicine: Bibliometrics and future directions,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib123.2.2" style="font-size:80%;">Int’l Journal of Computational Intelligence Systems</span><span class="ltx_text" id="bib.bib123.3.3" style="font-size:80%;">, vol. 16, no. 1,
pp. 1–20, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib124.1.1" style="font-size:80%;">
L. Wang, M. Rastegar-Mojarad, Z. Ji, S. Liu, K. Liu, S. Moon, F. Shen, Y. Wang,
L. Yao, J. M. Davis III, </span><span class="ltx_text ltx_font_italic" id="bib.bib124.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib124.3.3" style="font-size:80%;">, “Detecting pharmacovigilance signals
combining electronic medical records with spontaneous reports: a case study
of conventional disease-modifying antirheumatic drugs for rheumatoid
arthritis,” </span><span class="ltx_text ltx_font_italic" id="bib.bib124.4.4" style="font-size:80%;">Frontiers in pharmacology</span><span class="ltx_text" id="bib.bib124.5.5" style="font-size:80%;">, vol. 9, p. 875, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib125.1.1" style="font-size:80%;">
M. F. Guiñazú, V. Cortés, C. F. Ibáñez, and J. D.
Velásquez, “Employing online social networks in precision-medicine
approach using information fusion predictive model to improve substance use
surveillance: A lesson from twitter and marijuana consumption,” </span><span class="ltx_text ltx_font_italic" id="bib.bib125.2.2" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib125.3.3" style="font-size:80%;">, vol. 55, pp. 150–163, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib126.1.1" style="font-size:80%;">
A. Choudhury, O. Asan, </span><span class="ltx_text ltx_font_italic" id="bib.bib126.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib126.3.3" style="font-size:80%;">, “Role of artificial intelligence in
patient safety outcomes: systematic literature review,” </span><span class="ltx_text ltx_font_italic" id="bib.bib126.4.4" style="font-size:80%;">JMIR medical
informatics</span><span class="ltx_text" id="bib.bib126.5.5" style="font-size:80%;">, vol. 8, no. 7, p. e18599, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib127.1.1" style="font-size:80%;">
A. Le Glaz, Y. Haralambous, D.-H. Kim-Dufor, P. Lenca, R. Billot, T. C. Ryan,
J. Marsh, J. Devylder, M. Walter, S. Berrouiguet, </span><span class="ltx_text ltx_font_italic" id="bib.bib127.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib127.3.3" style="font-size:80%;">, “Machine
learning and natural language processing in mental health: systematic
review,” </span><span class="ltx_text ltx_font_italic" id="bib.bib127.4.4" style="font-size:80%;">Journal of Medical Internet Research</span><span class="ltx_text" id="bib.bib127.5.5" style="font-size:80%;">, vol. 23, no. 5,
p. e15708, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib128.1.1" style="font-size:80%;">
J. Lipkova, R. J. Chen, B. Chen, M. Y. Lu, M. Barbieri, D. Shao, A. J. Vaidya,
C. Chen, L. Zhuang, D. F. Williamson, </span><span class="ltx_text ltx_font_italic" id="bib.bib128.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib128.3.3" style="font-size:80%;">, “Artificial intelligence
for multimodal data integration in oncology,” </span><span class="ltx_text ltx_font_italic" id="bib.bib128.4.4" style="font-size:80%;">Cancer Cell</span><span class="ltx_text" id="bib.bib128.5.5" style="font-size:80%;">, vol. 40,
no. 10, pp. 1095–1110, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib129.1.1" style="font-size:80%;">
B. N. Hiremath and M. M. Patil, “Enhancing optimized personalized therapy in
clinical decision support system using natural language processing,” </span><span class="ltx_text ltx_font_italic" id="bib.bib129.2.2" style="font-size:80%;">Journal of King Saud University-Computer and Information Sciences</span><span class="ltx_text" id="bib.bib129.3.3" style="font-size:80%;">, vol. 34,
no. 6, pp. 2840–2848, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib130.1.1" style="font-size:80%;">
I. Spasic, G. Nenadic, </span><span class="ltx_text ltx_font_italic" id="bib.bib130.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib130.3.3" style="font-size:80%;">, “Clinical text data in machine learning:
systematic review,” </span><span class="ltx_text ltx_font_italic" id="bib.bib130.4.4" style="font-size:80%;">JMIR medical informatics</span><span class="ltx_text" id="bib.bib130.5.5" style="font-size:80%;">, vol. 8, no. 3,
p. e17984, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib131.1.1" style="font-size:80%;">
I. Perez-Pozuelo, B. Zhai, J. Palotti, R. Mall, M. Aupetit, J. M. Garcia-Gomez,
S. Taheri, Y. Guan, and L. Fernandez-Luque, “The future of sleep health: a
data-driven revolution in sleep science and medicine,” </span><span class="ltx_text ltx_font_italic" id="bib.bib131.2.2" style="font-size:80%;">NPJ digital
medicine</span><span class="ltx_text" id="bib.bib131.3.3" style="font-size:80%;">, vol. 3, no. 1, p. 42, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib132.1.1" style="font-size:80%;">
J. Sun, H. Shi, J. Zhu, B. Song, Y. Tao, and S. Tan, “Self-attention-based
multi-block regression fusion neural network for quality-related process
monitoring,” </span><span class="ltx_text ltx_font_italic" id="bib.bib132.2.2" style="font-size:80%;">Journal of the Taiwan Institute of Chemical Engineers</span><span class="ltx_text" id="bib.bib132.3.3" style="font-size:80%;">,
vol. 133, p. 104140, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib133.1.1" style="font-size:80%;">
F. A. Reegu, H. Abas, Y. Gulzar, Q. Xin, A. A. Alwan, A. Jabbari, R. G.
Sonkamble, and R. A. Dziyauddin, “Blockchain-based framework for
interoperable electronic health records for an improved healthcare system,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib133.2.2" style="font-size:80%;">Sustainability</span><span class="ltx_text" id="bib.bib133.3.3" style="font-size:80%;">, vol. 15, no. 8, p. 6337, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib134.1.1" style="font-size:80%;">
C. Lyketsos, S. Roberts, E. K. Swift, A. Quina, G. Moon, I. Kremer, P. Tariot,
H. Fillit, D. Bovenkamp, P. Zandi, </span><span class="ltx_text ltx_font_italic" id="bib.bib134.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib134.3.3" style="font-size:80%;">, “Standardizing electronic
health record data on ad/adrd to accelerate health equity in prevention,
detection, and treatment,” </span><span class="ltx_text ltx_font_italic" id="bib.bib134.4.4" style="font-size:80%;">The journal of prevention of Alzheimers
disease</span><span class="ltx_text" id="bib.bib134.5.5" style="font-size:80%;">, vol. 9, no. 3, pp. 556–560, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib135.1.1" style="font-size:80%;">
G. Diraco, G. Rescio, P. Siciliano, and A. Leone, “Review on human action
recognition in smart living: Multimodality, real-time processing,
interoperability, resource-constrained processing, and sensing technology,”
2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib136.1.1" style="font-size:80%;">
C. Mwangi, C. Mukanya, and C. Maghanga, “Assessing the interoperability of
mlab and ushauri mhealth systems to enhance care for hiv/aids patients in
kenya,” </span><span class="ltx_text ltx_font_italic" id="bib.bib136.2.2" style="font-size:80%;">Journal of Intellectual Property and Information Technology Law
(JIPIT)</span><span class="ltx_text" id="bib.bib136.3.3" style="font-size:80%;">, vol. 2, no. 1, pp. 83–116, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib137.1.1" style="font-size:80%;">
M. Kor, I. Yitmen, and S. Alizadehsalehi, “An investigation for integration of
deep learning and digital twins towards construction 4.0,” </span><span class="ltx_text ltx_font_italic" id="bib.bib137.2.2" style="font-size:80%;">Smart and
Sustainable Built Environment</span><span class="ltx_text" id="bib.bib137.3.3" style="font-size:80%;">, vol. 12, no. 3, pp. 461–487, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib138.1.1" style="font-size:80%;">
X. Tao and J. D. Velásquez, “Multi-source information fusion for smart health
with artificial intelligence,” </span><span class="ltx_text ltx_font_italic" id="bib.bib138.2.2" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib138.3.3" style="font-size:80%;">, vol. 83–84,
pp. 93–95, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib139.1.1" style="font-size:80%;">
M. Paul, L. Maglaras, M. A. Ferrag, and I. AlMomani, “Digitization of
</span><span class="ltx_text" id="bib.bib139.2.2" style="font-size:80%;">healthcare sector: A study on privacy and security concerns,” </span><span class="ltx_text ltx_font_italic" id="bib.bib139.3.3" style="font-size:80%;">ICT
Express</span><span class="ltx_text" id="bib.bib139.4.4" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib140.1.1" style="font-size:80%;">
I. Yasser, A. T. Khalil, M. A. Mohamed, A. S. Samra, and F. Khalifa, “A robust
chaos-based technique for medical image encryption,” </span><span class="ltx_text ltx_font_italic" id="bib.bib140.2.2" style="font-size:80%;">IEEE Access</span><span class="ltx_text" id="bib.bib140.3.3" style="font-size:80%;">,
vol. 10, pp. 244–257, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib141.1.1" style="font-size:80%;">
P. B. Regade, A. A. Patil, S. S. Koli, R. B. Gokavi, and M. Bhandigare,
“Survey on secure file storage on cloud using hybrid cryptography,” </span><span class="ltx_text ltx_font_italic" id="bib.bib141.2.2" style="font-size:80%;">Int’l Research Journal of Modernization in Engineering Technology and
Science</span><span class="ltx_text" id="bib.bib141.3.3" style="font-size:80%;">, vol. 4, no. 06, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib142.1.1" style="font-size:80%;">
Y. Al-Issa, M. A. Ottom, and A. Tamrawi, “ehealth cloud security challenges: a
survey,” </span><span class="ltx_text ltx_font_italic" id="bib.bib142.2.2" style="font-size:80%;">Journal of healthcare engineering</span><span class="ltx_text" id="bib.bib142.3.3" style="font-size:80%;">, vol. 2019, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib143.1.1" style="font-size:80%;">
M. Mohammed, S. Desyansah, S. Al-Zubaidi, and E. Yusuf, “An internet of
things-based smart homes and healthcare monitoring and management system,”
in </span><span class="ltx_text ltx_font_italic" id="bib.bib143.2.2" style="font-size:80%;">Journal of Physics: Conf. Series</span><span class="ltx_text" id="bib.bib143.3.3" style="font-size:80%;">, vol. 1450, p. 012079, IOP
Publishing, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib144.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib144.2.2" style="font-size:80%;">J. J. Hathaliya, S. Tanwar, and P. Sharma, “Adversarial learning techniques
for security and privacy preservation: A comprehensive review,” </span><span class="ltx_text ltx_font_italic" id="bib.bib144.3.3" style="font-size:80%;">Security and Privacy</span><span class="ltx_text" id="bib.bib144.4.4" style="font-size:80%;">, vol. 5, no. 3, p. e209, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib145.1.1" style="font-size:80%;">
N. N. Neto, S. Madnick, A. M. G. de Paula, and N. Malara Borges, “A case study
of the capital one data breach: Why didn’t compliance requirements help
prevent it?,” </span><span class="ltx_text ltx_font_italic" id="bib.bib145.2.2" style="font-size:80%;">Journal of Information System Security</span><span class="ltx_text" id="bib.bib145.3.3" style="font-size:80%;">, vol. 17, no. 1,
2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib146.1.1" style="font-size:80%;">
R. Kumar and R. Goyal, “On cloud security requirements, threats,
vulnerabilities and countermeasures: A survey,” </span><span class="ltx_text ltx_font_italic" id="bib.bib146.2.2" style="font-size:80%;">Computer Science
Review</span><span class="ltx_text" id="bib.bib146.3.3" style="font-size:80%;">, vol. 33, pp. 1–48, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib147.1.1" style="font-size:80%;">
V. R. Kebande, N. M. Karie, and R. A. Ikuesan, “Real-time monitoring as a
supplementary security component of vigilantism in modern network
environments,” </span><span class="ltx_text ltx_font_italic" id="bib.bib147.2.2" style="font-size:80%;">Int’l Journal of Information Technology</span><span class="ltx_text" id="bib.bib147.3.3" style="font-size:80%;">, vol. 13,
pp. 5–17, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib148.1.1" style="font-size:80%;">
R. Bokade, A. Navato, R. Ouyang, X. Jin, C.-A. Chou, S. Ostadabbas, and A. V.
Mueller, “A cross-disciplinary comparison of multimodal data fusion
approaches and applications: Accelerating learning through trans-disciplinary
</span><span class="ltx_text" id="bib.bib148.2.2" style="font-size:80%;">information sharing,” </span><span class="ltx_text ltx_font_italic" id="bib.bib148.3.3" style="font-size:80%;">Expert Systems with Applications</span><span class="ltx_text" id="bib.bib148.4.4" style="font-size:80%;">, vol. 165,
p. 113885, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib149.1.1" style="font-size:80%;">
A. M. Flores, F. Demsas, N. J. Leeper, and E. G. Ross, “Leveraging machine
learning and artificial intelligence to improve peripheral artery disease
detection, treatment, and outcomes,” </span><span class="ltx_text ltx_font_italic" id="bib.bib149.2.2" style="font-size:80%;">Circulation research</span><span class="ltx_text" id="bib.bib149.3.3" style="font-size:80%;">, vol. 128,
no. 12, pp. 1833–1850, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib150.1.1" style="font-size:80%;">
M. Swathy and K. Saruladha, “A comparative study of classification and
prediction of cardio-vascular diseases (cvd) using machine learning and deep
learning techniques,” </span><span class="ltx_text ltx_font_italic" id="bib.bib150.2.2" style="font-size:80%;">ICT Express</span><span class="ltx_text" id="bib.bib150.3.3" style="font-size:80%;">, vol. 8, no. 1, pp. 109–116, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib151.1.1" style="font-size:80%;">
I. Banerjee, Y. Ling, M. C. Chen, S. A. Hasan, C. P. Langlotz, N. Moradzadeh,
B. Chapman, T. Amrhein, D. Mong, D. L. Rubin, </span><span class="ltx_text ltx_font_italic" id="bib.bib151.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib151.3.3" style="font-size:80%;">, “Comparative
effectiveness of convolutional neural network (cnn) and recurrent neural
network (rnn) architectures for radiology text report classification,” </span><span class="ltx_text ltx_font_italic" id="bib.bib151.4.4" style="font-size:80%;">Artificial intelligence in medicine</span><span class="ltx_text" id="bib.bib151.5.5" style="font-size:80%;">, vol. 97, pp. 79–88, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib152.1.1" style="font-size:80%;">
A. Coronato, M. Naeem, G. De Pietro, and G. Paragliola, “Reinforcement
learning for intelligent healthcare applications: A survey,” </span><span class="ltx_text ltx_font_italic" id="bib.bib152.2.2" style="font-size:80%;">Artificial
Intelligence in Medicine</span><span class="ltx_text" id="bib.bib152.3.3" style="font-size:80%;">, vol. 109, p. 101964, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib153.1.1" style="font-size:80%;">
D. Wang, J. D. Weisz, M. Muller, P. Ram, W. Geyer, C. Dugan, Y. Tausczik,
H. Samulowitz, and A. Gray, “Human-ai collaboration in data science:
Exploring data scientists’ perceptions of automated ai,” </span><span class="ltx_text ltx_font_italic" id="bib.bib153.2.2" style="font-size:80%;">Proceedings of
the ACM on human-computer interaction</span><span class="ltx_text" id="bib.bib153.3.3" style="font-size:80%;">, vol. 3, no. CSCW, pp. 1–24, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib154.1.1" style="font-size:80%;">
I. H. Sarker, “Data science and analytics: an overview from data-driven smart
computing, decision-making and applications perspective,” </span><span class="ltx_text ltx_font_italic" id="bib.bib154.2.2" style="font-size:80%;">SN Computer
Science</span><span class="ltx_text" id="bib.bib154.3.3" style="font-size:80%;">, vol. 2, no. 5, p. 377, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib155.1.1" style="font-size:80%;">
S. Steyaert, M. Pizurica, D. Nagaraj, P. Khandelwal, T. Hernandez-Boussard,
A. J. Gentles, and O. Gevaert, “Multimodal data fusion for cancer biomarker
discovery with deep learning,” </span><span class="ltx_text ltx_font_italic" id="bib.bib155.2.2" style="font-size:80%;">Nature Machine Intelligence</span><span class="ltx_text" id="bib.bib155.3.3" style="font-size:80%;">, pp. 1–12,
2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib156.1.1" style="font-size:80%;">
D. Wang, L. Wang, Z. Zhang, D. Wang, H. Zhu, Y. Gao, X. Fan, and F. Tian,
““brilliant ai doctor” in rural clinics: Challenges in ai-powered
clinical decision support system deployment,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib156.2.2" style="font-size:80%;">Proceedings of the
2021 CHI Conf. on human factors in computing systems</span><span class="ltx_text" id="bib.bib156.3.3" style="font-size:80%;">, pp. 1–18, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib157.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib157.2.2" style="font-size:80%;">E. Nazari, M. H. Shahriari, and H. Tabesh, “Bigdata analysis in healthcare:
apache hadoop, apache spark and apache flink,” </span><span class="ltx_text ltx_font_italic" id="bib.bib157.3.3" style="font-size:80%;">Frontiers in Health
Informatics</span><span class="ltx_text" id="bib.bib157.4.4" style="font-size:80%;">, vol. 8, no. 1, p. 14, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib158.1.1" style="font-size:80%;">
A. Kaur, P. Singh, and A. Nayyar, “Fog computing: Building a road to iot with
fog analytics,” </span><span class="ltx_text ltx_font_italic" id="bib.bib158.2.2" style="font-size:80%;">Fog Data Analytics for IoT Applications: Next
Generation Process Model with State of the Art Technologies</span><span class="ltx_text" id="bib.bib158.3.3" style="font-size:80%;">, pp. 59–78,
2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib159.1.1" style="font-size:80%;">
R. Dwivedi, D. Mehrotra, and S. Chandra, “Potential of internet of medical
things (iomt) applications in building a smart healthcare system: A
systematic review,” </span><span class="ltx_text ltx_font_italic" id="bib.bib159.2.2" style="font-size:80%;">Journal of oral biology and craniofacial research</span><span class="ltx_text" id="bib.bib159.3.3" style="font-size:80%;">,
vol. 12, no. 2, pp. 302–318, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib160.1.1" style="font-size:80%;">
Q. Qi, F. Tao, T. Hu, N. Anwer, A. Liu, Y. Wei, L. Wang, and A. Nee, “Enabling
technologies and tools for digital twin,” </span><span class="ltx_text ltx_font_italic" id="bib.bib160.2.2" style="font-size:80%;">Journal of Manufacturing
Systems</span><span class="ltx_text" id="bib.bib160.3.3" style="font-size:80%;">, vol. 58, pp. 3–21, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib161.1.1" style="font-size:80%;">
P. K. R. Maddikunta, Q.-V. Pham, B. Prabadevi, N. Deepa, K. Dev, T. R.
Gadekallu, R. Ruby, and M. Liyanage, “Industry 5.0: A survey on enabling
technologies and potential applications,” </span><span class="ltx_text ltx_font_italic" id="bib.bib161.2.2" style="font-size:80%;">Journal of Industrial
Information Integration</span><span class="ltx_text" id="bib.bib161.3.3" style="font-size:80%;">, vol. 26, p. 100257, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib162.1.1" style="font-size:80%;">
A. Vakil, J. Liu, P. Zulch, E. Blasch, R. Ewing, and J. Li, “A survey of
multimodal sensor fusion for passive rf and eo information integration,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib162.2.2" style="font-size:80%;">IEEE Aerospace and Electronic Systems Magazine</span><span class="ltx_text" id="bib.bib162.3.3" style="font-size:80%;">, vol. 36, no. 7,
pp. 44–61, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib163.1.1" style="font-size:80%;">
L. You, M. Danaf, F. Zhao, J. Guan, C. L. Azevedo, B. Atasoy, and M. Ben-Akiva,
“A federated platform enabling a systematic collaboration among devices,
data and functions for smart mobility,” </span><span class="ltx_text ltx_font_italic" id="bib.bib163.2.2" style="font-size:80%;">IEEE Transactions on
Intelligent Transportation Systems</span><span class="ltx_text" id="bib.bib163.3.3" style="font-size:80%;">, vol. 24, no. 4, pp. 4060–4074, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib164.1.1" style="font-size:80%;">
R. Dabliz, S. K. Poon, A. Ritchie, R. Burke, and J. Penm, “Usability
evaluation of an integrated electronic medication management system
implemented in an oncology setting using the unified theory of acceptance and
use of technology,” </span><span class="ltx_text ltx_font_italic" id="bib.bib164.2.2" style="font-size:80%;">BMC Medical Informatics and Decision Making</span><span class="ltx_text" id="bib.bib164.3.3" style="font-size:80%;">,
vol. 21, no. 1, pp. 1–11, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib165.1.1" style="font-size:80%;">
B. N. Limketkai, K. Mauldin, N. Manitius, L. Jalilian, and B. R. Salonen, “The
age of artificial intelligence: use of digital technology in clinical
nutrition,” </span><span class="ltx_text ltx_font_italic" id="bib.bib165.2.2" style="font-size:80%;">Current surgery reports</span><span class="ltx_text" id="bib.bib165.3.3" style="font-size:80%;">, vol. 9, no. 7, p. 20, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib166.1.1" style="font-size:80%;">
X. Chen, H. Xie, Z. Li, G. Cheng, M. Leng, and F. L. Wang, “Information fusion
and artificial intelligence for smart healthcare: a bibliometric study,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib166.2.2" style="font-size:80%;">Information Processing &amp; Management</span><span class="ltx_text" id="bib.bib166.3.3" style="font-size:80%;">, vol. 60, no. 1, p. 103113, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib167.1.1" style="font-size:80%;">
V. M. O’Hara, S. V. Johnston, and N. T. Browne, “The paediatric weight
management office visit via telemedicine: pre-to post-covid-19 pandemic,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib167.2.2" style="font-size:80%;">Pediatric obesity</span><span class="ltx_text" id="bib.bib167.3.3" style="font-size:80%;">, vol. 15, no. 8, p. e12694, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib168.1.1" style="font-size:80%;">
A. Holzinger, M. Dehmer, F. Emmert-Streib, R. Cucchiara, I. Augenstein,
J. Del Ser, W. Samek, I. Jurisica, and N. Díaz-Rodríguez,
“Information fusion as an integrative cross-cutting enabler to achieve
robust, explainable, and trustworthy medical artificial intelligence,” </span><span class="ltx_text ltx_font_italic" id="bib.bib168.2.2" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib168.3.3" style="font-size:80%;">, vol. 79, pp. 263–278, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib169.1.1" style="font-size:80%;">
L. Baum, M. Johns, M. Poikela, R. Möller, B. Ananthasubramaniam, and
F. Prasser, “Data integration and analysis for circadian medicine,” </span><span class="ltx_text ltx_font_italic" id="bib.bib169.2.2" style="font-size:80%;">Acta Physiologica</span><span class="ltx_text" id="bib.bib169.3.3" style="font-size:80%;">, vol. 237, no. 4, p. e13951, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib170.1.1" style="font-size:80%;">
S. M. van Rooden, O. Aspevall, E. Carrara, S. Gubbels, A. Johansson, J.-C.
</span><span class="ltx_text" id="bib.bib170.2.2" style="font-size:80%;">Lucet, S. Mookerjee, Z. R. Palacios-Baena, E. Presterl, E. Tacconelli, </span><span class="ltx_text ltx_font_italic" id="bib.bib170.3.3" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib170.4.4" style="font-size:80%;">, “Governance aspects of large-scale implementation of automated
surveillance of healthcare-associated infections,” </span><span class="ltx_text ltx_font_italic" id="bib.bib170.5.5" style="font-size:80%;">Clinical
Microbiology and Infection</span><span class="ltx_text" id="bib.bib170.6.6" style="font-size:80%;">, vol. 27, pp. S20–S28, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib171.1.1" style="font-size:80%;">
C. Thapa and S. Camtepe, “Precision health data: Requirements, challenges and
existing techniques for data security and privacy,” </span><span class="ltx_text ltx_font_italic" id="bib.bib171.2.2" style="font-size:80%;">Computers in
biology and medicine</span><span class="ltx_text" id="bib.bib171.3.3" style="font-size:80%;">, vol. 129, p. 104130, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib172.1.1" style="font-size:80%;">
N. Gaw, S. Yousefi, and M. R. Gahrooei, “Multimodal data fusion for systems
improvement: A review,” </span><span class="ltx_text ltx_font_italic" id="bib.bib172.2.2" style="font-size:80%;">IISE Transactions</span><span class="ltx_text" id="bib.bib172.3.3" style="font-size:80%;">, vol. 54, no. 11,
pp. 1098–1116, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib173.1.1" style="font-size:80%;">
J. Mökander, J. Morley, M. Taddeo, and L. Floridi, “Ethics-based auditing
of automated decision-making systems: Nature, scope, and limitations,” </span><span class="ltx_text ltx_font_italic" id="bib.bib173.2.2" style="font-size:80%;">Science and Engineering Ethics</span><span class="ltx_text" id="bib.bib173.3.3" style="font-size:80%;">, vol. 27, no. 4, p. 44, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib174.1.1" style="font-size:80%;">
L. Belgodère, D. P. Bertrand, M. C. Jaulent, V. Rabeharisoa, W. Janssens,
V. Rollason, J. Barbot, J. P. Vernant, W. O. Gonin, P. Maison, </span><span class="ltx_text ltx_font_italic" id="bib.bib174.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib174.3.3" style="font-size:80%;">,
“Patient and public involvement in the benefit–risk assessment and decision
concerning health products: position of the scientific advisory board of the
</span><span class="ltx_text" id="bib.bib174.4.4" style="font-size:80%;">french national agency for medicines and health products safety (ansm),”
</span><span class="ltx_text ltx_font_italic" id="bib.bib174.5.5" style="font-size:80%;">BMJ Global Health</span><span class="ltx_text" id="bib.bib174.6.6" style="font-size:80%;">, vol. 8, no. 5, p. e011966, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib175.1.1" style="font-size:80%;">
S. Ali, T. Abuhmed, S. El-Sappagh, K. Muhammad, J. M. Alonso-Moral,
R. Confalonieri, R. Guidotti, J. Del Ser, N. Díaz-Rodríguez, and
F. Herrera, “Explainable artificial intelligence (xai): What we know and
what is left to attain trustworthy artificial intelligence,” </span><span class="ltx_text ltx_font_italic" id="bib.bib175.2.2" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib175.3.3" style="font-size:80%;">, p. 101805, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib176.1.1" style="font-size:80%;">
N. Rostamzadeh, S. S. Abdullah, and K. Sedig, “Visual analytics for electronic
health records: a review,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib176.2.2" style="font-size:80%;">Informatics</span><span class="ltx_text" id="bib.bib176.3.3" style="font-size:80%;">, vol. 8, p. 12, MDPI, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib177.1.1" style="font-size:80%;">
T. Höllt, A. Vilanova, N. Pezzotti, B. P. Lelieveldt, and H. Hauser,
“Focus+ context exploration of hierarchical embeddings,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib177.2.2" style="font-size:80%;">Computer
Graphics Forum</span><span class="ltx_text" id="bib.bib177.3.3" style="font-size:80%;">, vol. 38, pp. 569–579, Wiley Online Library, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib178.1.1" style="font-size:80%;">
A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik,
A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins, </span><span class="ltx_text ltx_font_italic" id="bib.bib178.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib178.3.3" style="font-size:80%;">, “Explainable artificial intelligence (xai): Concepts, taxonomies,
opportunities and challenges toward responsible ai,” </span><span class="ltx_text ltx_font_italic" id="bib.bib178.4.4" style="font-size:80%;">Information
fusion</span><span class="ltx_text" id="bib.bib178.5.5" style="font-size:80%;">, vol. 58, pp. 82–115, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib179.1.1" style="font-size:80%;">
A. Holzinger, M. Dehmer, F. Emmert-Streib, R. Cucchiara, I. Augenstein, J. D.
Ser, W. Samek, I. Jurisica, and N. Díaz-Rodríguez,
“Information fusion as an integrative cross-cutting enabler to achieve
robust, explainable, and trustworthy medical artificial intelligence,” </span><span class="ltx_text ltx_font_italic" id="bib.bib179.2.2" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib179.3.3" style="font-size:80%;">, vol. 79, pp. 263–278, Mar. 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib180.1.1" style="font-size:80%;">
Y. Mao, D. Wang, M. Muller, K. R. Varshney, I. Baldini, C. Dugan, and
A. Mojsilović, “How data ScientistsWork together with domain experts
in scientific collaborations,” </span><span class="ltx_text ltx_font_italic" id="bib.bib180.2.2" style="font-size:80%;">Proceedings of the ACM on
Human-Computer Interaction</span><span class="ltx_text" id="bib.bib180.3.3" style="font-size:80%;">, vol. 3, pp. 1–23, Dec. 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib181.1.1" style="font-size:80%;">
L. Müller, A. Srinivasan, S. R. Abeles, A. Rajagopal, F. J. Torriani, and
E. Aronoff-Spencer, “A risk-based clinical decision support system for
patient-specific antimicrobial therapy (iBiogram): Design and retrospective
analysis,” </span><span class="ltx_text ltx_font_italic" id="bib.bib181.2.2" style="font-size:80%;">Journal of Medical Internet Research</span><span class="ltx_text" id="bib.bib181.3.3" style="font-size:80%;">, vol. 23, p. e23571,
Dec. 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib182.1.1" style="font-size:80%;">
T. Pham, X. Tao, J. Zhang, J. Yong, Y. Li, and H. Xie, “Graph-based
multi-label disease prediction model learning from medical data and domain
knowledge,” </span><span class="ltx_text ltx_font_italic" id="bib.bib182.2.2" style="font-size:80%;">Knowledge-based systems</span><span class="ltx_text" id="bib.bib182.3.3" style="font-size:80%;">, vol. 235, p. 107662, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib183.1.1" style="font-size:80%;">
G. Collatuzzo and P. Boffetta, “Application of p4 (predictive, preventive,
personalized, participatory) approach to occupational medicine,” </span><span class="ltx_text ltx_font_italic" id="bib.bib183.2.2" style="font-size:80%;">La
Medicina del Lavoro</span><span class="ltx_text" id="bib.bib183.3.3" style="font-size:80%;">, vol. 113, no. 1, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib184.1.1" style="font-size:80%;">
R. B. Ruiz and J. D. Velásquez, “Artificial intelligence for the future of
medicine,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib184.2.2" style="font-size:80%;">Artificial Intelligence and Machine Learning for
Healthcare: Vol. 2: Emerging Methodologies and Trends</span><span class="ltx_text" id="bib.bib184.3.3" style="font-size:80%;">, pp. 1–28, Springer,
2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib185.1.1" style="font-size:80%;">
A. Esteva, A. Robicquet, B. Ramsundar, V. Kuleshov, M. DePristo, K. Chou,
C. Cui, G. Corrado, S. Thrun, and J. Dean, “A guide to deep learning in
healthcare,” </span><span class="ltx_text ltx_font_italic" id="bib.bib185.2.2" style="font-size:80%;">Nature medicine</span><span class="ltx_text" id="bib.bib185.3.3" style="font-size:80%;">, vol. 25, no. 1, pp. 24–29, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib186.1.1" style="font-size:80%;">
M. A. Myszczynska, P. N. Ojamies, A. M. Lacoste, D. Neil, A. Saffari, R. Mead,
G. M. Hautbergue, J. D. Holbrook, and L. Ferraiuolo, “Applications of
machine learning to diagnosis and treatment of neurodegenerative diseases,”
</span><span class="ltx_text ltx_font_italic" id="bib.bib186.2.2" style="font-size:80%;">Nature Reviews Neurology</span><span class="ltx_text" id="bib.bib186.3.3" style="font-size:80%;">, vol. 16, no. 8, pp. 440–456, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib187.1.1" style="font-size:80%;">
Y.-D. Zhang, Z. Dong, S.-H. Wang, X. Yu, X. Yao, Q. Zhou, H. Hu, M. Li,
</span><span class="ltx_text" id="bib.bib187.2.2" style="font-size:80%;">C. Jiménez-Mesa, J. Ramirez, </span><span class="ltx_text ltx_font_italic" id="bib.bib187.3.3" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib187.4.4" style="font-size:80%;">, “Advances in multimodal data
fusion in neuroimaging: overview, challenges, and novel orientation,” </span><span class="ltx_text ltx_font_italic" id="bib.bib187.5.5" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib187.6.6" style="font-size:80%;">, vol. 64, pp. 149–187, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib188.1.1" style="font-size:80%;">
X.-a. Bi, X. Hu, Y. Xie, and H. Wu, “A novel cernne approach for predicting
parkinson’s disease-associated genes and brain regions based on multimodal
imaging genetics data,” </span><span class="ltx_text ltx_font_italic" id="bib.bib188.2.2" style="font-size:80%;">Medical Image Analysis</span><span class="ltx_text" id="bib.bib188.3.3" style="font-size:80%;">, vol. 67, p. 101830,
2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib189.1.1" style="font-size:80%;">
L. A. Vale-Silva and K. Rohr, “Long-term cancer survival prediction using
multimodal deep learning,” </span><span class="ltx_text ltx_font_italic" id="bib.bib189.2.2" style="font-size:80%;">Scientific Reports</span><span class="ltx_text" id="bib.bib189.3.3" style="font-size:80%;">, vol. 11, no. 1,
p. 13505, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib190.1.1" style="font-size:80%;">
R. Nabbout and M. Kuchenbuch, “Impact of predictive, preventive and precision
medicine strategies in epilepsy,” </span><span class="ltx_text ltx_font_italic" id="bib.bib190.2.2" style="font-size:80%;">Nature Reviews Neurology</span><span class="ltx_text" id="bib.bib190.3.3" style="font-size:80%;">, vol. 16,
no. 12, pp. 674–688, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib191.1.1" style="font-size:80%;">
T. Shaik, X. Tao, N. Higgins, H. Xie, R. Gururajan, and X. Zhou, “Ai enabled
rpm for mental health facility,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib191.2.2" style="font-size:80%;">Proceedings of the 1st ACM Workshop
on Mobile and Wireless Sensing for Smart Healthcare</span><span class="ltx_text" id="bib.bib191.3.3" style="font-size:80%;">, pp. 26–32, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib192.1.1" style="font-size:80%;">
M. C. Liefaard, E. H. Lips, J. Wesseling, N. M. Hylton, B. Lou, T. Mansi, and
L. Pusztai, “The way of the future: personalizing treatment plans through
technology,” </span><span class="ltx_text ltx_font_italic" id="bib.bib192.2.2" style="font-size:80%;">American Society of Clinical Oncology Educational Book</span><span class="ltx_text" id="bib.bib192.3.3" style="font-size:80%;">,
vol. 41, pp. 12–23, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib193.1.1" style="font-size:80%;">
T. Shaik, X. Tao, N. Higgins, R. Gururajan, Y. Li, X. Zhou, and U. R. Acharya,
“Fedstack: Personalized activity monitoring using stacked federated
learning,” </span><span class="ltx_text ltx_font_italic" id="bib.bib193.2.2" style="font-size:80%;">Knowledge-Based Systems</span><span class="ltx_text" id="bib.bib193.3.3" style="font-size:80%;">, vol. 257, p. 109929, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib194.1.1" style="font-size:80%;">
A. A. T. Naqvi, K. Fatima, T. Mohammad, U. Fatima, I. K. Singh, A. Singh, S. M.
Atif, G. Hariprasad, G. M. Hasan, and M. I. Hassan, “Insights into
sars-cov-2 genome, structure, evolution, pathogenesis and therapies:
Structural genomics approach,” </span><span class="ltx_text ltx_font_italic" id="bib.bib194.2.2" style="font-size:80%;">Biochimica et Biophysica Acta
(BBA)-Molecular Basis of Disease</span><span class="ltx_text" id="bib.bib194.3.3" style="font-size:80%;">, vol. 1866, no. 10, p. 165878, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib195.1.1" style="font-size:80%;">
D. Horgan, T. Čufer, F. Gatto, I. Lugowska, D. Verbanac, Â. Carvalho,
J. A. Lal, M. Kozaric, S. Toomey, H. Y. Ivanov, </span><span class="ltx_text ltx_font_italic" id="bib.bib195.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib195.3.3" style="font-size:80%;">, “Accelerating
the development and validation of liquid biopsy for early cancer screening
and treatment tailoring,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib195.4.4" style="font-size:80%;">Healthcare</span><span class="ltx_text" id="bib.bib195.5.5" style="font-size:80%;">, vol. 10, p. 1714, MDPI, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib196.1.1" style="font-size:80%;">
</span><span class="ltx_text" id="bib.bib196.2.2" style="font-size:80%;">H. Cai, Z. Qu, Z. Li, Y. Zhang, X. Hu, and B. Hu, “Feature-level fusion
approaches based on multimodal eeg data for depression recognition,” </span><span class="ltx_text ltx_font_italic" id="bib.bib196.3.3" style="font-size:80%;">Information Fusion</span><span class="ltx_text" id="bib.bib196.4.4" style="font-size:80%;">, vol. 59, pp. 127–138, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib197.1.1" style="font-size:80%;">
J. Mateo, L. Steuten, P. Aftimos, F. André, M. Davies, E. Garralda,
J. Geissler, D. Husereau, I. Martinez-Lopez, N. Normanno, </span><span class="ltx_text ltx_font_italic" id="bib.bib197.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib197.3.3" style="font-size:80%;">,
“Delivering precision oncology to patients with cancer,” </span><span class="ltx_text ltx_font_italic" id="bib.bib197.4.4" style="font-size:80%;">Nature
Medicine</span><span class="ltx_text" id="bib.bib197.5.5" style="font-size:80%;">, vol. 28, no. 4, pp. 658–665, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib198.1.1" style="font-size:80%;">
G. Aceto, V. Persico, and A. Pescapé, “Industry 4.0 and health: Internet
of things, big data, and cloud computing for healthcare 4.0,” </span><span class="ltx_text ltx_font_italic" id="bib.bib198.2.2" style="font-size:80%;">Journal
of Industrial Information Integration</span><span class="ltx_text" id="bib.bib198.3.3" style="font-size:80%;">, vol. 18, p. 100129, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib199.1.1" style="font-size:80%;">
K. M. Boehm, E. A. Aherne, L. Ellenson, I. Nikolovski, M. Alghamdi,
I. Vázquez-García, D. Zamarin, K. Long Roche, Y. Liu, D. Patel, </span><span class="ltx_text ltx_font_italic" id="bib.bib199.2.2" style="font-size:80%;">et al.</span><span class="ltx_text" id="bib.bib199.3.3" style="font-size:80%;">, “Multimodal data integration using machine learning improves risk
stratification of high-grade serous ovarian cancer,” </span><span class="ltx_text ltx_font_italic" id="bib.bib199.4.4" style="font-size:80%;">Nature cancer</span><span class="ltx_text" id="bib.bib199.5.5" style="font-size:80%;">,
vol. 3, no. 6, pp. 723–733, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib200.1.1" style="font-size:80%;">
P. Carayon, A. Wooldridge, P. Hoonakker, A. S. Hundt, and M. M. Kelly, “Seips
3.0: Human-centered design of the patient journey for patient safety,” </span><span class="ltx_text ltx_font_italic" id="bib.bib200.2.2" style="font-size:80%;">Applied ergonomics</span><span class="ltx_text" id="bib.bib200.3.3" style="font-size:80%;">, vol. 84, p. 103033, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib201.1.1" style="font-size:80%;">
H. Dhayne, R. Haque, R. Kilany, and Y. Taher, “In search of big medical data
integration solutions-a comprehensive survey,” </span><span class="ltx_text ltx_font_italic" id="bib.bib201.2.2" style="font-size:80%;">IEEE Access</span><span class="ltx_text" id="bib.bib201.3.3" style="font-size:80%;">, vol. 7,
pp. 91265–91290, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib202.1.1" style="font-size:80%;">
A. El Saddik, F. Laamarti, and M. Alja’Afreh, “The potential of digital
twins,” </span><span class="ltx_text ltx_font_italic" id="bib.bib202.2.2" style="font-size:80%;">IEEE Instrumentation &amp; Measurement Magazine</span><span class="ltx_text" id="bib.bib202.3.3" style="font-size:80%;">, vol. 24, no. 3,
pp. 36–41, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib203.1.1" style="font-size:80%;">
K. Walker, J. Yates, T. Dening, B. Völlm, J. Tomlin, and C. Griffiths,
“Quality of life, wellbeing, recovery, and progress for older forensic
mental health patients: a qualitative investigation based on the perspectives
of patients and staff,” </span><span class="ltx_text ltx_font_italic" id="bib.bib203.2.2" style="font-size:80%;">Int’l Journal of Qualitative Studies on Health
and Well-being</span><span class="ltx_text" id="bib.bib203.3.3" style="font-size:80%;">, vol. 18, no. 1, p. 2202978, 2023.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 21 02:18:56 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>

<div class="package-alerts" role="alert">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="30" role="presentation" viewbox="0 0 44 44" width="30">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>This paper uses the following packages that do not yet convert to HTML. These are known issues and are being worked on. Have free development cycles? <a href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">We welcome contributors</a>.</p>
<ul>
<li>failed: forest</li>
</ul>
</div>
<script> 
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
</body>
</html>
