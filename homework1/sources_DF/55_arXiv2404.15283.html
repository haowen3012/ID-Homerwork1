<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion</title>
<!--Generated on Tue Apr 30 20:39:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Wearable technology; Myosensors; Multimodal data fusion; EMG data; Speech.
" lang="en" name="keywords"/>
<base href="/html/2404.15283v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S1" title="In Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S2" title="In Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S3" title="In Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Machine Learning Implementation for Speech and EMG Data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S4" title="In Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Feature Level and Decision Level Fusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S5" title="In Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S6" title="In Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps"> Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S6.SS1" title="In VI Methodology ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Hardware</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S6.SS2" title="In VI Methodology ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Software</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S6.SS3" title="In VI Methodology ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Experiments performed</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S7" title="In Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Results and Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S8" title="In Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Limitations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S9" title="In Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IX </span><span class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tauheed Khan Mohd
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">School of Information Security and Applied Computing</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id2.2.id2">Eastern Michigan University
<br class="ltx_break"/></span>Ypsialnti, Michigan, USA 
<br class="ltx_break"/>tkhanmoh@emich.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ahmad Y Javaid
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.1.id1">EECS Department</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id4.2.id2">The University of Toledo
<br class="ltx_break"/></span>Toledo, Ohio, USA 
<br class="ltx_break"/>ahmad.javaid@utoledo.edu
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Interactions with electronic devices are changing in our daily lives. The day-to-day development brings curiosity to recent technology and challenges its use. The gadgets are becoming cumbersome, and their usage frustrates a segment of society. In specific scenarios, the user cannot use the modalities because of the challenges that bring in, e.g., the usage of touch screen devices by elderly people. The idea of multimodality provides an ease to accessing devices of our daily use through various modalities. In this paper, we suggest a solution that allows the operation of a microcontroller-based device using voice and speech. The model implemented will learn from the user’s behavior and decide based on prior knowledge.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Wearable technology; Myosensors; Multimodal data fusion; EMG data; Speech.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the advancement of technology, daily lives become more occupied with devices; the recent development allows users to access electronic devices using several methods and modalities. Cellphones, IoT devices, and smartwatches are all operated by either touch or human speech. Gestures do not play a crucial role in controlling devices in the current scenario. Moreover, if the feature of gestures is exploited, then we can control, operate, and manage various electronic devices throughout the day. From switching off the lights to closing the garage door, all these activities could be controlled by the limb’s movement. The idea we have planned to work on is to incorporate multiple modalities while using the technology for daily use. The software developed is a prototype of heavy robots used in industry for several jobs and accomplishing tasks in less time.
Moreover, the same concept can easily be applied to our daily activities using the microcontroller. Multimodal Human-Computer Interaction (MMHCI) is used to decide how we can make computer technology more usable by people, which perpetually requires the understanding of at least three things: the user who interacts with it, the system (the computer and its usability), and the communication between the user and the system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib1" title="">1</a>]</cite>. In our research, we have surveyed the usage of Multimodality in the myriad industry and daily use applications. For the extension of our work, we decided to use electromyography as the input for the application. To our knowledge, no application has used electromyography with human speech to generate commands for the robotic arm. Moreover, we have applied several machine learning techniques to determine which works best with our application.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The usage of technology is managed by several modalities available to access them. IoT-based devices are controlled by phone through touch or speech commands. Decision-level fusion involves the fusion of sensor information that is preliminarily determined by the sensors. Examples of decision-level fusion methods include weighted decision methods, classical inference, Bayesian inference, and Dempster–Shafer method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib2" title="">2</a>]</cite>. The fuzzy clustering algorithms, which involve assigning data points to clusters, while items belonging to different clusters are as dissimilar as possible, are proposed to authenticate a person <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib3" title="">3</a>]</cite>. The decision-based fusion for heterogeneous sensors for real-time monitoring is implemented using Bayesian clustering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib4" title="">4</a>]</cite>. Similar techniques are used in the pan-sharpening of images for remote sensing. Two renowned techniques are used for fusion remote images, viz. À trous wavelet transform-based pan sharpening (AWLP) and À trous wavelet transform followed by context-based decision fusion (AWCBD). The latter one is good for sharpening of larger objects in the image in suburban areas. In the 2006 IEEE Fusion contest, the Institute of Electrical and Electronics Engineers, À trous wavelet fusion (AWLP) and the Laplacian pyramid-based context decision fusion (CBD) were rated as the winner as they both had better results in urban areas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The fusion of three-dimensional data from cultural sites was analyzed to achieve comprehensive surveys. The idea is to combine full-resolution laser scanning and photogrammetry. Three-dimensional data streams were generated using Light detection and ranging (LiDAR) &amp; ground-based images were captured through drones. LiDAR is a method for calculating distances by measuring the time a laser takes to reflect back to a sensor (this technology can be used in mobile devices nowadays) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib6" title="">6</a>]</cite>. The data is fused to generate a holistic view, and the experiment was performed on an ancestral church. The fusion implemented is based on the capturing of two data collection techniques of images LiDAR and structure from motion (SfM), a photogrammetric range imaging technique which is used to create three-dimensional digital structures from two-dimensional series of images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib7" title="">7</a>]</cite>. The LiDAR captures the room’s interior, while SfM captures the exterior walls. The data fusion algorithm technique is not explicitly explained, and no empirical proof was provided to evaluate the implemented techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib8" title="">8</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib9" title="">9</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Remote sensing is one of the crucial areas where multimodality is used extensively. It uses three types of fusion, viz. raw-data level, feature level, and decision level. The decision-level fusion combines results from different modalities, provided they complement each other. One can expect to increase the robustness of the decision through fusion in comparison with the results obtained independently. The achieved fusion of results is confirmed due to consensus. In case of discordance, the result is concluded based on majority voting. Decision-based fusion approach used in pattern recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib9" title="">9</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Another approach for predicting the probability of events is through the stochastic model describing a sequence of possible events in which the probability of each event is contingent only on the state achieved in the preceding event, and this method called Markov Chain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">The classification of data for fusion techniques can be divided into four broad categories</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">The relation between the input data sources. These relations are defined as (a) complementary, (b) redundant, or (c) cooperative data.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">According to the input/output data types and their nature.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Following an abstraction level of the employed data, which are (a) raw measurement, (b) signals, and (c) characteristics or decisions.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1">Dependent on the architecture type: (a) centralized, (b) decentralized, or (c) distributed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">The decision-based fusion is classified into two broad categories: the first one is the Bayesian Method, and the second one is the Dempster-Shafer Theory.
The Bayesian Method, rooted in probability theory, serves as a powerful framework for updating and refining our beliefs in the face of new information. In technical terms, it enables the calculation of posterior probabilities by combining prior beliefs with the likelihood of observed data. This method provides a systematic and mathematically rigorous approach to inference, particularly in predictive modeling and decision-making scenarios. Its flexibility makes it applicable across various domains, from machine learning to statistical analysis, where dynamically adjusting our understanding based on incoming data is paramount.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1">Whereas the Dempster-Shafer Theory, in the realm of uncertainty and evidence fusion, the Dempster-Shafer Theory (DST) stands out as a robust framework. Developed to handle situations where multiple sources of evidence contribute to a decision-making process, DST utilizes belief functions to represent the degree of confidence or uncertainty associated with each piece of evidence. By combining these belief functions, DST offers a principled way to manage and process diverse, sometimes conflicting, information. This makes it a valuable tool in fields such as artificial intelligence, decision support systems, and forensic analysis, where complex decision-making requires careful consideration of uncertain and potentially conflicting sources of evidence.</p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p" id="S2.p8.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p9">
<p class="ltx_p" id="S2.p9.1">Multimodal fusion is the heart of any multimodal sentiment analysis engine. There are two main fusion techniques: feature-level fusion and decision-level fusion. Feature-level fusion is implemented by concatenating the feature vectors of all three modalities to form a single long feature vector. Despite its simplicity, this method produces accurate results. We concatenated the feature vector of each modality into a single feature vector stream. This feature vector is used for classifying each video segment into sentiment classes. To estimate the accuracy, we used tenfold cross-validation.</p>
</div>
<div class="ltx_para" id="S2.p10">
<p class="ltx_p" id="S2.p10.1">In another experiment, a quadcopter was controlled by Myo armband. The proposed solution used real-time hand gesture recognition for flight control of multiple quadrotors. It used electromyography signals (EMG) and Convolutional Neural Networks (CNN) to simplify flight operation control and make it more intuitive for the user. These days unmanned aerial vehicles are used in civil and military fields. The Myo armband exploited in the experiment consists of an 8-channel dry-electrode with a low sampling rate (200 Hz) integrated with Bluetooth for communication.
Moreover, the Myo sensor includes an inertial measurement unit (IMU) with 9 degrees of freedom (DoF). The convolutional neural network is implemented using the library implemented in Python called TFlearn. It is a transparent deep-learning library implemented in modules and built on top of TensorFlow. The TFLearn is designed to offer a higher-level API to TensorFlow to facilitate and speed up experiments. The architecture implemented was selected by trial and error based on the slow fusion model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p11">
<p class="ltx_p" id="S2.p11.1">MYO Mapper is a free, open-source, cross-platform application that maps EMG data into Open Source Control (OSC) messages. It is developed using MYO open-source API, and it helps musicians explore the possibility of creating musical interfaces using MYO armbands. The muscle activity in our body is analyzed via EMG data originating from the somatic nervous system and transported to the muscles through various nerves. The feedback is in the form of digital guidelines for effectively using the MYO Mapper to manage EMG data and IMU data from MYO in collaboration with a machine learning tool called Weakinator. This tool allows users to use machine learning for building novel musical instruments, gestural game controllers, computer vision or computer listening systems, and more <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p12">
<p class="ltx_p" id="S2.p12.1">Hand gestures have various applications in the engineering and medical fields. The foremost problem with the gesture is identifying the input in real-time. In another experiment, a MYO armband commercial sensor labeled the output. The model proposed uses k-NN and a dynamic warping algorithm. To evaluate the performance of the proposed model five classes of gestures are reviewed for accuracy. The result shows after performing the experiments, it can find 86% accurate gestures than the MYO system, which is 83% as per their performed experiments. The experiment comprises five stages: signal acquisition, preprocessing, feature extraction, classification, and post-processing. The experiments are performed several times to train the system in this process. The gesture is recognized based on the trained model, the empirical data is collected, and it performs well compared to conventional MYO gesture recognition software. The data mentioned justifying it is in synchronization with the data portrayed in our experiments for calculating the performance of the MYO armband while using it with speech commands <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p13">
<p class="ltx_p" id="S2.p13.1">The gestures are represented by Electromyography data, which is the recording of the electrical activity of the muscle tissue or its representation as a visual display or audible signal, using electrodes attached to the skin or inserted into the muscle. The challenge with electromyography data is that it is hard to club with deep learning algorithms; the primary challenge is to generate the voluminous data. Then, a model should be trained to exploit the user’s data. In another experiment, two datasets of 18 and 17 non-disabled participants are captured using a low-cost, low-sampling rate (200Hz), 8-channel, consumer-grade, dry electrode MYO armband is used. In machine learning, a convolutional neural network is a class of deep, feed-forward artificial neural networks most commonly applied to analyzing visual imagery. A convolutional neural network (CNN) is augmented using the transfer learning mechanism to utilize the data from the initial dataset used for training. The result shows that the proposed model achieves the same accuracy as a joystick implemented on a robotic arm. The robotic used has a degree of freedom in 6 operations using the servo motors. The CNN achieves an average of 97.81% accuracy on seven hand gestures from 17 participants from the second dataset datasets. The data processing of CNN for gesture classification shifts the focus from engineering to feature learning. The feature extraction is a part of CNN; the calculations required before feeding the input to the proposed model have been reduced significantly.
Moreover, the need for the proposed algorithm to execute on an embedded system requires substantial efforts to reduce the initial processing effort. The result was reduced to calculating the spectrograms of the raw sEMG data. The spectrograms are used to store non-stationary signals from sEMG data. For each sensor of the MYO armband, 52 samples (260ms) per spectrogram was leveraged using Hann windows of 28 points per FFT and an overlap of 20. The matrix yields 8*4*15 (channels * Time bins * frequency bins). The matrix is then rearranged to make it a suitable feed for the CNN.
The new matrix is 4*8*15. Finally, the baseline drift and motion artifact, the lowest frequency bin, were removed, and the finalized matrix was 4*8*14. The famous architecture for extracting information is called the slow fusion model, where temporal data start on disconnected parallel layers and slowly fuse across the network, which results in more temporal data for the higher layers. A similar model can be implemented successfully for Multiview image fusion. The matrix used for CNN for the fusion of images is represented as (Time * Spatial * Frequency), eventually representing non-stationary information. The foremost hypothesis of this experiment is the extraction of information from rich data of sEMG when subjects are performing gestures. The idea of using transfer learning for CNN training is that pre-training allows the model to capture more robust features. The CNN then can use general features to prepare an effective model of a new subject sEMG data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p14">
<p class="ltx_p" id="S2.p14.1">Several applications of MYO armband are found over the internet. Their usage varies from gaming control to managing Unmanned Aerial Vehicles (UAV) flights. Hand cricket is exploited to classify the data captured using the MYO armband. The proposed system is trained using the real-time data fed in MATLAB, which determines the probability of winning the game. The algorithm used in this experiment is a Support Vector Machine (SVM), which helps to train the data and achieve the accuracy of 92% and 84%, respectively. The data flow starts from the Raw format, a matrix of (K-Samples * M Points * N Channels) sent for pre-processing. The third stage required feature extraction and a model created with training set data; once the training set created; the test set data was applied to judge the model’s accuracy. Pre-processing of data includes DC-offset removal and Notch filter. The DC component in the signals doesn’t align with mean zero, creating a problem during the Fourier transformation. Whereas the notch filter is used to filter frequency with the range, here in this experiment, the notch filter is set to 60Hz, and it is used to remove the power line noise from the obtained raw signal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib16" title="">16</a>]</cite>. The third stage of training the dataset is feature extraction. In this stage, several mathematical operations are applied to each signal received. These operations are simple square integral, maximum, minimum, mean frequency, and mean power. The fourth stage is using a Support Vector Machine (SVM) to classify the data after pre-processing. The accuracy of SVM is judged when the trained data set can classify the gesture accurately. The results examined suggest the trained model can classify gestures 92.4% for player 1 and 84.27 for the second player with 3.075 and 3.3325 as their standard deviation respectively <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p15">
<p class="ltx_p" id="S2.p15.1">In the past year, there have been several technical papers published evaluating the performance of the MYO Armband. The use of MYO armband, as mentioned, is not only applicable to engineering but has various applications in bioengineering. One such experiment was performed in Denmark by bio-engineering students. This experiment aimed to compare the MYO armband’s narrow bandwidth with a conventional EMG acquisition system, which captures the full spectrum to assess its usability for pattern recognition. The MYO armband is a low-cost sensor; it does not have an extensive feature to capture the complete bandwidth like a professional EMG acquisition. A crossover study was performed on eight participants performing nine hand gestures. Six features were extracted from the data and classified using Linear Discriminant Analysis (LDA), which are six Myo band electrodes to cover the following anatomical landmarks 2 cm distal to the elbow: extensor carpi ulnaris, ulna, extensor digitorum, extensor carpi radialis longus, flexor carpi radialis, and palmaris longus. The empirical data achieved explains the classification error of 5.85±3.63% for the Conventional EMG Acquisition system, while the MYO Armband has an accuracy of 9.86±8.05% with no significant difference. Despite low bandwidth, it concludes the MYO Armband is suitable for pattern recognition applications. The data is acquired with Mr. Kick software at 2KHz sampling frequency with analog filtered between 10-500Hz. While the MYO armband data using MYO SDK MATLAB Mex Wrapper toolbox via Bluetooth 4.0, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Machine Learning Implementation for Speech and EMG Data</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Designing a system with multiple modalities brings challenges and complexities. The system design with several input methods requires a decision mechanism. There are several approaches available, which include the implementation of machine learning and Artificial Intelligence techniques. Machine learning is divided into two broad categories: supervised learning and unsupervised learning. Supervised learning algorithms learn and decide the system’s decision to work efficiently. It learns based on data provided initially and then gradually learns while the system is in use. The other approach is unsupervised learning algorithms based on the categorical classification of massive unlabeled data. The users do not need to supervise the model since the agent works independently.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">It is interesting to implement supervised machine learning algorithms and techniques to make the robotic arm capable of learning from its previous decisions. Based on previous decisions and gestures, the system will learn to execute a movement and enhance this throughout the experiment. We need the system to correctly analyze and evaluate the electrical signals of the muscle tissue in real-time. Therefore, we are interested in supervised learning for classification and supervised learning from regression techniques.
Classification is one type of supervised learning model. This model attempts to conclude observed values. One or more input is provided to the classification algorithm, and this one will try to predict the value with one or more outcomes. The system is trained first with inputs, and accordingly, it takes action later. Several algorithms are available for supervised learning based on speed, accuracy, and data size. Kernel SM, Random Forest, and gradient boosting tree are the algorithms available for precision, while Nave Bayes is used when data is too large. Decision trees and logistic regression are used for processing data quickly [18]. In addition, supervised learning is used for regression to predict and classify outputs from 0 or 1 numerically. For accuracy, the algorithms available are Random Forest, Gradient Boosting Tree, and Neural Network. For speed, decision tree and linear regression are used [18].</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">For our experiment, the input data will be the input data for limbic electrical signals gathered by the MYO armband and voice recognition instructions obtained by the API. An array with the input signals obtained from the sensor and voice commands received from the voice recognition software was created. This will be our training data. This data is composed of 900 interactions. An interaction is defined as either a gesture or a voice command. In addition, another set of data was generated. This one contains the fusion data generated from the training data. The fusion data will be our testing data. Therefore, the model will need to accurately classify, interpret, and label the signals to enhance the fusion data, which will eventually get to the robotic arm. This outcome produced by the agent will be a gesture that the robotic arm will execute. There are only a fixed number of movements that we can detect.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">According to these criteria, these are the machine learning techniques models that are available and can be implemented in Python:</p>
</div>
<div class="ltx_para" id="S3.p5">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Linear Discriminant Analysis: Linear discriminant analysis (LDA) is commonly used as a dimensionality reduction technique in the pre-processing step for pattern classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability to avoid overfitting (curse of dimensionality) and reduce computational costs [20]. This algorithm is critical for the project since it will maximize the separability of the different categories, maximizing the difference between the means of each of them. At the same time, the variation of each of the categories is minimized so the same categories are grouped together. By doing this, the model will be able to identify each of the data types easily. We have five singular types of EMG signals, and they need to be classified into movements (fist, wave left, wave right, fingers spread, and double tap). <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib18" title="">18</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">K nearest neighbors algorithm (KNN) is a non-parametric, indolent learning algorithm. Its motivation is to utilize a database in which the information focuses are isolated into a few classes to anticipate the characterization of another sample point, as shown in Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:KNN_Classifier.png</span>. The k nearest neighbors have been used widely in pattern recognition. The algorithm is easy to understand conceptually, and the tendency toward error is bounded twice by the Bayes error. The accuracy of K-neighbor surpasses those of sophisticated classifiers. The random subspace method relies on a stochastic process that randomly selects components [21]. We will use the data obtained from the sensor and the voice recognition software as our training data. Our testing data will be the data obtained from the data fusion process. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="218" id="S3.F1.g1" src="extracted/2404.15283v1/KNN_Classifier.png" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>K-NN classifier: Classifying using Knearest neighbors algorithm.png</figcaption>
</figure>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Decision Tree Classifier: Decision trees are a type of supervised machine learning (that is, they explain the input and corresponding output in the training data) where the data is continuously split according to a specific parameter. The tree can be explained by two entities: decision nodes and leaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. We will try to predict which of the six gestures will be executed. Again, the data obtained from the EMG sensor and the voice recognition software will be fed into the algorithm. This will be training the data. The data obtained after the fusion process will be the testing data. The agent will filter the data for us and output the actual gesture to be executed.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Gaussian Naive Bayes: Gaussian Naive Bayes is a classification algorithm for binary and multi-class classification problems. The technique is easier to understand when described using binary or categorical input values. The approach is called naive Bayes or idiot Bayes because the calculation of the probabilities for each hypothesis is simplified to make their calculation tractable. Rather than attempting to calculate the values of each attribute value P(d1, d2, d3—h), they are assumed to be conditionally independent given the target value and calculated as P(d1—h) * P(d2—H), and so on. This is an extreme assumption that is most unlikely in real data, that is, that the attributes do not interact. Nevertheless, the approach performs surprisingly well on data where this assumption does not hold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">Support Vector Machine: Support vector machine (SVM) is a supervised machine learning algorithm used for both classification and regression challenges. However, this is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is the number of features present), with each feature being the value of a coordinate. Then, we perform classification by finding the hyperplane that differentiates the two classes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1">Logistic Regression: Logistic regression is a machine learning technique extracted from the field of statistics. It is the go-to method for binary classification problems (problems with two class values). Logistic regression is named for the function used at the core of the method, the logistic function, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S3.F2" title="Figure 2 ‣ item 6 ‣ III Machine Learning Implementation for Speech and EMG Data ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">2</span></a>. The logistic function, also called the sigmoid function, was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It is an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib22" title="">22</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="161" id="S3.F2.g1" src="extracted/2404.15283v1/Data_Mining_graph_of_Logistic_Regression.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Data Mining graph of Logistic Regression</figcaption>
</figure>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Feature Level and Decision Level Fusion</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Multimodal fusion is the heart of any multimodal sentiment analysis engine. There are two main fusion techniques: feature-level fusion and decision-level fusion. Feature-level fusion is implemented by concatenating the feature vectors of all three modalities to form a single long feature vector. Despite its simplicity, this method produces accurate results. We concatenated the feature vector of each modality into a single feature vector stream. This feature vector is used for classifying each video segment into sentiment classes. To estimate the accuracy, we used tenfold cross-validation.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In decision-level fusion, we obtained feature vectors instead of concatenating the feature vectors, as in feature-level fusion, where we used a separate classifier for each modality. The output of each classifier is treated as a classification score. We obtained a probability score for each sentiment class from each classifier. As there are three sentiment classes, we obtained three probability scores from each modality.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Experimental Setup</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The objectives of Human-Computer Interaction are to create usable and safe systems, as well as functional systems. To create computer frameworks with great ease of use, developers must endeavor to comprehend the factors.</p>
</div>
<div class="ltx_para" id="S5.p2">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">that decides how people utilize technology,</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">create tools and procedures to empower existing systems,</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">accomplish productive, powerful, and safe collaboration</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">put users first</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Mapping of Gestures with Arduino Boards</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t" id="S5.T1.1.1.1.1">Fist</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.1.1.2">Pin 3</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S5.T1.1.2.1.1">Wave In</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.2.1.2">Pin 4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S5.T1.1.3.2.1">Wave Out</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.3.2.2">Pin 5</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S5.T1.1.4.3.1">Finger Spread</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.4.3.2">Pin 9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_t" id="S5.T1.1.5.4.1">Double Tap</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.5.4.2">Pin 10</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Underlying the whole theme of HCI is the conviction that how people utilize a computer system. Their necessities, abilities, and inclinations for leading different assignments should direct the developers in how they outline the systems. People should not have to change how they use a system to fit in with it. Rather, the system should be designed to coordinate their requirements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib23" title="">23</a>]</cite>. The continuous advancement in technology requires machines meant for different purposes, and the recent trend involves human participation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib24" title="">24</a>]</cite>. Human-robot communication is conceivable through two strategies.</p>
</div>
<div class="ltx_para" id="S5.p4">
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1">Accepting user input from peripheral gadgets that are autonomous of each other, and</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1">Accepting user input through various modalities and intertwining them as a method for acquiring the semantics related to the activities of the user.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">The second approach is the focal point of this paper. In 2005, a framework was composed which acknowledges contribution in the form of speech, keystrokes, and motions. This framework could resolve questionable sources of info and organize them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib25" title="">25</a>]</cite>. The combination of multiple inputs utilized in various applications of daily use and its extension isn’t just kept to robots, for example, combining speech and facial identification. A framework was composed in 1999 that could verify a client by looking at contributions against a pre-populated database <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib26" title="">26</a>]</cite>. The most recent change in Microsoft Windows is that it is capable of validating clients through a webcam connected to the computer system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib27" title="">27</a>]</cite>, even though it is an unimodal framework that could be upgraded with more modalities to enhance its precision and make it less vulnerable to outside attacks or spoofing. The utilization of EMG data for fusion is rarely encountered in any real-life application; one such application was executed to control electronic musical devices through EMG and relative position sensing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib28" title="">28</a>]</cite>. The concept of multimodal data fusion has been executed in mechanical robots utilizing the Microsoft Kinect and sensor equipment called Asus Xtion Monitor by catching hand gestures identified by two Leap Motion sensors and playing out the subsequent mapped activities on a robotic arm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib29" title="">29</a>]</cite>. During the most recent five years, human-robot interaction has mainly been performed utilizing the Microsoft Kinect system; not many multimodal framework plans have utilized EMG data to intertwine with speech, text, and other different modalities.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="160" id="S5.F3.g1" src="extracted/2404.15283v1/MYO_ARM_BAND.jpg" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>MYO Arm Band</figcaption>
</figure>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">The Microsft Kinect is equipped for capturing both voice and signals on an independent basis. Also, there is no feature to capture the EMG data of human limbs using Kinect. This constraint drove us to another special innovation called the MYO sensor armband. We have chosen to utilize it as one of the modalities and perform fusion to enhance the accuracy and performance. MYO armband is open-source programming that gives roads to tweak gestures and utilize them in gadgets utilized as a part of everyday life, for instance, controlling a wheelchair, turning a doorknob, and so forth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib30" title="">30</a>]</cite>. The outcomes accomplished with different experiments to assess the precision of MYO turn out to be 87.8 to 89.38%, which gives us avenues of improvement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib31" title="">31</a>]</cite>. MYO band is utilized as a part of an experiment to perform both search and select operations on a PC, and the average score for assessment is analyzed; the analysts infer that after embracing the limb’s temperature, the MYO band continually performs with a similar number of scores <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib32" title="">32</a>]</cite>. To the best of our knowledge, this is the first research work that utilizes EMG data to catch gestures utilizing MYO armband sensors for Multimodal data fusion.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Preliminary results for of Muscle sensor MYO band</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t" id="S5.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">Gesture</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1">Wrong/Missed gesture %</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1">Correct %</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S5.T2.1.2.1.1">Wave Out</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.2">9.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.2.1.3">90.5</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S5.T2.1.3.2.1">Wave In</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.2">9.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.3.2.3">90.9</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S5.T2.1.4.3.1">Fist</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.4.3.2">13.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.4.3.3">86.4</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S5.T2.1.5.4.1">Double Tap</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.5.4.2">20.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.5.4.3">79.4</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_t" id="S5.T2.1.6.5.1">Finger Spread</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.1.6.5.2">14.5</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.1.6.5.3">85.5</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Preliminary results for Microsoft Speech used by non-native speakers.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Command</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">Microsoft Speech API (Wrong output)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">Correct</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t" id="S5.T3.1.2.2.1">Move Right</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.2.2.2">10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.2.2.3">90</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S5.T3.1.3.1.1">Move Left</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.2">34.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.3.1.3">65.8</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S5.T3.1.4.2.1">Move Up</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.4.2.2">8.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.4.2.3">91.1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S5.T3.1.5.3.1">Move Down</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.5.3.2">22.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.5.3.3">77.5</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_t" id="S5.T3.1.6.4.1">Move Gripper</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T3.1.6.4.2">14.1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.6.4.3">85.9</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1"> Methodology</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The problems which have been tried to be solved deal with decision-making techniques when the system receives ambiguous user input. There are several machine learning algorithms available that help the model to learn the behavior of the user.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The robots initially introduced to the market were generally basic, the majority of them requiring a teaching phase and programming. Off lately, the robots have become dynamic, modern, and significantly more proficient than before <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib33" title="">33</a>]</cite>. Along with this modern sophistication came expanding requests to perform complex tasks that require both precision and accuracy. The robot used for the experiment in this paper opens the adequate opportunity to get better in both robustness and accuracy. Henceforth, it was chosen to enhance the precision of a robotic arm by the utilization of multi-modal data fusion. The experiment emphasizes the change of information provided through various channels into a single format which is comprehended by the robotic arm through mediation. The input modalities utilized are speech and gesture.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.4.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.5.2">Hardware</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">The framework designed for the experiment depicted in this paper is made out of the accompanying segments: An Arduino-based robotic arm and a MYO armband. These gadgets are outlined in <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S5.F3" title="Figure 3 ‣ V Experimental Setup ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S6.F4" title="Figure 4 ‣ VI-A Hardware ‣ VI Methodology ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">4</span></a> separately. The robotic arm utilized is made by Trossen Robotics. The robot utilized as a part of the analysis portrayed here is called ’RobotGeek Snapper Arduino Robotic Arm,’ and it contains five servo motors. An electromyography data-based sensor controls the robotic arm called a MYO. <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S5.F3" title="Figure 3 ‣ V Experimental Setup ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">3</span></a> portrays the utilization of the MYO band from which information is captured and manipulated to perform activities on different Arduino-based devices. The armband is fit for catching five signals: Fist, Wave Left, Wave Right, Double Tap, and Fingers Spread, as appeared in <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S6.F6" title="Figure 6 ‣ VI-C Experiments performed ‣ VI Methodology ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">6</span></a>. This armband allows tweaking an open library and perform activities per requirement. The robotic arm utilized involves Arduino Duemilanove and Diecimila microcontroller board for accepting inputs through USB. The Arduino board associated with the robot with pins is characterized for every servo motor, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S6.F4" title="Figure 4 ‣ VI-A Hardware ‣ VI Methodology ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">4</span></a>. A high-precision wireless H800 headset from Logitech <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S6.F5" title="Figure 5 ‣ VI-A Hardware ‣ VI Methodology ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">5</span></a> was utilized for capturing the human speech input.</p>
</div>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="240" id="S6.F4.g1" src="extracted/2404.15283v1/Arduino_based_robotic_arm.jpg" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Arduino based robotic arm.</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="192" id="S6.F5.g1" src="extracted/2404.15283v1/Logitech-Wireless-Headset-H800-1.jpg" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Logitech Wireless Headset H800</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.4.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.5.2">Software</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">The software is implemented utilizing C# and
C++ programming languages. C++ is utilized to execute the MYO API, while C# is utilized for giving speech input utilizing Microsoft’s speech API. At first, the robotic arm was operated using gestures only. The MYO armband was utilized to calculate the robot’s accuracy. It is presumed that the accuracy of the armband was not high. To enhance the precision of the commands, an imparted fusion of the data is incorporated. The speech modality is clubbed with the gestures defined in the MYO Armband API. The MYO armband gives an API to control the Arduino board, which is connected to the robotic arm, as appeared in Figure 5. The Arduino board comprises 14 pins and can interface with every servo motor controlling the robot.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.4.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.5.2">Experiments performed</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">The experimental framework is planned to utilize a client-server paradigm. The MYO armband comprises eight sensors that capture muscle development. Such sensors contrast and match hand developments with motions characterized in MYO. For instance, the Wave Out motion recorded in <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S5.T2" title="TABLE II ‣ V Experimental Setup ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">II</span></a> was performed by moving the hand in the vertical direction in the third gesture of <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S6.F6" title="Figure 6 ‣ VI-C Experiments performed ‣ VI Methodology ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">6</span></a>. The MYO program is working as the server while the Microsoft speech program is actualized as the client.</p>
</div>
<figure class="ltx_figure" id="S6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="51" id="S6.F6.g1" src="extracted/2404.15283v1/MYO_Gestures.png" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>MYO Gestures</figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">As mentioned above, the precision of capturing gestures isn’t high, which tends to bring about matching the hand development to the wrong motion; e.g., Wave Out might be recorded as Wave In. Additionally, the band, once in a while neglects to record the gestures completely. Both cases are considered errors. The MYO API permits customization as per the user’s requirement, and the prototype is implemented using threads responsible for listening to gestures. If the armband misses a gesture, voice input compensates for the missed information through human speech. Handling of speech input implemented in a client part, which sends commands to the server (MYO API). Priority is assigned to the MYO band; however, on account of an error, speech recognition actuates and helps enhance the accuracy of controlling the robot. Fusion is thus performed in the order of priority. Gestures were given the highest priority. If an occurrence of an inability to catch the input should arise, voice commands are utilized to redress and fill in as the main input. Priority-based fusion is utilized in different domains, including medical systems, and tends to enhance its precision significantly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib34" title="">34</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="82" id="S6.F7.g1" src="extracted/2404.15283v1/Split.png" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Split Out Validation Dataset for Gaussian NB</figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">The speech fused with EMG input received from MYO, which enables the robot to work precisely as per the user’s input command. When the user performs gestures using his/her arm, the input message is transmitted from the MYO band to the Arduino, and as a result, it moves the specific servo motor. The fused input sets the corresponding Arduino pin to high, i.e., 1, which then moves the robot. In the prototype constructed, five different Arduino pins were linked to various gestures as shown in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tableThank_:1</span>. Through gestures and speech, users should be able to control the robotic arm precisely and accurately; the results are shown in the next section.</p>
</div>
<div class="ltx_para" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1">We have implemented the machine learning algorithm with our test data and evaluated the results. The result explains that the K-neighbor classifier yields the most accurate results among all the algorithms. In our earlier results, the results were based on input from different modalities.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Results and Discussion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">A performance evaluation was executed, keeping in mind the end goal to evaluate how exactly the modalities are independent; subsequently, we tried them using machine learning techniques. The complete details of the results are outlined in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:table:13</span>. The test results demonstrate that the algorithm that works best for us is the K-Neighbors Classifier, with an accuracy of 92.45 percent.</p>
</div>
<figure class="ltx_table" id="S7.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Precision, Recall and F1-Score</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S7.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t" id="S7.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.1.1.1.1">ML Algorithm</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S7.T4.1.1.1.2.1">Precision</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S7.T4.1.1.1.3.1">Recall</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S7.T4.1.1.1.4.1">F1-Score</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S7.T4.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S7.T4.1.1.1.5.1">Support</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T4.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S7.T4.1.2.1.1">SVM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.2.1.2">0.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.2.1.3">1.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.2.1.4">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T4.1.2.1.5">9</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S7.T4.1.3.2.1">Gaussian NB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.3.2.2">0.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.3.2.3">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.3.2.4">0.57</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T4.1.3.2.5">4</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S7.T4.1.4.3.1">Decision Tree Classifier</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.4.3.2">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.4.3.3">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.4.3.4">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T4.1.4.3.5">6</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S7.T4.1.5.4.1">Lin. Discriminant Analysis</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.5.4.2">1.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.5.4.3">0.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.5.4.4">0.86</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T4.1.5.4.5">8</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S7.T4.1.6.5.1">Logistic Regression</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.6.5.2">0.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.6.5.3">1.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.6.5.4">0.88</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T4.1.6.5.5">7</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_t" id="S7.T4.1.7.6.1">KNN</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S7.T4.1.7.6.2">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S7.T4.1.7.6.3">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S7.T4.1.7.6.4">0.91</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T4.1.7.6.5">1</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Similarly, the MYO band results were captured to quantify accuracy and to find the scope of improvement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib35" title="">35</a>]</cite>. The preliminary results have shown the MYO band has a scope of improvement. The error rate lies between 9.1 and 20.6 percent. The data has been collected by experimenting ten times, with each experiment having one hundred gestures performed and then calculating the average percentages shown in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:table:10</span>. An error in the experiment occurs when a gesture is either missed or captured wrong. The trials have been performed in laboratory conditions. The MYO armband is capable of adapting to specific human limbs and improves its output once it has been trained completely.</p>
</div>
<figure class="ltx_table" id="S7.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Fusion results with Error % &amp; Variance</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S7.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T5.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S7.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.1.1">Fusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.2.1">50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.3.1">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.4.1">150</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.5.1">200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.6.1">Err</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T5.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.7.1">Var</span></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S7.T5.1.2.2.1">Move Gripper &amp; Double Tap</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.2.2.2">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.2.2.3">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.2.2.4">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.2.2.5">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.2.2.6">7.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T5.1.2.2.7">5.58</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S7.T5.1.3.3.1">Move Down &amp; Fist</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.3.3.2">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.3.3.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.3.3.4">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.3.3.5">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.3.3.6">4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T5.1.3.3.7">0.67</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S7.T5.1.4.4.1">Move Up &amp; Finger spread</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.4.4.2">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.4.4.3">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.4.4.4">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.4.4.5">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.4.4.6">5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T5.1.4.4.7">0.33</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S7.T5.1.5.5.1">Move Left &amp; Wave left</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.5.5.2">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.5.5.3">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.5.5.4">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.5.5.5">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.5.5.6">3.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T5.1.5.5.7">1.58</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_t" id="S7.T5.1.6.6.1">Move Right &amp; Wave out</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S7.T5.1.6.6.2">3</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S7.T5.1.6.6.3">3</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S7.T5.1.6.6.4">4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S7.T5.1.6.6.5">2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S7.T5.1.6.6.6">6</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T5.1.6.6.7">0.67</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">The armband sensors turn warm soon after it is worn on the arm of a user, and thus adapting to body temperature and then capture the gestures precisely after 1-2 minutes. If one’s arm is cold, the sensors are unable to capture the gestures precisely.
The Microsoft API results are assessed by speaking a command, and if the command is captured incorrectly, the instance is marked as an error. Incorrect capture is defined as the resulting string being caught twice or having incidental words or characters added to it.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">Multimodal data fusion of voice and motion utilizing the MYO band enhances the system performance significantly. The test results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S7.T4" title="TABLE IV ‣ VII Results and Discussion ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">IV</span></a>. After the implementation of the fusion of the robotic arm, the error rate decreased to 5.2%, which is an average of all errors. The fluctuation of the error rate is shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#S7.F8" title="Figure 8 ‣ VII Results and Discussion ‣ Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion"><span class="ltx_text ltx_ref_tag">8</span></a>. The errors are, for the most part, because of reading the wrong gesture, e.g., the finger spread sometimes captured as a fist, which is considered as an error. Experiments are performed on all five fusion input tests 200 times each, and the percentages are calculated respectively.</p>
</div>
<figure class="ltx_figure" id="S7.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="425" id="S7.F8.g1" src="x1.jpg" width="478"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Error % deviation of fused Inputs</figcaption>
</figure>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Limitations</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Speech and electromyography data were the modalities used in the system constructed. The MYO band used for capturing the EMG data is capable of recognizing five gestures. This limited the number of operations that could be performed on the robotic arm. The second challenge lies in capturing the speech commands using the Microsoft Speech API. Non-native speakers of the English language will face difficulties and challenges in approximating their accent to that of a native speaker. This created difficulty in conveying commands correctly. Four areas need to be worked on and improved regarding human-robot interaction. These include speech localization, language understanding, dialogue management, and speech synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15283v1#bib.bib36" title="">36</a>]</cite>. Also, as the ultimate goal of this research is to improve accuracy, the approach here described map commands to all possible options that the Microsoft Speech API recognizes as valid (for example, “move right” sometimes gets recognized as “override” – an incorrect response). This provided us with a way to quantify the accuracy of the system. We prepared a many-to-one mapping of all these possible combinations to a particular voice command.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span class="ltx_text ltx_font_smallcaps" id="S9.1.1">Conclusion and Future Work</span>
</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">The implementation of machine learning algorithms provides us with results with an accuracy of 90.56 percent. The future idea is to design a model using a Google Speech API, which processes the complete natural language and extracts meaning from it. When the user provides the statement ‘move the robotic arm ten degrees to the left,’ the system should capture the command and process it with the keywords ‘left’ and ‘ten degrees.’ The idea which shall be implemented is to process the string and pass the parameters to the server program. The server program captures the parameters and processes the command on the robotic arm. In the future, we would like to extend our work while incorporating the complete natural language commands using Google API, which allows the users to communicate and customize the input as per their needs. The designed system should be capable enough to learn commands using machine learning techniques.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Jaimes and N. Sebe, “Multimodal human computer interaction: A survey,” in
<span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">International Workshop on Human-Computer Interaction</span>, pp. 1–15,
Springer, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Llinas and D. L. Hall, “An introduction to multi-sensor data fusion,” in
<span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">ISCAS’98. Proceedings of the 1998 IEEE International Symposium on
Circuits and Systems (Cat. No. 98CH36187)</span>, vol. 6, pp. 537–540, IEEE, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
V. Chatzis, A. G. Bors, and I. Pitas, “Multimodal decision-level fusion for
person authentication,” <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">IEEE transactions on systems, man, and
cybernetics-part a: systems and humans</span>, vol. 29, no. 6, pp. 674–680, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
O. F. Beyca, P. K. Rao, Z. Kong, S. T. Bukkapatnam, and R. Komanduri,
“Heterogeneous sensor data fusion approach for real-time monitoring in
ultraprecision machining (upm) process using non-parametric bayesian
clustering and evidence theory,” <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">IEEE Transactions on Automation
Science and Engineering</span>, vol. 13, no. 2, pp. 1033–1044, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B. Luo, M. M. Khan, T. Bienvenu, J. Chanussot, and L. Zhang, “Decision-based
fusion for pansharpening of remote sensing images,” <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">IEEE Geoscience and
Remote Sensing Letters</span>, vol. 10, no. 1, pp. 19–23, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. E. Reutebuch, H.-E. Andersen, and R. J. McGaughey, “Light detection and
ranging (lidar): an emerging tool for multiple resource inventory,” <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Journal of forestry</span>, vol. 103, no. 6, pp. 286–292, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. L. Schonberger and J.-M. Frahm, “Structure-from-motion revisited,” in <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, June 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Hess, V. Petrovic, D. Meyer, D. Rissolo, and F. Kuester, “Fusion of
multimodal three-dimensional data for comprehensive digital documentation of
cultural heritage sites,” in <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">2015 Digital Heritage</span>, vol. 2,
pp. 595–602, IEEE, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Dalla Mura, S. Prasad, F. Pacifici, P. Gamba, J. Chanussot, and J. A.
Benediktsson, “Challenges and opportunities of multimodality and data fusion
in remote sensing,” <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proceedings of the IEEE</span>, vol. 103, no. 9,
pp. 1585–1601, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Blanchet, G. Gallego, and V. Goyal, “A markov chain approximation to choice
modeling,” <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Operations Research</span>, vol. 64, no. 4, pp. 886–905, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
F. Castanedo, “A review of data fusion techniques,” <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">The Scientific World
Journal</span>, vol. 2013, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
D. V. Redrovan and D. Kim, “Hand gestures recognition using machine learning
for control of multiple quadrotors,” in <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">2018 IEEE Sensors Applications
Symposium (SAS)</span>, pp. 1–6, IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
B. Di Donato, J. Bullock, and A. Tanaka, “Myo mapper: a myo armband to osc
mapper,” in <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2011 Conference on New Interfaces for
Musical Expression (NIME’18)</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. E. Benalcázar, A. G. Jaramillo, A. Zea, A. Páez, V. H. Andaluz, <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">et al.</span>, “Hand gesture recognition using machine learning and the myo
armband,” in <span class="ltx_text ltx_font_italic" id="bib.bib14.2.2">2017 25th European Signal Processing Conference
(EUSIPCO)</span>, pp. 1040–1044, IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
U. Cote-Allard, C. L. Fall, A. Campeau-Lecours, C. Gosselin, F. Laviolette, and
B. Gosselin, “Transfer learning for semg hand gestures recognition using
convolutional neural networks,” in <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">2017 IEEE International Conference
on Systems, Man, and Cybernetics (SMC)</span>, pp. 1663–1668, IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K. S. Krishnan, A. Saha, S. Ramachandran, and S. Kumar, “Recognition of human
arm gestures using myo armband for the game of hand cricket,” in <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">2017
IEEE International Symposium on Robotics and Intelligent Sensors (IRIS)</span>,
pp. 389–394, IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
I. Mendez, B. W. Hansen, C. M. Grabow, E. J. L. Smedegaard, N. B. Skogberg,
X. J. Uth, A. Bruhn, B. Geng, and E. N. Kamavuako, “Evaluation of the myo
armband for the classification of hand motions,” in <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">2017 International
Conference on Rehabilitation Robotics (ICORR)</span>, pp. 1211–1214, IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
P. Xanthopoulos, P. M. Pardalos, and T. B. Trafalis, “Linear discriminant
analysis,” in <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Robust data mining</span>, pp. 27–33, Springer, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T. K. Ho, “Nearest neighbors in random subspaces,” in <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Joint IAPR
International Workshops on Statistical Techniques in Pattern Recognition
(SPR) and Structural and Syntactic Pattern Recognition (SSPR)</span>, pp. 640–648,
Springer, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
G. H. John and P. Langley, “Estimating continuous distributions in bayesian
classifiers,” in <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of the Eleventh conference on Uncertainty
in artificial intelligence</span>, pp. 338–345, Morgan Kaufmann Publishers Inc.,
1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
C. Cortes and V. Vapnik, “Support-vector networks,” <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Machine learning</span>,
vol. 20, no. 3, pp. 273–297, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
G. King and L. Zeng, “Logistic regression in rare events data,” <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Political analysis</span>, vol. 9, no. 2, pp. 137–163, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
“A model for human and machine interaction: Human-machine teaming grows up.”
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://securingtomorrow.mcafee.com/business/model-human-machine-interaction-human-machine-teaming-grows/</span>.

</span>
<span class="ltx_bibblock">(Accessed on 06/22/2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M. Urban and P. Bajcsy, “Fusion of voice, gesture, and human-computer
interface controls for remotely operated robot,” in <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">2005 7th
International Conference on Information Fusion</span>, vol. 2, pp. 8–pp, IEEE,
2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Turk, “Multimodal interaction: A review,” <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Pattern Recognition
Letters</span>, vol. 36, pp. 189–195, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J. Thomas, “Problem solving by human-machine interaction,” in <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Human and
machine problem solving</span>, pp. 317–362, Springer, 1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Tanaka and R. B. Knapp, “Multimodal interaction in music using the
electromyogram and relative position sensing,” in <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Proceedings of the
2002 conference on New interfaces for musical expression</span>, pp. 1–6, National
University of Singapore, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. Schwab, <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">The fourth industrial revolution</span>.

</span>
<span class="ltx_bibblock">Currency, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
O. Salman, M. F. A. Rasid, M. I. Saripan, and S. K. Subramaniam,
“Multi-sources data fusion framework for remote triage prioritization in
telehealth,” <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Journal of medical systems</span>, vol. 38, no. 9, p. 103, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D. Gabriel, <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Gesture Recognition using Electromyographic, Spatial and
Temporal Input Data</span>.

</span>
<span class="ltx_bibblock">PhD thesis, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
G.-C. Luh, Y.-H. Ma, C.-J. Yen, and H.-A. Lin, “Muscle-gesture robot hand
control based on semg signals with wavelet transform features and neural
network classifier,” in <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">2016 International Conference on Machine
Learning and Cybernetics (ICMLC)</span>, vol. 2, pp. 627–632, IEEE, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. B. Csapo, H. Nagy, Á. Kristjánsson, and G. Wersényi,
“Evaluation of human-myo gesture control capabilities in continuous search
and select operations,” in <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">2016 7th IEEE International Conference on
Cognitive Infocommunications (CogInfoCom)</span>, pp. 000415–000420, IEEE, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Roitberg, N. Somani, A. Perzylo, M. Rickert, and A. Knoll, “Multimodal
human activity recognition for industrial manufacturing processes in robotic
workcells,” in <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 2015 ACM on International Conference
on Multimodal Interaction</span>, pp. 259–266, ACM, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Ben-Yacoub, Y. Abdeljaoued, and E. Mayoraz, “Fusion of face and speech data
for person identity verification,” <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">IEEE transactions on neural
networks</span>, vol. 10, no. 5, pp. 1065–1074, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
T. K. Mohd, J. Carvalho, and A. Y. Javaid, “Multi-modal data fusion of voice
and emg data for robotic control,” in <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">2017 IEEE 8th Annual Ubiquitous
Computing, Electronics and Mobile Communication Conference (UEMCON)</span>,
pp. 329–333, IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
R. P. Judd and A. B. Knasinski, “A technique to calibrate industrial robots
with experimental verification,” <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">IEEE Transactions on robotics and
automation</span>, vol. 6, no. 1, pp. 20–30, 1990.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Apr 30 20:39:19 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
