<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification</title>
<!--Generated on Thu Aug 22 23:12:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Hyperspectral image; Multi-source data classification; Hierarchical attention; Parallel filter fusion; Synthetic aperture radar.
" lang="en" name="keywords"/>
<base href="/html/2408.12760v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S1" title="In Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S2" title="In Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S2.SS1" title="In II Methodology â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Hierarchical Attention Module (HAM)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S2.SS2" title="In II Methodology â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Parallel Filter Fusion Module (PFFM)</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S3" title="In Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Experimental Results and Analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S3.SS1" title="In III Experimental Results and Analysis â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Datasets and Experimental Setting</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S3.SS2" title="In III Experimental Results and Analysis â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Experimental Results and Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S3.SS3" title="In III Experimental Results and Analysis â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Ablation Study</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S4" title="In Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Conclusions</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Han Luo, Feng Gao, Junyu Dong, and Lin Qi
</span><span class="ltx_author_notes">This work was supported in part by the National Science and Technology Major Project under Grant 2022ZD0117201, and in part by the Natural Science Foundation of Qingdao under Grant 23-2-1-222-ZYYD-JCH. (<span class="ltx_text ltx_font_italic" id="id1.1.id1">Corresponding author: Feng Gao</span>)
Han Luo, Feng Gao, Junyu Dong, Lin Qi are with the School of Information Science and Engineering, Ocean University of China, Qingdao 266100, China.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Hyperspectral image (HSI) and synthetic aperture radar (SAR) data joint classification is a crucial and yet challenging task in the field of remote sensing image interpretation. However, feature modeling in existing methods is deficient to exploit the abundant global, spectral, and local features simultaneously, leading to sub-optimal classification performance. To solve the problem, we propose a hierarchical attention and parallel filter fusion network for multi-source data classification. Concretely, we design a hierarchical attention module for hyperspectral feature extraction. This module integrates global, spectral, and local features simultaneously to provide more comprehensive feature representation. In addition, we develop parallel filter fusion module which enhances cross-modal feature interactions among different spatial locations in the frequency domain. Extensive experiments on two multi-source remote sensing data classification datasets verify the superiority of our proposed method over current state-of-the-art classification approaches. Specifically, our proposed method achieves 91.44% and 80.51% of overall accuracy (OA) on the respective datasets, highlighting its superior performance.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Hyperspectral image; Multi-source data classification; Hierarchical attention; Parallel filter fusion; Synthetic aperture radar.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the advancement of earth observation technology and satellite sensor platforms, a large amount of remote sensing data has been obtained. Among these data, Hyperspectral images (HSIs) have received a lot of attention due to their rich spectral information and have been widely used for land cover classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib1" title="">1</a>]</cite>. However, spectral mixing occurs when HSI contains multiple land cover types, and it affects the HSI classification performance.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To solve the problem, synthetic aperture radar (SAR) image is often used to provide complementary information for HSI, since the SAR sensor is particularly useful in cloudy or hazy environments when HSI sensors may be limited. By combining HSI with SAR data, land cover classification methods can mitigate the effects of atmospheric interference, leading to more robust classification results. Therefore, in this letter, we mainly focus on HSI and SAR data joint classification.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Traditionally, attribute and extinction profile <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib2" title="">2</a>]</cite> are used for multisource data feature extraction. Xia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib3" title="">3</a>]</cite> presented graph fusion-based method, in which morphological filters were used to the key components of cross-modal data. Then, spectral, spatial, and texture features were projected to a lower subspace to compute joint features. However, traditional feature extraction methods are limited in extracting high-level semantic information from raw data.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Recently, many deep learning-based HSI and SAR data joint classification methods have been proposed. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib4" title="">4</a>]</cite> presented a multi-scale interactive fusion network for multi-source data classification. Multi-scale features are extracted and fused via global dependence fusion module. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib5" title="">5</a>]</cite> presented an asymmetric feature fusion network, in which a feature calibration module is designed to exploit the spatial dependence of multisource features. In addition, graph neural network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib6" title="">6</a>]</cite>, bilinear fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib7" title="">7</a>]</cite>, and adversarial learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib8" title="">8</a>]</cite> are employed for multi-source remote sensing data classification.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="437" id="S1.F1.g1" src="x1.png" width="969"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the proposed HAPNet. In the HAPNet Block, multi-level spatial signals are fused through Hierarchical Attention Module (HAM), and the ability to interact and fuse feature information is enhanced through the Parallel Filter Fusion Module (PFFM).</figcaption>
</figure>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Although existing methods have achieved remarkable performance, it is non-trivial to build a robust classification model due to the following two challenges: <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">1) Multi-granularity feature modeling for HSI data.</span> Cross-scale feature extraction is important for HSI feature representation. Existing methods can hardly model information at multiple granularities. Hence, how to modeling global, spectral, and local features simultaneously for HSI is a non-trivial task. <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">2) How to enhance cross-modal feature interactions between HSI and SAR data.</span> Existing multi-source image classification methods commonly capture multi-source feature interactions in the spatial domain. Feature interactions in the frequency domain is rarely explored. How to uncover the cross-modal feature interactions in the frequency domain is of great importance.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To overcome the above limitations, we propose a <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.p6.1.1">H</span>ierarchical <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.p6.1.2">A</span>ttention and <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.p6.1.3">P</span>arallel filter fusion <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.p6.1.4">Net</span>work, dubbed as HAPNet. Specifically, to efficiently extract multi-scale information, we design a Hierarchical Attention Module (HAM) to capture global, spectral and local information from HSI simultaneously. In addition, to enhance cross-modal feature interactions between HSI and SAR data, we propose a Parallel Filter Fusion Module (PFFM) for feature fusion. The feature interactions between HSI and SAR data are modeled as a set of learnable global filters which are applied to the spectrum of the input features. Therefore, cross-modal feature interactions among spatial locations are enhanced in the frequency domain. Extensive experiments on two HSI and SAR datasets have fully validated that our proposed method is superior to other state-of-the-art competitors.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Our main contributions can be summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.p8">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present the HAM as an enhancement to the self-attention mechanism. This module integrates global, spectral, and local features to provide more comprehensive feature representation for multi-source remote sensing image classification.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We develop PFFM to enhance cross-modal feature interactions in the frequency domain. The module enhances feature interactions among different spatial locations in the frequency domain, and thus effectively improves the classification performance.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S1.F1" title="Figure 1 â€£ I Introduction â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">1</span></a>, the proposed HAPNet comprises HSI feature encoder, SAR feature encoder, feature fusion modules, and a classifier. Three hierarchical attention blocks are employed for HSI feature encoder, and three convolutional layers are used for SAR feature encoder. Subsequently, features from each level, derived from two encoder branches, are processed through feature fusion module. Finally, the fused features are fed into two fully-connected layers to generate the outcome. It should be noted that feature expansion is used here, and the expansion ratio is set to 2. In the HSI feature encoder, Principle Component Analysis (PCA) is employed to select the best 30 spectral bands. As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S1.F1" title="Figure 1 â€£ I Introduction â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">1</span></a>, HAM and PFFM are the key components to improve the classification results. Subsequently, we provide in-depth descriptions of both modules.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="569" id="S2.F2.g1" src="x2.png" width="457"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of Hierarchical Attention Module (HAM). Global, spectral and local features are modeled in parallel. The global and spectral features are extracted via anchored self-attention in the spatial and spectral dimensions, respectively. The local features captures local structures via depth-wise convolutions.</figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Hierarchical Attention Module (HAM)</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S2.F2" title="Figure 2 â€£ II Methodology â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">2</span></a>, HAM is the key component that provides the hierarchical feature modeling capacity in the global, spectral, and local range. HAM first split the input feature into three branches. The global branch models long-range feature dependencies in the spatial dimension via anchored self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib9" title="">9</a>]</cite>. The spectral branch models long-range feature dependencies in the spectral dimension via shifted windows attention. The local branch captures local structures in the input feature via depth-wise convolutions. Features maps generated from the global, spectral, and local branch are combined via element-wise summation, and finally fed to the FFN to enhance the non-linear feature transformation. It should be noted that HAM is an extension of the self-attention mechanism. HAM provides a more comprehensive multi-granularity feature modeling approach through multi-scale and parallel feature processing. HAM captures global, spectral, and local information simultaneously, while the self-attention only captures global feature dependencies.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.5"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.5.1">Global and Spectral Feature Modeling.</span> In global and spectral branches, we use anchor self-attention for feature modeling. As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S2.F2" title="Figure 2 â€£ II Methodology â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">2</span></a>, the triplet of <math alttext="\mathbf{Q}" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\mathbf{Q}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">bold_Q</annotation></semantics></math>, <math alttext="\mathbf{K}" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.1"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">ğŠ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">ğŠ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\mathbf{K}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">bold_K</annotation></semantics></math>, <math alttext="\mathbf{V}" class="ltx_Math" display="inline" id="S2.SS1.p2.3.m3.1"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">ğ•</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">ğ•</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">\mathbf{V}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m3.1d">bold_V</annotation></semantics></math> is generated by linear projections. Then, an average pooling layer is implemented to reduce the spatial/spectral dimensions to generate <span class="ltx_text ltx_markedasmath ltx_font_bold" id="S2.SS1.p2.5.2">A</span>. The spatial/spectral dimension is down-scaled by a factor of <math alttext="s" class="ltx_Math" display="inline" id="S2.SS1.p2.5.m5.1"><semantics id="S2.SS1.p2.5.m5.1a"><mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.5.m5.1d">italic_s</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">To be specific, the anchor self-attention is computed as follows:</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{Y}=\mathbf{M_{e}}\cdot\mathbf{Z}=\mathbf{M_{e}}\cdot\left(\mathbf{M_{d%
}}\cdot\mathbf{V}\right)," class="ltx_Math" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml">ğ˜</mi><mo id="S2.E1.m1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.4.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.5" xref="S2.E1.m1.1.1.1.1.5.cmml"><msub id="S2.E1.m1.1.1.1.1.5.2" xref="S2.E1.m1.1.1.1.1.5.2.cmml"><mi id="S2.E1.m1.1.1.1.1.5.2.2" xref="S2.E1.m1.1.1.1.1.5.2.2.cmml">ğŒ</mi><mi id="S2.E1.m1.1.1.1.1.5.2.3" xref="S2.E1.m1.1.1.1.1.5.2.3.cmml">ğ</mi></msub><mo id="S2.E1.m1.1.1.1.1.5.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.1.1.1.1.5.1.cmml">â‹…</mo><mi id="S2.E1.m1.1.1.1.1.5.3" xref="S2.E1.m1.1.1.1.1.5.3.cmml">ğ™</mi></mrow><mo id="S2.E1.m1.1.1.1.1.6" xref="S2.E1.m1.1.1.1.1.6.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.3.2.cmml">ğŒ</mi><mi id="S2.E1.m1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.3.3.cmml">ğ</mi></msub><mo id="S2.E1.m1.1.1.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.1.1.1.1.1.2.cmml">â‹…</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml">ğŒ</mi><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml">ğ</mi></msub><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml">â‹…</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml">ğ•</mi></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><and id="S2.E1.m1.1.1.1.1a.cmml" xref="S2.E1.m1.1.1.1"></and><apply id="S2.E1.m1.1.1.1.1b.cmml" xref="S2.E1.m1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.4"></eq><ci id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3">ğ˜</ci><apply id="S2.E1.m1.1.1.1.1.5.cmml" xref="S2.E1.m1.1.1.1.1.5"><ci id="S2.E1.m1.1.1.1.1.5.1.cmml" xref="S2.E1.m1.1.1.1.1.5.1">â‹…</ci><apply id="S2.E1.m1.1.1.1.1.5.2.cmml" xref="S2.E1.m1.1.1.1.1.5.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.5.2.1.cmml" xref="S2.E1.m1.1.1.1.1.5.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.5.2.2.cmml" xref="S2.E1.m1.1.1.1.1.5.2.2">ğŒ</ci><ci id="S2.E1.m1.1.1.1.1.5.2.3.cmml" xref="S2.E1.m1.1.1.1.1.5.2.3">ğ</ci></apply><ci id="S2.E1.m1.1.1.1.1.5.3.cmml" xref="S2.E1.m1.1.1.1.1.5.3">ğ™</ci></apply></apply><apply id="S2.E1.m1.1.1.1.1c.cmml" xref="S2.E1.m1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.6.cmml" xref="S2.E1.m1.1.1.1.1.6"></eq><share href="https://arxiv.org/html/2408.12760v1#S2.E1.m1.1.1.1.1.5.cmml" id="S2.E1.m1.1.1.1.1d.cmml" xref="S2.E1.m1.1.1.1"></share><apply id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><ci id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2">â‹…</ci><apply id="S2.E1.m1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.3.2">ğŒ</ci><ci id="S2.E1.m1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.3.3">ğ</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1"><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1">â‹…</ci><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2">ğŒ</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3">ğ</ci></apply><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3">ğ•</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\mathbf{Y}=\mathbf{M_{e}}\cdot\mathbf{Z}=\mathbf{M_{e}}\cdot\left(\mathbf{M_{d%
}}\cdot\mathbf{V}\right),</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">bold_Y = bold_M start_POSTSUBSCRIPT bold_e end_POSTSUBSCRIPT â‹… bold_Z = bold_M start_POSTSUBSCRIPT bold_e end_POSTSUBSCRIPT â‹… ( bold_M start_POSTSUBSCRIPT bold_d end_POSTSUBSCRIPT â‹… bold_V ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{M_{d}}=\text{Softmax}\left(\mathbf{A}\cdot\mathbf{K}^{T}/\sqrt{d}%
\right)," class="ltx_Math" display="block" id="S2.E2.m1.1"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><msub id="S2.E2.m1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.3.2.cmml">ğŒ</mi><mi id="S2.E2.m1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.3.3.cmml">ğ</mi></msub><mo id="S2.E2.m1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.cmml"><mtext id="S2.E2.m1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.3a.cmml">Softmax</mtext><mo id="S2.E2.m1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml">ğ€</mi><mo id="S2.E2.m1.1.1.1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml">â‹…</mo><msup id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">ğŠ</mi><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">T</mi></msup></mrow><mo id="S2.E2.m1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.cmml">/</mo><msqrt id="S2.E2.m1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml">d</mi></msqrt></mrow><mo id="S2.E2.m1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><eq id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.2"></eq><apply id="S2.E2.m1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2">ğŒ</ci><ci id="S2.E2.m1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3">ğ</ci></apply><apply id="S2.E2.m1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"><times id="S2.E2.m1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.2"></times><ci id="S2.E2.m1.1.1.1.1.1.3a.cmml" xref="S2.E2.m1.1.1.1.1.1.3"><mtext id="S2.E2.m1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.3">Softmax</mtext></ci><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1"><divide id="S2.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1"></divide><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2"><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.1">â‹…</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2">ğ€</ci><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3">superscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.2">ğŠ</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.3">ğ‘‡</ci></apply></apply><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3"><root id="S2.E2.m1.1.1.1.1.1.1.1.1.3a.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3"></root><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3.2">ğ‘‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\mathbf{M_{d}}=\text{Softmax}\left(\mathbf{A}\cdot\mathbf{K}^{T}/\sqrt{d}%
\right),</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.1d">bold_M start_POSTSUBSCRIPT bold_d end_POSTSUBSCRIPT = Softmax ( bold_A â‹… bold_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT / square-root start_ARG italic_d end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{M_{e}}=\text{Softmax}\left(\mathbf{Q}\cdot\mathbf{A}^{T}/\sqrt{d}%
\right)," class="ltx_Math" display="block" id="S2.E3.m1.1"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.3.2.cmml">ğŒ</mi><mi id="S2.E3.m1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.3.3.cmml">ğ</mi></msub><mo id="S2.E3.m1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml"><mtext id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.3a.cmml">Softmax</mtext><mo id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml">ğ</mi><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml">â‹…</mo><msup id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">ğ€</mi><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">T</mi></msup></mrow><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml">/</mo><msqrt id="S2.E3.m1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml">d</mi></msqrt></mrow><mo id="S2.E3.m1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.2"></eq><apply id="S2.E3.m1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.3.2">ğŒ</ci><ci id="S2.E3.m1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.3.3">ğ</ci></apply><apply id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><times id="S2.E3.m1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.2"></times><ci id="S2.E3.m1.1.1.1.1.1.3a.cmml" xref="S2.E3.m1.1.1.1.1.1.3"><mtext id="S2.E3.m1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3">Softmax</mtext></ci><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"><divide id="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1"></divide><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2"><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.1">â‹…</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2">ğ</ci><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3">superscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2">ğ€</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3">ğ‘‡</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3"><root id="S2.E3.m1.1.1.1.1.1.1.1.1.3a.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3"></root><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.2">ğ‘‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\mathbf{M_{e}}=\text{Softmax}\left(\mathbf{Q}\cdot\mathbf{A}^{T}/\sqrt{d}%
\right),</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.1d">bold_M start_POSTSUBSCRIPT bold_e end_POSTSUBSCRIPT = Softmax ( bold_Q â‹… bold_A start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT / square-root start_ARG italic_d end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.p4.3">where <math alttext="\mathbf{A}" class="ltx_Math" display="inline" id="S2.SS1.p4.1.m1.1"><semantics id="S2.SS1.p4.1.m1.1a"><mi id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">ğ€</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">ğ€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">\mathbf{A}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.1.m1.1d">bold_A</annotation></semantics></math> is the anchor, <math alttext="\mathbf{M}_{e}" class="ltx_Math" display="inline" id="S2.SS1.p4.2.m2.1"><semantics id="S2.SS1.p4.2.m2.1a"><msub id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml"><mi id="S2.SS1.p4.2.m2.1.1.2" xref="S2.SS1.p4.2.m2.1.1.2.cmml">ğŒ</mi><mi id="S2.SS1.p4.2.m2.1.1.3" xref="S2.SS1.p4.2.m2.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><apply id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.2.m2.1.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p4.2.m2.1.1.2.cmml" xref="S2.SS1.p4.2.m2.1.1.2">ğŒ</ci><ci id="S2.SS1.p4.2.m2.1.1.3.cmml" xref="S2.SS1.p4.2.m2.1.1.3">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">\mathbf{M}_{e}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.2.m2.1d">bold_M start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math> denotes the attention maps between the query-anchor pair, and <math alttext="\mathbf{M}_{d}" class="ltx_Math" display="inline" id="S2.SS1.p4.3.m3.1"><semantics id="S2.SS1.p4.3.m3.1a"><msub id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml"><mi id="S2.SS1.p4.3.m3.1.1.2" xref="S2.SS1.p4.3.m3.1.1.2.cmml">ğŒ</mi><mi id="S2.SS1.p4.3.m3.1.1.3" xref="S2.SS1.p4.3.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><apply id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.1.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p4.3.m3.1.1.2.cmml" xref="S2.SS1.p4.3.m3.1.1.2">ğŒ</ci><ci id="S2.SS1.p4.3.m3.1.1.3.cmml" xref="S2.SS1.p4.3.m3.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">\mathbf{M}_{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.3.m3.1d">bold_M start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> denotes the attention maps between the anchor-key pair.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p5.1.1">Local Feature Modeling.</span> In local branch, channel-attention enhanced convolution is used for local feature modeling. Specifically, two depth-wise convolution layers are employed for feature extraction, and then channel-wise attention is used for feature enhancement.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Parallel Filter Fusion Module (PFFM)</span>
</h3>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="237" id="S2.F3.g1" src="x3.png" width="457"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of the Parallel Filter Fusion Network (PFFM).</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.7">To enhance cross-model feature interactions, we propose the PFFM to adaptively fuse HSI and SAR features. It integrates Fourier transform-based filters into existing feature fusion network. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S2.F3" title="Figure 3 â€£ II-B Parallel Filter Fusion Module (PFFM) â€£ II Methodology â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">3</span></a>, the proposed PFFM comprises of three parallel paths. We use <math alttext="F_{h}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><msub id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">ğ¹</ci><ci id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">F_{h}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="F_{s}" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">ğ¹</ci><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">F_{s}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">italic_F start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> to represent features from HSI and SAR, respectively. First, we use element-wise multiplication between <math alttext="F_{h}" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><msub id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">ğ¹</ci><ci id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">F_{h}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">italic_F start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="F_{s}" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><msub id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mi id="S2.SS2.p1.4.m4.1.1.2" xref="S2.SS2.p1.4.m4.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">ğ¹</ci><ci id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">F_{s}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">italic_F start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> to generate <math alttext="F_{f}" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m5.1"><semantics id="S2.SS2.p1.5.m5.1a"><msub id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.p1.5.m5.1.1.2" xref="S2.SS2.p1.5.m5.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.5.m5.1.1.3" xref="S2.SS2.p1.5.m5.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1.2">ğ¹</ci><ci id="S2.SS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">F_{f}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.5.m5.1d">italic_F start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math>. Then, <math alttext="F_{f}" class="ltx_Math" display="inline" id="S2.SS2.p1.6.m6.1"><semantics id="S2.SS2.p1.6.m6.1a"><msub id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml"><mi id="S2.SS2.p1.6.m6.1.1.2" xref="S2.SS2.p1.6.m6.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.6.m6.1.1.3" xref="S2.SS2.p1.6.m6.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><apply id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.6.m6.1.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p1.6.m6.1.1.2.cmml" xref="S2.SS2.p1.6.m6.1.1.2">ğ¹</ci><ci id="S2.SS2.p1.6.m6.1.1.3.cmml" xref="S2.SS2.p1.6.m6.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">F_{f}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.6.m6.1d">italic_F start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> is fed into the global filter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib10" title="">10</a>]</cite> to generate the attention weights <math alttext="\mathbf{W}" class="ltx_Math" display="inline" id="S2.SS2.p1.7.m7.1"><semantics id="S2.SS2.p1.7.m7.1a"><mi id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml">ğ–</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><ci id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">ğ–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">\mathbf{W}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.7.m7.1d">bold_W</annotation></semantics></math>. This process can be formulated as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{W}=\mathcal{F}^{-1}(K\odot\mathcal{F}(F_{h}\otimes F_{s}))," class="ltx_Math" display="block" id="S2.E4.m1.1"><semantics id="S2.E4.m1.1a"><mrow id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mi id="S2.E4.m1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.3.cmml">ğ–</mi><mo id="S2.E4.m1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E4.m1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.cmml"><msup id="S2.E4.m1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.1.1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.1.3.2.cmml">â„±</mi><mrow id="S2.E4.m1.1.1.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.1.3.3.cmml"><mo id="S2.E4.m1.1.1.1.1.1.3.3a" xref="S2.E4.m1.1.1.1.1.1.3.3.cmml">âˆ’</mo><mn id="S2.E4.m1.1.1.1.1.1.3.3.2" xref="S2.E4.m1.1.1.1.1.1.3.3.2.cmml">1</mn></mrow></msup><mo id="S2.E4.m1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E4.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml">K</mi><mo id="S2.E4.m1.1.1.1.1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml">âŠ™</mo><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml">â„±</mi></mrow><mo id="S2.E4.m1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">F</mi><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">h</mi></msub><mo id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">âŠ—</mo><msub id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">F</mi><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">s</mi></msub></mrow><mo id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E4.m1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"><eq id="S2.E4.m1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.2"></eq><ci id="S2.E4.m1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.3">ğ–</ci><apply id="S2.E4.m1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1"><times id="S2.E4.m1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.2"></times><apply id="S2.E4.m1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.1.3.2">â„±</ci><apply id="S2.E4.m1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.1.1.1.3.3"><minus id="S2.E4.m1.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.3.3"></minus><cn id="S2.E4.m1.1.1.1.1.1.3.3.2.cmml" type="integer" xref="S2.E4.m1.1.1.1.1.1.3.3.2">1</cn></apply></apply><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1"><times id="S2.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2"></times><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="latexml" id="S2.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.1">direct-product</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.2">ğ¾</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.3">â„±</ci></apply><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1">tensor-product</csymbol><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ¹</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.3">â„</ci></apply><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ¹</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘ </ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">\mathbf{W}=\mathcal{F}^{-1}(K\odot\mathcal{F}(F_{h}\otimes F_{s})),</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.1d">bold_W = caligraphic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_K âŠ™ caligraphic_F ( italic_F start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT âŠ— italic_F start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.p1.10">where <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S2.SS2.p1.8.m1.1"><semantics id="S2.SS2.p1.8.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.8.m1.1.1" xref="S2.SS2.p1.8.m1.1.1.cmml">â„±</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m1.1b"><ci id="S2.SS2.p1.8.m1.1.1.cmml" xref="S2.SS2.p1.8.m1.1.1">â„±</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.8.m1.1d">caligraphic_F</annotation></semantics></math> denotes the 2D FFT, <math alttext="K" class="ltx_Math" display="inline" id="S2.SS2.p1.9.m2.1"><semantics id="S2.SS2.p1.9.m2.1a"><mi id="S2.SS2.p1.9.m2.1.1" xref="S2.SS2.p1.9.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.9.m2.1b"><ci id="S2.SS2.p1.9.m2.1.1.cmml" xref="S2.SS2.p1.9.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.9.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.9.m2.1d">italic_K</annotation></semantics></math> is a learnable filter, <math alttext="\mathcal{F}^{-1}" class="ltx_Math" display="inline" id="S2.SS2.p1.10.m3.1"><semantics id="S2.SS2.p1.10.m3.1a"><msup id="S2.SS2.p1.10.m3.1.1" xref="S2.SS2.p1.10.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.10.m3.1.1.2" xref="S2.SS2.p1.10.m3.1.1.2.cmml">â„±</mi><mrow id="S2.SS2.p1.10.m3.1.1.3" xref="S2.SS2.p1.10.m3.1.1.3.cmml"><mo id="S2.SS2.p1.10.m3.1.1.3a" xref="S2.SS2.p1.10.m3.1.1.3.cmml">âˆ’</mo><mn id="S2.SS2.p1.10.m3.1.1.3.2" xref="S2.SS2.p1.10.m3.1.1.3.2.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.10.m3.1b"><apply id="S2.SS2.p1.10.m3.1.1.cmml" xref="S2.SS2.p1.10.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.10.m3.1.1.1.cmml" xref="S2.SS2.p1.10.m3.1.1">superscript</csymbol><ci id="S2.SS2.p1.10.m3.1.1.2.cmml" xref="S2.SS2.p1.10.m3.1.1.2">â„±</ci><apply id="S2.SS2.p1.10.m3.1.1.3.cmml" xref="S2.SS2.p1.10.m3.1.1.3"><minus id="S2.SS2.p1.10.m3.1.1.3.1.cmml" xref="S2.SS2.p1.10.m3.1.1.3"></minus><cn id="S2.SS2.p1.10.m3.1.1.3.2.cmml" type="integer" xref="S2.SS2.p1.10.m3.1.1.3.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.10.m3.1c">\mathcal{F}^{-1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.10.m3.1d">caligraphic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the inverse FFT. Finally, The enhanced features are combined via element-wise summation as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F_{fus}=(\mathbf{W}\otimes F_{h})\oplus(\mathbf{W}\otimes F_{s})." class="ltx_Math" display="block" id="S2.E5.m1.1"><semantics id="S2.E5.m1.1a"><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><mrow id="S2.E5.m1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><msub id="S2.E5.m1.1.1.1.1.4" xref="S2.E5.m1.1.1.1.1.4.cmml"><mi id="S2.E5.m1.1.1.1.1.4.2" xref="S2.E5.m1.1.1.1.1.4.2.cmml">F</mi><mrow id="S2.E5.m1.1.1.1.1.4.3" xref="S2.E5.m1.1.1.1.1.4.3.cmml"><mi id="S2.E5.m1.1.1.1.1.4.3.2" xref="S2.E5.m1.1.1.1.1.4.3.2.cmml">f</mi><mo id="S2.E5.m1.1.1.1.1.4.3.1" xref="S2.E5.m1.1.1.1.1.4.3.1.cmml">â¢</mo><mi id="S2.E5.m1.1.1.1.1.4.3.3" xref="S2.E5.m1.1.1.1.1.4.3.3.cmml">u</mi><mo id="S2.E5.m1.1.1.1.1.4.3.1a" xref="S2.E5.m1.1.1.1.1.4.3.1.cmml">â¢</mo><mi id="S2.E5.m1.1.1.1.1.4.3.4" xref="S2.E5.m1.1.1.1.1.4.3.4.cmml">s</mi></mrow></msub><mo id="S2.E5.m1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.3.cmml">=</mo><mrow id="S2.E5.m1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.2.cmml"><mrow id="S2.E5.m1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E5.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2.cmml">ğ–</mi><mo id="S2.E5.m1.1.1.1.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.E5.m1.1.1.1.1.1.1.1.1.1.cmml">âŠ—</mo><msub id="S2.E5.m1.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml">F</mi><mi id="S2.E5.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml">h</mi></msub></mrow><mo id="S2.E5.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E5.m1.1.1.1.1.2.3" xref="S2.E5.m1.1.1.1.1.2.3.cmml">âŠ•</mo><mrow id="S2.E5.m1.1.1.1.1.2.2.1" xref="S2.E5.m1.1.1.1.1.2.2.1.1.cmml"><mo id="S2.E5.m1.1.1.1.1.2.2.1.2" stretchy="false" xref="S2.E5.m1.1.1.1.1.2.2.1.1.cmml">(</mo><mrow id="S2.E5.m1.1.1.1.1.2.2.1.1" xref="S2.E5.m1.1.1.1.1.2.2.1.1.cmml"><mi id="S2.E5.m1.1.1.1.1.2.2.1.1.2" xref="S2.E5.m1.1.1.1.1.2.2.1.1.2.cmml">ğ–</mi><mo id="S2.E5.m1.1.1.1.1.2.2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.E5.m1.1.1.1.1.2.2.1.1.1.cmml">âŠ—</mo><msub id="S2.E5.m1.1.1.1.1.2.2.1.1.3" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3.cmml"><mi id="S2.E5.m1.1.1.1.1.2.2.1.1.3.2" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3.2.cmml">F</mi><mi id="S2.E5.m1.1.1.1.1.2.2.1.1.3.3" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3.3.cmml">s</mi></msub></mrow><mo id="S2.E5.m1.1.1.1.1.2.2.1.3" stretchy="false" xref="S2.E5.m1.1.1.1.1.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E5.m1.1.1.1.2" lspace="0em" xref="S2.E5.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.1b"><apply id="S2.E5.m1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><eq id="S2.E5.m1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.3"></eq><apply id="S2.E5.m1.1.1.1.1.4.cmml" xref="S2.E5.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.4.1.cmml" xref="S2.E5.m1.1.1.1.1.4">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.4.2.cmml" xref="S2.E5.m1.1.1.1.1.4.2">ğ¹</ci><apply id="S2.E5.m1.1.1.1.1.4.3.cmml" xref="S2.E5.m1.1.1.1.1.4.3"><times id="S2.E5.m1.1.1.1.1.4.3.1.cmml" xref="S2.E5.m1.1.1.1.1.4.3.1"></times><ci id="S2.E5.m1.1.1.1.1.4.3.2.cmml" xref="S2.E5.m1.1.1.1.1.4.3.2">ğ‘“</ci><ci id="S2.E5.m1.1.1.1.1.4.3.3.cmml" xref="S2.E5.m1.1.1.1.1.4.3.3">ğ‘¢</ci><ci id="S2.E5.m1.1.1.1.1.4.3.4.cmml" xref="S2.E5.m1.1.1.1.1.4.3.4">ğ‘ </ci></apply></apply><apply id="S2.E5.m1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.2"><csymbol cd="latexml" id="S2.E5.m1.1.1.1.1.2.3.cmml" xref="S2.E5.m1.1.1.1.1.2.3">direct-sum</csymbol><apply id="S2.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.1">tensor-product</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2">ğ–</ci><apply id="S2.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.2">ğ¹</ci><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.3">â„</ci></apply></apply><apply id="S2.E5.m1.1.1.1.1.2.2.1.1.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1"><csymbol cd="latexml" id="S2.E5.m1.1.1.1.1.2.2.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.1">tensor-product</csymbol><ci id="S2.E5.m1.1.1.1.1.2.2.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.2">ğ–</ci><apply id="S2.E5.m1.1.1.1.1.2.2.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.2.2.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.2.2.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3.2">ğ¹</ci><ci id="S2.E5.m1.1.1.1.1.2.2.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3.3">ğ‘ </ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.1c">F_{fus}=(\mathbf{W}\otimes F_{h})\oplus(\mathbf{W}\otimes F_{s}).</annotation><annotation encoding="application/x-llamapun" id="S2.E5.m1.1d">italic_F start_POSTSUBSCRIPT italic_f italic_u italic_s end_POSTSUBSCRIPT = ( bold_W âŠ— italic_F start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) âŠ• ( bold_W âŠ— italic_F start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The proposed PFFM use global filters to capture the feature interactions in the frequency domain. Therefore, cross-modal spectrum interactions are explored, which effectively improves the accuracy of multi-source data joint classification.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="469" id="S2.F4.g1" src="x4.png" width="457"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Classification results of different methods on the Augsburg dataset.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="586" id="S2.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Classification results of different methods on the Berlin dataset.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Experimental Results and Analysis</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Datasets and Experimental Setting</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">We evaluate the performance of the proposed HAPNet on two HSI and SAR datasets. Specifically, the first dataset is the Augsburg dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib11" title="">11</a>]</cite>. The HSI is captured by DAS-EOC, German Aerospace Center (DLR), over the city of Augsburg, Germany. The SAR data is collected by the Sentinel-1 sensor. For the HSI, there are 332 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mo id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><times id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">Ã—</annotation></semantics></math> 485 pixels and 180 spectral bands ranging from 0.4 to 2.5 <math alttext="\mu" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_Î¼</annotation></semantics></math>m. There are 7 distinct land cover classes in the ground truth. The second dataset is the Berlin dataset. The dataset contains 797 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mo id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><times id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">Ã—</annotation></semantics></math> 220 pixels, and the HSI contains 244 spectral bands. The are 8 distinct land cover classes in the ground truth. Due to the number of spectral bands in the Augsburg dataset and Berlin dataset being 180 and 244 respectively, to retain key feature information and reduce the input data volume of the neural network to improve its computational efficiency, we choose the PCA method to reduce the dimensionality of the spectral features in hyperspectral images to 30 channels. We use the standard training and test sets in both datasets. Specifically, 761 training samples are used on the Augsburg dataset, and 2820 training samples are used on the Berlin dataset.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To demonstrate the effectiveness of the proposed HAPNet, six state-of-the-art methods are selected for comparison: TBCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib12" title="">12</a>]</cite>, S<sup class="ltx_sup" id="S3.SS1.p2.1.1">2</sup>ENet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib13" title="">13</a>]</cite>, ExViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib14" title="">14</a>]</cite>, FusAtNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib15" title="">15</a>]</cite> and DFINet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#bib.bib16" title="">16</a>]</cite>. These methods are evaluated by visual comparison and quantitative metrics of individual class accuracy, overall accuracy (OA), average accuracy (AA), and Kappa coefficient. OA denotes the ratio of the total number of correctly classified pixels to the total number of pixels in the dataset, and it provides an overall assessment of the classification accuracy. AA provides a balanced measure by taking into account the accuracy for each class. Kappa provides a more robust measure of accuracy by taking into account the possibility of agreement by random chance.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">The proposed HAPNet was conducted on NVIDIA RTX 3090 GPU. The training phase spanned over 100 epochs. The Adam optimizer is used with the learning rate of 0.0003. The batch size is set as 128. The input patch size for the proposed HAPNet is <math alttext="11\times 11" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mn id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">11</mn><mo id="S3.SS1.p3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p3.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">11</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><times id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></times><cn id="S3.SS1.p3.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p3.1.m1.1.1.2">11</cn><cn id="S3.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p3.1.m1.1.1.3">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">11\times 11</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">11 Ã— 11</annotation></semantics></math> pixels.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Experimental Results and Discussion</span>
</h3>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Experimental results on the Augsburg dataset.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.1" style="width:411.9pt;height:219.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(20.1pt,-10.7pt) scale(1.108036159166,1.108036159166) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.1">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="S3.T1.1.1.1.2">Class</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T1.1.1.1.3">TBCNN</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T1.1.1.1.1">S<sup class="ltx_sup" id="S3.T1.1.1.1.1.1">2</sup>ENet</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T1.1.1.1.4">FusAtNet</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T1.1.1.1.5">DFINet</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T1.1.1.1.6">ExViT</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T1.1.1.1.7">HAPNet</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1">Forest</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.2">90.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.3.1">98.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.4">93.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.5">97.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.6">90.04</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.7">96.57</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.3.1">Residential area</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.2">93.89</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.3.3.1">99.08</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.4">97.58</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.5">98.37</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.6">95.44</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.7">95.78</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.4.1">Industrail area</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.2">8.28</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.3">12.19</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.4">26.48</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.5">61.31</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.6">34.58</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.4.7.1">68.02</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.5.1">Low plants</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.2">91.97</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.3">91.78</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.5.4.1">97.67</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.5">92.63</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.6">90.68</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.7">94.83</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.6.1">Allotment</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.2">38.24</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.3">45.12</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.4">52.77</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.5">49.33</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.6">51.82</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.6.7.1">64.24</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.7.1">Commercial area</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.2">1.40</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.3">1.22</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.4">24.66</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.5">3.54</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.6"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.7.6.1">28.63</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.7">18.08</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.8.1">Water</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.2">10.82</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.3">24.09</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.4">47.51</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.5">26.61</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.6">17.65</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.8.7.1">48.00</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.9.1">OA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.9.2">84.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.9.3">88.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.9.4">90.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.9.5">90.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.9.6">86.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.9.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.9.7.1">91.44</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.10.1">AA</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.2">47.92</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.3">53.08</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.4">62.92</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.5">61.30</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.6">58.40</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.10.7.1">69.36</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.11">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r" id="S3.T1.1.1.11.1">Kappa</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T1.1.1.11.2">77.13</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T1.1.1.11.3">82.61</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T1.1.1.11.4">86.33</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T1.1.1.11.5">86.47</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T1.1.1.11.6">80.79</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T1.1.1.11.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.11.7.1">87.75</span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Experimental results on the Berlin dataset.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:411.9pt;height:239.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(20.1pt,-11.7pt) scale(1.108036159166,1.108036159166) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="S3.T2.1.1.1.2">Class</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T2.1.1.1.3">TBCNN</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T2.1.1.1.1">S<sup class="ltx_sup" id="S3.T2.1.1.1.1.1">2</sup>ENet</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T2.1.1.1.4">FusAtNet</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T2.1.1.1.5">DFINet</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T2.1.1.1.6">ExViT</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T2.1.1.1.7">HAPNet</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1">Forest</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2">81.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.3">81.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.2.4.1">86.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.5">80.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.6">78.01</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.7">82.84</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.3.1">Residential area</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.2">76.26</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.3">73.05</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.3.4.1">91.38</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.5">72.81</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.6">74.05</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.7">89.47</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.4.1">Industrial area</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.2">39.67</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.3.1">62.61</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.4">19.76</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.5">38.89</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.6">39.48</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.7">47.43</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.5.1">Low plant</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.2">49.78</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.3">82.82</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.4">20.00</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5">78.09</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.5.6.1">84.15</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.7">81.71</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.6.1">Soil</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.6.2.1">89.42</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.3">86.41</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.4">48.72</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.5">73.48</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.6">88.03</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.7">71.12</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.7.1">Allotment</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.7.2">54.36</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.7.3">54.61</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.7.4">38.89</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.7.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.5.1">72.54</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.7.6">70.00</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.7.7">67.02</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.8.1">Commercial area</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.2">4.65</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.3">2.56</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.4">18.47</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.5">22.80</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.8.6.1">38.18</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.7">24.74</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.9.1">Water</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.2">41.93</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.3">75.96</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.4">29.61</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.5">68.15</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.6">56.41</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.9.7.1">77.19</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.10.1">OA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.10.2">67.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.10.3">71.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.10.4">70.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.10.5">70.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.10.6">72.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.10.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.10.7.1">80.51</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.11.1">AA</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.11.2">54.72</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.11.3">64.88</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.11.4">44.13</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.11.5">63.45</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.11.6">66.04</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.11.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.11.7.1">66.44</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r" id="S3.T2.1.1.12.1">Kappa</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T2.1.1.12.2">50.96</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T2.1.1.12.3">58.02</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T2.1.1.12.4">51.07</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T2.1.1.12.5">56.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T2.1.1.12.6">60.48</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S3.T2.1.1.12.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.12.7.1">68.77</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Results on the Augsburg dataset.</span> The classification maps of different methods on the Augsburg dataset is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S2.F4" title="Figure 4 â€£ II-B Parallel Filter Fusion Module (PFFM) â€£ II Methodology â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">4</span></a>, and the corresponding quantitative evaluations are illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S3.T1" title="TABLE I â€£ III-B Experimental Results and Discussion â€£ III Experimental Results and Analysis â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">I</span></a>. It should be noted that all the methods for comparison are designed for multi-source image joint classification. As can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S3.T1" title="TABLE I â€£ III-B Experimental Results and Discussion â€£ III Experimental Results and Analysis â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">I</span></a>, the OA value for the proposed HAPNet achieves 91.44%. It surpasses existing methods by at least 1.04%. This indicates that HAPNet effectively integrates global, spectral, and local features, providing a more comprehensive feature representation for multi-source data classification. Specifically, HAPNet performs better than the S<sup class="ltx_sup" id="S3.SS2.p1.1.2">2</sup>ENet, which is a self-attention-based method. It is evident that hierarchical attention provides better feature modeling capabilities than self-attention mechanism. Additionally, HAPNet performs better than the DFINet in which cross-attention fusion is employed. It demonstrated that cross-modal feature fusion in the frequency domain is more efficient than cross-attention.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.3"><span class="ltx_text ltx_font_italic" id="S3.SS2.p2.3.1">Results on the Berlin dataset.</span> The classification results on the Berlin dataset is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S2.F5" title="Figure 5 â€£ II-B Parallel Filter Fusion Module (PFFM) â€£ II Methodology â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">5</span></a>, and the corresponding quantitative evaluations are illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S3.T2" title="TABLE II â€£ III-B Experimental Results and Discussion â€£ III Experimental Results and Analysis â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">II</span></a>. The proposed HAPNet achieves 80.51<math alttext="\%" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mo id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">%</annotation></semantics></math>, 66.44<math alttext="\%" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mo id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><csymbol cd="latexml" id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">%</annotation></semantics></math> and 68.77<math alttext="\%" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mo id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><csymbol cd="latexml" id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">%</annotation></semantics></math> for OA, AA and Kappa, respectively. We found that Berlin dataset is more challenging compared to the Ausburg dataset. Nevertheless, the proposed HAPNet achieves the best performance on â€˜Waterâ€™ class. The reason might be that PFFM complements the water-sensitive features in SAR data with the HSI features. At the same time, the classification results of the other classes also achieve satisfying results. As a result, the proposed HAPNet performs better than the other methods on the Berlin dataset.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.1">Computational Cost Analysis.</span> The computational costs of different methods are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S3.T3" title="TABLE III â€£ III-B Experimental Results and Discussion â€£ III Experimental Results and Analysis â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">III</span></a>. As can be observed that, only ExViT is more computational efficient than the proposed HAPNet. However, the classification results of ExViT is not satisfying on both datasets. Compared with the other methods, the computational cost of the proposed HAPNet is quite competitive.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Computational costs of different methods.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:411.9pt;height:44.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(40.9pt,-4.5pt) scale(1.24797887425676,1.24797887425676) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<td class="ltx_td ltx_border_r ltx_border_tt ltx_border_t" id="S3.T3.1.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T3.1.1.1.3">TBCNN</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T3.1.1.1.1">S<sup class="ltx_sup" id="S3.T3.1.1.1.1.1">2</sup>ENet</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T3.1.1.1.4">FusAtNet</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T3.1.1.1.5">DFINet</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T3.1.1.1.6">ExViT</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S3.T3.1.1.1.7">HAPNet</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.1.2.1">FLOPs</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T3.1.1.2.2">135.2M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T3.1.1.2.3">107.9M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T3.1.1.2.4">670.6M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T3.1.1.2.5">297.5M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T3.1.1.2.6">53.6M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T3.1.1.2.7">103.7M</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Ablation Study</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We conduct a series of ablation experiments on two datasets to validate the effectiveness of the HAM and PFFM. We design two variants of HAPNet, i.e. without the HAM (w/o HAM) and without the PFFM (w/o PFFM). The experimental results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12760v1#S3.T4" title="TABLE IV â€£ III-C Ablation Study â€£ III Experimental Results and Analysis â€£ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification"><span class="ltx_text ltx_ref_tag">IV</span></a>. We find that the HAPNet always achieves better performance than its two variants on the two datasets. This demonstrates the necessity of the HAM and PFFM designed in HAPNet.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Ablation study of the proposed HAPNet.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T4.1">
<tr class="ltx_tr" id="S3.T4.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T4.1.1.2" rowspan="2"><span class="ltx_text" id="S3.T4.1.1.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T4.1.1.1">OA on different datasets (<math alttext="\%" class="ltx_Math" display="inline" id="S3.T4.1.1.1.m1.1"><semantics id="S3.T4.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S3.T4.1.1.1.m1.1d">%</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.2.1">Ausburg</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.2.2">Berlin</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.3.1">HAPNet w/o HAM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.3.2">90.35</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.3.3">74.49</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T4.1.4.1">HAPNet w/o PFFM</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.4.2">89.80</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.4.3">76.75</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T4.1.5.1">Proposed HAPNet</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.5.2"><span class="ltx_text ltx_font_bold" id="S3.T4.1.5.2.1">91.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.5.3"><span class="ltx_text ltx_font_bold" id="S3.T4.1.5.3.1">80.51</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Conclusions</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this letter, we developed an HSI and SAR data joint classification method which can model multi-granularity features simultaneously. To achieve this, we design HAM which integrates global, spectral, and local features to provide more comprehensive feature representation. In addition, we develop PFFM to enhance cross-modal feature interactions in the frequency domain. The module enhances feature interactions among different spatial locations in the frequency domain, and thus effectively improves the classification performance. Our HAPNet achieves very competitive performance with state-of-the-art methods on two HSI and SAR joint classification datasets.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
F.Â Luo, T.Â Zhou, J.Â Liu, T.Â Guo, X.Â Gong, and X.Â Gao, â€œDCENet: Diff-feature contrast enhancement network for semi-supervised hyperspectral change detection,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, vol.Â 62, pp. 1â€“14, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P.Â Ghamisi, B.Â HÃ¶fle, and X.Â X. Zhu, â€œHyperspectral and LiDAR data fusion using extinction profiles and deep convolutional neural network,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, vol.Â 10, no.Â 6, pp. 3011â€“3024, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J.Â Xia, W.Â Liao, and P.Â Du, â€œHyperspectral and LiDAR classification with semisupervised graph fusion,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Geoscience and Remote Sensing Letters</em>, vol.Â 17, no.Â 4, pp. 666â€“670, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J.Â Wang, W.Â Li, Y.Â Gao, M.Â Zhang, R.Â Tao, and Q.Â Du, â€œHyperspectral and SAR image classification via multiscale interactive fusion network,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE Transactions on Neural Networks and Learning Systems</em>, vol.Â 34, no.Â 12, pp. 10â€‰823â€“10â€‰837, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W.Â Li, Y.Â Gao, M.Â Zhang, R.Â Tao, and Q.Â Du, â€œAsymmetric feature fusion network for hyperspectral and SAR image classification,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">IEEE Transactions on Neural Networks and Learning Systems</em>, vol.Â 34, no.Â 10, pp. 8057â€“8070, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J.-Y. Yang, H.-C. Li, J.-H. Yang, L.Â Pan, Q.Â Du, and A.Â Plaza, â€œMultifrequency graph convolutional network with cross-modality mutual enhancement for multisource remote sensing data classification,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, vol.Â 62, pp. 1â€“14, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
X.Â Song, L.Â Li, L.Â Jiao, F.Â Liu, X.Â Liu, and S.Â Yang, â€œA spatialâ€“spectral bilinear representation fusion network for multimodal classification,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, vol.Â 61, pp. 1â€“17, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y.Â Gao, M.Â Zhang, W.Â Li, X.Â Song, X.Â Jiang, and Y.Â Ma, â€œAdversarial complementary learning for multisource remote sensing classification,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, vol.Â 61, pp. 1â€“13, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y.Â Li, Y.Â Fan, X.Â Xiang, D.Â Demandolx, R.Â Ranjan, R.Â Timofte, and L.Â VanÂ Gool, â€œEfficient and explicit modelling of image hierarchies for image restoration,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023, pp. 18â€‰278â€“18â€‰289.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y.Â Rao, W.Â Zhao, Z.Â Zhu, J.Â Zhou, and J.Â Lu, â€œGFNet: Global filter networks for visual recognition,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol.Â 45, no.Â 9, pp. 10â€‰960â€“10â€‰973, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D.Â Hong, J.Â Hu, J.Â Yao, J.Â Chanussot, and X.Â X. Zhu, â€œMultimodal remote sensing benchmark datasets for land cover classification with a shared and specific feature learning model,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">ISPRS Journal of Photogrammetry and Remote Sensing</em>, vol. 178, pp. 68â€“80, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X.Â Xu, W.Â Li, Q.Â Ran, Q.Â Du, L.Â Gao, and B.Â Zhang, â€œMultisource remote sensing data classification based on convolutional neural network,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, vol.Â 56, no.Â 2, pp. 937â€“949, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S.Â Fang, K.Â Li, and Z.Â Li, â€œS2ENet: Spatialâ€“spectral cross-modal enhancement network for classification of hyperspectral and LiDAR data,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE Geoscience and Remote Sensing Letters</em>, vol.Â 19, pp. 1â€“5, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J.Â Yao, B.Â Zhang, C.Â Li, D.Â Hong, and J.Â Chanussot, â€œExtended vision transformer (ExViT) for land use and land cover classification: A multimodal deep learning framework,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, vol.Â 61, pp. 1â€“15, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S.Â Mohla, S.Â Pande, B.Â Banerjee, and S.Â Chaudhuri, â€œFusAtNet: Dual attention based spectrospatial multimodal fusion network for hyperspectral and LiDAR classification,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2020, pp. 416â€“425.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y.Â Gao, W.Â Li, M.Â Zhang, J.Â Wang, W.Â Sun, R.Â Tao, and Q.Â Du, â€œHyperspectral and multispectral classification for coastal wetland using depthwise feature interaction network,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, vol.Â 60, pp. 1â€“15, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 22 23:12:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
