<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility</title>
<!--Generated on Tue Jul 23 02:05:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Spatial-temporal Data Mining,  Intelligent Transportation Systems,  Travel Time Estimation,  Transportation Mode Detection,  Trajectory Recovery." lang="en" name="keywords"/>
<base href="/html/2407.16123v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S1" title="In Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S2" title="In Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Preliminary</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S2.SS1" title="In 2. Preliminary ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Research scope, questions and scenarios</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S2.SS2" title="In 2. Preliminary ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Multi-region Route Travel Time Estimation (MRRTTE)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S2.SS3" title="In 2. Preliminary ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Multi-transportation-mode En Route Travel Time Estimation (MTERTTE)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S2.SS4" title="In 2. Preliminary ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Multi-transportation-mode Trajectory Recovery (MTRec)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S3" title="In Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed methods and results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S3.SS1" title="In 3. Proposed methods and results ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>MRRTTE for RQ1</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S3.SS2" title="In 3. Proposed methods and results ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>MTERTTE for RQ2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S3.SS3" title="In 3. Proposed methods and results ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>MTRec for RQ3</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S4" title="In Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion and future direction</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenxing Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wangchenxing@bupt.edu.cn">wangchenxing@bupt.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-4096-7972" title="ORCID identifier">0000-0003-4096-7972</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">School of Computer Science, Beijing University of Posts and Telecommunications</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id4.id1">With the rapid development of location based services, multimodal spatio-temporal (ST) data including trajectories, transportation modes, traffic flow and social check-ins are being collected for deep learning based methods. These deep learning based methods learn ST correlations to support the downstream tasks in the fields such as smart mobility, smart city and other intelligent transportation systems. Despite their effectiveness, ST data fusion and forecasting methods face practical challenges in real-world scenarios.
First, forecasting performance for ST data-insufficient area is inferior, making it necessary to transfer meta knowledge from heterogeneous area to enhance the sparse representations.
Second, it is nontrivial to accurately forecast in multi-transportation-mode scenarios due to the fine-grained ST features of similar transportation modes, making it necessary to distinguish and measure the ST correlations to alleviate the influence caused by entangled ST features.
At last, partial data modalities (e.g., transportation mode) are lost due to privacy or technical issues in certain scenarios, making it necessary to effectively fuse the multimodal sparse ST features and enrich the ST representations. To tackle these challenges, our research work aim to develop effective fusion and forecasting methods for multimodal ST data in smart mobility scenario. In this paper, we will introduce our recent works that investigates the challenges in terms of various real-world applications and establish the open challenges in this field for future work.</p>
</div>
<div class="ltx_keywords">Spatial-temporal Data Mining, Intelligent Transportation Systems, Travel Time Estimation, Transportation Mode Detection, Trajectory Recovery.
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 33rd ACM International Conference on Information and Knowledge Management; October 21‚Äì25, 2024; Boise, ID, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM ‚Äô24), October 21‚Äì25, 2024, Boise, ID, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">doi: </span>10.1145/3627673.3680261</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0436-9/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">ccs: </span>Information systems¬†Data mining</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">ccs: </span>Information systems¬†Location based services</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the acceleration of urbanization, multimodalspatio-temporal (ST) data fusion and forecasting techniques for smart mobility scenarios are increasingly becoming a research hotspot. Currently, previous studies <cite class="ltx_cite ltx_citemacro_citep">(Fang et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib7" title="">2023</a>; Qin et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib11" title="">2022</a>; Fang et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib8" title="">2024</a>; Chen et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib2" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib3" title="">2022</a>)</cite> have shown high prediction and recognition performance in homogeneous urban unimodal scenes. However, when dealing with heterogeneous, fine-grained and multimodal sparse scenes, ST data fusion and forecasting face many challenges, such as the heterogeneous of ST features in different regions, the similarity entanglement of fine-grained ST features, and the sparsity of partial modalities data in multimodal scenes, which all significantly degrade the performance of ST forecasting. To this end, our research aims to explore and solve the above challenges to improve the performance of ST data fusion and forecasting in smart mobility scenarios, especially in heterogeneous, fine-grained, and multimodal sparse scenarios.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Preliminary</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">To better understand our proposed research problem, we first introduce the research scope with illustrations and then introduce the problem definitions of scenarios related to our research.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Research scope, questions and scenarios</h3>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="228" id="S2.F1.g1" src="x1.png" width="418"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Our research scope and related scenarios. </figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S2.F1" title="Figure 1 ‚Ä£ 2.1. Research scope, questions and scenarios ‚Ä£ 2. Preliminary ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_tag">1</span></a>, our research scope mainly contains three research questions (RQ), which deals with <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">heterogeneous</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">fine-grained</span> and <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.3">multimodal sparse</span> scenes, respectively.</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">RQ1:</span> Existing ST forecasting methods perform well with the support of large-scale spatio-temporal data, but the performance degradation is obvious in cities and regions with less population or ST data <cite class="ltx_cite ltx_citemacro_citep">(Fan et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib5" title="">2022a</a>)</cite>. The critical to solve this problem is how to effectively extract potential ST knowledge of cities, learn heterogeneous ST correlations among different cities, and gradually transfer this rich meta-knowledge to urban regions with less ST data to improve the performance of ST forecasting methods in real applications.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">RQ2:</span> Accurate ST forecasting in multi-transportation-mode scenario is a challenging task because fine-grained transportation modes have similar and entangled ST characteristics that are difficult to accurately distinguish <cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib13" title="">2021</a>)</cite>. The key point to answer this research question is how to effectively measure the complex ST correlations and reduce the impact of fine-grained ST feature entanglement to enhance the forecasting performance in multi-transportation-mode scenario.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">RQ3:</span> Existing ST data forecasting algorithms usually rely on quantities of annotated data, but in urban scenarios, partial modalities (e.g., transportation mode) tend to be sparse due to privacy and technical limitations <cite class="ltx_cite ltx_citemacro_citep">(Xu et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib15" title="">2020</a>)</cite>. Our research focus on how to effectively fuse multimodal sparse ST features in urban scenes and enrich ST feature representations in order to improve the model‚Äôs performance in real-world scenario.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">To answer these research questions, we mainly focus on three scenarios, including <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">multi-region travel time estimation</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.2">multi-transportation-mode travel time estimation</span> and <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.3">multi-transportation-mode trajectory recovery</span>. We then introduce their problem formulations, respectively.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="171" id="S2.F2.sf1.g1" src="x2.png" width="418"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Route TTE.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="174" id="S2.F2.sf2.g1" src="x3.png" width="418"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>En Route TTE.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Illustration of Route TTE and En Route TTE.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Multi-region Route Travel Time Estimation (MRRTTE)</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.4">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S2.F2.sf1" title="In Figure 2 ‚Ä£ 2.1. Research scope, questions and scenarios ‚Ä£ 2. Preliminary ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_tag">2a</span></a>, when user sends the query at departure time <math alttext="t_{d}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><msub id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">t</mi><mi id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">ùë°</ci><ci id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">t_{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">italic_t start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> for a given route <math alttext="r" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">ùëü</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">italic_r</annotation></semantics></math>, the oracle will output the estimation through the deep learning model. It has been widely investigated for single-region route travel time estimation <cite class="ltx_cite ltx_citemacro_citep">(Lin et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib9" title="">2023</a>; Liu et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib10" title="">2023</a>; Zou et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib17" title="">2023</a>; Chen et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib3" title="">2022</a>)</cite> recently. Nevertheless, multi-region route travel time estimation has not been well explored in the literature. Since the knowledge transfer of ST features is reasonable <cite class="ltx_cite ltx_citemacro_citep">(Yao et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib16" title="">2019</a>)</cite>, we hope to propose the meta learning based model to learn critical knowledge for estimation based on data-sufficient region <math alttext="\mathcal{R}_{s}" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><msub id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">‚Ñõ</mi><mi id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">‚Ñõ</ci><ci id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\mathcal{R}_{s}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">caligraphic_R start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and then enhance the ST representations for those data-insufficient region <math alttext="\mathcal{R}_{is}" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><msub id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.4.m4.1.1.2" xref="S2.SS2.p1.4.m4.1.1.2.cmml">‚Ñõ</mi><mrow id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml"><mi id="S2.SS2.p1.4.m4.1.1.3.2" xref="S2.SS2.p1.4.m4.1.1.3.2.cmml">i</mi><mo id="S2.SS2.p1.4.m4.1.1.3.1" xref="S2.SS2.p1.4.m4.1.1.3.1.cmml">‚Å¢</mo><mi id="S2.SS2.p1.4.m4.1.1.3.3" xref="S2.SS2.p1.4.m4.1.1.3.3.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">‚Ñõ</ci><apply id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3"><times id="S2.SS2.p1.4.m4.1.1.3.1.cmml" xref="S2.SS2.p1.4.m4.1.1.3.1"></times><ci id="S2.SS2.p1.4.m4.1.1.3.2.cmml" xref="S2.SS2.p1.4.m4.1.1.3.2">ùëñ</ci><ci id="S2.SS2.p1.4.m4.1.1.3.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">\mathcal{R}_{is}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">caligraphic_R start_POSTSUBSCRIPT italic_i italic_s end_POSTSUBSCRIPT</annotation></semantics></math> so that its performance can be improved.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Multi-transportation-mode En Route Travel Time Estimation (MTERTTE)</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.3">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S2.F2.sf2" title="In Figure 2 ‚Ä£ 2.1. Research scope, questions and scenarios ‚Ä£ 2. Preliminary ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_tag">2b</span></a>, different from route travel time estimation, when user sends the query at departure time <math alttext="t_{d}" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><msub id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">t</mi><mi id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">ùë°</ci><ci id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">t_{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">italic_t start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math>, not only the route <math alttext="r" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">ùëü</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.2.m2.1d">italic_r</annotation></semantics></math> for the remaining path but also the traveled path <math alttext="r_{t}" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m3.1"><semantics id="S2.SS3.p1.3.m3.1a"><msub id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml"><mi id="S2.SS3.p1.3.m3.1.1.2" xref="S2.SS3.p1.3.m3.1.1.2.cmml">r</mi><mi id="S2.SS3.p1.3.m3.1.1.3" xref="S2.SS3.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><apply id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.3.m3.1.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS3.p1.3.m3.1.1.2.cmml" xref="S2.SS3.p1.3.m3.1.1.2">ùëü</ci><ci id="S2.SS3.p1.3.m3.1.1.3.cmml" xref="S2.SS3.p1.3.m3.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">r_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.3.m3.1d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> are sent to the oracle to output the estimation. Existing works employ meta learning to transfer knowledge from the traveled the path to the remaining path in few shot learning scheme which limits the efficiency <cite class="ltx_cite ltx_citemacro_citep">(Fan et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib4" title="">2022b</a>; Fang et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib6" title="">2021</a>)</cite>. Moreover, these works only consider the vehicle-based (i.e., cars) queries which cannot be applied to queries that consist of multiple transportation modes. It is nontrivial to achieve this goal, since the entangled ST features between similar transportation modes are difficult to distinguish and measure. For example, train and subway modes (or car and bus modes) are difficult to distinguish in relatively short time window <cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib13" title="">2021</a>)</cite>.
To this end, in our research, we hope to propose a solution that estimates the multi-transportation-mode en route travel time for given queries that can disentangle the ST features for better performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Multi-transportation-mode Trajectory Recovery (MTRec)</h3>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="187" id="S2.F3.g1" src="x4.png" width="381"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Illustration of multi-transportation-mode trajectory recovery.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S2.F3" title="Figure 3 ‚Ä£ 2.4. Multi-transportation-mode Trajectory Recovery (MTRec) ‚Ä£ 2. Preliminary ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_tag">3</span></a>, given the low-sample sequence in multiple transportation modes, the oracle will output the recovered <math alttext="\epsilon_{p}" class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.1"><semantics id="S2.SS4.p1.1.m1.1a"><msub id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.1.1.2" xref="S2.SS4.p1.1.m1.1.1.2.cmml">œµ</mi><mi id="S2.SS4.p1.1.m1.1.1.3" xref="S2.SS4.p1.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><apply id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.p1.1.m1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.1.1.2">italic-œµ</ci><ci id="S2.SS4.p1.1.m1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">\epsilon_{p}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.1.m1.1d">italic_œµ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math>-sample sequence. Previous works <cite class="ltx_cite ltx_citemacro_citep">(Ren et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib12" title="">2021</a>; Chen et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib2" title="">2023</a>)</cite> treat this task as the vehicle-only recovery which ignores the inherent heterogeneity. Moreover, some modalities (e.g., the transportation modes) tend to be sparse due to privacy or technical issues which brings difficulties to fusion of multimodal ST features. To this end, we hope to propose a deep learning based solution to tackle the fusion issues caused by sparsity so that multimodal ST features can be enriched and the recovery performance can be improved.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Proposed methods and results</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>MRRTTE for RQ1</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Most existing MRRTTE rely on large amounts of historical trajectory data to construct deep learning models to predict travel time in specific urban areas. These algorithms often rely on spatio-temporally dense datasets for training to achieve a high degree of generalization in a specific urban area. However, the historical trajectory data that can be collected in less densely populated areas are usually insufficient to train predictive models with high robustness. To solve this problem, our research utilizes meta-learning methods to comprehensively analyze historical trajectory data from different urban areas and design a meta-learning-based deep learning algorithm called MetaTTE to learn spatio-temporal features in urban areas and migrating this meta-knowledge to data-sparse urban areas in order to improve the accuracy of the estimation. Our proposed the MetaTTE achieves higher accuracy compared with the baselines (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S3.T1" title="Table 1 ‚Ä£ 3.1. MRRTTE for RQ1 ‚Ä£ 3. Proposed methods and results ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_tag">1</span></a>) and the detailed results can be found at <cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#bib.bib14" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Performance comparison for MRRTTE.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.1" style="width:433.6pt;height:247.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(27.9pt,-15.9pt) scale(1.14764665300284,1.14764665300284) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S3.T1.1.1.1.1.1.1">Baselines</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.1.1.1.1.2">Chengdu</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.1.1.1.1.3">Porto</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.2.2.1">MAE</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.2.2.2">MAPE (%)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.2.2.3">RMSE</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.2.2.4">MAE</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.2.2.5">MAPE (%)</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.2.2.6">RMSE</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.1">AVG</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.2">442.20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.3">39.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.4">8443.60</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.5">182.64</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.6">26.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.3.3.7">1128.21</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T1.1.1.4.4.1">LR</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.4.4.2">516.23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.4.4.3">49.09</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.4.4.4">1204.99</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.4.4.5">194.40</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.4.4.6">33.90</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.4.7">279.20</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T1.1.1.5.5.1">GBM</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.5.5.2">454.50</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.5.5.3">41.67</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.5.5.4">1121.32</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.5.5.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.5.5.5.1">148.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.5.5.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.5.5.6.1">24.59</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.5.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.5.5.7.1">209.07</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T1.1.1.6.6.1">TEMP</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.6.6.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.6.6.2.1">334.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.6.6.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.6.6.3.1">39.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.6.6.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.6.6.4.1">761.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.6.6.5">174.44</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.6.6.6">28.73</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.6.7">260.81</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.1">WDR</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.2">433.99</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.3">29.74</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.4">1024.92</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.5">164.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.6">22.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.7.7.7">244.41</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T1.1.1.8.8.1">DeepTTE</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.8.8.2">413.09</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.8.8.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.8.8.3.1">24.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.8.8.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.8.8.4.1">926.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.8.8.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.8.8.5.1">84.29</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.8.8.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.8.8.6.1">14.79</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.8.7"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.1.1.8.8.7.1">90.29</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T1.1.1.9.9.1">STNN</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.9.9.2">427.33</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.9.9.3">30.08</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.9.9.4">1011.88</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.9.9.5">226.30</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.9.9.6">35.44</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.9.9.7">331.75</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T1.1.1.10.10.1">MURAT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.10.10.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.10.10.2.1">396.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.10.10.3">29.29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.10.10.4">994.95</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.10.10.5">165.91</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.10.10.6">27.10</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.10.7">177.83</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T1.1.1.11.11.1">Nei-TTE</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.11.11.2">414.16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.11.11.3">30.04</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.11.11.4">1038.71</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.11.11.5">106.30</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.11.11.6">15.23</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.11.11.7">183.03</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.1">MetaTTE (ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.2"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.1.1.12.12.2.1">236.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.1.1.12.12.3.1">23.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.4"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.1.1.12.12.4.1">745.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.5"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.1.1.12.12.5.1">62.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.6"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.1.1.12.12.6.1">8.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.1.12.12.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.12.12.7.1">196.78</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>MTERTTE for RQ2</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">MTERTTE faces multiple challenges, including entangled spatio-temporal features of fine-grained transportation modes as well as diverse user behaviors and travel intentions, which make it difficult to achieve accurate estimation. Particularly, in practical applications, certain transportation modes are prone to cause confusion in model differentiation due to the possession of entangled spatio-temporal features, resulting in insufficient distance between classes to differentiate them. To cope with the above problems, we design an attention based hybrid transportation mode modeling approach. Specifically, we design an algorithm based on autocorrelation attention mechanism for predicting the travel time of multiple transportation modes. The sub-sequence correlations in multimodal spatio-temporal data are learnt through the autocorrelation attention mechanism, and we carry out personalized embedding and feature learning for the user‚Äôs personalized mode characteristics, and then conduct the adaptive fusion of multimodal spatio-temporal features, and finally, we design a multi-tasking framework to introduce spatio-temporal features of the transportation modes to assist in supervising the training optimization which helps achieve the accurate estimation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>MTRec for RQ3</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In our research, we focus on the transportation-aware trajectory recovery problem, which is distinct from the conventional vehicle-based trajectory recovery, facing two major challenges: heterogeneity and personalization. For the heterogeneity, the velocity of the mobile object is intrinsically correlated with the specific transportation mode, containing inherent heterogeneity. For the personalization, the trajectory data is complicated by substantial variations in users, which are different in personalized behaviors. To address these challenges, we design a novel effective multi-modal deep model, coined as PTrajRec, for transportation-aware trajectory recovery. Specifically, we initially embed location, behavior, and transportation mode modalities in distinct channels, which not only reflect spatio-temporal information encapsulated in location sequences but also introduce the heterogeneity and personalization characteristics associated with mode and behavior sequences. For further modeling these modalities, we employ the auto-correlation mechanism to learn periodic dependencies on the temporal dimension and the graph attention mechanism to learn road network dependencies on the spatial dimension. At last, we propose a dual-view constraint mechanism to assist the fine-grained trajectory recovery and design three auxiliary tasks to address the inherent heterogeneity. The experimental results in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.16123v1#S3.T2" title="Table 2 ‚Ä£ 3.3. MTRec for RQ3 ‚Ä£ 3. Proposed methods and results ‚Ä£ Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrates its superior.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Performance comparison of MTRec.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.10" style="width:303.5pt;height:108.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-151.1pt,53.9pt) scale(0.501154018989887,0.501154018989887) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.10.10">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.10.10.11.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.10.10.11.1.1" rowspan="2"><span class="ltx_text" id="S3.T2.10.10.11.1.1.1">Baselines</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="5" id="S3.T2.10.10.11.1.2">Geolife</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S3.T2.10.10.11.1.3">Singapore</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.1.1">Recall <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.1.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T2.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.1.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.2.2">Precision <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.2.2.2.2.m1.1"><semantics id="S3.T2.2.2.2.2.m1.1a"><mo id="S3.T2.2.2.2.2.m1.1.1" stretchy="false" xref="S3.T2.2.2.2.2.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.2.m1.1b"><ci id="S3.T2.2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.2.2.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.3.3.3">Accuracy <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.3.3.3.3.m1.1"><semantics id="S3.T2.3.3.3.3.m1.1a"><mo id="S3.T2.3.3.3.3.m1.1.1" stretchy="false" xref="S3.T2.3.3.3.3.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.3.m1.1b"><ci id="S3.T2.3.3.3.3.m1.1.1.cmml" xref="S3.T2.3.3.3.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.3.3.3.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.4.4.4">MAE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.4.4.4.4.m1.1"><semantics id="S3.T2.4.4.4.4.m1.1a"><mo id="S3.T2.4.4.4.4.m1.1.1" stretchy="false" xref="S3.T2.4.4.4.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.4.4.m1.1b"><ci id="S3.T2.4.4.4.4.m1.1.1.cmml" xref="S3.T2.4.4.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.4.4.4.4.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.5.5.5.5">RMSE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.5.5.5.5.m1.1"><semantics id="S3.T2.5.5.5.5.m1.1a"><mo id="S3.T2.5.5.5.5.m1.1.1" stretchy="false" xref="S3.T2.5.5.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.5.5.m1.1b"><ci id="S3.T2.5.5.5.5.m1.1.1.cmml" xref="S3.T2.5.5.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.5.5.5.5.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.6.6.6.6">Recall <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.6.6.6.6.m1.1"><semantics id="S3.T2.6.6.6.6.m1.1a"><mo id="S3.T2.6.6.6.6.m1.1.1" stretchy="false" xref="S3.T2.6.6.6.6.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.6.6.m1.1b"><ci id="S3.T2.6.6.6.6.m1.1.1.cmml" xref="S3.T2.6.6.6.6.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.6.6.6.6.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.7.7.7.7">Precision <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.7.7.7.7.m1.1"><semantics id="S3.T2.7.7.7.7.m1.1a"><mo id="S3.T2.7.7.7.7.m1.1.1" stretchy="false" xref="S3.T2.7.7.7.7.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.7.7.7.7.m1.1b"><ci id="S3.T2.7.7.7.7.m1.1.1.cmml" xref="S3.T2.7.7.7.7.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.7.7.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.7.7.7.7.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.8.8.8">Accuracy <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.8.8.8.8.m1.1"><semantics id="S3.T2.8.8.8.8.m1.1a"><mo id="S3.T2.8.8.8.8.m1.1.1" stretchy="false" xref="S3.T2.8.8.8.8.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.8.8.8.8.m1.1b"><ci id="S3.T2.8.8.8.8.m1.1.1.cmml" xref="S3.T2.8.8.8.8.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.8.8.8.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.8.8.8.8.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.9.9.9.9">MAE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.9.9.9.9.m1.1"><semantics id="S3.T2.9.9.9.9.m1.1a"><mo id="S3.T2.9.9.9.9.m1.1.1" stretchy="false" xref="S3.T2.9.9.9.9.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.9.9.9.9.m1.1b"><ci id="S3.T2.9.9.9.9.m1.1.1.cmml" xref="S3.T2.9.9.9.9.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.9.9.9.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.9.9.9.9.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.10.10">RMSE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.10.10.10.10.m1.1"><semantics id="S3.T2.10.10.10.10.m1.1a"><mo id="S3.T2.10.10.10.10.m1.1.1" stretchy="false" xref="S3.T2.10.10.10.10.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.10.10.10.10.m1.1b"><ci id="S3.T2.10.10.10.10.m1.1.1.cmml" xref="S3.T2.10.10.10.10.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.10.10.10.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.10.10.10.10.m1.1d">‚Üì</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.12.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.10.10.12.2.1">Linear</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.12.2.2">0.2519</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.12.2.3">0.3822</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.12.2.4">0.2139</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.12.2.5">0.902</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.10.10.12.2.6">1.292</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.12.2.7">0.6456</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.12.2.8">0.6260</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.12.2.9">0.5635</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.12.2.10">0.967</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.12.2.11">1.536</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.13.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.10.10.13.3.1">DHTR</th>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.13.3.2">0.2654</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.13.3.3">0.4301</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.13.3.4">0.2149</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.13.3.5">0.713</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.10.10.13.3.6">1.019</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.13.3.7">0.6556</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.13.3.8">0.7520</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.13.3.9">0.5668</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.13.3.10">0.874</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.13.3.11">0.964</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.14.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.10.10.14.4.1">T3S</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.14.4.2">0.2877</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.14.4.3">0.4883</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.14.4.4">0.2592</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.14.4.5">0.471</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.10.10.14.4.6">0.786</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.14.4.7">0.6884</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.14.4.8">0.7815</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.14.4.9">0.5816</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.14.4.10">0.600</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.14.4.11">0.944</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.15.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.10.10.15.5.1">T2Vec</th>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.15.5.2">0.2720</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.15.5.3">0.4878</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.15.5.4">0.2437</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.15.5.5">0.489</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.10.10.15.5.6">0.668</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.15.5.7">0.6816</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.15.5.8">0.7807</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.15.5.9">0.5810</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.15.5.10">0.642</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.15.5.11">0.983</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.16.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.10.10.16.6.1">Transformer</th>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.16.6.2">0.2641</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.16.6.3">0.4772</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.16.6.4">0.2408</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.16.6.5">0.534</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.10.10.16.6.6">0.722</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.16.6.7">0.6695</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.16.6.8">0.7691</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.16.6.9">0.5711</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.16.6.10">0.620</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.16.6.11">0.993</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.17.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.10.10.17.7.1">NeuTraj</th>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.17.7.2">0.3206</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.17.7.3">0.4125</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.17.7.4">0.2448</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.17.7.5">0.488</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.10.10.17.7.6">0.668</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.17.7.7">0.6748</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.17.7.8">0.7704</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.17.7.9">0.5548</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.17.7.10">0.628</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.17.7.11">1.001</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.18.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.10.10.18.8.1">GTS</th>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.18.8.2">0.3166</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.18.8.3">0.4697</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.18.8.4">0.2517</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.18.8.5">0.492</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.10.10.18.8.6">0.667</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.18.8.7">0.6664</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.18.8.8">0.7596</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.18.8.9">0.5705</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.18.8.10">0.633</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.18.8.11">1.000</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.19.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.10.10.19.9.1">MTrajRec</th>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.19.9.2">0.3267</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.19.9.3">0.4764</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.19.9.4">0.2616</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.19.9.5">0.440</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.10.10.19.9.6">0.601</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.19.9.7">0.6829</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.19.9.8">0.7928</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.19.9.9">0.5955</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.19.9.10">0.540</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.19.9.11">0.881</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.20.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.10.10.20.10.1">RNTrajRec</th>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.20.10.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.10.10.20.10.2.1">0.3300</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.20.10.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.10.10.20.10.3.1">0.4911</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.20.10.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.10.10.20.10.4.1">0.2630</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.20.10.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.10.10.20.10.5.1">0.414</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.10.10.20.10.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.10.10.20.10.6.1">0.575</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.20.10.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.10.10.20.10.7.1">0.6946</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.20.10.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.10.10.20.10.8.1">0.7942</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.20.10.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.10.10.20.10.9.1">0.5961</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.20.10.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.10.10.20.10.10.1">0.533</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.10.20.10.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.10.10.20.10.11.1">0.862</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.10.21.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.10.10.21.11.1">PTrajRec (ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T2.10.10.21.11.2"><span class="ltx_text ltx_font_bold" id="S3.T2.10.10.21.11.2.1">0.4351</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T2.10.10.21.11.3"><span class="ltx_text ltx_font_bold" id="S3.T2.10.10.21.11.3.1">0.5418</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T2.10.10.21.11.4"><span class="ltx_text ltx_font_bold" id="S3.T2.10.10.21.11.4.1">0.3824</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T2.10.10.21.11.5"><span class="ltx_text ltx_font_bold" id="S3.T2.10.10.21.11.5.1">0.266</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.10.10.21.11.6"><span class="ltx_text ltx_font_bold" id="S3.T2.10.10.21.11.6.1">0.358</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T2.10.10.21.11.7"><span class="ltx_text ltx_font_bold" id="S3.T2.10.10.21.11.7.1">0.7278</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T2.10.10.21.11.8"><span class="ltx_text ltx_font_bold" id="S3.T2.10.10.21.11.8.1">0.8544</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T2.10.10.21.11.9"><span class="ltx_text ltx_font_bold" id="S3.T2.10.10.21.11.9.1">0.6746</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T2.10.10.21.11.10"><span class="ltx_text ltx_font_bold" id="S3.T2.10.10.21.11.10.1">0.487</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" id="S3.T2.10.10.21.11.11"><span class="ltx_text ltx_font_bold" id="S3.T2.10.10.21.11.11.1">0.744</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Conclusion and future direction</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In general, the topic for heterogeneous, fine-grained and multimodal sparse characteristics of ST data are still open challenges for future works. In the era of foundation models, we would like to share, at least, the following research directions:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Multimodal foundation models for ST data that address the heterogeneity.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Finetuning and knowledge transfer based on LLM to support ST downstream tasks.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Effective prompt learning for ST downstream tasks to enrich the original ST feature representations.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Biography</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1"><span class="ltx_text ltx_font_bold" id="Sx1.p1.1.1">Chenxing Wang</span> is a Ph.D student at School of Computer Science, Beijing University of Posts and Telecommunications. His research interests mainly focus on spatio-temporal data mining including but not limited to: trajectory recovery, travel time estimation, transportation mode detection, traffic flow forecasting and next POI recommendation. He has published several papers in top journals and conference proceedings, such as ICDE, SIGIR, and T-ITS. Additionally, he is now serving as the research intern at Weixin Group, Tencent supported by 2024 Tencent Rhino-bird Research Elite Program.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was supported in part by the National Natural Science Foundation of China under Grant 62261042, the Key Research Projects of the Joint Research Fund for Beijing Natural Science Foundation and the Fengtai Rail Transit Frontier Research Joint Fund under Grant L221003, Beijing Natural Science Foundation under Grant 4232035 and 4222034, the Strategic Priority Research Program of Chinese Academy of Sciences under Grant XDA28040500 and BUPT Excellent Ph.D. Students Foundation under Grant CX2022132.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuqi Chen, Hanyuan Zhang, Weiwei Sun, and Baihua Zheng. 2023.

</span>
<span class="ltx_bibblock">RNTrajRec: Road Network Enhanced Trajectory Recovery with Spatial-Temporal Transformer. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">39th IEEE International Conference on Data Engineering, ICDE 2023, Anaheim, CA, USA, April 3-7, 2023</em>. IEEE, 829‚Äì842.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zebin Chen, Xiaolin Xiao, Yue-Jiao Gong, Jun Fang, Nan Ma, Hua Chai, and Zhiguang Cao. 2022.

</span>
<span class="ltx_bibblock">Interpreting Trajectories from Multiple Views: A Hierarchical Self-Attention Network for Estimating the Time of Arrival. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">KDD ‚Äô22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022</em>. ACM, 2771‚Äì2779.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et¬†al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yu Fan, Jiajie Xu, Rui Zhou, Jianxin Li, Kai Zheng, Lu Chen, and Chengfei Liu. 2022b.

</span>
<span class="ltx_bibblock">MetaER-TTE: An Adaptive Meta-learning Model for En Route Travel Time Estimation. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022</em>. ijcai.org, 2023‚Äì2029.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et¬†al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yu Fan, Jiajie Xu, Rui Zhou, and Chengfei Liu. 2022a.

</span>
<span class="ltx_bibblock">Transportation-Mode Aware Travel Time Estimation via Meta-learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Database Systems for Advanced Applications - 27th International Conference, DASFAA 2022, Virtual Event, April 11-14, 2022, Proceedings, Part II</em> <em class="ltx_emph ltx_font_italic" id="bib.bib5.4.2">(Lecture Notes in Computer Science, Vol.¬†13246)</em>, Arnab Bhattacharya, Janice Lee, Mong Li, Divyakant Agrawal, P.¬†Krishna Reddy, Mukesh¬†K. Mohania, Anirban Mondal, Vikram Goyal, and Rage¬†Uday Kiran (Eds.). Springer, 472‚Äì488.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et¬†al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiaomin Fang, Jizhou Huang, Fan Wang, Lihang Liu, Yibo Sun, and Haifeng Wang. 2021.

</span>
<span class="ltx_bibblock">SSML: Self-Supervised Meta-Learner for En Route Travel Time Estimation at Baidu Maps. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">KDD ‚Äô21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021</em>. ACM, 2840‚Äì2848.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et¬†al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuchen Fang, Yanjun Qin, Haiyong Luo, Fang Zhao, Bingbing Xu, Liang Zeng, and Chenxing Wang. 2023.

</span>
<span class="ltx_bibblock">When Spatio-Temporal Meet Wavelets: Disentangled Traffic Forecasting via Efficient Spectral Graph Attention Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">39th IEEE International Conference on Data Engineering, ICDE 2023, Anaheim, CA, USA, April 3-7, 2023</em>. IEEE, 517‚Äì529.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et¬†al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yuchen Fang, Yanjun Qin, Haiyong Luo, Fang Zhao, and Kai Zheng. 2024.

</span>
<span class="ltx_bibblock">STWave+: A Multi-Scale Efficient Spectral Graph Attention Network With Long-Term Trends for Disentangled Traffic Flow Forecasting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">IEEE Trans. Knowl. Data Eng.</em> 36, 6 (2024), 2671‚Äì2685.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et¬†al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yan Lin, Huaiyu Wan, Jilin Hu, Shengnan Guo, Bin Yang, Youfang Lin, and Christian¬†S. Jensen. 2023.

</span>
<span class="ltx_bibblock">Origin-Destination Travel Time Oracle for Map-based Services.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proc. ACM Manag. Data</em> 1, 3 (2023), 217:1‚Äì217:27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hao Liu, Wenzhao Jiang, Shui Liu, and Xi Chen. 2023.

</span>
<span class="ltx_bibblock">Uncertainty-Aware Probabilistic Travel Time Prediction for On-Demand Ride-Hailing at DiDi. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023</em>. ACM, 4516‚Äì4526.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et¬†al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yanjun Qin, Yuchen Fang, Haiyong Luo, Fang Zhao, and Chenxing Wang. 2022.

</span>
<span class="ltx_bibblock">Next Point-of-Interest Recommendation with Auto-Correlation Enhanced Multi-Modal Transformer Network. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">SIGIR ‚Äô22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022</em>, Enrique Amig√≥, Pablo Castells, Julio Gonzalo, Ben Carterette, J.¬†Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 2612‚Äì2616.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et¬†al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Huimin Ren, Sijie Ruan, Yanhua Li, Jie Bao, Chuishi Meng, Ruiyuan Li, and Yu Zheng. 2021.

</span>
<span class="ltx_bibblock">MTrajRec: Map-Constrained Trajectory Recovery via Seq2Seq Multi-task Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">SIGKDD</em>. 1410‚Äì1419.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Chenxing Wang, Haiyong Luo, Fang Zhao, and Yanjun Qin. 2021.

</span>
<span class="ltx_bibblock">Combining Residual and LSTM Recurrent Networks for Transportation Mode Detection Using Multimodal Sensors Integrated in Smartphones.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">IEEE Trans. Intell. Transp. Syst.</em> 22, 9 (2021), 5473‚Äì5485.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Chenxing Wang, Fang Zhao, Haichao Zhang, Haiyong Luo, Yanjun Qin, and Yuchen Fang. 2022.

</span>
<span class="ltx_bibblock">Fine-Grained Trajectory-Based Travel Time Estimation for Multi-City Scenarios Based on Deep Meta-Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">IEEE Trans. Intell. Transp. Syst.</em> 23, 9 (2022), 15716‚Äì15728.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et¬†al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Saijun Xu, Jiajie Xu, Rui Zhou, Chengfei Liu, Zhixu Li, and An Liu. 2020.

</span>
<span class="ltx_bibblock">TADNM: A Transportation-Mode Aware Deep Neural Model for Travel Time Estimation. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Database Systems for Advanced Applications - 25th International Conference, DASFAA 2020, Jeju, South Korea, September 24-27, 2020, Proceedings, Part I</em> <em class="ltx_emph ltx_font_italic" id="bib.bib15.4.2">(Lecture Notes in Computer Science, Vol.¬†12112)</em>, Yunmook Nah, Bin Cui, Sang-Won Lee, Jeffrey¬†Xu Yu, Yang-Sae Moon, and Steven¬†Euijong Whang (Eds.). Springer, 468‚Äì484.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et¬†al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Huaxiu Yao, Yiding Liu, Ying Wei, Xianfeng Tang, and Zhenhui Li. 2019.

</span>
<span class="ltx_bibblock">Learning from Multiple Cities: A Meta-Learning Approach for Spatial-Temporal Prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019</em>. ACM, 2181‚Äì2191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et¬†al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Guojian Zou, Ziliang Lai, Changxi Ma, Meiting Tu, Jing Fan, and Ye Li. 2023.

</span>
<span class="ltx_bibblock">When Will We Arrive? A Novel Multi-Task Spatio-Temporal Attention Network Based on Individual Preference for Estimating Travel Time.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">IEEE Trans. Intell. Transp. Syst.</em> 24, 10 (2023), 11438‚Äì11452.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul 23 02:05:51 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
