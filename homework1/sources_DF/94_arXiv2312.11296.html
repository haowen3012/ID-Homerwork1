<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection</title>
<!--Generated on Mon Dec 18 15:37:11 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
natural language processing,  personalization,  transfer learning,  humor detection,  data fusion
" lang="en" name="keywords"/>
<base href="/html/2312.11296v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="I Introduction ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S2" title="II Related work ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Data Fusion in Subjective Humor Detection</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="III-A Personalized Humor Detection ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Personalized Humor Detection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="III-B Human-Centered Data Fusion ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Human-Centered Data Fusion</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Datasets</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS1" title="IV-A Personalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Personalized datasets</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS1.SSS1" title="IV-A1 Cockamamie Gobbledegook ‚Ä£ IV-A Personalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>Cockamamie Gobbledegook</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS1.SSS2" title="IV-A2 Humor ‚Ä£ IV-A Personalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>Humor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS1.SSS3" title="IV-A3 Humicroedit ‚Ä£ IV-A Personalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>3 </span>Humicroedit</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS1.SSS4" title="IV-A4 Doccano 1 ‚Ä£ IV-A Personalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>4 </span>Doccano 1</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS1.SSS5" title="IV-A5 Doccano 2 ‚Ä£ IV-A Personalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>5 </span>Doccano 2</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS2" title="IV-B Generalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Generalized datasets</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS6" title="IV-B6 ColBERT ‚Ä£ IV-B Generalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>6 </span>ColBERT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS7" title="IV-B7 HahaIberlef2021 ‚Ä£ IV-B Generalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>7 </span>HahaIberlef2021</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS8" title="IV-B8 Hahackathon ‚Ä£ IV-B Generalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>8 </span>Hahackathon</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS9" title="IV-B9 Sarcasmania ‚Ä£ IV-B Generalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>9 </span>Sarcasmania</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S5" title="V Models ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS1" title="V-A TXT-Baseline ‚Ä£ V Models ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">TXT-Baseline</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS2" title="V-B OneHot ‚Ä£ V Models ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">OneHot</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS3" title="V-C SHEEP-Formula ‚Ä£ V Models ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">SHEEP-Formula</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS4" title="V-D SHEEP-Simple ‚Ä£ V Models ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">SHEEP-Simple</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS5" title="V-E SHEEP-Medium ‚Ä£ V Models ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span> </span><span class="ltx_text ltx_font_italic">SHEEP-Medium</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS6" title="V-F UserId ‚Ä£ V Models ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-F</span> </span><span class="ltx_text ltx_font_italic">UserId</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S6" title="VI Experimental setup ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Experimental setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S7" title="VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S7.SS1" title="VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-A</span> </span><span class="ltx_text ltx_font_italic">Evaluation of Data Fusion Techniques</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S7.SS2" title="VII-B Knowledge Transfer in Majority Voting and Personalization ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-B</span> </span><span class="ltx_text ltx_font_italic">Knowledge Transfer in Majority Voting and Personalization</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S7.SS3" title="VII-C Impact of Data Fusion on Personalized Architectures ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-C</span> </span><span class="ltx_text ltx_font_italic">Impact of Data Fusion on Personalized Architectures</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S8" title="VIII Discussion ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S9" title="IX Conclusions and Future work ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IX </span><span class="ltx_text ltx_font_smallcaps">Conclusions and Future work</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2312.11296v1 [cs.CL] 18 Dec 2023</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
From Generalized Laughter to Personalized Chuckles:
Unleashing the Power of Data Fusion in Subjective Humor Detection
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Julita Bielaniewicz and
Przemys≈Çaw Kazienko
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">Department of Artificial Intelligence, Wroc≈Çaw University of Science and Technology, Poland</span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_typewriter" id="id2.2.id1" style="font-size:80%;">{julita.bielaniewicz, kazienko}@pwr.edu.pl</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">The vast area of subjectivity in Natural Language Processing (NLP) poses a challenge to the solutions typically used in generalized tasks. As exploration in the scope of generalized NLP is much more advanced, it implies the tremendous gap that is still to be addressed amongst all feasible tasks where an opinion, taste, or feelings are inherent, thus creating a need for a solution, where a data fusion could take place. We have chosen the task of funniness, as it heavily relies on the sense of humor, which is fundamentally subjective.
Our experiments across five personalized and four generalized datasets involving several personalized deep neural architectures have shown that the task of humor detection greatly benefits from the inclusion of personalized data in the training process. We tested five scenarios of training data fusion that focused on either generalized (majority voting) or personalized approaches to humor detection. The best results were obtained for the setup, in which all available personalized datasets were joined to train the personalized reasoning model. It boosted the prediction performance by up to approximately 35% of the macro F1 score. Such a significant gain was observed for all five personalized test sets. At the same time, the impact of the model‚Äôs architecture was much less than the personalization itself.
It seems that concatenating personalized datasets, even with the cost of normalizing the range of annotations across all datasets, if combined with the personalized models, results in an enormous increase in the performance of humor detection.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
natural language processing, personalization, transfer learning, humor detection, data fusion

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent years have shown that the subjective field of natural language processing is gradually receiving more attention, yet still comparatively less than the predominant generalization approach. This is especially apparent when focusing on a single personalization task, such as emotion detection, hate speech, or funniness. Although this implicitly signals the difference in the amount of research, the possibilities that lie in personalized data utilization even in a single subjective task are very promising<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>, <a class="ltx_ref" href="#bib.bib2" title="">2</a>]</cite>.
An indirect issue that tends to be ignored is the nature of groups and subgroups that can be distinguished according to general characteristics in the sense of humor of individuals. Those groups could be sorted according to age, gender, political views, occupation, and many other characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib3" title="">3</a>]</cite>. Undoubtedly, this will allow for a more accurate image of the sense of humor per group, but will still remain overall generalized. What is crucial is that in each group we would still observe unique individuals who end up omitted from the majority. Regardless, following this path, we could create subgroups in the said groups that will sort users more accurately, yet still have a generalization approach. Furthermore, this would lead to more and more subgroups that will set the gold standard of a single group anew, still bearing the generalization, until we focus on each person separately. This final product sets a standard for the sense of humor per person, which is, in essence, humor personalization. In this way, we eliminate the possibility that anyone with an unusual sense of humor would be ignored.
Given the prospect concealed within personalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib4" title="">4</a>, <a class="ltx_ref" href="#bib.bib5" title="">5</a>, <a class="ltx_ref" href="#bib.bib6" title="">6</a>, <a class="ltx_ref" href="#bib.bib7" title="">7</a>]</cite>, there was a need for scenarios where there was the possibility of checking different combinations of subjectivity and its impact on the model performance. We intended to implement data fusion that could improve the overall performance of personalized reasoning by showing which combination of subjectivity to generalization ratio was optimal. Our research resulted in discovering the major gain for the models trained on a fully subjective fused dataset, as well as noticing that it effectively increases the quality of personalized deep learning architectures. This achievement is especially exciting, as it opens the door to the possibilities that lie within the data fusion domain.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this work, our aim is to answer the following research questions:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Does providing knowledge from other datasets help the model better understand the task of humor detection? (see Sec.¬†<a class="ltx_ref" href="#S7.SS1" title="VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VII-A</span></span></a>)</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Does the fusion of data about individual users sense of humor from various datasets affect the reasoning quality for other personalized datasets in personalized and majority voting-based scenarios? (see Sec.¬†<a class="ltx_ref" href="#S7.SS1" title="VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VII-A</span></span></a>)</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Does the addition of generalized datasets improve the quality of humor prediction for personalized datasets in a majority voting scenario? (see Sec.¬†<a class="ltx_ref" href="#S7.SS2" title="VII-B Knowledge Transfer in Majority Voting and Personalization ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VII-B</span></span></a>)</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Does the knowledge transfer between personalized datasets affect the model performance in a similar way as the knowledge transfer between the datasets with majority voting? (see Sec.¬†<a class="ltx_ref" href="#S7.SS2" title="VII-B Knowledge Transfer in Majority Voting and Personalization ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VII-B</span></span></a>)</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">Does the gain received from data fusion depend on the language or the domain of the datasets? (see Sec.¬†<a class="ltx_ref" href="#S7.SS2" title="VII-B Knowledge Transfer in Majority Voting and Personalization ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VII-B</span></span></a>)</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S1.I1.i6.p1">
<p class="ltx_p" id="S1.I1.i6.p1.1">Does the use of personalized data fusion techniques improve the prediction quality of various personalized architectures in a similar way? (see Sec.¬†<a class="ltx_ref" href="#S7.SS3" title="VII-C Impact of Data Fusion on Personalized Architectures ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VII-C</span></span></a>)</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">There is no doubt that tasks that are subjective in nature in the area of Natural Language Processing tend to receive a somewhat dismissive treatment, being simplified in order to analyze what kind of content the majority of users are anticipating. Despite best efforts, this setup divides all users into people who fall into the general majority of each subset, who are directly involved throughout all research, and users who end up being abruptly cut off from the analysis right from the very beginning. That group of people never had the opportunity to be included because of their diversity compared to their respectful groups. This way, a huge part of useful information is lost, simply because the initial assumption is based on generalization. This simplification relies on the absence of user information and, thus, is made up of texts and annotations. After all, the sole detection of an anticipated feature does not need data on the subjective perception of a given content as such. However, we argue that setting the gold standard per user enriches the knowledge about the chosen area of NLP.
The usual tendency in the scope of Natural Language Processing when aiming for better results in generalized humor detection is to create and utilize many different categorizations.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>, <a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite> The most prominent, 12 types of humor included in the Theory of Humor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite> have withstanded the test of time in the span of the last 30 years and remain relevant in humor detection to this day, setting the ground for many humor-related research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>, <a class="ltx_ref" href="#bib.bib12" title="">12</a>, <a class="ltx_ref" href="#bib.bib13" title="">13</a>]</cite> as well as other categorizations of humor in text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib14" title="">14</a>, <a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>.
This research provides a general insight into why some people may find something humorous while others may not. The analysis in this scope is focused on standard per group. It was determined to be a key factor in the realm of user-centric funniness if outliers of humor were omitted <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>]</cite>. Comparing different jokes can help identify groups of people who have a similar sense of humor. Calculating the similarity between different jokes facilitates identifying human groups with a common sense of humor. Nonetheless, more defined groups are only scratching the surface of possibilities that can be easily uncovered if switching to a personalized approach. If a goal towards capturing a group of users accurately enough to hit the target includes outliers who may grow indifferent to such a solution, it is not viable in the long run in our reality.
Humor is a subjective experience that can bring about varying degrees of amusement in its viewers, but can also lead to other unexpected responses, such as feeling insulted <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>, <a class="ltx_ref" href="#bib.bib18" title="">18</a>]</cite>. If analyzing, for example, dark humor, many people are not enjoying it purely because of different levels of tolerance and sensitivity to disturbing, sometimes also offensive humor. As such, it may not be an extreme situation, yet it is possible that a user in certain humor groups will receive content upsetting enough to experience mental turmoil, only because of their age group or other categorizing factors. This experience is completely opposite to planned, despite best efforts, because the humor norm was simplified.
With this in mind, it is better to use personalization when possible. If these conditions are met, and therefore we have access to different personalization datasets, it is time to strategize data fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib19" title="">19</a>]</cite> to obtain an effective prediction quality. Research on the specific scope of humor data fusion does not focus on personalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib20" title="">20</a>, <a class="ltx_ref" href="#bib.bib21" title="">21</a>, <a class="ltx_ref" href="#bib.bib22" title="">22</a>]</cite> and so we propose our own take on this domain.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Data Fusion in Subjective Humor Detection</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Personalized Humor Detection</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The feeling of amusement is experienced differently by each person. This phenomenon is further enhanced in the case of texts, where the choice of words and their diverse impact on a particular user also play an important role. Therefore, the natural need to model the individual user‚Äôs sense of humor is outlined. To quantify the funniness of a text, a binary classification<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>, <a class="ltx_ref" href="#bib.bib24" title="">24</a>, <a class="ltx_ref" href="#bib.bib25" title="">25</a>, <a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite> is often used: <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">funny</span> (class 1) or <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">not funny</span> (class 0). This generalized approach ignores the peculiarities of human perspective and assumes training the model to predict the same class for every user (see Fig.¬†<a class="ltx_ref" href="#S3.F1" title="Figure 1 ‚Ä£ III-A Personalized Humor Detection ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">1</span></a>, top).</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The idea of personalized humor detection<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> is quite different. It uses the assumption that the individual user‚Äôs sense of humor should be taken into account when predicting the funniness of a text along with the content of the evaluated text. In this approach, the trained model provides various predictions for different people. The personalized humor detection approach is presented in the bottom row of Fig.¬†<a class="ltx_ref" href="#S3.F1" title="Figure 1 ‚Ä£ III-A Personalized Humor Detection ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="749" id="S3.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Difference between the generalized (top) and personalized (bottom) perspective. In the first one, the same prediction is provided for every user. In the second one, the model outputs various predictions on the basis of individual user preferences.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Human-Centered Data Fusion</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The perception of humor in a text is a personality trait characterized by a very high subjectivity<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib28" title="">28</a>]</cite>. To tackle this problem, we propose a data fusion approach focused on combining user annotations from various humor datasets. The basic variant assumes the integration of knowledge from multiple humor datasets with user annotations aggregated via majority voting to reduce the uncertainty caused by multiple users annotating texts from various domains and written in diverse languages (first column in Fig.¬†<a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ III-B Human-Centered Data Fusion ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">2</span></a>). Our technique can also be used to synthesize the knowledge obtained from the majority voting annotations with generic knowledge about the funniness of textual content from generalized datasets (second column in Fig.¬†<a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ III-B Human-Centered Data Fusion ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">2</span></a>). The most advanced variant of our method leverages the original user annotations to preserve every person‚Äôs individual sense of humor. In this way, we maximize the available knowledge that the model can acquire during the training procedure (third column in Fig.¬†<a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ III-B Human-Centered Data Fusion ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="458" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Our human-centered data fusion approaches: (1) multiple combined majority voting datasets, (2) majority voting datasets combined with generalized datasets, and (3) multiple personalized datasets.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Datasets</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We have chosen 9 datasets that consisted of annotated pieces of humorous text in the form of individual words, phrases, or even pairs of text before and after a slight change. 5 of those datasets were strictly personalized, which means that they included information about the user IDs, as well as many annotations for an independent text.
The remaining 4 datasets are generalized, with only one annotation per text available. The vast majority of texts were in English, but there are also ones in Spanish, as well as Polish languages. More detailed information on personalized datasets is available in Tab.¬†<a class="ltx_ref" href="#S4.T1" title="TABLE I ‚Ä£ IV-B9 Sarcasmania ‚Ä£ IV-B Generalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">I</span></a> and the generalized datasets are described in Tab.¬†<a class="ltx_ref" href="#S4.T2" title="TABLE II ‚Ä£ IV-B9 Sarcasmania ‚Ä£ IV-B Generalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">II</span></a></p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Personalized datasets</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.5.1.1">IV-A</span>1 </span>Cockamamie Gobbledegook</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">This dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib29" title="">29</a>]</cite> is made of 10,000 English word expressions (1-2 words), usually word formations. Annotators are workers of the Amazon Mechanical Turk of the United States. For experiment purposes, we used only words that had at least one positive and one negative annotation in order to better distinguish between user preferences.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.5.1.1">IV-A</span>2 </span>Humor</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">A dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite> consisting of 27k Spanish tweets annotated by crowd-sourcing workers.
We have used the version<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pln-fing-udelar/humor/tree/main/previous" title="">https://github.com/pln-fing-udelar/humor/tree/main/previous</a></span></span></span> of the dataset that contains text, not only tweet IDs. For the purposes of our experiments, we have considered only texts with more than one annotation.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS3.5.1.1">IV-A</span>3 </span>Humicroedit</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite> was used for competition in the computational recognition of humor in SemEval-2020 Task 7 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite>. The authors used Amazon Mechanical Turk annotators to edit a single word of 14k news headlines to make them funny.
The embeddings of both original and edited texts with their scores are provided at the input to all models so that the context is taken into account in humor recognition.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS4.5.1.1">IV-A</span>4 </span>Doccano 1</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.2">This dataset is one of the iterations of the Doccano 1.0 project, which captures the perceptions and feelings evoked by textual content.
Each text has a length of up to 132 words (<math alttext="\mu" class="ltx_Math" display="inline" id="S4.SS1.SSS4.p1.1.m1.1"><semantics id="S4.SS1.SSS4.p1.1.m1.1a"><mi id="S4.SS1.SSS4.p1.1.m1.1.1" xref="S4.SS1.SSS4.p1.1.m1.1.1.cmml">Œº</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.1.m1.1b"><ci id="S4.SS1.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS4.p1.1.m1.1.1">ùúá</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS4.p1.1.m1.1d">italic_Œº</annotation></semantics></math> = 24.5, <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.SS1.SSS4.p1.2.m2.1"><semantics id="S4.SS1.SSS4.p1.2.m2.1a"><mi id="S4.SS1.SSS4.p1.2.m2.1.1" xref="S4.SS1.SSS4.p1.2.m2.1.1.cmml">œÉ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.2.m2.1b"><ci id="S4.SS1.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS4.p1.2.m2.1.1">ùúé</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.2.m2.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS4.p1.2.m2.1d">italic_œÉ</annotation></semantics></math> = 16.2). The average number of texts annotated by each person was approximately 790, and the average number of annotators for each text was approximately 32. In total, the number of annotations is around 31,700.
On average, labels with zero value appear 62% of the time, with a standard deviation of 22%. Additionally, empty labels are seen 4% of the time, with a standard deviation of 8%.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS5.5.1.1">IV-A</span>5 </span>Doccano 2</h4>
<div class="ltx_para" id="S4.SS1.SSS5.p1">
<p class="ltx_p" id="S4.SS1.SSS5.p1.1">The dataset which, similarly to Doccano 1, is one of the iterations of the Doccano 1.0 project. On average, each person annotated around 358 texts, and each was annotated by 2 annotators. In its entirety, it is a little under 17,700 annotations. As in Doccano 1, each annotation consists of 26 independent dimensions.
The possible annotation values were set to be chosen during the annotation procedure in the same way as in Doccano 1.
</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Generalized datasets</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS6.5.1.1">IV-B</span>6 </span>ColBERT</h4>
<div class="ltx_para" id="S4.SS2.SSS6.p1">
<p class="ltx_p" id="S4.SS2.SSS6.p1.1">a large dataset for the task of humor detection. Several existing humor detection datasets combined some non-humorous formal texts with short informal humorous texts.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib30" title="">30</a>]</cite> This dataset contains 200,000 short texts (100,000 positive and 100,000 negative).
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS7.5.1.1">IV-B</span>7 </span>HahaIberlef2021</h4>
<div class="ltx_para" id="S4.SS2.SSS7.p1">
<p class="ltx_p" id="S4.SS2.SSS7.p1.1">is a corpus of crowd-annotated tweets separated into three subsets: training (24,000 tweets), development (6,000 tweets), and testing (6,000 tweets) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib31" title="">31</a>]</cite>. The annotation uses a voting scheme in which users could select one of six options <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib32" title="">32</a>]</cite>: the tweet is not humorous, or the tweet is humorous, and a score is given between one (not funny) and five (excellent).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS8">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS8.5.1.1">IV-B</span>8 </span>Hahackathon</h4>
<div class="ltx_para" id="S4.SS2.SSS8.p1">
<p class="ltx_p" id="S4.SS2.SSS8.p1.1">a dataset that contains 10,000 humor and offense annotated texts by 20 annotators aged 18 to 70 years. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib33" title="">33</a>]</cite> It had 80% of its data sourced from Twitter. The remaining 20% of the texts were selected from the Kaggle Short Jokes dataset.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS9">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS9.5.1.1">IV-B</span>9 </span>Sarcasmania</h4>
<div class="ltx_para" id="S4.SS2.SSS9.p1">
<p class="ltx_p" id="S4.SS2.SSS9.p1.1">is a Kaggle-sourced dataset that consists of 39,780 texts from the Twitter platform. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib18" title="">18</a>]</cite> Each text is annotated for three dimensions: ‚Äùsarcasm‚Äù, ‚Äùhumor‚Äù, and ‚Äùinsult‚Äù. To incorporate this dataset, we have focused purely on the humor aspect of annotations within this collection of texts.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Personalized dataset profiles. Each dataset contains a set number of labels. Details and further information about them can be found in Section <a class="ltx_ref" href="#S4" title="IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">IV</span></a>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td ltx_nopad ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1"><svg height="21.61" overflow="visible" version="1.1" width="207.56"><g transform="translate(0,21.61) scale(1,-1)"><path d="M 0,21.61 207.56,0" stroke="black" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,12.15) scale(1, -1)"><foreignobject height="12.15" overflow="visible" width="53.7">
<span class="ltx_inline-block" id="S4.T1.1.1.1.pic1.1.1">
<span class="ltx_inline-block ltx_align_left" id="S4.T1.1.1.1.pic1.1.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.pic1.1.1.1.1">Property</span>
</span>
</span></foreignobject></g></g><g class="ltx_svg_fog" transform="translate(160.78,12.15)"><g transform="translate(0,9.46) scale(1, -1)"><foreignobject height="9.46" overflow="visible" width="46.78">
<span class="ltx_inline-block" id="S4.T1.1.1.1.pic1.2.1">
<span class="ltx_inline-block ltx_align_right" id="S4.T1.1.1.1.pic1.2.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.pic1.2.1.1.1">Dataset</span>
</span>
</span></foreignobject></g></g></g></svg></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2">
<span class="ltx_text" id="S4.T1.1.1.2.1"></span> <span class="ltx_text" id="S4.T1.1.1.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.2.2.1">
<span class="ltx_tr" id="S4.T1.1.1.2.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.2.2.1.1.1">Cockamamie</span></span>
<span class="ltx_tr" id="S4.T1.1.1.2.2.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.2.2.1.2.1">Gobbledegook</span></span>
</span></span> <span class="ltx_text" id="S4.T1.1.1.2.3"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.3">Humor</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.4">Humicroedit</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.5">Doccano 1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.6">Doccano 2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.1">Textual content profile</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2">1-2 words</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.3">tweets</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.4">paired news &amp; headlines</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.5">comments</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.6">comments</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.3.1">No. of texts</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.2">10,884</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3">8,284</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.4">14,886</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.5">880</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.6">8,891</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.4.1">No. of annotations</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.2">40,673</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.3">26,967</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.4">74,430</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.5">31,521</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.6">17,533</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.5.1">Number of annotators</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.5.2">351</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.5.3">3,137</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.5.4">5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.5.5">39</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.5.6">49</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.6.1">Avg. annotations per text</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.6.2">3.74</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.6.3">3.26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.6.4">5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.6.5">35.81</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.6.6">1.97</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.7.1">Avg. annotations per annotator</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.7.2">115.88</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.7.3">57.44</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.7.4">14,886</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.7.5">808.23</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.7.6">357.81</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.8.1">Class balance (0/1)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.8.2">382,083 / 51,197</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.8.3">12,091 / 9,978</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.8.4">31,863 / 42,572</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.8.5">26,706 / 4,815</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.8.6">7,661 / 9,872</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.9.1">Language</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.9.2">English</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.9.3">Spanish</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.9.4">English</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.9.5">Polish</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.9.6">Polish</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Generalized dataset profiles. Each dataset contains a set number of labels. The number of annotations is equal to the number of tests because of the lack of any user distinction. Details and further information about them can be found in Section <a class="ltx_ref" href="#S4" title="IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">IV</span></a>.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.1">
<tr class="ltx_tr" id="S4.T2.1.1">
<td class="ltx_td ltx_nopad ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.1.1"><svg height="21.61" overflow="visible" version="1.1" width="207.56"><g transform="translate(0,21.61) scale(1,-1)"><path d="M 0,21.61 207.56,0" stroke="black" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,12.15) scale(1, -1)"><foreignobject height="12.15" overflow="visible" width="53.7">
<span class="ltx_inline-block" id="S4.T2.1.1.1.pic1.1.1">
<span class="ltx_inline-block ltx_align_left" id="S4.T2.1.1.1.pic1.1.1.1">
<span class="ltx_p" id="S4.T2.1.1.1.pic1.1.1.1.1">Property</span>
</span>
</span></foreignobject></g></g><g class="ltx_svg_fog" transform="translate(160.78,12.15)"><g transform="translate(0,9.46) scale(1, -1)"><foreignobject height="9.46" overflow="visible" width="46.78">
<span class="ltx_inline-block" id="S4.T2.1.1.1.pic1.2.1">
<span class="ltx_inline-block ltx_align_right" id="S4.T2.1.1.1.pic1.2.1.1">
<span class="ltx_p" id="S4.T2.1.1.1.pic1.2.1.1.1">Dataset</span>
</span>
</span></foreignobject></g></g></g></svg></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.2">ColBERT</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.3">HahaIberlef2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4">Hahackathon</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5">Sarcasmania</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.2.1">Textual content profile</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.2">short texts</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.3">tweets</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.4">tweets &amp; jokes</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.5">tweets</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.3.1">No. of texts</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2">199,996</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.3">33,824</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.4">7,958</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.5">39,778</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.4.1">Class balance (0/1)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.2">100,000 / 99,996</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.3">20,778 / 13,046</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.4">3,068 / 4,890</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.5">20,135 / 19,643</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.5.1">Language</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.5.2">English</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.5.3">Spanish</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.5.4">English</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.5.5">English</td>
</tr>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Models</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In the personalized approach, it is assumed that the funniness level of a text is determined by the users and their individual sense of humor, rather than just the text itself and the average or combined annotations. This premise is based on the user‚Äôs individual preferences, which should be utilized in order to receive a distinct output for each user, based on their individual preferences as presented in Fig.¬†<a class="ltx_ref" href="#S3.F1" title="Figure 1 ‚Ä£ III-A Personalized Humor Detection ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">1</span></a>. For this purpose, we need the information about user preferences extracted from their past annotations, and the models in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> are doing exactly that, which is why we use them in our research. As an addition to the deep learning models, we also leveraged a slightly more complex model, the UserId architecture. It was sourced from the work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite></p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">TXT-Baseline</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">This deep neural architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> has an input of only a text embedding.It consists of the layers of the deep language model and the layers associated with the learning task. It is characterized by representing a generalized method with a unified output for all users, widely used in NLP. The main goal of exploiting this model is to accurately measure and compare differences between generalized and personalized approaches.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">OneHot</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Consisting of a most simple and yet very effective method for the representation of annotators, this model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> utilizes information about the user ID to incorporate it into a one-hot vector. A simple variant of personalized architectures.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">SHEEP-Formula</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">This architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> is characterized by the use of a metric called Human Sense of Humor, i.e. a <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">HSH(u)</span>. This value is sent directly to the model input, and a single estimate is calculated for each user individually and has its basis on the Z-score.
When the SHEEP-Formula clashes with a random language model, the learning and reasoning processes are based solely on the information about the user.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">SHEEP-Simple</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">SHEEP-Simple <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> is a straightforward personalized model, which builds upon TXT-Baseline‚Äôs textual prediction by incorporating the individual‚Äôs preferences and the words used in the text. Biases are incorporated into the final prediction, which can be interpreted as adjusting the inference for a particular individual by taking into account their average annotation in the dataset. The biases are set randomly at the start and then learned through backpropagation. This model focuses on neither the relation between a user nor the text, but only the statistics of human annotations.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS5.5.1.1">V-E</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS5.6.2">SHEEP-Medium</span>
</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">This model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> trains a multidimensional latent vector of an annotator to grasp the knowledge of their sense of humor.
To address the problem of performance in personalization during the processing of unknown texts during model training, SHEEP-Medium utilizes a vector representation of a human. This allows for capturing their approach to a specific text in the context of their sense of humor.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS6.5.1.1">V-F</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS6.6.2">UserId</span>
</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">The model architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite> is based on appending the user ID token to the start of the annotated text in order to store information about a person. Subsequently, the text containing the user ID is transformed into a vector representation using a transformer model. To prevent the tokenizer from splitting the user ID tokens, the original model was extended by manually adding them to the special tokens set of the model. The transformer weights were trained with the entire model to learn the dependencies between the user and the text.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Experimental setup</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">To evaluate our human-centered data fusion techniques we leveraged several experimental configurations: (1) training the model on the samples from a single dataset aggregated via majority voting (<span class="ltx_text ltx_font_bold" id="S6.p1.1.1">Majority single</span>), (2) combining samples from multiple personalized datasets aggregated via majority voting in the train set (the first column in Fig.¬†<a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ III-B Human-Centered Data Fusion ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">2</span></a>, <span class="ltx_text ltx_font_bold" id="S6.p1.1.2">Majority multi</span>), (3) fusing samples from multiple personalized datasets and generalized datasets during model training (the middle column in Fig.¬†<a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ III-B Human-Centered Data Fusion ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">2</span></a>, <span class="ltx_text ltx_font_bold" id="S6.p1.1.3">Majority + Generalized multi</span>), (4) training the model on individual user annotations from a single personalized dataset (<span class="ltx_text ltx_font_bold" id="S6.p1.1.4">Personalized single</span>), (5) combining individual user annotations from all personalized datasets in the train set (the last, third column in Fig.¬†<a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ III-B Human-Centered Data Fusion ‚Ä£ III Data Fusion in Subjective Humor Detection ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">2</span></a>, <span class="ltx_text ltx_font_bold" id="S6.p1.1.5">Personalized multi</span>).
</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">To enable the use of our data fusion techniques for any of the selected datasets regardless of their diverse label ranges, we mapped any value different than zero to class 1 (<span class="ltx_text ltx_font_italic" id="S6.p2.1.1">funny</span>). In this way, we preserved the initial information about the funniness of the text, which was also the source of knowledge about the user perspective in the case of personalized datasets presented in Sec.¬†<a class="ltx_ref" href="#S4.SS1" title="IV-A Personalized datasets ‚Ä£ IV Datasets ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">We addressed any potential disparity and data leak between text samples by utilizing the text-based data division described in Fig.¬†<a class="ltx_ref" href="#S6.F3" title="Figure 3 ‚Ä£ VI Experimental setup ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">3</span></a>. In this setup, the model is validated and tested only on texts that were not present in the train set.</p>
</div>
<figure class="ltx_figure" id="S6.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="770" id="S6.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Data split strategy used for each dataset. White blocks are texts, which are not annoated by a specific user</figcaption>
</figure>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">To measure the stability of the results, we used the 10-fold cross-validation. During each iteration, the model was trained on 8 folds, while 9<sup class="ltx_sup" id="S6.p4.1.1">th</sup> and 10<sup class="ltx_sup" id="S6.p4.1.2">th</sup> fold were used as validation and test sets, respectively. In the next step, we calculated the mean and standard deviation of the selected evaluation metrics. Furthermore, we measured the statistical significance of the obtained results via statistical tests. For data that satisfied the test assumptions, we used the <span class="ltx_text ltx_font_italic" id="S6.p4.1.3">t</span>-test for independent samples <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib35" title="">35</a>]</cite> with Bonferroni correction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib36" title="">36</a>]</cite>. Otherwise, we leveraged the Mann-Whitney <span class="ltx_text ltx_font_italic" id="S6.p4.1.4">U</span>-test <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib37" title="">37</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">For evaluation, we used the macro F1 score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib38" title="">38</a>]</cite> and the <span class="ltx_text ltx_font_italic" id="S6.p5.1.1">Gain</span> measure, which is equal to the difference between the macro F1 score achieved by the model trained on the dataset obtained via data fusion and the macro F1 score of the model trained on the original dataset.</p>
</div>
<div class="ltx_para" id="S6.p6">
<p class="ltx_p" id="S6.p6.1">Due to the multilingual character of the datasets included in the experiments, we obtained the text representations through the LaBSE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib39" title="">39</a>]</cite> model available in the HuggingFace <span class="ltx_text ltx_font_italic" id="S6.p6.1.1">transformers</span> library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib40" title="">40</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Results</span>
</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS1.5.1.1">VII-A</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS1.6.2">Evaluation of Data Fusion Techniques</span>
</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">To evaluate the impact of our human-centered data fusion techniques, we carried out experiments in configurations described in Sec.¬†<a class="ltx_ref" href="#S6" title="VI Experimental setup ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">VI</span></a>. The results for the Cockamamie Gobbledegook dataset are presented in Tab.¬†<a class="ltx_ref" href="#S7.F4" title="Figure 4 ‚Ä£ VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">4</span></a>. The best results were obtained using the UserId model. Almost every personalized architecture achieved the best results in Personalized multi configuration. The slightly lower results for the SHEEP-Formula model in this configuration in comparison to training on the original dataset may be related to the nature of this dataset containing texts no longer than 2 words. In addition, the words are very uncommon, making it even more difficult to reliably assess the uniqueness of the user‚Äôs perception against others.</p>
</div>
<figure class="ltx_figure" id="S7.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="650" id="S7.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Macro F1 score values for all five experimental configurations tested on the personalized Cockamamie Gobbledegook dataset. The bar whiskers outline the standard deviation of the results measured during the cross-validation.</figcaption>
</figure>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">The evaluation results for the Humor dataset are presented in Fig.¬†<a class="ltx_ref" href="#S7.F5" title="Figure 5 ‚Ä£ VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">5</span></a>. The use of human-centered data fusion (Personalized multi) resulted in an improvement of up to 8.72 in macro F1 score compared to training on the original dataset (Personalized single). The best results were achieved by the SHEEP-Medium, UserId, and SHEEP-Simple models.</p>
</div>
<figure class="ltx_figure" id="S7.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="650" id="S7.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Macro F1 score values for all five experimental configurations tested on the personalized Humor dataset. The bar whiskers outline the standard deviation of the results measured during the cross-validation.</figcaption>
</figure>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1">The results of the conducted on the Humicroedit dataset are shown in Fig.¬†<a class="ltx_ref" href="#S7.F6" title="Figure 6 ‚Ä£ VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">6</span></a>. Every tested architecture achieved the best results by training on the combined personalized datasets with individual user annotations (Personalized multi). The highest difference in macro F1 score compared to training on the original dataset (Personalized single) is 34.19 for SHEEP-Medium. The significantly higher standard deviation of the results of this model observed in the personalized single configuration may be due to the fact that there are only five users in the entire dataset. Having nearly 15,000 annotations for each user makes it difficult for this model to generate a vector representation that would highlight the differences between the preferences of only five users for such a large number of annotations per user. Moreover, the stabilization of this standard deviation in the Personalized multi configuration is an additional benefit of applying our personalized data fusion.</p>
</div>
<figure class="ltx_figure" id="S7.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="650" id="S7.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Macro F1 score values for all five experimental configurations tested on the personalized Humicroedit dataset. The bar whiskers outline the standard deviation of the results measured during the cross-validation.</figcaption>
</figure>
<div class="ltx_para" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.1">Fig.¬†<a class="ltx_ref" href="#S7.F7" title="Figure 7 ‚Ä£ VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">7</span></a> shows the results on the Doccano 1 dataset. The best results were obtained in Personalized multi configuration for all personalized models. The use of our personalized data fusion technique allowed for achieving up to 21.50 improvement in macro F1 score compared to training on the original dataset (Personalized single). Furthermore, the use of our data fusion technique based on the personalized datasets aggregated by majority voting (Majority multi) resulted in significant performance improvements in comparison to training on the original dataset (Personalized single) for every model except UserId. This is caused by the use of a special user token in this architecture. While this mechanism helps the model to better obtain the user representation, in majority voting-based configurations, the same user token added at the start of a text sample causes only unnecessary noise that can obscure more important text features from the model or steer the learning process in the wrong direction.</p>
</div>
<figure class="ltx_figure" id="S7.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="650" id="S7.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Macro F1 score values for all five experimental configurations tested on the personalized Docano 1 dataset. The bar whiskers outline the standard deviation of the results measured during the cross-validation.</figcaption>
</figure>
<div class="ltx_para" id="S7.SS1.p5">
<p class="ltx_p" id="S7.SS1.p5.1">The results for the Doccano 2 dataset are shown in Fig.¬†<a class="ltx_ref" href="#S7.F8" title="Figure 8 ‚Ä£ VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">8</span></a>. The use of human-centered data fusion techniques (Personalized multi) resulted in achieving significant improvement in performance for all personalized architectures. The highest increase of 28.53 in macro F1 score was observed compared to training on the original data (Personalized single) for the SHEEP-Medium model. Moreover, every architecture (including the non-personalized TXT-Baseline model) achieved significantly better results by training on the combined personalized datasets aggregated via majority voting (Majority multi) in comparison to the results of a model trained on the original dataset (Personalized single). The lower performance of the TXT-Baseline in the Personalized multi configuration is related to the lack of leveraging any user information in this model. By ignoring any information about the user, the model sees the larger amount of diverse annotations as unnecessary noise, concealing the important patterns in the data. On the other hand, training the TXT-Baseline model on combined personalized datasets aggregated by majority voting (Majority multi) distilled the knowledge extractable by the model from the dataset and thus highlighted the most important patterns in the data.</p>
</div>
<figure class="ltx_figure" id="S7.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="650" id="S7.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Macro F1 score values for all five experimental configurations tested on the personalized Doccano 2 dataset. The bar whiskers outline the standard deviation of the results measured during the cross-validation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS2.5.1.1">VII-B</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS2.6.2">Knowledge Transfer in Majority Voting and Personalization</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">The evaluation results of models trained on the combined personalized datasets containing samples aggregated by majority voting (Majority multi) are presented in Fig.¬†<a class="ltx_ref" href="#S7.F9" title="Figure 9 ‚Ä£ VII-B Knowledge Transfer in Majority Voting and Personalization ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">9</span></a>. Similar results obtained by the TXT-Baseline model and all personalized architectures are caused by the lack of user information in this setup. On the other hand, the gains obtained via leveraging the majority voting-based variant of our data fusion technique are presented in Fig.¬†<a class="ltx_ref" href="#S7.F10" title="Figure 10 ‚Ä£ VII-B Knowledge Transfer in Majority Voting and Personalization ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">10</span></a>. The values are equal to the difference between the macro F1 score obtained by the model trained on the combined personalized datasets aggregated by majority voting (Majority multi) and the model trained on the original data (Personalized single). The highest gains were observed for the Doccano 2 dataset. This may be related to the small size of this dataset, which, combined with very diverse assessments of a large number of users, may significantly hinder the model extraction of the knowledge needed in the humor recognition task from the representation of the text annotated in a very ambiguous way. The negative gains observed for the Humicroedit dataset in the case of the UserId model may be related to a much different profile of other datasets. This dataset is characterized by a large number of annotations and a small number of users, which lowers the variety of evaluations of individual texts. Therefore, the inter-annotator agreement is significantly higher than in the case of other personalized datasets. This results in a situation where adding information about texts annotated more ambiguously by a larger number of users may not change the general patterns helpful for this particular dataset, but only add unnecessary noise during the training procedure. The lower complexity of the SHEEP-Simple and SHEEP-Medium models makes them more resistant to this phenomenon. The lack of a proper increase for the Cockamamie Gobbledegook dataset may be related to the very low length of each sample in this dataset and the occurrence of very unique words, which may not have proper vector representations. The Humor dataset is the only personalized dataset containing texts in Spanish, which may reduce the gain from annotations regarding texts in different languages.</p>
</div>
<figure class="ltx_figure" id="S7.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="650" id="S7.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Macro F1 score values for all personalized datasets achieved by models trained on combined personalized datasets aggregated via majority voting (Majority multi). The bar whiskers outline the standard deviation of the results measured during the cross-validation.</figcaption>
</figure>
<figure class="ltx_figure" id="S7.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="650" id="S7.F10.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Difference between macro F1 score values obtained by models trained on combined personalized datasets aggregated via majority voting (Majority multi) and the models trained on a single personalized dataset aggregated via majority voting. The bar whiskers outline the standard deviation of the results measured during the cross-validation.</figcaption>
</figure>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">The results of experiments involving models trained on fused personalized datasets with individual user annotations (Personalized multi) are presented in Fig.¬†<a class="ltx_ref" href="#S7.F11" title="Figure 11 ‚Ä£ VII-B Knowledge Transfer in Majority Voting and Personalization ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">11</span></a>. The best results were achieved by the SHEEP-Formula, SHEEP-Medium, and UserId models. The gains obtained by the use of the proposed human-centered data fusion are shown in Fig.¬†<a class="ltx_ref" href="#S7.F12" title="Figure 12 ‚Ä£ VII-B Knowledge Transfer in Majority Voting and Personalization ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">12</span></a>. The values are equal to the difference in macro F1 score between the model trained on the combined personalized datasets with individual user annotations (Personalized multi) and the model trained on the original dataset (Personalized single). The highest gains are observed for the Humicroedit, Doccano 2, and Doccano 1 datasets. The first one originally lacked useful information to distinguish the user from the others. With the additional data from different datasets included during the training procedure, the model improved its ability to leverage information about the user and how to better focus on the differences between users. In the case of the Doccano 1 and Doccano 2 datasets, additional fused data contained not only information about other users but also allowed the model to extract general knowledge about the relation between the textual content and its funniness. The negative gain value for the Doccano 2 dataset in the case of the TXT-Baseline model may be related to the extension of the train set with texts in languages other than Polish. Although the language model used is language-agnostic, there is still a significant change in the distribution of embeddings in the train set, which could negatively affect the performance of the model, which does not use any information about the user. On the other hand, the lack of significant gains for the Cockamamie Gobbledegook dataset may be related to the nature of the samples present in this dataset. It contains texts no longer than two words and some of them are word-formations. It also interferes with the model‚Äôs ability to generalize patterns extracted from other datasets to such specific samples.
</p>
</div>
<figure class="ltx_figure" id="S7.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="650" id="S7.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Macro F1 score values for all personalized datasets achieved by models trained on combined personalized datasets containing individual user annotations (Personalized multi). The bar whiskers outline the standard deviation of the results measured during the cross-validation.</figcaption>
</figure>
<figure class="ltx_figure" id="S7.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="650" id="S7.F12.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Difference between macro F1 score values obtained by models trained on combined personalized datasets with individual user annotations (Personalized multi) and the models trained on a single personalized dataset containing individual user annotations. The bar whiskers outline the standard deviation of the results measured during the cross-validation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS3.5.1.1">VII-C</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS3.6.2">Impact of Data Fusion on Personalized Architectures</span>
</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">When comparing the effectiveness of our human-centered data fusion techniques in TXT-Baseline and the personalized architectures presented in Fig.¬†<a class="ltx_ref" href="#S7.F13" title="Figure 13 ‚Ä£ VII-C Impact of Data Fusion on Personalized Architectures ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">13</span></a>, it is prominent that each individual case favors training on the combined personalized datasets (Personalized multi) over training on the single personalized datasets (Personalized single). This outlines the versatile nature of the proposed technique and the possibility of applying it regardless of the chosen model architecture. The influence of combining many different personalized datasets in this matter indicates that due to the subjective nature of humor perception, better results may be obtained by increasing the variety of user perspectives seen by the model during the training procedure. A higher diversity of annotations can also improve the quality of the patterns extracted by the model from the data, increasing their consistency with the real characteristics of the phenomenon, as indicated by the improvement of the TXT-Baseline scores.
</p>
</div>
<figure class="ltx_figure" id="S7.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="650" id="S7.F13.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Average macro F1 scores achieved by the personalized architectures trained on single personalized datasets with individual user annotations compared with the performance of models trained on combined personalized datasets containing individual user annotaions (Personalized multi).</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Experimental studies in nine datasets, of which the majority is personalized, revealed that the concatenation of all subjective data massively increases the reasoning quality of a model, which is especially prominent in Fig.¬†<a class="ltx_ref" href="#S7.F6" title="Figure 6 ‚Ä£ VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">6</span></a> and Fig.¬†<a class="ltx_ref" href="#S7.F8" title="Figure 8 ‚Ä£ VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">The smaller gains observed in the Cockamamie Gobbledegook dataset in Fig.¬†<a class="ltx_ref" href="#S7.F4" title="Figure 4 ‚Ä£ VII-A Evaluation of Data Fusion Techniques ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">4</span></a> are the result of the word-formation characteristics of this data collection, such as where headlines (Humicroedit) and tweets (Humor) contain a more standard form in which humor is contained.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">Another crucial aspect of the results is the fact that the language of the dataset also affected the level of increase in reasoning performance. The general results of the experiments on the datasets are also linked to the language of its content. There is no doubt that certain features related to English, Spanish, and Polish are involved in the research, although a language-agnostic model is used to generate text representations. As seen in Fig.¬†<a class="ltx_ref" href="#S7.F12" title="Figure 12 ‚Ä£ VII-B Knowledge Transfer in Majority Voting and Personalization ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">12</span></a>, the model performance on the Spanish dataset Humor is noticeably worse than on the English and Polish data, as it may be the result of concatenating multiple different languages with vastly different characteristics.</p>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.1">Through our research, each of our experimental results shows that no matter how much we increase the quantity of a generalized dataset in the Generalized + Majority scenario, it will never exceed the boost guaranteed by the incorporation of combined personalized datasets in the Personalized multi scenario, as seen in Fig.¬†<a class="ltx_ref" href="#S7.F13" title="Figure 13 ‚Ä£ VII-C Impact of Data Fusion on Personalized Architectures ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">13</span></a>. This fact strongly highlights the subjective nature of the humor recognition task in NLP. The message indicated by the results of our experiments implies that knowledge about the user‚Äôs beliefs, feelings, and experiences is much more crucial than information regarding the text itself. The incorporation of generalized data sets did not improve the quality of reasoning, regardless of the selected model. This could be caused by the fact that the annotation process in the case of generalized datasets was focused on maximizing the inter-annotator agreement. As a result, this approach discards the information about the funniness of the text that includes different perspectives of the anotators. It is evident that if an annotation process is to be performed, it should take into account the individual perspective of the user.</p>
</div>
<div class="ltx_para" id="S8.p5">
<p class="ltx_p" id="S8.p5.1">As for our majority-based data fusion method (Majority multi) based on the majority voting in personalized datasets, it can also be used to improve the performance of the non-personalized models as can be observed for the TXT-Baseline model in Fig.¬†<a class="ltx_ref" href="#S7.F13" title="Figure 13 ‚Ä£ VII-C Impact of Data Fusion on Personalized Architectures ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<div class="ltx_para" id="S8.p6">
<p class="ltx_p" id="S8.p6.1">On the other hand, similar results of TXT-Baseline and personalized architectures trained on datasets aggregated by majority voting (Majority multi) seen in Fig.¬†<a class="ltx_ref" href="#S7.F9" title="Figure 9 ‚Ä£ VII-B Knowledge Transfer in Majority Voting and Personalization ‚Ä£ VII Results ‚Ä£ From Generalized Laughter to Personalized Chuckles: Unleashing the Power of Data Fusion in Subjective Humor Detection"><span class="ltx_text ltx_ref_tag">9</span></a> indicate the universal nature of the personalized architectures used in the experiments and the possibility of their application to non-personalized datasets. This could allow for a synthesis of knowledge from the humor domain from multiple sources, which enhances the overall versatility of the model performance.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span class="ltx_text ltx_font_smallcaps" id="S9.1.1">Conclusions and Future work</span>
</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">Our research experiments allowed us to recognize the importance of human-centered data fusion for natural language processing reasoning, especially in terms of work in the area of subjective NLP. Contrary to generalized NLP problems, here it is only the user who decides what is funny or not, and this fundamental fact is the key factor in unlocking the potential of personalized reasoning. We have proven that even if the amount of data in generalized scenarios is much greater, the aspect of user preference appears to be the best out of all five scenarios considered.</p>
</div>
<div class="ltx_para" id="S9.p2">
<p class="ltx_p" id="S9.p2.1">We have demonstrated that incorporating knowledge from other datasets can be beneficial for the model by improving its general understanding of the task of humor detection. The fusion of data regarding an individual user‚Äôs sense of humor from multiple sources has an effect on the model performance on other personalized datasets in both personalized and majority-voting-based situations. The incorporation of generalized datasets improved the effectiveness of humor prediction for personalized datasets in a majority voting context, and knowledge transfer between personalized datasets has a similar effect on model performance as knowledge transfer between datasets with majority voting.</p>
</div>
<div class="ltx_para" id="S9.p3">
<p class="ltx_p" id="S9.p3.1">The results of data fusion were found to depend on the language or field of the datasets, and it was concluded that the use of human-centered data fusion techniques improves the performance of a variety of personalized structures in a similar way.</p>
</div>
<div class="ltx_para" id="S9.p4">
<p class="ltx_p" id="S9.p4.1">We are confident that our method of data fusion is capable of accurately representing variety of user beliefs and this could be the way forward for subjective NLP tasks, such as humor detection and any other. Therefore, we plan to adapt our human-centered data fusion techniques to other subjective NLP tasks. In our other future research, we want to exploit categories of humor in a completely personalized dataset that could combine the humor perception of each user. Every individual may have a distinct sense of humor, so it is essential to recognize any potential feature where the perception and humor standard may vary. Such identification would enable us to compare people more precisely, thus uncovering the element of a potential individualized rating system.
</p>
</div>
<div class="ltx_para" id="S9.p5">
<p class="ltx_p" id="S9.p5.1">The source code used during the experiments is publicly available <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/CLARIN-PL/personalized-nlp/releases/tag/2023-icdm-sentire-humor" title="">https://github.com/CLARIN-PL/personalized-nlp/releases/tag/2023-icdm-sentire-humor</a></span></span></span>.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was financed by
(1) the National Science Centre, Poland, project no. 2021/41/B/ST6/04471;
(2) Contribution to the European Research Infrastructure ‚ÄôCLARIN ERIC - European Research Infrastructure Consortium: Common Language Resources and Technology Infrastructure‚Äô, 2022-23 (CLARIN Q);
(3) the Polish Ministry of Education and Science, CLARIN-PL;
(4) the European Regional Development Fund as a part of the 2014-2020 Smart Growth Operational Programme, projects no. POIR.04.02.00-00C002/19, POIR.01.01.01-00-0288/22 and POIR.01.01.01-00-0923/20;
(5) the statutory funds of the Department of Artificial Intelligence, Wroclaw University of Science and Technology;
(6) the Polish Ministry of Education and Science within the programme ‚ÄúInternational Projects Co-Funded‚Äù;
(7) the European Union under the Horizon Europe, grant no. 101086321 (OMINO). However, the views and opinions expressed are those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Executive Agency. Neither the European Union nor European Research Executive Agency can be held responsible for them.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K.¬†Kanclerz <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">et¬†al.</em>, ‚ÄúCross-lingual deep neural transfer learning in
sentiment analysis,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2">Procedia Computer Science</em>, vol. 176, pp.
128‚Äì137, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ‚ÄúControversy and conformity: from generalized to personalized
aggressiveness detection,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 59th Annual Meeting
of the Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>,
2021, pp. 5915‚Äì5926.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J.¬†Hofmann <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">et¬†al.</em>, ‚ÄúGender differences in humor-related traits, humor
appreciation, production, comprehension,(neural) responses, use, and
correlates: A systematic review,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2">Current Psychology</em>, pp. 1‚Äì14,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J.¬†Koco≈Ñ <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">et¬†al.</em>, ‚ÄúLearning personal human biases and
representations for subjective tasks in natural language processing,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">2021 IEEE International Conference on Data Mining (ICDM)</em>.¬†¬†¬†IEEE, 2021, pp. 1168‚Äì1173.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ‚ÄúChatgpt: Jack of all trades, master of none,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Information
Fusion</em>, p. 101861, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W.¬†Mieleszczenko-Kowszewicz <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">et¬†al.</em>, ‚ÄúCapturing human perspectives in
nlp: Questionnaires, annotations, and biases,‚Äù 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P.¬†Kazienko, J.¬†Bielaniewicz, M.¬†Gruza, K.¬†Kanclerz, K.¬†Karanowski,
P.¬†Mi≈Çkowski, and J.¬†Koco≈Ñ, ‚ÄúHuman-centered neural reasoning for
subjective content processing: Hate speech, emotions, and humor,‚Äù
<em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Information Fusion</em>, vol.¬†94, pp. 43‚Äì65, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
G.¬†D. Wilson and J.¬†R. Patterson, ‚ÄúConservatism as a predictor of humor
preferences.‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Journal of consulting and clinical Psychology</em>, p. 271,
1969.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
B.¬†W. others, ‚ÄúAppropriate and inappropriate uses of humor by teachers,‚Äù
<em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Communication education</em>, pp. 178‚Äì196, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J.¬†Hay, ‚ÄúGender and humour: Beyond a joke,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Unpublished Master‚Äôs
thesis, Victoria University of Wellington, Wellington, New Zealand</em>, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
C.¬†Strapparava <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">et¬†al.</em>, ‚ÄúComputational humour,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2">Emotion-Oriented
Systems: The Humaine Handbook</em>, pp. 609‚Äì634, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y.¬†Raz, ‚ÄúAutomatic humor classification on twitter,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of
the NAACL HLT 2012 student research workshop</em>, 2012, pp. 66‚Äì70.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A.¬†Khandelwal <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">et¬†al.</em>, ‚ÄúHumor detection in english-hindi code-mixed
social media content: Corpus and baseline system,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2">arXiv preprint
arXiv:1806.05513</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M.¬†Dynel, ‚ÄúBeyond a joke: Types of conversational humour,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Language and
linguistics compass</em>, vol.¬†3, no.¬†5, pp. 1284‚Äì1299, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
C.¬†E. Davies, ‚ÄúSociolinguistic approaches to humor,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">The Routledge
handbook of language and humor</em>, vol.¬†1, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S.¬†Agrawal, ‚ÄúJoke recommender system using humor theory,‚Äù Ph.D. dissertation,
Purdue University, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J.¬†Meghana and R.¬†Vijaya, ‚ÄúHumour and gender stereotypes.‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">IASSI
Quarterly</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
R.¬†Siddiqui, ‚ÄúSarcasmania: Sarcasm exposed,‚Äù 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J.¬†Bleiholder and F.¬†Naumann, ‚ÄúData fusion,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ACM computing surveys
(CSUR)</em>, pp. 1‚Äì41, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H.¬†Xu <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">et¬†al.</em>, ‚ÄúHybrid multimodal fusion for humor detection,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">Proceedings of the 3rd International on Multimodal Sentiment Analysis
Workshop and Challenge</em>, 2022, pp. 15‚Äì21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
L.¬†Christ <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et¬†al.</em>, ‚ÄúMultimodal prediction of spontaneous humour: A novel
dataset and first results,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2">arXiv preprint arXiv:2209.14272</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ‚ÄúThe muse 2023 multimodal sentiment analysis challenge: Mimicked
emotions, cross-cultural humour, and personalisation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint
arXiv:2305.03369</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
D.¬†Yang <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">et¬†al.</em>, ‚ÄúHumor recognition and humor anchor extraction,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2">Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing</em>.¬†¬†¬†Lisbon,
Portugal: Association for Computational Linguistics, Sep. 2015, pp.
2367‚Äì2376.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S.¬†Castro <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">et¬†al.</em>, ‚ÄúA crowd-annotated spanish corpus for humor
analysis,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2">Proc. of SocialNLP 2018</em>.¬†¬†¬†ACL, 2018, pp. 7‚Äì11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
N.¬†Hossain <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">et¬†al.</em>, ‚Äú‚Äúpresident vows to cut
&lt;taxes&gt; hair‚Äù: Dataset and analysis of creative
text editing for humorous headlines,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib25.2.2">Proc. of the NAACL: Human
Language Technologies</em>.¬†¬†¬†ACL, 2019, pp.
133‚Äì142.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ‚ÄúSemEval-2020 task 7: Assessing humor in edited news headlines,‚Äù
in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">SemEval2020</em>.¬†¬†¬†ICCL, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J.¬†Bielaniewicz <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">et¬†al.</em>, ‚ÄúDeep-sheep: Sense of humor extraction from
embeddings in the personalized context,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">2022 IEEE International
Conference on Data Mining Workshops (ICDMW)</em>, 2022, pp. 967‚Äì974.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A.¬†Ziv, ‚ÄúSociometry of humor: Objectifying the subjective,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Perceptual
and Motor Skills</em>, pp. 97‚Äì98, 1979.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
T.¬†Engelthaler <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">et¬†al.</em>, ‚ÄúHumor norms for 4,997 english words,‚Äù
<em class="ltx_emph ltx_font_italic" id="bib.bib29.2.2">Behavior research methods</em>, vol.¬†50, no.¬†3, pp. 1116‚Äì1124, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
I.¬†Annamoradnejad and G.¬†Zoghi, ‚ÄúColbert: Using bert sentence embedding for
humor detection,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2004.12765</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
L.¬†Chiruzzo <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">et¬†al.</em>, ‚ÄúOverview of haha at iberlef 2021: Detecting,
rating and analyzing humor in spanish,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib31.2.2">Procesamiento del Lenguaje
Natural</em>, pp. 257‚Äì268, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ‚ÄúHaha 2019 dataset: A corpus for humor analysis in spanish,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the Twelfth Language Resources and Evaluation
Conference</em>, 2020, pp. 5106‚Äì5112.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J.¬†A. Meaney <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">et¬†al.</em>, ‚ÄúSemEval 2021 task 7: HaHackathon,
detecting and rating humor and offense,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib33.2.2">Proceedings of the 15th
International Workshop on Semantic Evaluation (SemEval-2021)</em>.¬†¬†¬†Online: Association for Computational Linguistics,
Aug. 2021, pp. 105‚Äì119.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J.¬†Koco≈Ñ <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">et¬†al.</em>, ‚ÄúOffensive, aggressive, and hate speech analysis:
From data-centric to human-centered approach,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib34.2.2">Information Processing
&amp; Management</em>, vol.¬†58, no.¬†5, p. 102643, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Student, ‚ÄúThe probable error of a mean,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Biometrika</em>, vol.¬†6, no.¬†1,
pp. 1‚Äì25, 1908.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
C.¬†Bonferroni, <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Teoria statistica delle classi e calcolo delle
probabilit√†</em>, ser. Pubblicazioni del R. Istituto superiore di scienze
economiche e commerciali di Firenze.¬†¬†¬†Seeber, 1936.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
H.¬†B. Mann and D.¬†R. Whitney, ‚ÄúOn a test of whether one of two random
variables is stochastically larger than the other,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">The Annals of
Mathematical Statistics</em>, vol.¬†18, no.¬†1, pp. 50‚Äì60, 1947.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
M.¬†Sokolova <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">et¬†al.</em>, ‚ÄúBeyond accuracy, f-score and roc: a family of
discriminant measures for performance evaluation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib38.2.2">Australasian
joint conference on artificial intelligence</em>.¬†¬†¬†Springer, 2006, pp. 1015‚Äì1021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
F.¬†Feng <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">et¬†al.</em>, ‚ÄúLanguage-agnostic BERT sentence embedding,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib39.2.2">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>.¬†¬†¬†Dublin, Ireland: Association for Computational Linguistics,
May 2022, pp. 878‚Äì891.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
T.¬†Wolf <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">et¬†al.</em>, ‚ÄúTransformers: State-of-the-art natural language
processing,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib40.2.2">Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System Demonstrations</em>.¬†¬†¬†Online: Association for Computational
Linguistics, Oct. 2020, pp. 38‚Äì45.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Dec 18 15:37:11 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
