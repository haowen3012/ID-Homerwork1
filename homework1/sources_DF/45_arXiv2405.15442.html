<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Precision Healthcare: Robust Fusion of Time Series and Image Data</title>
<!--Generated on Fri May 24 13:45:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.15442v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S1" title="In Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S2" title="In Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S2.SS1" title="In II Related Work ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Multimodal Machine Learning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S2.SS2" title="In II Related Work ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Machine Learning in Healthcare</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3" title="In Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Multimodal Data Preparation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.SS1" title="In III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">MIMIC dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.SS2" title="In III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">MIMIC-CXR</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.SS3" title="In III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">MIMIC-IV</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.SS4" title="In III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Generating Multimodal Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.SS5" title="In III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Preprocessing</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S4" title="In Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Proposed Framework</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S4.SS1" title="In IV Proposed Framework ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Model Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S4.SS2" title="In IV Proposed Framework ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Attention-Based Multimodal Fusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S4.SS3" title="In IV Proposed Framework ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Uncertainty Loss</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5" title="In Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Experiments and Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.SS1" title="In V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.SS2" title="In V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Baselines</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.SS3" title="In V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">In-Hospital Mortality Prediction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.SS4" title="In V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Phenotype Classification</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.SS5" title="In V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span> </span><span class="ltx_text ltx_font_italic">Ablation Study</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.SS6" title="In V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-F</span> </span><span class="ltx_text ltx_font_italic">Task-wise Uncertainties</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.SS7" title="In V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-G</span> </span><span class="ltx_text ltx_font_italic">Robustness</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S6" title="In Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Precision Healthcare: Robust Fusion of Time Series and Image Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ali Rasekh<sup class="ltx_sup" id="id11.11.id1"><span class="ltx_text ltx_font_italic" id="id11.11.id1.1">1</span></sup>, Reza Heidari<sup class="ltx_sup" id="id12.12.id2"><span class="ltx_text ltx_font_italic" id="id12.12.id2.1">2</span></sup>, Amir Hosein Haji Mohammad Rezaie<sup class="ltx_sup" id="id13.13.id3"><span class="ltx_text ltx_font_italic" id="id13.13.id3.1">2∗</span></sup>, Parsa Sharifi Sedeh<sup class="ltx_sup" id="id14.14.id4"><span class="ltx_text ltx_font_italic" id="id14.14.id4.1">2∗</span></sup>, Zahra Ahmadi<sup class="ltx_sup" id="id15.15.id5"><span class="ltx_text ltx_font_italic" id="id15.15.id5.1">1</span></sup>, Prasenjit Mitra<sup class="ltx_sup" id="id16.16.id6"><span class="ltx_text ltx_font_italic" id="id16.16.id6.1">1</span></sup>, Wolfgang Nejdl<sup class="ltx_sup" id="id17.17.id7"><span class="ltx_text ltx_font_italic" id="id17.17.id7.1">1</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id18.18.id8"><span class="ltx_text ltx_font_italic" id="id18.18.id8.1">1</span></sup>{ali.rasekh, ahmadi, mitra, nejdl}@l3s.de
<br class="ltx_break"/><sup class="ltx_sup" id="id19.19.id9"><span class="ltx_text ltx_font_italic" id="id19.19.id9.1">2</span></sup>{r4heidari, haji80as, sharifyparsa1381}@gmail.com
<br class="ltx_break"/><sup class="ltx_sup" id="id20.20.id10"><span class="ltx_text ltx_font_italic" id="id20.20.id10.1">∗</span></sup>Amir Hosein Haji Mohammad rezaie and Parsa Sharifi Sedeh contributed equally to this work
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id21.id1">With the increasing availability of diverse data types, particularly images and time series data from medical experiments, there is a growing demand for techniques designed to combine various modalities of data effectively. Our motivation comes from the important areas of predicting mortality and phenotyping where using different modalities of data could significantly improve our ability to predict. To tackle this challenge, we introduce a new method that uses two separate encoders, one for each type of data, allowing the model to understand complex patterns in both visual and time-based information. Apart from the technical challenges, our goal is to make the predictive model more robust in noisy conditions and perform better than current methods. We also deal with imbalanced datasets and use an uncertainty loss function, yielding improved results while simultaneously providing a principled means of modeling uncertainty. Additionally, we include attention mechanisms to fuse different modalities, allowing the model to focus on what’s important for each task. We tested our approach using the comprehensive multimodal MIMIC dataset, combining MIMIC-IV and MIMIC-CXR datasets. Our experiments show that our method is effective in improving multimodal deep learning for clinical applications. The code will be made available online.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Artificial intelligence (AI) has become increasingly essential in medical fields, transforming healthcare by offering advanced capabilities in predicting mortality, identifying diseases, and conducting various diagnostic tasks. With the rise of deep learning techniques, AI has shown outstanding effectiveness and accuracy, especially in medical applications. Multimodal learning, a recent advancement, uses various data sources like electronic health records and medical images to strengthen predictive modeling and diagnostic abilities.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The integration of AI in medical practice brings several benefits. Firstly, it allows healthcare professionals to use large amounts of data to make quick and accurate decisions. For example, in predicting mortality, AI algorithms can analyze patient data such as vital signs, lab results, and medical images to identify signs of deteriorating health and take timely action. Secondly, AI supports personalized medicine by identifying patient groups with distinct characteristics, helping tailor treatment strategies. This personalized approach improves patient outcomes and reduces the risk of adverse reactions to treatments. Additionally, AI-powered diagnostic tools are highly sensitive and specific, aiding in early disease detection. Overall, integrating AI into medical practice holds great promise for improving patient care, simplifying processes, and ultimately saving lives.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, seamlessly integrating different types of data, like medical images and time series data, brings significant challenges in realizing AI’s full potential in healthcare. Combining these diverse data types requires innovative approaches to address the complexities and variations within clinical datasets. For example, when diagnosing complex conditions like sepsis, integrating data from multiple sources such as physiological measurements and imaging studies is crucial for accurate diagnosis and timely intervention. Overcoming these integration challenges is essential for unlocking the transformative power of AI in healthcare and maximizing its impact on patient outcomes.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">This paper addresses these challenges in critical healthcare tasks such as predicting mortality and phenotyping. Our main goal is to design a robust and flexible multimodal framework capable of handling the complexities of clinical datasets effectively. Ultimately, we aim to contribute to improved patient outcomes and more informed healthcare decision-making.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The rich diversity of clinical data, characterized by its multimodal nature, requires innovative approaches to extract meaningful insights. Our research focuses on achieving the following key objectives:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Enhanced Modality Fusion via Attention Mechanism:</span> We introduced an attention mechanism enabling dynamic allocation of attention across modalities, enhancing model flexibility and improving predictive accuracy. This underscores the importance of modality fusion in multimodal architectures.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Uncertainty-Aware Multi-Task Learning with Uncertainty Loss:</span> Employing an uncertainty loss function for multi-task phenotype classification, our approach prioritizes simpler and more certain tasks, enhancing overall performance by adapting to complex and uncertain ones.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Robustness in Noisy Environments:</span> Develop methods to ensure robust performance even in noisy settings commonly encountered in real-world hospital scenarios, where data may exhibit variability and imperfections.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Our research yields strong results, demonstrating the practical usefulness and resilience of our multimodal framework under challenging conditions, including noisy settings and data. We chose the MIMIC dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib2" title="">2</a>]</cite> for its diverse modalities and comprehensive nature, encompassing electronic health records, time series data, and chest X-ray images. The objectives of our study, including predicting mortality, identifying diseases, and labeling radiology images, align closely with the rich annotations and labels available within the MIMIC dataset. This consolidation facilitates a comprehensive understanding of patient health, allowing our model to learn correlations between temporal health records and visual representations. The resulting multimodal dataset is carefully preprocessed, aligning timestamps and standardizing imaging data, ensuring a coherent fusion of modalities for robust model training and evaluation.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Our thorough exploration of multimodal deep neural networks for clinical applications has revealed impactful high-level findings. The specialized encoders designed for images and time series data can successfully capture patterns within each modality, significantly enhancing the model’s discriminative power. Additionally, the integration of attention mechanisms for modality fusion empowers our model to allocate attention dynamically based on task and modality relevance. This not only improves adaptability but also enhances interpretability across predicting mortality and phenotyping labels such as chronic kidney diseases, other liver diseases, and complications of surgical procedures or medical care. Another critical aspect, particularly relevant in multi-label classification, is the consideration of uncertainty and how to model it effectively. We have shown that the uncertainty loss function not only improves performance but also provides a principled means of modeling, specifically in identifying diseases. Our approach showcases strong results, demonstrating the practical usefulness and resilience of our multimodal framework, even under challenging conditions such as noisy settings.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">In the following sections, we first review existing work in multimodal learning and machine learning methods in healthcare data (Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S2" title="II Related Work ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">II</span></a>). Then, we provide a comprehensive overview of our dataset, detailing its composition, preprocessing steps, and rationale for integrating the MIMIC-IV and MIMIC-CXR datasets, following the approach outlined in the MedFuse paper<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib3" title="">3</a>]</cite>. We detail the methodologies guiding our multimodal model training in (Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S4" title="IV Proposed Framework ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">IV</span></a>). Subsequent sections cover experimental outcomes and our findings, highlighting the robustness and superior performance of our approach compared to state-of-the-art methods (Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5" title="V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">V</span></a>), and conclude the paper by outlining future directions in multimodal deep learning for healthcare (Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S6" title="VI Conclusion ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">VI</span></a>).</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The field of multimodal learning has seen significant attention for its potential to extract richer representations from heterogeneous data. Our focus in this paper lies at the intersection of diverse endeavors seeking to capitalize on synergies among different data modalities. We explore two key domains in the following: Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S2.SS1" title="II-A Multimodal Machine Learning ‣ II Related Work ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a> explores multimodal machine learning, emphasizing approaches integrating disparate data modalities for enhanced model performance. In Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S2.SS2" title="II-B Machine Learning in Healthcare ‣ II Related Work ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>, we focus on machine learning applications in healthcare, reviewing studies that have led computational methods in improving patient outcomes.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Multimodal Machine Learning</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Researchers have been exploring various methodologies to leverage multimodal data effectively in machine learning tasks. Rahim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib4" title="">4</a>]</cite>, for instance, highlight the importance of integrating longitudinal MRI images with clinical data for disease progression prediction. Their study underscores the potential of combining imaging data with other clinical variables to achieve more accurate predictions.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In a similar work, Niu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib5" title="">5</a>]</cite> propose a joint modeling strategy that integrates time series and clinical data to enhance mortality prediction. Their approach aims to provide a more holistic understanding of the underlying patterns influencing patient outcomes. However, the image modality is not among the modalities in their dataset.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Soenksen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib6" title="">6</a>]</cite> take a step further by developing a platform capable of generating embeddings for various modalities, including images, text, and tabular data. Addressing the need for attention mechanisms in multimodal learning, Qiao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib7" title="">7</a>]</cite> introduce a multimodal attention mechanism that effectively incorporates textual information into learning scenarios. This approach demonstrates the benefits of attending to different modalities dynamically, leading to enhanced model performance.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">In the realm of anomaly detection, Hsieh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib8" title="">8</a>]</cite> propose a modular method inspired by Mask-RCNN for modeling both images and medical records in lung nodule detection. By combining information from multiple sources, their approach shows promise in improving the accuracy of anomaly detection tasks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">Addressing the issue of missing modalities in multimodal datasets, Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib9" title="">9</a>]</cite> propose a method that incorporates modality-missing aware prompts to handle missing data during training and testing. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib10" title="">10</a>]</cite> tackle the problem of missing data by leveraging auxiliary information from similar patients to impute task-related information for missing modalities in the latent space. This strategy offers a viable solution for dealing with missing data, especially when task-related information is absent.</p>
</div>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib11" title="">11</a>]</cite> introduce a knowledge distillation-based framework that leverages modality-specific information through a teacher-student approach. By distilling knowledge from a teacher model to a student model, their approach leads to improved generalization across tasks, demonstrating the effectiveness of knowledge transfer in multimodal learning.</p>
</div>
<div class="ltx_para" id="S2.SS1.p7">
<p class="ltx_p" id="S2.SS1.p7.1">Ma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib12" title="">12</a>]</cite> explore the behavior of Transformers with modal-incomplete data and propose a method for searching optimal fusion strategies via multi-task optimization. Their approach demonstrates dataset-dependent optimal fusion strategies and improved model robustness, albeit with acknowledged limitations for safety-critical systems.</p>
</div>
<div class="ltx_para" id="S2.SS1.p8">
<p class="ltx_p" id="S2.SS1.p8.1">In the domain of medical imaging, Nie et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib13" title="">13</a>]</cite> propose a deep learning model for Alzheimer’s Disease (AD) diagnosis that integrates MRI and PET data through separate CNNs. Enhanced with a novel correlation calculation method, their approach outperforms other models, showcasing promise for superior diagnostic efficiency in AD.</p>
</div>
<div class="ltx_para" id="S2.SS1.p9">
<p class="ltx_p" id="S2.SS1.p9.1">Nie et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib14" title="">14</a>]</cite> introduce a two-stage learning method for predicting overall survival (OS) time in high-grade glioma patients. By leveraging a multi-channel 3D CNN to integrate various MRI modalities followed by an SVM stage, they achieve high accuracy in predicting OS time. In addition, Srinivas and Sasibhushana Rao <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib15" title="">15</a>]</cite> present a comprehensive two-stage learning method focused on predicting overall survival (OS) time in high-grade glioma patients. This approach integrates a multi-channel 3D CNN, combining contrast-enhanced T1 MRI, diffusion tensor imaging (DTI), and resting-state functional MRI (rs-fMRI). Following this, the method assimilates features with select demographic and tumor-related features into a support vector machine (SVM). The results underscore the potential of deep learning in neuro-oncological applications for individualized treatment planning.</p>
</div>
<div class="ltx_para" id="S2.SS1.p10">
<p class="ltx_p" id="S2.SS1.p10.1">Muduli et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib16" title="">16</a>]</cite> present a deep convolutional neural network (CNN) model for automated breast cancer classification using mammograms and ultrasound images. Their streamlined architecture demonstrates effective feature extraction, yielding superior classification performance compared to existing models, highlighting the potential of automated systems for accurate breast cancer detection and early diagnosis.</p>
</div>
<div class="ltx_para" id="S2.SS1.p11">
<p class="ltx_p" id="S2.SS1.p11.1">Introducing a multimodal deep belief network (DBN), Liang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib17" title="">17</a>]</cite> showcase its effectiveness in integrative analysis of multi-platform cancer data. Their study emphasizes the practical impact of this approach in cancer pathogenesis studies, offering valuable insights for personalized treatment strategies.</p>
</div>
<div class="ltx_para" id="S2.SS1.p12">
<p class="ltx_p" id="S2.SS1.p12.1">Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib18" title="">18</a>]</cite> propose a multimodal deep neural network by integrating multi-dimensional data (MDNNMD) for predicting breast cancer prognosis. Their study achieves superior performance compared to existing approaches, emphasizing the potential of deep learning and multi-dimensional data integration for improving breast cancer prognosis prediction.</p>
</div>
<div class="ltx_para" id="S2.SS1.p13">
<p class="ltx_p" id="S2.SS1.p13.1">Developing a multimodal deep learning model, Joo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib19" title="">19</a>]</cite> focus on predicting the pathologic complete response (pCR) to neoadjuvant chemotherapy (NAC) in breast cancer patients. Integrating clinical information and high-dimensional MR images, their model significantly outperforms models using only clinical information or MR images, showcasing the potential of deep learning in combining diverse information sources for improved pCR prediction.</p>
</div>
<div class="ltx_para" id="S2.SS1.p14">
<p class="ltx_p" id="S2.SS1.p14.1">In their research, Khan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib20" title="">20</a>]</cite> propose a multimodal deep neural network for multi-class malignant liver diagnosis. Integrating portal venous computed tomography (CT) scans and pathology data, they employ transfer learning to address insufficient and imbalanced datasets. Their study demonstrates superior performance compared to existing liver diagnostic studies, offering a significant contribution to malignant liver diagnosis.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Machine Learning in Healthcare</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Despite promising advancements in applying machine learning to healthcare, significant limitations persist, including the need for extensive labeled data, interpretability challenges, and potential biases. This section examines these issues and current research efforts aimed at addressing them to ensure ethical and practical implications are carefully considered.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Combining structured clinical data with unstructured clinical narratives from electronic health records (EHR), Zeng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib21" title="">21</a>]</cite> tackle the prediction of distant recurrences in breast cancer. Similarly, Harerimana et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib22" title="">22</a>]</cite> explores the potential of deep learning in leveraging Electronic Health Records for clinical tasks, offering intuitive explanations and blueprints for health informatics professionals to apply deep learning algorithms effectively in medical settings.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Addressing the needs of type 1 diabetes patients, Smith et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib23" title="">23</a>]</cite> focus on predicting blood glucose concentration, employing sophisticated data imputation techniques and various feature engineering methods.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Navigating missing longitudinal clinical data in ICU laboratory test results, Smith et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib24" title="">24</a>]</cite> propose a hybrid approach, combining linear interpolation with weighted K-Nearest Neighbors (KNN) methods. Their method outperforms existing techniques in most analytes, showcasing robustness in clinical data imputation.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">In the realm of predictive modeling for inpatient mortality among Medicare patients, researchers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib25" title="">25</a>]</cite> enhance Healthcare Cost and Utilization Project (HCUP) tools with acuity and diagnosis presence information. This augmentation not only refines predictive models but also offers insights into factors influencing inpatient mortality rates.</p>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1">Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib26" title="">26</a>]</cite> introduce DeepFall, a pioneering fall detection framework leveraging deep spatio-temporal convolutional autoencoders. Their method outperforms traditional approaches on publicly available datasets, promising advancements in privacy-preserving fall detection mechanisms.</p>
</div>
<div class="ltx_para" id="S2.SS2.p7">
<p class="ltx_p" id="S2.SS2.p7.1">Tekiroğlu and Erkan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib27" title="">27</a>]</cite> contribute to the field by developing Turkish biomedical language models, specifically introducing the BioBERTurk family and a labeled dataset for head CT radiology report classification. Their work enhances classification performance in clinical contexts, catering to linguistic diversity within medical data analysis.</p>
</div>
<div class="ltx_para" id="S2.SS2.p8">
<p class="ltx_p" id="S2.SS2.p8.1">Recent studies have emphasized the vital role of chest X-rays (CXR) in predicting mortality and ventilatory support requirements for COVID-19 patients. Balbi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib28" title="">28</a>]</cite> particularly highlight CXR as a complementary tool alongside clinical and demographic data for early prognosis and risk assessment. This underscores the importance of integrating imaging modalities with traditional diagnostic approaches to enhance patient care in critical conditions like COVID-19.</p>
</div>
<div class="ltx_para" id="S2.SS2.p9">
<p class="ltx_p" id="S2.SS2.p9.1">Machine learning techniques, particularly those leveraging Big Data Analytics, have shown promise in disease prediction and prognosis. Mir and Dhage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib29" title="">29</a>]</cite> explored the prediction of diabetes using machine learning algorithms, evaluating their performance on the Pima Indians Diabetes Database. Their study highlights the potential of machine learning in enabling early disease diagnosis and facilitating informed decision-making in healthcare, thereby enhancing patient outcomes.</p>
</div>
<div class="ltx_para" id="S2.SS2.p10">
<p class="ltx_p" id="S2.SS2.p10.1">Maity and Das <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib30" title="">30</a>]</cite> delved into the applications of machine learning in healthcare, with a specific focus on diagnosis and prognosis. Through case studies on Alzheimer’s disease diagnosis and breast cancer severity classification, Maity demonstrates how machine learning algorithms can efficiently analyze complex healthcare data, enabling timely intervention and personalized treatment strategies.</p>
</div>
<div class="ltx_para" id="S2.SS2.p11">
<p class="ltx_p" id="S2.SS2.p11.1">In the domain of oncology, Rahane <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib31" title="">31</a>]</cite> explored the use of image processing and machine learning techniques for lung cancer detection. By employing SVM and image processing on CT scan images and blood samples, Rahane’s study underscores the importance of early detection in improving treatment outcomes for lung cancer patients. This highlights the critical role of advanced imaging modalities and computational techniques in enhancing cancer care and patient survival rates.</p>
</div>
<div class="ltx_para" id="S2.SS2.p12">
<p class="ltx_p" id="S2.SS2.p12.1">Moving to hepatology, Gogi and Vijayalakshmi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib32" title="">32</a>]</cite> explored the use of machine learning algorithms for liver disease prognosis. By analyzing Liver Function Tests (LFT) parameters using Decision Tree, Linear Discriminant, SVM Fine Gaussian, and Logistic Regression algorithms, Gogi demonstrates the potential of machine learning in enhancing prognosis methods for liver diseases.</p>
</div>
<div class="ltx_para" id="S2.SS2.p13">
<p class="ltx_p" id="S2.SS2.p13.1">In the domain of disease risk prediction, Shuai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib33" title="">33</a>]</cite> proposed a method that integrates attention mechanisms with Clinical-BERT to improve interpretability in disease risk prediction using textual inputs. Their approach focuses on joint embedding of words and labels, effectively utilizing medical notes to enhance prediction accuracy while maintaining interpretability.</p>
</div>
<div class="ltx_para" id="S2.SS2.p14">
<p class="ltx_p" id="S2.SS2.p14.1">Jacenkow et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib34" title="">34</a>]</cite> explored the influence of textual information, specifically the indication field in radiology reports, on image classification tasks. By fine-tuning a transformer network pre-trained on text data for multimodal classification, they achieved improved performance in identifying image features relevant to clinical indications.</p>
</div>
<div class="ltx_para" id="S2.SS2.p15">
<p class="ltx_p" id="S2.SS2.p15.1">Bezirganyan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib35" title="">35</a>]</cite> introduced M2-Mixer, a novel architecture for multimodal classification tasks. Their approach, based on MLP-Mixer, incorporates a multi-head loss function to address modality predominance issues. By simplifying the architecture and introducing multi-head loss, they achieved superior performance compared to existing baseline models on benchmark datasets.</p>
</div>
<div class="ltx_para" id="S2.SS2.p16">
<p class="ltx_p" id="S2.SS2.p16.1">Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib36" title="">36</a>]</cite> focused on predicting mortality among patients with liver cirrhosis using electronic health record (EHR) data and machine learning techniques. By comparing deep learning models with traditional Models for End Stage Liver disease (MELD) scores, they showcased the superior performance of deep learning in mortality prediction.</p>
</div>
<div class="ltx_para" id="S2.SS2.p17">
<p class="ltx_p" id="S2.SS2.p17.1">Suvon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib37" title="">37</a>]</cite> tackled the challenging task of predicting mortality in patients with Pulmonary Arterial Hypertension (PAH) using a multimodal learning approach. By integrating features from numerical imaging, echo report categorical features, and echo report text features extracted using BERT, they developed a comprehensive framework for mortality prediction. Their experiments showed significant improvements in prediction accuracy.</p>
</div>
<div class="ltx_para" id="S2.SS2.p18">
<p class="ltx_p" id="S2.SS2.p18.1">The paper MedFuse<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib3" title="">3</a>]</cite> is the most related work to ours. This research introduces a novel LSTM-based fusion module designed to integrate uni-modal and multimodal input, addressing challenges in multimodal fusion where data collection is asynchronous. MedFuse demonstrates significant performance improvements in in-hospital mortality prediction and phenotype classification tasks by using clinical time series data and chest X-ray images from the MIMIC-IV and MIMIC-CXR datasets. Unlike other fusion approaches, MedFuse treats multimodal representations as a sequence of uni-modal representations, performing even with partially paired data. However, the selected architecture is not the best-performing and robust solution. Their model also does not consider the uncertainty of the different tasks in multi-task phenotyping classification. Our proposed approach, including our architecture, attention mechanism, preprocessing methods, and the incorporation of uncertainty modeling through a loss function, enabled us to outperform their method in both noisy and noise-free environments.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Multimodal Data Preparation</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section describes the process of preparing our multimodal dataset. First, we will discuss the details of the MIMIC dataset (Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.SS1" title="III-A MIMIC dataset ‣ III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>), including its overall structure and relevant information for our analysis. Next, we will explore the specifics of the MIMIC-CXR subset (Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.SS2" title="III-B MIMIC-CXR ‣ III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>), which focuses on chest X-ray images within the MIMIC dataset. Finally, we will review the MIMIC-IV dataset (Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.SS3" title="III-C MIMIC-IV ‣ III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>), which provides additional information relevant to our study. At the end (Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.SS4" title="III-D Generating Multimodal Dataset ‣ III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>), we’ll discuss how the data from MIMIC-IV and MIMIC-CXR are combined to generate the multimodal dataset.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">MIMIC dataset</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The Medical Information Mart for Intensive Care (MIMIC) is a large, freely available database containing healthcare data. MIMIC-IV (v2.2) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib2" title="">2</a>]</cite>, the latest version released in January 2023, incorporates data from patients admitted between the years 2008 and 2019. It improves upon numerous aspects of its predecessors by adopting a modular data organization approach, highlighting data provenance. This section provides a high-level overview of the MIMIC-IV dataset, highlighting its structure and relevant information for our analysis. MIMIC-IV’s rich data allows researchers to explore a wide range of healthcare topics, including patient demographics, diagnoses, procedures, medications, laboratory measurements, vital signs, and even information from the online medical record system (e.g., height and weight). Importantly, the data is deidentified using a strict protocol to protect patient privacy while still enabling valuable medical research.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">MIMIC-CXR</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">MIMIC-CXR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib1" title="">1</a>]</cite> is a valuable subset of MIMIC specifically focusing on chest X-ray images. This publicly available resource provides a rich dataset for researchers in medical image analysis. It contains 377,110 de-identified chest radiographs, including both frontal and lateral views captured during patients’ hospital admissions. In this research paper, the multimodal dataset utilized combines MIMIC-CXR with MIMIC-IV, integrating chest X-ray images from MIMIC-CXR as the second modality. This fusion allows for a comprehensive dataset including both time series data and image modalities.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">MIMIC-IV</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The MIMIC-IV database represents a significant advancement in medical data resources, building upon the success of MIMIC-III. Incorporating contemporary data from 2008 to 2019, MIMIC-IV is sourced from two in-hospital database systems, a custom hospital-wide Electronic Health Record (EHR) and an ICU-specific clinical information system. The latest version of this dataset includes the information of 299,712 patients, 431,231 admissions, and 73,181 ICU stays. This dataset is used to run experiments on the proposed model for two tasks; namely in-hospital mortality prediction, and phenotyping task. The latter includes 25 binary labels for predicting a range of diseases which can be categorized into groups of acute, mixed, and chronic diseases. For instance, the <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">chronic kidney diseases</span> label in the chronic group is considered for patients who have long-term damage to the kidneys, often progressive and irreversible, leading to impaired kidney function. As an acute disease, <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">complications of surgical procedures or medical care</span> are Adverse events or complications arising from surgical or medical treatments and <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.3">other liver diseases</span> label determines the various liver disorders not classified under specific categories, including conditions like fatty liver disease and hepatitis. MIMIC-IV serves as a valuable resource for driving advancements in clinical informatics, epidemiology, and machine learning to improve patient care and outcomes. In our study, we have used this dataset to get the time series modality for our research.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.4.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.5.2">Generating Multimodal Dataset</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">As previously mentioned, we employ the MIMIC-IV dataset to assess and benchmark our models. To generate this dataset, we adopt the data extraction and preprocessing approach proposed in the MedFuse<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib3" title="">3</a>]</cite>. In the original dataset, each patient report may contain zero or more chest X-ray images taken during the patient’s hospital stay, in addition to time series data. Our data generation process involves selecting the last captured image for each patient report and combining it with the associated time series data to create a sample.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">We utilize consistent dataset settings for reporting our results. Employing the patient identifier from the clinical time series data, we randomly partition the dataset into 70% for training, 10% for validation, and 20% for the test set. In our notation, we denote the clinical time series data as EHR and the chest X-ray images as CXR. The dataset is categorized into (EHR+CXR)PARTIAL, containing paired and partially paired samples (i.e., samples with missing chest X-rays), and (EHR + CXR)PAIRED, containing data samples where both modalities are present.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">For instance, the (EHR + CXR)PARTIAL training set for patient phenotyping comprises 7,756 samples, with chest X-rays included among 42,628 samples. Chest X-ray images are extracted from MIMIC-CXR and split based on a random patient split. Images from the training set are then transferred to either the validation or test set if associated with patients in the validation or test splits of the clinical time series data. This procedure results in 325,188 images in the training set, 15,282 images in the validation set, and 36,625 images in the test set.</p>
</div>
<figure class="ltx_figure" id="S3.F1">
<p class="ltx_p ltx_align_center" id="S3.F1.2">[]<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="180" id="S3.F1.1.g1" src="extracted/5618045/original_image.jpg" width="150"/>
[]<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="180" id="S3.F1.2.g2" src="extracted/5618045/clahe_image.jpg" width="150"/></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.4.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.5.2" style="font-size:90%;">The effect of CLAHE augmentation: (a) original chest X-ray image, (b) CLAHE augmented image. CLAHE significantly improves the visibility of inner body parts, showcasing intricate details such as the kidney on the right side of the image. Additionally, it enhances the depiction of bone density, providing clearer insights. Such refined details play a crucial role in mortality prediction and phenotype classification. </span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.4.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.5.2">Preprocessing</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">We use the MedFuse<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib3" title="">3</a>]</cite> data extraction and preprocessing procedure along with another preprocessing method for images known as CLAHE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib38" title="">38</a>]</cite>. For chest X-ray images, a consistent set of transformations is applied during both pre-training and fine-tuning across all experiments and tasks. Specifically, each image is resized to 256 × 256 pixels, undergoes a random horizontal flip, and experiences various random affine transformations, including rotation, scaling, shearing, and translation. Subsequently, a random crop is applied to achieve an image size of 224 × 224 pixels. During the validation and testing phases, image resizing to 256 × 256 and a center crop to 224 × 224 pixels are performed.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">To ensure fair comparisons and showcase the efficacy of multimodal learning, we utilize a consistent set of 17 clinical variables.
Among these, five are categorical: capillary refill rate, glasgow coma scale eye opening, glasgow coma scale motor response, glasgow coma scale verbal response, and glasgow coma scale total. The remaining 12 are continuous variables: diastolic blood pressure, fraction of inspired oxygen, glucose, heart rate, height, mean blood pressure, oxygen saturation, respiratory rate, systolic blood pressure, temperature, weight, and pH. The input for all tasks is regularly sampled every two hours, with discretization and standardization of clinical variables following established protocols, as detailed in prior work.</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.2">After data preprocessing and one-hot encoding of categorical features, we obtain a vector representation of size 76 at each time step in the clinical time series data. For a given instance, the representation is denoted as <math alttext="x_{ehr}\in\mathbb{R}^{t\times 76}" class="ltx_Math" display="inline" id="S3.SS5.p3.1.m1.1"><semantics id="S3.SS5.p3.1.m1.1a"><mrow id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml"><msub id="S3.SS5.p3.1.m1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.cmml"><mi id="S3.SS5.p3.1.m1.1.1.2.2" xref="S3.SS5.p3.1.m1.1.1.2.2.cmml">x</mi><mrow id="S3.SS5.p3.1.m1.1.1.2.3" xref="S3.SS5.p3.1.m1.1.1.2.3.cmml"><mi id="S3.SS5.p3.1.m1.1.1.2.3.2" xref="S3.SS5.p3.1.m1.1.1.2.3.2.cmml">e</mi><mo id="S3.SS5.p3.1.m1.1.1.2.3.1" xref="S3.SS5.p3.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS5.p3.1.m1.1.1.2.3.3" xref="S3.SS5.p3.1.m1.1.1.2.3.3.cmml">h</mi><mo id="S3.SS5.p3.1.m1.1.1.2.3.1a" xref="S3.SS5.p3.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS5.p3.1.m1.1.1.2.3.4" xref="S3.SS5.p3.1.m1.1.1.2.3.4.cmml">r</mi></mrow></msub><mo id="S3.SS5.p3.1.m1.1.1.1" xref="S3.SS5.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS5.p3.1.m1.1.1.3" xref="S3.SS5.p3.1.m1.1.1.3.cmml"><mi id="S3.SS5.p3.1.m1.1.1.3.2" xref="S3.SS5.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS5.p3.1.m1.1.1.3.3" xref="S3.SS5.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS5.p3.1.m1.1.1.3.3.2" xref="S3.SS5.p3.1.m1.1.1.3.3.2.cmml">t</mi><mo id="S3.SS5.p3.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS5.p3.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS5.p3.1.m1.1.1.3.3.3" xref="S3.SS5.p3.1.m1.1.1.3.3.3.cmml">76</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"><in id="S3.SS5.p3.1.m1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1.1"></in><apply id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.p3.1.m1.1.1.2.1.cmml" xref="S3.SS5.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS5.p3.1.m1.1.1.2.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2.2">𝑥</ci><apply id="S3.SS5.p3.1.m1.1.1.2.3.cmml" xref="S3.SS5.p3.1.m1.1.1.2.3"><times id="S3.SS5.p3.1.m1.1.1.2.3.1.cmml" xref="S3.SS5.p3.1.m1.1.1.2.3.1"></times><ci id="S3.SS5.p3.1.m1.1.1.2.3.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2.3.2">𝑒</ci><ci id="S3.SS5.p3.1.m1.1.1.2.3.3.cmml" xref="S3.SS5.p3.1.m1.1.1.2.3.3">ℎ</ci><ci id="S3.SS5.p3.1.m1.1.1.2.3.4.cmml" xref="S3.SS5.p3.1.m1.1.1.2.3.4">𝑟</ci></apply></apply><apply id="S3.SS5.p3.1.m1.1.1.3.cmml" xref="S3.SS5.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS5.p3.1.m1.1.1.3.1.cmml" xref="S3.SS5.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS5.p3.1.m1.1.1.3.2.cmml" xref="S3.SS5.p3.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS5.p3.1.m1.1.1.3.3.cmml" xref="S3.SS5.p3.1.m1.1.1.3.3"><times id="S3.SS5.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS5.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS5.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS5.p3.1.m1.1.1.3.3.2">𝑡</ci><cn id="S3.SS5.p3.1.m1.1.1.3.3.3.cmml" type="integer" xref="S3.SS5.p3.1.m1.1.1.3.3.3">76</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">x_{ehr}\in\mathbb{R}^{t\times 76}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.1.m1.1d">italic_x start_POSTSUBSCRIPT italic_e italic_h italic_r end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_t × 76 end_POSTSUPERSCRIPT</annotation></semantics></math>, where the value of <math alttext="t" class="ltx_Math" display="inline" id="S3.SS5.p3.2.m2.1"><semantics id="S3.SS5.p3.2.m2.1a"><mi id="S3.SS5.p3.2.m2.1.1" xref="S3.SS5.p3.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.2.m2.1b"><ci id="S3.SS5.p3.2.m2.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.2.m2.1d">italic_t</annotation></semantics></math> is dependent on the specific instance and task.</p>
</div>
<div class="ltx_para" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.1">In addition, we explored a data preprocessing method known as CLAHE contrast enhancement. Developed as an extension of traditional histogram equalization, CLAHE provides a dynamic and localized approach to contrast enhancement. The method involves dividing an image into small, non-overlapping tiles and independently applying histogram equalization to each tile. This adaptive approach ensures that contrast enhancement is tailored to the unique characteristics of local regions, preventing the over-amplification of noise in homogeneous areas. We applied the CLAHE method to all images to enhance image quality, thereby facilitating the extraction of more information by the model.
In Figure.<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.F1" title="Figure 1 ‣ III-D Generating Multimodal Dataset ‣ III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">1</span></a> the impact of CLAHE on the image is visible. One of the primary benefits of CLAHE in chest X-ray imaging is its ability to enhance the visibility of lung parenchyma. In addition to lung pathology, CLAHE enhances the visualization of thoracic skeletal structures, including the ribs, vertebrae, and mediastinum. This improved depiction of bony anatomy is invaluable for detecting fractures, degenerative changes, and mediastinal masses, thereby assisting in the diagnosis of conditions ranging from traumatic injuries to neoplastic processes.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="127" id="S3.F2.g1" src="extracted/5618045/model_arch.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Model architecture consisting of modality-specific encoders and a multilayer transformer encoder as our multimodal fusion network. </span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Proposed Framework</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In the following sections, we will describe our proposed framework for multimodal healthcare analysis and prediction. We will explore the architecture of our model including the modality-specific encoders and the attention mechanism used for modality fusion (Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S4.SS1" title="IV-A Model Architecture ‣ IV Proposed Framework ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>). Then we will go into more detail about our attention-based model in Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S4.SS2" title="IV-B Attention-Based Multimodal Fusion ‣ IV Proposed Framework ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>. Finally, we will explain the motives for using the uncertainty loss function and its advantages (Section <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S4.SS3" title="IV-C Uncertainty Loss ‣ IV Proposed Framework ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Model Architecture</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Our proposed model shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S3.F2" title="Figure 2 ‣ III-E Preprocessing ‣ III Multimodal Data Preparation ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">2</span></a> consists of two major parts: modality-specific encoders and a multimodal Transformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib39" title="">39</a>]</cite> as our modality fusion network. We use an image encoder (e.g., a ResNet-34 model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib40" title="">40</a>]</cite>) to extract features from our image modality and an LSTM network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib41" title="">41</a>]</cite> to extract latent feature representations from our time series modality. We then utilize a projection layer to project the image embeddings to the time series embedding dimension. Finally, we concatenate these feature representations and feed them to a Transformer encoder to predict the labels.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In the first part, we pre-train our encoders to independently extract meaningful representations from each of our modalities. An LSTM architecture works best for extracting feature embeddings from our time series data due to the quantity of available data and its consecutive nature. We use the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib42" title="">42</a>]</cite> to optimize our Binary Cross-Entropy losses and pre-train our modality-specific encoders.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.3">We use a ResNet-34 model as the backbone for our image encoder and we set the output dimension of its classifier layer to be equal to the number of labels in our specific task. For our time series backbone, we use an LSTM network with <math alttext="N=2" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">N</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><eq id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">𝑁</ci><cn id="S4.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">N=2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_N = 2</annotation></semantics></math> layers stacked on top of each other with a hidden dimension of <math alttext="d=256" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mi id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml">d</mi><mo id="S4.SS1.p3.2.m2.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><eq id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1"></eq><ci id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">𝑑</ci><cn id="S4.SS1.p3.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.p3.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">d=256</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_d = 256</annotation></semantics></math> and a dropout layer with a dropout probability of <math alttext="p=0.3" class="ltx_Math" display="inline" id="S4.SS1.p3.3.m3.1"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mi id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">p</mi><mo id="S4.SS1.p3.3.m3.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml">0.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><eq id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1"></eq><ci id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">𝑝</ci><cn id="S4.SS1.p3.3.m3.1.1.3.cmml" type="float" xref="S4.SS1.p3.3.m3.1.1.3">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">p=0.3</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.3.m3.1d">italic_p = 0.3</annotation></semantics></math>. We also utilize a linear layer as the final classifier for our LSTM network.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.8">In the second part, we dismiss the classifiers for our encoders and use latent feature embeddings <math alttext="f^{’}_{cxr}" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><msubsup id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2.2" xref="S4.SS1.p4.1.m1.1.1.2.2.cmml">f</mi><mrow id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.cmml">c</mi><mo id="S4.SS1.p4.1.m1.1.1.3.1" xref="S4.SS1.p4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.1.m1.1.1.3.3" xref="S4.SS1.p4.1.m1.1.1.3.3.cmml">x</mi><mo id="S4.SS1.p4.1.m1.1.1.3.1a" xref="S4.SS1.p4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.1.m1.1.1.3.4" xref="S4.SS1.p4.1.m1.1.1.3.4.cmml">r</mi></mrow><mi id="S4.SS1.p4.1.m1.1.1.2.3" mathvariant="normal" xref="S4.SS1.p4.1.m1.1.1.2.3.cmml">’</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">subscript</csymbol><apply id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.2.1.cmml" xref="S4.SS1.p4.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.2.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2.2">𝑓</ci><ci id="S4.SS1.p4.1.m1.1.1.2.3.cmml" xref="S4.SS1.p4.1.m1.1.1.2.3">’</ci></apply><apply id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3"><times id="S4.SS1.p4.1.m1.1.1.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1"></times><ci id="S4.SS1.p4.1.m1.1.1.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2">𝑐</ci><ci id="S4.SS1.p4.1.m1.1.1.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3">𝑥</ci><ci id="S4.SS1.p4.1.m1.1.1.3.4.cmml" xref="S4.SS1.p4.1.m1.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">f^{’}_{cxr}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">italic_f start_POSTSUPERSCRIPT ’ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c italic_x italic_r end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{ehr}" class="ltx_Math" display="inline" id="S4.SS1.p4.2.m2.1"><semantics id="S4.SS1.p4.2.m2.1a"><msub id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">f</mi><mrow id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml"><mi id="S4.SS1.p4.2.m2.1.1.3.2" xref="S4.SS1.p4.2.m2.1.1.3.2.cmml">e</mi><mo id="S4.SS1.p4.2.m2.1.1.3.1" xref="S4.SS1.p4.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.2.m2.1.1.3.3" xref="S4.SS1.p4.2.m2.1.1.3.3.cmml">h</mi><mo id="S4.SS1.p4.2.m2.1.1.3.1a" xref="S4.SS1.p4.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.2.m2.1.1.3.4" xref="S4.SS1.p4.2.m2.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">𝑓</ci><apply id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3"><times id="S4.SS1.p4.2.m2.1.1.3.1.cmml" xref="S4.SS1.p4.2.m2.1.1.3.1"></times><ci id="S4.SS1.p4.2.m2.1.1.3.2.cmml" xref="S4.SS1.p4.2.m2.1.1.3.2">𝑒</ci><ci id="S4.SS1.p4.2.m2.1.1.3.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3.3">ℎ</ci><ci id="S4.SS1.p4.2.m2.1.1.3.4.cmml" xref="S4.SS1.p4.2.m2.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">f_{ehr}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.2.m2.1d">italic_f start_POSTSUBSCRIPT italic_e italic_h italic_r end_POSTSUBSCRIPT</annotation></semantics></math>. We feed <math alttext="f^{’}_{cxr}" class="ltx_Math" display="inline" id="S4.SS1.p4.3.m3.1"><semantics id="S4.SS1.p4.3.m3.1a"><msubsup id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><mi id="S4.SS1.p4.3.m3.1.1.2.2" xref="S4.SS1.p4.3.m3.1.1.2.2.cmml">f</mi><mrow id="S4.SS1.p4.3.m3.1.1.3" xref="S4.SS1.p4.3.m3.1.1.3.cmml"><mi id="S4.SS1.p4.3.m3.1.1.3.2" xref="S4.SS1.p4.3.m3.1.1.3.2.cmml">c</mi><mo id="S4.SS1.p4.3.m3.1.1.3.1" xref="S4.SS1.p4.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.3.m3.1.1.3.3" xref="S4.SS1.p4.3.m3.1.1.3.3.cmml">x</mi><mo id="S4.SS1.p4.3.m3.1.1.3.1a" xref="S4.SS1.p4.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.3.m3.1.1.3.4" xref="S4.SS1.p4.3.m3.1.1.3.4.cmml">r</mi></mrow><mi id="S4.SS1.p4.3.m3.1.1.2.3" mathvariant="normal" xref="S4.SS1.p4.3.m3.1.1.2.3.cmml">’</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">subscript</csymbol><apply id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.3.m3.1.1.2.1.cmml" xref="S4.SS1.p4.3.m3.1.1">superscript</csymbol><ci id="S4.SS1.p4.3.m3.1.1.2.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2.2">𝑓</ci><ci id="S4.SS1.p4.3.m3.1.1.2.3.cmml" xref="S4.SS1.p4.3.m3.1.1.2.3">’</ci></apply><apply id="S4.SS1.p4.3.m3.1.1.3.cmml" xref="S4.SS1.p4.3.m3.1.1.3"><times id="S4.SS1.p4.3.m3.1.1.3.1.cmml" xref="S4.SS1.p4.3.m3.1.1.3.1"></times><ci id="S4.SS1.p4.3.m3.1.1.3.2.cmml" xref="S4.SS1.p4.3.m3.1.1.3.2">𝑐</ci><ci id="S4.SS1.p4.3.m3.1.1.3.3.cmml" xref="S4.SS1.p4.3.m3.1.1.3.3">𝑥</ci><ci id="S4.SS1.p4.3.m3.1.1.3.4.cmml" xref="S4.SS1.p4.3.m3.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">f^{’}_{cxr}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.3.m3.1d">italic_f start_POSTSUPERSCRIPT ’ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c italic_x italic_r end_POSTSUBSCRIPT</annotation></semantics></math> to a fully connected projection layer to get <math alttext="f_{cxr}" class="ltx_Math" display="inline" id="S4.SS1.p4.4.m4.1"><semantics id="S4.SS1.p4.4.m4.1a"><msub id="S4.SS1.p4.4.m4.1.1" xref="S4.SS1.p4.4.m4.1.1.cmml"><mi id="S4.SS1.p4.4.m4.1.1.2" xref="S4.SS1.p4.4.m4.1.1.2.cmml">f</mi><mrow id="S4.SS1.p4.4.m4.1.1.3" xref="S4.SS1.p4.4.m4.1.1.3.cmml"><mi id="S4.SS1.p4.4.m4.1.1.3.2" xref="S4.SS1.p4.4.m4.1.1.3.2.cmml">c</mi><mo id="S4.SS1.p4.4.m4.1.1.3.1" xref="S4.SS1.p4.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.4.m4.1.1.3.3" xref="S4.SS1.p4.4.m4.1.1.3.3.cmml">x</mi><mo id="S4.SS1.p4.4.m4.1.1.3.1a" xref="S4.SS1.p4.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.4.m4.1.1.3.4" xref="S4.SS1.p4.4.m4.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.1b"><apply id="S4.SS1.p4.4.m4.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.4.m4.1.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p4.4.m4.1.1.2.cmml" xref="S4.SS1.p4.4.m4.1.1.2">𝑓</ci><apply id="S4.SS1.p4.4.m4.1.1.3.cmml" xref="S4.SS1.p4.4.m4.1.1.3"><times id="S4.SS1.p4.4.m4.1.1.3.1.cmml" xref="S4.SS1.p4.4.m4.1.1.3.1"></times><ci id="S4.SS1.p4.4.m4.1.1.3.2.cmml" xref="S4.SS1.p4.4.m4.1.1.3.2">𝑐</ci><ci id="S4.SS1.p4.4.m4.1.1.3.3.cmml" xref="S4.SS1.p4.4.m4.1.1.3.3">𝑥</ci><ci id="S4.SS1.p4.4.m4.1.1.3.4.cmml" xref="S4.SS1.p4.4.m4.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.1c">f_{cxr}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.4.m4.1d">italic_f start_POSTSUBSCRIPT italic_c italic_x italic_r end_POSTSUBSCRIPT</annotation></semantics></math> that has the same dimensionality as <math alttext="f_{ehr}" class="ltx_Math" display="inline" id="S4.SS1.p4.5.m5.1"><semantics id="S4.SS1.p4.5.m5.1a"><msub id="S4.SS1.p4.5.m5.1.1" xref="S4.SS1.p4.5.m5.1.1.cmml"><mi id="S4.SS1.p4.5.m5.1.1.2" xref="S4.SS1.p4.5.m5.1.1.2.cmml">f</mi><mrow id="S4.SS1.p4.5.m5.1.1.3" xref="S4.SS1.p4.5.m5.1.1.3.cmml"><mi id="S4.SS1.p4.5.m5.1.1.3.2" xref="S4.SS1.p4.5.m5.1.1.3.2.cmml">e</mi><mo id="S4.SS1.p4.5.m5.1.1.3.1" xref="S4.SS1.p4.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.5.m5.1.1.3.3" xref="S4.SS1.p4.5.m5.1.1.3.3.cmml">h</mi><mo id="S4.SS1.p4.5.m5.1.1.3.1a" xref="S4.SS1.p4.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.5.m5.1.1.3.4" xref="S4.SS1.p4.5.m5.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m5.1b"><apply id="S4.SS1.p4.5.m5.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.5.m5.1.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p4.5.m5.1.1.2.cmml" xref="S4.SS1.p4.5.m5.1.1.2">𝑓</ci><apply id="S4.SS1.p4.5.m5.1.1.3.cmml" xref="S4.SS1.p4.5.m5.1.1.3"><times id="S4.SS1.p4.5.m5.1.1.3.1.cmml" xref="S4.SS1.p4.5.m5.1.1.3.1"></times><ci id="S4.SS1.p4.5.m5.1.1.3.2.cmml" xref="S4.SS1.p4.5.m5.1.1.3.2">𝑒</ci><ci id="S4.SS1.p4.5.m5.1.1.3.3.cmml" xref="S4.SS1.p4.5.m5.1.1.3.3">ℎ</ci><ci id="S4.SS1.p4.5.m5.1.1.3.4.cmml" xref="S4.SS1.p4.5.m5.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m5.1c">f_{ehr}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.5.m5.1d">italic_f start_POSTSUBSCRIPT italic_e italic_h italic_r end_POSTSUBSCRIPT</annotation></semantics></math>. We then concatenate <math alttext="f_{ehr}" class="ltx_Math" display="inline" id="S4.SS1.p4.6.m6.1"><semantics id="S4.SS1.p4.6.m6.1a"><msub id="S4.SS1.p4.6.m6.1.1" xref="S4.SS1.p4.6.m6.1.1.cmml"><mi id="S4.SS1.p4.6.m6.1.1.2" xref="S4.SS1.p4.6.m6.1.1.2.cmml">f</mi><mrow id="S4.SS1.p4.6.m6.1.1.3" xref="S4.SS1.p4.6.m6.1.1.3.cmml"><mi id="S4.SS1.p4.6.m6.1.1.3.2" xref="S4.SS1.p4.6.m6.1.1.3.2.cmml">e</mi><mo id="S4.SS1.p4.6.m6.1.1.3.1" xref="S4.SS1.p4.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.6.m6.1.1.3.3" xref="S4.SS1.p4.6.m6.1.1.3.3.cmml">h</mi><mo id="S4.SS1.p4.6.m6.1.1.3.1a" xref="S4.SS1.p4.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.6.m6.1.1.3.4" xref="S4.SS1.p4.6.m6.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.6.m6.1b"><apply id="S4.SS1.p4.6.m6.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.6.m6.1.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.p4.6.m6.1.1.2.cmml" xref="S4.SS1.p4.6.m6.1.1.2">𝑓</ci><apply id="S4.SS1.p4.6.m6.1.1.3.cmml" xref="S4.SS1.p4.6.m6.1.1.3"><times id="S4.SS1.p4.6.m6.1.1.3.1.cmml" xref="S4.SS1.p4.6.m6.1.1.3.1"></times><ci id="S4.SS1.p4.6.m6.1.1.3.2.cmml" xref="S4.SS1.p4.6.m6.1.1.3.2">𝑒</ci><ci id="S4.SS1.p4.6.m6.1.1.3.3.cmml" xref="S4.SS1.p4.6.m6.1.1.3.3">ℎ</ci><ci id="S4.SS1.p4.6.m6.1.1.3.4.cmml" xref="S4.SS1.p4.6.m6.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.6.m6.1c">f_{ehr}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.6.m6.1d">italic_f start_POSTSUBSCRIPT italic_e italic_h italic_r end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{cxr}" class="ltx_Math" display="inline" id="S4.SS1.p4.7.m7.1"><semantics id="S4.SS1.p4.7.m7.1a"><msub id="S4.SS1.p4.7.m7.1.1" xref="S4.SS1.p4.7.m7.1.1.cmml"><mi id="S4.SS1.p4.7.m7.1.1.2" xref="S4.SS1.p4.7.m7.1.1.2.cmml">f</mi><mrow id="S4.SS1.p4.7.m7.1.1.3" xref="S4.SS1.p4.7.m7.1.1.3.cmml"><mi id="S4.SS1.p4.7.m7.1.1.3.2" xref="S4.SS1.p4.7.m7.1.1.3.2.cmml">c</mi><mo id="S4.SS1.p4.7.m7.1.1.3.1" xref="S4.SS1.p4.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.7.m7.1.1.3.3" xref="S4.SS1.p4.7.m7.1.1.3.3.cmml">x</mi><mo id="S4.SS1.p4.7.m7.1.1.3.1a" xref="S4.SS1.p4.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.7.m7.1.1.3.4" xref="S4.SS1.p4.7.m7.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.7.m7.1b"><apply id="S4.SS1.p4.7.m7.1.1.cmml" xref="S4.SS1.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.7.m7.1.1.1.cmml" xref="S4.SS1.p4.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.p4.7.m7.1.1.2.cmml" xref="S4.SS1.p4.7.m7.1.1.2">𝑓</ci><apply id="S4.SS1.p4.7.m7.1.1.3.cmml" xref="S4.SS1.p4.7.m7.1.1.3"><times id="S4.SS1.p4.7.m7.1.1.3.1.cmml" xref="S4.SS1.p4.7.m7.1.1.3.1"></times><ci id="S4.SS1.p4.7.m7.1.1.3.2.cmml" xref="S4.SS1.p4.7.m7.1.1.3.2">𝑐</ci><ci id="S4.SS1.p4.7.m7.1.1.3.3.cmml" xref="S4.SS1.p4.7.m7.1.1.3.3">𝑥</ci><ci id="S4.SS1.p4.7.m7.1.1.3.4.cmml" xref="S4.SS1.p4.7.m7.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.7.m7.1c">f_{cxr}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.7.m7.1d">italic_f start_POSTSUBSCRIPT italic_c italic_x italic_r end_POSTSUBSCRIPT</annotation></semantics></math> to create the sequence <math alttext="f_{fused}" class="ltx_Math" display="inline" id="S4.SS1.p4.8.m8.1"><semantics id="S4.SS1.p4.8.m8.1a"><msub id="S4.SS1.p4.8.m8.1.1" xref="S4.SS1.p4.8.m8.1.1.cmml"><mi id="S4.SS1.p4.8.m8.1.1.2" xref="S4.SS1.p4.8.m8.1.1.2.cmml">f</mi><mrow id="S4.SS1.p4.8.m8.1.1.3" xref="S4.SS1.p4.8.m8.1.1.3.cmml"><mi id="S4.SS1.p4.8.m8.1.1.3.2" xref="S4.SS1.p4.8.m8.1.1.3.2.cmml">f</mi><mo id="S4.SS1.p4.8.m8.1.1.3.1" xref="S4.SS1.p4.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.8.m8.1.1.3.3" xref="S4.SS1.p4.8.m8.1.1.3.3.cmml">u</mi><mo id="S4.SS1.p4.8.m8.1.1.3.1a" xref="S4.SS1.p4.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.8.m8.1.1.3.4" xref="S4.SS1.p4.8.m8.1.1.3.4.cmml">s</mi><mo id="S4.SS1.p4.8.m8.1.1.3.1b" xref="S4.SS1.p4.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.8.m8.1.1.3.5" xref="S4.SS1.p4.8.m8.1.1.3.5.cmml">e</mi><mo id="S4.SS1.p4.8.m8.1.1.3.1c" xref="S4.SS1.p4.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.8.m8.1.1.3.6" xref="S4.SS1.p4.8.m8.1.1.3.6.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.8.m8.1b"><apply id="S4.SS1.p4.8.m8.1.1.cmml" xref="S4.SS1.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.8.m8.1.1.1.cmml" xref="S4.SS1.p4.8.m8.1.1">subscript</csymbol><ci id="S4.SS1.p4.8.m8.1.1.2.cmml" xref="S4.SS1.p4.8.m8.1.1.2">𝑓</ci><apply id="S4.SS1.p4.8.m8.1.1.3.cmml" xref="S4.SS1.p4.8.m8.1.1.3"><times id="S4.SS1.p4.8.m8.1.1.3.1.cmml" xref="S4.SS1.p4.8.m8.1.1.3.1"></times><ci id="S4.SS1.p4.8.m8.1.1.3.2.cmml" xref="S4.SS1.p4.8.m8.1.1.3.2">𝑓</ci><ci id="S4.SS1.p4.8.m8.1.1.3.3.cmml" xref="S4.SS1.p4.8.m8.1.1.3.3">𝑢</ci><ci id="S4.SS1.p4.8.m8.1.1.3.4.cmml" xref="S4.SS1.p4.8.m8.1.1.3.4">𝑠</ci><ci id="S4.SS1.p4.8.m8.1.1.3.5.cmml" xref="S4.SS1.p4.8.m8.1.1.3.5">𝑒</ci><ci id="S4.SS1.p4.8.m8.1.1.3.6.cmml" xref="S4.SS1.p4.8.m8.1.1.3.6">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.8.m8.1c">f_{fused}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.8.m8.1d">italic_f start_POSTSUBSCRIPT italic_f italic_u italic_s italic_e italic_d end_POSTSUBSCRIPT</annotation></semantics></math> that consists of our unimodal feature embeddings:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{fused}=[f_{\text{ehr}},projection(f^{{}^{\prime}}_{\text{cxr}})]." class="ltx_Math" display="block" id="S4.E1.m1.1"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.1.4" xref="S4.E1.m1.1.1.1.1.4.cmml"><mi id="S4.E1.m1.1.1.1.1.4.2" xref="S4.E1.m1.1.1.1.1.4.2.cmml">f</mi><mrow id="S4.E1.m1.1.1.1.1.4.3" xref="S4.E1.m1.1.1.1.1.4.3.cmml"><mi id="S4.E1.m1.1.1.1.1.4.3.2" xref="S4.E1.m1.1.1.1.1.4.3.2.cmml">f</mi><mo id="S4.E1.m1.1.1.1.1.4.3.1" xref="S4.E1.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.4.3.3" xref="S4.E1.m1.1.1.1.1.4.3.3.cmml">u</mi><mo id="S4.E1.m1.1.1.1.1.4.3.1a" xref="S4.E1.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.4.3.4" xref="S4.E1.m1.1.1.1.1.4.3.4.cmml">s</mi><mo id="S4.E1.m1.1.1.1.1.4.3.1b" xref="S4.E1.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.4.3.5" xref="S4.E1.m1.1.1.1.1.4.3.5.cmml">e</mi><mo id="S4.E1.m1.1.1.1.1.4.3.1c" xref="S4.E1.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.4.3.6" xref="S4.E1.m1.1.1.1.1.4.3.6.cmml">d</mi></mrow></msub><mo id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S4.E1.m1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.2.3.cmml"><mo id="S4.E1.m1.1.1.1.1.2.2.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.2.3.cmml">[</mo><msub id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.2.cmml">f</mi><mtext id="S4.E1.m1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.3a.cmml">ehr</mtext></msub><mo id="S4.E1.m1.1.1.1.1.2.2.4" xref="S4.E1.m1.1.1.1.1.2.3.cmml">,</mo><mrow id="S4.E1.m1.1.1.1.1.2.2.2" xref="S4.E1.m1.1.1.1.1.2.2.2.cmml"><mi id="S4.E1.m1.1.1.1.1.2.2.2.3" xref="S4.E1.m1.1.1.1.1.2.2.2.3.cmml">p</mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.2" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.2.2.2.4" xref="S4.E1.m1.1.1.1.1.2.2.2.4.cmml">r</mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.2a" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.2.2.2.5" xref="S4.E1.m1.1.1.1.1.2.2.2.5.cmml">o</mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.2b" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.2.2.2.6" xref="S4.E1.m1.1.1.1.1.2.2.2.6.cmml">j</mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.2c" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.2.2.2.7" xref="S4.E1.m1.1.1.1.1.2.2.2.7.cmml">e</mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.2d" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.2.2.2.8" xref="S4.E1.m1.1.1.1.1.2.2.2.8.cmml">c</mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.2e" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.2.2.2.9" xref="S4.E1.m1.1.1.1.1.2.2.2.9.cmml">t</mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.2f" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.2.2.2.10" xref="S4.E1.m1.1.1.1.1.2.2.2.10.cmml">i</mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.2g" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.2.2.2.11" xref="S4.E1.m1.1.1.1.1.2.2.2.11.cmml">o</mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.2h" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mi id="S4.E1.m1.1.1.1.1.2.2.2.12" xref="S4.E1.m1.1.1.1.1.2.2.2.12.cmml">n</mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.2i" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mrow id="S4.E1.m1.1.1.1.1.2.2.2.1.1" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.cmml"><mo id="S4.E1.m1.1.1.1.1.2.2.2.1.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.cmml">(</mo><msubsup id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.2.cmml">f</mi><mtext id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.3" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.3a.cmml">cxr</mtext><msup id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.3.cmml"><mi id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.3a" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.3.cmml"></mi><mo id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.3.1" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.3.1.cmml">′</mo></msup></msubsup><mo id="S4.E1.m1.1.1.1.1.2.2.2.1.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.1.1.1.1.2.2.5" stretchy="false" xref="S4.E1.m1.1.1.1.1.2.3.cmml">]</mo></mrow></mrow><mo id="S4.E1.m1.1.1.1.2" lspace="0em" xref="S4.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><eq id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"></eq><apply id="S4.E1.m1.1.1.1.1.4.cmml" xref="S4.E1.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.4.1.cmml" xref="S4.E1.m1.1.1.1.1.4">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.4.2.cmml" xref="S4.E1.m1.1.1.1.1.4.2">𝑓</ci><apply id="S4.E1.m1.1.1.1.1.4.3.cmml" xref="S4.E1.m1.1.1.1.1.4.3"><times id="S4.E1.m1.1.1.1.1.4.3.1.cmml" xref="S4.E1.m1.1.1.1.1.4.3.1"></times><ci id="S4.E1.m1.1.1.1.1.4.3.2.cmml" xref="S4.E1.m1.1.1.1.1.4.3.2">𝑓</ci><ci id="S4.E1.m1.1.1.1.1.4.3.3.cmml" xref="S4.E1.m1.1.1.1.1.4.3.3">𝑢</ci><ci id="S4.E1.m1.1.1.1.1.4.3.4.cmml" xref="S4.E1.m1.1.1.1.1.4.3.4">𝑠</ci><ci id="S4.E1.m1.1.1.1.1.4.3.5.cmml" xref="S4.E1.m1.1.1.1.1.4.3.5">𝑒</ci><ci id="S4.E1.m1.1.1.1.1.4.3.6.cmml" xref="S4.E1.m1.1.1.1.1.4.3.6">𝑑</ci></apply></apply><interval closure="closed" id="S4.E1.m1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.2"><apply id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2">𝑓</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.3a.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3"><mtext id="S4.E1.m1.1.1.1.1.1.1.1.3.cmml" mathsize="70%" xref="S4.E1.m1.1.1.1.1.1.1.1.3">ehr</mtext></ci></apply><apply id="S4.E1.m1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2"><times id="S4.E1.m1.1.1.1.1.2.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.2"></times><ci id="S4.E1.m1.1.1.1.1.2.2.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.3">𝑝</ci><ci id="S4.E1.m1.1.1.1.1.2.2.2.4.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.4">𝑟</ci><ci id="S4.E1.m1.1.1.1.1.2.2.2.5.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.5">𝑜</ci><ci id="S4.E1.m1.1.1.1.1.2.2.2.6.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.6">𝑗</ci><ci id="S4.E1.m1.1.1.1.1.2.2.2.7.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.7">𝑒</ci><ci id="S4.E1.m1.1.1.1.1.2.2.2.8.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.8">𝑐</ci><ci id="S4.E1.m1.1.1.1.1.2.2.2.9.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.9">𝑡</ci><ci id="S4.E1.m1.1.1.1.1.2.2.2.10.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.10">𝑖</ci><ci id="S4.E1.m1.1.1.1.1.2.2.2.11.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.11">𝑜</ci><ci id="S4.E1.m1.1.1.1.1.2.2.2.12.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.12">𝑛</ci><apply id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1">subscript</csymbol><apply id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1">superscript</csymbol><ci id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.2">𝑓</ci><apply id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.3"><ci id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.3.1.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.2.3.1">′</ci></apply></apply><ci id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.3a.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.3"><mtext id="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.3.cmml" mathsize="70%" xref="S4.E1.m1.1.1.1.1.2.2.2.1.1.1.3">cxr</mtext></ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">f_{fused}=[f_{\text{ehr}},projection(f^{{}^{\prime}}_{\text{cxr}})].</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.1d">italic_f start_POSTSUBSCRIPT italic_f italic_u italic_s italic_e italic_d end_POSTSUBSCRIPT = [ italic_f start_POSTSUBSCRIPT ehr end_POSTSUBSCRIPT , italic_p italic_r italic_o italic_j italic_e italic_c italic_t italic_i italic_o italic_n ( italic_f start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT cxr end_POSTSUBSCRIPT ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">We use a Transformer Encoder without positional embeddings with a linear layer on top of that as our modality fusion network to resolve the issue of modality bias in our baselines. In models that use an LSTM network for fusion a major problem is the order of modality embeddings in the input sequence. Therefore, the sequence order may create a bias towards the modalities that come first in the input sequence, and changing it may vary results significantly when in reality the modalities do not possess a specific ordering. By using an attention-based network for fusion our model learns the importance of every modality in each of our tasks and thus performs better than state-of-the-art LSTM or MLP architectures. Finally, we optimize the multi-task uncertainty loss (<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S4.E3" title="In IV-C Uncertainty Loss ‣ IV Proposed Framework ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">3</span></a>) introduced in Kendall et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib43" title="">43</a>]</cite> with an Adam optimizer to fine-tune our network.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Attention-Based Multimodal Fusion</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.5">We feed our concatenated unimodal feature embedding <math alttext="f_{fused}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">f</mi><mrow id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">f</mi><mo id="S4.SS2.p1.1.m1.1.1.3.1" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.p1.1.m1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml">u</mi><mo id="S4.SS2.p1.1.m1.1.1.3.1a" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.p1.1.m1.1.1.3.4" xref="S4.SS2.p1.1.m1.1.1.3.4.cmml">s</mi><mo id="S4.SS2.p1.1.m1.1.1.3.1b" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.p1.1.m1.1.1.3.5" xref="S4.SS2.p1.1.m1.1.1.3.5.cmml">e</mi><mo id="S4.SS2.p1.1.m1.1.1.3.1c" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.p1.1.m1.1.1.3.6" xref="S4.SS2.p1.1.m1.1.1.3.6.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑓</ci><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><times id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3.1"></times><ci id="S4.SS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.2">𝑓</ci><ci id="S4.SS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3">𝑢</ci><ci id="S4.SS2.p1.1.m1.1.1.3.4.cmml" xref="S4.SS2.p1.1.m1.1.1.3.4">𝑠</ci><ci id="S4.SS2.p1.1.m1.1.1.3.5.cmml" xref="S4.SS2.p1.1.m1.1.1.3.5">𝑒</ci><ci id="S4.SS2.p1.1.m1.1.1.3.6.cmml" xref="S4.SS2.p1.1.m1.1.1.3.6">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">f_{fused}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_f italic_u italic_s italic_e italic_d end_POSTSUBSCRIPT</annotation></semantics></math> to a Transformer Encoder network with <math alttext="L=2" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">L</mi><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><eq id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></eq><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝐿</ci><cn id="S4.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">L=2</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_L = 2</annotation></semantics></math> encoder layers stacked on top of one another, each of them having <math alttext="h=8" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">h</mi><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><eq id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></eq><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">ℎ</ci><cn id="S4.SS2.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS2.p1.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">h=8</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">italic_h = 8</annotation></semantics></math> heads and a feedforward dimension of <math alttext="c=1024" class="ltx_Math" display="inline" id="S4.SS2.p1.4.m4.1"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mi id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">c</mi><mo id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><eq id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1"></eq><ci id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">𝑐</ci><cn id="S4.SS2.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS2.p1.4.m4.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">c=1024</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.4.m4.1d">italic_c = 1024</annotation></semantics></math>. We then feed the output of this network to a linear classifier to get the final predictions <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S4.SS2.p1.5.m5.1"><semantics id="S4.SS2.p1.5.m5.1a"><mover accent="true" id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">y</mi><mo id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><ci id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1">^</ci><ci id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.5.m5.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math>. Finally, we optimize the following multi-task uncertainty loss function with an Adam optimizer to fine-tune our model:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L(\hat{y},y)=UncertaintlyLoss(\hat{y},y)." class="ltx_Math" display="block" id="S4.E2.m1.5"><semantics id="S4.E2.m1.5a"><mrow id="S4.E2.m1.5.5.1" xref="S4.E2.m1.5.5.1.1.cmml"><mrow id="S4.E2.m1.5.5.1.1" xref="S4.E2.m1.5.5.1.1.cmml"><mrow id="S4.E2.m1.5.5.1.1.2" xref="S4.E2.m1.5.5.1.1.2.cmml"><mi id="S4.E2.m1.5.5.1.1.2.2" xref="S4.E2.m1.5.5.1.1.2.2.cmml">L</mi><mo id="S4.E2.m1.5.5.1.1.2.1" xref="S4.E2.m1.5.5.1.1.2.1.cmml">⁢</mo><mrow id="S4.E2.m1.5.5.1.1.2.3.2" xref="S4.E2.m1.5.5.1.1.2.3.1.cmml"><mo id="S4.E2.m1.5.5.1.1.2.3.2.1" stretchy="false" xref="S4.E2.m1.5.5.1.1.2.3.1.cmml">(</mo><mover accent="true" id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mi id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml">y</mi><mo id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">^</mo></mover><mo id="S4.E2.m1.5.5.1.1.2.3.2.2" xref="S4.E2.m1.5.5.1.1.2.3.1.cmml">,</mo><mi id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml">y</mi><mo id="S4.E2.m1.5.5.1.1.2.3.2.3" stretchy="false" xref="S4.E2.m1.5.5.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.5.5.1.1.1" xref="S4.E2.m1.5.5.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.5.5.1.1.3" xref="S4.E2.m1.5.5.1.1.3.cmml"><mi id="S4.E2.m1.5.5.1.1.3.2" xref="S4.E2.m1.5.5.1.1.3.2.cmml">U</mi><mo id="S4.E2.m1.5.5.1.1.3.1" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.3" xref="S4.E2.m1.5.5.1.1.3.3.cmml">n</mi><mo id="S4.E2.m1.5.5.1.1.3.1a" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.4" xref="S4.E2.m1.5.5.1.1.3.4.cmml">c</mi><mo id="S4.E2.m1.5.5.1.1.3.1b" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.5" xref="S4.E2.m1.5.5.1.1.3.5.cmml">e</mi><mo id="S4.E2.m1.5.5.1.1.3.1c" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.6" xref="S4.E2.m1.5.5.1.1.3.6.cmml">r</mi><mo id="S4.E2.m1.5.5.1.1.3.1d" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.7" xref="S4.E2.m1.5.5.1.1.3.7.cmml">t</mi><mo id="S4.E2.m1.5.5.1.1.3.1e" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.8" xref="S4.E2.m1.5.5.1.1.3.8.cmml">a</mi><mo id="S4.E2.m1.5.5.1.1.3.1f" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.9" xref="S4.E2.m1.5.5.1.1.3.9.cmml">i</mi><mo id="S4.E2.m1.5.5.1.1.3.1g" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.10" xref="S4.E2.m1.5.5.1.1.3.10.cmml">n</mi><mo id="S4.E2.m1.5.5.1.1.3.1h" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.11" xref="S4.E2.m1.5.5.1.1.3.11.cmml">t</mi><mo id="S4.E2.m1.5.5.1.1.3.1i" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.12" xref="S4.E2.m1.5.5.1.1.3.12.cmml">l</mi><mo id="S4.E2.m1.5.5.1.1.3.1j" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.13" xref="S4.E2.m1.5.5.1.1.3.13.cmml">y</mi><mo id="S4.E2.m1.5.5.1.1.3.1k" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.14" xref="S4.E2.m1.5.5.1.1.3.14.cmml">L</mi><mo id="S4.E2.m1.5.5.1.1.3.1l" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.15" xref="S4.E2.m1.5.5.1.1.3.15.cmml">o</mi><mo id="S4.E2.m1.5.5.1.1.3.1m" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.16" xref="S4.E2.m1.5.5.1.1.3.16.cmml">s</mi><mo id="S4.E2.m1.5.5.1.1.3.1n" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S4.E2.m1.5.5.1.1.3.17" xref="S4.E2.m1.5.5.1.1.3.17.cmml">s</mi><mo id="S4.E2.m1.5.5.1.1.3.1o" xref="S4.E2.m1.5.5.1.1.3.1.cmml">⁢</mo><mrow id="S4.E2.m1.5.5.1.1.3.18.2" xref="S4.E2.m1.5.5.1.1.3.18.1.cmml"><mo id="S4.E2.m1.5.5.1.1.3.18.2.1" stretchy="false" xref="S4.E2.m1.5.5.1.1.3.18.1.cmml">(</mo><mover accent="true" id="S4.E2.m1.3.3" xref="S4.E2.m1.3.3.cmml"><mi id="S4.E2.m1.3.3.2" xref="S4.E2.m1.3.3.2.cmml">y</mi><mo id="S4.E2.m1.3.3.1" xref="S4.E2.m1.3.3.1.cmml">^</mo></mover><mo id="S4.E2.m1.5.5.1.1.3.18.2.2" xref="S4.E2.m1.5.5.1.1.3.18.1.cmml">,</mo><mi id="S4.E2.m1.4.4" xref="S4.E2.m1.4.4.cmml">y</mi><mo id="S4.E2.m1.5.5.1.1.3.18.2.3" stretchy="false" xref="S4.E2.m1.5.5.1.1.3.18.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E2.m1.5.5.1.2" lspace="0em" xref="S4.E2.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.5b"><apply id="S4.E2.m1.5.5.1.1.cmml" xref="S4.E2.m1.5.5.1"><eq id="S4.E2.m1.5.5.1.1.1.cmml" xref="S4.E2.m1.5.5.1.1.1"></eq><apply id="S4.E2.m1.5.5.1.1.2.cmml" xref="S4.E2.m1.5.5.1.1.2"><times id="S4.E2.m1.5.5.1.1.2.1.cmml" xref="S4.E2.m1.5.5.1.1.2.1"></times><ci id="S4.E2.m1.5.5.1.1.2.2.cmml" xref="S4.E2.m1.5.5.1.1.2.2">𝐿</ci><interval closure="open" id="S4.E2.m1.5.5.1.1.2.3.1.cmml" xref="S4.E2.m1.5.5.1.1.2.3.2"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><ci id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1">^</ci><ci id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2">𝑦</ci></apply><ci id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2">𝑦</ci></interval></apply><apply id="S4.E2.m1.5.5.1.1.3.cmml" xref="S4.E2.m1.5.5.1.1.3"><times id="S4.E2.m1.5.5.1.1.3.1.cmml" xref="S4.E2.m1.5.5.1.1.3.1"></times><ci id="S4.E2.m1.5.5.1.1.3.2.cmml" xref="S4.E2.m1.5.5.1.1.3.2">𝑈</ci><ci id="S4.E2.m1.5.5.1.1.3.3.cmml" xref="S4.E2.m1.5.5.1.1.3.3">𝑛</ci><ci id="S4.E2.m1.5.5.1.1.3.4.cmml" xref="S4.E2.m1.5.5.1.1.3.4">𝑐</ci><ci id="S4.E2.m1.5.5.1.1.3.5.cmml" xref="S4.E2.m1.5.5.1.1.3.5">𝑒</ci><ci id="S4.E2.m1.5.5.1.1.3.6.cmml" xref="S4.E2.m1.5.5.1.1.3.6">𝑟</ci><ci id="S4.E2.m1.5.5.1.1.3.7.cmml" xref="S4.E2.m1.5.5.1.1.3.7">𝑡</ci><ci id="S4.E2.m1.5.5.1.1.3.8.cmml" xref="S4.E2.m1.5.5.1.1.3.8">𝑎</ci><ci id="S4.E2.m1.5.5.1.1.3.9.cmml" xref="S4.E2.m1.5.5.1.1.3.9">𝑖</ci><ci id="S4.E2.m1.5.5.1.1.3.10.cmml" xref="S4.E2.m1.5.5.1.1.3.10">𝑛</ci><ci id="S4.E2.m1.5.5.1.1.3.11.cmml" xref="S4.E2.m1.5.5.1.1.3.11">𝑡</ci><ci id="S4.E2.m1.5.5.1.1.3.12.cmml" xref="S4.E2.m1.5.5.1.1.3.12">𝑙</ci><ci id="S4.E2.m1.5.5.1.1.3.13.cmml" xref="S4.E2.m1.5.5.1.1.3.13">𝑦</ci><ci id="S4.E2.m1.5.5.1.1.3.14.cmml" xref="S4.E2.m1.5.5.1.1.3.14">𝐿</ci><ci id="S4.E2.m1.5.5.1.1.3.15.cmml" xref="S4.E2.m1.5.5.1.1.3.15">𝑜</ci><ci id="S4.E2.m1.5.5.1.1.3.16.cmml" xref="S4.E2.m1.5.5.1.1.3.16">𝑠</ci><ci id="S4.E2.m1.5.5.1.1.3.17.cmml" xref="S4.E2.m1.5.5.1.1.3.17">𝑠</ci><interval closure="open" id="S4.E2.m1.5.5.1.1.3.18.1.cmml" xref="S4.E2.m1.5.5.1.1.3.18.2"><apply id="S4.E2.m1.3.3.cmml" xref="S4.E2.m1.3.3"><ci id="S4.E2.m1.3.3.1.cmml" xref="S4.E2.m1.3.3.1">^</ci><ci id="S4.E2.m1.3.3.2.cmml" xref="S4.E2.m1.3.3.2">𝑦</ci></apply><ci id="S4.E2.m1.4.4.cmml" xref="S4.E2.m1.4.4">𝑦</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.5c">L(\hat{y},y)=UncertaintlyLoss(\hat{y},y).</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.5d">italic_L ( over^ start_ARG italic_y end_ARG , italic_y ) = italic_U italic_n italic_c italic_e italic_r italic_t italic_a italic_i italic_n italic_t italic_l italic_y italic_L italic_o italic_s italic_s ( over^ start_ARG italic_y end_ARG , italic_y ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Our attention-based model can learn the importance of each modality for each task simultaneously and find meaningful relations between the latent features of different modalities. This allows our model to deeply integrate different modalities to better understand our task and achieve more precise results.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Uncertainty Loss</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">A key issue with our baseline models is their dependency on the relative weights of each task’s loss. For instance, in phenotype classification we have 25 different labels (each label corresponds to a separate task) and usually, the same weight is given to each of their losses, or the weights are manually selected. Giving the same weights to the losses of different tasks could negatively impact our model’s performance due to the separate nature of each task. Manually choosing the relative weights is also a time-consuming ordeal that should be done for every single classification problem separately and requires vast resources.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">The multi-task uncertainty loss shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S4.F3" title="Figure 3 ‣ IV-C Uncertainty Loss ‣ IV Proposed Framework ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">3</span></a> largely resolves this issue by weighing multiple losses and considering the homoscedastic uncertainty of each task. This method learns the weighing parameter <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">italic_σ</annotation></semantics></math> for each of the losses during the training process. The multi-task uncertainty loss function for classification is the following:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="UncertaintlyLoss(\hat{y},y)=\sum\limits_{i=1}^{N}\frac{1}{\sigma_{i}^{2}}L(%
\hat{y},y)+log\sigma_{i}^{2}," class="ltx_Math" display="block" id="S4.E3.m1.5"><semantics id="S4.E3.m1.5a"><mrow id="S4.E3.m1.5.5.1" xref="S4.E3.m1.5.5.1.1.cmml"><mrow id="S4.E3.m1.5.5.1.1" xref="S4.E3.m1.5.5.1.1.cmml"><mrow id="S4.E3.m1.5.5.1.1.2" xref="S4.E3.m1.5.5.1.1.2.cmml"><mi id="S4.E3.m1.5.5.1.1.2.2" xref="S4.E3.m1.5.5.1.1.2.2.cmml">U</mi><mo id="S4.E3.m1.5.5.1.1.2.1" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.3" xref="S4.E3.m1.5.5.1.1.2.3.cmml">n</mi><mo id="S4.E3.m1.5.5.1.1.2.1a" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.4" xref="S4.E3.m1.5.5.1.1.2.4.cmml">c</mi><mo id="S4.E3.m1.5.5.1.1.2.1b" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.5" xref="S4.E3.m1.5.5.1.1.2.5.cmml">e</mi><mo id="S4.E3.m1.5.5.1.1.2.1c" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.6" xref="S4.E3.m1.5.5.1.1.2.6.cmml">r</mi><mo id="S4.E3.m1.5.5.1.1.2.1d" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.7" xref="S4.E3.m1.5.5.1.1.2.7.cmml">t</mi><mo id="S4.E3.m1.5.5.1.1.2.1e" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.8" xref="S4.E3.m1.5.5.1.1.2.8.cmml">a</mi><mo id="S4.E3.m1.5.5.1.1.2.1f" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.9" xref="S4.E3.m1.5.5.1.1.2.9.cmml">i</mi><mo id="S4.E3.m1.5.5.1.1.2.1g" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.10" xref="S4.E3.m1.5.5.1.1.2.10.cmml">n</mi><mo id="S4.E3.m1.5.5.1.1.2.1h" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.11" xref="S4.E3.m1.5.5.1.1.2.11.cmml">t</mi><mo id="S4.E3.m1.5.5.1.1.2.1i" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.12" xref="S4.E3.m1.5.5.1.1.2.12.cmml">l</mi><mo id="S4.E3.m1.5.5.1.1.2.1j" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.13" xref="S4.E3.m1.5.5.1.1.2.13.cmml">y</mi><mo id="S4.E3.m1.5.5.1.1.2.1k" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.14" xref="S4.E3.m1.5.5.1.1.2.14.cmml">L</mi><mo id="S4.E3.m1.5.5.1.1.2.1l" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.15" xref="S4.E3.m1.5.5.1.1.2.15.cmml">o</mi><mo id="S4.E3.m1.5.5.1.1.2.1m" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.16" xref="S4.E3.m1.5.5.1.1.2.16.cmml">s</mi><mo id="S4.E3.m1.5.5.1.1.2.1n" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.2.17" xref="S4.E3.m1.5.5.1.1.2.17.cmml">s</mi><mo id="S4.E3.m1.5.5.1.1.2.1o" xref="S4.E3.m1.5.5.1.1.2.1.cmml">⁢</mo><mrow id="S4.E3.m1.5.5.1.1.2.18.2" xref="S4.E3.m1.5.5.1.1.2.18.1.cmml"><mo id="S4.E3.m1.5.5.1.1.2.18.2.1" stretchy="false" xref="S4.E3.m1.5.5.1.1.2.18.1.cmml">(</mo><mover accent="true" id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mi id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml">y</mi><mo id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml">^</mo></mover><mo id="S4.E3.m1.5.5.1.1.2.18.2.2" xref="S4.E3.m1.5.5.1.1.2.18.1.cmml">,</mo><mi id="S4.E3.m1.2.2" xref="S4.E3.m1.2.2.cmml">y</mi><mo id="S4.E3.m1.5.5.1.1.2.18.2.3" stretchy="false" xref="S4.E3.m1.5.5.1.1.2.18.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.5.5.1.1.1" rspace="0.111em" xref="S4.E3.m1.5.5.1.1.1.cmml">=</mo><mrow id="S4.E3.m1.5.5.1.1.3" xref="S4.E3.m1.5.5.1.1.3.cmml"><mrow id="S4.E3.m1.5.5.1.1.3.2" xref="S4.E3.m1.5.5.1.1.3.2.cmml"><munderover id="S4.E3.m1.5.5.1.1.3.2.1" xref="S4.E3.m1.5.5.1.1.3.2.1.cmml"><mo id="S4.E3.m1.5.5.1.1.3.2.1.2.2" movablelimits="false" xref="S4.E3.m1.5.5.1.1.3.2.1.2.2.cmml">∑</mo><mrow id="S4.E3.m1.5.5.1.1.3.2.1.2.3" xref="S4.E3.m1.5.5.1.1.3.2.1.2.3.cmml"><mi id="S4.E3.m1.5.5.1.1.3.2.1.2.3.2" xref="S4.E3.m1.5.5.1.1.3.2.1.2.3.2.cmml">i</mi><mo id="S4.E3.m1.5.5.1.1.3.2.1.2.3.1" xref="S4.E3.m1.5.5.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="S4.E3.m1.5.5.1.1.3.2.1.2.3.3" xref="S4.E3.m1.5.5.1.1.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E3.m1.5.5.1.1.3.2.1.3" xref="S4.E3.m1.5.5.1.1.3.2.1.3.cmml">N</mi></munderover><mrow id="S4.E3.m1.5.5.1.1.3.2.2" xref="S4.E3.m1.5.5.1.1.3.2.2.cmml"><mfrac id="S4.E3.m1.5.5.1.1.3.2.2.2" xref="S4.E3.m1.5.5.1.1.3.2.2.2.cmml"><mn id="S4.E3.m1.5.5.1.1.3.2.2.2.2" xref="S4.E3.m1.5.5.1.1.3.2.2.2.2.cmml">1</mn><msubsup id="S4.E3.m1.5.5.1.1.3.2.2.2.3" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3.cmml"><mi id="S4.E3.m1.5.5.1.1.3.2.2.2.3.2.2" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3.2.2.cmml">σ</mi><mi id="S4.E3.m1.5.5.1.1.3.2.2.2.3.2.3" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3.2.3.cmml">i</mi><mn id="S4.E3.m1.5.5.1.1.3.2.2.2.3.3" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3.3.cmml">2</mn></msubsup></mfrac><mo id="S4.E3.m1.5.5.1.1.3.2.2.1" xref="S4.E3.m1.5.5.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.3.2.2.3" xref="S4.E3.m1.5.5.1.1.3.2.2.3.cmml">L</mi><mo id="S4.E3.m1.5.5.1.1.3.2.2.1a" xref="S4.E3.m1.5.5.1.1.3.2.2.1.cmml">⁢</mo><mrow id="S4.E3.m1.5.5.1.1.3.2.2.4.2" xref="S4.E3.m1.5.5.1.1.3.2.2.4.1.cmml"><mo id="S4.E3.m1.5.5.1.1.3.2.2.4.2.1" stretchy="false" xref="S4.E3.m1.5.5.1.1.3.2.2.4.1.cmml">(</mo><mover accent="true" id="S4.E3.m1.3.3" xref="S4.E3.m1.3.3.cmml"><mi id="S4.E3.m1.3.3.2" xref="S4.E3.m1.3.3.2.cmml">y</mi><mo id="S4.E3.m1.3.3.1" xref="S4.E3.m1.3.3.1.cmml">^</mo></mover><mo id="S4.E3.m1.5.5.1.1.3.2.2.4.2.2" xref="S4.E3.m1.5.5.1.1.3.2.2.4.1.cmml">,</mo><mi id="S4.E3.m1.4.4" xref="S4.E3.m1.4.4.cmml">y</mi><mo id="S4.E3.m1.5.5.1.1.3.2.2.4.2.3" stretchy="false" xref="S4.E3.m1.5.5.1.1.3.2.2.4.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E3.m1.5.5.1.1.3.1" xref="S4.E3.m1.5.5.1.1.3.1.cmml">+</mo><mrow id="S4.E3.m1.5.5.1.1.3.3" xref="S4.E3.m1.5.5.1.1.3.3.cmml"><mi id="S4.E3.m1.5.5.1.1.3.3.2" xref="S4.E3.m1.5.5.1.1.3.3.2.cmml">l</mi><mo id="S4.E3.m1.5.5.1.1.3.3.1" xref="S4.E3.m1.5.5.1.1.3.3.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.3.3.3" xref="S4.E3.m1.5.5.1.1.3.3.3.cmml">o</mi><mo id="S4.E3.m1.5.5.1.1.3.3.1a" xref="S4.E3.m1.5.5.1.1.3.3.1.cmml">⁢</mo><mi id="S4.E3.m1.5.5.1.1.3.3.4" xref="S4.E3.m1.5.5.1.1.3.3.4.cmml">g</mi><mo id="S4.E3.m1.5.5.1.1.3.3.1b" xref="S4.E3.m1.5.5.1.1.3.3.1.cmml">⁢</mo><msubsup id="S4.E3.m1.5.5.1.1.3.3.5" xref="S4.E3.m1.5.5.1.1.3.3.5.cmml"><mi id="S4.E3.m1.5.5.1.1.3.3.5.2.2" xref="S4.E3.m1.5.5.1.1.3.3.5.2.2.cmml">σ</mi><mi id="S4.E3.m1.5.5.1.1.3.3.5.2.3" xref="S4.E3.m1.5.5.1.1.3.3.5.2.3.cmml">i</mi><mn id="S4.E3.m1.5.5.1.1.3.3.5.3" xref="S4.E3.m1.5.5.1.1.3.3.5.3.cmml">2</mn></msubsup></mrow></mrow></mrow><mo id="S4.E3.m1.5.5.1.2" xref="S4.E3.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.5b"><apply id="S4.E3.m1.5.5.1.1.cmml" xref="S4.E3.m1.5.5.1"><eq id="S4.E3.m1.5.5.1.1.1.cmml" xref="S4.E3.m1.5.5.1.1.1"></eq><apply id="S4.E3.m1.5.5.1.1.2.cmml" xref="S4.E3.m1.5.5.1.1.2"><times id="S4.E3.m1.5.5.1.1.2.1.cmml" xref="S4.E3.m1.5.5.1.1.2.1"></times><ci id="S4.E3.m1.5.5.1.1.2.2.cmml" xref="S4.E3.m1.5.5.1.1.2.2">𝑈</ci><ci id="S4.E3.m1.5.5.1.1.2.3.cmml" xref="S4.E3.m1.5.5.1.1.2.3">𝑛</ci><ci id="S4.E3.m1.5.5.1.1.2.4.cmml" xref="S4.E3.m1.5.5.1.1.2.4">𝑐</ci><ci id="S4.E3.m1.5.5.1.1.2.5.cmml" xref="S4.E3.m1.5.5.1.1.2.5">𝑒</ci><ci id="S4.E3.m1.5.5.1.1.2.6.cmml" xref="S4.E3.m1.5.5.1.1.2.6">𝑟</ci><ci id="S4.E3.m1.5.5.1.1.2.7.cmml" xref="S4.E3.m1.5.5.1.1.2.7">𝑡</ci><ci id="S4.E3.m1.5.5.1.1.2.8.cmml" xref="S4.E3.m1.5.5.1.1.2.8">𝑎</ci><ci id="S4.E3.m1.5.5.1.1.2.9.cmml" xref="S4.E3.m1.5.5.1.1.2.9">𝑖</ci><ci id="S4.E3.m1.5.5.1.1.2.10.cmml" xref="S4.E3.m1.5.5.1.1.2.10">𝑛</ci><ci id="S4.E3.m1.5.5.1.1.2.11.cmml" xref="S4.E3.m1.5.5.1.1.2.11">𝑡</ci><ci id="S4.E3.m1.5.5.1.1.2.12.cmml" xref="S4.E3.m1.5.5.1.1.2.12">𝑙</ci><ci id="S4.E3.m1.5.5.1.1.2.13.cmml" xref="S4.E3.m1.5.5.1.1.2.13">𝑦</ci><ci id="S4.E3.m1.5.5.1.1.2.14.cmml" xref="S4.E3.m1.5.5.1.1.2.14">𝐿</ci><ci id="S4.E3.m1.5.5.1.1.2.15.cmml" xref="S4.E3.m1.5.5.1.1.2.15">𝑜</ci><ci id="S4.E3.m1.5.5.1.1.2.16.cmml" xref="S4.E3.m1.5.5.1.1.2.16">𝑠</ci><ci id="S4.E3.m1.5.5.1.1.2.17.cmml" xref="S4.E3.m1.5.5.1.1.2.17">𝑠</ci><interval closure="open" id="S4.E3.m1.5.5.1.1.2.18.1.cmml" xref="S4.E3.m1.5.5.1.1.2.18.2"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><ci id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1">^</ci><ci id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2">𝑦</ci></apply><ci id="S4.E3.m1.2.2.cmml" xref="S4.E3.m1.2.2">𝑦</ci></interval></apply><apply id="S4.E3.m1.5.5.1.1.3.cmml" xref="S4.E3.m1.5.5.1.1.3"><plus id="S4.E3.m1.5.5.1.1.3.1.cmml" xref="S4.E3.m1.5.5.1.1.3.1"></plus><apply id="S4.E3.m1.5.5.1.1.3.2.cmml" xref="S4.E3.m1.5.5.1.1.3.2"><apply id="S4.E3.m1.5.5.1.1.3.2.1.cmml" xref="S4.E3.m1.5.5.1.1.3.2.1"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.3.2.1.1.cmml" xref="S4.E3.m1.5.5.1.1.3.2.1">superscript</csymbol><apply id="S4.E3.m1.5.5.1.1.3.2.1.2.cmml" xref="S4.E3.m1.5.5.1.1.3.2.1"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.3.2.1.2.1.cmml" xref="S4.E3.m1.5.5.1.1.3.2.1">subscript</csymbol><sum id="S4.E3.m1.5.5.1.1.3.2.1.2.2.cmml" xref="S4.E3.m1.5.5.1.1.3.2.1.2.2"></sum><apply id="S4.E3.m1.5.5.1.1.3.2.1.2.3.cmml" xref="S4.E3.m1.5.5.1.1.3.2.1.2.3"><eq id="S4.E3.m1.5.5.1.1.3.2.1.2.3.1.cmml" xref="S4.E3.m1.5.5.1.1.3.2.1.2.3.1"></eq><ci id="S4.E3.m1.5.5.1.1.3.2.1.2.3.2.cmml" xref="S4.E3.m1.5.5.1.1.3.2.1.2.3.2">𝑖</ci><cn id="S4.E3.m1.5.5.1.1.3.2.1.2.3.3.cmml" type="integer" xref="S4.E3.m1.5.5.1.1.3.2.1.2.3.3">1</cn></apply></apply><ci id="S4.E3.m1.5.5.1.1.3.2.1.3.cmml" xref="S4.E3.m1.5.5.1.1.3.2.1.3">𝑁</ci></apply><apply id="S4.E3.m1.5.5.1.1.3.2.2.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2"><times id="S4.E3.m1.5.5.1.1.3.2.2.1.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.1"></times><apply id="S4.E3.m1.5.5.1.1.3.2.2.2.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.2"><divide id="S4.E3.m1.5.5.1.1.3.2.2.2.1.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.2"></divide><cn id="S4.E3.m1.5.5.1.1.3.2.2.2.2.cmml" type="integer" xref="S4.E3.m1.5.5.1.1.3.2.2.2.2">1</cn><apply id="S4.E3.m1.5.5.1.1.3.2.2.2.3.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.3.2.2.2.3.1.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3">superscript</csymbol><apply id="S4.E3.m1.5.5.1.1.3.2.2.2.3.2.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.3.2.2.2.3.2.1.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3">subscript</csymbol><ci id="S4.E3.m1.5.5.1.1.3.2.2.2.3.2.2.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3.2.2">𝜎</ci><ci id="S4.E3.m1.5.5.1.1.3.2.2.2.3.2.3.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3.2.3">𝑖</ci></apply><cn id="S4.E3.m1.5.5.1.1.3.2.2.2.3.3.cmml" type="integer" xref="S4.E3.m1.5.5.1.1.3.2.2.2.3.3">2</cn></apply></apply><ci id="S4.E3.m1.5.5.1.1.3.2.2.3.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.3">𝐿</ci><interval closure="open" id="S4.E3.m1.5.5.1.1.3.2.2.4.1.cmml" xref="S4.E3.m1.5.5.1.1.3.2.2.4.2"><apply id="S4.E3.m1.3.3.cmml" xref="S4.E3.m1.3.3"><ci id="S4.E3.m1.3.3.1.cmml" xref="S4.E3.m1.3.3.1">^</ci><ci id="S4.E3.m1.3.3.2.cmml" xref="S4.E3.m1.3.3.2">𝑦</ci></apply><ci id="S4.E3.m1.4.4.cmml" xref="S4.E3.m1.4.4">𝑦</ci></interval></apply></apply><apply id="S4.E3.m1.5.5.1.1.3.3.cmml" xref="S4.E3.m1.5.5.1.1.3.3"><times id="S4.E3.m1.5.5.1.1.3.3.1.cmml" xref="S4.E3.m1.5.5.1.1.3.3.1"></times><ci id="S4.E3.m1.5.5.1.1.3.3.2.cmml" xref="S4.E3.m1.5.5.1.1.3.3.2">𝑙</ci><ci id="S4.E3.m1.5.5.1.1.3.3.3.cmml" xref="S4.E3.m1.5.5.1.1.3.3.3">𝑜</ci><ci id="S4.E3.m1.5.5.1.1.3.3.4.cmml" xref="S4.E3.m1.5.5.1.1.3.3.4">𝑔</ci><apply id="S4.E3.m1.5.5.1.1.3.3.5.cmml" xref="S4.E3.m1.5.5.1.1.3.3.5"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.3.3.5.1.cmml" xref="S4.E3.m1.5.5.1.1.3.3.5">superscript</csymbol><apply id="S4.E3.m1.5.5.1.1.3.3.5.2.cmml" xref="S4.E3.m1.5.5.1.1.3.3.5"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.3.3.5.2.1.cmml" xref="S4.E3.m1.5.5.1.1.3.3.5">subscript</csymbol><ci id="S4.E3.m1.5.5.1.1.3.3.5.2.2.cmml" xref="S4.E3.m1.5.5.1.1.3.3.5.2.2">𝜎</ci><ci id="S4.E3.m1.5.5.1.1.3.3.5.2.3.cmml" xref="S4.E3.m1.5.5.1.1.3.3.5.2.3">𝑖</ci></apply><cn id="S4.E3.m1.5.5.1.1.3.3.5.3.cmml" type="integer" xref="S4.E3.m1.5.5.1.1.3.3.5.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.5c">UncertaintlyLoss(\hat{y},y)=\sum\limits_{i=1}^{N}\frac{1}{\sigma_{i}^{2}}L(%
\hat{y},y)+log\sigma_{i}^{2},</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.5d">italic_U italic_n italic_c italic_e italic_r italic_t italic_a italic_i italic_n italic_t italic_l italic_y italic_L italic_o italic_s italic_s ( over^ start_ARG italic_y end_ARG , italic_y ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG italic_L ( over^ start_ARG italic_y end_ARG , italic_y ) + italic_l italic_o italic_g italic_σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.p2.3">where <math alttext="L(\hat{y},y)" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m1.2"><semantics id="S4.SS3.p2.2.m1.2a"><mrow id="S4.SS3.p2.2.m1.2.3" xref="S4.SS3.p2.2.m1.2.3.cmml"><mi id="S4.SS3.p2.2.m1.2.3.2" xref="S4.SS3.p2.2.m1.2.3.2.cmml">L</mi><mo id="S4.SS3.p2.2.m1.2.3.1" xref="S4.SS3.p2.2.m1.2.3.1.cmml">⁢</mo><mrow id="S4.SS3.p2.2.m1.2.3.3.2" xref="S4.SS3.p2.2.m1.2.3.3.1.cmml"><mo id="S4.SS3.p2.2.m1.2.3.3.2.1" stretchy="false" xref="S4.SS3.p2.2.m1.2.3.3.1.cmml">(</mo><mover accent="true" id="S4.SS3.p2.2.m1.1.1" xref="S4.SS3.p2.2.m1.1.1.cmml"><mi id="S4.SS3.p2.2.m1.1.1.2" xref="S4.SS3.p2.2.m1.1.1.2.cmml">y</mi><mo id="S4.SS3.p2.2.m1.1.1.1" xref="S4.SS3.p2.2.m1.1.1.1.cmml">^</mo></mover><mo id="S4.SS3.p2.2.m1.2.3.3.2.2" xref="S4.SS3.p2.2.m1.2.3.3.1.cmml">,</mo><mi id="S4.SS3.p2.2.m1.2.2" xref="S4.SS3.p2.2.m1.2.2.cmml">y</mi><mo id="S4.SS3.p2.2.m1.2.3.3.2.3" stretchy="false" xref="S4.SS3.p2.2.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m1.2b"><apply id="S4.SS3.p2.2.m1.2.3.cmml" xref="S4.SS3.p2.2.m1.2.3"><times id="S4.SS3.p2.2.m1.2.3.1.cmml" xref="S4.SS3.p2.2.m1.2.3.1"></times><ci id="S4.SS3.p2.2.m1.2.3.2.cmml" xref="S4.SS3.p2.2.m1.2.3.2">𝐿</ci><interval closure="open" id="S4.SS3.p2.2.m1.2.3.3.1.cmml" xref="S4.SS3.p2.2.m1.2.3.3.2"><apply id="S4.SS3.p2.2.m1.1.1.cmml" xref="S4.SS3.p2.2.m1.1.1"><ci id="S4.SS3.p2.2.m1.1.1.1.cmml" xref="S4.SS3.p2.2.m1.1.1.1">^</ci><ci id="S4.SS3.p2.2.m1.1.1.2.cmml" xref="S4.SS3.p2.2.m1.1.1.2">𝑦</ci></apply><ci id="S4.SS3.p2.2.m1.2.2.cmml" xref="S4.SS3.p2.2.m1.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m1.2c">L(\hat{y},y)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m1.2d">italic_L ( over^ start_ARG italic_y end_ARG , italic_y )</annotation></semantics></math> is the Binary Cross-Entropy loss and <math alttext="N" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m2.1"><semantics id="S4.SS3.p2.3.m2.1a"><mi id="S4.SS3.p2.3.m2.1.1" xref="S4.SS3.p2.3.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m2.1b"><ci id="S4.SS3.p2.3.m2.1.1.cmml" xref="S4.SS3.p2.3.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m2.1d">italic_N</annotation></semantics></math> is the number of tasks.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">By using this loss to fine-tune our model and learn its parameters while simultaneously learning the value of <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mi id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><ci id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">italic_σ</annotation></semantics></math> for each task, we were able to boost the performance of our model in phenotype classification and achieve better results.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="S4.F3.g1" src="extracted/5618045/loss_uncertainty.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">We combine and weigh multiple losses according to the uncertainty of each task to compute the multi-task uncertainty loss.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Experiments and Results</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we explain our experiments. Starting from the complex architecture described in the method section, we explain how we put it into practice with a detailed demonstration of our experimental setup and baseline models. We experimented with our method on two important medical tasks: in-hospital mortality prediction and phenotypes classification. After analyzing these tasks, we break down the different parts of our approach to see how each contributes. Additionally, we investigate uncertainties and test the model’s robustness.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Experimental Setup</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In this study, we establish our experimental framework using the MIMIC-IV and the MIMIC-CXR datasets. Our choice of MIMIC is based on its extensive scale, comprehensive documentation and standardized formatting. Our experiments focus on two key objectives:</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Predicting the binary in-hospital mortality label after the first 48 hours of a patient’s ICU stay.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Classify a set of 25 phenotype labels for the patients during their ICU stays.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">We train our proposed network separately for each task and evaluate the results. We use a batch size of <math alttext="16" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mn id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><cn id="S5.SS1.p3.1.m1.1.1.cmml" type="integer" xref="S5.SS1.p3.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">16</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">16</annotation></semantics></math> for our data loader. For each task, we first pre-train our ResNet-34 encoder with images from the MIMIC-CXR dataset, and pre-train the LSTM encoder network with time series data from the MIMIC-IV dataset.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.6">For the phenotyping task, we use learning rates of <math alttext="5\times 10^{-4}" class="ltx_Math" display="inline" id="S5.SS1.p4.1.m1.1"><semantics id="S5.SS1.p4.1.m1.1a"><mrow id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml"><mn id="S5.SS1.p4.1.m1.1.1.2" xref="S5.SS1.p4.1.m1.1.1.2.cmml">5</mn><mo id="S5.SS1.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p4.1.m1.1.1.1.cmml">×</mo><msup id="S5.SS1.p4.1.m1.1.1.3" xref="S5.SS1.p4.1.m1.1.1.3.cmml"><mn id="S5.SS1.p4.1.m1.1.1.3.2" xref="S5.SS1.p4.1.m1.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p4.1.m1.1.1.3.3" xref="S5.SS1.p4.1.m1.1.1.3.3.cmml"><mo id="S5.SS1.p4.1.m1.1.1.3.3a" xref="S5.SS1.p4.1.m1.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p4.1.m1.1.1.3.3.2" xref="S5.SS1.p4.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><apply id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1"><times id="S5.SS1.p4.1.m1.1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1.1"></times><cn id="S5.SS1.p4.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.p4.1.m1.1.1.2">5</cn><apply id="S5.SS1.p4.1.m1.1.1.3.cmml" xref="S5.SS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.1.m1.1.1.3.1.cmml" xref="S5.SS1.p4.1.m1.1.1.3">superscript</csymbol><cn id="S5.SS1.p4.1.m1.1.1.3.2.cmml" type="integer" xref="S5.SS1.p4.1.m1.1.1.3.2">10</cn><apply id="S5.SS1.p4.1.m1.1.1.3.3.cmml" xref="S5.SS1.p4.1.m1.1.1.3.3"><minus id="S5.SS1.p4.1.m1.1.1.3.3.1.cmml" xref="S5.SS1.p4.1.m1.1.1.3.3"></minus><cn id="S5.SS1.p4.1.m1.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p4.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">5\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.1.m1.1d">5 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="S5.SS1.p4.2.m2.1"><semantics id="S5.SS1.p4.2.m2.1a"><mrow id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml"><mn id="S5.SS1.p4.2.m2.1.1.2" xref="S5.SS1.p4.2.m2.1.1.2.cmml">1</mn><mo id="S5.SS1.p4.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p4.2.m2.1.1.1.cmml">×</mo><msup id="S5.SS1.p4.2.m2.1.1.3" xref="S5.SS1.p4.2.m2.1.1.3.cmml"><mn id="S5.SS1.p4.2.m2.1.1.3.2" xref="S5.SS1.p4.2.m2.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p4.2.m2.1.1.3.3" xref="S5.SS1.p4.2.m2.1.1.3.3.cmml"><mo id="S5.SS1.p4.2.m2.1.1.3.3a" xref="S5.SS1.p4.2.m2.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p4.2.m2.1.1.3.3.2" xref="S5.SS1.p4.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b"><apply id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1"><times id="S5.SS1.p4.2.m2.1.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1.1"></times><cn id="S5.SS1.p4.2.m2.1.1.2.cmml" type="integer" xref="S5.SS1.p4.2.m2.1.1.2">1</cn><apply id="S5.SS1.p4.2.m2.1.1.3.cmml" xref="S5.SS1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.2.m2.1.1.3.1.cmml" xref="S5.SS1.p4.2.m2.1.1.3">superscript</csymbol><cn id="S5.SS1.p4.2.m2.1.1.3.2.cmml" type="integer" xref="S5.SS1.p4.2.m2.1.1.3.2">10</cn><apply id="S5.SS1.p4.2.m2.1.1.3.3.cmml" xref="S5.SS1.p4.2.m2.1.1.3.3"><minus id="S5.SS1.p4.2.m2.1.1.3.3.1.cmml" xref="S5.SS1.p4.2.m2.1.1.3.3"></minus><cn id="S5.SS1.p4.2.m2.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p4.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">1\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.2.m2.1d">1 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> for pre-training the image and time series encoders, respectively and for the in-hospital mortality task, we use learning rates of <math alttext="5\times 10^{-4}" class="ltx_Math" display="inline" id="S5.SS1.p4.3.m3.1"><semantics id="S5.SS1.p4.3.m3.1a"><mrow id="S5.SS1.p4.3.m3.1.1" xref="S5.SS1.p4.3.m3.1.1.cmml"><mn id="S5.SS1.p4.3.m3.1.1.2" xref="S5.SS1.p4.3.m3.1.1.2.cmml">5</mn><mo id="S5.SS1.p4.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p4.3.m3.1.1.1.cmml">×</mo><msup id="S5.SS1.p4.3.m3.1.1.3" xref="S5.SS1.p4.3.m3.1.1.3.cmml"><mn id="S5.SS1.p4.3.m3.1.1.3.2" xref="S5.SS1.p4.3.m3.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p4.3.m3.1.1.3.3" xref="S5.SS1.p4.3.m3.1.1.3.3.cmml"><mo id="S5.SS1.p4.3.m3.1.1.3.3a" xref="S5.SS1.p4.3.m3.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p4.3.m3.1.1.3.3.2" xref="S5.SS1.p4.3.m3.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.3.m3.1b"><apply id="S5.SS1.p4.3.m3.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1"><times id="S5.SS1.p4.3.m3.1.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1.1"></times><cn id="S5.SS1.p4.3.m3.1.1.2.cmml" type="integer" xref="S5.SS1.p4.3.m3.1.1.2">5</cn><apply id="S5.SS1.p4.3.m3.1.1.3.cmml" xref="S5.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.3.m3.1.1.3.1.cmml" xref="S5.SS1.p4.3.m3.1.1.3">superscript</csymbol><cn id="S5.SS1.p4.3.m3.1.1.3.2.cmml" type="integer" xref="S5.SS1.p4.3.m3.1.1.3.2">10</cn><apply id="S5.SS1.p4.3.m3.1.1.3.3.cmml" xref="S5.SS1.p4.3.m3.1.1.3.3"><minus id="S5.SS1.p4.3.m3.1.1.3.3.1.cmml" xref="S5.SS1.p4.3.m3.1.1.3.3"></minus><cn id="S5.SS1.p4.3.m3.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p4.3.m3.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.3.m3.1c">5\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.3.m3.1d">5 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="3\times 10^{-5}" class="ltx_Math" display="inline" id="S5.SS1.p4.4.m4.1"><semantics id="S5.SS1.p4.4.m4.1a"><mrow id="S5.SS1.p4.4.m4.1.1" xref="S5.SS1.p4.4.m4.1.1.cmml"><mn id="S5.SS1.p4.4.m4.1.1.2" xref="S5.SS1.p4.4.m4.1.1.2.cmml">3</mn><mo id="S5.SS1.p4.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p4.4.m4.1.1.1.cmml">×</mo><msup id="S5.SS1.p4.4.m4.1.1.3" xref="S5.SS1.p4.4.m4.1.1.3.cmml"><mn id="S5.SS1.p4.4.m4.1.1.3.2" xref="S5.SS1.p4.4.m4.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p4.4.m4.1.1.3.3" xref="S5.SS1.p4.4.m4.1.1.3.3.cmml"><mo id="S5.SS1.p4.4.m4.1.1.3.3a" xref="S5.SS1.p4.4.m4.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p4.4.m4.1.1.3.3.2" xref="S5.SS1.p4.4.m4.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.4.m4.1b"><apply id="S5.SS1.p4.4.m4.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1"><times id="S5.SS1.p4.4.m4.1.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1.1"></times><cn id="S5.SS1.p4.4.m4.1.1.2.cmml" type="integer" xref="S5.SS1.p4.4.m4.1.1.2">3</cn><apply id="S5.SS1.p4.4.m4.1.1.3.cmml" xref="S5.SS1.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.4.m4.1.1.3.1.cmml" xref="S5.SS1.p4.4.m4.1.1.3">superscript</csymbol><cn id="S5.SS1.p4.4.m4.1.1.3.2.cmml" type="integer" xref="S5.SS1.p4.4.m4.1.1.3.2">10</cn><apply id="S5.SS1.p4.4.m4.1.1.3.3.cmml" xref="S5.SS1.p4.4.m4.1.1.3.3"><minus id="S5.SS1.p4.4.m4.1.1.3.3.1.cmml" xref="S5.SS1.p4.4.m4.1.1.3.3"></minus><cn id="S5.SS1.p4.4.m4.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p4.4.m4.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.1c">3\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.4.m4.1d">3 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> for pre-training the image and time series encoders. Then we jointly fine-tune the fusion module with the encoders, using a learning rate of <math alttext="7\times 10^{-5}" class="ltx_Math" display="inline" id="S5.SS1.p4.5.m5.1"><semantics id="S5.SS1.p4.5.m5.1a"><mrow id="S5.SS1.p4.5.m5.1.1" xref="S5.SS1.p4.5.m5.1.1.cmml"><mn id="S5.SS1.p4.5.m5.1.1.2" xref="S5.SS1.p4.5.m5.1.1.2.cmml">7</mn><mo id="S5.SS1.p4.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p4.5.m5.1.1.1.cmml">×</mo><msup id="S5.SS1.p4.5.m5.1.1.3" xref="S5.SS1.p4.5.m5.1.1.3.cmml"><mn id="S5.SS1.p4.5.m5.1.1.3.2" xref="S5.SS1.p4.5.m5.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p4.5.m5.1.1.3.3" xref="S5.SS1.p4.5.m5.1.1.3.3.cmml"><mo id="S5.SS1.p4.5.m5.1.1.3.3a" xref="S5.SS1.p4.5.m5.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p4.5.m5.1.1.3.3.2" xref="S5.SS1.p4.5.m5.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.5.m5.1b"><apply id="S5.SS1.p4.5.m5.1.1.cmml" xref="S5.SS1.p4.5.m5.1.1"><times id="S5.SS1.p4.5.m5.1.1.1.cmml" xref="S5.SS1.p4.5.m5.1.1.1"></times><cn id="S5.SS1.p4.5.m5.1.1.2.cmml" type="integer" xref="S5.SS1.p4.5.m5.1.1.2">7</cn><apply id="S5.SS1.p4.5.m5.1.1.3.cmml" xref="S5.SS1.p4.5.m5.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.5.m5.1.1.3.1.cmml" xref="S5.SS1.p4.5.m5.1.1.3">superscript</csymbol><cn id="S5.SS1.p4.5.m5.1.1.3.2.cmml" type="integer" xref="S5.SS1.p4.5.m5.1.1.3.2">10</cn><apply id="S5.SS1.p4.5.m5.1.1.3.3.cmml" xref="S5.SS1.p4.5.m5.1.1.3.3"><minus id="S5.SS1.p4.5.m5.1.1.3.3.1.cmml" xref="S5.SS1.p4.5.m5.1.1.3.3"></minus><cn id="S5.SS1.p4.5.m5.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p4.5.m5.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.5.m5.1c">7\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.5.m5.1d">7 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> for the phenotyping task and <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="S5.SS1.p4.6.m6.1"><semantics id="S5.SS1.p4.6.m6.1a"><mrow id="S5.SS1.p4.6.m6.1.1" xref="S5.SS1.p4.6.m6.1.1.cmml"><mn id="S5.SS1.p4.6.m6.1.1.2" xref="S5.SS1.p4.6.m6.1.1.2.cmml">1</mn><mo id="S5.SS1.p4.6.m6.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p4.6.m6.1.1.1.cmml">×</mo><msup id="S5.SS1.p4.6.m6.1.1.3" xref="S5.SS1.p4.6.m6.1.1.3.cmml"><mn id="S5.SS1.p4.6.m6.1.1.3.2" xref="S5.SS1.p4.6.m6.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p4.6.m6.1.1.3.3" xref="S5.SS1.p4.6.m6.1.1.3.3.cmml"><mo id="S5.SS1.p4.6.m6.1.1.3.3a" xref="S5.SS1.p4.6.m6.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p4.6.m6.1.1.3.3.2" xref="S5.SS1.p4.6.m6.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.6.m6.1b"><apply id="S5.SS1.p4.6.m6.1.1.cmml" xref="S5.SS1.p4.6.m6.1.1"><times id="S5.SS1.p4.6.m6.1.1.1.cmml" xref="S5.SS1.p4.6.m6.1.1.1"></times><cn id="S5.SS1.p4.6.m6.1.1.2.cmml" type="integer" xref="S5.SS1.p4.6.m6.1.1.2">1</cn><apply id="S5.SS1.p4.6.m6.1.1.3.cmml" xref="S5.SS1.p4.6.m6.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.6.m6.1.1.3.1.cmml" xref="S5.SS1.p4.6.m6.1.1.3">superscript</csymbol><cn id="S5.SS1.p4.6.m6.1.1.3.2.cmml" type="integer" xref="S5.SS1.p4.6.m6.1.1.3.2">10</cn><apply id="S5.SS1.p4.6.m6.1.1.3.3.cmml" xref="S5.SS1.p4.6.m6.1.1.3.3"><minus id="S5.SS1.p4.6.m6.1.1.3.3.1.cmml" xref="S5.SS1.p4.6.m6.1.1.3.3"></minus><cn id="S5.SS1.p4.6.m6.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p4.6.m6.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.6.m6.1c">1\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.6.m6.1d">1 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> for the in-hospital mortality task.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Baselines</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">MedFuse</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib3" title="">3</a>]</cite> is a distinctive approach involving the utilization of a fusion module based on an LSTM architecture. This module effectively combines information from both image and time series data. Fine-tuning of the MedFuse model follows a two-step pre-training process. Initially, the image encoder is pre-trained on 14 radiology labels, focusing on the detection of a specific disease in unpaired chest X-rays. Simultaneously, the LSTM is pre-trained using unpaired time series data for in-hospital mortality prediction or phenotype classification. It’s worth noting that, despite the use of unpaired data during pre-training, MedFuse requires labels for a distinct task in the image modality. For experimental evaluation, the publicly available MedFuse model is applied to the multimodal MIMIC dataset. The dataset is partitioned into similar splits as those employed in our work and other baseline models, ensuring a fair comparison.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Contrastive-based</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib44" title="">44</a>]</cite>
Contrastive learning is a machine learning paradigm that aims to teach a model the differences and similarities between different data modalities. The fundamental idea behind contrastive learning is to embed similar samples closer to each other in a latent space while pushing dissimilar samples apart. In establishing this baseline, we have adopted an approach similar to that in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib45" title="">45</a>]</cite>, wherein we construct a model with 2 headers, using a ViT-Base image encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib46" title="">46</a>]</cite> and an LSTM time series encoder as the backbone of the model. One header is designated for inter-modality optimization, while the other optimizes the intra-modality loss. For every pair of image and time series data, we apply random augmentations and try to maximize the similarity of the data and the augmented version. We also employ the other header to align the representations of the image and time series more closely.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Diffusion-based classifier</span>
In the domain of classifiers utilizing diffusion mechanisms, our literature review revealed an absence of pre-existing multimodal diffusion-based classifiers. We extended the recent unimodal diffusion-based classifier CARD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#bib.bib47" title="">47</a>]</cite> to a multimodal version. The original architecture employs an encoder(ResNet-34) to convert image data samples into prior vectors. Subsequently, the diffusion backward process, facilitated by a denoising deep neural network, aims to denoise these prior vectors, refining them into more accurate feature vectors for classification. The final denoised prior vectors are then used for classification. To create a multimodal diffusion-based classifier, we replaced the CARD encoder with an encoder concatenating the separate embeddings of image and time series data.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.4.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.5.2">In-Hospital Mortality Prediction</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">For the critical task of in-hospital mortality prediction, we evaluate the performance of our proposed model against established baselines, including MedFuse, CARD, and Contrastive Learning. Table <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.T1" title="TABLE I ‣ V-C In-Hospital Mortality Prediction ‣ V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">I</span></a> presents a comparative analysis of macro average F1-score, binary F1-score, AUROC, and AUPRC metrics. Our model consistently outperforms state-of-the-art approaches, demonstrating its efficacy in predicting mortality across diverse modalities.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Upon reviewing the performance of the models, it’s clear that the CARD model falls short, demonstrating the lowest performance among all our baselines. The contrastive model, which employs the ViT architecture, surpasses MedFuse across all metrics, with the exception of AUROC. Our multimodal attention-based model outperforms MedFuse and CARD on all metrics and surpasses the contrastive model on macro average F1-score and AUROC. By training our proposed model with the CLAHE augmentation on images, we achieve superior results in all metrics excluding the macro average F1-score, where the filter slightly decreases the performance of our attention-based model.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">For this experiment we use time series data and chest X-ray images, to predict the in-hospital mortality of the patients according to the first 48 hours of ICU stay in a binary classification task. There are 18845 samples in our training set, 2138 samples in our validation set and 5243 samples in our test set. All samples include time series data, but 4885, 540 and 1373 samples have both modalities in the training, validation and test sets, respectively. The rest of the samples in each set have missing chest X-ray images.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T1.2.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S5.T1.3.2" style="font-size:90%;">In-hospital mortality prediction performance.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.1.1.1">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.1.1.2">Macro Average F1-score</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.1.1.3">Binary F1-score</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.1.1.4">AUROC</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.1.1.5">AUPRC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.2.1.1">MedFuse</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.2.1.2">0.677</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.2.1.3">0.412</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.2.1.4">0.857</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T1.4.2.1.5">0.507</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.3.2">
<td class="ltx_td ltx_align_left" id="S5.T1.4.3.2.1">Contrastive + ViT</td>
<td class="ltx_td ltx_align_left" id="S5.T1.4.3.2.2">0.690</td>
<td class="ltx_td ltx_align_left" id="S5.T1.4.3.2.3">0.441</td>
<td class="ltx_td ltx_align_left" id="S5.T1.4.3.2.4">0.852</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T1.4.3.2.5">0.520</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.4.3">
<td class="ltx_td ltx_align_left" id="S5.T1.4.4.3.1">CARD</td>
<td class="ltx_td ltx_align_left" id="S5.T1.4.4.3.2">0.660</td>
<td class="ltx_td ltx_align_left" id="S5.T1.4.4.3.3">0.400</td>
<td class="ltx_td ltx_align_left" id="S5.T1.4.4.3.4">0.690</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T1.4.4.3.5">0.341</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.5.4">
<td class="ltx_td ltx_align_left" id="S5.T1.4.5.4.1">Attention</td>
<td class="ltx_td ltx_align_left" id="S5.T1.4.5.4.2"><span class="ltx_text ltx_font_bold" id="S5.T1.4.5.4.2.1">0.691</span></td>
<td class="ltx_td ltx_align_left" id="S5.T1.4.5.4.3">0.438</td>
<td class="ltx_td ltx_align_left" id="S5.T1.4.5.4.4">0.857</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T1.4.5.4.5">0.514</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.6.5">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.4.6.5.1">Attention + CLAHE</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.4.6.5.2">0.685</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.4.6.5.3"><span class="ltx_text ltx_font_bold" id="S5.T1.4.6.5.3.1">0.453</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.4.6.5.4"><span class="ltx_text ltx_font_bold" id="S5.T1.4.6.5.4.1">0.858</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_b" id="S5.T1.4.6.5.5"><span class="ltx_text ltx_font_bold" id="S5.T1.4.6.5.5.1">0.524</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.2.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S5.T2.3.2" style="font-size:90%;">Phenotype classification performance.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.1.1.1">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.1.1.2">Macro Average F1-score</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.1.1.3">Binary F1-score</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.1.1.4">AUROC</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.1.1.5">AUPRC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.2.1.1">MedFuse</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.2.1.2">0.589</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.2.1.3">0.282</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.2.1.4">0.763</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T2.4.2.1.5">0.422</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.3.2">
<td class="ltx_td ltx_align_left" id="S5.T2.4.3.2.1">CARD</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.3.2.2">0.585</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.3.2.3">0.344</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.3.2.4">0.600</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T2.4.3.2.5">0.310</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.4.3">
<td class="ltx_td ltx_align_left" id="S5.T2.4.4.3.1">Attention + CLAHE</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.4.3.2">0.611</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.4.3.3">0.327</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.4.3.4"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.3.4.1">0.770</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T2.4.4.3.5">0.431</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.5.4">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.4.5.4.1">Attention + Uncertainty + CLAHE</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.4.5.4.2"><span class="ltx_text ltx_font_bold" id="S5.T2.4.5.4.2.1">0.614</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.4.5.4.3"><span class="ltx_text ltx_font_bold" id="S5.T2.4.5.4.3.1">0.362</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.4.5.4.4">0.759</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_b" id="S5.T2.4.5.4.5"><span class="ltx_text ltx_font_bold" id="S5.T2.4.5.4.5.1">0.466</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.2.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S5.T3.3.2" style="font-size:90%;">Ablation study results.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.1.1.1">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.1.1.2">Macro Average F1-score</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.1.1.3">Binary F1-score</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.1.1.4">AUROC</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.1.1.5">AUPRC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.2.1.1">Time series only</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.2.1.2">0.581</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.2.1.3">0.270</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.2.1.4">0.759</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T3.4.2.1.5">0.421</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.3.2">
<td class="ltx_td ltx_align_left" id="S5.T3.4.3.2.1">Image only</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.3.2.2">0.535</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.3.2.3">0.200</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.3.2.4">0.670</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.4.3.2.5">0.358</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.4.3">
<td class="ltx_td ltx_align_left" id="S5.T3.4.4.3.1">Multimodal LSTM Fusion</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.4.3.2">0.589</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.4.3.3">0.282</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.4.3.4">0.763</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.4.4.3.5">0.422</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.5.4">
<td class="ltx_td ltx_align_left" id="S5.T3.4.5.4.1">Multimodal Attention</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.5.4.2">0.604</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.5.4.3">0.314</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.5.4.4">0.765</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.4.5.4.5">0.424</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.6.5">
<td class="ltx_td ltx_align_left" id="S5.T3.4.6.5.1">Attention + Uncertainty Loss</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.6.5.2">0.604</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.6.5.3">0.311</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.6.5.4">0.767</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.4.6.5.5">0.427</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.7.6">
<td class="ltx_td ltx_align_left" id="S5.T3.4.7.6.1">Attention + CLAHE</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.7.6.2">0.611</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.7.6.3">0.327</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.7.6.4"><span class="ltx_text ltx_font_bold" id="S5.T3.4.7.6.4.1">0.770</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.4.7.6.5">0.431</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.8.7">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.4.8.7.1">Attention + Uncertainty + CLAHE</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.4.8.7.2"><span class="ltx_text ltx_font_bold" id="S5.T3.4.8.7.2.1">0.614</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.4.8.7.3"><span class="ltx_text ltx_font_bold" id="S5.T3.4.8.7.3.1">0.362</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.4.8.7.4">0.759</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_b" id="S5.T3.4.8.7.5"><span class="ltx_text ltx_font_bold" id="S5.T3.4.8.7.5.1">0.466</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.2.1.1" style="font-size:90%;">TABLE IV</span>: </span><span class="ltx_text" id="S5.T4.3.2" style="font-size:90%;">Robustness experiment results on models trained on noisy data.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.1.1.1">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.1.1.2">Percentage of Noise</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.1.1.3">AUROC</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.1.1.4">AUPRC</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.1.1.5">Macro Average F1-score</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.1.1.6">Binary F1-score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.2.1.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.2.1.2">%10</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.4.2.1.3.1">0.768</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.2.1.4"><span class="ltx_text ltx_font_bold" id="S5.T4.4.2.1.4.1">0.426</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.2.1.5"><span class="ltx_text ltx_font_bold" id="S5.T4.4.2.1.5.1">0.601</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4.4.2.1.6"><span class="ltx_text ltx_font_bold" id="S5.T4.4.2.1.6.1">0.306</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.3.2">
<td class="ltx_td ltx_align_left" id="S5.T4.4.3.2.1">MedFuse</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.3.2.2">%10</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.3.2.3">0.765</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.3.2.4">0.422</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.3.2.5">0.585</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.4.3.2.6">0.274</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.4.3.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.4.3.2">%20</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.3.3.1">0.763</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.4.3.4"><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.3.4.1">0.423</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.4.3.5"><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.3.5.1">0.600</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4.4.4.3.6"><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.3.6.1">0.308</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.5.4">
<td class="ltx_td ltx_align_left" id="S5.T4.4.5.4.1">MedFuse</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.5.4.2">%20</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.5.4.3">0.761</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.5.4.4">0.420</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.5.4.5">0.590</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.4.5.4.6">0.281</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.6.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.6.5.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.6.5.2">%30</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.6.5.3">0.751</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.6.5.4"><span class="ltx_text ltx_font_bold" id="S5.T4.4.6.5.4.1">0.403</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.6.5.5"><span class="ltx_text ltx_font_bold" id="S5.T4.4.6.5.5.1">0.610</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4.4.6.5.6"><span class="ltx_text ltx_font_bold" id="S5.T4.4.6.5.6.1">0.322</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.7.6">
<td class="ltx_td ltx_align_left" id="S5.T4.4.7.6.1">MedFuse</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.7.6.2">%30</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.7.6.3"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.6.3.1">0.757</span></td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.7.6.4">0.410</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.7.6.5">0.591</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.4.7.6.6">0.288</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.8.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.8.7.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.8.7.2">%40</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.8.7.3"><span class="ltx_text ltx_font_bold" id="S5.T4.4.8.7.3.1">0.763</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.8.7.4"><span class="ltx_text ltx_font_bold" id="S5.T4.4.8.7.4.1">0.421</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.8.7.5"><span class="ltx_text ltx_font_bold" id="S5.T4.4.8.7.5.1">0.602</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4.4.8.7.6"><span class="ltx_text ltx_font_bold" id="S5.T4.4.8.7.6.1">0.310</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.9.8">
<td class="ltx_td ltx_align_left" id="S5.T4.4.9.8.1">MedFuse</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.9.8.2">%40</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.9.8.3">0.757</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.9.8.4">0.414</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.9.8.5">0.594</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.4.9.8.6">0.295</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.10.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.10.9.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.10.9.2">%50</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.10.9.3"><span class="ltx_text ltx_font_bold" id="S5.T4.4.10.9.3.1">0.761</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.10.9.4"><span class="ltx_text ltx_font_bold" id="S5.T4.4.10.9.4.1">0.417</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.10.9.5"><span class="ltx_text ltx_font_bold" id="S5.T4.4.10.9.5.1">0.594</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4.4.10.9.6"><span class="ltx_text ltx_font_bold" id="S5.T4.4.10.9.6.1">0.291</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.11.10">
<td class="ltx_td ltx_align_left" id="S5.T4.4.11.10.1">MedFuse</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.11.10.2">%50</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.11.10.3">0.757</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.11.10.4">0.410</td>
<td class="ltx_td ltx_align_left" id="S5.T4.4.11.10.5">0.583</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.4.11.10.6">0.270</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.12.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.12.11.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.12.11.2">%60</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.12.11.3">0.755</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.12.11.4">0.412</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.4.12.11.5"><span class="ltx_text ltx_font_bold" id="S5.T4.4.12.11.5.1">0.602</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4.4.12.11.6"><span class="ltx_text ltx_font_bold" id="S5.T4.4.12.11.6.1">0.312</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.13.12">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T4.4.13.12.1">MedFuse</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T4.4.13.12.2">%60</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T4.4.13.12.3">0.755</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T4.4.13.12.4">0.412</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T4.4.13.12.5">0.580</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_b" id="S5.T4.4.13.12.6">0.271</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.2.1.1" style="font-size:90%;">TABLE V</span>: </span><span class="ltx_text" id="S5.T5.3.2" style="font-size:90%;">robustness experiment results on models trained on noise-free data.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.4.1.1.1">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.4.1.1.2">Percentage of Noise</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.4.1.1.3">AUROC</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.4.1.1.4">AUPRC</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.4.1.1.5">Macro Average F1-score</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.4.1.1.6">Binary F1-score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.2.1.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.2.1.2">%10</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.4.2.1.3.1">0.761</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.2.1.4"><span class="ltx_text ltx_font_bold" id="S5.T5.4.2.1.4.1">0.418</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.2.1.5"><span class="ltx_text ltx_font_bold" id="S5.T5.4.2.1.5.1">0.590</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T5.4.2.1.6"><span class="ltx_text ltx_font_bold" id="S5.T5.4.2.1.6.1">0.299</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.3.2">
<td class="ltx_td ltx_align_left" id="S5.T5.4.3.2.1">MedFuse</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.3.2.2">%10</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.3.2.3">0.757</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.3.2.4">0.412</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.3.2.5">0.581</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T5.4.3.2.6">0.284</td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.4.3.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.4.3.2">%20</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.4.3.3">0.737</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.4.3.4">0.387</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.4.3.5">0.581</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T5.4.4.3.6"><span class="ltx_text ltx_font_bold" id="S5.T5.4.4.3.6.1">0.279</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.5.4">
<td class="ltx_td ltx_align_left" id="S5.T5.4.5.4.1">MedFuse</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.5.4.2">%20</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.5.4.3"><span class="ltx_text ltx_font_bold" id="S5.T5.4.5.4.3.1">0.743</span></td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.5.4.4"><span class="ltx_text ltx_font_bold" id="S5.T5.4.5.4.4.1">0.395</span></td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.5.4.5"><span class="ltx_text ltx_font_bold" id="S5.T5.4.5.4.5.1">0.559</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T5.4.5.4.6">0.239</td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.6.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.6.5.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.6.5.2">%30</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.6.5.3"><span class="ltx_text ltx_font_bold" id="S5.T5.4.6.5.3.1">0.757</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.6.5.4"><span class="ltx_text ltx_font_bold" id="S5.T5.4.6.5.4.1">0.411</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.6.5.5"><span class="ltx_text ltx_font_bold" id="S5.T5.4.6.5.5.1">0.581</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T5.4.6.5.6"><span class="ltx_text ltx_font_bold" id="S5.T5.4.6.5.6.1">0.268</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.7.6">
<td class="ltx_td ltx_align_left" id="S5.T5.4.7.6.1">MedFuse</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.7.6.2">%30</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.7.6.3">0.753</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.7.6.4">0.404</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.7.6.5">0.557</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T5.4.7.6.6">0.220</td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.8.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.8.7.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.8.7.2">%40</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.8.7.3"><span class="ltx_text ltx_font_bold" id="S5.T5.4.8.7.3.1">0.743</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.8.7.4">0.396</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.8.7.5"><span class="ltx_text ltx_font_bold" id="S5.T5.4.8.7.5.1">0.570</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T5.4.8.7.6"><span class="ltx_text ltx_font_bold" id="S5.T5.4.8.7.6.1">0.246</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.9.8">
<td class="ltx_td ltx_align_left" id="S5.T5.4.9.8.1">MedFuse</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.9.8.2">%40</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.9.8.3">0.724</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.9.8.4">0.369</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.9.8.5">0.557</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T5.4.9.8.6">0.202</td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.10.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.10.9.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.10.9.2">%50</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.10.9.3"><span class="ltx_text ltx_font_bold" id="S5.T5.4.10.9.3.1">0.732</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.10.9.4"><span class="ltx_text ltx_font_bold" id="S5.T5.4.10.9.4.1">0.379</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.10.9.5"><span class="ltx_text ltx_font_bold" id="S5.T5.4.10.9.5.1">0.561</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T5.4.10.9.6"><span class="ltx_text ltx_font_bold" id="S5.T5.4.10.9.6.1">0.245</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.11.10">
<td class="ltx_td ltx_align_left" id="S5.T5.4.11.10.1">MedFuse</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.11.10.2">%50</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.11.10.3">0.717</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.11.10.4">0.363</td>
<td class="ltx_td ltx_align_left" id="S5.T5.4.11.10.5">0.546</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T5.4.11.10.6">0.204</td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.12.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.12.11.1">Attention</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.12.11.2">%60</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.12.11.3"><span class="ltx_text ltx_font_bold" id="S5.T5.4.12.11.3.1">0.689</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.12.11.4"><span class="ltx_text ltx_font_bold" id="S5.T5.4.12.11.4.1">0.334</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.12.11.5"><span class="ltx_text ltx_font_bold" id="S5.T5.4.12.11.5.1">0.540</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T5.4.12.11.6"><span class="ltx_text ltx_font_bold" id="S5.T5.4.12.11.6.1">0.195</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.13.12">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T5.4.13.12.1">MedFuse</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T5.4.13.12.2">%60</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T5.4.13.12.3">0.674</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T5.4.13.12.4">0.315</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T5.4.13.12.5">0.510</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_b" id="S5.T5.4.13.12.6">0.153</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T6.2.1.1" style="font-size:90%;">TABLE VI</span>: </span><span class="ltx_text" id="S5.T6.3.2" style="font-size:90%;">Task-wise uncertainty impacts.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.4.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S5.T6.4.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T6.4.1.1.2">Attention with Uncertainty</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T6.4.1.1.3">Attention without Uncertainty</th>
</tr>
<tr class="ltx_tr" id="S5.T6.4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T6.4.2.2.1">Phenotype</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.4.2.2.2">AUROC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.4.2.2.3">AUPRC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.4.2.2.4">AUROC</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.4.2.2.5">AUPRC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.4.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T6.4.3.1.1">Acute and unspecified renal failure</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.4.3.1.2">0.793</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.4.3.1.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.3.1.3.1">0.592</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.4.3.1.4">0.793</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.4.3.1.5">0.590</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.4.2">
<td class="ltx_td ltx_align_left" id="S5.T6.4.4.2.1">Acute cerebrovascular disease</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.4.2.2">0.908</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.4.2.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.4.2.3.1">0.468</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.4.2.4">0.908</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.4.2.5">0.462</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.5.3">
<td class="ltx_td ltx_align_left" id="S5.T6.4.5.3.1">Acute myocardial infarction</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.5.3.2">0.760</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.5.3.3">0.216</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.5.3.4">0.762</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.5.3.5">0.221</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.6.4">
<td class="ltx_td ltx_align_left" id="S5.T6.4.6.4.1">Cardiac dysrhythmias</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.6.4.2">0.681</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.6.4.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.6.4.3.1">0.484</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.6.4.4">0.681</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.6.4.5">0.482</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.7.5">
<td class="ltx_td ltx_align_left" id="S5.T6.4.7.5.1">Chronic kidney disease</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.7.5.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.7.5.2.1">0.746</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.7.5.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.7.5.3.1">0.439</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.7.5.4">0.736</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.7.5.5">0.429</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.8.6">
<td class="ltx_td ltx_align_left" id="S5.T6.4.8.6.1">Chronic obstructive pulmonary disease and bronchiectasis</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.8.6.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.8.6.2.1">0.704</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.8.6.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.8.6.3.1">0.292</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.8.6.4">0.701</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.8.6.5">0.289</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.9.7">
<td class="ltx_td ltx_align_left" id="S5.T6.4.9.7.1">Complications of surgical procedures or medical care</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.9.7.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.9.7.2.1">0.731</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.9.7.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.9.7.3.1">0.406</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.9.7.4">0.730</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.9.7.5">0.405</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.10.8">
<td class="ltx_td ltx_align_left" id="S5.T6.4.10.8.1">Conduction disorders</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.10.8.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.10.8.2.1">0.718</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.10.8.3">0.246</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.10.8.4">0.717</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.10.8.5">0.248</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.11.9">
<td class="ltx_td ltx_align_left" id="S5.T6.4.11.9.1">Congestive heart failure; nonhypertensive</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.11.9.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.11.9.2.1">0.765</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.11.9.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.11.9.3.1">0.522</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.11.9.4">0.760</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.11.9.5">0.516</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.12.10">
<td class="ltx_td ltx_align_left" id="S5.T6.4.12.10.1">Coronary atherosclerosis and other heart disease</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.12.10.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.12.10.2.1">0.761</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.12.10.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.12.10.3.1">0.591</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.12.10.4">0.760</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.12.10.5">0.588</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.13.11">
<td class="ltx_td ltx_align_left" id="S5.T6.4.13.11.1">Diabetes mellitus with complications</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.13.11.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.13.11.2.1">0.900</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.13.11.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.13.11.3.1">0.595</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.13.11.4">0.897</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.13.11.5">0.592</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.14.12">
<td class="ltx_td ltx_align_left" id="S5.T6.4.14.12.1">Diabetes mellitus without complication</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.14.12.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.14.12.2.1">0.789</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.14.12.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.14.12.3.1">0.413</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.14.12.4">0.787</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.14.12.5">0.406</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.15.13">
<td class="ltx_td ltx_align_left" id="S5.T6.4.15.13.1">Disorders of lipid metabolism</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.15.13.2">0.704</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.15.13.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.15.13.3.1">0.619</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.15.13.4">0.706</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.15.13.5">0.617</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.16.14">
<td class="ltx_td ltx_align_left" id="S5.T6.4.16.14.1">Essential hypertension</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.16.14.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.16.14.2.1">0.668</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.16.14.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.16.14.3.1">0.579</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.16.14.4">0.660</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.16.14.5">0.570</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.17.15">
<td class="ltx_td ltx_align_left" id="S5.T6.4.17.15.1">Fluid and electrolyte disorders</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.17.15.2">0.759</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.17.15.3">0.644</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.17.15.4">0.760</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.17.15.5">0.644</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.18.16">
<td class="ltx_td ltx_align_left" id="S5.T6.4.18.16.1">Gastrointestinal hemorrhage</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.18.16.2">0.772</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.18.16.3">0.213</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.18.16.4">0.772</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.18.16.5">0.215</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.19.17">
<td class="ltx_td ltx_align_left" id="S5.T6.4.19.17.1">Hypertension with complications and secondary hypertension</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.19.17.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.19.17.2.1">0.738</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.19.17.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.19.17.3.1">0.432</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.19.17.4">0.728</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.19.17.5">0.421</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.20.18">
<td class="ltx_td ltx_align_left" id="S5.T6.4.20.18.1">Other liver diseases</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.20.18.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.20.18.2.1">0.741</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.20.18.3">0.297</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.20.18.4">0.737</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.20.18.5">0.307</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.21.19">
<td class="ltx_td ltx_align_left" id="S5.T6.4.21.19.1">Other lower respiratory disease</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.21.19.2">0.655</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.21.19.3">0.165</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.21.19.4">0.657</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.21.19.5">0.169</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.22.20">
<td class="ltx_td ltx_align_left" id="S5.T6.4.22.20.1">Other upper respiratory disease</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.22.20.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.22.20.2.1">0.752</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.22.20.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.22.20.3.1">0.261</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.22.20.4">0.747</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.22.20.5">0.249</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.23.21">
<td class="ltx_td ltx_align_left" id="S5.T6.4.23.21.1">Pleurisy; pneumothorax; pulmonary collapse</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.23.21.2">0.708</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.23.21.3">0.155</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.23.21.4">0.714</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.23.21.5">0.158</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.24.22">
<td class="ltx_td ltx_align_left" id="S5.T6.4.24.22.1">Pneumonia</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.24.22.2">0.<span class="ltx_text ltx_font_bold" id="S5.T6.4.24.22.2.1">813</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.24.22.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.24.22.3.1">0.384</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.24.22.4">0.811</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.24.22.5">0.382</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.25.23">
<td class="ltx_td ltx_align_left" id="S5.T6.4.25.23.1">Respiratory failure; insufficiency; arrest (adult)</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.25.23.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.25.23.2.1">0.872</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.25.23.3">0.565</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.25.23.4">0.871</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.25.23.5">0.565</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.26.24">
<td class="ltx_td ltx_align_left" id="S5.T6.4.26.24.1">Septicemia (except in labor)</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.26.24.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.26.24.2.1">0.846</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.26.24.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.26.24.3.1">0.522</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.26.24.4">0.842</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.26.24.5">0.512</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.27.25">
<td class="ltx_td ltx_align_left" id="S5.T6.4.27.25.1">Shock</td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.27.25.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.27.25.2.1">0.890</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.4.27.25.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.27.25.3.1">0.569</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.4.27.25.4">0.888</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.4.27.25.5">0.552</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.28.26">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T6.4.28.26.1">Average</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T6.4.28.26.2"><span class="ltx_text ltx_font_bold" id="S5.T6.4.28.26.2.1">0.767</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T6.4.28.26.3"><span class="ltx_text ltx_font_bold" id="S5.T6.4.28.26.3.1">0.427</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T6.4.28.26.4">0.765</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S5.T6.4.28.26.5">0.424</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="402" id="S5.F4.g1" src="extracted/5618045/robust3.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.3.2" style="font-size:90%;">Performance comparison of models trained on noisy or noise-free datasets, and evaluated on noisy datasets. The plot employs different colors to represent specific configurations:
blue indicates the attention-based fusion model trained on noisy data; red shows the MedFuse model trained on noisy data; green denotes the attention-based fusion trained on noise-free data; and orange represents the MedFuse model trained on noise-free data. As noise levels increase, a general decline in performance is observed for all models across various metrics. Notably, the use of attention mechanisms appears to mitigate performance degradation, showcasing enhanced robustness against noise.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.4.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.5.2">Phenotype Classification</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">In the domain of phenotyping, our model is evaluated against baselines, including MedFuse, CARD, and Contrastive learning. Table <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.T2" title="TABLE II ‣ V-C In-Hospital Mortality Prediction ‣ V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">II</span></a> illustrates the model’s proficiency in addressing the complex task of multi-label phenotyping. Metrics such as macro average F1-score, binary F1-score, AUROC, and AUPRC are employed to evaluate the model’s accuracy in capturing diverse phenotypic characteristics. The results showcase the model’s ability to handle the intricacies of multi-label phenotyping, outperforming the baselines.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">When we compare the models, we find that the MedFuse model performs better than the CARD model in all metrics, with the exception of the binary F1-score. Our attention-based model trained on images augmented by the CLAHE filter surpasses CARD and MedFuse in macro average F1-score and AUROC but it has a lower binary F1-score compared to CARD. When we optimize our model with the uncertainty loss it outperforms all of our baselines in all metrics except for AUROC, where it appears that combining uncertainty loss with the CLAHE augmentation slightly decreases the performance of our model. It’s important to note that all the best results across various metrics were achieved by our models.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">The objective of this experiment is to predict 25 different conditions given to patients during the length of their ICU stay. We utilize time series data and chest X-ray images for this multi-label classification task.
There are 42628 samples in our training set, 4802 samples in our validation set and 11914 samples in our test set. All samples include time series data, but 7756, 882 and 2166 samples have both modalities in the training, validation and test sets. The rest of the samples in each set have missing chest X-ray images.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS5.4.1.1">V-E</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS5.5.2">Ablation Study</span>
</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">The ablation study examines the model’s architecture, exploring distinctive configurations to understand their impact. These configurations include variations such as using time series only, images only and fusing image and time series data through LSTM or attention mechanisms. The results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.T3" title="TABLE III ‣ V-C In-Hospital Mortality Prediction ‣ V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">III</span></a>, providing insights into the contributions of the model parts.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">In our primary observation, the LSTM-fused multimodal model demonstrates superior performance in phenotype classification compared to uni-modal models across all metrics. This outcome underscores the effectiveness of the multimodal approach in fusing the information derived from both modalities for classification purposes. Subsequently, the attention-based fusion model surpasses the LSTM-fused model, emphasizing the contribution of the transformer layers in enhancing the multimodal model’s classification performance.</p>
</div>
<div class="ltx_para" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.1">Incorporating uncertainty loss into the model yields improved performance, particularly evident in AUPRC and AUROC metrics, without significantly impacting the macro F1-score. A minor decrease (approximately 0.003) is observed in the binary F1-score. In the end, we analyzed the impact of the CLAHE filter on CXR images. Results indicate that using CLAHE improves the performance of our attention-based model across all metrics. Also, adding this filter increases our attention-based uncertainty model’s performance across all metrics except for AUROC.</p>
</div>
<div class="ltx_para" id="S5.SS5.p4">
<p class="ltx_p" id="S5.SS5.p4.1">In summary, the comprehensive analysis presented in this table highlights the positive impact of the modules and methods employed, collectively contributing to improved model performance, particularly evident in terms of AUPRC and AUROC metrics.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS6.4.1.1">V-F</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS6.5.2">Task-wise Uncertainties</span>
</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">To understand the importance of utilizing homoscedastic uncertainty in our multi-label phenotyping task, we have compared the performance of our attention-based model fine-tuned using the uncertainty loss with the same model fine-tuned using the Cross-Entropy loss.</p>
</div>
<div class="ltx_para" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.1">As you can see in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.T6" title="TABLE VI ‣ V-C In-Hospital Mortality Prediction ‣ V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">VI</span></a>, The uncertainty loss not only enhances the average performance of our proposed framework but more importantly, this loss function increases the individual performance on most labels in our multi-label setup. This loss function allows our model to focus more on tasks that are easier to predict and have less uncertainty without causing a significant decrease in performance in the less uncertain and more complicated tasks, thus achieving a higher average performance. Although the overall improvements might not be considered drastic, the constant improvement in different tasks, especially those that had high performances prior to using the uncertainty loss, further supports the claim that the loss function makes our model more flexible and allows it to focus on more certain tasks, and indicates the importance of using this loss function in our multi-label setup.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS7.4.1.1">V-G</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS7.5.2">Robustness</span>
</h3>
<div class="ltx_para" id="S5.SS7.p1">
<p class="ltx_p" id="S5.SS7.p1.1">In order to explore the robustness of our attention-based model, we compared the performance of our model against MedFuse in noisy configurations. To do so, we prepared a noisy version of the multimodal MIMIC dataset. Table <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.T4" title="TABLE IV ‣ V-C In-Hospital Mortality Prediction ‣ V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">IV</span></a> presents the results across various noise levels, ranging from 10% to 60%, on both the training and testing sets. In this experiment, we subject all samples to varying levels of noise. For instance, in the case of images, we introduce noise by perturbing 10% of the pixels within the data. Similarly, for the time-series data, we apply noise to 10% of the time steps in each sample. The introduced noise is Gaussian and its mean and standard deviation are estimated by measuring these parameters in 1000 random samples of the data. These parameters are calculated individually for each feature in the time-series data and all pixels in the images.</p>
</div>
<div class="ltx_para" id="S5.SS7.p2">
<p class="ltx_p" id="S5.SS7.p2.1">Two different modes are considered for the evaluation process. In one, models are trained and tested on noisy datasets. In the other, testing is conducted on noisy datasets without any prior training on noisy datasets. The latter scenario is very common in real-world applications, where unexpected noise is often encountered in the data. The results for each setting are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.T4" title="TABLE IV ‣ V-C In-Hospital Mortality Prediction ‣ V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">IV</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.T5" title="TABLE V ‣ V-C In-Hospital Mortality Prediction ‣ V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">V</span></a>, while the corresponding Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.15442v1#S5.F4" title="Figure 4 ‣ V-C In-Hospital Mortality Prediction ‣ V Experiments and Results ‣ Towards Precision Healthcare: Robust Fusion of Time Series and Image Data"><span class="ltx_text ltx_ref_tag">4</span></a> mirrors the tabulated results. It is evident that as noise levels increase, the model accuracy decreases. However, the attention-based fusion results in superior overall performance compared to the MedFuse model.
<br class="ltx_break"/>Unlike the MedFuse model, which exhibits significant performance degradation in the presence of noise, our attention-based model demonstrates high levels of robustness. For instance, even at 60% noise levels, the decrease in performance is minimal, with results showcasing as little as a 2% reduction in performance. These results underscore the efficacy of our attention mechanism in mitigating the effects of noise, ensuring consistent and reliable performance across diverse environmental conditions.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our research introduces an innovative approach to multimodal deep neural networks, specifically designed for integrating heterogeneous data modalities such as images and time series data in mortality prediction and phenotyping label assignments. By employing dedicated encoders for each modality, our model effectively captures nuanced patterns inherent in both visual and temporal information, thus enhancing predictive capabilities. Our experiments conducted under noisy settings demonstrate the robustness of our model, surpassing state-of-the-art methods and showcasing its efficacy in handling real-world challenges with noisy clinical data. Additionally, our innovative use of an uncertainty loss function addresses the complexity of multi-label classification, contributing to improved model performance. Furthermore, the integration of attention mechanisms for modality fusion enhances adaptability by dynamically allocating attention based on task relevance. In summary, our research advances robust multimodal deep learning for clinical applications, offering a flexible and robust framework capable of addressing challenges in real-world clinical data, with promising outcomes for clinical decision support systems.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Looking ahead, several promising directions for future research emerge from the outcomes of this study. First and foremost, enhancing the interpretability of the multimodal deep neural network remains a critical area of investigation. Developing methodologies that shed light on the decision-making process of the model will be essential for building trust in clinical applications. This may involve exploring novel visualization techniques or model-agnostic interpretability tools to dissect how the network integrates information from diverse modalities, providing clinicians with valuable insights into its decision rationale.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Our research sets the stage for exploring the integration of additional data modalities into our multimodal deep neural network framework. While our current focus has been on combining images and time series data for mortality prediction and phenotyping, there’s immense potential in extending our approach to include other modalities such as textual data or genomic information. By incorporating these additional modalities, we can investigate how our model performs across a broader spectrum of clinical data and further enhance its predictive capabilities. This exploration opens up avenues for understanding how different types of data interact and contribute to the overall predictive power of our model, helping to have more comprehensive and robust clinical decision support systems.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng.

</span>
<span class="ltx_bibblock">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:1901.07042</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al.

</span>
<span class="ltx_bibblock">Mimic-iv, a freely accessible electronic health record dataset.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Scientific data</span>, 10(1):1, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Nasir Hayat, Krzysztof J Geras, and Farah E Shamout.

</span>
<span class="ltx_bibblock">Medfuse: Multi-modal fusion with clinical time-series data and chest x-ray images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Machine Learning for Healthcare Conference</span>, pages 479–503. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Nasir Rahim, Shaker El-Sappagh, Sajid Ali, Khan Muhammad, Javier Del Ser, and Tamer Abuhmed.

</span>
<span class="ltx_bibblock">Prediction of alzheimer’s progression based on multimodal deep-learning-based fusion and visual explainability of time-series data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Information Fusion</span>, 92:363–388, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ke Niu, Ke Zhang, Xueping Peng, Yijie Pan, and Naian Xiao.

</span>
<span class="ltx_bibblock">Deep multi-modal intermediate fusion of clinical record and time series data in mortality prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Frontiers in Molecular Biosciences</span>, 10:1136071, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Luis R Soenksen, Yu Ma, Cynthia Zeng, Leonard Boussioux, Kimberly Villalobos Carballo, Liangyuan Na, Holly M Wiberg, Michael L Li, Ignacio Fuentes, and Dimitris Bertsimas.

</span>
<span class="ltx_bibblock">Integrated multimodal artificial intelligence framework for healthcare applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">NPJ digital medicine</span>, 5(1):149, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Zhi Qiao, Xian Wu, Shen Ge, and Wei Fan.

</span>
<span class="ltx_bibblock">Mnn: multimodal attentional neural networks for diagnosis prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Extraction</span>, 1(2019):A1, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chihcheng Hsieh, Isabel Blanco Nobre, Sandra Costa Sousa, Chun Ouyang, Margot Brereton, Jacinto C Nascimento, Joaquim Jorge, and Catarina Moreira.

</span>
<span class="ltx_bibblock">Mdf-net: Multimodal dual-fusion network for abnormality detection using cxr images and clinical data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2302.13390</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu, and Chen-Yu Lee.

</span>
<span class="ltx_bibblock">Multimodal prompting with missing modalities for visual recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 14943–14952, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Chaohe Zhang, Xu Chu, Liantao Ma, Yinghao Zhu, Yasha Wang, Jiangtao Wang, and Junfeng Zhao.

</span>
<span class="ltx_bibblock">M3care: Learning with missing modalities in multimodal healthcare data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</span>, pages 2418–2428, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Qi Wang, Liang Zhan, Paul Thompson, and Jiayu Zhou.

</span>
<span class="ltx_bibblock">Multimodal learning with incomplete modalities by knowledge distillation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</span>, pages 1828–1838, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, and Xi Peng.

</span>
<span class="ltx_bibblock">Are multimodal transformers robust to missing modality?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 18177–18186, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Dong Nie, Han Zhang, Ehsan Adeli, Luyan Liu, and Dinggang Shen.

</span>
<span class="ltx_bibblock">3d deep learning for multi-modal imaging-guided survival time prediction of brain tumor patients.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19</span>, pages 212–220. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Dong Nie, Junfeng Lu, Han Zhang, Ehsan Adeli, Jun Wang, Zhengda Yu, LuYan Liu, Qian Wang, Jinsong Wu, and Dinggang Shen.

</span>
<span class="ltx_bibblock">Multi-channel 3d deep feature learning for survival time prediction of brain tumor patients using multi-modal neuroimages.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Scientific reports</span>, 9(1):1103, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
B Srinivas and Gottapu Sasibhushana Rao.

</span>
<span class="ltx_bibblock">Segmentation of multi-modal mri brain tumor sub-regions using deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Journal of Electrical Engineering &amp; Technology</span>, 15(4):1899–1909, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Debendra Muduli, Ratnakar Dash, and Banshidhar Majhi.

</span>
<span class="ltx_bibblock">Automated diagnosis of breast cancer using multi-modal datasets: A deep convolution neural network based approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Biomedical Signal Processing and Control</span>, 71:102825, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Muxuan Liang, Zhizhong Li, Ting Chen, and Jianyang Zeng.

</span>
<span class="ltx_bibblock">Integrative data analysis of multi-platform cancer data with a multimodal deep learning approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">IEEE/ACM transactions on computational biology and bioinformatics</span>, 12(4):928–937, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Dongdong Sun, Minghui Wang, and Ao Li.

</span>
<span class="ltx_bibblock">A multimodal deep neural network for human breast cancer prognosis prediction by integrating multi-dimensional data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">IEEE/ACM transactions on computational biology and bioinformatics</span>, 16(3):841–850, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Sunghoon Joo, Eun Sook Ko, Soonhwan Kwon, Eunjoo Jeon, Hyungsik Jung, Ji-Yeon Kim, Myung Jin Chung, and Young-Hyuck Im.

</span>
<span class="ltx_bibblock">Multimodal deep learning models for the prediction of pathologic response to neoadjuvant chemotherapy in breast cancer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Scientific reports</span>, 11(1):18800, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Rayyan Azam Khan, Minghan Fu, Brent Burbridge, Yigang Luo, and Fang-Xiang Wu.

</span>
<span class="ltx_bibblock">A multi-modal deep neural network for multi-class liver cancer diagnosis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Neural Networks</span>, 165:553–561, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Zexian Zeng, Liang Yao, Ankita Roy, Xiaoyu Li, Sasa Espino, Susan E Clare, Seema A Khan, and Yuan Luo.

</span>
<span class="ltx_bibblock">Identifying breast cancer distant recurrences from electronic health records using machine learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Journal of healthcare informatics research</span>, 3:283–299, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Gaspard Harerimana, Jong Wook Kim, Hoon Yoo, and Beakcheol Jang.

</span>
<span class="ltx_bibblock">Deep learning for electronic health records analytics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">IEEE Access</span>, 7:101245–101259, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jouhyun Jeon, Peter J Leimbigler, Gaurav Baruah, Michael H Li, Yan Fossat, and Alfred J Whitehead.

</span>
<span class="ltx_bibblock">Predicting glycaemia in type 1 diabetes patients: experiments in feature engineering and data imputation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Journal of healthcare informatics research</span>, 4(1):71–90, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Sebastian Daberdaku, Erica Tavazzi, and Barbara Di Camillo.

</span>
<span class="ltx_bibblock">A combined interpolation and weighted k-nearest neighbours approach for the imputation of longitudinal icu laboratory data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Journal of Healthcare Informatics Research</span>, 4:174–188, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Dimitrios Zikos, Aashara Shrestha, and Leonidas Fegaras.

</span>
<span class="ltx_bibblock">A cross-sectional study to predict mortality for medicare patients based on the combined use of hcup tools.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Journal of Healthcare Informatics Research</span>, pages 1–19, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Taiyu Zhu, Kezhi Li, Jianwei Chen, Pau Herrero, and Pantelis Georgiou.

</span>
<span class="ltx_bibblock">Dilated recurrent neural networks for glucose forecasting in type 1 diabetes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Journal of Healthcare Informatics Research</span>, 4:308–324, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Hazal Türkmen, Oğuz Dikenelli, Cenk Eraslan, Mehmet Cem Callı, and Süha Süreyya Özbek.

</span>
<span class="ltx_bibblock">Bioberturk: Exploring turkish biomedical language model development strategies in low-resource setting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Journal of Healthcare Informatics Research</span>, pages 1–14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Maurizio Balbi, Anna Caroli, Andrea Corsi, Gianluca Milanese, Alessandra Surace, Fabiano Di Marco, Luca Novelli, Mario Silva, Ferdinando Luca Lorini, Andrea Duca, et al.

</span>
<span class="ltx_bibblock">Chest x-ray for predicting mortality and the need for ventilatory support in covid-19 patients presenting to the emergency department.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">European radiology</span>, 31:1999–2012, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Ayman Mir and Sudhir N Dhage.

</span>
<span class="ltx_bibblock">Diabetes disease prediction using machine learning on big data of healthcare.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">2018 fourth international conference on computing communication control and automation (ICCUBEA)</span>, pages 1–6. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Niharika G Maity and Sreerupa Das.

</span>
<span class="ltx_bibblock">Machine learning for improved diagnosis and prognosis in healthcare.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">2017 IEEE aerospace conference</span>, pages 1–9. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Wasudeo Rahane, Himali Dalvi, Yamini Magar, Anjali Kalane, and Satyajeet Jondhale.

</span>
<span class="ltx_bibblock">Lung cancer detection using image processing and machine learning healthcare.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">2018 International Conference on Current Trends towards Converging Technologies (ICCTCT)</span>, pages 1–5. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Vyshali J Gogi and MN Vijayalakshmi.

</span>
<span class="ltx_bibblock">Prognosis of liver disease: Using machine learning algorithms.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">2018 International Conference on Recent Innovations in Electrical, Electronics &amp; Communication Engineering (ICRIEECE)</span>, pages 875–879. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Shuai Niu, Qing Yin, Yunya Song, Yike Guo, and Xian Yang.

</span>
<span class="ltx_bibblock">Label dependent attention model for disease risk prediction using multimodal electronic health records.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">2021 IEEE International Conference on Data Mining (ICDM)</span>, pages 449–458. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Grzegorz Jacenków, Alison Q O’Neil, and Sotirios A Tsaftaris.

</span>
<span class="ltx_bibblock">Indication as prior knowledge for multimodal disease classification in chest radiographs with transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)</span>, pages 1–5. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Grigor Bezirganyan, Sana Sellami, Laure Berti-ÉQuille, and Sébastien Fournier.

</span>
<span class="ltx_bibblock">M2-mixer: A multimodal mixer with multi-head loss for classification from multimodal data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">2023 IEEE International Conference on Big Data (BigData)</span>, pages 1052–1058. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Aixia Guo, Nikhilesh R Mazumder, Daniela P Ladner, and Randi E Foraker.

</span>
<span class="ltx_bibblock">Predicting mortality among patients with liver cirrhosis in electronic health records with machine learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">PloS one</span>, 16(8):e0256428, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Mohammod NI Suvon, Prasun C Tripathi, Samer Alabed, Andrew J Swift, and Haiping Lu.

</span>
<span class="ltx_bibblock">Multimodal learning for predicting mortality in patients with pulmonary arterial hypertension.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</span>, pages 2704–2710. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Garima Yadav, Saurabh Maheshwari, and Anjali Agarwal.

</span>
<span class="ltx_bibblock">Contrast limited adaptive histogram equalization based enhancement for real time video system.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)</span>, pages 2392–2397, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Advances in Neural Information Processing Systems</span>, volume 30. Curran Associates, Inc., 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 770–778, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Neural Computation</span>, 9(8):1735–1780, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">CoRR</span>, abs/1412.6980, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Alex Kendall, Yarin Gal, and Roberto Cipolla.

</span>
<span class="ltx_bibblock">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 7482–7491, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual representations, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael Maire, Ajinkya Kale, and Baldo Faieta.

</span>
<span class="ltx_bibblock">Multimodal contrastive training for visual representation learning, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Xizewen Han, Huangjie Zheng, and Mingyuan Zhou.

</span>
<span class="ltx_bibblock">Card: Classification and regression diffusion models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Advances in Neural Information Processing Systems</span>, 35:18100–18115, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 24 13:45:07 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
