<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer</title>
<!--Generated on Mon Jul  1 08:33:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="" lang="en" name="keywords"/>
<base href="/html/2407.04734v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S1" title="In Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2" title="In Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.SS1" title="In II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Activity Recognition Approaches</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.SS2" title="In II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.SS3" title="In II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Dataset Pre-Processing and VAE Architectures</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3" title="In Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.SS1" title="In III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Primer on Neuro-Symbolic AI: the DeepProbLog Approach</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.SS2" title="In III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Domain-Dependent Rule from Different Modality</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.SS3" title="In III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_sansserif">DeepProbHAR</span><span class="ltx_text ltx_font_italic">: A Neuro-Symbolic Architecture for Human Activity Recognition Using Wi-Fi Data</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4" title="In Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experimental Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.SS1" title="In IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Validation of Declarative Knowledge</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.SS2" title="In IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Performance of <span class="ltx_text ltx_font_sansserif ltx_font_upright">DeepProbHAR</span></span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S5" title="In Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S6" title="In Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" lang="en-US">Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en-US">

Marco Cominelli1,
Francesco Gringoli2,
Lance M. Kaplan3,
Mani B. Srivastava4,
<br class="ltx_break"/>Trevor Bihl6,
Erik P. Blasch6,
Nandini Iyer6,
and Federico Cerutti2

<br class="ltx_break"/>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation" lang="en-US">
1DEIB,
Politecnico di Milano, Italy.
marco.cominelli@polimi.it

</span>
<span class="ltx_contact ltx_role_affiliation" lang="en-US">
2DII,
University of Brescia, Italy.
{francesco.gringoli, federico.cerutti}@unibs.it

</span>
<span class="ltx_contact ltx_role_affiliation" lang="en-US">
3DEVCOM Army Research Lab, USA.
lance.m.kaplan.civ@army.mil

</span>
<span class="ltx_contact ltx_role_affiliation" lang="en-US">
4ECE Department,
University of California, Los Angeles, USA.
mbs@ucla.edu

</span>
<span class="ltx_contact ltx_role_affiliation" lang="en-US">
6Air Force Research Laboratory, USA.
{trevor.bihl.2, erik.blasch.1, nandini.iyer.2}@us.af.mil

</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id8.id1"><span class="ltx_text" id="id8.id1.1" lang="en-US">Wi-Fi</span><span class="ltx_text" id="id8.id1.2" lang="en-US"> devices, akin to passive radars, can discern human activities within indoor settings due to the human bodyâ€™s interaction with electromagnetic signals. Current <span class="ltx_text" id="id8.id1.2.1">Wi-Fi</span> sensing applications predominantly employ data-driven learning techniques to associate the fluctuations in the physical properties of the communication channel with the human activity causing them. However, these techniques often lack the desired flexibility and transparency.
This paper introduces <span class="ltx_text ltx_font_sansserif" id="id8.id1.2.2">DeepProbHAR</span>, a neuro-symbolic architecture for <span class="ltx_text" id="id8.id1.2.3">Wi-Fi</span> sensing, providing initial evidence that <span class="ltx_text" id="id8.id1.2.4">Wi-Fi</span> signals can differentiate between simple movements, such as leg or arm movements, which are integral to human activities like running or walking. The neuro-symbolic approach affords gathering such evidence without needing additional specialised data collection or labelling.
The training of <span class="ltx_text ltx_font_sansserif" id="id8.id1.2.5">DeepProbHAR</span> is facilitated by declarative domain knowledge obtained from a camera feed and by fusing signals from various antennas of the <span class="ltx_text" id="id8.id1.2.6">Wi-Fi</span> receivers.
<span class="ltx_text ltx_font_sansserif" id="id8.id1.2.7">DeepProbHAR</span> achieves results comparable to the state-of-the-art in human activity recognition. Moreover, as a by-product of the learning process, <span class="ltx_text ltx_font_sansserif" id="id8.id1.2.8">DeepProbHAR</span> generates specialised classifiers for simple movements that match the accuracy of models trained on finely labelled datasets, which would be particularly costly.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
<span class="ltx_text" id="id9.id1" lang="en-US">
neuro-symbolic AI, data fusion, Wi-Fi sensing
</span>
</div>
<section class="ltx_section" id="S1" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_text" id="S1.p1.1.1">Wi-Fi</span> devices can be used as <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">passive radars</span> to recognise specific human activities in indoor environments because of the physical interaction of the human body with communication signalsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib1" title="">1</a>]</cite>.
In <span class="ltx_text" id="S1.p1.1.3">Wi-Fi</span>, the <span class="ltx_glossaryref" title="">channel state information (CSI)</span> is a complex-valued vector computed at the receiver for every incoming frame that measures the wireless channelâ€™s properties and equalises the received signal.
However, the <span class="ltx_glossaryref" title="">CSI</span> also provides an electromagnetic fingerprint of the environment.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S1.F1" title="In I Introduction â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> (top) illustrates a snippet of the <span class="ltx_glossaryref" title="">CSI</span> captured by one single antenna while a person runs.
As the person moves around the room, the environmentâ€™s effect on the signal changes due to the varying scattering on the human body.
The result is captured in a <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">spectrogram</em> that highlights how the relative intensity of the signal changes over time and frequency.
The fundamental assumption of <span class="ltx_glossaryref" title="">CSI</span>-based <span class="ltx_glossaryref" title="">human activity recognition (HAR)</span> is that it is possible to trace these variations back to the human activity that caused them, and in particular, to distinguish different types of activities, like running instead of standing still and clapping, <span class="ltx_ERROR undefined" id="S1.p2.1.2">\cf</span><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S1.F1" title="In I Introduction â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> (bottom).
However, such <span class="ltx_text" id="S1.p2.1.3">Wi-Fi</span> sensing applications employ techniques that often lack the desired flexibility and transparency.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="598" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.4.2" style="font-size:90%;">Magnitude of the <span class="ltx_glossaryref" title="">CSI</span> collected by the same antenna when a person performs two different activities, namely running (top) and clapping (bottom). <span class="ltx_glossaryref" title="">CSI</span> values are dimensionless and are reported as measured by the <span class="ltx_text" id="S1.F1.4.2.1">Wi-Fi</span> chipset.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we introduce <span class="ltx_text ltx_font_sansserif" id="S1.p3.1.1">DeepProbHAR</span>, a neuro-symbolic approach to <span class="ltx_glossaryref" title="">HAR</span> using a passive <span class="ltx_text" id="S1.p3.1.2">Wi-Fi</span> radar, providing initial evidence that <span class="ltx_text" id="S1.p3.1.3">Wi-Fi</span> signals can differentiate between simple movements, such as leg or arm movements, which are integral to human activities like running or walking.
<span class="ltx_text ltx_font_sansserif" id="S1.p3.1.4">DeepProbHAR</span> builds on top of a recent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite> pre-processed dataset of human activities sensed through commercial <span class="ltx_text" id="S1.p3.1.5">Wi-Fi</span> devices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib3" title="">3</a>]</cite>, that makes use of <span class="ltx_glossaryref" title="">Variational Auto-Encoders (VAEs)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib4" title="">4</a>]</cite> to identify a generative latent distribution seen as a compressed view of the original <span class="ltx_glossaryref" title="">CSI</span> signal.
Specifically, the paper discusses (<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2" title="II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">II</span></a>) the differences between the two main approaches to <span class="ltx_glossaryref" title="">HAR</span>: declarative and data-driven. Declarative approaches provide classification rules for defining activities but struggle with unstructured data â€“ indeed, to our knowledge, they have not been proposed for <span class="ltx_glossaryref" title="">CSI</span> data; while data-driven approaches handle complex data types but are less flexible and more opaque. The paper expands on relevant references to the literature concerning data-driven approaches, including a description of our previous work published in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite>, which provides a principled way to compress the <span class="ltx_glossaryref" title="">CSI</span> data and several architectures to fuse the signals coming from the different <span class="ltx_text" id="S1.p3.1.6">Wi-Fi</span> antennas of the passive radar.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">A third method for <span class="ltx_glossaryref" title="">HAR</span>, provided by neuro-symbolic approaches (<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3" title="III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">III</span></a>), combines symbolic (declarative) reasoning techniques and <span class="ltx_glossaryref" title="">neural network (NN)</span> methods, aiming to improve the performance of AI systemsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib5" title="">5</a>]</cite>.
Neuro-symbolic systems can merge the approximation capabilities of <span class="ltx_glossaryref" title="">NNs</span> with the abstract reasoning abilities of symbolic methods, enabling them to extrapolate from limited data and produce interpretable results.
In particular, we extract declarative knowledge (a decision tree) of the human activities from a video feed captured by a camera observing the same environment as the <span class="ltx_text" id="S1.p4.1.1">Wi-Fi</span> receiver senses.
We then transfer such knowledge to train with just the label of the activity <span class="ltx_text ltx_font_sansserif" id="S1.p4.1.2">DeepProbHAR</span>, a neuro-symbolic architecture seeing different data modality (the <span class="ltx_glossaryref" title="">CSI</span>) and that builds on top of DeepProbLog <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our experimental results (<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4" title="IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">IV</span></a>) demonstrate that <span class="ltx_text ltx_font_sansserif" id="S1.p5.1.1">DeepProbHAR</span> achieves comparable if not better results than the state-of-the-art approaches while at the same time reaching a higher degree of transparency.
In particular, <span class="ltx_text ltx_font_sansserif" id="S1.p5.1.2">DeepProbHAR</span> can identify occurrences of simple movements (such as <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">moving the upper arm</span>) without requiring specific labelling for such concepts.
Finally, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S5" title="V Discussion â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">V</span></a> highlights the ability to distinguish simple movements of the subject.</p>
</div>
</section>
<section class="ltx_section" id="S2" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Activity Recognition Approaches</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">There are two primary approaches to activity recognition: <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">declarative</span> and <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">data-driven</span>.
Declarative approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib10" title="">10</a>]</cite> provide classification rules that can be utilised to define the activity. An example of such a rule â€“ in natural language â€“ could be: <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.3">running is the rapid, alternating action of pushing off and landing on the ground with oneâ€™s feet</span>.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Microsoft Copilot on 14th March 2024.</span></span></span> However, the types of input data declarative approaches can handle are often limited.
Specifically, declarative rules typically require direct processing of the input data, which can pose challenges for unstructured data such as <span class="ltx_text" id="S2.SS1.p1.1.4">Wi-Fi</span> <span class="ltx_glossaryref" title="">CSI</span> (further discussed in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.SS2" title="II-B Dataset â€£ II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>).
Indeed, the authors are unaware of any declarative approaches for <span class="ltx_glossaryref" title="">HAR</span> operating over <span class="ltx_glossaryref" title="">CSI</span> data.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Data-driven approaches (<span class="ltx_ERROR undefined" id="S2.SS1.p2.1.1">\eg</span><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite>) are specifically designed to handle data types for which it is difficult to define rules directly. Despite their advantages, these approaches are more opaque and less flexible than their declarative counterparts, often making it impossible for the systemâ€™s end-user to define patterns entirely.
Indeed, several <span class="ltx_glossaryref" title="">HAR</span> systems work by deriving some physically-related quantity from some sensors (<span class="ltx_ERROR undefined" id="S2.SS1.p2.1.2">\eg</span>the <span class="ltx_glossaryref" title="">CSIs</span>) that is then used to train a deep learning classification system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib13" title="">13</a>]</cite>.
In a previous work, we showed a principled approach to <span class="ltx_glossaryref" title="">HAR</span> using a <span class="ltx_glossaryref" title="">VAE</span> generative model to compress the sensorsâ€™ information and various architectures for fusing multiple antennasâ€™ signalsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Dataset</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In this work, we rely on a <span class="ltx_glossaryref" title="">CSI</span> dataset publicly released by members of the author list.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/ansresearch/exposing-the-csi</span></span></span>
Further details about the dataset are available inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib3" title="">3</a>]</cite>.
The experimental testbed comprises two Asus RT-AX86U devices placed on opposite sides of an approximately 46-square-metre room.
One device generates dummy IEEEÂ 802.11ax (<span class="ltx_text" id="S2.SS2.p1.1.1">Wi-Fi</span>Â 6) traffic at a constant rate of 150 frames per second using the frame injection feature in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib14" title="">14</a>]</cite>.
The other device (also called <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.2">monitor</em>) receives the <span class="ltx_text" id="S2.SS2.p1.1.3">Wi-Fi</span> frames and stores the associated <span class="ltx_glossaryref" title="">CSI</span> for each of its four receiving antennas independently.
Meanwhile, one candidate performs different activities in the middle of the room.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="257" id="S2.F2.g1" src="x2.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.4.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.5.2" style="font-size:90%;">Sample of the video dataset for two different activities: a) <span class="ltx_text ltx_font_italic" id="S2.F2.5.2.1">walking</span> and b) <span class="ltx_text ltx_font_italic" id="S2.F2.5.2.2">waving both hands</span>. The key points in every video frame help to discern the right side (highlighted with coloured dots) from the left side of the candidate.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The <span class="ltx_glossaryref" title="">CSI</span> dataset is coarsely synchronised with a video recording of the activities, collected using a smartphone camera placed in a fixed location and then anonymised.
Specifically, to preserve the participantsâ€™ identity, VideoPose3DÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib15" title="">15</a>]</cite> was used to extract a model of the candidate performing the activities.
VideoPose3D identifies 17 key points to track the motion of the main human joints, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.F2" title="In II-B Dataset â€£ II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.
The key points are stored as a list of <math alttext="(x,y)" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.2"><semantics id="S2.SS2.p2.1.m1.2a"><mrow id="S2.SS2.p2.1.m1.2.3.2" xref="S2.SS2.p2.1.m1.2.3.1.cmml"><mo id="S2.SS2.p2.1.m1.2.3.2.1" stretchy="false" xref="S2.SS2.p2.1.m1.2.3.1.cmml">(</mo><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">x</mi><mo id="S2.SS2.p2.1.m1.2.3.2.2" xref="S2.SS2.p2.1.m1.2.3.1.cmml">,</mo><mi id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">y</mi><mo id="S2.SS2.p2.1.m1.2.3.2.3" stretchy="false" xref="S2.SS2.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.2b"><interval closure="open" id="S2.SS2.p2.1.m1.2.3.1.cmml" xref="S2.SS2.p2.1.m1.2.3.2"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">ğ‘¥</ci><ci id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.2d">( italic_x , italic_y )</annotation></semantics></math> coordinates in the camera viewport for each video frame.
Even though the dataset includes the <span class="ltx_glossaryref" title="">CSI</span> data of twelve different activities, there are seven activities in total for which both <span class="ltx_glossaryref" title="">CSI</span> and video data are available: <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.1">walking, running, jumping, squatting, waving both hands, clapping,</em> and <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.2">wiping</em>.
For each activity, the dataset contains 80 seconds of <span class="ltx_glossaryref" title="">CSI</span> data (sampled at 150 <span class="ltx_glossaryref" title="">CSI</span> per second) and the corresponding video data (<span class="ltx_ERROR undefined" id="S2.SS2.p2.1.3">\ie</span>the key points of the candidate, sampled at 30Â fps).
VideoPose3D can also reconstruct a 3D model of the candidate using a deep learning algorithm; however, we found some numerical instability in the 3D coordinates reconstructed by the tool.
Hence, in this work, we only consider the 2D coordinates of the joints extracted from the original video traces.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Dataset Pre-Processing and VAE Architectures</span>
</h3>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="393" id="S2.F3.g1" src="x3.png" width="681"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.6.3.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.4.2" style="font-size:90%;">The <span class="ltx_glossaryref" title="">VAE</span> models map the input <span class="ltx_glossaryref" title="">CSI</span> data onto the four parameters of a bivariate Gaussian distribution (mean <math alttext="Z_{\mu}" class="ltx_Math" display="inline" id="S2.F3.3.1.m1.1"><semantics id="S2.F3.3.1.m1.1b"><msub id="S2.F3.3.1.m1.1.1" xref="S2.F3.3.1.m1.1.1.cmml"><mi id="S2.F3.3.1.m1.1.1.2" xref="S2.F3.3.1.m1.1.1.2.cmml">Z</mi><mi id="S2.F3.3.1.m1.1.1.3" xref="S2.F3.3.1.m1.1.1.3.cmml">Î¼</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F3.3.1.m1.1c"><apply id="S2.F3.3.1.m1.1.1.cmml" xref="S2.F3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F3.3.1.m1.1.1.1.cmml" xref="S2.F3.3.1.m1.1.1">subscript</csymbol><ci id="S2.F3.3.1.m1.1.1.2.cmml" xref="S2.F3.3.1.m1.1.1.2">ğ‘</ci><ci id="S2.F3.3.1.m1.1.1.3.cmml" xref="S2.F3.3.1.m1.1.1.3">ğœ‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.3.1.m1.1d">Z_{\mu}</annotation><annotation encoding="application/x-llamapun" id="S2.F3.3.1.m1.1e">italic_Z start_POSTSUBSCRIPT italic_Î¼ end_POSTSUBSCRIPT</annotation></semantics></math> and variance <math alttext="Z_{\sigma^{2}}" class="ltx_Math" display="inline" id="S2.F3.4.2.m2.1"><semantics id="S2.F3.4.2.m2.1b"><msub id="S2.F3.4.2.m2.1.1" xref="S2.F3.4.2.m2.1.1.cmml"><mi id="S2.F3.4.2.m2.1.1.2" xref="S2.F3.4.2.m2.1.1.2.cmml">Z</mi><msup id="S2.F3.4.2.m2.1.1.3" xref="S2.F3.4.2.m2.1.1.3.cmml"><mi id="S2.F3.4.2.m2.1.1.3.2" xref="S2.F3.4.2.m2.1.1.3.2.cmml">Ïƒ</mi><mn id="S2.F3.4.2.m2.1.1.3.3" xref="S2.F3.4.2.m2.1.1.3.3.cmml">2</mn></msup></msub><annotation-xml encoding="MathML-Content" id="S2.F3.4.2.m2.1c"><apply id="S2.F3.4.2.m2.1.1.cmml" xref="S2.F3.4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.F3.4.2.m2.1.1.1.cmml" xref="S2.F3.4.2.m2.1.1">subscript</csymbol><ci id="S2.F3.4.2.m2.1.1.2.cmml" xref="S2.F3.4.2.m2.1.1.2">ğ‘</ci><apply id="S2.F3.4.2.m2.1.1.3.cmml" xref="S2.F3.4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.F3.4.2.m2.1.1.3.1.cmml" xref="S2.F3.4.2.m2.1.1.3">superscript</csymbol><ci id="S2.F3.4.2.m2.1.1.3.2.cmml" xref="S2.F3.4.2.m2.1.1.3.2">ğœ</ci><cn id="S2.F3.4.2.m2.1.1.3.3.cmml" type="integer" xref="S2.F3.4.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.4.2.m2.1d">Z_{\sigma^{2}}</annotation><annotation encoding="application/x-llamapun" id="S2.F3.4.2.m2.1e">italic_Z start_POSTSUBSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> along two axes), which we can be used as a compressed representation of the input <span class="ltx_glossaryref" title="">CSI</span>.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.6">The work in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite> introduced several modular architectures for <span class="ltx_glossaryref" title="">HAR</span> using <span class="ltx_glossaryref" title="">CSI</span> data, practically splitting the problem into two separate sub-tasks.
First, a <span class="ltx_glossaryref" title="">VAE</span> provided a concise (yet informative) characterisation of the different activities as perceived by the <span class="ltx_text" id="S2.SS3.p1.6.1">Wi-Fi</span> monitor through the <span class="ltx_glossaryref" title="">CSI</span>.
Specifically, the <span class="ltx_glossaryref" title="">VAE</span> mapped short sequences of <span class="ltx_glossaryref" title="">CSI</span> data â€“ sampled using a sliding window in the time domain â€“ onto a latent bivariate Gaussian distribution defined by 4 parameters (<span class="ltx_ERROR undefined" id="S2.SS3.p1.6.2">\ie</span>mean and variance along two dimensions).
Second, a <span class="ltx_glossaryref" title="">multi-layer perceptron (MLP)</span> trained on the latent space parameters of the <span class="ltx_glossaryref" title="">VAE</span> was used to classify the different activities.
The unsupervised training of the <span class="ltx_glossaryref" title="">VAE</span> can be carried out separately from the training of the <span class="ltx_glossaryref" title="">MLP</span>.
However, since the <span class="ltx_text" id="S2.SS3.p1.6.3">Wi-Fi</span> monitor is sensing the environment using four physically-spaced antennas, several <span class="ltx_glossaryref" title="">VAE</span> architectures were proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite> to evaluate different strategies to fuse the information sensed by every antenna.
<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.F3" title="In II-C Dataset Pre-Processing and VAE Architectures â€£ II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> summarises the general architecture of the <span class="ltx_glossaryref" title="">VAEs</span>.
The input data structure is a tensor of size <math alttext="(W\times S\times A)" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.1.cmml"><mo id="S2.SS3.p1.1.m1.1.1.1.2" stretchy="false" xref="S2.SS3.p1.1.m1.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.1.m1.1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.1.1.2.cmml">W</mi><mo id="S2.SS3.p1.1.m1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p1.1.m1.1.1.1.1.1.cmml">Ã—</mo><mi id="S2.SS3.p1.1.m1.1.1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.1.1.3.cmml">S</mi><mo id="S2.SS3.p1.1.m1.1.1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p1.1.m1.1.1.1.1.1.cmml">Ã—</mo><mi id="S2.SS3.p1.1.m1.1.1.1.1.4" xref="S2.SS3.p1.1.m1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S2.SS3.p1.1.m1.1.1.1.3" stretchy="false" xref="S2.SS3.p1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"><times id="S2.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1"></times><ci id="S2.SS3.p1.1.m1.1.1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.2">ğ‘Š</ci><ci id="S2.SS3.p1.1.m1.1.1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.3">ğ‘†</ci><ci id="S2.SS3.p1.1.m1.1.1.1.1.4.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.4">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">(W\times S\times A)</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">( italic_W Ã— italic_S Ã— italic_A )</annotation></semantics></math>, where <math alttext="W" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">W</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.2.m2.1d">italic_W</annotation></semantics></math> is the number of <span class="ltx_glossaryref" title="">CSI</span> samples in a given time window, <math alttext="S" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m3.1"><semantics id="S2.SS3.p1.3.m3.1a"><mi id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.3.m3.1d">italic_S</annotation></semantics></math> is the number of subcarriers in a <span class="ltx_text" id="S2.SS3.p1.6.4">Wi-Fi</span> frame, and <math alttext="A" class="ltx_Math" display="inline" id="S2.SS3.p1.4.m4.1"><semantics id="S2.SS3.p1.4.m4.1a"><mi id="S2.SS3.p1.4.m4.1.1" xref="S2.SS3.p1.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m4.1b"><ci id="S2.SS3.p1.4.m4.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m4.1c">A</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.4.m4.1d">italic_A</annotation></semantics></math> is the number of antennas considered by the <span class="ltx_glossaryref" title="">VAE</span>.
We fixed the time window to 3Â seconds, so <math alttext="W" class="ltx_Math" display="inline" id="S2.SS3.p1.5.m5.1"><semantics id="S2.SS3.p1.5.m5.1a"><mi id="S2.SS3.p1.5.m5.1.1" xref="S2.SS3.p1.5.m5.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.5.m5.1b"><ci id="S2.SS3.p1.5.m5.1.1.cmml" xref="S2.SS3.p1.5.m5.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.5.m5.1c">W</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.5.m5.1d">italic_W</annotation></semantics></math>=450, while the considered dataset contains <span class="ltx_text" id="S2.SS3.p1.6.5">Wi-Fi</span>Â 6 frames with <math alttext="S" class="ltx_Math" display="inline" id="S2.SS3.p1.6.m6.1"><semantics id="S2.SS3.p1.6.m6.1a"><mi id="S2.SS3.p1.6.m6.1.1" xref="S2.SS3.p1.6.m6.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.6.m6.1b"><ci id="S2.SS3.p1.6.m6.1.1.cmml" xref="S2.SS3.p1.6.m6.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.6.m6.1c">S</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.6.m6.1d">italic_S</annotation></semantics></math>=2048 subcarriers (<span class="ltx_text" id="S2.SS3.p1.6.6">160-MHz</span> bandwidth).</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">In this work, we start from the compressed representation of the <span class="ltx_glossaryref" title="">CSI</span> data windows in the <span class="ltx_glossaryref" title="">VAE</span>â€™s latent space to develop a neuro-symbolic architecture for <span class="ltx_glossaryref" title="">HAR</span>.
This pre-processed dataset was obtained by training the <span class="ltx_glossaryref" title="">VAEs</span> described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite> on the complete set of activities in the original dataset.
Here, we briefly report the resultant architectures for convenience.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.6">First, we consider a set of architectures called <span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">No-Fused-<math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S2.SS3.p3.1.1.1.m1.1"><semantics id="S2.SS3.p3.1.1.1.m1.1a"><mi id="S2.SS3.p3.1.1.1.m1.1.1" xref="S2.SS3.p3.1.1.1.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.1.1.m1.1b"><ci id="S2.SS3.p3.1.1.1.m1.1.1.cmml" xref="S2.SS3.p3.1.1.1.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.1.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.1.1.1.m1.1d">bold_x</annotation></semantics></math></span>.
These architectures have been trained using the data incoming from one single antenna of the <span class="ltx_text" id="S2.SS3.p3.6.6">Wi-Fi</span> monitor (<math alttext="A" class="ltx_Math" display="inline" id="S2.SS3.p3.2.m1.1"><semantics id="S2.SS3.p3.2.m1.1a"><mi id="S2.SS3.p3.2.m1.1.1" xref="S2.SS3.p3.2.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.2.m1.1b"><ci id="S2.SS3.p3.2.m1.1.1.cmml" xref="S2.SS3.p3.2.m1.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.2.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.2.m1.1d">italic_A</annotation></semantics></math>=1 in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.F3" title="In II-C Dataset Pre-Processing and VAE Architectures â€£ II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>).
We denote with the letter x the antenna of the monitor whose data we used to train the <span class="ltx_glossaryref" title="">VAE</span>.
Hence, we define four separate architectures, one for each antenna: <span class="ltx_text ltx_font_bold" id="S2.SS3.p3.3.2">No-Fused-<math alttext="\mathbf{1}" class="ltx_Math" display="inline" id="S2.SS3.p3.3.2.1.m1.1"><semantics id="S2.SS3.p3.3.2.1.m1.1a"><mn id="S2.SS3.p3.3.2.1.m1.1.1" xref="S2.SS3.p3.3.2.1.m1.1.1.cmml">ğŸ</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.3.2.1.m1.1b"><cn id="S2.SS3.p3.3.2.1.m1.1.1.cmml" type="integer" xref="S2.SS3.p3.3.2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.3.2.1.m1.1c">\mathbf{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.3.2.1.m1.1d">bold_1</annotation></semantics></math></span>, <span class="ltx_text ltx_font_bold" id="S2.SS3.p3.4.3">No-Fused-<math alttext="\mathbf{2}" class="ltx_Math" display="inline" id="S2.SS3.p3.4.3.1.m1.1"><semantics id="S2.SS3.p3.4.3.1.m1.1a"><mn id="S2.SS3.p3.4.3.1.m1.1.1" xref="S2.SS3.p3.4.3.1.m1.1.1.cmml">ğŸ</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.4.3.1.m1.1b"><cn id="S2.SS3.p3.4.3.1.m1.1.1.cmml" type="integer" xref="S2.SS3.p3.4.3.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.4.3.1.m1.1c">\mathbf{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.4.3.1.m1.1d">bold_2</annotation></semantics></math></span>, <span class="ltx_text ltx_font_bold" id="S2.SS3.p3.5.4">No-Fused-<math alttext="\mathbf{3}" class="ltx_Math" display="inline" id="S2.SS3.p3.5.4.1.m1.1"><semantics id="S2.SS3.p3.5.4.1.m1.1a"><mn id="S2.SS3.p3.5.4.1.m1.1.1" xref="S2.SS3.p3.5.4.1.m1.1.1.cmml">ğŸ‘</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.5.4.1.m1.1b"><cn id="S2.SS3.p3.5.4.1.m1.1.1.cmml" type="integer" xref="S2.SS3.p3.5.4.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.5.4.1.m1.1c">\mathbf{3}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.5.4.1.m1.1d">bold_3</annotation></semantics></math></span>, and <span class="ltx_text ltx_font_bold" id="S2.SS3.p3.6.5">No-Fused-<math alttext="\mathbf{4}" class="ltx_Math" display="inline" id="S2.SS3.p3.6.5.1.m1.1"><semantics id="S2.SS3.p3.6.5.1.m1.1a"><mn id="S2.SS3.p3.6.5.1.m1.1.1" xref="S2.SS3.p3.6.5.1.m1.1.1.cmml">ğŸ’</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.6.5.1.m1.1b"><cn id="S2.SS3.p3.6.5.1.m1.1.1.cmml" type="integer" xref="S2.SS3.p3.6.5.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.6.5.1.m1.1c">\mathbf{4}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.6.5.1.m1.1d">bold_4</annotation></semantics></math></span>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">While using data from one single antenna can be enough for some <span class="ltx_glossaryref" title="">HAR</span> applicationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib16" title="">16</a>]</cite>, we proved in previous work that there are consistent advantages in fusing the <span class="ltx_glossaryref" title="">CSI</span> data from different antennasÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite>.
Therefore, we also consider a second type of architecture, called <span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.1">Early-Fusing</span>.
In this case, the <span class="ltx_glossaryref" title="">CSI</span> data from all four monitor antennas are stacked together in the input data structure (<math alttext="A" class="ltx_Math" display="inline" id="S2.SS3.p4.1.m1.1"><semantics id="S2.SS3.p4.1.m1.1a"><mi id="S2.SS3.p4.1.m1.1.1" xref="S2.SS3.p4.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.1.m1.1b"><ci id="S2.SS3.p4.1.m1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.1.m1.1d">italic_A</annotation></semantics></math>=4 in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.F3" title="In II-C Dataset Pre-Processing and VAE Architectures â€£ II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>).
Still, the latent space of <span class="ltx_text" id="S2.SS3.p4.1.2">VAE-F</span> has a bivariate latent normal distribution which can condense together cross-antennas regularities.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">The last architecture we consider, called <span class="ltx_text ltx_font_bold" id="S2.SS3.p5.1.1">Delayed-Fusing</span>, employs all the four <span class="ltx_glossaryref" title="">VAEs</span> trained independently on each monitor antenna and concatenates their latent space parameters into a single vector of 16 elements (4 features for each <span class="ltx_glossaryref" title="">VAE</span>) that becomes the input of the following classification stage.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section we present <span class="ltx_text ltx_font_sansserif" id="S3.p1.1.1">DeepProbHAR</span>, the first neuro-symbolic approach to human activity recognition fusing information from <span class="ltx_text" id="S3.p1.1.2">Wi-Fi</span> <span class="ltx_glossaryref" title="">CSIs</span>.
First, we briefly introduce neuro-symbolic AI (<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.SS1" title="III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>), particularly the DeepProbLog approachÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib7" title="">7</a>]</cite>.
Then, we discuss how we extracted domain-dependent knowledge for classifying different activities using a more interpretable modality, <span class="ltx_ERROR undefined" id="S3.p1.1.3">\viz</span>the video recording of the performed activities (<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.SS2" title="III-B Domain-Dependent Rule from Different Modality â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>).
For this work, we relied on such a knowledge extraction to reasonably assume that the <span class="ltx_text" id="S3.p1.1.4">Wi-Fi</span> sensor could have captured the way we would describe activities, as both the camera and the antennas were looking at the same environment.
Finally, we describe in detail the <span class="ltx_text ltx_font_sansserif" id="S3.p1.1.5">DeepProbHAR</span> architecture (<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.SS3" title="III-C DeepProbHAR: A Neuro-Symbolic Architecture for Human Activity Recognition Using Wi-Fi Data â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Primer on Neuro-Symbolic AI: the DeepProbLog Approach</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Neurosymbolic AI, <span class="ltx_ERROR undefined" id="S3.SS1.p1.1.1">\eg</span><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib5" title="">5</a>]</cite>, is often referred to as the combination of symbolic reasoning techniques and neural network methods to improve the performance of AI systems. These systems can merge the robust approximation capabilities of neural networks with the abstract reasoning abilities of symbolic methods, enabling them to extrapolate from limited data and produce interpretable results.
Neurosymbolic AI techniques can be broadly categorised into two groups. The first considers techniques that condense structured symbolic knowledge for integration with neural patterns and reason using these integrated neural patterns. The second considers techniques that extract information from neural patterns to facilitate mapping to structured symbolic knowledge (<span class="ltx_ERROR undefined" id="S3.SS1.p1.1.2">\ie</span><span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">lifting</span>) and carry out symbolic reasoning.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.8">This paper focuses on a specific approach within the second group of neurosymbolic AI techniques, <span class="ltx_ERROR undefined" id="S3.SS1.p2.8.1">\viz</span>DeepProbLog <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib7" title="">7</a>]</cite>. To present it, we first need to briefly introduce ProbLog, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib17" title="">17</a>]</cite>, which is a probabilistic logic programming language. A ProbLog program comprises a collection of probabilistic facts, denoted as <math alttext="F" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_F</annotation></semantics></math>, and a set of rules, denoted as <math alttext="R" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_R</annotation></semantics></math>. Facts are expressed in the form <math alttext="p::f" class="ltx_math_unparsed" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1b"><mi id="S3.SS1.p2.3.m3.1.1">p</mi><mo id="S3.SS1.p2.3.m3.1.2" lspace="0.278em" rspace="0em">:</mo><mo id="S3.SS1.p2.3.m3.1.3" rspace="0.278em">:</mo><mi id="S3.SS1.p2.3.m3.1.4">f</mi></mrow><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">p::f</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_p : : italic_f</annotation></semantics></math>, where <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_f</annotation></semantics></math> is an atom symbolising a notion that can either be true or false. <math alttext="p" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_p</annotation></semantics></math> is a probability value ranging from 0 to 1, which signifies the probability of the fact being true. Rules are expressed in <math alttext="h\leftarrow b_{1},...,b_{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.3"><semantics id="S3.SS1.p2.6.m6.3a"><mrow id="S3.SS1.p2.6.m6.3.3" xref="S3.SS1.p2.6.m6.3.3.cmml"><mi id="S3.SS1.p2.6.m6.3.3.4" xref="S3.SS1.p2.6.m6.3.3.4.cmml">h</mi><mo id="S3.SS1.p2.6.m6.3.3.3" stretchy="false" xref="S3.SS1.p2.6.m6.3.3.3.cmml">â†</mo><mrow id="S3.SS1.p2.6.m6.3.3.2.2" xref="S3.SS1.p2.6.m6.3.3.2.3.cmml"><msub id="S3.SS1.p2.6.m6.2.2.1.1.1" xref="S3.SS1.p2.6.m6.2.2.1.1.1.cmml"><mi id="S3.SS1.p2.6.m6.2.2.1.1.1.2" xref="S3.SS1.p2.6.m6.2.2.1.1.1.2.cmml">b</mi><mn id="S3.SS1.p2.6.m6.2.2.1.1.1.3" xref="S3.SS1.p2.6.m6.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p2.6.m6.3.3.2.2.3" xref="S3.SS1.p2.6.m6.3.3.2.3.cmml">,</mo><mi id="S3.SS1.p2.6.m6.1.1" mathvariant="normal" xref="S3.SS1.p2.6.m6.1.1.cmml">â€¦</mi><mo id="S3.SS1.p2.6.m6.3.3.2.2.4" xref="S3.SS1.p2.6.m6.3.3.2.3.cmml">,</mo><msub id="S3.SS1.p2.6.m6.3.3.2.2.2" xref="S3.SS1.p2.6.m6.3.3.2.2.2.cmml"><mi id="S3.SS1.p2.6.m6.3.3.2.2.2.2" xref="S3.SS1.p2.6.m6.3.3.2.2.2.2.cmml">b</mi><mi id="S3.SS1.p2.6.m6.3.3.2.2.2.3" xref="S3.SS1.p2.6.m6.3.3.2.2.2.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.3b"><apply id="S3.SS1.p2.6.m6.3.3.cmml" xref="S3.SS1.p2.6.m6.3.3"><ci id="S3.SS1.p2.6.m6.3.3.3.cmml" xref="S3.SS1.p2.6.m6.3.3.3">â†</ci><ci id="S3.SS1.p2.6.m6.3.3.4.cmml" xref="S3.SS1.p2.6.m6.3.3.4">â„</ci><list id="S3.SS1.p2.6.m6.3.3.2.3.cmml" xref="S3.SS1.p2.6.m6.3.3.2.2"><apply id="S3.SS1.p2.6.m6.2.2.1.1.1.cmml" xref="S3.SS1.p2.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.1.1.1.2.cmml" xref="S3.SS1.p2.6.m6.2.2.1.1.1.2">ğ‘</ci><cn id="S3.SS1.p2.6.m6.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.6.m6.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">â€¦</ci><apply id="S3.SS1.p2.6.m6.3.3.2.2.2.cmml" xref="S3.SS1.p2.6.m6.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.3.3.2.2.2.1.cmml" xref="S3.SS1.p2.6.m6.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.6.m6.3.3.2.2.2.2.cmml" xref="S3.SS1.p2.6.m6.3.3.2.2.2.2">ğ‘</ci><ci id="S3.SS1.p2.6.m6.3.3.2.2.2.3.cmml" xref="S3.SS1.p2.6.m6.3.3.2.2.2.3">ğ‘›</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.3c">h\leftarrow b_{1},...,b_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.3d">italic_h â† italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_b start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="h" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1"><semantics id="S3.SS1.p2.7.m7.1a"><mi id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.1d">italic_h</annotation></semantics></math> is an atom and <math alttext="b_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.1"><semantics id="S3.SS1.p2.8.m8.1a"><msub id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">b</mi><mi id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">ğ‘</ci><ci id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m8.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are literals. A literal can be an atom or the negation of an atom.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.6">ProbLog includes <span class="ltx_glossaryref" title="">Annotated Disjunctions (ADs)</span> as a syntactic extension of the form
<math alttext="p_{1}::h_{1};...;p_{n}::h_{n}\leftarrow b_{1},...,b_{m}." class="ltx_math_unparsed" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1b"><msub id="S3.SS1.p3.1.m1.1.1"><mi id="S3.SS1.p3.1.m1.1.1.2">p</mi><mn id="S3.SS1.p3.1.m1.1.1.3">1</mn></msub><mo id="S3.SS1.p3.1.m1.1.2" lspace="0.278em" rspace="0em">:</mo><mo id="S3.SS1.p3.1.m1.1.3" rspace="0.278em">:</mo><msub id="S3.SS1.p3.1.m1.1.4"><mi id="S3.SS1.p3.1.m1.1.4.2">h</mi><mn id="S3.SS1.p3.1.m1.1.4.3">1</mn></msub><mo id="S3.SS1.p3.1.m1.1.5">;</mo><mi id="S3.SS1.p3.1.m1.1.6" mathvariant="normal">â€¦</mi><mo id="S3.SS1.p3.1.m1.1.7">;</mo><msub id="S3.SS1.p3.1.m1.1.8"><mi id="S3.SS1.p3.1.m1.1.8.2">p</mi><mi id="S3.SS1.p3.1.m1.1.8.3">n</mi></msub><mo id="S3.SS1.p3.1.m1.1.9" lspace="0.278em" rspace="0em">:</mo><mo id="S3.SS1.p3.1.m1.1.10" rspace="0.278em">:</mo><msub id="S3.SS1.p3.1.m1.1.11"><mi id="S3.SS1.p3.1.m1.1.11.2">h</mi><mi id="S3.SS1.p3.1.m1.1.11.3">n</mi></msub><mo id="S3.SS1.p3.1.m1.1.12" stretchy="false">â†</mo><msub id="S3.SS1.p3.1.m1.1.13"><mi id="S3.SS1.p3.1.m1.1.13.2">b</mi><mn id="S3.SS1.p3.1.m1.1.13.3">1</mn></msub><mo id="S3.SS1.p3.1.m1.1.14">,</mo><mi id="S3.SS1.p3.1.m1.1.15" mathvariant="normal">â€¦</mi><mo id="S3.SS1.p3.1.m1.1.16">,</mo><msub id="S3.SS1.p3.1.m1.1.17"><mi id="S3.SS1.p3.1.m1.1.17.2">b</mi><mi id="S3.SS1.p3.1.m1.1.17.3">m</mi></msub><mo id="S3.SS1.p3.1.m1.1.18" lspace="0em">.</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">p_{1}::h_{1};...;p_{n}::h_{n}\leftarrow b_{1},...,b_{m}.</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : : italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; â€¦ ; italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT : : italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT â† italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_b start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT .</annotation></semantics></math>
where the <math alttext="p_{i}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">p</mi><mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">ğ‘</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are probabilities such that <math alttext="\sum p_{i}=1" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><mrow id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mrow id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml"><mo id="S3.SS1.p3.3.m3.1.1.2.1" xref="S3.SS1.p3.3.m3.1.1.2.1.cmml">âˆ‘</mo><msub id="S3.SS1.p3.3.m3.1.1.2.2" xref="S3.SS1.p3.3.m3.1.1.2.2.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2.2.2" xref="S3.SS1.p3.3.m3.1.1.2.2.2.cmml">p</mi><mi id="S3.SS1.p3.3.m3.1.1.2.2.3" xref="S3.SS1.p3.3.m3.1.1.2.2.3.cmml">i</mi></msub></mrow><mo id="S3.SS1.p3.3.m3.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><eq id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1"></eq><apply id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2"><sum id="S3.SS1.p3.3.m3.1.1.2.1.cmml" xref="S3.SS1.p3.3.m3.1.1.2.1"></sum><apply id="S3.SS1.p3.3.m3.1.1.2.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.2.2.1.cmml" xref="S3.SS1.p3.3.m3.1.1.2.2">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.2.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2.2.2">ğ‘</ci><ci id="S3.SS1.p3.3.m3.1.1.2.2.3.cmml" xref="S3.SS1.p3.3.m3.1.1.2.2.3">ğ‘–</ci></apply></apply><cn id="S3.SS1.p3.3.m3.1.1.3.cmml" type="integer" xref="S3.SS1.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\sum p_{i}=1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">âˆ‘ italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1</annotation></semantics></math>, and <math alttext="h_{i}" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><msub id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">h</mi><mi id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">â„</ci><ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">h_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="b_{j}" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m5.1"><semantics id="S3.SS1.p3.5.m5.1a"><msub id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml">b</mi><mi id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2">ğ‘</ci><ci id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">b_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.m5.1d">italic_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> are atoms.
It is immediate to see that they encode categorical distributions over the possible results of a random variable that can take on one of <math alttext="K" class="ltx_Math" display="inline" id="S3.SS1.p3.6.m6.1"><semantics id="S3.SS1.p3.6.m6.1a"><mi id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><ci id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.6.m6.1d">italic_K</annotation></semantics></math> possible categories. Moreover, it is also a way to capture a Bernoulli distribution where we wish to name the two outcomes explicitly, <span class="ltx_ERROR undefined" id="S3.SS1.p3.6.1">\eg</span><span class="ltx_text ltx_font_italic" id="S3.SS1.p3.6.2">head</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.6.3">tail</span> as possible results of tossing a coin, rather than limiting ourselves to just one, let us say <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.6.4">head</span>, and deriving the other â€“ <span class="ltx_ERROR undefined" id="S3.SS1.p3.6.5">\ie</span><span class="ltx_text ltx_font_italic" id="S3.SS1.p3.6.6">tail</span> â€“ as the negation.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">A Problog program can be encoded in a probabilistic circuit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib18" title="">18</a>]</cite>, which is a graphical model that compactly represents probability distributions. Each fact and rule in a Problog program can be translated into a node or a set of nodes in a probabilistic circuit, and the probabilities associated with the facts correspond to the parameters of the probabilistic circuit.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.4">DeepProbLog <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib7" title="">7</a>]</cite> is a programming language that combines neural networks with probabilistic logic. It extends ProbLog by introducing <span class="ltx_glossaryref" title="">Neural Annotated Disjunctions (nADs)</span>. Differently from <span class="ltx_glossaryref" title="">ADs</span>, in <span class="ltx_glossaryref" title="">nADs</span> the probabilities of the categorical distribution are the output layer of a neural network <math alttext="f(\bm{x},\bm{\theta})" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.2"><semantics id="S3.SS1.p5.1.m1.2a"><mrow id="S3.SS1.p5.1.m1.2.3" xref="S3.SS1.p5.1.m1.2.3.cmml"><mi id="S3.SS1.p5.1.m1.2.3.2" xref="S3.SS1.p5.1.m1.2.3.2.cmml">f</mi><mo id="S3.SS1.p5.1.m1.2.3.1" xref="S3.SS1.p5.1.m1.2.3.1.cmml">â¢</mo><mrow id="S3.SS1.p5.1.m1.2.3.3.2" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml"><mo id="S3.SS1.p5.1.m1.2.3.3.2.1" stretchy="false" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">ğ’™</mi><mo id="S3.SS1.p5.1.m1.2.3.3.2.2" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p5.1.m1.2.2" xref="S3.SS1.p5.1.m1.2.2.cmml">ğœ½</mi><mo id="S3.SS1.p5.1.m1.2.3.3.2.3" stretchy="false" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.2b"><apply id="S3.SS1.p5.1.m1.2.3.cmml" xref="S3.SS1.p5.1.m1.2.3"><times id="S3.SS1.p5.1.m1.2.3.1.cmml" xref="S3.SS1.p5.1.m1.2.3.1"></times><ci id="S3.SS1.p5.1.m1.2.3.2.cmml" xref="S3.SS1.p5.1.m1.2.3.2">ğ‘“</ci><interval closure="open" id="S3.SS1.p5.1.m1.2.3.3.1.cmml" xref="S3.SS1.p5.1.m1.2.3.3.2"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">ğ’™</ci><ci id="S3.SS1.p5.1.m1.2.2.cmml" xref="S3.SS1.p5.1.m1.2.2">ğœ½</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.2c">f(\bm{x},\bm{\theta})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.2d">italic_f ( bold_italic_x , bold_italic_Î¸ )</annotation></semantics></math>. There are no restrictions on the form of <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1"><semantics id="S3.SS1.p5.2.m2.1a"><mrow id="S3.SS1.p5.2.m2.1.2" xref="S3.SS1.p5.2.m2.1.2.cmml"><mi id="S3.SS1.p5.2.m2.1.2.2" xref="S3.SS1.p5.2.m2.1.2.2.cmml">f</mi><mo id="S3.SS1.p5.2.m2.1.2.1" xref="S3.SS1.p5.2.m2.1.2.1.cmml">â¢</mo><mrow id="S3.SS1.p5.2.m2.1.2.3.2" xref="S3.SS1.p5.2.m2.1.2.cmml"><mo id="S3.SS1.p5.2.m2.1.2.3.2.1" stretchy="false" xref="S3.SS1.p5.2.m2.1.2.cmml">(</mo><mo id="S3.SS1.p5.2.m2.1.1" lspace="0em" rspace="0em" xref="S3.SS1.p5.2.m2.1.1.cmml">â‹…</mo><mo id="S3.SS1.p5.2.m2.1.2.3.2.2" stretchy="false" xref="S3.SS1.p5.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><apply id="S3.SS1.p5.2.m2.1.2.cmml" xref="S3.SS1.p5.2.m2.1.2"><times id="S3.SS1.p5.2.m2.1.2.1.cmml" xref="S3.SS1.p5.2.m2.1.2.1"></times><ci id="S3.SS1.p5.2.m2.1.2.2.cmml" xref="S3.SS1.p5.2.m2.1.2.2">ğ‘“</ci><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">f(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.2.m2.1d">italic_f ( â‹… )</annotation></semantics></math> as long as it outputs a categorical distribution over <math alttext="K" class="ltx_Math" display="inline" id="S3.SS1.p5.3.m3.1"><semantics id="S3.SS1.p5.3.m3.1a"><mi id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><ci id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.3.m3.1d">italic_K</annotation></semantics></math> classes, <span class="ltx_ERROR undefined" id="S3.SS1.p5.4.1">\eg</span>using a <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.4.2">softmax</span> activation function at the network output. Each <span class="ltx_glossaryref" title="">nAD</span> is thus associated to a specific neural network <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S3.SS1.p5.4.m4.1"><semantics id="S3.SS1.p5.4.m4.1a"><mrow id="S3.SS1.p5.4.m4.1.2" xref="S3.SS1.p5.4.m4.1.2.cmml"><mi id="S3.SS1.p5.4.m4.1.2.2" xref="S3.SS1.p5.4.m4.1.2.2.cmml">f</mi><mo id="S3.SS1.p5.4.m4.1.2.1" xref="S3.SS1.p5.4.m4.1.2.1.cmml">â¢</mo><mrow id="S3.SS1.p5.4.m4.1.2.3.2" xref="S3.SS1.p5.4.m4.1.2.cmml"><mo id="S3.SS1.p5.4.m4.1.2.3.2.1" stretchy="false" xref="S3.SS1.p5.4.m4.1.2.cmml">(</mo><mo id="S3.SS1.p5.4.m4.1.1" lspace="0em" rspace="0em" xref="S3.SS1.p5.4.m4.1.1.cmml">â‹…</mo><mo id="S3.SS1.p5.4.m4.1.2.3.2.2" stretchy="false" xref="S3.SS1.p5.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><apply id="S3.SS1.p5.4.m4.1.2.cmml" xref="S3.SS1.p5.4.m4.1.2"><times id="S3.SS1.p5.4.m4.1.2.1.cmml" xref="S3.SS1.p5.4.m4.1.2.1"></times><ci id="S3.SS1.p5.4.m4.1.2.2.cmml" xref="S3.SS1.p5.4.m4.1.2.2">ğ‘“</ci><ci id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">f(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.4.m4.1d">italic_f ( â‹… )</annotation></semantics></math></p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.2">For each <span class="ltx_glossaryref" title="">nAD</span>, DeepPropLog computes the gradient of the loss w.r.t. the output of the associated <math alttext="f(\bm{x},\bm{\theta})" class="ltx_Math" display="inline" id="S3.SS1.p6.1.m1.2"><semantics id="S3.SS1.p6.1.m1.2a"><mrow id="S3.SS1.p6.1.m1.2.3" xref="S3.SS1.p6.1.m1.2.3.cmml"><mi id="S3.SS1.p6.1.m1.2.3.2" xref="S3.SS1.p6.1.m1.2.3.2.cmml">f</mi><mo id="S3.SS1.p6.1.m1.2.3.1" xref="S3.SS1.p6.1.m1.2.3.1.cmml">â¢</mo><mrow id="S3.SS1.p6.1.m1.2.3.3.2" xref="S3.SS1.p6.1.m1.2.3.3.1.cmml"><mo id="S3.SS1.p6.1.m1.2.3.3.2.1" stretchy="false" xref="S3.SS1.p6.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">ğ’™</mi><mo id="S3.SS1.p6.1.m1.2.3.3.2.2" xref="S3.SS1.p6.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p6.1.m1.2.2" xref="S3.SS1.p6.1.m1.2.2.cmml">ğœ½</mi><mo id="S3.SS1.p6.1.m1.2.3.3.2.3" stretchy="false" xref="S3.SS1.p6.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.2b"><apply id="S3.SS1.p6.1.m1.2.3.cmml" xref="S3.SS1.p6.1.m1.2.3"><times id="S3.SS1.p6.1.m1.2.3.1.cmml" xref="S3.SS1.p6.1.m1.2.3.1"></times><ci id="S3.SS1.p6.1.m1.2.3.2.cmml" xref="S3.SS1.p6.1.m1.2.3.2">ğ‘“</ci><interval closure="open" id="S3.SS1.p6.1.m1.2.3.3.1.cmml" xref="S3.SS1.p6.1.m1.2.3.3.2"><ci id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">ğ’™</ci><ci id="S3.SS1.p6.1.m1.2.2.cmml" xref="S3.SS1.p6.1.m1.2.2">ğœ½</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.2c">f(\bm{x},\bm{\theta})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.1.m1.2d">italic_f ( bold_italic_x , bold_italic_Î¸ )</annotation></semantics></math>. Standard backpropagation algorithms use the gradient to train the parameters <math alttext="\bm{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p6.2.m2.1"><semantics id="S3.SS1.p6.2.m2.1a"><mi id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml">ğœ½</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><ci id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1">ğœ½</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">\bm{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.2.m2.1d">bold_italic_Î¸</annotation></semantics></math>. Such a computation leverages the differentiability of the ProbLog program, the computational machinery of which can be expressed over the associated probabilistic circuits. For further details, the interested reader is referred to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib7" title="">7</a>]</cite> and for applications of DeepProbLog to analogous tasks such as complex event processing, to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="S3.F4.g1" src="x4.png" width="548"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.4.2.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.2.1" style="font-size:90%;">Computation of the right upper armâ€™s angle <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.F4.2.1.m1.1"><semantics id="S3.F4.2.1.m1.1b"><mi id="S3.F4.2.1.m1.1.1" xref="S3.F4.2.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.F4.2.1.m1.1c"><ci id="S3.F4.2.1.m1.1.1.cmml" xref="S3.F4.2.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.2.1.m1.1d">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.F4.2.1.m1.1e">italic_Î±</annotation></semantics></math>. The same operation applies to all the other limb segments.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="332" id="S3.F5.g1" src="x5.png" width="780"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.4.2.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.2.1" style="font-size:90%;">The feature <math alttext="\delta_{l}" class="ltx_Math" display="inline" id="S3.F5.2.1.m1.1"><semantics id="S3.F5.2.1.m1.1b"><msub id="S3.F5.2.1.m1.1.1" xref="S3.F5.2.1.m1.1.1.cmml"><mi id="S3.F5.2.1.m1.1.1.2" xref="S3.F5.2.1.m1.1.1.2.cmml">Î´</mi><mi id="S3.F5.2.1.m1.1.1.3" xref="S3.F5.2.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F5.2.1.m1.1c"><apply id="S3.F5.2.1.m1.1.1.cmml" xref="S3.F5.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F5.2.1.m1.1.1.1.cmml" xref="S3.F5.2.1.m1.1.1">subscript</csymbol><ci id="S3.F5.2.1.m1.1.1.2.cmml" xref="S3.F5.2.1.m1.1.1.2">ğ›¿</ci><ci id="S3.F5.2.1.m1.1.1.3.cmml" xref="S3.F5.2.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.2.1.m1.1d">\delta_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.2.1.m1.1e">italic_Î´ start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> corresponding the right lower leg indicates the motion of that limb for each target activity.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="350" id="S3.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.4.2.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.2.1" style="font-size:90%;">Decision tree derived from the video data analysis. Leaf nodes represent the target activities. At every decision node, a single feature <math alttext="\delta_{l}" class="ltx_Math" display="inline" id="S3.F6.2.1.m1.1"><semantics id="S3.F6.2.1.m1.1b"><msub id="S3.F6.2.1.m1.1.1" xref="S3.F6.2.1.m1.1.1.cmml"><mi id="S3.F6.2.1.m1.1.1.2" xref="S3.F6.2.1.m1.1.1.2.cmml">Î´</mi><mi id="S3.F6.2.1.m1.1.1.3" xref="S3.F6.2.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F6.2.1.m1.1c"><apply id="S3.F6.2.1.m1.1.1.cmml" xref="S3.F6.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F6.2.1.m1.1.1.1.cmml" xref="S3.F6.2.1.m1.1.1">subscript</csymbol><ci id="S3.F6.2.1.m1.1.1.2.cmml" xref="S3.F6.2.1.m1.1.1.2">ğ›¿</ci><ci id="S3.F6.2.1.m1.1.1.3.cmml" xref="S3.F6.2.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.2.1.m1.1d">\delta_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.F6.2.1.m1.1e">italic_Î´ start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> is tested against a threshold to determine whether the corresponding limb is moving.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Domain-Dependent Rule from Different Modality</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To operate using a neuro-symbolic approach, we must use some declarative knowledge to describe the activities we plan to classify.
We assume that every target activity can be defined by combining basic movements; these â€œatomicâ€ movements should be sufficiently easy to identify.
For instance, <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.1">running</em> mandates a rapid alternate motion of the legs accompanied by broad armsâ€™ movement; conversely, <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.2">clapping</em> can be defined by the repetitive motion of the forearms.
Building on this assumption, we can combine a limited subset of basic movements into a potentially large set of target activities.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">In the following, we describe the process of extracting indicators of simple movements from the video dataset introduced in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.SS2" title="II-B Dataset â€£ II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>.
The idea is to compute a vector of scalar values (one for each limb) that characterise the <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.1">amount of motion</em> of the limbs in a given time window.
Such values will serve as numerical features that can be combined and used to describe more complex target human activities.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.9">A brief analysis of the video dataset revealed that we have access to the <math alttext="(x,y)" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.2"><semantics id="S3.SS2.p3.1.m1.2a"><mrow id="S3.SS2.p3.1.m1.2.3.2" xref="S3.SS2.p3.1.m1.2.3.1.cmml"><mo id="S3.SS2.p3.1.m1.2.3.2.1" stretchy="false" xref="S3.SS2.p3.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">x</mi><mo id="S3.SS2.p3.1.m1.2.3.2.2" xref="S3.SS2.p3.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.p3.1.m1.2.2" xref="S3.SS2.p3.1.m1.2.2.cmml">y</mi><mo id="S3.SS2.p3.1.m1.2.3.2.3" stretchy="false" xref="S3.SS2.p3.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.2b"><interval closure="open" id="S3.SS2.p3.1.m1.2.3.1.cmml" xref="S3.SS2.p3.1.m1.2.3.2"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğ‘¥</ci><ci id="S3.SS2.p3.1.m1.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.2d">( italic_x , italic_y )</annotation></semantics></math> coordinates of 17 key points in the camera viewport for each video frame.
Specifically, 12 key points correspond to the personâ€™s shoulders, elbows, hands, hips, knees, and feet (one for each side, <span class="ltx_ERROR undefined" id="S3.SS2.p3.9.1">\cf</span><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.F2" title="In II-B Dataset â€£ II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>).
Given the location of the joints and using basic trigonometry, we can precisely locate the limbsâ€™ position and orientation in the 2D frame.
We consider <math alttext="L=8" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">L</mi><mo id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><eq id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1"></eq><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ğ¿</ci><cn id="S3.SS2.p3.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">L=8</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_L = 8</annotation></semantics></math> limb segments: two upper arms, lower arms, upper legs, and lower legs.
Each segment is defined by two joints, a parent joint <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_p</annotation></semantics></math> (<span class="ltx_ERROR undefined" id="S3.SS2.p3.9.2">\eg</span>the shoulder for the upper arm limb) and a child joint <math alttext="c" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">italic_c</annotation></semantics></math> (<span class="ltx_ERROR undefined" id="S3.SS2.p3.9.3">\eg</span>the elbow for the upper arm).
<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F4" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> shows that if a parent joint <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5.1"><semantics id="S3.SS2.p3.5.m5.1a"><mi id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m5.1d">italic_p</annotation></semantics></math> has coordinates <math alttext="(x_{p},y_{p})" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m6.2"><semantics id="S3.SS2.p3.6.m6.2a"><mrow id="S3.SS2.p3.6.m6.2.2.2" xref="S3.SS2.p3.6.m6.2.2.3.cmml"><mo id="S3.SS2.p3.6.m6.2.2.2.3" stretchy="false" xref="S3.SS2.p3.6.m6.2.2.3.cmml">(</mo><msub id="S3.SS2.p3.6.m6.1.1.1.1" xref="S3.SS2.p3.6.m6.1.1.1.1.cmml"><mi id="S3.SS2.p3.6.m6.1.1.1.1.2" xref="S3.SS2.p3.6.m6.1.1.1.1.2.cmml">x</mi><mi id="S3.SS2.p3.6.m6.1.1.1.1.3" xref="S3.SS2.p3.6.m6.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.SS2.p3.6.m6.2.2.2.4" xref="S3.SS2.p3.6.m6.2.2.3.cmml">,</mo><msub id="S3.SS2.p3.6.m6.2.2.2.2" xref="S3.SS2.p3.6.m6.2.2.2.2.cmml"><mi id="S3.SS2.p3.6.m6.2.2.2.2.2" xref="S3.SS2.p3.6.m6.2.2.2.2.2.cmml">y</mi><mi id="S3.SS2.p3.6.m6.2.2.2.2.3" xref="S3.SS2.p3.6.m6.2.2.2.2.3.cmml">p</mi></msub><mo id="S3.SS2.p3.6.m6.2.2.2.5" stretchy="false" xref="S3.SS2.p3.6.m6.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.2b"><interval closure="open" id="S3.SS2.p3.6.m6.2.2.3.cmml" xref="S3.SS2.p3.6.m6.2.2.2"><apply id="S3.SS2.p3.6.m6.1.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1.2">ğ‘¥</ci><ci id="S3.SS2.p3.6.m6.1.1.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1.3">ğ‘</ci></apply><apply id="S3.SS2.p3.6.m6.2.2.2.2.cmml" xref="S3.SS2.p3.6.m6.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.2.2.2.2.1.cmml" xref="S3.SS2.p3.6.m6.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.6.m6.2.2.2.2.2.cmml" xref="S3.SS2.p3.6.m6.2.2.2.2.2">ğ‘¦</ci><ci id="S3.SS2.p3.6.m6.2.2.2.2.3.cmml" xref="S3.SS2.p3.6.m6.2.2.2.2.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.2c">(x_{p},y_{p})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m6.2d">( italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT )</annotation></semantics></math> and a child joint <math alttext="c" class="ltx_Math" display="inline" id="S3.SS2.p3.7.m7.1"><semantics id="S3.SS2.p3.7.m7.1a"><mi id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><ci id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.7.m7.1d">italic_c</annotation></semantics></math> has coordinates <math alttext="(x_{c},y_{c})" class="ltx_Math" display="inline" id="S3.SS2.p3.8.m8.2"><semantics id="S3.SS2.p3.8.m8.2a"><mrow id="S3.SS2.p3.8.m8.2.2.2" xref="S3.SS2.p3.8.m8.2.2.3.cmml"><mo id="S3.SS2.p3.8.m8.2.2.2.3" stretchy="false" xref="S3.SS2.p3.8.m8.2.2.3.cmml">(</mo><msub id="S3.SS2.p3.8.m8.1.1.1.1" xref="S3.SS2.p3.8.m8.1.1.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.1.1.2" xref="S3.SS2.p3.8.m8.1.1.1.1.2.cmml">x</mi><mi id="S3.SS2.p3.8.m8.1.1.1.1.3" xref="S3.SS2.p3.8.m8.1.1.1.1.3.cmml">c</mi></msub><mo id="S3.SS2.p3.8.m8.2.2.2.4" xref="S3.SS2.p3.8.m8.2.2.3.cmml">,</mo><msub id="S3.SS2.p3.8.m8.2.2.2.2" xref="S3.SS2.p3.8.m8.2.2.2.2.cmml"><mi id="S3.SS2.p3.8.m8.2.2.2.2.2" xref="S3.SS2.p3.8.m8.2.2.2.2.2.cmml">y</mi><mi id="S3.SS2.p3.8.m8.2.2.2.2.3" xref="S3.SS2.p3.8.m8.2.2.2.2.3.cmml">c</mi></msub><mo id="S3.SS2.p3.8.m8.2.2.2.5" stretchy="false" xref="S3.SS2.p3.8.m8.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.2b"><interval closure="open" id="S3.SS2.p3.8.m8.2.2.3.cmml" xref="S3.SS2.p3.8.m8.2.2.2"><apply id="S3.SS2.p3.8.m8.1.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.8.m8.1.1.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1.2">ğ‘¥</ci><ci id="S3.SS2.p3.8.m8.1.1.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1.3">ğ‘</ci></apply><apply id="S3.SS2.p3.8.m8.2.2.2.2.cmml" xref="S3.SS2.p3.8.m8.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.2.2.2.2.1.cmml" xref="S3.SS2.p3.8.m8.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.8.m8.2.2.2.2.2.cmml" xref="S3.SS2.p3.8.m8.2.2.2.2.2">ğ‘¦</ci><ci id="S3.SS2.p3.8.m8.2.2.2.2.3.cmml" xref="S3.SS2.p3.8.m8.2.2.2.2.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.2c">(x_{c},y_{c})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.8.m8.2d">( italic_x start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT )</annotation></semantics></math>, then the limb segment vector draws an angle
<math alttext="\alpha=\arctan\left(\frac{y_{c}-y_{p}}{x_{c}-x_{p}}\right)" class="ltx_Math" display="inline" id="S3.SS2.p3.9.m9.2"><semantics id="S3.SS2.p3.9.m9.2a"><mrow id="S3.SS2.p3.9.m9.2.3" xref="S3.SS2.p3.9.m9.2.3.cmml"><mi id="S3.SS2.p3.9.m9.2.3.2" xref="S3.SS2.p3.9.m9.2.3.2.cmml">Î±</mi><mo id="S3.SS2.p3.9.m9.2.3.1" xref="S3.SS2.p3.9.m9.2.3.1.cmml">=</mo><mrow id="S3.SS2.p3.9.m9.2.3.3.2" xref="S3.SS2.p3.9.m9.2.3.3.1.cmml"><mi id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml">arctan</mi><mo id="S3.SS2.p3.9.m9.2.3.3.2a" xref="S3.SS2.p3.9.m9.2.3.3.1.cmml">â¡</mo><mrow id="S3.SS2.p3.9.m9.2.3.3.2.1" xref="S3.SS2.p3.9.m9.2.3.3.1.cmml"><mo id="S3.SS2.p3.9.m9.2.3.3.2.1.1" xref="S3.SS2.p3.9.m9.2.3.3.1.cmml">(</mo><mfrac id="S3.SS2.p3.9.m9.2.2" xref="S3.SS2.p3.9.m9.2.2.cmml"><mrow id="S3.SS2.p3.9.m9.2.2.2" xref="S3.SS2.p3.9.m9.2.2.2.cmml"><msub id="S3.SS2.p3.9.m9.2.2.2.2" xref="S3.SS2.p3.9.m9.2.2.2.2.cmml"><mi id="S3.SS2.p3.9.m9.2.2.2.2.2" xref="S3.SS2.p3.9.m9.2.2.2.2.2.cmml">y</mi><mi id="S3.SS2.p3.9.m9.2.2.2.2.3" xref="S3.SS2.p3.9.m9.2.2.2.2.3.cmml">c</mi></msub><mo id="S3.SS2.p3.9.m9.2.2.2.1" xref="S3.SS2.p3.9.m9.2.2.2.1.cmml">âˆ’</mo><msub id="S3.SS2.p3.9.m9.2.2.2.3" xref="S3.SS2.p3.9.m9.2.2.2.3.cmml"><mi id="S3.SS2.p3.9.m9.2.2.2.3.2" xref="S3.SS2.p3.9.m9.2.2.2.3.2.cmml">y</mi><mi id="S3.SS2.p3.9.m9.2.2.2.3.3" xref="S3.SS2.p3.9.m9.2.2.2.3.3.cmml">p</mi></msub></mrow><mrow id="S3.SS2.p3.9.m9.2.2.3" xref="S3.SS2.p3.9.m9.2.2.3.cmml"><msub id="S3.SS2.p3.9.m9.2.2.3.2" xref="S3.SS2.p3.9.m9.2.2.3.2.cmml"><mi id="S3.SS2.p3.9.m9.2.2.3.2.2" xref="S3.SS2.p3.9.m9.2.2.3.2.2.cmml">x</mi><mi id="S3.SS2.p3.9.m9.2.2.3.2.3" xref="S3.SS2.p3.9.m9.2.2.3.2.3.cmml">c</mi></msub><mo id="S3.SS2.p3.9.m9.2.2.3.1" xref="S3.SS2.p3.9.m9.2.2.3.1.cmml">âˆ’</mo><msub id="S3.SS2.p3.9.m9.2.2.3.3" xref="S3.SS2.p3.9.m9.2.2.3.3.cmml"><mi id="S3.SS2.p3.9.m9.2.2.3.3.2" xref="S3.SS2.p3.9.m9.2.2.3.3.2.cmml">x</mi><mi id="S3.SS2.p3.9.m9.2.2.3.3.3" xref="S3.SS2.p3.9.m9.2.2.3.3.3.cmml">p</mi></msub></mrow></mfrac><mo id="S3.SS2.p3.9.m9.2.3.3.2.1.2" xref="S3.SS2.p3.9.m9.2.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.2b"><apply id="S3.SS2.p3.9.m9.2.3.cmml" xref="S3.SS2.p3.9.m9.2.3"><eq id="S3.SS2.p3.9.m9.2.3.1.cmml" xref="S3.SS2.p3.9.m9.2.3.1"></eq><ci id="S3.SS2.p3.9.m9.2.3.2.cmml" xref="S3.SS2.p3.9.m9.2.3.2">ğ›¼</ci><apply id="S3.SS2.p3.9.m9.2.3.3.1.cmml" xref="S3.SS2.p3.9.m9.2.3.3.2"><arctan id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1"></arctan><apply id="S3.SS2.p3.9.m9.2.2.cmml" xref="S3.SS2.p3.9.m9.2.2"><divide id="S3.SS2.p3.9.m9.2.2.1.cmml" xref="S3.SS2.p3.9.m9.2.2"></divide><apply id="S3.SS2.p3.9.m9.2.2.2.cmml" xref="S3.SS2.p3.9.m9.2.2.2"><minus id="S3.SS2.p3.9.m9.2.2.2.1.cmml" xref="S3.SS2.p3.9.m9.2.2.2.1"></minus><apply id="S3.SS2.p3.9.m9.2.2.2.2.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.2.2.2.2.1.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.9.m9.2.2.2.2.2.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2.2">ğ‘¦</ci><ci id="S3.SS2.p3.9.m9.2.2.2.2.3.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2.3">ğ‘</ci></apply><apply id="S3.SS2.p3.9.m9.2.2.2.3.cmml" xref="S3.SS2.p3.9.m9.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.2.2.2.3.1.cmml" xref="S3.SS2.p3.9.m9.2.2.2.3">subscript</csymbol><ci id="S3.SS2.p3.9.m9.2.2.2.3.2.cmml" xref="S3.SS2.p3.9.m9.2.2.2.3.2">ğ‘¦</ci><ci id="S3.SS2.p3.9.m9.2.2.2.3.3.cmml" xref="S3.SS2.p3.9.m9.2.2.2.3.3">ğ‘</ci></apply></apply><apply id="S3.SS2.p3.9.m9.2.2.3.cmml" xref="S3.SS2.p3.9.m9.2.2.3"><minus id="S3.SS2.p3.9.m9.2.2.3.1.cmml" xref="S3.SS2.p3.9.m9.2.2.3.1"></minus><apply id="S3.SS2.p3.9.m9.2.2.3.2.cmml" xref="S3.SS2.p3.9.m9.2.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.2.2.3.2.1.cmml" xref="S3.SS2.p3.9.m9.2.2.3.2">subscript</csymbol><ci id="S3.SS2.p3.9.m9.2.2.3.2.2.cmml" xref="S3.SS2.p3.9.m9.2.2.3.2.2">ğ‘¥</ci><ci id="S3.SS2.p3.9.m9.2.2.3.2.3.cmml" xref="S3.SS2.p3.9.m9.2.2.3.2.3">ğ‘</ci></apply><apply id="S3.SS2.p3.9.m9.2.2.3.3.cmml" xref="S3.SS2.p3.9.m9.2.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.2.2.3.3.1.cmml" xref="S3.SS2.p3.9.m9.2.2.3.3">subscript</csymbol><ci id="S3.SS2.p3.9.m9.2.2.3.3.2.cmml" xref="S3.SS2.p3.9.m9.2.2.3.3.2">ğ‘¥</ci><ci id="S3.SS2.p3.9.m9.2.2.3.3.3.cmml" xref="S3.SS2.p3.9.m9.2.2.3.3.3">ğ‘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.2c">\alpha=\arctan\left(\frac{y_{c}-y_{p}}{x_{c}-x_{p}}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.9.m9.2d">italic_Î± = roman_arctan ( divide start_ARG italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT - italic_y start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_ARG start_ARG italic_x start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_ARG )</annotation></semantics></math>
which changes at every frame depending on its motion.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="425" id="S3.F7.g1" src="x7.png" width="631"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.5.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S3.F7.6.2" style="font-size:90%;">Architecture of <span class="ltx_text ltx_font_sansserif" id="S3.F7.6.2.1">DeepProbHAR</span>. The neural part extracts <em class="ltx_emph ltx_font_italic" id="S3.F7.6.2.2">concepts</em> from the compressed <span class="ltx_glossaryref" title="">CSI</span>, while a logic circuit combines such <em class="ltx_emph ltx_font_italic" id="S3.F7.6.2.3">concepts</em> to derive the target activity.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S3.F8.sf1.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F8.sf1.3.2" style="font-size:90%;">The DeepProbLog program.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="411" id="S3.F8.sf2.g1" src="x9.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F8.sf2.4.2" style="font-size:90%;">Logic circuit for the query <span class="ltx_text ltx_font_typewriter" id="S3.F8.sf2.4.2.1">activity(X,wave)</span>.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.3.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S3.F8.4.2" style="font-size:90%;">(a) DeepProbLog code implementing the neuro-symbolic architecture and (b) representation of the logic circuit for the query <span class="ltx_text ltx_font_typewriter" id="S3.F8.4.2.1">activity(X,wave)</span>. Notice we drop the left/right distinction when features are uniquely defined in the code.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.14">We index the different limb angles with <math alttext="\alpha_{l}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">Î±</mi><mi id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">ğ›¼</ci><ci id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\alpha_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_Î± start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">italic_l</annotation></semantics></math> can range from 1 to <math alttext="L" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">italic_L</annotation></semantics></math>.
To quantify the motion of each limb, we consider a sliding time window of duration <math alttext="T=3" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4.1"><semantics id="S3.SS2.p4.4.m4.1a"><mrow id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml">T</mi><mo id="S3.SS2.p4.4.m4.1.1.1" xref="S3.SS2.p4.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><eq id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1.1"></eq><ci id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2">ğ‘‡</ci><cn id="S3.SS2.p4.4.m4.1.1.3.cmml" type="integer" xref="S3.SS2.p4.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">T=3</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.4.m4.1d">italic_T = 3</annotation></semantics></math> seconds, matching the time window already applied on the <span class="ltx_glossaryref" title="">CSI</span> data processed by the <span class="ltx_glossaryref" title="">VAEs</span>.
Since the video was recorded at 30Â fps, every time window contains 90 samples of the angles <math alttext="\alpha_{l}" class="ltx_Math" display="inline" id="S3.SS2.p4.5.m5.1"><semantics id="S3.SS2.p4.5.m5.1a"><msub id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">Î±</mi><mi id="S3.SS2.p4.5.m5.1.1.3" xref="S3.SS2.p4.5.m5.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">ğ›¼</ci><ci id="S3.SS2.p4.5.m5.1.1.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">\alpha_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.5.m5.1d">italic_Î± start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math>.
Then, for each time window <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p4.6.m6.1"><semantics id="S3.SS2.p4.6.m6.1a"><mi id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><ci id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.6.m6.1d">italic_t</annotation></semantics></math>, we compute the dynamic range of every angle <math alttext="\alpha_{l}" class="ltx_Math" display="inline" id="S3.SS2.p4.7.m7.1"><semantics id="S3.SS2.p4.7.m7.1a"><msub id="S3.SS2.p4.7.m7.1.1" xref="S3.SS2.p4.7.m7.1.1.cmml"><mi id="S3.SS2.p4.7.m7.1.1.2" xref="S3.SS2.p4.7.m7.1.1.2.cmml">Î±</mi><mi id="S3.SS2.p4.7.m7.1.1.3" xref="S3.SS2.p4.7.m7.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m7.1b"><apply id="S3.SS2.p4.7.m7.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.7.m7.1.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p4.7.m7.1.1.2.cmml" xref="S3.SS2.p4.7.m7.1.1.2">ğ›¼</ci><ci id="S3.SS2.p4.7.m7.1.1.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m7.1c">\alpha_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.7.m7.1d">italic_Î± start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math>.
The result is a feature vector <math alttext="\boldsymbol{\delta}(t)" class="ltx_Math" display="inline" id="S3.SS2.p4.8.m8.1"><semantics id="S3.SS2.p4.8.m8.1a"><mrow id="S3.SS2.p4.8.m8.1.2" xref="S3.SS2.p4.8.m8.1.2.cmml"><mi id="S3.SS2.p4.8.m8.1.2.2" xref="S3.SS2.p4.8.m8.1.2.2.cmml">ğœ¹</mi><mo id="S3.SS2.p4.8.m8.1.2.1" xref="S3.SS2.p4.8.m8.1.2.1.cmml">â¢</mo><mrow id="S3.SS2.p4.8.m8.1.2.3.2" xref="S3.SS2.p4.8.m8.1.2.cmml"><mo id="S3.SS2.p4.8.m8.1.2.3.2.1" stretchy="false" xref="S3.SS2.p4.8.m8.1.2.cmml">(</mo><mi id="S3.SS2.p4.8.m8.1.1" xref="S3.SS2.p4.8.m8.1.1.cmml">t</mi><mo id="S3.SS2.p4.8.m8.1.2.3.2.2" stretchy="false" xref="S3.SS2.p4.8.m8.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m8.1b"><apply id="S3.SS2.p4.8.m8.1.2.cmml" xref="S3.SS2.p4.8.m8.1.2"><times id="S3.SS2.p4.8.m8.1.2.1.cmml" xref="S3.SS2.p4.8.m8.1.2.1"></times><ci id="S3.SS2.p4.8.m8.1.2.2.cmml" xref="S3.SS2.p4.8.m8.1.2.2">ğœ¹</ci><ci id="S3.SS2.p4.8.m8.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m8.1c">\boldsymbol{\delta}(t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.8.m8.1d">bold_italic_Î´ ( italic_t )</annotation></semantics></math> with <math alttext="L" class="ltx_Math" display="inline" id="S3.SS2.p4.9.m9.1"><semantics id="S3.SS2.p4.9.m9.1a"><mi id="S3.SS2.p4.9.m9.1.1" xref="S3.SS2.p4.9.m9.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.9.m9.1b"><ci id="S3.SS2.p4.9.m9.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.9.m9.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.9.m9.1d">italic_L</annotation></semantics></math> elements describing the amount of motion of each limb during a time window.
Let us assume that a limb was not moving during the time window <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p4.10.m10.1"><semantics id="S3.SS2.p4.10.m10.1a"><mi id="S3.SS2.p4.10.m10.1.1" xref="S3.SS2.p4.10.m10.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.10.m10.1b"><ci id="S3.SS2.p4.10.m10.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.10.m10.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.10.m10.1d">italic_t</annotation></semantics></math> (<span class="ltx_ERROR undefined" id="S3.SS2.p4.14.1">\eg</span>one leg when the candidate is clapping); then, the associated <math alttext="\alpha_{l}" class="ltx_Math" display="inline" id="S3.SS2.p4.11.m11.1"><semantics id="S3.SS2.p4.11.m11.1a"><msub id="S3.SS2.p4.11.m11.1.1" xref="S3.SS2.p4.11.m11.1.1.cmml"><mi id="S3.SS2.p4.11.m11.1.1.2" xref="S3.SS2.p4.11.m11.1.1.2.cmml">Î±</mi><mi id="S3.SS2.p4.11.m11.1.1.3" xref="S3.SS2.p4.11.m11.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.11.m11.1b"><apply id="S3.SS2.p4.11.m11.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.11.m11.1.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.p4.11.m11.1.1.2.cmml" xref="S3.SS2.p4.11.m11.1.1.2">ğ›¼</ci><ci id="S3.SS2.p4.11.m11.1.1.3.cmml" xref="S3.SS2.p4.11.m11.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.11.m11.1c">\alpha_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.11.m11.1d">italic_Î± start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> is barely changing, and <math alttext="\delta_{l}(t)" class="ltx_Math" display="inline" id="S3.SS2.p4.12.m12.1"><semantics id="S3.SS2.p4.12.m12.1a"><mrow id="S3.SS2.p4.12.m12.1.2" xref="S3.SS2.p4.12.m12.1.2.cmml"><msub id="S3.SS2.p4.12.m12.1.2.2" xref="S3.SS2.p4.12.m12.1.2.2.cmml"><mi id="S3.SS2.p4.12.m12.1.2.2.2" xref="S3.SS2.p4.12.m12.1.2.2.2.cmml">Î´</mi><mi id="S3.SS2.p4.12.m12.1.2.2.3" xref="S3.SS2.p4.12.m12.1.2.2.3.cmml">l</mi></msub><mo id="S3.SS2.p4.12.m12.1.2.1" xref="S3.SS2.p4.12.m12.1.2.1.cmml">â¢</mo><mrow id="S3.SS2.p4.12.m12.1.2.3.2" xref="S3.SS2.p4.12.m12.1.2.cmml"><mo id="S3.SS2.p4.12.m12.1.2.3.2.1" stretchy="false" xref="S3.SS2.p4.12.m12.1.2.cmml">(</mo><mi id="S3.SS2.p4.12.m12.1.1" xref="S3.SS2.p4.12.m12.1.1.cmml">t</mi><mo id="S3.SS2.p4.12.m12.1.2.3.2.2" stretchy="false" xref="S3.SS2.p4.12.m12.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.12.m12.1b"><apply id="S3.SS2.p4.12.m12.1.2.cmml" xref="S3.SS2.p4.12.m12.1.2"><times id="S3.SS2.p4.12.m12.1.2.1.cmml" xref="S3.SS2.p4.12.m12.1.2.1"></times><apply id="S3.SS2.p4.12.m12.1.2.2.cmml" xref="S3.SS2.p4.12.m12.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.12.m12.1.2.2.1.cmml" xref="S3.SS2.p4.12.m12.1.2.2">subscript</csymbol><ci id="S3.SS2.p4.12.m12.1.2.2.2.cmml" xref="S3.SS2.p4.12.m12.1.2.2.2">ğ›¿</ci><ci id="S3.SS2.p4.12.m12.1.2.2.3.cmml" xref="S3.SS2.p4.12.m12.1.2.2.3">ğ‘™</ci></apply><ci id="S3.SS2.p4.12.m12.1.1.cmml" xref="S3.SS2.p4.12.m12.1.1">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.12.m12.1c">\delta_{l}(t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.12.m12.1d">italic_Î´ start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_t )</annotation></semantics></math> will be minimal.
On the other hand, if we look at the same leg when the person is walking, we will see that <math alttext="\alpha_{l}" class="ltx_Math" display="inline" id="S3.SS2.p4.13.m13.1"><semantics id="S3.SS2.p4.13.m13.1a"><msub id="S3.SS2.p4.13.m13.1.1" xref="S3.SS2.p4.13.m13.1.1.cmml"><mi id="S3.SS2.p4.13.m13.1.1.2" xref="S3.SS2.p4.13.m13.1.1.2.cmml">Î±</mi><mi id="S3.SS2.p4.13.m13.1.1.3" xref="S3.SS2.p4.13.m13.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.13.m13.1b"><apply id="S3.SS2.p4.13.m13.1.1.cmml" xref="S3.SS2.p4.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.13.m13.1.1.1.cmml" xref="S3.SS2.p4.13.m13.1.1">subscript</csymbol><ci id="S3.SS2.p4.13.m13.1.1.2.cmml" xref="S3.SS2.p4.13.m13.1.1.2">ğ›¼</ci><ci id="S3.SS2.p4.13.m13.1.1.3.cmml" xref="S3.SS2.p4.13.m13.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.13.m13.1c">\alpha_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.13.m13.1d">italic_Î± start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> describes broader trajectories, and <math alttext="\delta_{l}(t)" class="ltx_Math" display="inline" id="S3.SS2.p4.14.m14.1"><semantics id="S3.SS2.p4.14.m14.1a"><mrow id="S3.SS2.p4.14.m14.1.2" xref="S3.SS2.p4.14.m14.1.2.cmml"><msub id="S3.SS2.p4.14.m14.1.2.2" xref="S3.SS2.p4.14.m14.1.2.2.cmml"><mi id="S3.SS2.p4.14.m14.1.2.2.2" xref="S3.SS2.p4.14.m14.1.2.2.2.cmml">Î´</mi><mi id="S3.SS2.p4.14.m14.1.2.2.3" xref="S3.SS2.p4.14.m14.1.2.2.3.cmml">l</mi></msub><mo id="S3.SS2.p4.14.m14.1.2.1" xref="S3.SS2.p4.14.m14.1.2.1.cmml">â¢</mo><mrow id="S3.SS2.p4.14.m14.1.2.3.2" xref="S3.SS2.p4.14.m14.1.2.cmml"><mo id="S3.SS2.p4.14.m14.1.2.3.2.1" stretchy="false" xref="S3.SS2.p4.14.m14.1.2.cmml">(</mo><mi id="S3.SS2.p4.14.m14.1.1" xref="S3.SS2.p4.14.m14.1.1.cmml">t</mi><mo id="S3.SS2.p4.14.m14.1.2.3.2.2" stretchy="false" xref="S3.SS2.p4.14.m14.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.14.m14.1b"><apply id="S3.SS2.p4.14.m14.1.2.cmml" xref="S3.SS2.p4.14.m14.1.2"><times id="S3.SS2.p4.14.m14.1.2.1.cmml" xref="S3.SS2.p4.14.m14.1.2.1"></times><apply id="S3.SS2.p4.14.m14.1.2.2.cmml" xref="S3.SS2.p4.14.m14.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.14.m14.1.2.2.1.cmml" xref="S3.SS2.p4.14.m14.1.2.2">subscript</csymbol><ci id="S3.SS2.p4.14.m14.1.2.2.2.cmml" xref="S3.SS2.p4.14.m14.1.2.2.2">ğ›¿</ci><ci id="S3.SS2.p4.14.m14.1.2.2.3.cmml" xref="S3.SS2.p4.14.m14.1.2.2.3">ğ‘™</ci></apply><ci id="S3.SS2.p4.14.m14.1.1.cmml" xref="S3.SS2.p4.14.m14.1.1">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.14.m14.1c">\delta_{l}(t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.14.m14.1d">italic_Î´ start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_t )</annotation></semantics></math> will be much more significant.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.4">The computation of the dynamic range of the angles <math alttext="\alpha_{l}" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><msub id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">Î±</mi><mi id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">ğ›¼</ci><ci id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\alpha_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_Î± start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> provides a quick estimation of each limbâ€™s motion amount. It is a simple way to discriminate the target activities in our dataset.
In <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F5" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>, we show how just the feature <math alttext="\delta_{l}" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><msub id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">Î´</mi><mi id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">ğ›¿</ci><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\delta_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">italic_Î´ start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> (where <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m3.1"><semantics id="S3.SS2.p5.3.m3.1a"><mi id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><ci id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.3.m3.1d">italic_l</annotation></semantics></math> is the right lower leg) is enough to separate three families of activities.
The first family involves broad movements of the lower legs (<span class="ltx_text ltx_font_italic" id="S3.SS2.p5.4.1">walking</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.4.2">running</span>); the second one involves modest movements of the lower legs (<span class="ltx_text ltx_font_italic" id="S3.SS2.p5.4.3">jumping</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.4.4">squatting</span>); finally, the last family of activities consists of no movement of the leg at all (<span class="ltx_text ltx_font_italic" id="S3.SS2.p5.4.5">waving</span>, <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.4.6">clapping</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.4.7">wiping</span>).
Arguably, the limbsâ€™ motion can be estimated using more sophisticated approaches, <span class="ltx_ERROR undefined" id="S3.SS2.p5.4.8">\eg</span>involving a Fourier analysis of the angles <math alttext="\alpha_{l}" class="ltx_Math" display="inline" id="S3.SS2.p5.4.m4.1"><semantics id="S3.SS2.p5.4.m4.1a"><msub id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml"><mi id="S3.SS2.p5.4.m4.1.1.2" xref="S3.SS2.p5.4.m4.1.1.2.cmml">Î±</mi><mi id="S3.SS2.p5.4.m4.1.1.3" xref="S3.SS2.p5.4.m4.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p5.4.m4.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1.2">ğ›¼</ci><ci id="S3.SS2.p5.4.m4.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">\alpha_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.4.m4.1d">italic_Î± start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> to identify periodic patterns or taking into account the perspective effects.
However, we argue that taking the dynamic range is a reasonably simple and effective solution.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">The video dataset analysis and the features extracted from the limbsâ€™ motion resulted in the decision tree shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F6" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>.
This rule-based classification combines six <em class="ltx_emph ltx_font_italic" id="S3.SS2.p6.1.1">concepts</em> extracted from the dataset that can take binary values.
At every decision node, one single value of the <math alttext="\boldsymbol{\delta}" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.1"><semantics id="S3.SS2.p6.1.m1.1a"><mi id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml">ğœ¹</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><ci id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">ğœ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">\boldsymbol{\delta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.1d">bold_italic_Î´</annotation></semantics></math> vector is compared against a threshold that determines whether the corresponding limb is moving.
Note that, in general, we are not bound to consider just binary decisions; indeed, according to what we already observed in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F5" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>, the first node at the top of the tree in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F6" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a> could have had three outputs.
However, we forced the model to have only binary decisions to simplify the implementation of our neuro-symbolic architecture.
In this way, the same feature is tested twice against two different thresholds (<span class="ltx_ERROR undefined" id="S3.SS2.p6.1.2">\cf</span>, <span class="ltx_text ltx_font_italic" id="S3.SS2.p6.1.3">Right lower leg moves</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p6.1.4">Right lower leg moves a lot</span> in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F6" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_sansserif" id="S3.SS3.6.2">DeepProbHAR</span><span class="ltx_text ltx_font_italic" id="S3.SS3.7.3">: A Neuro-Symbolic Architecture for Human Activity Recognition Using Wi-Fi Data</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.SS2" title="III-B Domain-Dependent Rule from Different Modality â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>, we have defined the declarative knowledge necessary to identify the different target activities and a simple way to extract the information from the video data.
This section introduces <span class="ltx_text ltx_font_sansserif" id="S3.SS3.p1.1.1">DeepProbHAR</span>, the first neuro-symbolic system designed explicitly for <span class="ltx_glossaryref" title="">HAR</span> applications using <span class="ltx_text" id="S3.SS3.p1.1.2">Wi-Fi</span> sensing data.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.2">A schematic overview of the proposed architecture is shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F7" title="In III-B Domain-Dependent Rule from Different Modality â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">7</span></a>.
The system takes in input the compressed <span class="ltx_glossaryref" title="">CSI</span> data encoded by one of the <span class="ltx_glossaryref" title="">VAEs</span> introduced in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.SS3" title="II-C Dataset Pre-Processing and VAE Architectures â€£ II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>.
This implies that we can define several architectures depending on the <span class="ltx_glossaryref" title="">VAE</span> used to pre-process the dataset.
We consider four single-antenna architectures <span class="ltx_text" id="S3.SS3.p2.1.1">No-Fused-<math alttext="\mathrm{x}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.1.m1.1"><semantics id="S3.SS3.p2.1.1.m1.1a"><mi id="S3.SS3.p2.1.1.m1.1.1" mathvariant="normal" xref="S3.SS3.p2.1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.1.m1.1b"><ci id="S3.SS3.p2.1.1.m1.1.1.cmml" xref="S3.SS3.p2.1.1.m1.1.1">x</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.1.m1.1c">\mathrm{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.1.m1.1d">roman_x</annotation></semantics></math></span> (<math alttext="\mathrm{x}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m1.1"><semantics id="S3.SS3.p2.2.m1.1a"><mi id="S3.SS3.p2.2.m1.1.1" mathvariant="normal" xref="S3.SS3.p2.2.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m1.1b"><ci id="S3.SS3.p2.2.m1.1.1.cmml" xref="S3.SS3.p2.2.m1.1.1">x</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m1.1c">\mathrm{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m1.1d">roman_x</annotation></semantics></math> ranging from 1 to 4) and two architectures fusing the data from multiple antennas, namely <span class="ltx_text" id="S3.SS3.p2.2.2">Early-Fusing</span> and <span class="ltx_text" id="S3.SS3.p2.2.3">Delayed-Fusing</span> (<span class="ltx_ERROR undefined" id="S3.SS3.p2.2.4">\cf</span><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S2.SS3" title="II-C Dataset Pre-Processing and VAE Architectures â€£ II Background â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>).
All the <span class="ltx_text ltx_font_sansserif" id="S3.SS3.p2.2.5">DeepProbHAR</span> architectures employ six small <span class="ltx_glossaryref" title="">MLPs</span> to extract binary symbols from the input <span class="ltx_glossaryref" title="">CSI</span> data, which are combined using logic rules to estimate the target activity.
Every <span class="ltx_glossaryref" title="">MLP</span> contains only two hidden layers with 8 neurons each, activated using a ReLU function, and a binary output layer with a SoftMax activation function.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">The DeepProbLog code used to combine the neural networksâ€™ output with the logic part of the architecture is listed in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F8.sf1" title="In Figure 8 â€£ III-B Domain-Dependent Rule from Different Modality â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">8(a)</span></a>.
The DeepProbLog program defines the six <span class="ltx_glossaryref" title="">MLPs</span> such that each <span class="ltx_glossaryref" title="">MLP</span> should correspond to a different decision node in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F6" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>.
Then, a list of predicates implements the rules described in the decision tree constructed starting from the video data analysis.
<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F8.sf2" title="In Figure 8 â€£ III-B Domain-Dependent Rule from Different Modality â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">8(b)</span></a> shows an example of the logic circuit derived from the DeepProbLog code, where the grey rectangles correspond to the probabilistic facts identified by the neural networks <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.1">net1</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.2">net5</span> and the red rectangle corresponds to the query defined by the formula on line 11 of the code listing.
The white box with the <math alttext="\otimes" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mo id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">âŠ—</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><csymbol cd="latexml" id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">tensor-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\otimes</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">âŠ—</annotation></semantics></math> symbol represents the logical operator <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.3">AND</span> applied to its children.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experimental Results</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We now evaluate the classification performance of all the <span class="ltx_text ltx_font_sansserif" id="S4.p1.1.1">DeepProbHAR</span> models on the selected public dataset, which comprises both the <span class="ltx_glossaryref" title="">CSI</span> data and anonymised video recordingsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib3" title="">3</a>]</cite>.
First, we certify that the rules extracted from the video dataset yield good accuracy in estimating the target activities (<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.SS1" title="IV-A Validation of Declarative Knowledge â€£ IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>).
Then, we measure the classification accuracy of the <span class="ltx_text ltx_font_sansserif" id="S4.p1.1.2">DeepProbHAR</span> models (<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.SS2" title="IV-B Performance of DeepProbHAR â€£ IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>).
Since <span class="ltx_text ltx_font_sansserif" id="S4.p1.1.3">DeepProbHAR</span> leverages declarative knowledge gathered for a different modality (the video data feed), we expect such a reference to be the upper limit that <span class="ltx_text ltx_font_sansserif" id="S4.p1.1.4">DeepProbHAR</span> can reach.
Finally, we compare the results obtained with the neuro-symbolic architecture with those obtained by more traditional approaches based on neural networks<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Code available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/marcocominelli/csi-vae/tree/fusion2024" title="">https://github.com/marcocominelli/csi-vae/tree/fusion2024</a></span></span></span>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.5.1.1" style="font-size:90%;">Table I</span>: </span><span class="ltx_text" id="S4.T1.6.2" style="font-size:90%;">Accuracy and average precision, recall and F1 score of <span class="ltx_text ltx_font_sansserif" id="S4.T1.6.2.1">DeepProbHAR</span> with different data fusion strategies. The accuracy of the extracted declarative knowledge tested over the video feed is provided as <span class="ltx_text ltx_font_italic" id="S4.T1.6.2.2">Video reference</span> and is the upper limit of <span class="ltx_text ltx_font_sansserif" id="S4.T1.6.2.3">DeepProbHAR</span>â€™s performance.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.7.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.1.1.1">Architecture</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.1.2.1">Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.1.3.1">Precision</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.1.4.1">Recall</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.1.5.1">F1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.7.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.7.2.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.7.2.2.1.1">Video reference</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.2.2.2"><span class="ltx_text ltx_font_italic" id="S4.T1.7.2.2.2.1">0.98</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.2.2.3"><span class="ltx_text ltx_font_italic" id="S4.T1.7.2.2.3.1">0.99</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.2.2.4"><span class="ltx_text ltx_font_italic" id="S4.T1.7.2.2.4.1">0.98</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.2.2.5"><span class="ltx_text ltx_font_italic" id="S4.T1.7.2.2.5.1">0.98</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.7.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.3.1.1">No-Fused-1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.3.1.2">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.3.1.3">0.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.3.1.4">0.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.3.1.5">0.50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.4.2.1">No-Fused-2</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.4.2.2">0.62</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.4.2.3">0.62</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.4.2.4">0.63</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.4.2.5">0.62</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.5.3.1">No-Fused-3</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.3.2">0.53</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.3.3">0.53</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.3.4">0.51</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.3.5">0.52</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.6.4.1">No-Fused-4</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.6.4.2">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.6.4.3">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.6.4.4">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.6.4.5">0.77</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.5.1">Early-Fusing</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.5.2">0.84</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.5.3">0.84</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.5.4">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.5.5">0.84</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.7.8.6.1">Delayed-Fusing</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.8.6.2"><span class="ltx_text ltx_font_bold" id="S4.T1.7.8.6.2.1">0.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.8.6.3"><span class="ltx_text ltx_font_bold" id="S4.T1.7.8.6.3.1">0.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.8.6.4"><span class="ltx_text ltx_font_bold" id="S4.T1.7.8.6.4.1">0.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.8.6.5"><span class="ltx_text ltx_font_bold" id="S4.T1.7.8.6.5.1">0.95</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.2.1.1" style="font-size:90%;">Table II</span>: </span><span class="ltx_text" id="S4.T2.3.2" style="font-size:90%;">Comparison with state-of-the-art non-neuro-symbolic approaches using a single MLP trained on the corresponding dataset of each different architecture.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.1.1">Architecture</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.1.1.2"><span class="ltx_text ltx_font_sansserif" id="S4.T2.4.1.1.2.1">DeepProbHAR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.3.1">Small MLP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.4.1">Large MLP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.4.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.4.2.1.1">No-Fused-1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.2.1.2">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.2.1.3">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.2.1.4">0.62</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.3.2.1">No-Fused-2</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.3.2.2">0.62</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.3.2.3">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.3.2.4">0.76</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.4.3.1">No-Fused-3</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.3.2">0.53</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.3.3">0.58</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.3.4">0.59</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.5.4.1">No-Fused-4</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.5.4.2">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.5.4.3">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.5.4.4">0.80</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.6.5.1">Early-Fusing</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.6.5.2">0.84</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.6.5.3">0.88</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.6.5.4">0.89</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.4.7.6.1">Delayed-Fusing</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T2.4.7.6.2.1">0.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T2.4.7.6.3.1">0.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.7.6.4.1">0.98</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Validation of Declarative Knowledge</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To ensure that the declarative knowledge gathered from the video data is enough to produce sensible guesses about the target activity, we implemented the rule-based classifier introduced in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F6" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, using manually fine-tuned thresholds on the anglesâ€™ features <math alttext="{\boldsymbol{\delta}}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">ğœ¹</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğœ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">{\boldsymbol{\delta}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">bold_italic_Î´</annotation></semantics></math>) extracted directly from the video data.
Such a classifier operating over the video feed achieves <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.1">an accuracy of 98%</em> over the seven target activities.
Interestingly, classification errors happen between the activities <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.2">walk</em> and <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.3">run</em>.
Arguably, these are the most challenging activities to discriminate, even for a human viewer, mainly because of the indoor experimental setting.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">Table III</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">Classification accuracy of the various <span class="ltx_text ltx_font_sansserif" id="S4.T3.4.2.1">DeepProbHAR</span>â€™s MLPs compared to specialised MLPs trained on a finely labelled dataset of simple movements.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="8" id="S4.T3.5.1.1.1">
<span class="ltx_text ltx_font_sansserif" id="S4.T3.5.1.1.1.1">DeepProbHAR</span><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.1.2">â€™s MLPs</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.5.2.1">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.5.2.1.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.2.1">MLP 1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.3.1">MLP 2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.4.1">MLP 3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.2.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.5.1">MLP 4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.2.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.6.1">MLP 5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.2.1.7"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.7.1">MLP 6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.2.1.8"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.8.1">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.2.1.1">Architecture</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.3.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.2.2.1">(right lower leg)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.3.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.2.3.1">(right lower leg #2)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.3.2.4"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.2.4.1">(right arm)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.3.2.5"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.2.5.1">(left upper leg)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.3.2.6"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.2.6.1">(left arm)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.3.2.7"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.2.7.1">(left forearm)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.3.2.8"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.2.8.1">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.5.4.3.1">No-Fused-1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.4.3.2">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.4.3.3">0.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.4.3.4">0.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.4.3.5">0.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.4.3.6">0.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.4.3.7">0.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.4.3.8">0.50</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.5.4.1">No-Fused-2</th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.4.2">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.4.3">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.4.4">0.97</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.4.5">0.98</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.4.6">0.89</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.4.7">0.93</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.4.8">0.62</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.6.5.1">No-Fused-3</th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.5.2">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.5.3">0.66</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.5.4">0.72</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.5.5">0.70</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.5.6">0.88</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.5.7">0.97</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.5.8">0.53</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.7.6.1">No-Fused-4</th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.6.2">0.87</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.7.6.3.1">0.98</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.6.4">0.92</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.6.5">0.87</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.6.6">0.88</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.6.7"><span class="ltx_text ltx_font_bold" id="S4.T3.5.7.6.7.1">1.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.6.8">0.76</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.8.7.1">Early-Fusing</th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.7.2">0.94</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.7.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.8.7.3.1">0.98</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.7.4">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.7.5">0.99</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.7.6">0.88</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.7.7">0.99</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.7.8">0.84</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.9.8.1">Delayed-Fusing</th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.8.2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.9.8.2.1">0.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.8.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.9.8.3.1">0.98</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.8.4"><span class="ltx_text ltx_font_bold" id="S4.T3.5.9.8.4.1">0.96</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.8.5"><span class="ltx_text ltx_font_bold" id="S4.T3.5.9.8.5.1">1.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.8.6"><span class="ltx_text ltx_font_bold" id="S4.T3.5.9.8.6.1">0.96</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.8.7"><span class="ltx_text ltx_font_bold" id="S4.T3.5.9.8.7.1">1.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.8.8"><span class="ltx_text ltx_font_bold" id="S4.T3.5.9.8.8.1">0.95</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_border_tt" colspan="8" id="S4.T3.5.10.9.1"><span class="ltx_text ltx_font_bold" id="S4.T3.5.10.9.1.1">MLPs trained independently on finely labelled dataset of simple movements</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.5.11.10">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.5.11.10.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.11.10.2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.11.10.2.1">MLP 1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.11.10.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.11.10.3.1">MLP 2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.11.10.4"><span class="ltx_text ltx_font_bold" id="S4.T3.5.11.10.4.1">MLP 3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.11.10.5"><span class="ltx_text ltx_font_bold" id="S4.T3.5.11.10.5.1">MLP 4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.11.10.6"><span class="ltx_text ltx_font_bold" id="S4.T3.5.11.10.6.1">MLP 5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.11.10.7"><span class="ltx_text ltx_font_bold" id="S4.T3.5.11.10.7.1">MLP 6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.11.10.8"><span class="ltx_text ltx_font_bold" id="S4.T3.5.11.10.8.1">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.12.11.1"><span class="ltx_text ltx_font_bold" id="S4.T3.5.12.11.1.1">Architecture</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.11.2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.12.11.2.1">(right lower leg)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.11.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.12.11.3.1">(right lower leg #2)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.11.4"><span class="ltx_text ltx_font_bold" id="S4.T3.5.12.11.4.1">(right arm)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.11.5"><span class="ltx_text ltx_font_bold" id="S4.T3.5.12.11.5.1">(left upper leg)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.11.6"><span class="ltx_text ltx_font_bold" id="S4.T3.5.12.11.6.1">(left arm)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.11.7"><span class="ltx_text ltx_font_bold" id="S4.T3.5.12.11.7.1">(left forearm)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.11.8"><span class="ltx_text ltx_font_bold" id="S4.T3.5.12.11.8.1">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.5.13.12.1">No-Fused-1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.13.12.2">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.13.12.3">0.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.13.12.4">0.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.13.12.5">0.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.13.12.6">0.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.13.12.7">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.13.12.8">0.59</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.14.13.1">No-Fused-2</th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.13.2">0.87</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.13.3">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.13.4">0.97</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.13.5">0.99</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.13.6">0.90</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.13.7">0.93</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.13.8">0.70</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.15.14.1">No-Fused-3</th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.15.14.2">0.89</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.15.14.3">0.68</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.15.14.4">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.15.14.5">0.70</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.15.14.6">0.90</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.15.14.7">0.99</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.15.14.8">0.58</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.16.15.1">No-Fused-4</th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.15.2">0.90</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.15.3">0.98</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.15.4">0.92</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.15.5">0.88</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.15.6">0.88</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.15.7"><span class="ltx_text ltx_font_bold" id="S4.T3.5.16.15.7.1">1.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.15.8">0.79</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.17.16.1">Early-Fusing</th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.16.2">0.96</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.16.3">0.98</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.16.4">0.87</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.16.5"><span class="ltx_text ltx_font_bold" id="S4.T3.5.17.16.5.1">1.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.16.6">0.89</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.16.7"><span class="ltx_text ltx_font_bold" id="S4.T3.5.17.16.7.1">1.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.16.8">0.86</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.5.18.17.1">Delayed-Fusing</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.17.2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.18.17.2.1">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.17.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.18.17.3.1">0.99</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.17.4"><span class="ltx_text ltx_font_bold" id="S4.T3.5.18.17.4.1">0.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.17.5"><span class="ltx_text ltx_font_bold" id="S4.T3.5.18.17.5.1">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.17.6"><span class="ltx_text ltx_font_bold" id="S4.T3.5.18.17.6.1">0.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.17.7"><span class="ltx_text ltx_font_bold" id="S4.T3.5.18.17.7.1">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.17.8"><span class="ltx_text ltx_font_bold" id="S4.T3.5.18.17.8.1">0.98</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Performance of <span class="ltx_text ltx_font_sansserif ltx_font_upright" id="S4.SS2.6.2.1">DeepProbHAR</span></span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To evaluate the performance of the different architectures, we partition every compressed <span class="ltx_glossaryref" title="">CSI</span> dataset into a training and a testing set with an 80/20 split.
In the following, all the models are trained with a learning rate of 0.001 for 20 epochs.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.T1" title="In IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">I</span></a> summarises the main results of the <span class="ltx_text ltx_font_sansserif" id="S4.SS2.p2.1.1">DeepProbHAR</span> architectures for all the fusion strategies considered in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite> (<span class="ltx_ERROR undefined" id="S4.SS2.p2.1.2">\cf</span><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.SS3" title="III-C DeepProbHAR: A Neuro-Symbolic Architecture for Human Activity Recognition Using Wi-Fi Data â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>).
Similarly to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite>â€™s results, the <span class="ltx_text" id="S4.SS2.p2.1.3">Delayed-Fusing</span> fusion strategy yields the best results.
The modelâ€™s accuracy that combines the data of different antennas, each processed by a separate <span class="ltx_glossaryref" title="">VAE</span>, closely approaches the accuracy of the reference classifier trained on video data.
However, as highlighted by the confusion matrixes of <span class="ltx_text ltx_font_sansserif" id="S4.SS2.p2.1.4">DeepProbHAR</span> for the various data fusion techniques (<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S5.F9" title="In V Discussion â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">9</span></a>), classification errors are not limited to the classes <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.5">walk</em> and <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.6">run</em>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">In <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.T2" title="In IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">II</span></a>, we compare the results with two state-of-the-art non-neuro-symbolic architectures derived from the work in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite>.
These architectures use the same <span class="ltx_glossaryref" title="">VAE</span> and one single <span class="ltx_glossaryref" title="">MLP</span> substituting the entire neuro-symbolic architecture.
In the neuro-symbolic architecture, each of the six <span class="ltx_glossaryref" title="">MLPs</span> learning a separate feature has 130 parameters (226 for the <span class="ltx_text" id="S4.SS2.p3.1.1">Delayed-Fusing</span> approach).
If we try to approximate the neuro-symbolic architecture using a single <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">small <span class="ltx_glossaryref" title="">MLP</span></span> (2 hidden layers, 8 neurons each), the resulting model has 175 parameters (271 for the <span class="ltx_text" id="S4.SS2.p3.1.3">Delayed-Fusing</span> approach).
Arguably, since the neuro-symbolic architecture features six <span class="ltx_glossaryref" title="">MLPs</span>, it would be interesting to consider a <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.4">large <span class="ltx_glossaryref" title="">MLP</span></span> (2 hidden layers, 22 neurons each) whose number of parameters closely matches the one of all the neuro-symbolic <span class="ltx_glossaryref" title="">MLPs</span>.
The results in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.T2" title="In IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">II</span></a> reveal that the neuro-symbolic architectures perform worse than the single <span class="ltx_glossaryref" title="">MLPs</span> when considering just one antenna, but their accuracy becomes similar when fusing the data from multiple antennas.
We also highlight that the models in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib2" title="">2</a>]</cite> were evaluated on different activities, so the corresponding <span class="ltx_glossaryref" title="">MLPs</span> have been trained from scratch in this work.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.T3" title="In IV-A Validation of Declarative Knowledge â€£ IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">III</span></a> (top) shows the results of the six <span class="ltx_glossaryref" title="">MLPs</span> that have been trained in <span class="ltx_text ltx_font_sansserif" id="S5.p1.1.1">DeepProbHAR</span>, one for each feature as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F6" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>. For the sake of comparison, we also trained independently six <span class="ltx_glossaryref" title="">MLP</span> over a finely labelled dataset of simple movements â€“ it is worth mentioning that labelling such a dataset could be extremely costly in less controlled settings â€“ so that each <span class="ltx_glossaryref" title="">MLP</span> was optimised to classify only one of the relevant features, see <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.T3" title="In IV-A Validation of Declarative Knowledge â€£ IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">III</span></a> (bottom).</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We wish to point out three aspects.
First, the performance of each of the <span class="ltx_text ltx_font_sansserif" id="S5.p2.1.1">DeepProbHAR</span>â€™s <span class="ltx_glossaryref" title="">MLP</span> trained on sparse data (top of the table) appears to be close to the optimised <span class="ltx_glossaryref" title="">MLPs</span> trained over the finely-labelled dataset.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.2">Secondly, the overall accuracy (last column) is lower than each of the accuracies for all the <span class="ltx_glossaryref" title="">MLPs</span>.
This is due to the independence assumptions underlying the training of all such neural networks and their subsequent usage for classification, whether via <span class="ltx_text ltx_font_sansserif" id="S5.p3.2.1">DeepProbHAR</span> (which relies on the same strong independence assumptions of ProbLog <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib18" title="">18</a>]</cite><span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>For a more comprehensive discussion on the role of probabilistic dependencies among variables in probabilistic circuits â€“ including those derived from ProbLog â€“ we refer the interested reader to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib21" title="">21</a>]</cite>.</span></span></span>) or by a deterministic classifier that follows the decision tree in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F6" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>.
Indeed, given a neural network <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S5.p3.1.m1.1"><semantics id="S5.p3.1.m1.1a"><mrow id="S5.p3.1.m1.1.2" xref="S5.p3.1.m1.1.2.cmml"><mi id="S5.p3.1.m1.1.2.2" xref="S5.p3.1.m1.1.2.2.cmml">f</mi><mo id="S5.p3.1.m1.1.2.1" xref="S5.p3.1.m1.1.2.1.cmml">â¢</mo><mrow id="S5.p3.1.m1.1.2.3.2" xref="S5.p3.1.m1.1.2.cmml"><mo id="S5.p3.1.m1.1.2.3.2.1" stretchy="false" xref="S5.p3.1.m1.1.2.cmml">(</mo><mo id="S5.p3.1.m1.1.1" lspace="0em" rspace="0em" xref="S5.p3.1.m1.1.1.cmml">â‹…</mo><mo id="S5.p3.1.m1.1.2.3.2.2" stretchy="false" xref="S5.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><apply id="S5.p3.1.m1.1.2.cmml" xref="S5.p3.1.m1.1.2"><times id="S5.p3.1.m1.1.2.1.cmml" xref="S5.p3.1.m1.1.2.1"></times><ci id="S5.p3.1.m1.1.2.2.cmml" xref="S5.p3.1.m1.1.2.2">ğ‘“</ci><ci id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">f(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S5.p3.1.m1.1d">italic_f ( â‹… )</annotation></semantics></math>, its accuracy can be seen as the probability of <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S5.p3.2.m2.1"><semantics id="S5.p3.2.m2.1a"><mrow id="S5.p3.2.m2.1.2" xref="S5.p3.2.m2.1.2.cmml"><mi id="S5.p3.2.m2.1.2.2" xref="S5.p3.2.m2.1.2.2.cmml">f</mi><mo id="S5.p3.2.m2.1.2.1" xref="S5.p3.2.m2.1.2.1.cmml">â¢</mo><mrow id="S5.p3.2.m2.1.2.3.2" xref="S5.p3.2.m2.1.2.cmml"><mo id="S5.p3.2.m2.1.2.3.2.1" stretchy="false" xref="S5.p3.2.m2.1.2.cmml">(</mo><mo id="S5.p3.2.m2.1.1" lspace="0em" rspace="0em" xref="S5.p3.2.m2.1.1.cmml">â‹…</mo><mo id="S5.p3.2.m2.1.2.3.2.2" stretchy="false" xref="S5.p3.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.1b"><apply id="S5.p3.2.m2.1.2.cmml" xref="S5.p3.2.m2.1.2"><times id="S5.p3.2.m2.1.2.1.cmml" xref="S5.p3.2.m2.1.2.1"></times><ci id="S5.p3.2.m2.1.2.2.cmml" xref="S5.p3.2.m2.1.2.2">ğ‘“</ci><ci id="S5.p3.2.m2.1.1.cmml" xref="S5.p3.2.m2.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.1c">f(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S5.p3.2.m2.1d">italic_f ( â‹… )</annotation></semantics></math> to return the correct answer for a given input.
In <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F6" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, for instance, we see that classifying <span class="ltx_text ltx_font_italic" id="S5.p3.2.2">running</span> and <span class="ltx_text ltx_font_italic" id="S5.p3.2.3">walking</span> relies on three of the classifiers whose accuracies are available in <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.T3" title="In IV-A Validation of Declarative Knowledge â€£ IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">III</span></a>: <span class="ltx_text ltx_font_bold" id="S5.p3.2.4">MLP 1</span> that tells if the right lower leg moves; <span class="ltx_text ltx_font_bold" id="S5.p3.2.5">MLP 2</span> that tells if the right lower leg moves a lot; and <span class="ltx_text ltx_font_bold" id="S5.p3.2.6">MLP 3</span> that tells if the right upper arm moves.
Under independence assumptions, the probability of correct classification of <span class="ltx_text ltx_font_italic" id="S5.p3.2.7">running</span> and <span class="ltx_text ltx_font_italic" id="S5.p3.2.8">walking</span> is the product of the probability that each of the three classifiers returns a correct answer.
The average of the probabilities of correct classification for each of the activities as the product of the probabilities of correct classifications for the <span class="ltx_glossaryref" title="">MLPs</span> used for such a classification according to <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F6" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">6</span></a> amounts to the same overall accuracy as computed in the last column of <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S4.T3" title="In IV-A Validation of Declarative Knowledge â€£ IV Experimental Results â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Finally, we observe that some <span class="ltx_glossaryref" title="">MLPs</span> (<span class="ltx_ERROR undefined" id="S5.p4.1.1">\eg</span>Â <span class="ltx_text ltx_font_bold" id="S5.p4.1.2">MLP 4</span>) reach <math alttext="1.00" class="ltx_Math" display="inline" id="S5.p4.1.m1.1"><semantics id="S5.p4.1.m1.1a"><mn id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">1.00</mn><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><cn id="S5.p4.1.m1.1.1.cmml" type="float" xref="S5.p4.1.m1.1.1">1.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">1.00</annotation><annotation encoding="application/x-llamapun" id="S5.p4.1.m1.1d">1.00</annotation></semantics></math> accuracy. From <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S3.F6" title="In III-A Primer on Neuro-Symbolic AI: the DeepProbLog Approach â€£ III Methodology â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, such a <span class="ltx_glossaryref" title="">MLP</span> is responsible for distinguishing between <span class="ltx_text ltx_font_italic" id="S5.p4.1.3">squatting</span> and <span class="ltx_text ltx_font_italic" id="S5.p4.1.4">jumping</span>.
A quick inspection of the confusion matrixes (<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#S5.F9" title="In V Discussion â€£ Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">9</span></a>) reveals that such accuracy is the product of perfect split among those two classes over the test set, indicating that it can be relatively easy for an <span class="ltx_glossaryref" title="">MLP</span> to separate the remaining two activities.</p>
</div>
<figure class="ltx_figure" id="S5.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="826" id="S5.F9.sf1.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.sf1.4.2.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S5.F9.sf1.2.1" style="font-size:90%;">No-Fused-<math alttext="\mathrm{1}" class="ltx_Math" display="inline" id="S5.F9.sf1.2.1.m1.1"><semantics id="S5.F9.sf1.2.1.m1.1b"><mn id="S5.F9.sf1.2.1.m1.1.1" xref="S5.F9.sf1.2.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.F9.sf1.2.1.m1.1c"><cn id="S5.F9.sf1.2.1.m1.1.1.cmml" type="integer" xref="S5.F9.sf1.2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.sf1.2.1.m1.1d">\mathrm{1}</annotation><annotation encoding="application/x-llamapun" id="S5.F9.sf1.2.1.m1.1e">1</annotation></semantics></math></span><span class="ltx_text" id="S5.F9.sf1.5.3" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="826" id="S5.F9.sf2.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.sf2.4.2.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S5.F9.sf2.2.1" style="font-size:90%;">No-Fused-<math alttext="\mathrm{2}" class="ltx_Math" display="inline" id="S5.F9.sf2.2.1.m1.1"><semantics id="S5.F9.sf2.2.1.m1.1b"><mn id="S5.F9.sf2.2.1.m1.1.1" xref="S5.F9.sf2.2.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.F9.sf2.2.1.m1.1c"><cn id="S5.F9.sf2.2.1.m1.1.1.cmml" type="integer" xref="S5.F9.sf2.2.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.sf2.2.1.m1.1d">\mathrm{2}</annotation><annotation encoding="application/x-llamapun" id="S5.F9.sf2.2.1.m1.1e">2</annotation></semantics></math></span><span class="ltx_text" id="S5.F9.sf2.5.3" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F9.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="826" id="S5.F9.sf3.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.sf3.4.2.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S5.F9.sf3.2.1" style="font-size:90%;">No-Fused-<math alttext="\mathrm{3}" class="ltx_Math" display="inline" id="S5.F9.sf3.2.1.m1.1"><semantics id="S5.F9.sf3.2.1.m1.1b"><mn id="S5.F9.sf3.2.1.m1.1.1" xref="S5.F9.sf3.2.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.F9.sf3.2.1.m1.1c"><cn id="S5.F9.sf3.2.1.m1.1.1.cmml" type="integer" xref="S5.F9.sf3.2.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.sf3.2.1.m1.1d">\mathrm{3}</annotation><annotation encoding="application/x-llamapun" id="S5.F9.sf3.2.1.m1.1e">3</annotation></semantics></math></span><span class="ltx_text" id="S5.F9.sf3.5.3" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F9.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="826" id="S5.F9.sf4.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.sf4.4.2.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S5.F9.sf4.2.1" style="font-size:90%;">No-Fused-<math alttext="\mathrm{4}" class="ltx_Math" display="inline" id="S5.F9.sf4.2.1.m1.1"><semantics id="S5.F9.sf4.2.1.m1.1b"><mn id="S5.F9.sf4.2.1.m1.1.1" xref="S5.F9.sf4.2.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S5.F9.sf4.2.1.m1.1c"><cn id="S5.F9.sf4.2.1.m1.1.1.cmml" type="integer" xref="S5.F9.sf4.2.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.sf4.2.1.m1.1d">\mathrm{4}</annotation><annotation encoding="application/x-llamapun" id="S5.F9.sf4.2.1.m1.1e">4</annotation></semantics></math></span><span class="ltx_text" id="S5.F9.sf4.5.3" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F9.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="826" id="S5.F9.sf5.g1" src="x14.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.sf5.3.1.1" style="font-size:90%;">(e)</span> </span><span class="ltx_text" id="S5.F9.sf5.4.2" style="font-size:90%;">Early-Fusing</span><span class="ltx_text" id="S5.F9.sf5.5.3" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F9.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="826" id="S5.F9.sf6.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.sf6.3.1.1" style="font-size:90%;">(f)</span> </span><span class="ltx_text" id="S5.F9.sf6.4.2" style="font-size:90%;">Delayed-Fusing</span><span class="ltx_text" id="S5.F9.sf6.5.3" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.3.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S5.F9.4.2" style="font-size:90%;">Confusion matrixes of <span class="ltx_text ltx_font_sansserif" id="S5.F9.4.2.1">DeepProbHAR</span> for the different fusion strategies. Activities are labelled as: A)Â Walk; B)Â Run; C)Â Squat; D)Â Jump; E)Â Wave; F)Â Clap; G)Â Wipe.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S6" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduced <span class="ltx_text ltx_font_sansserif" id="S6.p1.1.1">DeepProbHAR</span>, a novel neuro-symbolic fusion approach to <span class="ltx_glossaryref" title="">HAR</span>, and provided initial evidence that simple movements, such as leg or arm movements, which are integral to human activities like running or walking, can be discerned using <span class="ltx_text" id="S6.p1.1.2">Wi-Fi</span> signals.
Leveraging declarative knowledge of several activities extracted from a video feed, <span class="ltx_text ltx_font_sansserif" id="S6.p1.1.3">DeepProbHAR</span> achieves results comparable to the state-of-the-art in <span class="ltx_glossaryref" title="">CSI</span>-based <span class="ltx_glossaryref" title="">HAR</span>.
Moreover, as a by-product of the learning process, <span class="ltx_text ltx_font_sansserif" id="S6.p1.1.4">DeepProbHAR</span> generates specialised classifiers for simple movements whose accuracy is on par with that of models trained on finely labelled datasets with a much higher cost.
However, we expect that as the complexity of the events to be detected increases, the neuro-symbolic approach can even outperform only-neural techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib22" title="">22</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In future work, we will examine the efficacy of discerning simple movements when categorising unseen activities, <span class="ltx_ERROR undefined" id="S6.p2.1.1">\eg</span><span class="ltx_text ltx_font_italic" id="S6.p2.1.2">parkour</span>.
We shall also evaluate whether the inductive bias provided by the symbolic part enables learning with smaller training dataset sizes w.r.t. other state-of-the-art <span class="ltx_glossaryref" title="">HAR</span> models.
Second, we intend to utilise the declarative knowledge of <span class="ltx_text ltx_font_sansserif" id="S6.p2.1.3">DeepProbHAR</span> to explain the latent space of the <span class="ltx_glossaryref" title="">VAEs</span> employed as input.
This will help us better comprehend the underlying structure and distribution of the data, potentially leading to more precise and efficient models.
Third, we will consider an <span class="ltx_glossaryref" title="">Evidential Deep Learning (EDL)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04734v1#bib.bib24" title="">24</a>]</cite> approach to enhance robustness against out-of-distribution data, thereby improving the generalisability and reliability of our models, also across different indoor locations.</p>
</div>
</section>
<section class="ltx_section" id="Sx1" lang="en-US">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was partially supported by the European Office of Aerospace Research &amp; Development (EOARD) under award number FA8655-22-1-7017 and by the US DEVCOM Army Research Laboratory (ARL) under Cooperative Agreements #W911NF2220243 and #W911NF1720196. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the United States government.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">This work was conducted while M. Cominelli was affiliated with the DII, University of Brescia, Italy.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">The authors thank Dr. Marc Roig Vilamala for his help in debugging part of the DeepProbLog code used in this work.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">While preparing this work, the authors used GPT-3.5 and 4.0 to improve readability and language. After using them, the authors reviewed and edited the content as needed, and they take full responsibility for the publicationâ€™s content.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en-US">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
W.Â Li, R.Â J. Piechocki, K.Â Woodbridge, C.Â Tang, and K.Â Chetty, â€œPassive
WiFi radar for human sensing using a stand-alone access point,â€
<em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, vol.Â 59, no.Â 3,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M.Â Cominelli, F.Â Gringoli, L.Â M. Kaplan, M.Â B. Srivastava, and F.Â Cerutti,
â€œAccurate passive radar via an uncertainty-aware fusion of Wi-Fi sensing
data,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">26th International Conference on Information Fusion
(FUSION)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M.Â Cominelli, F.Â Gringoli, and F.Â Restuccia, â€œExposing the CSI: A
systematic investigation of CSI-based Wi-Fi sensing capabilities and
limitations,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE International Conference on Pervasive Computing
and Communications (PerCom)</em>, 2023, pp. 81â€“90.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D.Â P. Kingma and M.Â Welling, â€œAuto-Encoding Variational Bayes,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">ICLR2014</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A.Â Sheth, K.Â Roy, and M.Â Gaur, â€œNeurosymbolic artificial intelligence (why,
what, and how),â€ <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">IEEE Intelligent Systems</em>, vol.Â 38, no.Â 3, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R.Â Manhaeve, S.Â Dumancic, A.Â Kimmig, T.Â Demeester, and L.Â DeÂ Raedt,
â€œDeepProbLog: Neural probabilistic logic programming,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances
in neural information processing systems</em>, vol.Â 31, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
R.Â Manhaeve, S.Â DumanÄiÄ‡, A.Â Kimmig, T.Â Demeester, and L.Â DeÂ Raedt,
â€œNeural probabilistic logic programming in DeepProbLog,â€
<em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Artificial Intelligence</em>, vol. 298, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H.Â Storf, M.Â Becker, and M.Â Riedl, â€œRule-based activity recognition framework:
Challenges, technique and learning,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">3rd International Conference
on Pervasive Computing Technologies for Healthcare</em>.Â Â Â IEEE, 2009, pp. 1â€“7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
P.Â Theekakul, S.Â Thiemjarus, E.Â Nantajeewarawat, T.Â Supnithi, and K.Â Hirota,
â€œA rule-based approach to activity recognition,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of
the 5th International Conference on Knowledge, Information, and Creativity
Support Systems</em>.Â Â Â Springer-Verlag,
2010, pp. 204â€“215.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M.Â Atzmueller, N.Â Hayat, M.Â Trojahn, and D.Â Kroll, â€œExplicative human activity
recognition using adaptive association rule-based classification,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2018 IEEE International Conference on Future IoT Technologies (Future
IoT)</em>.Â Â Â IEEE, 2018, pp. 1â€“6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
F.Â Meneghello, D.Â Garlisi, N.Â DalÂ Fabbro, I.Â Tinnirello, and M.Â Rossi,
â€œSHARP: Environment and person independent activity recognition with
commodity IEEE 802.11 access points,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">IEEE Transactions on Mobile
Computing</em>, pp. 1â€“16, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
N.Â Bahadori, J.Â Ashdown, and F.Â Restuccia, â€œReWiS: Reliable Wi-Fi
sensing through few-shot multi-antenna multi-receiver CSI learning,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of IEEE International Symposium on a World of Wireless,
Mobile and Multimedia Networks (WoWMoM)</em>, 2022, pp. 50â€“59.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J.Â Liu, H.Â Mu, A.Â Vakil, R.Â Ewing, X.Â Shen, E.Â Blasch, and J.Â Li, â€œHuman
occupancy detection via passive cognitive radio,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Sensors</em>, vol.Â 20,
no.Â 15, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
F.Â Gringoli, M.Â Cominelli, A.Â Blanco, and J.Â Widmer, â€œAX-CSI: Enabling
CSI extraction on commercial 802.11ax Wi-Fi platforms,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 15th ACM Workshop on Wireless Network Testbeds,
Experimental Evaluation &amp; CHaracterization</em>, 2021, p. 46â€“53.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D.Â Pavllo, C.Â Feichtenhofer, D.Â Grangier, and M.Â Auli, â€œ3D human pose
estimation in video with temporal convolutions and semi-supervised
training,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2019, pp. 7745â€“7754.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S.Â Ding, Z.Â Chen, T.Â Zheng, and J.Â Luo, â€œRF-Net: A unified meta-learning
framework for RF-enabled one-shot human activity recognition,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 18th Conference on Embedded Networked Sensor Systems
(SenSys)</em>, 2020, p. 517â€“530.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
L.Â DeÂ Raedt, A.Â Kimmig, and H.Â Toivonen, â€œProblog: A probabilistic prolog and
its application in link discovery,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">IJCAI 2007, Proceedings of the
20th international joint conference on artificial intelligence</em>, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D.Â Fierens, G.Â VanÂ den Broeck, J.Â Renkens, D.Â Shterionov, B.Â Gutmann, I.Â Thon,
G.Â Janssens, and L.Â DeÂ Raedt, â€œInference and learning in probabilistic logic
programs using weighted boolean formulas,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Theory and Practice of
Logic Programming</em>, vol.Â 15, no.Â 3, pp. 358â€“401, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M.Â RoigÂ Vilamala, T.Â Xing, H.Â Taylor, L.Â Garcia, M.Â Srivastava, L.Â Kaplan,
A.Â Preece, A.Â Kimmig, and F.Â Cerutti, â€œDeepProbCEP: A neuro-symbolic
approach for complex event processing in adversarial settings,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Expert
Systems with Applications</em>, vol. 215, p. 119376, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
L.Â DeÂ Raedt, A.Â Dries, I.Â Thon, G.Â Van den Broeck, and M.Â Verbeke, â€œInducing
probabilistic relational rules from probabilistic examples,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">IJCAI</em>, 2015, pp. 1835â€“1843.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
F.Â Cerutti, L.Â M. Kaplan, A.Â Kimmig, and M.Â Åensoy, â€œHandling epistemic
and aleatory uncertainties in probabilistic circuits,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Machine
Learning</em>, pp. 1â€“43, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
L.Â Han and M.Â B. Srivastava, â€œAn empirical evaluation of neural and
neuro-symbolic approaches to real-time multimodal complex event detection,â€
<em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2402.11403</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M.Â Sensoy, L.Â Kaplan, and M.Â Kandemir, â€œEvidential Deep Learning to
quantify classification uncertainty,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">NeurIPS</em>, Jun. 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M.Â Sensoy, L.Â Kaplan, F.Â Cerutti, and M.Â Saleki, â€œUncertainty-aware deep
classifiers using generative models,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, 2020, pp. 5620â€“5627.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jul  1 08:33:38 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
