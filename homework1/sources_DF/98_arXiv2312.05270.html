<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Image and AIS Data Fusion Technique for Maritime Computer Vision Applications</title>
<!--Generated on Thu Dec  7 20:44:23 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2312.05270v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 Background ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="2.1 Automatic Identification System ‣ 2 Background ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Automatic Identification System</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="2.2 Object Detection for Vessels ‣ 2 Background ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Object Detection for Vessels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS3" title="2.3 Fusion of Images and AIS Data ‣ 2 Background ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Fusion of Images and AIS Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1 Object Detection for Vessels ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Object Detection for Vessels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS2" title="3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Transformation Between World Coordinates and Image Coordinates</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS1" title="3.2.1 Keypoint Selection ‣ 3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Keypoint Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS2" title="3.2.2 Azimuth and Distance Estimation by Interpolation ‣ 3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Azimuth and Distance Estimation by Interpolation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="#S3.SS2.SSS3" title="3.2.3 Homography ‣ 3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Homography</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS3.Px1" title="Transformation in Fixed Cameras ‣ 3.2.3 Homography ‣ 3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title">Transformation in Fixed Cameras</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS3.Px2" title="Transformation in Panning Cameras ‣ 3.2.3 Homography ‣ 3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title">Transformation in Panning Cameras</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="3.3 Bounding Box-AIS Message Association ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Bounding Box-AIS Message Association</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS4" title="3.4 Dataset ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Results and Discussion ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="4.1 Coordinate Transformation ‣ 4 Results and Discussion ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Coordinate Transformation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="4.2 Image-AIS Matching ‣ 4 Results and Discussion ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Image-AIS Matching</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="5 Conclusion ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S6" title="6 Future Work ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="package-alerts ltx_document" role="alert">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: axessibility</li>
<li>failed: epic</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by selecting from this list of <a href="https://corpora.mathweb.org/corpus/arxmliv/tex_to_html/info/loaded_file" target="_blank">supported packages</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2312.05270v1 [cs.CV] 07 Dec 2023</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Image and AIS Data Fusion Technique 
<br class="ltx_break"/>for Maritime Computer Vision Applications</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Emre Gülsoylu
<br class="ltx_break"/>University of Hamburg
<br class="ltx_break"/>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paul Koch
<br class="ltx_break"/>Fraunhofer CML
<br class="ltx_break"/>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mert Yıldız
<br class="ltx_break"/>Fraunhofer CML
<br class="ltx_break"/>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Manfred Constapel
<br class="ltx_break"/>Fraunhofer CML
<br class="ltx_break"/>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">André Peter Kelm
<br class="ltx_break"/>University of Hamburg
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1" style="font-size:90%;">{emre.guelsoylu, andre.kelm}@uni-hamburg.de
<br class="ltx_break"/>{paul.koch, mert.yildiz, manfred.constapel}@cml.fraunhofer.de</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Deep learning object detection methods, like YOLOv5, are effective in identifying maritime vessels but often lack detailed information important for practical applications. In this paper, we addressed this problem by developing a technique that fuses Automatic Identification System (AIS) data with vessels detected in images to create datasets. This fusion enriches ship images with vessel-related data, such as type, size, speed, and direction. Our approach associates detected ships to their corresponding AIS messages by estimating distance and azimuth using a homography-based method suitable for both fixed and periodically panning cameras. This technique is useful for creating datasets for waterway traffic management, encounter detection, and surveillance. We introduce a novel dataset comprising of images taken in various weather conditions and their corresponding AIS messages. This dataset offers a stable baseline for refining vessel detection algorithms and trajectory prediction models. To assess our method’s performance, we manually annotated a portion of this dataset. The results are showing an overall association accuracy of 74.76 %, with the association accuracy for fixed cameras reaching 85.06 %. This demonstrates the potential of our approach in creating datasets for vessel detection, pose estimation and auto-labelling pipelines.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Maritime computer vision datasets are limited <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib29" title="">29</a>]</cite>, particularly due to costly manual annotation and lack of available images.
Considering that the state-of-the-art Deep Learning (DL) object detection approaches like YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib13" title="">13</a>]</cite> perform well for localizing maritime vessels, a technique to create maritime computer vision datasets can be developed. However, these object detection models can only provide limited maritime-related classes out-of-the-box. Applications like maritime autonomous surface ships (MAAS) require contextualized information such as the type, dimensions, speed, and course of a vessel <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite>. A system that matches AIS data with detected vessels from an object detection algorithm can help estimate the pose of the vessel, support in performing collision avoidance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib5" title="">5</a>]</cite> and aid in the creation of extensive maritime computer vision datasets.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this paper, an easy to use technique that fuses Automatic Identification System (AIS) data with webcam-based images was developed. The technique adapts an already existing homography based coordinate transformation to associate detected ships in image space and available location information in world coodinates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite>. However, this paper extends the existing technique for rotating cameras. The proposed technique enables a scalable way to fuse images from fixed and panning cameras with the extensive amount of data provided by AIS. This type of system can support the creation of maritime computer vision datasets with less to no human intervention needed. Moreover, alterations on the technique have been made to ensure accurate spatial resolution when transforming from image-to-world coordinates.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">A dataset has been created with this method and made publicly available <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/egulsoylu/image-ais-fusion" title="">https://github.com/egulsoylu/image-ais-fusion</a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Automatic Identification System</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The Automatic Identification System <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite> is a radio-based communication system for the exchange of ship related parameters between ships and vessel traffic services (VTS). AIS was initially introduced to improve efficiency and safety, especially for collision avoidance in maritime navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib12" title="">12</a>]</cite>. Besides collision avoidance, it is currently used for various purposes, including maritime surveillance, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite>, waterway management <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>]</cite>, and environment protection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib20" title="">20</a>]</cite>. As defined in the International Convention for the Safety of Life at Sea (SOLAS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>, the transmission of AIS messages is mandatory for certain vessels, including cargo ships, passenger vessels, and in some cases, fishing vessels. Since there are webcams based on the banks of the Elbe river that provide publicly available video streams, a huge volume of images can be collected and fused with AIS data.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Although AIS messages have issues with reliability and manipulation, they still provide valuable data for maritime navigation. Some of the well-known issues of AIS are uncertainty associated with technical equipment for Global Positioning System (GPS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib19" title="">19</a>]</cite>, temporal differences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib12" title="">12</a>]</cite>, and malicious crew that switches off the transceiver <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib18" title="">18</a>]</cite>.
Moreover, data redundancy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>, and noise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite> are the problems mentioned by previous studies. The range and the equipment significantly impact GPS-related issues, which can be minimized by applying trajectory correction methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>, <a class="ltx_ref" href="#bib.bib30" title="">30</a>]</cite>. As AIS messages contain detailed information about the vessel, matching images with AIS data can benefit maritime informatics in training an object detection model to infer more information from an image and support marine traffic against the reliability issues of AIS.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Object Detection for Vessels</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Object detection models can effectively localize and classify vessels in various images. Adaptations of DL vessel detection methods for maritime applications include Gupta et al.’s integration of Support Vector Machine, bag of features, and CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib14" title="">14</a>]</cite>, and Li et al.’s lightweight model modifying YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>]</cite>. Chang et al. enhanced YOLOv3 for better ship detection in both visible and infrared images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite>, and Xie et al. developed a more efficient network by integrating YOLOv4 with several advanced techniques, achieving high performance with significantly fewer parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib31" title="">31</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">As can be seen, the vessel detection field is very active, with different methods being proposed on a regular basis, most of which depends on YOLO family object detection methods. In terms of vessel localization and maritime-related classification, these modifications improves the performance and sometimes efficiency. However, complex tasks require more than classifying a vessel type and this paper focuses on fusion of bounding boxes with AIS data. Therefore, a more recent version of a general purpose object detection model, YOLOv5 was fine-tuned for this paper.
</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Fusion of Images and AIS Data</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The literature shows four directly related works on the fusion of camera-based images and AIS data. National Land Survey of Finland worked on sensor fusion for autonomous vessel navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib3" title="">3</a>]</cite>, funded by European Space Agency. However, this work’s details are unclear due to the lack of published material.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite> introduced a framework for vessel identification by fusing images with AIS data. They proposed a distance estimation method based on the ship size by monocular vision. To improve the fusion accuracy, a method that is commonly used by the seaman called Dead Reckoning (DR) was utilised. It is a method to predict the current position based on a ship’s previously observed speed, heading, and course. The most significant limitation of this work is the distance and bearing estimation, which leads to lower accuracy in matching the bounding boxes of vessels with related AIS messages. As the distance is predicted by the ship’s overall length (LOA) and the width of the detected bounding box, the accuracy of distance estimation depends on the ship’s heading and is highly sensitive to the localisation performance. Since the dimensions of bounding boxes can be affected by the background, water reflections, and weather conditions in general, distance estimation based on bounding boxes is not reliable. Moreover, this method produces incorrect results when the azimuth angle between the ship’s heading and the camera is small. In other words, if a vessel is perpendicular to the camera’s image plane, the width of the bounding box will represent the vessel’s breadth instead of LOA.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Qu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib22" title="">22</a>]</cite> introduced another framework for the fusion of camera-based vessel detection and AIS data. They utilise a YOLO-based vessel detection network and arrival time estimation to associate AIS messages with the detected vessels. The arrival time estimation takes the distance between the camera and a vessel into account for predicting the time the vessel will be in the field of view.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">Carillo-Perez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite> propose a method for georeferencing ship masks that uses the homography method to transform coordinates from image coordinates to world coordinates along with a novel dataset for maritime monitoring that contains images with taken by a static camera. They investigated instance segmentation methods focusing on more robust performance such as Mask-RCNN and DetectoRS as well as faster models such as YOLACT, Centermask-Lite. Their method of calculating the homography matrix relies on manually detecting antennas on each vessel in the image and matching them to the associated AIS message. The creation of pairs of image coordinates and world coordinates in this way is based on an assumption: The location information from the AIS data is reliable. However, as discussed in section <a class="ltx_ref" href="#S2.SS1" title="2.1 Automatic Identification System ‣ 2 Background ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">2.1</span></a>, AIS data may not be reliable and may lead to errors in the estimation of the homography matrix. Also, this method would only work for fixed cameras, and more steps are required to apply it to a panning camera.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">In this paper, we have generalised the method proposed by Carillo-Perez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite> on the panning cameras and created a novel dataset using this method which is publicly available. Moreover, we created image space-world coordinates pairs for the calculation of homography matrix with artificial structures that does not move such as concrete levee crowns, pier poles and corners of buildings. The improvements on georeferencing method using homography is discussed in Section <a class="ltx_ref" href="#S3" title="3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">3</span></a> and the details about the dataset is presented in Section <a class="ltx_ref" href="#S3.SS4" title="3.4 Dataset ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The proposed procedure has two main steps, (1) vessel detection and (2) coordinate transformation to match vessel images with related AIS messages. First, the images are preprocessed to filter irrelevant images out. Amongst the filtered images, an image is fed into a fine-tuned YOLO model and bounding boxes are extracted. The image’s timestamp is taken with the camera location, and AIS messages are filtered based on the time and location information. Then bounding boxes are matched with AIS messages, and the results are saved. The overview of the pipeline is shown in Figure <a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="287" id="S3.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">The pipeline for the technique. The technique takes an image, its timestamp and AIS messages of that day. After generating bounding boxes of the vessels on the image and filtering the AIS data, the system associates the predicted locations with the predicted bounding boxes. Red labels represent the location prediction based on AIS data and the unique identifier number. Orange labels represent the bounding box centre and associated unique identifier number.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Object Detection for Vessels</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Object detection for vessels is one of the two main prerequisites for the fusion of images and AIS messages. As mentioned in Section <a class="ltx_ref" href="#S2.SS2" title="2.2 Object Detection for Vessels ‣ 2 Background ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">2.2</span></a>, various YOLO models and their modified variants have been successfully used for vessel detection. Also, Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite> demonstrated that a fine-tuned YOLOv5 model performs well for maritime surveillance. In this paper, similar to Lu et al.’s work, a YOLOv5XL model was fine-tuned with the manually annotated dataset by changing the last layer from 80 neurons to only one, which represents the "Vessel" class. The actual vessel types are redundant as this information can be extracted from successfully matched AIS messages. Before fine-tuning, the mean average precision (mAP) of the model was 0.332 at an intersection over union (IoU) threshold of 0.5 and fine-tuned version performs 0.951 mAP at 0.5 IoU threshold on the test set.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Transformation Between World Coordinates and Image Coordinates</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">AIS messages contain the spatial location of vessels according to World Geodetic System 1984 (WGS84) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>. Transforming the world coordinates (latitude, longitude) into image coordinates <math alttext="(x,y)" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.2"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2.3.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml"><mo id="S3.SS2.p1.1.m1.2.3.2.1" stretchy="false" xref="S3.SS2.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">x</mi><mo id="S3.SS2.p1.1.m1.2.3.2.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml">y</mi><mo id="S3.SS2.p1.1.m1.2.3.2.3" stretchy="false" xref="S3.SS2.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><interval closure="open" id="S3.SS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.2"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑥</ci><ci id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.2d">( italic_x , italic_y )</annotation></semantics></math> enables the fusion of bounding boxes with AIS messages. In this section, two methods will be introduced for transformation between world coordinates and image coordinates. First, azimuth and distance estimation by interpolation will be presented in the Section <a class="ltx_ref" href="#S3.SS2.SSS2" title="3.2.2 Azimuth and Distance Estimation by Interpolation ‣ 3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>. Then the homography approach will be explained in Section <a class="ltx_ref" href="#S3.SS2.SSS3" title="3.2.3 Homography ‣ 3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">3.2.3</span></a> for different camera characteristics.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Keypoint Selection</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">For each camera, we selected an image with optimal visibility where the scene’s features were distinctly clear, in order to manually select keypoints. More than ten keypoints, such as fixed buoys, antennas, dock walls, if applicable, significant landmarks (e.g., Elbphilharmonie), were selected for each image. Image coordinates of each key point were noted, and their corresponding world coordinates were detected with the help of Google Maps. The azimuth angle and the distance were derived from world coordinates for each key point by using inverse geodetic function with WGS84 ellipsoid <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>. Figure <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3.2.1 Keypoint Selection ‣ 3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">2</span></a> shows the keypoints on the map and the image for the Neumühlen camera.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="282" id="S3.F2.g1" src="extracted/5281748/figs/keypoints_world_coordinates.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="337" id="S3.F2.g2" src="extracted/5281748/figs/keypoints_image_coordinates.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">World (above) and image (below) coordinates for the keypoints of the Neumühlen camera represented with red points.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Azimuth and Distance Estimation by Interpolation</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.3">Azimuth is the horizontal angle between the north and a point of interest in degrees. Every image aligned with the horizon has a linear relationship between the azimuth and the pixel coordinates on the <math alttext="X" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.1.m1.1"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mi id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.1.m1.1d">italic_X</annotation></semantics></math> axis. In the dataset, the images are almost perfectly aligned with the horizon and demonstrate the linear relationship between the <math alttext="X" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.2.m2.1"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><mi id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><ci id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.2.m2.1d">italic_X</annotation></semantics></math> axis and the azimuth degrees. Since the camera and the ship locations are roughly known, linear interpolation between the image’s left and right edge can map each pixel on the <math alttext="X" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.3.m3.1"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><mi id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><ci id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.3.m3.1d">italic_X</annotation></semantics></math> axis with an azimuth angle. The distance can be estimated with interpolation approach assuming that the keypoints cover the related parts of the image.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Homography</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.6">The lack of intrinsic and some extrinsic parameters prevented the estimation of the camera matrix, which would be helpful for transformation from world coordinates to image coordinates. Nevertheless, the relation between two images of the same planar surface can be found with homography. In this paper, instead of two images of the same planar surface, an image and a map were used of the same area to estimate the homography matrix. Therefore, the transformation between the map coordinates, <math alttext="lon" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.1.m1.1"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mrow id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.1.1.2" xref="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml">l</mi><mo id="S3.SS2.SSS3.p1.1.m1.1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p1.1.m1.1.1.3" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml">o</mi><mo id="S3.SS2.SSS3.p1.1.m1.1.1.1a" xref="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p1.1.m1.1.1.4" xref="S3.SS2.SSS3.p1.1.m1.1.1.4.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><apply id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1"><times id="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.1"></times><ci id="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.2">𝑙</ci><ci id="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.3">𝑜</ci><ci id="S3.SS2.SSS3.p1.1.m1.1.1.4.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.4">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">lon</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.1.m1.1d">italic_l italic_o italic_n</annotation></semantics></math>, <math alttext="lat" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.2.m2.1"><semantics id="S3.SS2.SSS3.p1.2.m2.1a"><mrow id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS3.p1.2.m2.1.1.2" xref="S3.SS2.SSS3.p1.2.m2.1.1.2.cmml">l</mi><mo id="S3.SS2.SSS3.p1.2.m2.1.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p1.2.m2.1.1.3" xref="S3.SS2.SSS3.p1.2.m2.1.1.3.cmml">a</mi><mo id="S3.SS2.SSS3.p1.2.m2.1.1.1a" xref="S3.SS2.SSS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p1.2.m2.1.1.4" xref="S3.SS2.SSS3.p1.2.m2.1.1.4.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.1b"><apply id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1"><times id="S3.SS2.SSS3.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1.1"></times><ci id="S3.SS2.SSS3.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1.2">𝑙</ci><ci id="S3.SS2.SSS3.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1.3">𝑎</ci><ci id="S3.SS2.SSS3.p1.2.m2.1.1.4.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1.4">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.1c">lat</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.2.m2.1d">italic_l italic_a italic_t</annotation></semantics></math>, and the image coordinates, <math alttext="x" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.3.m3.1"><semantics id="S3.SS2.SSS3.p1.3.m3.1a"><mi id="S3.SS2.SSS3.p1.3.m3.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.3.m3.1b"><ci id="S3.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.3.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.3.m3.1d">italic_x</annotation></semantics></math>, <math alttext="y" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.4.m4.1"><semantics id="S3.SS2.SSS3.p1.4.m4.1a"><mi id="S3.SS2.SSS3.p1.4.m4.1.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.4.m4.1b"><ci id="S3.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.4.m4.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.4.m4.1d">italic_y</annotation></semantics></math> in pixels can be performed as shown in Equation <a class="ltx_ref" href="#S3.E1" title="1 ‣ 3.2.3 Homography ‣ 3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">1</span></a>, where <math alttext="w" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.5.m5.1"><semantics id="S3.SS2.SSS3.p1.5.m5.1a"><mi id="S3.SS2.SSS3.p1.5.m5.1.1" xref="S3.SS2.SSS3.p1.5.m5.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.5.m5.1b"><ci id="S3.SS2.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.5.m5.1c">w</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.5.m5.1d">italic_w</annotation></semantics></math> is the scale factor and <math alttext="h_{i,j}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.6.m6.2"><semantics id="S3.SS2.SSS3.p1.6.m6.2a"><msub id="S3.SS2.SSS3.p1.6.m6.2.3" xref="S3.SS2.SSS3.p1.6.m6.2.3.cmml"><mi id="S3.SS2.SSS3.p1.6.m6.2.3.2" xref="S3.SS2.SSS3.p1.6.m6.2.3.2.cmml">h</mi><mrow id="S3.SS2.SSS3.p1.6.m6.2.2.2.4" xref="S3.SS2.SSS3.p1.6.m6.2.2.2.3.cmml"><mi id="S3.SS2.SSS3.p1.6.m6.1.1.1.1" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.cmml">i</mi><mo id="S3.SS2.SSS3.p1.6.m6.2.2.2.4.1" xref="S3.SS2.SSS3.p1.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.SS2.SSS3.p1.6.m6.2.2.2.2" xref="S3.SS2.SSS3.p1.6.m6.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.6.m6.2b"><apply id="S3.SS2.SSS3.p1.6.m6.2.3.cmml" xref="S3.SS2.SSS3.p1.6.m6.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.6.m6.2.3.1.cmml" xref="S3.SS2.SSS3.p1.6.m6.2.3">subscript</csymbol><ci id="S3.SS2.SSS3.p1.6.m6.2.3.2.cmml" xref="S3.SS2.SSS3.p1.6.m6.2.3.2">ℎ</ci><list id="S3.SS2.SSS3.p1.6.m6.2.2.2.3.cmml" xref="S3.SS2.SSS3.p1.6.m6.2.2.2.4"><ci id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1">𝑖</ci><ci id="S3.SS2.SSS3.p1.6.m6.2.2.2.2.cmml" xref="S3.SS2.SSS3.p1.6.m6.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.6.m6.2c">h_{i,j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.6.m6.2d">italic_h start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math> denotes the elements of the homography matrix. The value 1 is appended to the image coordinates to obtain their homogeneous matrix to avoid dimensional mismatch.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}w\,lat\\
w\,lon\\
w\end{bmatrix}=\begin{bmatrix}h_{11}&amp;h_{12}&amp;h_{13}\\
h_{21}&amp;h_{22}&amp;h_{23}\\
h_{31}&amp;h_{32}&amp;h_{33}\end{bmatrix}\begin{bmatrix}x\\
y\\
1\end{bmatrix}" class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.4" xref="S3.E1.m1.3.4.cmml"><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.2.1.cmml">[</mo><mtable displaystyle="true" id="S3.E1.m1.1.1.1.1" rowspacing="0pt" xref="S3.E1.m1.1.1.1.1.cmml"><mtr id="S3.E1.m1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1b" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">w</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" lspace="0.170em" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.4.cmml">a</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1b" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.1.1.1.5.cmml">t</mi></mrow></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1c" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1d" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.2.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1.2" xref="S3.E1.m1.1.1.1.1.2.1.1.2.cmml">w</mi><mo id="S3.E1.m1.1.1.1.1.2.1.1.1" lspace="0.170em" xref="S3.E1.m1.1.1.1.1.2.1.1.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.2.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.1.3.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.2.1.1.1a" xref="S3.E1.m1.1.1.1.1.2.1.1.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.2.1.1.4" xref="S3.E1.m1.1.1.1.1.2.1.1.4.cmml">o</mi><mo id="S3.E1.m1.1.1.1.1.2.1.1.1b" xref="S3.E1.m1.1.1.1.1.2.1.1.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.2.1.1.5" xref="S3.E1.m1.1.1.1.1.2.1.1.5.cmml">n</mi></mrow></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1e" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1f" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3.1.1" xref="S3.E1.m1.1.1.1.1.3.1.1.cmml">w</mi></mtd></mtr></mtable><mo id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.2.1.cmml">]</mo></mrow><mo id="S3.E1.m1.3.4.1" xref="S3.E1.m1.3.4.1.cmml">=</mo><mrow id="S3.E1.m1.3.4.2" xref="S3.E1.m1.3.4.2.cmml"><mrow id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.2.cmml"><mo id="S3.E1.m1.2.2.3.1" xref="S3.E1.m1.2.2.2.1.cmml">[</mo><mtable columnspacing="5pt" displaystyle="true" id="S3.E1.m1.2.2.1.1" rowspacing="0pt" xref="S3.E1.m1.2.2.1.1.cmml"><mtr id="S3.E1.m1.2.2.1.1a" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1b" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml">h</mi><mn id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.cmml">11</mn></msub></mtd><mtd id="S3.E1.m1.2.2.1.1c" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.2.1" xref="S3.E1.m1.2.2.1.1.1.2.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.2.1.2" xref="S3.E1.m1.2.2.1.1.1.2.1.2.cmml">h</mi><mn id="S3.E1.m1.2.2.1.1.1.2.1.3" xref="S3.E1.m1.2.2.1.1.1.2.1.3.cmml">12</mn></msub></mtd><mtd id="S3.E1.m1.2.2.1.1d" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.1.3.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.1.2" xref="S3.E1.m1.2.2.1.1.1.3.1.2.cmml">h</mi><mn id="S3.E1.m1.2.2.1.1.1.3.1.3" xref="S3.E1.m1.2.2.1.1.1.3.1.3.cmml">13</mn></msub></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1e" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1f" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.2.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.2.cmml">h</mi><mn id="S3.E1.m1.2.2.1.1.2.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.3.cmml">21</mn></msub></mtd><mtd id="S3.E1.m1.2.2.1.1g" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.2.2.1" xref="S3.E1.m1.2.2.1.1.2.2.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.2.2.1.2.cmml">h</mi><mn id="S3.E1.m1.2.2.1.1.2.2.1.3" xref="S3.E1.m1.2.2.1.1.2.2.1.3.cmml">22</mn></msub></mtd><mtd id="S3.E1.m1.2.2.1.1h" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.2.3.1" xref="S3.E1.m1.2.2.1.1.2.3.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.3.1.2" xref="S3.E1.m1.2.2.1.1.2.3.1.2.cmml">h</mi><mn id="S3.E1.m1.2.2.1.1.2.3.1.3" xref="S3.E1.m1.2.2.1.1.2.3.1.3.cmml">23</mn></msub></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1i" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1j" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.3.1.1" xref="S3.E1.m1.2.2.1.1.3.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.3.1.1.2" xref="S3.E1.m1.2.2.1.1.3.1.1.2.cmml">h</mi><mn id="S3.E1.m1.2.2.1.1.3.1.1.3" xref="S3.E1.m1.2.2.1.1.3.1.1.3.cmml">31</mn></msub></mtd><mtd id="S3.E1.m1.2.2.1.1k" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.3.2.1" xref="S3.E1.m1.2.2.1.1.3.2.1.cmml"><mi id="S3.E1.m1.2.2.1.1.3.2.1.2" xref="S3.E1.m1.2.2.1.1.3.2.1.2.cmml">h</mi><mn id="S3.E1.m1.2.2.1.1.3.2.1.3" xref="S3.E1.m1.2.2.1.1.3.2.1.3.cmml">32</mn></msub></mtd><mtd id="S3.E1.m1.2.2.1.1l" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.3.3.1" xref="S3.E1.m1.2.2.1.1.3.3.1.cmml"><mi id="S3.E1.m1.2.2.1.1.3.3.1.2" xref="S3.E1.m1.2.2.1.1.3.3.1.2.cmml">h</mi><mn id="S3.E1.m1.2.2.1.1.3.3.1.3" xref="S3.E1.m1.2.2.1.1.3.3.1.3.cmml">33</mn></msub></mtd></mtr></mtable><mo id="S3.E1.m1.2.2.3.2" xref="S3.E1.m1.2.2.2.1.cmml">]</mo></mrow><mo id="S3.E1.m1.3.4.2.1" xref="S3.E1.m1.3.4.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.2.cmml"><mo id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.2.1.cmml">[</mo><mtable displaystyle="true" id="S3.E1.m1.3.3.1.1" rowspacing="0pt" xref="S3.E1.m1.3.3.1.1.cmml"><mtr id="S3.E1.m1.3.3.1.1a" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1b" xref="S3.E1.m1.3.3.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">x</mi></mtd></mtr><mtr id="S3.E1.m1.3.3.1.1c" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1d" xref="S3.E1.m1.3.3.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.2.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.cmml">y</mi></mtd></mtr><mtr id="S3.E1.m1.3.3.1.1e" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1f" xref="S3.E1.m1.3.3.1.1.cmml"><mn id="S3.E1.m1.3.3.1.1.3.1.1" xref="S3.E1.m1.3.3.1.1.3.1.1.cmml">1</mn></mtd></mtr></mtable><mo id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.4.cmml" xref="S3.E1.m1.3.4"><eq id="S3.E1.m1.3.4.1.cmml" xref="S3.E1.m1.3.4.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="latexml" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><matrixrow id="S3.E1.m1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">𝑤</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.4">𝑎</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.5">𝑡</ci></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.2.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1"><times id="S3.E1.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1"></times><ci id="S3.E1.m1.1.1.1.1.2.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2">𝑤</ci><ci id="S3.E1.m1.1.1.1.1.2.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.2.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.4">𝑜</ci><ci id="S3.E1.m1.1.1.1.1.2.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.5">𝑛</ci></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1c.cmml" xref="S3.E1.m1.1.1.1.1"><ci id="S3.E1.m1.1.1.1.1.3.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1">𝑤</ci></matrixrow></matrix></apply><apply id="S3.E1.m1.3.4.2.cmml" xref="S3.E1.m1.3.4.2"><times id="S3.E1.m1.3.4.2.1.cmml" xref="S3.E1.m1.3.4.2.1"></times><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.3"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.3.1">matrix</csymbol><matrix id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><matrixrow id="S3.E1.m1.2.2.1.1a.cmml" xref="S3.E1.m1.2.2.1.1"><apply id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2">ℎ</ci><cn id="S3.E1.m1.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.1.1.1.3">11</cn></apply><apply id="S3.E1.m1.2.2.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.2.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.2.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2.1.2">ℎ</ci><cn id="S3.E1.m1.2.2.1.1.1.2.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.1.2.1.3">12</cn></apply><apply id="S3.E1.m1.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.3.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.2">ℎ</ci><cn id="S3.E1.m1.2.2.1.1.1.3.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.1.3.1.3">13</cn></apply></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1b.cmml" xref="S3.E1.m1.2.2.1.1"><apply id="S3.E1.m1.2.2.1.1.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2">ℎ</ci><cn id="S3.E1.m1.2.2.1.1.2.1.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.2.1.1.3">21</cn></apply><apply id="S3.E1.m1.2.2.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1.2">ℎ</ci><cn id="S3.E1.m1.2.2.1.1.2.2.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.2.2.1.3">22</cn></apply><apply id="S3.E1.m1.2.2.1.1.2.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.3.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.3.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.3.1.2">ℎ</ci><cn id="S3.E1.m1.2.2.1.1.2.3.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.2.3.1.3">23</cn></apply></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1c.cmml" xref="S3.E1.m1.2.2.1.1"><apply id="S3.E1.m1.2.2.1.1.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.3.1.1.2">ℎ</ci><cn id="S3.E1.m1.2.2.1.1.3.1.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.3.1.1.3">31</cn></apply><apply id="S3.E1.m1.2.2.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.2.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.2.1.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2.1.2">ℎ</ci><cn id="S3.E1.m1.2.2.1.1.3.2.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.3.2.1.3">32</cn></apply><apply id="S3.E1.m1.2.2.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.3.1.2.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1.2">ℎ</ci><cn id="S3.E1.m1.2.2.1.1.3.3.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.3.3.1.3">33</cn></apply></matrixrow></matrix></apply><apply id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.3"><csymbol cd="latexml" id="S3.E1.m1.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.1">matrix</csymbol><matrix id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1"><matrixrow id="S3.E1.m1.3.3.1.1a.cmml" xref="S3.E1.m1.3.3.1.1"><ci id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1">𝑥</ci></matrixrow><matrixrow id="S3.E1.m1.3.3.1.1b.cmml" xref="S3.E1.m1.3.3.1.1"><ci id="S3.E1.m1.3.3.1.1.2.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1">𝑦</ci></matrixrow><matrixrow id="S3.E1.m1.3.3.1.1c.cmml" xref="S3.E1.m1.3.3.1.1"><cn id="S3.E1.m1.3.3.1.1.3.1.1.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.3.1.1">1</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\begin{bmatrix}w\,lat\\
w\,lon\\
w\end{bmatrix}=\begin{bmatrix}h_{11}&amp;h_{12}&amp;h_{13}\\
h_{21}&amp;h_{22}&amp;h_{23}\\
h_{31}&amp;h_{32}&amp;h_{33}\end{bmatrix}\begin{bmatrix}x\\
y\\
1\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">[ start_ARG start_ROW start_CELL italic_w italic_l italic_a italic_t end_CELL end_ROW start_ROW start_CELL italic_w italic_l italic_o italic_n end_CELL end_ROW start_ROW start_CELL italic_w end_CELL end_ROW end_ARG ] = [ start_ARG start_ROW start_CELL italic_h start_POSTSUBSCRIPT 11 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 12 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 13 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 21 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 22 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 23 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 31 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 33 end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL italic_x end_CELL end_ROW start_ROW start_CELL italic_y end_CELL end_ROW start_ROW start_CELL 1 end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">Transformation in Fixed Cameras</h5>
<div class="ltx_para" id="S3.SS2.SSS3.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS3.Px1.p1.1">First, the keypoints were detected manually to estimate the homography matrix for the fixed cameras. Then the corresponding world coordinates were detected from Google Maps. After having two sets of points in different coordinate systems, the homography matrix was estimated.
Therefore, the world coordinates provided by AIS messages can be transformed into image coordinates which can be later matched with related bounding box centres.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">Transformation in Panning Cameras</h5>
<div class="ltx_para" id="S3.SS2.SSS3.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS3.Px2.p1.1">Due to irregular image collection intervals, the images collected from panning cameras require localisation first. Since the images from various angles are present, a panorama for each panning camera was created by stitch images. After having a panorama image, the method followed for fixed cameras can be applied for homography estimation to transform world coordinates into panorama image coordinates. Unlike fixed cameras, there is one more step to take: transforming query image coordinates to panorama image coordinates. Each image was used as a query image and localised in the related panorama image with template matching, using cross-correlation. The transformation from query to panorama image coordinates was applied using the highest cross-correlation value as the offset. Thus the ships were localised in the query image based on the AIS message in two steps (1) world coordinates to panorama image coordinates, (2) query image coordinates to panorama image coordinates.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS3.Px2.p2.1">Cross-correlation is sensitive to small changes as it is a pixel-level comparison <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite>. To improve the localisation of query images in panoramas, a future extraction algorithm, Oriented FAST and rotated BRIEF (ORB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>, was tried. However, it did not outperform the template matching approach. The main reason for this is that the images contain cranes, wind turbines and other structurally identical and moving objects, which limits the performance of the algorithm during the keypoint matching process.</p>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Bounding Box-AIS Message Association</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">AIS data is filtered by a region of interest defined manually for each camera and 30 seconds period before and after the image’s timestamp. The region of interest is the whole area that panning cameras cover. Figure <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.3 Bounding Box-AIS Message Association ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the area for each webcam. Then filtered messages are compared with the bounding boxes in the related image. In this dataset, the images are less frequent than the AIS messages, and there is no one-to-one relation between the images and the messages. Therefore, the location of vessels is predicted by interpolation according to the image’s timestamp, assuming that the vessel speed is constant since the time frame is not big enough to significantly change the speed. If there are not enough datapoints to interpolate for a vessel at a time, then the location transmitted in AIS message is used. Applying interpolation also helps reducing the negative effects of the unstable video stream problem. After getting the predictions, AIS messages are assigned to the closest bounding box in the image by using nearest neighbour search based on k-dimensional tree <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="286" id="S3.F3.g1" src="extracted/5281748/figs/camera_roi.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Regions of interest for each camera.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Dataset</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The visible light images in this dataset were collected from five public webcam streams between 11.01.2022 and 20.03.2022 during the daylight hours.
These webcams are located on the bank of Elbe river and its tributary Norderelbe, near the centre of Hamburg. Three of these webcams are fixed, while the other three are panning, one of which demonstrates both characteristics.
Fixed cameras are those that do not change direction and always point in one direction. Although the wind changes their direction slightly from time to time, these changes are so small that they can be ignored. Panning cameras, on the other hand, change their direction with yaw rotation and therefore cover a wider area than the fixed cameras.
</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">One of the cameras shows two characteristics: It is primarily a panning camera, however, after a short transition, it focuses on a building for a minute and behaves like a fixed camera. Therefore, this camera can be treated as two cameras with different characteristics. Although the camera angle is slightly different because of the panning movement during the transition from panning to fixed characteristic, it is still closer to a fixed camera than a panning camera. The images produced by this camera were divided into three classes with appearance-based classification
(1) Panning images, which were treated as query images and compared with a panorama, (2) Fixed images, which contain the images of Dockland Office, (3) Transition images, which were captured during the transition.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">The histogram of each image is compared with other histograms that belong to one of these classes. The other histograms are from a subset of the images that were captured on 11.01.2022. In this subset, 25 panning, 24 fixed, and 17 transition image histograms are present. A class was assigned based on the smallest Euclidean Distance between the histograms. Before this method, Structural Similarity Index (SSIM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib28" title="">28</a>]</cite> and edge difference zero-count were used, but these methods did not provide satisfactory results. Although the edge detection method seemed logical initially, as the transition images were out of focus, it could not outperform the histogram comparison.
Table <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.4 Dataset ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">1</span></a> shows the characteristics of the webcams.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.1.1">Name</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.2.1">Type</span></th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.3.1">Resolution (px)</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.4.1">Location (Lat, Lon)</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.5.1">Direction</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S3.T1.2.2.2.1">Altona</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T1.2.2.2.2">Fixed</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S3.T1.2.2.2.3">1280 x 720</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S3.T1.2.2.2.4">53.54387, 9.94275</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S3.T1.2.2.2.5">South West</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.2.3.3.1">Blockbräu</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.2.3.3.2">Fixed</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.3.3.3">1280 x 720</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.3.3.4">53.54553, 9.96957</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.3.3.5">South East</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.2.4.4.1">Neumühlen</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.2.4.4.2">Fixed</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.4.4.3">1920 x 1080</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.4.4.4">53.54388, 9.91692</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.4.4.5">South East</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S3.T1.2.5.5.1">Altona</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T1.2.5.5.2">Panning</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S3.T1.2.5.5.3">1280 x 720</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S3.T1.2.5.5.4">53.54387, 9.94275</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S3.T1.2.5.5.5">South East-South</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.2.6.6.1">Elbe</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.2.6.6.2">Panning</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.6.6.3">1280 x 720</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.6.6.4">53.54722, 9.96338</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.2.6.6.5">South East-South West</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.2.7.7.1">Hafencity</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.2.7.7.2">Panning</th>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.2.7.7.3">1920 x 1080</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.2.7.7.4">53.53903, 9.99345</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.2.7.7.5">South-South West</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Characteristics of the webcams on the banks of the Elbe river.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1">There are a few drawbacks to collect images from a video stream. First, network problems affect image collection significantly, as a delay can occur both on the server and the client sides. Delays affect the timestamps during image collection, which result in predictions that are significantly in front of the vessel. Moreover, some images were repeated without a clear repetition pattern due to an unreliable data stream. Because of the random nature of image repetition, a preprocessing step was used for filtering them out. The images were compared with 20 previous frames by using histogram comparison, and the duplicate images were removed. As the traffic is not always dense, the images did not always have a ship in them. By using both the fine-tuned YOLO model and checking the presence of AIS messages for each camera’s RoI, the images without any ships were detected and removed. These preprocessing steps resulted in the removal of images with rain drops which covers the whole lens, dark lighting conditions and heavy fog.</p>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">Panning cameras got affected the most by the infrequent data collection. Since the images were collected in various intervals, the camera angle was different each time an image was captured, making it harder to handle images from panning cameras individually. Panoramas were created for each camera, and each individual image was compared with the corresponding panorama image to overcome this problem (see Section <a class="ltx_ref" href="#S3.SS2.SSS3" title="3.2.3 Homography ‣ 3.2 Transformation Between World Coordinates and Image Coordinates ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS4.p6">
<p class="ltx_p" id="S3.SS4.p6.1">Due to the difficulty of annotating the high amount of collected images, only a subset from collected images were annotated in YOLO format with "makesense.ai". In total, 1515 images were annotated of which 1062 images were used during training, 299 images were during validation, and 154 images during test while fine-tuning the YOLOv5 model. After bounding box annotation, the unique identifier numbers were assigned for bounding boxes manually with the help of AIS data provided. During the annotation process, "marinetraffic.com" was also used as it provides images and MMSI numbers of vessels. It is worth mentioning that the colour changes of the ferries due to advertisements made the annotation process more difficult. 1658 bounding box-AIS message pairs were created in 381 images.</p>
</div>
<div class="ltx_para" id="S3.SS4.p7">
<p class="ltx_p" id="S3.SS4.p7.1">Besides image_id, bbox, category_id and unique_id the annotations contain details about the vessel taken from the associated AIS message. A sample annotation is shown in the Listing <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:annotation_snippet</span>.
</p>
</div>
<figure class="ltx_float ltx_lstlisting" id="LST1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float">Listing 1: </span>A sample annotation snippet for the vessel information that was extracted from the associated AIS message.</figcaption>
<div class="ltx_listing ltx_lstlisting ltx_listing" id="LST1.1">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,ICAgICAgICAiYW5ub3RhdGlvbnMiOiBbCiAgICAgICAgewogICAgICAgICAgICAiaW1hZ2VfaWQiOiAwLAogICAgICAgICAgICAiYmJveCI6IFsKICAgICAgICAgICAgICAgIDM2My4wLAogICAgICAgICAgICAgICAgNjAyLjAsCiAgICAgICAgICAgICAgICAxOTkuMCwKICAgICAgICAgICAgICAgIDU2LjAKICAgICAgICAgICAgXSwKICAgICAgICAgICAgInZlc3NlbF9pbmZvIjogewogICAgICAgICAgICAgICAgInR5cGUiOiA3MCwKICAgICAgICAgICAgICAgICJsYXRpdHVkZSI6ICI1My41NDI5NjgiLAogICAgICAgICAgICAgICAgImxvbmdpdHVkZSI6ICI5LjkzNTQwMSIsCiAgICAgICAgICAgICAgICAiaGVhZGluZyI6IDI2OC4zLAogICAgICAgICAgICAgICAgImNvdXJzZSI6IDI3MS4zOCwKICAgICAgICAgICAgICAgICJsZW5ndGgiOiAyOSwKICAgICAgICAgICAgICAgICJ3aWR0aCI6IDcsCiAgICAgICAgICAgICAgICAic3BlZWQiOiA4LjczLAogICAgICAgICAgICB9CiAgICAgICAgfSwKICAgICAgICAuLi4KICAgICAgICBd">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_lst_space" id="lstnumberx1.1">        </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx1.2">annotations</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx1.3"> </span>[
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_text ltx_lst_space" id="lstnumberx2.1">        </span>{
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_lst_space" id="lstnumberx3.1">            </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx3.2">image_id</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx3.3"> </span>0,
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_text ltx_lst_space" id="lstnumberx4.1">            </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.2">bbox</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx4.3"> </span>[
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_text ltx_lst_space" id="lstnumberx5.1">                </span>363.0,
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_text ltx_lst_space" id="lstnumberx6.1">                </span>602.0,
</div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_text ltx_lst_space" id="lstnumberx7.1">                </span>199.0,
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_text ltx_lst_space" id="lstnumberx8.1">                </span>56.0
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_text ltx_lst_space" id="lstnumberx9.1">            </span>],
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_text ltx_lst_space" id="lstnumberx10.1">            </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx10.2">vessel_info</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx10.3"> </span>{
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_text ltx_lst_space" id="lstnumberx11.1">                </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx11.2">type</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx11.3"> </span>70,
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_text ltx_lst_space" id="lstnumberx12.1">                </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx12.2">latitude</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx12.3"> </span>"53.542968",
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_text ltx_lst_space" id="lstnumberx13.1">                </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx13.2">longitude</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx13.3"> </span>"9.935401",
</div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_text ltx_lst_space" id="lstnumberx14.1">                </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx14.2">heading</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx14.3"> </span>268.3,
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_text ltx_lst_space" id="lstnumberx15.1">                </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx15.2">course</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx15.3"> </span>271.38,
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_text ltx_lst_space" id="lstnumberx16.1">                </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx16.2">length</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx16.3"> </span>29,
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_text ltx_lst_space" id="lstnumberx17.1">                </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx17.2">width</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx17.3"> </span>7,
</div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_text ltx_lst_space" id="lstnumberx18.1">                </span>"<span class="ltx_text ltx_lst_identifier" id="lstnumberx18.2">speed</span>":<span class="ltx_text ltx_lst_space" id="lstnumberx18.3"> </span>8.73,
</div>
<div class="ltx_listingline" id="lstnumberx19">
<span class="ltx_text ltx_lst_space" id="lstnumberx19.1">            </span>}
</div>
<div class="ltx_listingline" id="lstnumberx20">
<span class="ltx_text ltx_lst_space" id="lstnumberx20.1">        </span>},
</div>
<div class="ltx_listingline" id="lstnumberx21">
<span class="ltx_text ltx_lst_space" id="lstnumberx21.1">        </span>…
</div>
<div class="ltx_listingline" id="lstnumberx22">
<span class="ltx_text ltx_lst_space" id="lstnumberx22.1">        </span>]
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Discussion</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Coordinate Transformation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Preliminary results show that homography outperforms azimuth and distance estimation by interpolation, as interpolation needs full coverage of the related parts of the image. However, it is hard to find keypoints that cover the whole area. Extrapolation can be a solution for this in exchange of precision. Because of this reason, homography is used as the technique for transforming coordinates.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.1.1">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1" style="width:54.1pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T2.2.1.1.1.1">Camera Name</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.1.1.2" style="width:48.4pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T2.2.1.1.2.1">Camera Type</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.1.1.3" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T2.2.1.1.3.1">Maximum 
<br class="ltx_break"/>Error (px)</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.1.1.4" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T2.2.1.1.4.1">Minimum Error (px)</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.1.1.5" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T2.2.1.1.5.1">Mean 
<br class="ltx_break"/>Error (px)</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.1.1.6" style="width:54.1pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T2.2.1.1.6.1">Standard Deviation</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.1.1.7" style="width:51.2pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T2.2.1.1.7.1">Keypoint Count</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.2">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_tt" id="S4.T2.2.2.2.1" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.2.2.1.1">Blockbräu</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.2.2.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.2.2.2.1">Fixed</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.2.2.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.2.2.3.1">161.51</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.2.2.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.2.2.4.1">4.24</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.2.2.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.2.2.5.1">39.97</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.2.2.6" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.2.2.6.1">31.84</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.2.2.7" style="width:51.2pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.2.2.7.1">27</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3.3">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.3.3.1" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.3.3.1.1">Altona</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.3.3.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.3.3.2.1">Fixed</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.3.3.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.3.3.3.1">40.26</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.3.3.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.3.3.4.1">4.12</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.3.3.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.3.3.5.1">16.58</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.3.3.6" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.3.3.6.1">9.46</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.3.3.7" style="width:51.2pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.3.3.7.1">15</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4.4">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.4.4.1" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.4.4.1.1">Neumühlen</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.4.4.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.4.4.2.1">Fixed</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.4.4.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.4.4.3.1">31.38</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.4.4.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.4.4.4.1">0.00</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.4.4.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.4.4.5.1">11.39</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.4.4.6" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.4.4.6.1">10.29</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.4.4.7" style="width:51.2pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.4.4.7.1">16</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5.5">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_tt" id="S4.T2.2.5.5.1" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.5.5.1.1">Elbe</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.5.5.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.5.5.2.1">Panning</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.5.5.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.5.5.3.1">451.46</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.5.5.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.5.5.4.1">43.46</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.5.5.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.5.5.5.1">146.85</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.5.5.6" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.5.5.6.1">123.38</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T2.2.5.5.7" style="width:51.2pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.5.5.7.1">11</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6.6">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.6.6.1" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.6.6.1.1">Altona</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.6.6.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.6.6.2.1">Panning</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.6.6.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.6.6.3.1">267.32</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.6.6.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.6.6.4.1">7.28</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.6.6.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.6.6.5.1">90.04</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.6.6.6" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.6.6.6.1">88.29</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T2.2.6.6.7" style="width:51.2pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.6.6.7.1">16</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7.7">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.7.7.1" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.7.7.1.1">Hafencity</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.7.7.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.7.7.2.1">Panning</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.7.7.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.7.7.3.1">245.00</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.7.7.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.7.7.4.1">17.80</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.7.7.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.7.7.5.1">124.04</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.7.7.6" style="width:54.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.7.7.6.1">63.02</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.7.7.7" style="width:51.2pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.2.7.7.7.1">17</p>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Coordinate transformation error from world coordinates to (panorama) image coordinates.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Table <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.1 Coordinate Transformation ‣ 4 Results and Discussion ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">2</span></a> shows the coordinate transformation error introduced by homography. As the table shows, the fixed cameras produce less error compared to panning cameras because of the distortion introduced during the panorama creation in the coordinate transformation step. Also, considering that the panorama image is significantly bigger than an individual image, they are more likely to have bigger errors. It is clear that an increase in the number of keypoints affects the performance positively. It is to be noted that continuous values for the pixel errors stem from the homography matrix and are purely used for evaluation purposes.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Image-AIS Matching</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The rate of successfully associated bounding box-AIS message pairs was used to evaluate the performance of image-AIS fusion. The accuracy values were calculated as shown in equation <a class="ltx_ref" href="#S4.E2" title="2 ‣ 4.2 Image-AIS Matching ‣ 4 Results and Discussion ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">2</span></a>. The Maritime Mobile Service Identity (MMSI) number assigned to the bounding box was manually verified using the vessel images at "marinetraffic.com". If the visual inspection can confirm that the detected vessel is associated with the correct MMSI, the association is counted as correct. For the privacy reasons, the public dataset does not contain MMSI but a unique identifier number.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">As the panorama images have bigger regions of interest, AIS messages that do not land on the query image are not filtered out. Since AIS messages are sequentially assigned with the closest bounding box, sometimes AIS messages from outside the query image can be associated with the closest bounding box, which is usually an incorrect match. Vessels that transmit at large intervals or not at all, for example moored vessels, create a problem as their bounding box can be detected but cannot be associated with an AIS message correctly.</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Accuracy=\frac{Correct\,Associations}{Total\,Number\,of\,Pairs}*100" class="ltx_Math" display="block" id="S4.E2.m1.1"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mrow id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.2.2" xref="S4.E2.m1.1.1.2.2.cmml">A</mi><mo id="S4.E2.m1.1.1.2.1" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.3" xref="S4.E2.m1.1.1.2.3.cmml">c</mi><mo id="S4.E2.m1.1.1.2.1a" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.4" xref="S4.E2.m1.1.1.2.4.cmml">c</mi><mo id="S4.E2.m1.1.1.2.1b" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.5" xref="S4.E2.m1.1.1.2.5.cmml">u</mi><mo id="S4.E2.m1.1.1.2.1c" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.6" xref="S4.E2.m1.1.1.2.6.cmml">r</mi><mo id="S4.E2.m1.1.1.2.1d" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.7" xref="S4.E2.m1.1.1.2.7.cmml">a</mi><mo id="S4.E2.m1.1.1.2.1e" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.8" xref="S4.E2.m1.1.1.2.8.cmml">c</mi><mo id="S4.E2.m1.1.1.2.1f" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.9" xref="S4.E2.m1.1.1.2.9.cmml">y</mi></mrow><mo id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mfrac id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml"><mrow id="S4.E2.m1.1.1.3.2.2" xref="S4.E2.m1.1.1.3.2.2.cmml"><mi id="S4.E2.m1.1.1.3.2.2.2" xref="S4.E2.m1.1.1.3.2.2.2.cmml">C</mi><mo id="S4.E2.m1.1.1.3.2.2.1" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.3" xref="S4.E2.m1.1.1.3.2.2.3.cmml">o</mi><mo id="S4.E2.m1.1.1.3.2.2.1a" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.4" xref="S4.E2.m1.1.1.3.2.2.4.cmml">r</mi><mo id="S4.E2.m1.1.1.3.2.2.1b" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.5" xref="S4.E2.m1.1.1.3.2.2.5.cmml">r</mi><mo id="S4.E2.m1.1.1.3.2.2.1c" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.6" xref="S4.E2.m1.1.1.3.2.2.6.cmml">e</mi><mo id="S4.E2.m1.1.1.3.2.2.1d" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.7" xref="S4.E2.m1.1.1.3.2.2.7.cmml">c</mi><mo id="S4.E2.m1.1.1.3.2.2.1e" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.8" xref="S4.E2.m1.1.1.3.2.2.8.cmml">t</mi><mo id="S4.E2.m1.1.1.3.2.2.1f" lspace="0.170em" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.9" xref="S4.E2.m1.1.1.3.2.2.9.cmml">A</mi><mo id="S4.E2.m1.1.1.3.2.2.1g" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.10" xref="S4.E2.m1.1.1.3.2.2.10.cmml">s</mi><mo id="S4.E2.m1.1.1.3.2.2.1h" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.11" xref="S4.E2.m1.1.1.3.2.2.11.cmml">s</mi><mo id="S4.E2.m1.1.1.3.2.2.1i" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.12" xref="S4.E2.m1.1.1.3.2.2.12.cmml">o</mi><mo id="S4.E2.m1.1.1.3.2.2.1j" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.13" xref="S4.E2.m1.1.1.3.2.2.13.cmml">c</mi><mo id="S4.E2.m1.1.1.3.2.2.1k" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.14" xref="S4.E2.m1.1.1.3.2.2.14.cmml">i</mi><mo id="S4.E2.m1.1.1.3.2.2.1l" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.15" xref="S4.E2.m1.1.1.3.2.2.15.cmml">a</mi><mo id="S4.E2.m1.1.1.3.2.2.1m" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.16" xref="S4.E2.m1.1.1.3.2.2.16.cmml">t</mi><mo id="S4.E2.m1.1.1.3.2.2.1n" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.17" xref="S4.E2.m1.1.1.3.2.2.17.cmml">i</mi><mo id="S4.E2.m1.1.1.3.2.2.1o" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.18" xref="S4.E2.m1.1.1.3.2.2.18.cmml">o</mi><mo id="S4.E2.m1.1.1.3.2.2.1p" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.19" xref="S4.E2.m1.1.1.3.2.2.19.cmml">n</mi><mo id="S4.E2.m1.1.1.3.2.2.1q" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.20" xref="S4.E2.m1.1.1.3.2.2.20.cmml">s</mi></mrow><mrow id="S4.E2.m1.1.1.3.2.3" xref="S4.E2.m1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.1.1.3.2.3.2" xref="S4.E2.m1.1.1.3.2.3.2.cmml">T</mi><mo id="S4.E2.m1.1.1.3.2.3.1" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.3" xref="S4.E2.m1.1.1.3.2.3.3.cmml">o</mi><mo id="S4.E2.m1.1.1.3.2.3.1a" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.4" xref="S4.E2.m1.1.1.3.2.3.4.cmml">t</mi><mo id="S4.E2.m1.1.1.3.2.3.1b" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.5" xref="S4.E2.m1.1.1.3.2.3.5.cmml">a</mi><mo id="S4.E2.m1.1.1.3.2.3.1c" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.6" xref="S4.E2.m1.1.1.3.2.3.6.cmml">l</mi><mo id="S4.E2.m1.1.1.3.2.3.1d" lspace="0.170em" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.7" xref="S4.E2.m1.1.1.3.2.3.7.cmml">N</mi><mo id="S4.E2.m1.1.1.3.2.3.1e" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.8" xref="S4.E2.m1.1.1.3.2.3.8.cmml">u</mi><mo id="S4.E2.m1.1.1.3.2.3.1f" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.9" xref="S4.E2.m1.1.1.3.2.3.9.cmml">m</mi><mo id="S4.E2.m1.1.1.3.2.3.1g" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.10" xref="S4.E2.m1.1.1.3.2.3.10.cmml">b</mi><mo id="S4.E2.m1.1.1.3.2.3.1h" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.11" xref="S4.E2.m1.1.1.3.2.3.11.cmml">e</mi><mo id="S4.E2.m1.1.1.3.2.3.1i" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.12" xref="S4.E2.m1.1.1.3.2.3.12.cmml">r</mi><mo id="S4.E2.m1.1.1.3.2.3.1j" lspace="0.170em" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.13" xref="S4.E2.m1.1.1.3.2.3.13.cmml">o</mi><mo id="S4.E2.m1.1.1.3.2.3.1k" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.14" xref="S4.E2.m1.1.1.3.2.3.14.cmml">f</mi><mo id="S4.E2.m1.1.1.3.2.3.1l" lspace="0.170em" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.15" xref="S4.E2.m1.1.1.3.2.3.15.cmml">P</mi><mo id="S4.E2.m1.1.1.3.2.3.1m" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.16" xref="S4.E2.m1.1.1.3.2.3.16.cmml">a</mi><mo id="S4.E2.m1.1.1.3.2.3.1n" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.17" xref="S4.E2.m1.1.1.3.2.3.17.cmml">i</mi><mo id="S4.E2.m1.1.1.3.2.3.1o" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.18" xref="S4.E2.m1.1.1.3.2.3.18.cmml">r</mi><mo id="S4.E2.m1.1.1.3.2.3.1p" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.19" xref="S4.E2.m1.1.1.3.2.3.19.cmml">s</mi></mrow></mfrac><mo id="S4.E2.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S4.E2.m1.1.1.3.1.cmml">*</mo><mn id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml">100</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"></eq><apply id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2"><times id="S4.E2.m1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.2.1"></times><ci id="S4.E2.m1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.2.2">𝐴</ci><ci id="S4.E2.m1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.2.3">𝑐</ci><ci id="S4.E2.m1.1.1.2.4.cmml" xref="S4.E2.m1.1.1.2.4">𝑐</ci><ci id="S4.E2.m1.1.1.2.5.cmml" xref="S4.E2.m1.1.1.2.5">𝑢</ci><ci id="S4.E2.m1.1.1.2.6.cmml" xref="S4.E2.m1.1.1.2.6">𝑟</ci><ci id="S4.E2.m1.1.1.2.7.cmml" xref="S4.E2.m1.1.1.2.7">𝑎</ci><ci id="S4.E2.m1.1.1.2.8.cmml" xref="S4.E2.m1.1.1.2.8">𝑐</ci><ci id="S4.E2.m1.1.1.2.9.cmml" xref="S4.E2.m1.1.1.2.9">𝑦</ci></apply><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><times id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3.1"></times><apply id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2"><divide id="S4.E2.m1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.3.2"></divide><apply id="S4.E2.m1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.3.2.2"><times id="S4.E2.m1.1.1.3.2.2.1.cmml" xref="S4.E2.m1.1.1.3.2.2.1"></times><ci id="S4.E2.m1.1.1.3.2.2.2.cmml" xref="S4.E2.m1.1.1.3.2.2.2">𝐶</ci><ci id="S4.E2.m1.1.1.3.2.2.3.cmml" xref="S4.E2.m1.1.1.3.2.2.3">𝑜</ci><ci id="S4.E2.m1.1.1.3.2.2.4.cmml" xref="S4.E2.m1.1.1.3.2.2.4">𝑟</ci><ci id="S4.E2.m1.1.1.3.2.2.5.cmml" xref="S4.E2.m1.1.1.3.2.2.5">𝑟</ci><ci id="S4.E2.m1.1.1.3.2.2.6.cmml" xref="S4.E2.m1.1.1.3.2.2.6">𝑒</ci><ci id="S4.E2.m1.1.1.3.2.2.7.cmml" xref="S4.E2.m1.1.1.3.2.2.7">𝑐</ci><ci id="S4.E2.m1.1.1.3.2.2.8.cmml" xref="S4.E2.m1.1.1.3.2.2.8">𝑡</ci><ci id="S4.E2.m1.1.1.3.2.2.9.cmml" xref="S4.E2.m1.1.1.3.2.2.9">𝐴</ci><ci id="S4.E2.m1.1.1.3.2.2.10.cmml" xref="S4.E2.m1.1.1.3.2.2.10">𝑠</ci><ci id="S4.E2.m1.1.1.3.2.2.11.cmml" xref="S4.E2.m1.1.1.3.2.2.11">𝑠</ci><ci id="S4.E2.m1.1.1.3.2.2.12.cmml" xref="S4.E2.m1.1.1.3.2.2.12">𝑜</ci><ci id="S4.E2.m1.1.1.3.2.2.13.cmml" xref="S4.E2.m1.1.1.3.2.2.13">𝑐</ci><ci id="S4.E2.m1.1.1.3.2.2.14.cmml" xref="S4.E2.m1.1.1.3.2.2.14">𝑖</ci><ci id="S4.E2.m1.1.1.3.2.2.15.cmml" xref="S4.E2.m1.1.1.3.2.2.15">𝑎</ci><ci id="S4.E2.m1.1.1.3.2.2.16.cmml" xref="S4.E2.m1.1.1.3.2.2.16">𝑡</ci><ci id="S4.E2.m1.1.1.3.2.2.17.cmml" xref="S4.E2.m1.1.1.3.2.2.17">𝑖</ci><ci id="S4.E2.m1.1.1.3.2.2.18.cmml" xref="S4.E2.m1.1.1.3.2.2.18">𝑜</ci><ci id="S4.E2.m1.1.1.3.2.2.19.cmml" xref="S4.E2.m1.1.1.3.2.2.19">𝑛</ci><ci id="S4.E2.m1.1.1.3.2.2.20.cmml" xref="S4.E2.m1.1.1.3.2.2.20">𝑠</ci></apply><apply id="S4.E2.m1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.3.2.3"><times id="S4.E2.m1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.1.1.3.2.3.1"></times><ci id="S4.E2.m1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.1.1.3.2.3.2">𝑇</ci><ci id="S4.E2.m1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.1.1.3.2.3.3">𝑜</ci><ci id="S4.E2.m1.1.1.3.2.3.4.cmml" xref="S4.E2.m1.1.1.3.2.3.4">𝑡</ci><ci id="S4.E2.m1.1.1.3.2.3.5.cmml" xref="S4.E2.m1.1.1.3.2.3.5">𝑎</ci><ci id="S4.E2.m1.1.1.3.2.3.6.cmml" xref="S4.E2.m1.1.1.3.2.3.6">𝑙</ci><ci id="S4.E2.m1.1.1.3.2.3.7.cmml" xref="S4.E2.m1.1.1.3.2.3.7">𝑁</ci><ci id="S4.E2.m1.1.1.3.2.3.8.cmml" xref="S4.E2.m1.1.1.3.2.3.8">𝑢</ci><ci id="S4.E2.m1.1.1.3.2.3.9.cmml" xref="S4.E2.m1.1.1.3.2.3.9">𝑚</ci><ci id="S4.E2.m1.1.1.3.2.3.10.cmml" xref="S4.E2.m1.1.1.3.2.3.10">𝑏</ci><ci id="S4.E2.m1.1.1.3.2.3.11.cmml" xref="S4.E2.m1.1.1.3.2.3.11">𝑒</ci><ci id="S4.E2.m1.1.1.3.2.3.12.cmml" xref="S4.E2.m1.1.1.3.2.3.12">𝑟</ci><ci id="S4.E2.m1.1.1.3.2.3.13.cmml" xref="S4.E2.m1.1.1.3.2.3.13">𝑜</ci><ci id="S4.E2.m1.1.1.3.2.3.14.cmml" xref="S4.E2.m1.1.1.3.2.3.14">𝑓</ci><ci id="S4.E2.m1.1.1.3.2.3.15.cmml" xref="S4.E2.m1.1.1.3.2.3.15">𝑃</ci><ci id="S4.E2.m1.1.1.3.2.3.16.cmml" xref="S4.E2.m1.1.1.3.2.3.16">𝑎</ci><ci id="S4.E2.m1.1.1.3.2.3.17.cmml" xref="S4.E2.m1.1.1.3.2.3.17">𝑖</ci><ci id="S4.E2.m1.1.1.3.2.3.18.cmml" xref="S4.E2.m1.1.1.3.2.3.18">𝑟</ci><ci id="S4.E2.m1.1.1.3.2.3.19.cmml" xref="S4.E2.m1.1.1.3.2.3.19">𝑠</ci></apply></apply><cn id="S4.E2.m1.1.1.3.3.cmml" type="integer" xref="S4.E2.m1.1.1.3.3">100</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">Accuracy=\frac{Correct\,Associations}{Total\,Number\,of\,Pairs}*100</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.1d">italic_A italic_c italic_c italic_u italic_r italic_a italic_c italic_y = divide start_ARG italic_C italic_o italic_r italic_r italic_e italic_c italic_t italic_A italic_s italic_s italic_o italic_c italic_i italic_a italic_t italic_i italic_o italic_n italic_s end_ARG start_ARG italic_T italic_o italic_t italic_a italic_l italic_N italic_u italic_m italic_b italic_e italic_r italic_o italic_f italic_P italic_a italic_i italic_r italic_s end_ARG * 100</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.1.1">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.1.1.1" style="width:45.5pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.2.1.1.1.1">Camera Name</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.1.1.2" style="width:22.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.2.1.1.2.1">Image Count</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.1.1.3" style="width:37.0pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.2.1.1.3.1">Accuracy (%)</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.1.1.4" style="width:37.0pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.2.1.1.4.1">Correct 
<br class="ltx_break"/>Pred.</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.1.1.5" style="width:37.0pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.2.1.1.5.1">Total 
<br class="ltx_break"/>Pred.</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_tt" id="S4.T3.2.2.2.1" style="width:45.5pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.2.2.1.1">Altona</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T3.2.2.2.2" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.2.2.2.1">60</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T3.2.2.2.3" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.2.2.3.1">34.52</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T3.2.2.2.4" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.2.2.4.1">29</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T3.2.2.2.5" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.2.2.5.1">84</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.3.3">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.3.3.1" style="width:45.5pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.3.3.1.1">Altona-F</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.3.3.2" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.3.3.2.1">61</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.3.3.3" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.3.3.3.1">43.00</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.3.3.4" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.3.3.4.1">43</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.3.3.5" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.3.3.5.1">100</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4.4">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.4.4.1" style="width:45.5pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.4.4.1.1">Blockbraeu</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.4.4.2" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.4.4.2.1">70</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.4.4.3" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.4.4.3.1">77.25</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.4.4.4" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.4.4.4.1">180</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.4.4.5" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.4.4.5.1">233</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.5.5">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.5.5.1" style="width:45.5pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.5.5.1.1">Elbe</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.5.5.2" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.5.5.2.1">60</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.5.5.3" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.5.5.3.1">24.48</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.5.5.4" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.5.5.4.1">24</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.5.5.5" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.5.5.5.1">143</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.6.6">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.6.6.1" style="width:45.5pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.6.6.1.1">Hafencity</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.6.6.2" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.6.6.2.1">60</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.6.6.3" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.6.6.3.1">80.33</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.6.6.4" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.6.6.4.1">49</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.6.6.5" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.6.6.5.1">61</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.7.7">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.7.7.1" style="width:45.5pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.7.7.1.1">Neumühlen</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.7.7.2" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.7.7.2.1">70</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.7.7.3" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.7.7.3.1">94.13</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.7.7.4" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.7.7.4.1">625</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.7.7.5" style="width:37.0pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.7.7.5.1">664</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.8.8">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_tt" id="S4.T3.2.8.8.1" style="width:45.5pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.2.8.8.1.1">Total</span></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt" id="S4.T3.2.8.8.2" style="width:22.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.2.8.8.2.1">381</span></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt" id="S4.T3.2.8.8.3" style="width:37.0pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.2.8.8.3.1">74.79</span></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt" id="S4.T3.2.8.8.4" style="width:37.0pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.2.8.8.4.1">961</span></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt" id="S4.T3.2.8.8.5" style="width:37.0pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.2.8.8.5.1">1285</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">The accuracy of successful associations for cameras. Altona-F is the fixed behaviour of the Altona camera.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Table <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.2 Image-AIS Matching ‣ 4 Results and Discussion ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">3</span></a> shows the accuracy of successful associations for each camera. The overall performance of the system is 74.79 % accuracy in matching bounding boxes with AIS messages. For fixed and panning cameras, the system achieves 85.06 % and 39.24 %, respectively.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Fixed cameras outperform panning cameras because of the extra step of localisation of query images in panorama images, which fixed cameras do not have. Any bounding box-AIS message pairs that are present on an unsuccessfully localised query image are counted as unsuccessful matches whether they are successfully matched or not, as the localisation step comes prior to the matching step.
</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">Tidal changes slightly affect the accuracy negatively since the keypoints were selected on sea level and the plane predicted by homography does not update itself accordingly. Weather conditions, such as fog and rain, can blur or entirely obstruct vessels in images. Although it is a rare scenario, sometimes birds can block the view of cameras.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">The camera Elbe, which covers the widest area compared to the other cameras, is the worst performing one with an accuracy of 24.48 %. This poor performance can be explained in two aspects. Considering the hardware aspect, Elbe is a panning camera located furthest from the river without a cover for the rain. It has the lowest number of keypoints, which results in the poor transformation from AIS message coordinates to image coordinates. Moreover, it introduces the highest amount of distortion when the panorama is created because of the wide area it covers, which results in poor performance for localising a query image in the panorama image. The accuracy of correct localisation of a query image is 58.33 %. Considering these drawbacks, the Elbe camera requires more manual work for the selection of new keypoints, and a better technique of localisation of query images in panorama images.</p>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1">Neumühlen is a fixed camera that faces towards a pier where service boats and tug boats moor. More importantly, the direction of the camera is not perpendicular to the river. This makes the association easier as the error introduced by the unstable video stream becomes more tolerable because of the smaller displacement in the image, as mentioned in Section <a class="ltx_ref" href="#S3.SS4" title="3.4 Dataset ‣ 3 Methodology ‣ Image and AIS Data Fusion Technique for Maritime Computer Vision Applications"><span class="ltx_text ltx_ref_tag">3.4</span></a>. When the vessels move slower or moor, it is easier to correctly associate bounding boxes with AIS messages as long as they keep transmitting AIS messages which tug boats mostly do since they only moor for a short period of time. The slow but consistent traffic can explain the high performance of the Neumühlen camera.</p>
</div>
<div class="ltx_para" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.1">Comparing the results with Lu et al.’s framework, which obtains 69.35 % without the short-time trajectory prediction method (DR), and 75.70 % with DR, Qu et al.’s framework outperform Lu’s framework with 81.42 % overall accuracy. Our approach, on the other hand, outperforms both works for static cameras while the overall accuracy is slightly lower than Qu et al.’s framework. These works do not use a dataset consisting of images taken from web cameras with different and unknown parameters. Additionally, the datasets do not contain weather conditions such as fog and rain. Lu’s dataset consists of images of the English Channel, while Qu’s dataset consists of images of the Yangtze River, both of which are significantly broader than the Elbe river. Therefore, the traffic of the Elbe River is expected to be denser which requires more precision in the predictions and handling for the occluded vessels.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, a technique was developed for creating maritime computer vision datasets by fusing ship bounding boxes with related AIS messages. The technique extends Carillo’s technique <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite> by adapting the homography-based coordinate transformation for non-static cameras, thus increases the number of application use cases. The technique consists of a fine-tuned YOLOv5 vessel detection model and a homography-based coordinate transformation module that can project world coordinates extracted from AIS messages onto images to match detected vessels’ bounding boxes with AIS messages.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The results show that the proposed system achieves an accuracy of 85.06 % with fixed cameras while achieving an overall accuracy of 74.79 %. These results were achieved with images taken in various weather conditions during the day. Compared to the similar frameworks, it performs better for fixed cameras while can work with the panning cameras.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Using the proposed technique, a dataset consisting of images and associated AIS messages can be created covering the Elbe River in Hamburg, a major hub and the third busiest port in Europe. Therefore, images offer a rich variety of ship types and multi-ship-encounter scenarios. Consequently, it stands as a valuable cornerstone for advancing maritime research through machine learning and artificial intelligence, serving as an indispensable resource for ongoing projects such as LEAS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib4" title="">4</a>]</cite>, AUTOSHIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib7" title="">7</a>]</cite>, and AEGIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib2" title="">2</a>]</cite>. Some possible use cases include inter alia research on elevating the precision of multi-sensor fusion techniques, thereby significantly enhancing navigational safety in the maritime domain and the creation of digital twins, opening doors to a multitude of possibilities for maritime system modeling and simulation, ultimately fostering a comprehensive understanding of the maritime ecosystem.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The proposed dataset is particularly useful for designing auto-labelling pipelines to enhance existing object detection models or to solve pose estimation tasks. Current investigations use the annotated data and the homography matrix to generate labelled bounding box annotations in 3D world space, which are fed into keypoint detection networks to learn pose estimation from image space without the need to generate a homography matrix.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In terms of improving the proposed technique, the localisation of query images in panoramic images using template matching can be considered as the main limitation of this work. Improving this aspect of the fusion process of panning cameras would increase the accuracy significantly as the results show that the main problem is not the coordinate transformation but the template matching. This problem can be overcome by implementing feature-based localisation techniques for most cases. However, ships that occlude the landscape, or weather conditions such as fog and rain require a completely different approach.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Currently, keypoint selection for homography estimation is a process that has to be done manually. Considering that the dataset only contains images of the Elbe river and the camera locations are roughly known, it is possible to apply shape alignment methods to align the landmarks with the map with a similar approach of Shi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>. Although sports fields have proper shapes since they are human-made, in the future, trying to register the sea charts with images of Elbe can be fruitful. Moreover, ground-to-satellite image matching methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite> can be an area to explore for automatic keypoint selection.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
World geodetic system 1984.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.unoosa.org/pdf/icg/2012/template/WGS_84.pdf" style="font-size:90%;" title="">https://www.unoosa.org/pdf/icg/2012/template/WGS_84.pdf</a><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">, 1984.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Advanced, efficient and green intermodal systems.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aegis.autonomous-ship.org/" style="font-size:90%;" title="">https://aegis.autonomous-ship.org/</a><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Artificial intelligence / machine learning sensor fusion for autonomous vessel
navigation (navisp-el1-020 maritime ai-nav), May 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Leas – shore-side decision support for traffic situations with highly
automated or autonomous vessels using ai.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://research.fleetmon.com/projects/leas-decision-recommendations-for-vts-with-autonomous-vessels-using-ai/" style="font-size:90%;" title="">https://research.fleetmon.com/projects/leas-decision-recommendations-for-vts-with-autonomous-vessels-using-ai/</a><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
M Baldauf, K Benedict, S Fischer, F Motz, and J-U Schröder-Hinrichs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Collision avoidance systems in air and maritime traffic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.3.1" style="font-size:90%;">Proceedings of the Institution of Mechanical Engineers, Part O:
Journal of Risk and Reliability</span><span class="ltx_text" id="bib.bib5.4.2" style="font-size:90%;">, 225(3):333–343, 2011.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Jon Louis Bentley.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Multidimensional binary search trees used for associative searching.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">Communications of the ACM</span><span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">, 18(9):509–517, 1975.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
V Bolbot, G Theotokatos, E Boulougouris, LAL Wennersberg, H Nordahl, Ø J
Rødseth, J Faivre, and M Molica Colella.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Paving the way toward autonomous shipping development for european
waters–the autoship project.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="font-size:90%;">2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Borja Carrillo-Perez, Sarah Barnes, and Maurice Stephan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">Ship segmentation and georeferencing from static oblique view images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">Sensors</span><span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">, 22(7):2713, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Lena Chang, Yi-Ting Chen, Jung-Hua Wang, and Yang-Lang Chang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Modified yolov3 for ship detection with visible and infrared images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">Electronics</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, 11(5):739, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Misganu Debella-Gilo and Andreas Kääb.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Sub-pixel precision image matching for measuring surface
displacements on mass movements using normalized cross-correlation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.3.1" style="font-size:90%;">Remote Sensing of Environment</span><span class="ltx_text" id="bib.bib10.4.2" style="font-size:90%;">, 115(1):130–142, 2011.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Lei Du, Floris Goerlandt, and Pentti Kujala.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Review and analysis of methods for assessing maritime waterway risk
based on non-accident critical events detected from ais data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">Reliability Engineering &amp; System Safety</span><span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">, 200:106933, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Ties Emmens, Chintan Amrit, Asad Abdi, and Mayukh Ghosh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">The promises and perils of automatic identification system data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">Expert Systems with Applications</span><span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">, 178:114975, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Glenn Jocher et. al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">ultralytics/yolov5: v6.0 - YOLOv5n ’Nano’ models, Roboflow
integration, TensorFlow export, OpenCV DNN support, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Vishal Gupta, Monish Gupta, and Parveen Singla.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Ship detection from highly cluttered images using convolutional
</span><span class="ltx_text" id="bib.bib14.3.2" style="font-size:90%;">neural network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.4.1" style="font-size:90%;">Wireless Personal Communications</span><span class="ltx_text" id="bib.bib14.5.2" style="font-size:90%;">, 121(1):287–305, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
London IMO.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Solas. consolidated edition, 2001.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">2001.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Zhelin Li, Lining Zhao, Xu Han, and Mingyang Pan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Lightweight ship detection methods based on yolov3 and densenet.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.3.1" style="font-size:90%;">Mathematical Problems in Engineering</span><span class="ltx_text" id="bib.bib16.4.2" style="font-size:90%;">, 2020, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Yongqiang Lu, Hongjie Ma, Edward Smart, Branislav Vuksanovic, John Chiverton,
Shanker Radhakrishna Prabhu, Malcolm Glaister, Eric Dunston, and Chris
Hancock.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Fusion of camera-based vessel detection and ais for maritime
surveillance.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">2021 26th International Conference on Automation and
Computing (ICAC)</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, pages 1–6, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Fabio Mazzarella, Michele Vespe, and Carlos Santamaria.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Sar ship detection and self-reporting data fusion based on traffic
</span><span class="ltx_text" id="bib.bib18.3.2" style="font-size:90%;">knowledge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.4.1" style="font-size:90%;">IEEE Geoscience and Remote Sensing Letters</span><span class="ltx_text" id="bib.bib18.5.2" style="font-size:90%;">, 12(8):1685–1689,
2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
United States Department of Defense.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Global positioning system standard positioning service performance
standard.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.gps.gov/technical/ps/2008-SPS-performance-standard.pdf" style="font-size:90%;" title="">https://www.gps.gov/technical/ps/2008-SPS-performance-standard.pdf</a><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">,
2008.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Captain Edward Page.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Maximizing maritime safety and environmental protection with ais:
(automatic identification system).
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib20.4.2" style="font-size:90%;">OCEANS 2017 - Anchorage</span><span class="ltx_text" id="bib.bib20.5.3" style="font-size:90%;">, pages 1–4, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Giuliana Pallotta, Michele Vespe, and Karna Bryan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Vessel pattern knowledge discovery from ais data: A framework for
anomaly detection and route prediction.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.3.1" style="font-size:90%;">Entropy</span><span class="ltx_text" id="bib.bib21.4.2" style="font-size:90%;">, 15(6):2218–2245, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib22.2.2" style="font-size:90%;">Jingxiang Qu, Yu Guo, Yuxu Lu, Fenghua Zhu, Yingchun Huan, and Ryan Wen Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">Intelligent maritime surveillance framework driven by fusion of
camera-based vessel detection and ais data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.4.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.5.2" style="font-size:90%;">2022 IEEE 25th International Conference on Intelligent
Transportation Systems (ITSC)</span><span class="ltx_text" id="bib.bib22.6.3" style="font-size:90%;">, pages 2280–2285, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Orb: An efficient alternative to sift or surf.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib23.4.2" style="font-size:90%;">2011 International conference on computer vision</span><span class="ltx_text" id="bib.bib23.5.3" style="font-size:90%;">, pages
2564–2571. Ieee, 2011.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Feng Shi, Paul Marchwica, Juan Camilo Gamboa Higuera, Mike Jamieson, Mehrsan
Javan, and Parthipan Siva.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Self-supervised shape alignment for sports field registration.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib24.4.2" style="font-size:90%;">2022 IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV)</span><span class="ltx_text" id="bib.bib24.5.3" style="font-size:90%;">, pages 3768–3777, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Yujiao Shi, Xin Yu, Liu Liu, Dylan Campbell, Piotr Koniusz, and Hongdong Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Accurate 3-dof camera geo-localization via ground-to-satellite image
matching.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">arXiv preprint arXiv:2203.14148</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Ming-Cheng Tsou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Discovering knowledge from ais database for application in vts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.3.1" style="font-size:90%;">The Journal of Navigation</span><span class="ltx_text" id="bib.bib26.4.2" style="font-size:90%;">, 63(3):449–469, 2010.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Tamara A. Volkova, Yulia E. Balykina, and Alexander Bespalov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Predicting ship trajectory based on neural networks using ais data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">Journal of Marine Science and Engineering</span><span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">, 9(3), 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Image quality assessment: from error visibility to structural
similarity.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.3.1" style="font-size:90%;">IEEE transactions on image processing</span><span class="ltx_text" id="bib.bib28.4.2" style="font-size:90%;">, 13(4):600–612, 2004.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Chris M Ward, Josh Harguess, and Cameron Hilton.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Ship classification from overhead imagery using synthetic data and
domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib29.4.2" style="font-size:90%;">OCEANS 2018 MTS/IEEE Charleston</span><span class="ltx_text" id="bib.bib29.5.3" style="font-size:90%;">, pages 1–5. IEEE, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Tong Xiaopeng, Chen Xu, Sang Lingzhi, Mao Zhe, and Wu Qing.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Vessel trajectory prediction in curving channel of inland river.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib30.4.2" style="font-size:90%;">2015 International Conference on Transportation Information
and Safety (ICTIS)</span><span class="ltx_text" id="bib.bib30.5.3" style="font-size:90%;">, pages 706–714, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Puchun Xie, Ran Tao, Xin Luo, and Youqun Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Yolov4-mobilenetv2-dw-lcarm: A real-time ship detection network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib31.4.2" style="font-size:90%;">International Conference on Knowledge Management in
Organizations</span><span class="ltx_text" id="bib.bib31.5.3" style="font-size:90%;">, pages 281–293. Springer, 2022.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Dec  7 20:44:23 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
