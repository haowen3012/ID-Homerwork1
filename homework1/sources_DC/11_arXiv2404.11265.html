<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data</title>
<!--Generated on Fri May 31 15:58:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.11265v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.11265v2#S1" title="In The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11265v2#S2" title="In The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11265v2#S2.SS1" title="In 2 Related Work ‣ The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11265v2#S2.SS2" title="In 2 Related Work ‣ The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Backdoor Attack and Defense</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zixuan Zhu, Rui Wang ,
Cong Zou, Lihua Jing
<br class="ltx_break"/>SKLOIS, Institute of Information Engineering, CAS, Beijing, China
<br class="ltx_break"/>School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1" style="font-size:90%;">{zhuzixuan, wangrui, zoucong, jinglihua}@iie.ac.cn</span>
</span><span class="ltx_author_notes">Corresponding author</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Recently, backdoor attacks have posed a serious security threat to the training process of deep neural networks (DNNs). The attacked model behaves normally on benign samples but outputs a specific result when the trigger is present. However, compared with the rocketing progress of backdoor attacks, existing defenses are difficult to deal with these threats effectively or require benign samples to work, which may be unavailable in real scenarios. In this paper, we find that the poisoned samples and benign samples can be distinguished with prediction entropy. This inspires us to propose a novel dual-network training framework: The Victim and The Beneficiary (V&amp;B), which exploits a poisoned model to train a clean model without extra benign samples. Firstly, we sacrifice the Victim network to be a powerful poisoned sample detector by training on suspicious samples. Secondly, we train the Beneficiary network on the credible samples selected by the Victim to inhibit backdoor injection. Thirdly, a semi-supervised suppression strategy is adopted for erasing potential backdoors and improving model performance. Furthermore, to better inhibit missed poisoned samples, we propose a strong data augmentation method, AttentionMix, which works well with our proposed V&amp;B framework. Extensive experiments on two widely used datasets against 6 state-of-the-art attacks demonstrate that our framework is effective in preventing backdoor injection and robust to various attacks while maintaining the performance on benign samples.
Our code is available at <a class="ltx_ref ltx_href" href="https://github.com/Zixuan-Zhu/VaB" title="">https://github.com/Zixuan-Zhu/VaB</a>.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="507" id="S0.F1.1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The average prediction entropy of benign samples versus poisoned samples crafted by 6 backdoor attacks. We conduct the experiment on CIFAR-10 with ResNet-18 under poisoning rate 10%, where our special training strategy (described in section <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:section:_warmup</span>) was adopted.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large amounts of training data and powerful computing resources are two key factors for the success of deep neural networks (DNNs). While in practical applications, obtaining enough samples for training is laborious, and training is likely to be resource-constrained. As a result, more and more third-party data (<em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">e.g</em>.<span class="ltx_text" id="S1.p1.1.2"></span> data released on the Internet) and training platforms (<em class="ltx_emph ltx_font_italic" id="S1.p1.1.3">e.g</em>.<span class="ltx_text" id="S1.p1.1.4"></span> Google Colab) are adopted to reduce the overhead, which brings new security risks.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Backdoor attacks <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">proflip</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">trojannet</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">qi2022towards</span>]</cite> pose a serious security threat to the training process of DNNs and can be easily executed through data poisoning. Specifically, attackers <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">blend</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">badnets</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">wanet</span>]</cite> inject the designed <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">trigger</em> (<em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">e.g</em>.<span class="ltx_text" id="S1.p2.1.3"></span> a small patch or random noise) to a few benign samples selected from the training set and change their labels to the attacker-defined <em class="ltx_emph ltx_font_italic" id="S1.p2.1.4">target label</em>. These poisoned samples will force models to learn the correlation between the trigger and the target label. In the reference process, the attacked model will behave normally on benign samples but output the target label when the trigger is present. Deploying attacked models can lead to severe consequences, even life-threatening in some scenarios (<em class="ltx_emph ltx_font_italic" id="S1.p2.1.5">e.g</em>.<span class="ltx_text" id="S1.p2.1.6"></span> autonomous driving). Hence, a secure training framework is needed when using third-party data or training platforms.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Many researchers have devoted themselves to this topic and proposed feasible defenses. Li <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">et al</em>.<span class="ltx_text" id="S1.p3.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">NAD</span>]</cite> utilize local benign samples to erase existing backdoors in DNNs, and Borgnia <em class="ltx_emph ltx_font_italic" id="S1.p3.1.3">et al</em>.<span class="ltx_text" id="S1.p3.1.4"></span> <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">borgnia2021strong</span>]</cite> employ strong data augmentations to inhibit backdoor injection during training. Li <em class="ltx_emph ltx_font_italic" id="S1.p3.1.5">et al</em>.<span class="ltx_text" id="S1.p3.1.6"></span> <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">ABL</span>]</cite> find that the training loss decreased faster for poisoned samples than benign samples and designed a loss function to separate them. Finally, they select a fixed portion of samples with the lowest loss to erase potential backdoors. In real scenarios, it is challenging to distinguish poisoned samples from benign ones in this way due to the unknown poisoning rate and close loss values. In experiments, we find the entropy of model prediction is a more discriminative property, and a fixed threshold could filter out most poisoned samples, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11265v2#S0.F1" title="Figure 1 ‣ The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data"><span class="ltx_text ltx_ref_tag">1</span></a>. During training, poisoned samples will be learned faster than benign samples due to the similar feature of their triggers <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">ABL</span>]</cite>. Hence, the poisoned network can confidently predict poisoned samples as the target label in early epochs but hesitates about the benign samples.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Based on this observation, we propose a novel secure training framework, <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">The Victim and The Beneficiary (V&amp;B)</em>, that can directly train clean models on poisoned data without resorting to benign samples. Firstly, in warming up stage, the training dataset is recomposed into a suspicious set and a credible set according to the prediction entropy is lower or higher than the predefined threshold. Then the Victim network is trained iteratively with the suspicious set to obtain a strong poisoned sample detector. Secondly, in the clean training stage, the Beneficiary network is trained with credible samples filtered by the Victim to inhibit backdoor injection. In later training epochs, the Victim network is fully exploited to provide its learned knowledge for the Beneficiary network to help the latter erase potential backdoors through semi-supervised learning. To diminish the threat of missed poisoned samples, we design a strong data augmentation <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">AttentionMix</em>, which mixes the image region with high activation values according to the attention map. Compared with existing augmentations <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">attentivecutmix</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">cutmix</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">mixup</span>]</cite>, it has a stronger inhibition effect against stealthy backdoor attacks <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">Dynamic</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">wanet</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">CL</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The main contributions of this paper are as follows: <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">(1)</span> We propose a novel dual-network secure training framework, V&amp;B, which can train clean models on the poisoned dataset without resorting to benign samples. <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">(2)</span> We design a powerful data augmentation method called AttentionMix, which has a stronger inhibition effect against stealthy backdoor attacks. <span class="ltx_text ltx_font_bold" id="S1.p5.1.3">(3)</span> Extensive experiments on two popular benchmark datasets demonstrate the effectiveness and robustness of our framework.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Augmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Data Augmentation is a common-used technique to improve the generalization capabilities of DNNs. In addition to some popular weak data augmentations (such as cropping, flipping, rotation, etc.), many strong data augmentation techniques <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">attentivecutmix</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">cutmix</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">mixup</span>]</cite> have been proposed to further improve model performance. Mixup <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">mixup</span>]</cite> linearly interpolates any two images and their labels, then trains the model with generated samples. However, the generated samples tend to be unnatural, which will decrease model performances on localization and object detection tasks. Instead of linear interpolation, Cutmix <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">cutmix</span>]</cite> replaces a random patch with the same region from another image and mixes their labels according to the ratio of the patch area. Further, Attentive Cutmix <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">attentivecutmix</span>]</cite> only pastes the influential region located by an attention mechanism to other images, effectively forcing models to learn discriminative details. Yet it also mixes labels based on the ratio of the region area, ignoring the importance of the region in two original images, and direct pasting may bring complete triggers into benign samples. Our AttentionMix takes into account the importance of the region in both images and blends the influential region with the same area in another image, which can destroy the completeness of triggers.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Backdoor Attack and Defense</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In this paper, we only consider poisoning-based attacks toward image classification that can occur during the data preparation stage.
</p>
<p class="ltx_p ltx_align_left" id="S2.SS2.p1.2"><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.2.1">Poisoned-based backdoor attack.</span>
In early studies, attackers usually adopt simple and visible patterns as the trigger, such as a black-white checkerboard <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">badnets</span>]</cite>. To escape human inspections, some works use human-imperceptible deformations <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">wanet</span>]</cite> as the trigger or design unique triggers <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2021invisible</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">Dynamic</span>]</cite> for each poisoned sample. However, they randomly select benign samples to inject triggers and change their labels (called poison-label attacks), possibly causing the content of an image not to match its label. Instead of randomly selecting samples to poison, Turner <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.2.2">et al</em>.<span class="ltx_text" id="S2.SS2.p1.2.3"></span> <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">CL</span>]</cite>, Mauro <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.2.4">et al</em>.<span class="ltx_text" id="S2.SS2.p1.2.5"></span> <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">SIG</span>]</cite>, and Liu <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.2.6">et al</em>.<span class="ltx_text" id="S2.SS2.p1.2.7"></span> <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">reflection</span>]</cite> only inject triggers into benign samples under the target label (called clean-label attacks). Although some triggers are imperceptible in the input space, they can be easily detected by defenses focused on latent space extracted by CNNs. Therefore, recent studies <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:conf/cvpr/Zhao0XDWL22</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">imperceptible</span>]</cite> not only ensure the trigger’s invisibility in the input space but also restrict the similarity between poison samples and corresponding benign samples in the latent space.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 31 15:58:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
