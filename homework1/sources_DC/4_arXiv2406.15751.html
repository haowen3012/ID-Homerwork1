<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data</title>
<!--Generated on Sat Jun 22 06:30:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.15751v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S1" title="In Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S2" title="In Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S2.SS1" title="In 2 Related Works â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Neural Amplifier modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S2.SS2" title="In 2 Related Works â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Generative Adversarial Networks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S2.SS3" title="In 2 Related Works â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Backbone Model for Generator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S2.SS4" title="In 2 Related Works â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Discriminators for GANs training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S2.SS5" title="In 2 Related Works â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Clean Audio from Existing Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S3" title="In Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S3.SS1" title="In 3 Methods â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Generator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S3.SS2" title="In 3 Methods â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Discriminator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S3.SS3" title="In 3 Methods â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>GAN Loss</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4" title="In Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.SS1" title="In 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.SS2" title="In 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.SS3" title="In 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Implementation details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.SS4" title="In 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Evaluation settings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S5" title="In Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Result</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S5.SS1" title="In 5 Experimental Result â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Comparison with Baseline Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S5.SS2" title="In 5 Experimental Result â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Clean Audio Combination</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S6" title="In Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S6.SS1" title="In 6 Discussion â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Benefit of the GAN-based Approach for VA Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S6.SS2" title="In 6 Discussion â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Artifacts Generated by the Proposed Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S7" title="In Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S8" title="In Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>ACKNOWLEDGEMENTS</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document" style="font-size:90%;">Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1"><span class="ltx_text" id="id1.id1.1" style="font-size:90%;">Recent years have seen increasing interest in applying deep learning methods to the modeling of guitar amplifiers or effect pedals.
Existing methods are mainly based on the supervised approach, requiring temporally-aligned data pairs of unprocessed and rendered audio.
However, this approach does not scale well,
due to the complicated process involved in creating the data pairs.
A very recent work done by Wright <em class="ltx_emph ltx_font_italic" id="id1.id1.1.1">et al.</em> has explored the potential of leveraging unpaired data for training, using a generative adversarial network (GAN)-based framework.
This paper extends their work by using more advanced discriminators in the GAN, and using more unpaired data for training.
Specifically, drawing inspiration from recent advancements in neural vocoders, we employ in our GAN-based model for guitar amplifier modeling two sets of discriminators, one based on multi-scale discriminator (MSD) and the other multi-period discriminator (MPD). Moreover, we experiment with adding unprocessed audio signals that do not have the corresponding rendered audio of a target tone to the training data, to see how much the GAN model benefits from the unpaired data. Our experiments show that the proposed two extensions contribute to the modeling of both low-gain and high-gain guitar amplifiers.</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_text" id="S1.p1.1.1" style="font-size:90%;">Amplifier modeling involves developing algorithms to emulate the behavior of real amplifiers. The amplifiers typically discussed in the literature are vacuum tube amplifiers. This task can also be considered a virtual analog (VA) modeling problem. Recent studies have demonstrated the potential to apply neural networks to VA modeling tasks using supervised learning. Various network architectures have been proposed in the literature, such as convolution-based and recurrent-based networks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib5" title="">5</a><span class="ltx_text" id="S1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.4" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text" id="S1.p2.1.1" style="font-size:90%;">Training in a supervised setting has already yielded promising results for VA modeling and guitar amplifier modeling. Many commercial applications have adopted this approach, using mainly the minimization of the error-to-signal ratio (ESR) as the training objective </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a><span class="ltx_text" id="S1.p2.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.4" style="font-size:90%;">.
However, the supervised approach does not scale well, for it requires paired data to model the transformation process from a clean audio input to a rendered audio output for a target tone.
Each data pair has to be temporally aligned and be about the same content of guitar performance.
In many cases,
however, the tone or timbre of a </span><em class="ltx_emph ltx_font_italic" id="S1.p2.1.5" style="font-size:90%;">rendered</em><span class="ltx_text" id="S1.p2.1.6" style="font-size:90%;"> audio signal often lacks the </span><em class="ltx_emph ltx_font_italic" id="S1.p2.1.7" style="font-size:90%;">unprocessed</em><span class="ltx_text" id="S1.p2.1.8" style="font-size:90%;">, or direct-input (DI), audio counterpart, making supervised methods impractical. While it might be possible to create such data pairs by inverting the clean audio directly from a rendered audio by means of a guitar effect removal model, the development of such models is still an ongoing area of research </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.9.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib7" title="">7</a><span class="ltx_text" id="S1.p2.1.10.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.11" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text" id="S1.p3.1.1" style="font-size:90%;">In other audio synthesis tasks such as neural vocoding </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib9" title="">9</a><span class="ltx_text" id="S1.p3.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.4" style="font-size:90%;"> and voice conversion </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib11" title="">11</a><span class="ltx_text" id="S1.p3.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.7" style="font-size:90%;">,
many advanced generative adversarial network (GAN) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib12" title="">12</a><span class="ltx_text" id="S1.p3.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.10" style="font-size:90%;"> models have been developed to generate realistic waveforms.
For example, MelGAN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib8" title="">8</a><span class="ltx_text" id="S1.p3.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.13" style="font-size:90%;"> proposed a multi-scale discriminator (MSD) for distinguishing between real audio and generated audio. HiFi-GAN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib13" title="">13</a><span class="ltx_text" id="S1.p3.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.16" style="font-size:90%;"> proposed a multi-period discriminator (MPD) that collaborates with the MSD.
Compared to models that minimize directly the reconstruction loss, GAN models employ such discriminators to learn customized loss functions in a data-driven fashion, usually leading to models that generate audio with finer details and better perceptual quality empirically.
</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text" id="S1.p4.1.1" style="font-size:90%;">We conjecture that a GAN-based approach can similarly offer two advantages for guitar amplifier modeling.</span></p>
<dl class="ltx_description" id="S1.I1">
<dt class="ltx_item" id="S1.I1.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.ix1.1.1.1">Adversarial losses</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1"><span class="ltx_text" id="S1.I1.ix1.p1.1.1" style="font-size:90%;">Adversarial losses offer a way to learn complicated, high-dimensional probability distributions from diverse and high-quality training data samples without explicitly modeling the underlying probability density function.
Specifically, GANs implicitly learn the data distribution using a self-learned loss function that is dynamically-adjusted as the training process unfolds.
</span></p>
</div>
</dd>
<dt class="ltx_item" id="S1.I1.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.ix2.1.1.1">Unsupervised training</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1"><span class="ltx_text" id="S1.I1.ix2.p1.1.1" style="font-size:90%;">We can use any available unpaired clean data as input to the generator during the training process, with the target being the designated amplifier-rendered data, thus potentially improving the generalizability of the model while reducing the burden of collecting paired data.</span></p>
</div>
</dd>
</dl>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text" id="S1.p5.1.1" style="font-size:90%;">To the best of our knowledge, the work of Wright </span><em class="ltx_emph ltx_font_italic" id="S1.p5.1.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="S1.p5.1.3" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p5.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a><span class="ltx_text" id="S1.p5.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p5.1.6" style="font-size:90%;"> represents the first and the only existing work that adopts GANs for guitar amplifier modeling.
Viewing amplifier modeling as a style transfer problem, they showed that a GAN-based model using the MSD proposed in MelGANÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p5.1.7.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib8" title="">8</a><span class="ltx_text" id="S1.p5.1.8.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p5.1.9" style="font-size:90%;"> as the discriminator can learn the amplifier modeling process without using reconstruction loss functions such as the ESR.
Moreover, they conducted experiments involving mismatched guitar timbre conversion between two timbres produced from distinct guitars. These experiments demonstrate the potential of adapting the unsupervised approach for guitar amplifier modeling.</span></p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text" id="S1.p6.1.1" style="font-size:90%;">Being inspired by the work of Wright </span><em class="ltx_emph ltx_font_italic" id="S1.p6.1.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="S1.p6.1.3" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p6.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a><span class="ltx_text" id="S1.p6.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p6.1.6" style="font-size:90%;">, we set forth to further extend this GAN-based approach by presenting the following two extensions.
First, while they position their work in the context of audio </span><em class="ltx_emph ltx_font_italic" id="S1.p6.1.7" style="font-size:90%;">style transfer</em><span class="ltx_text" id="S1.p6.1.8" style="font-size:90%;"> </span><span class="ltx_text" id="S1.p6.1.9" style="font-size:90%;color:#000000;">by employ MelGAN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib8" title="">8</a>]</cite> and several choices of spectral discriminator,</span><span class="ltx_text" id="S1.p6.1.10" style="font-size:90%;">
</span><span class="ltx_text" id="S1.p6.1.11" style="font-size:90%;color:#000000;">we further highlight the potential of integrating more advanced discriminators as proposed in neural vocoder research.</span><span class="ltx_text" id="S1.p6.1.12" style="font-size:90%;">
For example, it is well known that HiFi-GAN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p6.1.13.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib13" title="">13</a><span class="ltx_text" id="S1.p6.1.14.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p6.1.15" style="font-size:90%;"> empirically generates audio waveforms with higher quality than MelGANÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p6.1.16.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib8" title="">8</a><span class="ltx_text" id="S1.p6.1.17.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p6.1.18" style="font-size:90%;">.
Research on neural vocoders is relevant, because both vocoders and guitar amplifier modeling aim to produce high-quality audio waveforms given some input conditions.
Consequently, our first extension replaces the MSD discriminator used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p6.1.19.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a><span class="ltx_text" id="S1.p6.1.20.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p6.1.21" style="font-size:90%;"> by a combination of MSD and MPD discriminators, to study whether advanced discriminators can similarly contribute to better result for guitar amplifier modeling as the case seen in neural vocoding.</span></p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><span class="ltx_text" id="S1.p7.1.1" style="font-size:90%;">Our second contribution investigates more deeply the benefits of a GAN-based model in utilizing unpaired data. Specifically, we note that during the training process, Wright </span><em class="ltx_emph ltx_font_italic" id="S1.p7.1.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="S1.p7.1.3" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p7.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a><span class="ltx_text" id="S1.p7.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p7.1.6" style="font-size:90%;"> only used the unprocessed audio that </span><em class="ltx_emph ltx_font_italic" id="S1.p7.1.7" style="font-size:90%;">do</em><span class="ltx_text" id="S1.p7.1.8" style="font-size:90%;"> have the corresponding rendered audio of the target tone as the input to the generator.
However, as the GAN training does not require paired data, it is actually possible to utilize unprocessed audio that </span><em class="ltx_emph ltx_font_italic" id="S1.p7.1.9" style="font-size:90%;">do not</em><span class="ltx_text" id="S1.p7.1.10" style="font-size:90%;"> have the rendered audio counterpart of the target tone as the generatorâ€™s input.
We study such a case in our work, using input audio signals that do not align with the target output audio signals in training our model.</span></p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><span class="ltx_text" id="S1.p8.1.1" style="font-size:90%;">We conduct experiments on two public-domain guitar datasets, the </span><em class="ltx_emph ltx_font_italic" id="S1.p8.1.2" style="font-size:90%;">EGDB</em><span class="ltx_text" id="S1.p8.1.3" style="font-size:90%;"> datasetÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p8.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib15" title="">15</a><span class="ltx_text" id="S1.p8.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p8.1.6" style="font-size:90%;"> that have both low-gain and high-gain tones, and the </span><em class="ltx_emph ltx_font_italic" id="S1.p8.1.7" style="font-size:90%;">EGFxset</em><span class="ltx_text" id="S1.p8.1.8" style="font-size:90%;"> datasetÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p8.1.9.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib16" title="">16</a><span class="ltx_text" id="S1.p8.1.10.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p8.1.11" style="font-size:90%;"> for an extremely high-gain tone.
Experimental results show that the proposed extensions contribute positively to the modeling result, especially for the extremely high-gain case.
We provide audio samples online.</span><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ampDaFX24.notionlinker.com" title="">https://ampDaFX24.notionlinker.com</a></span></span></span><span class="ltx_text" id="S1.p8.1.12" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1"><span class="ltx_text" id="S1.p9.1.1" style="font-size:90%;">The paper is structured as follows: Section 2 reviews neural amplifier modeling methods and GANs. Section 3 presents our proposed method.
Section 4 describes the dataset and experimental setup; Section 5 reports the objective evaluation results. Section 6 discusses the results further. Finally, Section 7 concludes the paper with some ideas of future work.</span></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Neural Amplifier modeling</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text" id="S2.SS1.p1.1.1" style="font-size:90%;">Thanks to advancements in deep learning, neural networks have been utilized in several studies on amplifier modeling </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib20" title="">20</a><span class="ltx_text" id="S2.SS1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.4" style="font-size:90%;">. The neural network approach shares similarities with traditional black-box methods. For example, a convolutional layer can be conceptualized as a Wiener model </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib21" title="">21</a><span class="ltx_text" id="S2.SS1.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.7" style="font-size:90%;">. Existing neural network models for amplifier modeling are usually adapted from neural network models that are initially proposed for speech-related tasks.
While speech signals commonly operate at a 16â€‰kHz sampling rate, overdrive or distortion sound characteristics frequently manifest in the higher frequency range, requiring sampling rates of 44.1â€‰kHz or 48â€‰kHz.
As such, the adaptations may result in increased complexity and model size, which can be unfavorable given the requirements on real-time efficiency and low latency of VA modeling.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Generative Adversarial Networks</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.11"><span class="ltx_text" id="S2.SS2.p1.11.1" style="font-size:90%;">A GANÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.11.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib12" title="">12</a><span class="ltx_text" id="S2.SS2.p1.11.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.11.4" style="font-size:90%;"> is a generative model contains two components: a generator </span><math alttext="G" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" mathsize="90%" xref="S2.SS2.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">italic_G</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.5" style="font-size:90%;"> and a discriminator </span><math alttext="D" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" mathsize="90%" xref="S2.SS2.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">italic_D</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.6" style="font-size:90%;">. The discriminator </span><math alttext="D" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" mathsize="90%" xref="S2.SS2.p1.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">D</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">italic_D</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.7" style="font-size:90%;"> is essentially a classifier and it aims to output a value close to 1 for samples from â€œrealâ€ data distribution </span><math alttext="x\sim p_{d}" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><mrow id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mi id="S2.SS2.p1.4.m4.1.1.2" mathsize="90%" xref="S2.SS2.p1.4.m4.1.1.2.cmml">x</mi><mo id="S2.SS2.p1.4.m4.1.1.1" mathsize="90%" xref="S2.SS2.p1.4.m4.1.1.1.cmml">âˆ¼</mo><msub id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml"><mi id="S2.SS2.p1.4.m4.1.1.3.2" mathsize="90%" xref="S2.SS2.p1.4.m4.1.1.3.2.cmml">p</mi><mi id="S2.SS2.p1.4.m4.1.1.3.3" mathsize="90%" xref="S2.SS2.p1.4.m4.1.1.3.3.cmml">d</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><csymbol cd="latexml" id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1.1">similar-to</csymbol><ci id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">ğ‘¥</ci><apply id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.1.3.1.cmml" xref="S2.SS2.p1.4.m4.1.1.3">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.1.3.2.cmml" xref="S2.SS2.p1.4.m4.1.1.3.2">ğ‘</ci><ci id="S2.SS2.p1.4.m4.1.1.3.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">x\sim p_{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">italic_x âˆ¼ italic_p start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.8" style="font-size:90%;">, and a value close to 0 for </span><span class="ltx_text ltx_font_italic" id="S2.SS2.p1.11.9" style="font-size:90%;">â€œ</span><span class="ltx_text" id="S2.SS2.p1.11.10" style="font-size:90%;">fake</span><span class="ltx_text ltx_font_italic" id="S2.SS2.p1.11.11" style="font-size:90%;">â€</span><span class="ltx_text" id="S2.SS2.p1.11.12" style="font-size:90%;"> samples </span><math alttext="G({z})" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m5.1"><semantics id="S2.SS2.p1.5.m5.1a"><mrow id="S2.SS2.p1.5.m5.1.2" xref="S2.SS2.p1.5.m5.1.2.cmml"><mi id="S2.SS2.p1.5.m5.1.2.2" mathsize="90%" xref="S2.SS2.p1.5.m5.1.2.2.cmml">G</mi><mo id="S2.SS2.p1.5.m5.1.2.1" xref="S2.SS2.p1.5.m5.1.2.1.cmml">â¢</mo><mrow id="S2.SS2.p1.5.m5.1.2.3.2" xref="S2.SS2.p1.5.m5.1.2.cmml"><mo id="S2.SS2.p1.5.m5.1.2.3.2.1" maxsize="90%" minsize="90%" xref="S2.SS2.p1.5.m5.1.2.cmml">(</mo><mi id="S2.SS2.p1.5.m5.1.1" mathsize="90%" xref="S2.SS2.p1.5.m5.1.1.cmml">z</mi><mo id="S2.SS2.p1.5.m5.1.2.3.2.2" maxsize="90%" minsize="90%" xref="S2.SS2.p1.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.2.cmml" xref="S2.SS2.p1.5.m5.1.2"><times id="S2.SS2.p1.5.m5.1.2.1.cmml" xref="S2.SS2.p1.5.m5.1.2.1"></times><ci id="S2.SS2.p1.5.m5.1.2.2.cmml" xref="S2.SS2.p1.5.m5.1.2.2">ğº</ci><ci id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">G({z})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.5.m5.1d">italic_G ( italic_z )</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.13" style="font-size:90%;"> generated by the generator </span><math alttext="G" class="ltx_Math" display="inline" id="S2.SS2.p1.6.m6.1"><semantics id="S2.SS2.p1.6.m6.1a"><mi id="S2.SS2.p1.6.m6.1.1" mathsize="90%" xref="S2.SS2.p1.6.m6.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><ci id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">G</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.6.m6.1d">italic_G</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.14" style="font-size:90%;">, whose input </span><math alttext="z" class="ltx_Math" display="inline" id="S2.SS2.p1.7.m7.1"><semantics id="S2.SS2.p1.7.m7.1a"><mi id="S2.SS2.p1.7.m7.1.1" mathsize="90%" xref="S2.SS2.p1.7.m7.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><ci id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">z</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.7.m7.1d">italic_z</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.15" style="font-size:90%;"> is sampled from a prior distribution </span><math alttext="p_{z}" class="ltx_Math" display="inline" id="S2.SS2.p1.8.m8.1"><semantics id="S2.SS2.p1.8.m8.1a"><msub id="S2.SS2.p1.8.m8.1.1" xref="S2.SS2.p1.8.m8.1.1.cmml"><mi id="S2.SS2.p1.8.m8.1.1.2" mathsize="90%" xref="S2.SS2.p1.8.m8.1.1.2.cmml">p</mi><mi id="S2.SS2.p1.8.m8.1.1.3" mathsize="90%" xref="S2.SS2.p1.8.m8.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m8.1b"><apply id="S2.SS2.p1.8.m8.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.8.m8.1.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.p1.8.m8.1.1.2.cmml" xref="S2.SS2.p1.8.m8.1.1.2">ğ‘</ci><ci id="S2.SS2.p1.8.m8.1.1.3.cmml" xref="S2.SS2.p1.8.m8.1.1.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m8.1c">p_{z}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.8.m8.1d">italic_p start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.16" style="font-size:90%;">. On the other hand, the generator seeks to deceive the discriminator by generating samples that are indistinguishable from real ones.
The two-player minimax game with the value function </span><math alttext="V(G,D)" class="ltx_Math" display="inline" id="S2.SS2.p1.9.m9.2"><semantics id="S2.SS2.p1.9.m9.2a"><mrow id="S2.SS2.p1.9.m9.2.3" xref="S2.SS2.p1.9.m9.2.3.cmml"><mi id="S2.SS2.p1.9.m9.2.3.2" mathsize="90%" xref="S2.SS2.p1.9.m9.2.3.2.cmml">V</mi><mo id="S2.SS2.p1.9.m9.2.3.1" xref="S2.SS2.p1.9.m9.2.3.1.cmml">â¢</mo><mrow id="S2.SS2.p1.9.m9.2.3.3.2" xref="S2.SS2.p1.9.m9.2.3.3.1.cmml"><mo id="S2.SS2.p1.9.m9.2.3.3.2.1" maxsize="90%" minsize="90%" xref="S2.SS2.p1.9.m9.2.3.3.1.cmml">(</mo><mi id="S2.SS2.p1.9.m9.1.1" mathsize="90%" xref="S2.SS2.p1.9.m9.1.1.cmml">G</mi><mo id="S2.SS2.p1.9.m9.2.3.3.2.2" mathsize="90%" xref="S2.SS2.p1.9.m9.2.3.3.1.cmml">,</mo><mi id="S2.SS2.p1.9.m9.2.2" mathsize="90%" xref="S2.SS2.p1.9.m9.2.2.cmml">D</mi><mo id="S2.SS2.p1.9.m9.2.3.3.2.3" maxsize="90%" minsize="90%" xref="S2.SS2.p1.9.m9.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.9.m9.2b"><apply id="S2.SS2.p1.9.m9.2.3.cmml" xref="S2.SS2.p1.9.m9.2.3"><times id="S2.SS2.p1.9.m9.2.3.1.cmml" xref="S2.SS2.p1.9.m9.2.3.1"></times><ci id="S2.SS2.p1.9.m9.2.3.2.cmml" xref="S2.SS2.p1.9.m9.2.3.2">ğ‘‰</ci><interval closure="open" id="S2.SS2.p1.9.m9.2.3.3.1.cmml" xref="S2.SS2.p1.9.m9.2.3.3.2"><ci id="S2.SS2.p1.9.m9.1.1.cmml" xref="S2.SS2.p1.9.m9.1.1">ğº</ci><ci id="S2.SS2.p1.9.m9.2.2.cmml" xref="S2.SS2.p1.9.m9.2.2">ğ·</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.9.m9.2c">V(G,D)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.9.m9.2d">italic_V ( italic_G , italic_D )</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.17" style="font-size:90%;"> is defined as follows, updating </span><math alttext="G" class="ltx_Math" display="inline" id="S2.SS2.p1.10.m10.1"><semantics id="S2.SS2.p1.10.m10.1a"><mi id="S2.SS2.p1.10.m10.1.1" mathsize="90%" xref="S2.SS2.p1.10.m10.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.10.m10.1b"><ci id="S2.SS2.p1.10.m10.1.1.cmml" xref="S2.SS2.p1.10.m10.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.10.m10.1c">G</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.10.m10.1d">italic_G</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.18" style="font-size:90%;"> and </span><math alttext="D" class="ltx_Math" display="inline" id="S2.SS2.p1.11.m11.1"><semantics id="S2.SS2.p1.11.m11.1a"><mi id="S2.SS2.p1.11.m11.1.1" mathsize="90%" xref="S2.SS2.p1.11.m11.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.11.m11.1b"><ci id="S2.SS2.p1.11.m11.1.1.cmml" xref="S2.SS2.p1.11.m11.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.11.m11.1c">D</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.11.m11.1d">italic_D</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p1.11.19" style="font-size:90%;"> iteratively as the training unfolds,</span></p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<table class="ltx_equationgroup ltx_eqn_table" id="S2.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\min_{G}\max_{D}V(D,G)=\mathbb{E}_{\boldsymbol{x}\sim p_{\text{d %
}}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+" class="ltx_Math" display="inline" id="S2.E1X.2.1.1.m1.5"><semantics id="S2.E1X.2.1.1.m1.5a"><mrow id="S2.E1X.2.1.1.m1.5.5" xref="S2.E1X.2.1.1.m1.5.5.cmml"><mrow id="S2.E1X.2.1.1.m1.5.5.3" xref="S2.E1X.2.1.1.m1.5.5.3.cmml"><mrow id="S2.E1X.2.1.1.m1.5.5.3.2" xref="S2.E1X.2.1.1.m1.5.5.3.2.cmml"><munder id="S2.E1X.2.1.1.m1.5.5.3.2.1" xref="S2.E1X.2.1.1.m1.5.5.3.2.1.cmml"><mi id="S2.E1X.2.1.1.m1.5.5.3.2.1.2" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.3.2.1.2.cmml">min</mi><mi id="S2.E1X.2.1.1.m1.5.5.3.2.1.3" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.3.2.1.3.cmml">G</mi></munder><mo id="S2.E1X.2.1.1.m1.5.5.3.2a" lspace="0.167em" xref="S2.E1X.2.1.1.m1.5.5.3.2.cmml">â¡</mo><mrow id="S2.E1X.2.1.1.m1.5.5.3.2.2" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.cmml"><munder id="S2.E1X.2.1.1.m1.5.5.3.2.2.1" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.1.cmml"><mi id="S2.E1X.2.1.1.m1.5.5.3.2.2.1.2" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.1.2.cmml">max</mi><mi id="S2.E1X.2.1.1.m1.5.5.3.2.2.1.3" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.1.3.cmml">D</mi></munder><mo id="S2.E1X.2.1.1.m1.5.5.3.2.2a" lspace="0.167em" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.cmml">â¡</mo><mi id="S2.E1X.2.1.1.m1.5.5.3.2.2.2" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.2.cmml">V</mi></mrow></mrow><mo id="S2.E1X.2.1.1.m1.5.5.3.1" xref="S2.E1X.2.1.1.m1.5.5.3.1.cmml">â¢</mo><mrow id="S2.E1X.2.1.1.m1.5.5.3.3.2" xref="S2.E1X.2.1.1.m1.5.5.3.3.1.cmml"><mo id="S2.E1X.2.1.1.m1.5.5.3.3.2.1" maxsize="90%" minsize="90%" xref="S2.E1X.2.1.1.m1.5.5.3.3.1.cmml">(</mo><mi id="S2.E1X.2.1.1.m1.2.2" mathsize="90%" xref="S2.E1X.2.1.1.m1.2.2.cmml">D</mi><mo id="S2.E1X.2.1.1.m1.5.5.3.3.2.2" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.3.3.1.cmml">,</mo><mi id="S2.E1X.2.1.1.m1.3.3" mathsize="90%" xref="S2.E1X.2.1.1.m1.3.3.cmml">G</mi><mo id="S2.E1X.2.1.1.m1.5.5.3.3.2.3" maxsize="90%" minsize="90%" xref="S2.E1X.2.1.1.m1.5.5.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E1X.2.1.1.m1.5.5.2" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.2.cmml">=</mo><mrow id="S2.E1X.2.1.1.m1.5.5.1" xref="S2.E1X.2.1.1.m1.5.5.1.cmml"><mrow id="S2.E1X.2.1.1.m1.5.5.1.1" xref="S2.E1X.2.1.1.m1.5.5.1.1.cmml"><msub id="S2.E1X.2.1.1.m1.5.5.1.1.3" xref="S2.E1X.2.1.1.m1.5.5.1.1.3.cmml"><mi id="S2.E1X.2.1.1.m1.5.5.1.1.3.2" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.1.1.3.2.cmml">ğ”¼</mi><mrow id="S2.E1X.2.1.1.m1.1.1.1" xref="S2.E1X.2.1.1.m1.1.1.1.cmml"><mi id="S2.E1X.2.1.1.m1.1.1.1.3" mathsize="90%" xref="S2.E1X.2.1.1.m1.1.1.1.3.cmml">ğ’™</mi><mo id="S2.E1X.2.1.1.m1.1.1.1.2" mathsize="90%" xref="S2.E1X.2.1.1.m1.1.1.1.2.cmml">âˆ¼</mo><mrow id="S2.E1X.2.1.1.m1.1.1.1.4" xref="S2.E1X.2.1.1.m1.1.1.1.4.cmml"><msub id="S2.E1X.2.1.1.m1.1.1.1.4.2" xref="S2.E1X.2.1.1.m1.1.1.1.4.2.cmml"><mi id="S2.E1X.2.1.1.m1.1.1.1.4.2.2" mathsize="90%" xref="S2.E1X.2.1.1.m1.1.1.1.4.2.2.cmml">p</mi><mtext id="S2.E1X.2.1.1.m1.1.1.1.4.2.3" mathsize="90%" xref="S2.E1X.2.1.1.m1.1.1.1.4.2.3a.cmml">dÂ </mtext></msub><mo id="S2.E1X.2.1.1.m1.1.1.1.4.1" xref="S2.E1X.2.1.1.m1.1.1.1.4.1.cmml">â¢</mo><mrow id="S2.E1X.2.1.1.m1.1.1.1.4.3.2" xref="S2.E1X.2.1.1.m1.1.1.1.4.cmml"><mo id="S2.E1X.2.1.1.m1.1.1.1.4.3.2.1" maxsize="90%" minsize="90%" xref="S2.E1X.2.1.1.m1.1.1.1.4.cmml">(</mo><mi id="S2.E1X.2.1.1.m1.1.1.1.1" mathsize="90%" xref="S2.E1X.2.1.1.m1.1.1.1.1.cmml">ğ’™</mi><mo id="S2.E1X.2.1.1.m1.1.1.1.4.3.2.2" maxsize="90%" minsize="90%" xref="S2.E1X.2.1.1.m1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mo id="S2.E1X.2.1.1.m1.5.5.1.1.2" xref="S2.E1X.2.1.1.m1.5.5.1.1.2.cmml">â¢</mo><mrow id="S2.E1X.2.1.1.m1.5.5.1.1.1.1" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.2.cmml"><mo id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.2.1.cmml">[</mo><mrow id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.cmml"><mrow id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.cmml"><mi id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.1" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.1.cmml">log</mi><mo id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2a" lspace="0.167em" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.cmml">â¡</mo><mi id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.2" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.2.cmml">D</mi></mrow><mo id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.1" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.1.cmml">â¢</mo><mrow id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.3.2" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.cmml"><mo id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.3.2.1" maxsize="90%" minsize="90%" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.cmml">(</mo><mi id="S2.E1X.2.1.1.m1.4.4" mathsize="90%" xref="S2.E1X.2.1.1.m1.4.4.cmml">ğ’™</mi><mo id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.3.2.2" maxsize="90%" minsize="90%" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S2.E1X.2.1.1.m1.5.5.1.3" mathsize="90%" xref="S2.E1X.2.1.1.m1.5.5.1.3.cmml">+</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1X.2.1.1.m1.5b"><apply id="S2.E1X.2.1.1.m1.5.5.cmml" xref="S2.E1X.2.1.1.m1.5.5"><eq id="S2.E1X.2.1.1.m1.5.5.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.2"></eq><apply id="S2.E1X.2.1.1.m1.5.5.3.cmml" xref="S2.E1X.2.1.1.m1.5.5.3"><times id="S2.E1X.2.1.1.m1.5.5.3.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.1"></times><apply id="S2.E1X.2.1.1.m1.5.5.3.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2"><apply id="S2.E1X.2.1.1.m1.5.5.3.2.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2.1"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.5.5.3.2.1.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2.1">subscript</csymbol><min id="S2.E1X.2.1.1.m1.5.5.3.2.1.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2.1.2"></min><ci id="S2.E1X.2.1.1.m1.5.5.3.2.1.3.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2.1.3">ğº</ci></apply><apply id="S2.E1X.2.1.1.m1.5.5.3.2.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2.2"><apply id="S2.E1X.2.1.1.m1.5.5.3.2.2.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.1"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.5.5.3.2.2.1.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.1">subscript</csymbol><max id="S2.E1X.2.1.1.m1.5.5.3.2.2.1.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.1.2"></max><ci id="S2.E1X.2.1.1.m1.5.5.3.2.2.1.3.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.1.3">ğ·</ci></apply><ci id="S2.E1X.2.1.1.m1.5.5.3.2.2.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.2.2.2">ğ‘‰</ci></apply></apply><interval closure="open" id="S2.E1X.2.1.1.m1.5.5.3.3.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.3.3.2"><ci id="S2.E1X.2.1.1.m1.2.2.cmml" xref="S2.E1X.2.1.1.m1.2.2">ğ·</ci><ci id="S2.E1X.2.1.1.m1.3.3.cmml" xref="S2.E1X.2.1.1.m1.3.3">ğº</ci></interval></apply><apply id="S2.E1X.2.1.1.m1.5.5.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.1"><csymbol cd="latexml" id="S2.E1X.2.1.1.m1.5.5.1.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.1">limit-from</csymbol><apply id="S2.E1X.2.1.1.m1.5.5.1.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1"><times id="S2.E1X.2.1.1.m1.5.5.1.1.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.2"></times><apply id="S2.E1X.2.1.1.m1.5.5.1.1.3.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.3"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.5.5.1.1.3.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.3">subscript</csymbol><ci id="S2.E1X.2.1.1.m1.5.5.1.1.3.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.3.2">ğ”¼</ci><apply id="S2.E1X.2.1.1.m1.1.1.1.cmml" xref="S2.E1X.2.1.1.m1.1.1.1"><csymbol cd="latexml" id="S2.E1X.2.1.1.m1.1.1.1.2.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.2">similar-to</csymbol><ci id="S2.E1X.2.1.1.m1.1.1.1.3.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.3">ğ’™</ci><apply id="S2.E1X.2.1.1.m1.1.1.1.4.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.4"><times id="S2.E1X.2.1.1.m1.1.1.1.4.1.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.4.1"></times><apply id="S2.E1X.2.1.1.m1.1.1.1.4.2.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.4.2"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.1.1.1.4.2.1.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.4.2">subscript</csymbol><ci id="S2.E1X.2.1.1.m1.1.1.1.4.2.2.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.4.2.2">ğ‘</ci><ci id="S2.E1X.2.1.1.m1.1.1.1.4.2.3a.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.4.2.3"><mtext id="S2.E1X.2.1.1.m1.1.1.1.4.2.3.cmml" mathsize="45%" xref="S2.E1X.2.1.1.m1.1.1.1.4.2.3">dÂ </mtext></ci></apply><ci id="S2.E1X.2.1.1.m1.1.1.1.1.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.1">ğ’™</ci></apply></apply></apply><apply id="S2.E1X.2.1.1.m1.5.5.1.1.1.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1"><csymbol cd="latexml" id="S2.E1X.2.1.1.m1.5.5.1.1.1.2.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1"><times id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.1"></times><apply id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2"><log id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.1.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.1"></log><ci id="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.2.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.1.1.1.1.2.2">ğ·</ci></apply><ci id="S2.E1X.2.1.1.m1.4.4.cmml" xref="S2.E1X.2.1.1.m1.4.4">ğ’™</ci></apply></apply></apply><plus id="S2.E1X.2.1.1.m1.5.5.1.3.cmml" xref="S2.E1X.2.1.1.m1.5.5.1.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1X.2.1.1.m1.5c">\displaystyle\min_{G}\max_{D}V(D,G)=\mathbb{E}_{\boldsymbol{x}\sim p_{\text{d %
}}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+</annotation><annotation encoding="application/x-llamapun" id="S2.E1X.2.1.1.m1.5d">roman_min start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT italic_V ( italic_D , italic_G ) = blackboard_E start_POSTSUBSCRIPT bold_italic_x âˆ¼ italic_p start_POSTSUBSCRIPT d end_POSTSUBSCRIPT ( bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_D ( bold_italic_x ) ] +</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E1Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{\boldsymbol{z}\sim p_{\boldsymbol{z}}(\boldsymbol{z})%
}[\log(1-D(G(\boldsymbol{z})))]" class="ltx_Math" display="inline" id="S2.E1Xa.2.1.1.m1.4"><semantics id="S2.E1Xa.2.1.1.m1.4a"><mrow id="S2.E1Xa.2.1.1.m1.4.4" xref="S2.E1Xa.2.1.1.m1.4.4.cmml"><msub id="S2.E1Xa.2.1.1.m1.4.4.3" xref="S2.E1Xa.2.1.1.m1.4.4.3.cmml"><mi id="S2.E1Xa.2.1.1.m1.4.4.3.2" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.3.2.cmml">ğ”¼</mi><mrow id="S2.E1Xa.2.1.1.m1.1.1.1" xref="S2.E1Xa.2.1.1.m1.1.1.1.cmml"><mi id="S2.E1Xa.2.1.1.m1.1.1.1.3" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.1.1.1.3.cmml">ğ’›</mi><mo id="S2.E1Xa.2.1.1.m1.1.1.1.2" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.1.1.1.2.cmml">âˆ¼</mo><mrow id="S2.E1Xa.2.1.1.m1.1.1.1.4" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.cmml"><msub id="S2.E1Xa.2.1.1.m1.1.1.1.4.2" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.2.cmml"><mi id="S2.E1Xa.2.1.1.m1.1.1.1.4.2.2" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.2.2.cmml">p</mi><mi id="S2.E1Xa.2.1.1.m1.1.1.1.4.2.3" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.2.3.cmml">ğ’›</mi></msub><mo id="S2.E1Xa.2.1.1.m1.1.1.1.4.1" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.1.cmml">â¢</mo><mrow id="S2.E1Xa.2.1.1.m1.1.1.1.4.3.2" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.cmml"><mo id="S2.E1Xa.2.1.1.m1.1.1.1.4.3.2.1" maxsize="90%" minsize="90%" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.cmml">(</mo><mi id="S2.E1Xa.2.1.1.m1.1.1.1.1" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.1.1.1.1.cmml">ğ’›</mi><mo id="S2.E1Xa.2.1.1.m1.1.1.1.4.3.2.2" maxsize="90%" minsize="90%" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mo id="S2.E1Xa.2.1.1.m1.4.4.2" xref="S2.E1Xa.2.1.1.m1.4.4.2.cmml">â¢</mo><mrow id="S2.E1Xa.2.1.1.m1.4.4.1.1" xref="S2.E1Xa.2.1.1.m1.4.4.1.2.cmml"><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.2" maxsize="90%" minsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.2.1.cmml">[</mo><mrow id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.2.cmml"><mi id="S2.E1Xa.2.1.1.m1.3.3" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.3.3.cmml">log</mi><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1a" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.2.cmml">â¡</mo><mrow id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.2.cmml"><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.2.cmml">(</mo><mrow id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml"><mn id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3.cmml">1</mn><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.cmml"><mi id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.3" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.3.cmml">D</mi><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.2" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml">G</mi><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mrow id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.2.1" maxsize="90%" minsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S2.E1Xa.2.1.1.m1.2.2" mathsize="90%" xref="S2.E1Xa.2.1.1.m1.2.2.cmml">ğ’›</mi><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.2.2" maxsize="90%" minsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E1Xa.2.1.1.m1.4.4.1.1.3" maxsize="90%" minsize="90%" xref="S2.E1Xa.2.1.1.m1.4.4.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1Xa.2.1.1.m1.4b"><apply id="S2.E1Xa.2.1.1.m1.4.4.cmml" xref="S2.E1Xa.2.1.1.m1.4.4"><times id="S2.E1Xa.2.1.1.m1.4.4.2.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.2"></times><apply id="S2.E1Xa.2.1.1.m1.4.4.3.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.3"><csymbol cd="ambiguous" id="S2.E1Xa.2.1.1.m1.4.4.3.1.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.3">subscript</csymbol><ci id="S2.E1Xa.2.1.1.m1.4.4.3.2.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.3.2">ğ”¼</ci><apply id="S2.E1Xa.2.1.1.m1.1.1.1.cmml" xref="S2.E1Xa.2.1.1.m1.1.1.1"><csymbol cd="latexml" id="S2.E1Xa.2.1.1.m1.1.1.1.2.cmml" xref="S2.E1Xa.2.1.1.m1.1.1.1.2">similar-to</csymbol><ci id="S2.E1Xa.2.1.1.m1.1.1.1.3.cmml" xref="S2.E1Xa.2.1.1.m1.1.1.1.3">ğ’›</ci><apply id="S2.E1Xa.2.1.1.m1.1.1.1.4.cmml" xref="S2.E1Xa.2.1.1.m1.1.1.1.4"><times id="S2.E1Xa.2.1.1.m1.1.1.1.4.1.cmml" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.1"></times><apply id="S2.E1Xa.2.1.1.m1.1.1.1.4.2.cmml" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.2"><csymbol cd="ambiguous" id="S2.E1Xa.2.1.1.m1.1.1.1.4.2.1.cmml" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.2">subscript</csymbol><ci id="S2.E1Xa.2.1.1.m1.1.1.1.4.2.2.cmml" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.2.2">ğ‘</ci><ci id="S2.E1Xa.2.1.1.m1.1.1.1.4.2.3.cmml" xref="S2.E1Xa.2.1.1.m1.1.1.1.4.2.3">ğ’›</ci></apply><ci id="S2.E1Xa.2.1.1.m1.1.1.1.1.cmml" xref="S2.E1Xa.2.1.1.m1.1.1.1.1">ğ’›</ci></apply></apply></apply><apply id="S2.E1Xa.2.1.1.m1.4.4.1.2.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1"><csymbol cd="latexml" id="S2.E1Xa.2.1.1.m1.4.4.1.2.1.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.2">delimited-[]</csymbol><apply id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.2.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1"><log id="S2.E1Xa.2.1.1.m1.3.3.cmml" xref="S2.E1Xa.2.1.1.m1.3.3"></log><apply id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1"><minus id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2"></minus><cn id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3">1</cn><apply id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1"><times id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.2"></times><ci id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.3">ğ·</ci><apply id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1"><times id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2">ğº</ci><ci id="S2.E1Xa.2.1.1.m1.2.2.cmml" xref="S2.E1Xa.2.1.1.m1.2.2">ğ’›</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1Xa.2.1.1.m1.4c">\displaystyle\mathbb{E}_{\boldsymbol{z}\sim p_{\boldsymbol{z}}(\boldsymbol{z})%
}[\log(1-D(G(\boldsymbol{z})))]</annotation><annotation encoding="application/x-llamapun" id="S2.E1Xa.2.1.1.m1.4d">blackboard_E start_POSTSUBSCRIPT bold_italic_z âˆ¼ italic_p start_POSTSUBSCRIPT bold_italic_z end_POSTSUBSCRIPT ( bold_italic_z ) end_POSTSUBSCRIPT [ roman_log ( 1 - italic_D ( italic_G ( bold_italic_z ) ) ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.2"><span class="ltx_text" id="S2.SS2.p3.2.1" style="font-size:90%;">For VA modeling, it is the generator </span><math alttext="G" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.1"><semantics id="S2.SS2.p3.1.m1.1a"><mi id="S2.SS2.p3.1.m1.1.1" mathsize="90%" xref="S2.SS2.p3.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d">italic_G</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p3.2.2" style="font-size:90%;"> that performs the clean-to-rendered transformation during both the training and inference stages. The discriminator </span><math alttext="D" class="ltx_Math" display="inline" id="S2.SS2.p3.2.m2.1"><semantics id="S2.SS2.p3.2.m2.1a"><mi id="S2.SS2.p3.2.m2.1.1" mathsize="90%" xref="S2.SS2.p3.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><ci id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.2.m2.1d">italic_D</annotation></semantics></math><span class="ltx_text" id="S2.SS2.p3.2.3" style="font-size:90%;"> only functions during the training stage, guiding how the generator is optimized. Therefore, it is possible to use a computationally heavy discriminator to train a light generator, for better run-time efficiency of the generator.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Backbone Model for Generator</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1"><span class="ltx_text" id="S2.SS3.p1.1.1" style="font-size:90%;">Existing approaches to neural VA modeling can be categorized into two main types: convolutional (CNN) and recurrent neural networks (RNN).
From a digital signal processing viewpoint, CNN networks can be viewed as finite impulse response (FIR) filters.
CNN-based models have demonstrated superior performance in modeling various devices. For example, Wright </span><em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="S2.SS3.p1.1.3" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS3.p1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib1" title="">1</a><span class="ltx_text" id="S2.SS3.p1.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS3.p1.1.6" style="font-size:90%;"> applied a WaveNet modelÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS3.p1.1.7.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib22" title="">22</a><span class="ltx_text" id="S2.SS3.p1.1.8.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS3.p1.1.9" style="font-size:90%;"> to model the Blackstar HT-5 Metal and the Mesa Boogie 5:50 Plus amplifiers.
DamskÃ¤gg </span><em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.10" style="font-size:90%;">et al.</em><span class="ltx_text" id="S2.SS3.p1.1.11" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS3.p1.1.12.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib2" title="">2</a><span class="ltx_text" id="S2.SS3.p1.1.13.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS3.p1.1.14" style="font-size:90%;"> utilized a WaveNet model with conditioning control on the gain parameter to emulate the Fender Bassman 56F-A vacuum-tube amplifier.
In addition to amplifier modeling tasks, Steinmetz </span><em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.15" style="font-size:90%;">et al.</em><span class="ltx_text" id="S2.SS3.p1.1.16" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS3.p1.1.17.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib3" title="">3</a><span class="ltx_text" id="S2.SS3.p1.1.18.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS3.p1.1.19" style="font-size:90%;"> trained a conditional temporal convolutional network on compressor, analog delay, guitar amplifier, and reverberation effects.</span></p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><span class="ltx_text" id="S2.SS3.p2.1.1" style="font-size:90%;">On the other hand, RNN-based approaches often rely on long-short term memory (LSTM) or gated recurrent units (GRU). For instance, Wright </span><em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="S2.SS3.p2.1.3" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS3.p2.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib4" title="">4</a><span class="ltx_text" id="S2.SS3.p2.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS3.p2.1.6" style="font-size:90%;"> showed promising results using a recurrent-based model to model a high-gain channel Blackstar HT-1 vacuum tube amplifier and an Electro-Harmonix Big Muff Pi distortion/fuzz pedal. Juvela </span><em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.7" style="font-size:90%;">et al.</em><span class="ltx_text" id="S2.SS3.p2.1.8" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS3.p2.1.9.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib5" title="">5</a><span class="ltx_text" id="S2.SS3.p2.1.10.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS3.p2.1.11" style="font-size:90%;"> extended their work further by concatenating control parameters with a range of [0,1] as additional input channels to their LSTM network.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Discriminators for GANs training</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1"><span class="ltx_text" id="S2.SS4.p1.1.1" style="font-size:90%;">To apply adversarial losses within the GAN framework, a discriminator is needed to distinguish between real data and generated output.
</span><span class="ltx_text" id="S2.SS4.p1.1.2" style="font-size:90%;color:#000000;">
Several discriminators have been proposed for audio generative tasks such as neural vocoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib13" title="">13</a>]</cite>, voice conversion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib11" title="">11</a>]</cite>, and neural codec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib23" title="">23</a>]</cite>.
We categorize these discriminators into two types: spectral-based and waveform-based discriminators.</span><span class="ltx_text" id="S2.SS4.p1.1.3" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1"><span class="ltx_text" id="S2.SS4.p2.1.1" style="font-size:90%;">For spectral-based discriminators, DÃ©fossez </span><em class="ltx_emph ltx_font_italic" id="S2.SS4.p2.1.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="S2.SS4.p2.1.3" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS4.p2.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib23" title="">23</a><span class="ltx_text" id="S2.SS4.p2.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS4.p2.1.6" style="font-size:90%;"> proposed a a multi-scale STFT-based discriminator. They computed and summed the short-time Fourier transform (STFT) losses with different parameters (i.e., FFT size, window size, and hop length). These techniques compel the generator to not only focus on generating the waveform itself but also to generate reasonable results in the spectral domain.
On the other hand in MelGAN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS4.p2.1.7.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib8" title="">8</a><span class="ltx_text" id="S2.SS4.p2.1.8.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS4.p2.1.9" style="font-size:90%;">, a multi-scale discriminator (MSD) was introduced, operating on different audio scales (i.e., sample rates) </span><span class="ltx_text" id="S2.SS4.p2.1.10" style="font-size:90%;color:#000000;">in waveform domain</span><span class="ltx_text" id="S2.SS4.p2.1.11" style="font-size:90%;">. Each scaleâ€™s audio is processed by a 1-D convolution-based module to obtain an output. The outputs from each scale are then used to calculate adversarial losses for training the discriminator.
</span><span class="ltx_text" id="S2.SS4.p2.1.12" style="font-size:90%;color:#000000;">
In Hifi-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib13" title="">13</a>]</cite>, a multi-period discriminator (MPD) was proposed to capture both regular and prime number distances between sample points, resulting in improved speech synthesis quality.</span><span class="ltx_text" id="S2.SS4.p2.1.13" style="font-size:90%;">
While the previous GAN-based VA modeling model </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS4.p2.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a><span class="ltx_text" id="S2.SS4.p2.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS4.p2.1.16" style="font-size:90%;"> uses MSD alone, we use MSD </span><span class="ltx_text" id="S2.SS4.p2.1.17" style="font-size:90%;color:#000000;">as well as</span><span class="ltx_text" id="S2.SS4.p2.1.18" style="font-size:90%;"> MPD in our work.</span></p>
</div>
<figure class="ltx_table" id="S2.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T1.2" style="width:277.9pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S2.T1.2.2"><span class="ltx_text" id="S2.T1.2.2.2" style="font-size:90%;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.2.2.2.2">
<span class="ltx_thead">
<span class="ltx_tr" id="S2.T1.2.2.2.2.3.1">
<span class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S2.T1.2.2.2.2.3.1.1"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.2.3.1.2">GuitarSet</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.2.3.1.3">EGDB</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.2.3.1.4">GUITAR-FX-DIST</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.2.3.1.5">EGFxSet</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S2.T1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1.1.2">Clean</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1.1.1.3">Â Â 3h</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1.1.1.4">Â Â 2h</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1.1.1.5">Â Â 0.57h</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1.1.1.1">Â Â <math alttext="\sim" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.1.1.m1.1d">âˆ¼</annotation></semantics></math>1h</span></span>
<span class="ltx_tr" id="S2.T1.2.2.2.2.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.2.2.2.2.2">Rendered</span>
<span class="ltx_td ltx_align_center" id="S2.T1.2.2.2.2.2.3">N/A</span>
<span class="ltx_td ltx_align_center" id="S2.T1.2.2.2.2.2.4">10h</span>
<span class="ltx_td ltx_align_center" id="S2.T1.2.2.2.2.2.1"><math alttext="\sim" class="ltx_Math" display="inline" id="S2.T1.2.2.2.2.2.1.m1.1"><semantics id="S2.T1.2.2.2.2.2.1.m1.1a"><mo id="S2.T1.2.2.2.2.2.1.m1.1.1" xref="S2.T1.2.2.2.2.2.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.2.1.m1.1b"><csymbol cd="latexml" id="S2.T1.2.2.2.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.2.2.2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.2.2.1.m1.1d">âˆ¼</annotation></semantics></math>111h</span>
<span class="ltx_td ltx_align_center" id="S2.T1.2.2.2.2.2.5">11.5h</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>The total duration (in hours, or â€˜hâ€™) of the clean, unprocessed audio and the rendered audio (with effects applied) of four existing public paired datasets, GuitarSetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib24" title="">24</a>]</cite>, EGDBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib15" title="">15</a>]</cite>, GUITAR-FX-DISTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib25" title="">25</a>]</cite>, and EGFxSetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib16" title="">16</a>]</cite>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Clean Audio from Existing Datasets</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1"><span class="ltx_text" id="S2.SS5.p1.1.1" style="font-size:90%;">We refer to a dataset as a </span><em class="ltx_emph ltx_font_italic" id="S2.SS5.p1.1.2" style="font-size:90%;">paired dataset</em><span class="ltx_text" id="S2.SS5.p1.1.3" style="font-size:90%;"> when it contains the clean audio signal counterpart for each amplifier- or pedal-rendered audio signal. Referring to existing public-domain paired datasets, we show a comparison of clean audio and rendered audio duration in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S2.T1" style="font-size:90%;" title="Table 1 â€£ 2.4 Discriminators for GANs training â€£ 2 Related Works â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S2.SS5.p1.1.4" style="font-size:90%;">. As the table shows, the duration of amplifier or pedal rendered data in each dataset is much longer than the aligned clean audio in overall duration.
We see that clean audio is relatively scarcer than rendered audio.</span></p>
</div>
<div class="ltx_para" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1"><span class="ltx_text" id="S2.SS5.p2.1.1" style="font-size:90%;">We categorize the clean audio into two types during training. First is </span><em class="ltx_emph ltx_font_italic" id="S2.SS5.p2.1.2" style="font-size:90%;">target-aligned</em><span class="ltx_text" id="S2.SS5.p2.1.3" style="font-size:90%;"> clean audio, where a target audio can always find an aligned clean audio with the same musical content. Second is </span><em class="ltx_emph ltx_font_italic" id="S2.SS5.p2.1.4" style="font-size:90%;">target-unaligned</em><span class="ltx_text" id="S2.SS5.p2.1.5" style="font-size:90%;"> clean audio, where the musical content in this type of clean audio does not exist in target amplifier-rendered audio.
</span><span class="ltx_text" id="S2.SS5.p2.1.6" style="font-size:90%;color:#000000;">Please note that the objective evaluation metrics such as
ESR and Mel-spectrum loss (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.SS2" title="4.2 Metrics â€£ 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">4.2</span></a>)
still requires data from a target-aligned setting.</span><span class="ltx_text" id="S2.SS5.p2.1.7" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para" id="S2.SS5.p3">
<p class="ltx_p" id="S2.SS5.p3.1"><span class="ltx_text" id="S2.SS5.p3.1.1" style="font-size:90%;">As mentioned in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S1" style="font-size:90%;" title="1 Introduction â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S2.SS5.p3.1.2" style="font-size:90%;">, </span><span class="ltx_text" id="S2.SS5.p3.1.3" style="font-size:90%;color:#000000;">even though the clean data and rendered data from the <em class="ltx_emph ltx_font_italic" id="S2.SS5.p3.1.3.1">same</em> dataset is usually fully aligned (i.e., they are target-aligned),</span><span class="ltx_text" id="S2.SS5.p3.1.4" style="font-size:90%;">
we can take advantage of the GAN-based approach and further use clean data and rendered data from </span><em class="ltx_emph ltx_font_italic" id="S2.SS5.p3.1.5" style="font-size:90%;">different</em><span class="ltx_text" id="S2.SS5.p3.1.6" style="font-size:90%;"> datasets and employ such target-unaligned data in our unsupervised training.
</span><span class="ltx_text" id="S2.SS5.p3.1.7" style="font-size:90%;color:#000000;">The prior work of Wright <em class="ltx_emph ltx_font_italic" id="S2.SS5.p3.1.7.1">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a>]</cite> did not exploit such a potential,
as they used clean audio from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib26" title="">26</a>]</cite> and created rendered target audio from three different plugins, essentially creating target-aligned data.
Unlike their work, we study the use of target-unaligned data in our experiments.</span><span class="ltx_text" id="S2.SS5.p3.1.8" style="font-size:90%;"></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.4"><span class="ltx_text" id="S3.p1.4.1" style="font-size:90%;">We consider guitar amplifier modeling as a generative task that aims to generate high-fidelity audio waveforms. Given an input audio of </span><math alttext="T" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" mathsize="90%" xref="S3.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_T</annotation></semantics></math><span class="ltx_text" id="S3.p1.4.2" style="font-size:90%;"> samples, </span><math alttext="\mathbf{x}\in\mathbb{R}^{1\times T}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mrow id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" mathsize="90%" xref="S3.p1.2.m2.1.1.2.cmml">ğ±</mi><mo id="S3.p1.2.m2.1.1.1" mathsize="90%" xref="S3.p1.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml"><mi id="S3.p1.2.m2.1.1.3.2" mathsize="90%" xref="S3.p1.2.m2.1.1.3.2.cmml">â„</mi><mrow id="S3.p1.2.m2.1.1.3.3" xref="S3.p1.2.m2.1.1.3.3.cmml"><mn id="S3.p1.2.m2.1.1.3.3.2" mathsize="90%" xref="S3.p1.2.m2.1.1.3.3.2.cmml">1</mn><mo id="S3.p1.2.m2.1.1.3.3.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.p1.2.m2.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.p1.2.m2.1.1.3.3.3" mathsize="90%" xref="S3.p1.2.m2.1.1.3.3.3.cmml">T</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><in id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1"></in><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ğ±</ci><apply id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.3.1.cmml" xref="S3.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.p1.2.m2.1.1.3.2.cmml" xref="S3.p1.2.m2.1.1.3.2">â„</ci><apply id="S3.p1.2.m2.1.1.3.3.cmml" xref="S3.p1.2.m2.1.1.3.3"><times id="S3.p1.2.m2.1.1.3.3.1.cmml" xref="S3.p1.2.m2.1.1.3.3.1"></times><cn id="S3.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="S3.p1.2.m2.1.1.3.3.2">1</cn><ci id="S3.p1.2.m2.1.1.3.3.3.cmml" xref="S3.p1.2.m2.1.1.3.3.3">ğ‘‡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathbf{x}\in\mathbb{R}^{1\times T}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">bold_x âˆˆ blackboard_R start_POSTSUPERSCRIPT 1 Ã— italic_T end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.p1.4.3" style="font-size:90%;">,
we adopt the â€œblack-boxâ€ approach and train a neural network-based generator </span><math alttext="G" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" mathsize="90%" xref="S3.p1.3.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_G</annotation></semantics></math><span class="ltx_text" id="S3.p1.4.4" style="font-size:90%;"> that carries out the amplifier modeling process and generates </span><math alttext="\hat{\mathbf{x}}=G(\mathbf{x})" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mrow id="S3.p1.4.m4.1.2" xref="S3.p1.4.m4.1.2.cmml"><mover accent="true" id="S3.p1.4.m4.1.2.2" xref="S3.p1.4.m4.1.2.2.cmml"><mi id="S3.p1.4.m4.1.2.2.2" mathsize="90%" xref="S3.p1.4.m4.1.2.2.2.cmml">ğ±</mi><mo id="S3.p1.4.m4.1.2.2.1" mathsize="90%" xref="S3.p1.4.m4.1.2.2.1.cmml">^</mo></mover><mo id="S3.p1.4.m4.1.2.1" mathsize="90%" xref="S3.p1.4.m4.1.2.1.cmml">=</mo><mrow id="S3.p1.4.m4.1.2.3" xref="S3.p1.4.m4.1.2.3.cmml"><mi id="S3.p1.4.m4.1.2.3.2" mathsize="90%" xref="S3.p1.4.m4.1.2.3.2.cmml">G</mi><mo id="S3.p1.4.m4.1.2.3.1" xref="S3.p1.4.m4.1.2.3.1.cmml">â¢</mo><mrow id="S3.p1.4.m4.1.2.3.3.2" xref="S3.p1.4.m4.1.2.3.cmml"><mo id="S3.p1.4.m4.1.2.3.3.2.1" maxsize="90%" minsize="90%" xref="S3.p1.4.m4.1.2.3.cmml">(</mo><mi id="S3.p1.4.m4.1.1" mathsize="90%" xref="S3.p1.4.m4.1.1.cmml">ğ±</mi><mo id="S3.p1.4.m4.1.2.3.3.2.2" maxsize="90%" minsize="90%" xref="S3.p1.4.m4.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.2.cmml" xref="S3.p1.4.m4.1.2"><eq id="S3.p1.4.m4.1.2.1.cmml" xref="S3.p1.4.m4.1.2.1"></eq><apply id="S3.p1.4.m4.1.2.2.cmml" xref="S3.p1.4.m4.1.2.2"><ci id="S3.p1.4.m4.1.2.2.1.cmml" xref="S3.p1.4.m4.1.2.2.1">^</ci><ci id="S3.p1.4.m4.1.2.2.2.cmml" xref="S3.p1.4.m4.1.2.2.2">ğ±</ci></apply><apply id="S3.p1.4.m4.1.2.3.cmml" xref="S3.p1.4.m4.1.2.3"><times id="S3.p1.4.m4.1.2.3.1.cmml" xref="S3.p1.4.m4.1.2.3.1"></times><ci id="S3.p1.4.m4.1.2.3.2.cmml" xref="S3.p1.4.m4.1.2.3.2">ğº</ci><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ğ±</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\hat{\mathbf{x}}=G(\mathbf{x})</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">over^ start_ARG bold_x end_ARG = italic_G ( bold_x )</annotation></semantics></math><span class="ltx_text" id="S3.p1.4.5" style="font-size:90%;">.
We illustrate our training framework in FigureÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S3.F1" style="font-size:90%;" title="Figure 1 â€£ 3 Methods â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S3.p1.4.6" style="font-size:90%;">.</span></p>
</div>
<figure class="ltx_figure ltx_align_center" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="194" id="S3.F1.1.g1" src="extracted/5669049/DAFx24_Templates_LaTeX/Image/model.png" width="687"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Diagram of the proposed GAN-based model for VA modeling, using clean audio that may not be matched and aligned with the target audio segment, and using two types of discriminators: MSD and MPDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib13" title="">13</a>]</cite>.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Generator</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text" id="S3.SS1.p1.1.1" style="font-size:90%;">We employ the same causal feed-forward WaveNet model architecture as Wright </span><em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="S3.SS1.p1.1.3" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS1.p1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a><span class="ltx_text" id="S3.SS1.p1.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS1.p1.1.6" style="font-size:90%;"> for our generator. It consists of two stacks of nine dilated convolution layers. The dilation is one at the first stack and is increased by a factor of two after each stack to get a larger receptive field. We set a kernel size as 3 to get a growth receptive field from small area to larger area. Each convolution layer is equipped with a weight normalization. We use the same gated activation function as the original WaveNet modelÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS1.p1.1.7.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib22" title="">22</a><span class="ltx_text" id="S3.SS1.p1.1.8.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS1.p1.1.9" style="font-size:90%;">.</span></p>
</div>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T2.1" style="width:274.8pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S3.T2.1.1"><span class="ltx_text" id="S3.T2.1.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T2.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.1.1.1.1.1.1">Model</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S3.T2.1.1.1.1.1.1.2">channels</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.1.1.1.1.1.3">kernel sizes</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.1.1.1.1.1.4">stride</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.1.1.1.1.1.5">groups</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.1.1.1.1.1.6">padding</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T2.1.1.1.1.2.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.1.1.1.1.2.1.1">conv1d</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.1.1.1.2.1.2">Â Â Â (1, 128)</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.1.1.2.1.3">15</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.1.1.2.1.4">1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.1.1.2.1.5">1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.1.1.2.1.6">0</span></span>
<span class="ltx_tr" id="S3.T2.1.1.1.1.3.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.1.1.3.2.1">conv1d</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.1.1.1.1.3.2.2">(128, 128)</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.3.2.3">41</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.3.2.4">2</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.3.2.5">4</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.3.2.6">20</span></span>
<span class="ltx_tr" id="S3.T2.1.1.1.1.4.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.1.1.4.3.1">conv1d</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.1.1.1.1.4.3.2">(128, 256)</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.4.3.3">41</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.4.3.4">2</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.4.3.5">16</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.4.3.6">20</span></span>
<span class="ltx_tr" id="S3.T2.1.1.1.1.5.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.1.1.5.4.1">conv1d</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.1.1.1.1.5.4.2">(256, 512)</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.5.4.3">41</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.5.4.4">4</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.5.4.5">16</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.5.4.6">20</span></span>
<span class="ltx_tr" id="S3.T2.1.1.1.1.6.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.1.1.6.5.1">conv1d</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.1.1.1.1.6.5.2">(512, 1024)</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.6.5.3">41</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.6.5.4">4</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.6.5.5">16</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.6.5.6">20</span></span>
<span class="ltx_tr" id="S3.T2.1.1.1.1.7.6">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.1.1.7.6.1">conv1d</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.1.1.1.1.7.6.2">(1024, 1024)</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.7.6.3">41</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.7.6.4">1</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.7.6.5">16</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.7.6.6">20</span></span>
<span class="ltx_tr" id="S3.T2.1.1.1.1.8.7">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.1.1.8.7.1">conv1d</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.1.1.1.1.8.7.2">(1024, 1024)</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.8.7.3">5</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.8.7.4">1</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.8.7.5">2</span>
<span class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.8.7.6">0</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Parameter settings of the convolutional layers of the implemented MSD sub-discriminators.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T3.1" style="width:274.8pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S3.T3.1.1"><span class="ltx_text" id="S3.T3.1.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T3.1.1.1.1.1.1.1">Model</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.1.1.1.1.1.1.2">channels</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.1.1.1.1.1.1.3">kernel sizes</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.1.1.1.1.1.1.4">stride</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.1.1.1.1.1.1.5">groups</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.1.1.1.1.1.1.6">padding</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T3.1.1.1.1.2.1">
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1.2.1.1">conv2d</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.2.1.2">(1, 32)</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.2.1.3">(5, 1)</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.2.1.4">(3,1)</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.2.1.5">1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.2.1.6">2</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.3.2">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.1.1.3.2.1">conv2d</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.3.2.2">(32,128)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.3.2.3">(5, 1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.3.2.4">(3,1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.3.2.5">1</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.3.2.6">2</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.4.3">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.1.1.4.3.1">conv2d</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.4.3.2">(128, 512)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.4.3.3">(5, 1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.4.3.4">(3,1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.4.3.5">1</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.4.3.6">2</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.5.4">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.1.1.5.4.1">conv2d</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.5.4.2">(512, 1024)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.5.4.3">(5, 1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.5.4.4">(3,1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.5.4.5">1</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.5.4.6">2</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.6.5">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.1.1.6.5.1">conv2d</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.6.5.2">(1024, 1024)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.6.5.3">(5, 1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.6.5.4">(1,1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.6.5.5">1</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.6.5.6">2</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.7.6">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.1.1.7.6.1">conv2d</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.7.6.2">(1024, 1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.7.6.3">(1,1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.7.6.4">(3,1)</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.7.6.5">1</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.7.6.6">2</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Parameter settings of the convolutional layers of the implemented MPD sub-discriminator.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Discriminator</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text" id="S3.SS2.p1.1.1" style="font-size:90%;">Our discriminator consists of both MSD- and MPD-based ones.
The MSD consists of three sub-discriminators originally </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib13" title="">13</a><span class="ltx_text" id="S3.SS2.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p1.1.4" style="font-size:90%;">.
</span><span class="ltx_text" id="S3.SS2.p1.1.5" style="font-size:90%;color:#000000;">However, we remove the last sub-discriminator that processes the audio after two downsampling layers (i.e., the one that operates at the lowest temporal resolution), as this gives better results empirically in our pilot study.</span><span class="ltx_text" id="S3.SS2.p1.1.6" style="font-size:90%;">
The input flow for MSD is therefore: raw audio, the first sub-discriminator, </span><math alttext="\times 4" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"></mi><mo id="S3.SS2.p1.1.m1.1.1.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.p1.1.m1.1.1.3" mathsize="90%" xref="S3.SS2.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">absent</csymbol><cn id="S3.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">Ã— 4</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p1.1.7" style="font-size:90%;"> average-pooled audio, and finally the second sub-discriminator. We set the parameters as shown in TableÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S3.T2" style="font-size:90%;" title="Table 2 â€£ 3.1 Generator â€£ 3 Methods â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S3.SS2.p1.1.8" style="font-size:90%;">. Following the setting of Hifi-GAN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p1.1.9.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib13" title="">13</a><span class="ltx_text" id="S3.SS2.p1.1.10.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p1.1.11" style="font-size:90%;">, spectral normalization is applied for the first sub-discriminator, while weight normalization is applied for the second one.</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.6"><span class="ltx_text" id="S3.SS2.p2.6.3" style="font-size:90%;">The MPD comprises a collection of mixture sub-discriminators.
Unlike MSD, it only accepts equally-spaced sample points of the input audios. With audio length </span><math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" mathsize="90%" xref="S3.SS2.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_T</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p2.6.4" style="font-size:90%;"> and period </span><math alttext="P" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" mathsize="90%" xref="S3.SS2.p2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_P</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p2.6.5" style="font-size:90%;">, input for each sub-discriminator will be reshaped from the audio length </span><math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" mathsize="90%" xref="S3.SS2.p2.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_T</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p2.6.6" style="font-size:90%;"> to </span><math alttext="(T/P,P)" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.2"><semantics id="S3.SS2.p2.4.m4.2a"><mrow id="S3.SS2.p2.4.m4.2.2.1" xref="S3.SS2.p2.4.m4.2.2.2.cmml"><mo id="S3.SS2.p2.4.m4.2.2.1.2" maxsize="90%" minsize="90%" xref="S3.SS2.p2.4.m4.2.2.2.cmml">(</mo><mrow id="S3.SS2.p2.4.m4.2.2.1.1" xref="S3.SS2.p2.4.m4.2.2.1.1.cmml"><mi id="S3.SS2.p2.4.m4.2.2.1.1.2" mathsize="90%" xref="S3.SS2.p2.4.m4.2.2.1.1.2.cmml">T</mi><mo id="S3.SS2.p2.4.m4.2.2.1.1.1" maxsize="90%" minsize="90%" stretchy="true" symmetric="true" xref="S3.SS2.p2.4.m4.2.2.1.1.1.cmml">/</mo><mi id="S3.SS2.p2.4.m4.2.2.1.1.3" mathsize="90%" xref="S3.SS2.p2.4.m4.2.2.1.1.3.cmml">P</mi></mrow><mo id="S3.SS2.p2.4.m4.2.2.1.3" mathsize="90%" xref="S3.SS2.p2.4.m4.2.2.2.cmml">,</mo><mi id="S3.SS2.p2.4.m4.1.1" mathsize="90%" xref="S3.SS2.p2.4.m4.1.1.cmml">P</mi><mo id="S3.SS2.p2.4.m4.2.2.1.4" maxsize="90%" minsize="90%" xref="S3.SS2.p2.4.m4.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.2b"><interval closure="open" id="S3.SS2.p2.4.m4.2.2.2.cmml" xref="S3.SS2.p2.4.m4.2.2.1"><apply id="S3.SS2.p2.4.m4.2.2.1.1.cmml" xref="S3.SS2.p2.4.m4.2.2.1.1"><divide id="S3.SS2.p2.4.m4.2.2.1.1.1.cmml" xref="S3.SS2.p2.4.m4.2.2.1.1.1"></divide><ci id="S3.SS2.p2.4.m4.2.2.1.1.2.cmml" xref="S3.SS2.p2.4.m4.2.2.1.1.2">ğ‘‡</ci><ci id="S3.SS2.p2.4.m4.2.2.1.1.3.cmml" xref="S3.SS2.p2.4.m4.2.2.1.1.3">ğ‘ƒ</ci></apply><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">ğ‘ƒ</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.2c">(T/P,P)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.2d">( italic_T / italic_P , italic_P )</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p2.6.7" style="font-size:90%;"> (i.e., from 1D to 2D).
</span><span class="ltx_text" id="S3.SS2.p2.6.2" style="font-size:90%;color:#000000;">Following the setting of Hifi-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib13" title="">13</a>]</cite>,
we employ multiple sub-discriminators, each operating with a period <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.p2.5.1.m1.1"><semantics id="S3.SS2.p2.5.1.m1.1a"><mi id="S3.SS2.p2.5.1.m1.1.1" mathcolor="#000000" xref="S3.SS2.p2.5.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.1.m1.1b"><ci id="S3.SS2.p2.5.1.m1.1.1.cmml" xref="S3.SS2.p2.5.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.1.m1.1d">italic_p</annotation></semantics></math> in <math alttext="P=[2,3,5,7,11]" class="ltx_Math" display="inline" id="S3.SS2.p2.6.2.m2.5"><semantics id="S3.SS2.p2.6.2.m2.5a"><mrow id="S3.SS2.p2.6.2.m2.5.6" xref="S3.SS2.p2.6.2.m2.5.6.cmml"><mi id="S3.SS2.p2.6.2.m2.5.6.2" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.5.6.2.cmml">P</mi><mo id="S3.SS2.p2.6.2.m2.5.6.1" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.5.6.1.cmml">=</mo><mrow id="S3.SS2.p2.6.2.m2.5.6.3.2" xref="S3.SS2.p2.6.2.m2.5.6.3.1.cmml"><mo id="S3.SS2.p2.6.2.m2.5.6.3.2.1" mathcolor="#000000" stretchy="false" xref="S3.SS2.p2.6.2.m2.5.6.3.1.cmml">[</mo><mn id="S3.SS2.p2.6.2.m2.1.1" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.1.1.cmml">2</mn><mo id="S3.SS2.p2.6.2.m2.5.6.3.2.2" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.5.6.3.1.cmml">,</mo><mn id="S3.SS2.p2.6.2.m2.2.2" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.2.2.cmml">3</mn><mo id="S3.SS2.p2.6.2.m2.5.6.3.2.3" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.5.6.3.1.cmml">,</mo><mn id="S3.SS2.p2.6.2.m2.3.3" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.3.3.cmml">5</mn><mo id="S3.SS2.p2.6.2.m2.5.6.3.2.4" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.5.6.3.1.cmml">,</mo><mn id="S3.SS2.p2.6.2.m2.4.4" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.4.4.cmml">7</mn><mo id="S3.SS2.p2.6.2.m2.5.6.3.2.5" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.5.6.3.1.cmml">,</mo><mn id="S3.SS2.p2.6.2.m2.5.5" mathcolor="#000000" xref="S3.SS2.p2.6.2.m2.5.5.cmml">11</mn><mo id="S3.SS2.p2.6.2.m2.5.6.3.2.6" mathcolor="#000000" stretchy="false" xref="S3.SS2.p2.6.2.m2.5.6.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.2.m2.5b"><apply id="S3.SS2.p2.6.2.m2.5.6.cmml" xref="S3.SS2.p2.6.2.m2.5.6"><eq id="S3.SS2.p2.6.2.m2.5.6.1.cmml" xref="S3.SS2.p2.6.2.m2.5.6.1"></eq><ci id="S3.SS2.p2.6.2.m2.5.6.2.cmml" xref="S3.SS2.p2.6.2.m2.5.6.2">ğ‘ƒ</ci><list id="S3.SS2.p2.6.2.m2.5.6.3.1.cmml" xref="S3.SS2.p2.6.2.m2.5.6.3.2"><cn id="S3.SS2.p2.6.2.m2.1.1.cmml" type="integer" xref="S3.SS2.p2.6.2.m2.1.1">2</cn><cn id="S3.SS2.p2.6.2.m2.2.2.cmml" type="integer" xref="S3.SS2.p2.6.2.m2.2.2">3</cn><cn id="S3.SS2.p2.6.2.m2.3.3.cmml" type="integer" xref="S3.SS2.p2.6.2.m2.3.3">5</cn><cn id="S3.SS2.p2.6.2.m2.4.4.cmml" type="integer" xref="S3.SS2.p2.6.2.m2.4.4">7</cn><cn id="S3.SS2.p2.6.2.m2.5.5.cmml" type="integer" xref="S3.SS2.p2.6.2.m2.5.5">11</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.2.m2.5c">P=[2,3,5,7,11]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.2.m2.5d">italic_P = [ 2 , 3 , 5 , 7 , 11 ]</annotation></semantics></math>.</span><span class="ltx_text" id="S3.SS2.p2.6.8" style="font-size:90%;">
We set the parameters of the convolutional layers as shown in TableÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S3.T3" style="font-size:90%;" title="Table 3 â€£ 3.1 Generator â€£ 3 Methods â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S3.SS2.p2.6.9" style="font-size:90%;">.
Each sub-discriminator is a stack of convolutional layers, with weight normalization applied for every convolutional layers. This setup allows us to obtain a set of discriminative outputs for each period sub-discriminator, providing different perspectives based on different period settings.</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text" id="S3.SS2.p3.1.1" style="font-size:90%;">There are also other voice synthesis works under GANs framework collaborating their discriminators with spectral-based discriminator </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib28" title="">28</a><span class="ltx_text" id="S3.SS2.p3.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p3.1.4" style="font-size:90%;">. Either a hierarchical </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p3.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib28" title="">28</a><span class="ltx_text" id="S3.SS2.p3.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p3.1.7" style="font-size:90%;"> or multi-resolution </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p3.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib27" title="">27</a><span class="ltx_text" id="S3.SS2.p3.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p3.1.10" style="font-size:90%;"> approach aims to capture the information from different perspectives of sample rates or window sizes of short-time Fourier transform.
We opt to follow several settings from </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p3.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib13" title="">13</a><span class="ltx_text" id="S3.SS2.p3.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p3.1.13" style="font-size:90%;"> not solely due to its success in speech synthesis.
In our preliminary experiments, we found that applying MPD as a module of the discriminator can help model high frequency information in distortion or overdrive audio. Furthermore, the combination of MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mo id="S3.SS2.p3.1.m1.1.1" mathsize="90%" xref="S3.SS2.p3.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><plus id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p3.1.14" style="font-size:90%;">MPD has been not only applied in neural vocoder tasks but also in neural codec tasks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p3.1.15.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib9" title="">9</a><span class="ltx_text" id="S3.SS2.p3.1.16.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p3.1.17" style="font-size:90%;">. In summary, the main goal of these audio-related tasks is to generate high-fidelity sound, and utilizing multi-type discriminators can improve audio fidelity under GAN training.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>GAN Loss</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_text" id="S3.SS3.p1.1.1" style="font-size:90%;">We choose to apply a Hinge GAN loss function </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS3.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib29" title="">29</a><span class="ltx_text" id="S3.SS3.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS3.p1.1.4" style="font-size:90%;"> in our GAN training, due to its promising result in prior work </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS3.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a><span class="ltx_text" id="S3.SS3.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS3.p1.1.7" style="font-size:90%;">. The loss equation is defined as follows:</span></p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(D;G)=\mathbb{E}_{y}[\max(0,1-D(y))]+\mathbb{E}_{x}[\max(0,1+D(G(x)%
))]" class="ltx_Math" display="block" id="S3.E2.m1.10"><semantics id="S3.E2.m1.10a"><mrow id="S3.E2.m1.10.10" xref="S3.E2.m1.10.10.cmml"><mrow id="S3.E2.m1.10.10.4" xref="S3.E2.m1.10.10.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.10.10.4.2" mathsize="90%" xref="S3.E2.m1.10.10.4.2.cmml">â„’</mi><mo id="S3.E2.m1.10.10.4.1" xref="S3.E2.m1.10.10.4.1.cmml">â¢</mo><mrow id="S3.E2.m1.10.10.4.3.2" xref="S3.E2.m1.10.10.4.3.1.cmml"><mo id="S3.E2.m1.10.10.4.3.2.1" maxsize="90%" minsize="90%" xref="S3.E2.m1.10.10.4.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" mathsize="90%" xref="S3.E2.m1.1.1.cmml">D</mi><mo id="S3.E2.m1.10.10.4.3.2.2" mathsize="90%" xref="S3.E2.m1.10.10.4.3.1.cmml">;</mo><mi id="S3.E2.m1.2.2" mathsize="90%" xref="S3.E2.m1.2.2.cmml">G</mi><mo id="S3.E2.m1.10.10.4.3.2.3" maxsize="90%" minsize="90%" xref="S3.E2.m1.10.10.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.10.10.3" mathsize="90%" xref="S3.E2.m1.10.10.3.cmml">=</mo><mrow id="S3.E2.m1.10.10.2" xref="S3.E2.m1.10.10.2.cmml"><mrow id="S3.E2.m1.9.9.1.1" xref="S3.E2.m1.9.9.1.1.cmml"><msub id="S3.E2.m1.9.9.1.1.3" xref="S3.E2.m1.9.9.1.1.3.cmml"><mi id="S3.E2.m1.9.9.1.1.3.2" mathsize="90%" xref="S3.E2.m1.9.9.1.1.3.2.cmml">ğ”¼</mi><mi id="S3.E2.m1.9.9.1.1.3.3" mathsize="90%" xref="S3.E2.m1.9.9.1.1.3.3.cmml">y</mi></msub><mo id="S3.E2.m1.9.9.1.1.2" xref="S3.E2.m1.9.9.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.9.9.1.1.1.1" xref="S3.E2.m1.9.9.1.1.1.2.cmml"><mo id="S3.E2.m1.9.9.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E2.m1.9.9.1.1.1.2.1.cmml">[</mo><mrow id="S3.E2.m1.9.9.1.1.1.1.1.1" xref="S3.E2.m1.9.9.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.4.4" mathsize="90%" xref="S3.E2.m1.4.4.cmml">max</mi><mo id="S3.E2.m1.9.9.1.1.1.1.1.1a" xref="S3.E2.m1.9.9.1.1.1.1.1.2.cmml">â¡</mo><mrow id="S3.E2.m1.9.9.1.1.1.1.1.1.1" xref="S3.E2.m1.9.9.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.9.9.1.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E2.m1.9.9.1.1.1.1.1.2.cmml">(</mo><mn id="S3.E2.m1.5.5" mathsize="90%" xref="S3.E2.m1.5.5.cmml">0</mn><mo id="S3.E2.m1.9.9.1.1.1.1.1.1.1.3" mathsize="90%" xref="S3.E2.m1.9.9.1.1.1.1.1.2.cmml">,</mo><mrow id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.2" mathsize="90%" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.1" mathsize="90%" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.2" mathsize="90%" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.2.cmml">D</mi><mo id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.1.cmml">â¢</mo><mrow id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.cmml"><mo id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.3.2.1" maxsize="90%" minsize="90%" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E2.m1.3.3" mathsize="90%" xref="S3.E2.m1.3.3.cmml">y</mi><mo id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.3.2.2" maxsize="90%" minsize="90%" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.9.9.1.1.1.1.1.1.1.4" maxsize="90%" minsize="90%" xref="S3.E2.m1.9.9.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.9.9.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.E2.m1.9.9.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E2.m1.10.10.2.3" mathsize="90%" xref="S3.E2.m1.10.10.2.3.cmml">+</mo><mrow id="S3.E2.m1.10.10.2.2" xref="S3.E2.m1.10.10.2.2.cmml"><msub id="S3.E2.m1.10.10.2.2.3" xref="S3.E2.m1.10.10.2.2.3.cmml"><mi id="S3.E2.m1.10.10.2.2.3.2" mathsize="90%" xref="S3.E2.m1.10.10.2.2.3.2.cmml">ğ”¼</mi><mi id="S3.E2.m1.10.10.2.2.3.3" mathsize="90%" xref="S3.E2.m1.10.10.2.2.3.3.cmml">x</mi></msub><mo id="S3.E2.m1.10.10.2.2.2" xref="S3.E2.m1.10.10.2.2.2.cmml">â¢</mo><mrow id="S3.E2.m1.10.10.2.2.1.1" xref="S3.E2.m1.10.10.2.2.1.2.cmml"><mo id="S3.E2.m1.10.10.2.2.1.1.2" maxsize="90%" minsize="90%" xref="S3.E2.m1.10.10.2.2.1.2.1.cmml">[</mo><mrow id="S3.E2.m1.10.10.2.2.1.1.1.1" xref="S3.E2.m1.10.10.2.2.1.1.1.2.cmml"><mi id="S3.E2.m1.7.7" mathsize="90%" xref="S3.E2.m1.7.7.cmml">max</mi><mo id="S3.E2.m1.10.10.2.2.1.1.1.1a" xref="S3.E2.m1.10.10.2.2.1.1.1.2.cmml">â¡</mo><mrow id="S3.E2.m1.10.10.2.2.1.1.1.1.1" xref="S3.E2.m1.10.10.2.2.1.1.1.2.cmml"><mo id="S3.E2.m1.10.10.2.2.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.2.cmml">(</mo><mn id="S3.E2.m1.8.8" mathsize="90%" xref="S3.E2.m1.8.8.cmml">0</mn><mo id="S3.E2.m1.10.10.2.2.1.1.1.1.1.3" mathsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.2.cmml">,</mo><mrow id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.3" mathsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.3.cmml">1</mn><mo id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.2" mathsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.3" mathsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.3.cmml">D</mi><mo id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.2" mathsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">G</mi><mo id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mrow id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.3.2.1" maxsize="90%" minsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.6.6" mathsize="90%" xref="S3.E2.m1.6.6.cmml">x</mi><mo id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2" maxsize="90%" minsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.10.10.2.2.1.1.1.1.1.4" maxsize="90%" minsize="90%" xref="S3.E2.m1.10.10.2.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.10.10.2.2.1.1.3" maxsize="90%" minsize="90%" xref="S3.E2.m1.10.10.2.2.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.10b"><apply id="S3.E2.m1.10.10.cmml" xref="S3.E2.m1.10.10"><eq id="S3.E2.m1.10.10.3.cmml" xref="S3.E2.m1.10.10.3"></eq><apply id="S3.E2.m1.10.10.4.cmml" xref="S3.E2.m1.10.10.4"><times id="S3.E2.m1.10.10.4.1.cmml" xref="S3.E2.m1.10.10.4.1"></times><ci id="S3.E2.m1.10.10.4.2.cmml" xref="S3.E2.m1.10.10.4.2">â„’</ci><list id="S3.E2.m1.10.10.4.3.1.cmml" xref="S3.E2.m1.10.10.4.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ğ·</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">ğº</ci></list></apply><apply id="S3.E2.m1.10.10.2.cmml" xref="S3.E2.m1.10.10.2"><plus id="S3.E2.m1.10.10.2.3.cmml" xref="S3.E2.m1.10.10.2.3"></plus><apply id="S3.E2.m1.9.9.1.1.cmml" xref="S3.E2.m1.9.9.1.1"><times id="S3.E2.m1.9.9.1.1.2.cmml" xref="S3.E2.m1.9.9.1.1.2"></times><apply id="S3.E2.m1.9.9.1.1.3.cmml" xref="S3.E2.m1.9.9.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.9.9.1.1.3.1.cmml" xref="S3.E2.m1.9.9.1.1.3">subscript</csymbol><ci id="S3.E2.m1.9.9.1.1.3.2.cmml" xref="S3.E2.m1.9.9.1.1.3.2">ğ”¼</ci><ci id="S3.E2.m1.9.9.1.1.3.3.cmml" xref="S3.E2.m1.9.9.1.1.3.3">ğ‘¦</ci></apply><apply id="S3.E2.m1.9.9.1.1.1.2.cmml" xref="S3.E2.m1.9.9.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.9.9.1.1.1.2.1.cmml" xref="S3.E2.m1.9.9.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.9.9.1.1.1.1.1.2.cmml" xref="S3.E2.m1.9.9.1.1.1.1.1.1"><max id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"></max><cn id="S3.E2.m1.5.5.cmml" type="integer" xref="S3.E2.m1.5.5">0</cn><apply id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.1"></minus><cn id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3"><times id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.9.9.1.1.1.1.1.1.1.1.3.2">ğ·</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">ğ‘¦</ci></apply></apply></apply></apply></apply><apply id="S3.E2.m1.10.10.2.2.cmml" xref="S3.E2.m1.10.10.2.2"><times id="S3.E2.m1.10.10.2.2.2.cmml" xref="S3.E2.m1.10.10.2.2.2"></times><apply id="S3.E2.m1.10.10.2.2.3.cmml" xref="S3.E2.m1.10.10.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.10.10.2.2.3.1.cmml" xref="S3.E2.m1.10.10.2.2.3">subscript</csymbol><ci id="S3.E2.m1.10.10.2.2.3.2.cmml" xref="S3.E2.m1.10.10.2.2.3.2">ğ”¼</ci><ci id="S3.E2.m1.10.10.2.2.3.3.cmml" xref="S3.E2.m1.10.10.2.2.3.3">ğ‘¥</ci></apply><apply id="S3.E2.m1.10.10.2.2.1.2.cmml" xref="S3.E2.m1.10.10.2.2.1.1"><csymbol cd="latexml" id="S3.E2.m1.10.10.2.2.1.2.1.cmml" xref="S3.E2.m1.10.10.2.2.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.10.10.2.2.1.1.1.2.cmml" xref="S3.E2.m1.10.10.2.2.1.1.1.1"><max id="S3.E2.m1.7.7.cmml" xref="S3.E2.m1.7.7"></max><cn id="S3.E2.m1.8.8.cmml" type="integer" xref="S3.E2.m1.8.8">0</cn><apply id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1"><plus id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.2"></plus><cn id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.3">1</cn><apply id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1"><times id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.3">ğ·</ci><apply id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.10.10.2.2.1.1.1.1.1.1.1.1.1.1.2">ğº</ci><ci id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6">ğ‘¥</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.10c">\mathcal{L}(D;G)=\mathbb{E}_{y}[\max(0,1-D(y))]+\mathbb{E}_{x}[\max(0,1+D(G(x)%
))]</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.10d">caligraphic_L ( italic_D ; italic_G ) = blackboard_E start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT [ roman_max ( 0 , 1 - italic_D ( italic_y ) ) ] + blackboard_E start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT [ roman_max ( 0 , 1 + italic_D ( italic_G ( italic_x ) ) ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(G;D)=\mathbb{E}_{x}[-D(G(x))]" class="ltx_Math" display="block" id="S3.E3.m1.4"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mrow id="S3.E3.m1.4.4.3" xref="S3.E3.m1.4.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.4.4.3.2" mathsize="90%" xref="S3.E3.m1.4.4.3.2.cmml">â„’</mi><mo id="S3.E3.m1.4.4.3.1" xref="S3.E3.m1.4.4.3.1.cmml">â¢</mo><mrow id="S3.E3.m1.4.4.3.3.2" xref="S3.E3.m1.4.4.3.3.1.cmml"><mo id="S3.E3.m1.4.4.3.3.2.1" maxsize="90%" minsize="90%" xref="S3.E3.m1.4.4.3.3.1.cmml">(</mo><mi id="S3.E3.m1.1.1" mathsize="90%" xref="S3.E3.m1.1.1.cmml">G</mi><mo id="S3.E3.m1.4.4.3.3.2.2" mathsize="90%" xref="S3.E3.m1.4.4.3.3.1.cmml">;</mo><mi id="S3.E3.m1.2.2" mathsize="90%" xref="S3.E3.m1.2.2.cmml">D</mi><mo id="S3.E3.m1.4.4.3.3.2.3" maxsize="90%" minsize="90%" xref="S3.E3.m1.4.4.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.2" mathsize="90%" xref="S3.E3.m1.4.4.2.cmml">=</mo><mrow id="S3.E3.m1.4.4.1" xref="S3.E3.m1.4.4.1.cmml"><msub id="S3.E3.m1.4.4.1.3" xref="S3.E3.m1.4.4.1.3.cmml"><mi id="S3.E3.m1.4.4.1.3.2" mathsize="90%" xref="S3.E3.m1.4.4.1.3.2.cmml">ğ”¼</mi><mi id="S3.E3.m1.4.4.1.3.3" mathsize="90%" xref="S3.E3.m1.4.4.1.3.3.cmml">x</mi></msub><mo id="S3.E3.m1.4.4.1.2" xref="S3.E3.m1.4.4.1.2.cmml">â¢</mo><mrow id="S3.E3.m1.4.4.1.1.1" xref="S3.E3.m1.4.4.1.1.2.cmml"><mo id="S3.E3.m1.4.4.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E3.m1.4.4.1.1.2.1.cmml">[</mo><mrow id="S3.E3.m1.4.4.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.cmml"><mo id="S3.E3.m1.4.4.1.1.1.1a" mathsize="90%" xref="S3.E3.m1.4.4.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.3" mathsize="90%" xref="S3.E3.m1.4.4.1.1.1.1.1.3.cmml">D</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.2" mathsize="90%" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">G</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.1" maxsize="90%" minsize="90%" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E3.m1.3.3" mathsize="90%" xref="S3.E3.m1.3.3.cmml">x</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.2" maxsize="90%" minsize="90%" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.4.4.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.E3.m1.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4"><eq id="S3.E3.m1.4.4.2.cmml" xref="S3.E3.m1.4.4.2"></eq><apply id="S3.E3.m1.4.4.3.cmml" xref="S3.E3.m1.4.4.3"><times id="S3.E3.m1.4.4.3.1.cmml" xref="S3.E3.m1.4.4.3.1"></times><ci id="S3.E3.m1.4.4.3.2.cmml" xref="S3.E3.m1.4.4.3.2">â„’</ci><list id="S3.E3.m1.4.4.3.3.1.cmml" xref="S3.E3.m1.4.4.3.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğº</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">ğ·</ci></list></apply><apply id="S3.E3.m1.4.4.1.cmml" xref="S3.E3.m1.4.4.1"><times id="S3.E3.m1.4.4.1.2.cmml" xref="S3.E3.m1.4.4.1.2"></times><apply id="S3.E3.m1.4.4.1.3.cmml" xref="S3.E3.m1.4.4.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.3.1.cmml" xref="S3.E3.m1.4.4.1.3">subscript</csymbol><ci id="S3.E3.m1.4.4.1.3.2.cmml" xref="S3.E3.m1.4.4.1.3.2">ğ”¼</ci><ci id="S3.E3.m1.4.4.1.3.3.cmml" xref="S3.E3.m1.4.4.1.3.3">ğ‘¥</ci></apply><apply id="S3.E3.m1.4.4.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.4.4.1.1.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.2">delimited-[]</csymbol><apply id="S3.E3.m1.4.4.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1"><minus id="S3.E3.m1.4.4.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1"></minus><apply id="S3.E3.m1.4.4.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1"><times id="S3.E3.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2"></times><ci id="S3.E3.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.3">ğ·</ci><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1"><times id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.2">ğº</ci><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">ğ‘¥</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\mathcal{L}(G;D)=\mathbb{E}_{x}[-D(G(x))]</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.4d">caligraphic_L ( italic_G ; italic_D ) = blackboard_E start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT [ - italic_D ( italic_G ( italic_x ) ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.3"><span class="ltx_text" id="S3.SS3.p4.3.3" style="font-size:90%;color:#000000;">During training, the discriminator is trained to classify labeled data <math alttext="y" class="ltx_Math" display="inline" id="S3.SS3.p4.1.1.m1.1"><semantics id="S3.SS3.p4.1.1.m1.1a"><mi id="S3.SS3.p4.1.1.m1.1.1" mathcolor="#000000" xref="S3.SS3.p4.1.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.1.m1.1b"><ci id="S3.SS3.p4.1.1.m1.1.1.cmml" xref="S3.SS3.p4.1.1.m1.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.1.m1.1d">italic_y</annotation></semantics></math> as 1, and the samples generated from Generator <math alttext="G(x)" class="ltx_Math" display="inline" id="S3.SS3.p4.2.2.m2.1"><semantics id="S3.SS3.p4.2.2.m2.1a"><mrow id="S3.SS3.p4.2.2.m2.1.2" xref="S3.SS3.p4.2.2.m2.1.2.cmml"><mi id="S3.SS3.p4.2.2.m2.1.2.2" mathcolor="#000000" xref="S3.SS3.p4.2.2.m2.1.2.2.cmml">G</mi><mo id="S3.SS3.p4.2.2.m2.1.2.1" xref="S3.SS3.p4.2.2.m2.1.2.1.cmml">â¢</mo><mrow id="S3.SS3.p4.2.2.m2.1.2.3.2" xref="S3.SS3.p4.2.2.m2.1.2.cmml"><mo id="S3.SS3.p4.2.2.m2.1.2.3.2.1" mathcolor="#000000" stretchy="false" xref="S3.SS3.p4.2.2.m2.1.2.cmml">(</mo><mi id="S3.SS3.p4.2.2.m2.1.1" mathcolor="#000000" xref="S3.SS3.p4.2.2.m2.1.1.cmml">x</mi><mo id="S3.SS3.p4.2.2.m2.1.2.3.2.2" mathcolor="#000000" stretchy="false" xref="S3.SS3.p4.2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.2.m2.1b"><apply id="S3.SS3.p4.2.2.m2.1.2.cmml" xref="S3.SS3.p4.2.2.m2.1.2"><times id="S3.SS3.p4.2.2.m2.1.2.1.cmml" xref="S3.SS3.p4.2.2.m2.1.2.1"></times><ci id="S3.SS3.p4.2.2.m2.1.2.2.cmml" xref="S3.SS3.p4.2.2.m2.1.2.2">ğº</ci><ci id="S3.SS3.p4.2.2.m2.1.1.cmml" xref="S3.SS3.p4.2.2.m2.1.1">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.2.m2.1c">G(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.2.2.m2.1d">italic_G ( italic_x )</annotation></semantics></math> as 0. The generator is trained to deceive the discriminator into recognizing <math alttext="G(x)" class="ltx_Math" display="inline" id="S3.SS3.p4.3.3.m3.1"><semantics id="S3.SS3.p4.3.3.m3.1a"><mrow id="S3.SS3.p4.3.3.m3.1.2" xref="S3.SS3.p4.3.3.m3.1.2.cmml"><mi id="S3.SS3.p4.3.3.m3.1.2.2" mathcolor="#000000" xref="S3.SS3.p4.3.3.m3.1.2.2.cmml">G</mi><mo id="S3.SS3.p4.3.3.m3.1.2.1" xref="S3.SS3.p4.3.3.m3.1.2.1.cmml">â¢</mo><mrow id="S3.SS3.p4.3.3.m3.1.2.3.2" xref="S3.SS3.p4.3.3.m3.1.2.cmml"><mo id="S3.SS3.p4.3.3.m3.1.2.3.2.1" mathcolor="#000000" stretchy="false" xref="S3.SS3.p4.3.3.m3.1.2.cmml">(</mo><mi id="S3.SS3.p4.3.3.m3.1.1" mathcolor="#000000" xref="S3.SS3.p4.3.3.m3.1.1.cmml">x</mi><mo id="S3.SS3.p4.3.3.m3.1.2.3.2.2" mathcolor="#000000" stretchy="false" xref="S3.SS3.p4.3.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.3.m3.1b"><apply id="S3.SS3.p4.3.3.m3.1.2.cmml" xref="S3.SS3.p4.3.3.m3.1.2"><times id="S3.SS3.p4.3.3.m3.1.2.1.cmml" xref="S3.SS3.p4.3.3.m3.1.2.1"></times><ci id="S3.SS3.p4.3.3.m3.1.2.2.cmml" xref="S3.SS3.p4.3.3.m3.1.2.2">ğº</ci><ci id="S3.SS3.p4.3.3.m3.1.1.cmml" xref="S3.SS3.p4.3.3.m3.1.1">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.3.m3.1c">G(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.3.3.m3.1d">italic_G ( italic_x )</annotation></semantics></math> as real data, aiming for a classification close to 1.
</span><span class="ltx_text" id="S3.SS3.p4.3.4" style="font-size:90%;">
</span><span class="ltx_text" id="S3.SS3.p4.3.5" style="font-size:90%;color:#000000;">
Other auxiliary losses such as <em class="ltx_emph ltx_font_italic" id="S3.SS3.p4.3.5.1">mel-spectrogram loss</em> and <em class="ltx_emph ltx_font_italic" id="S3.SS3.p4.3.5.2">feature matching loss</em> were used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib8" title="">8</a>]</cite>.
The mel-spectrogram loss measures the L1 distance between the generated audioâ€™s mel-spectrogram and that of labeled data.
The feature matching loss computes the L1 distance in intermediate features from the discriminators between the generated audio and a labeled data.
Although these loss functions can improve the training efficiency, stability of the generator, and the quality of the generated audio,
they require a paired data setting during the training process.
Given an unpaired data setting, our model only utilizes an adversarial loss during the training process for both the generator and discriminator.
</span><span class="ltx_text" id="S3.SS3.p4.3.6" style="font-size:90%;"></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text" id="S4.SS1.p1.1.1" style="font-size:90%;">We select two electric guitar datasets for our experiments: EGDB </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib15" title="">15</a><span class="ltx_text" id="S4.SS1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.p1.1.4" style="font-size:90%;"> and EGFxset </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib16" title="">16</a><span class="ltx_text" id="S4.SS1.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.p1.1.7" style="font-size:90%;">.
For EGDB, the duration of a single tone is approximately 2 hours. We choose a subset consisting of </span><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.8" style="font-size:90%;">Marshall JCM2000</span><span class="ltx_text" id="S4.SS1.p1.1.9" style="font-size:90%;">, </span><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.10" style="font-size:90%;">Fender Twin Reverb</span><span class="ltx_text" id="S4.SS1.p1.1.11" style="font-size:90%;">, and </span><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.12" style="font-size:90%;">Mesa Boogie Mark</span><span class="ltx_text" id="S4.SS1.p1.1.13" style="font-size:90%;">.
For EGFxset, we select the </span><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.14" style="font-size:90%;">BD-2</span><span class="ltx_text" id="S4.SS1.p1.1.15" style="font-size:90%;"> dataset as the target tone for our experiments. As the gain value has been set to its maximum value for BD-2, this dataset contains highly distorted sound.
As the BD-2 tone has fairly high gain, we may consider the three tones from EGDB as relatively low-gain tone compared to BD-2.
</span><span class="ltx_text" id="S4.SS1.p1.1.16" style="font-size:90%;color:#000000;">We note that EGDB comprises musical phrases or licks, while EGFxset contains recordings of individual notes, each at different pitches and from various pickups.</span><span class="ltx_text" id="S4.SS1.p1.1.17" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.2"><span class="ltx_text" id="S4.SS1.p2.2.1" style="font-size:90%;">In our preliminary experiments, we found great differences in amplitude between the two datasets, possibly because they were collected under different device settings (e.g., guitar or audio interface) and recording environments. We found that GAN-based models is highly sensitive to differences in amplitude.
To address this, we normalize both datasets using pyloudnorm </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.p2.2.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib31" title="">31</a><span class="ltx_text" id="S4.SS1.p2.2.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.p2.2.4" style="font-size:90%;">.
Specifically, we normalize the peak of each audio to </span><math alttext="-1" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mo id="S4.SS1.p2.1.m1.1.1a" mathsize="90%" xref="S4.SS1.p2.1.m1.1.1.cmml">âˆ’</mo><mn id="S4.SS1.p2.1.m1.1.1.2" mathsize="90%" xref="S4.SS1.p2.1.m1.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><minus id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></minus><cn id="S4.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">-1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">- 1</annotation></semantics></math><span class="ltx_text" id="S4.SS1.p2.2.5" style="font-size:90%;"> dB, and then normalized each audio to </span><math alttext="-12" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mo id="S4.SS1.p2.2.m2.1.1a" mathsize="90%" xref="S4.SS1.p2.2.m2.1.1.cmml">âˆ’</mo><mn id="S4.SS1.p2.2.m2.1.1.2" mathsize="90%" xref="S4.SS1.p2.2.m2.1.1.2.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><minus id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"></minus><cn id="S4.SS1.p2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1.2">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">-12</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">- 12</annotation></semantics></math><span class="ltx_text" id="S4.SS1.p2.2.6" style="font-size:90%;"> dB LUFS. Without such a normalization, the training would be extremely unstable, leading to failures during the early stages of the GAN training process.</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text" id="S4.SS1.p3.1.1" style="font-size:90%;">We divide the dataset into training, validation, and test sets using an 80/10/10 ratio. For training, the input clean data and the output target tone data are randomly arranged in each batch for an unsupervised setting.
To evaluate the model performance, as the clean data and rendered data are aligned between the two datasets, validation and testing are conducted under a paired setting to calculate all metrics.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text" id="S4.SS2.p1.1.1" style="font-size:90%;">We consider the following three metrics for objective evaluation.</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.3"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.3.1" style="font-size:90%;">Error-to-signal ratio</span><span class="ltx_text" id="S4.SS2.p2.3.2" style="font-size:90%;"> (ESR) is a metric commonly employed for training and evaluating an amplifier modeling model. For </span><math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" mathsize="90%" xref="S4.SS2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_N</annotation></semantics></math><span class="ltx_text" id="S4.SS2.p2.3.3" style="font-size:90%;"> sample points, a pre-emphasis filter is applied to both the generated signal </span><math alttext="\widehat{\mathbf{y}}_{p}" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><msub id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mover accent="true" id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2.2" mathsize="90%" xref="S4.SS2.p2.2.m2.1.1.2.2.cmml">ğ²</mi><mo id="S4.SS2.p2.2.m2.1.1.2.1" mathsize="90%" xref="S4.SS2.p2.2.m2.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS2.p2.2.m2.1.1.3" mathsize="90%" xref="S4.SS2.p2.2.m2.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">subscript</csymbol><apply id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2"><ci id="S4.SS2.p2.2.m2.1.1.2.1.cmml" xref="S4.SS2.p2.2.m2.1.1.2.1">^</ci><ci id="S4.SS2.p2.2.m2.1.1.2.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2.2">ğ²</ci></apply><ci id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\widehat{\mathbf{y}}_{p}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">over^ start_ARG bold_y end_ARG start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS2.p2.3.4" style="font-size:90%;"> and a target signals </span><math alttext="\mathbf{y}_{p}" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1"><semantics id="S4.SS2.p2.3.m3.1a"><msub id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" mathsize="90%" xref="S4.SS2.p2.3.m3.1.1.2.cmml">ğ²</mi><mi id="S4.SS2.p2.3.m3.1.1.3" mathsize="90%" xref="S4.SS2.p2.3.m3.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">ğ²</ci><ci id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\mathbf{y}_{p}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">bold_y start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS2.p2.3.5" style="font-size:90%;"> before computing the ESR.</span></p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{\mathrm{ESR}}=\frac{\sum_{n=0}^{N-1}\left|\mathbf{y}_{p}[n]-\widehat{\mathbf{%
y}}_{p}[n]\right|^{2}}{\sum_{n=0}^{N-1}\left|\mathbf{y}_{p}[n]\right|^{2}}\,." class="ltx_Math" display="block" id="S4.Ex1.m1.6"><semantics id="S4.Ex1.m1.6a"><mrow id="S4.Ex1.m1.6.6.1" xref="S4.Ex1.m1.6.6.1.1.cmml"><mrow id="S4.Ex1.m1.6.6.1.1" xref="S4.Ex1.m1.6.6.1.1.cmml"><mi id="S4.Ex1.m1.6.6.1.1.2" mathsize="90%" xref="S4.Ex1.m1.6.6.1.1.2.cmml">ESR</mi><mo id="S4.Ex1.m1.6.6.1.1.1" mathsize="90%" xref="S4.Ex1.m1.6.6.1.1.1.cmml">=</mo><mfrac id="S4.Ex1.m1.5.5" xref="S4.Ex1.m1.5.5.cmml"><mrow id="S4.Ex1.m1.3.3.3" xref="S4.Ex1.m1.3.3.3.cmml"><msubsup id="S4.Ex1.m1.3.3.3.4" xref="S4.Ex1.m1.3.3.3.4.cmml"><mo id="S4.Ex1.m1.3.3.3.4.2.2" maxsize="90%" minsize="90%" stretchy="true" xref="S4.Ex1.m1.3.3.3.4.2.2.cmml">âˆ‘</mo><mrow id="S4.Ex1.m1.3.3.3.4.2.3" xref="S4.Ex1.m1.3.3.3.4.2.3.cmml"><mi id="S4.Ex1.m1.3.3.3.4.2.3.2" mathsize="90%" xref="S4.Ex1.m1.3.3.3.4.2.3.2.cmml">n</mi><mo id="S4.Ex1.m1.3.3.3.4.2.3.1" mathsize="90%" xref="S4.Ex1.m1.3.3.3.4.2.3.1.cmml">=</mo><mn id="S4.Ex1.m1.3.3.3.4.2.3.3" mathsize="90%" xref="S4.Ex1.m1.3.3.3.4.2.3.3.cmml">0</mn></mrow><mrow id="S4.Ex1.m1.3.3.3.4.3" xref="S4.Ex1.m1.3.3.3.4.3.cmml"><mi id="S4.Ex1.m1.3.3.3.4.3.2" mathsize="90%" xref="S4.Ex1.m1.3.3.3.4.3.2.cmml">N</mi><mo id="S4.Ex1.m1.3.3.3.4.3.1" mathsize="90%" xref="S4.Ex1.m1.3.3.3.4.3.1.cmml">âˆ’</mo><mn id="S4.Ex1.m1.3.3.3.4.3.3" mathsize="90%" xref="S4.Ex1.m1.3.3.3.4.3.3.cmml">1</mn></mrow></msubsup><msup id="S4.Ex1.m1.3.3.3.3" xref="S4.Ex1.m1.3.3.3.3.cmml"><mrow id="S4.Ex1.m1.3.3.3.3.1.1" xref="S4.Ex1.m1.3.3.3.3.1.2.cmml"><mo id="S4.Ex1.m1.3.3.3.3.1.1.2" lspace="0em" xref="S4.Ex1.m1.3.3.3.3.1.2.1.cmml">|</mo><mrow id="S4.Ex1.m1.3.3.3.3.1.1.1" xref="S4.Ex1.m1.3.3.3.3.1.1.1.cmml"><mrow id="S4.Ex1.m1.3.3.3.3.1.1.1.2" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.cmml"><msub id="S4.Ex1.m1.3.3.3.3.1.1.1.2.2" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.cmml"><mi id="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.2" mathsize="90%" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.2.cmml">ğ²</mi><mi id="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.3" mathsize="90%" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.3.cmml">p</mi></msub><mo id="S4.Ex1.m1.3.3.3.3.1.1.1.2.1" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.1.cmml">â¢</mo><mrow id="S4.Ex1.m1.3.3.3.3.1.1.1.2.3.2" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.3.1.cmml"><mo id="S4.Ex1.m1.3.3.3.3.1.1.1.2.3.2.1" maxsize="90%" minsize="90%" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.3.1.1.cmml">[</mo><mi id="S4.Ex1.m1.1.1.1.1" mathsize="90%" xref="S4.Ex1.m1.1.1.1.1.cmml">n</mi><mo id="S4.Ex1.m1.3.3.3.3.1.1.1.2.3.2.2" maxsize="90%" minsize="90%" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="S4.Ex1.m1.3.3.3.3.1.1.1.1" mathsize="90%" xref="S4.Ex1.m1.3.3.3.3.1.1.1.1.cmml">âˆ’</mo><mrow id="S4.Ex1.m1.3.3.3.3.1.1.1.3" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.cmml"><msub id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.cmml"><mover accent="true" id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2.cmml"><mi id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2.2" mathsize="90%" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2.2.cmml">ğ²</mi><mo id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2.1" mathsize="90%" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2.1.cmml">^</mo></mover><mi id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.3" mathsize="90%" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.3.cmml">p</mi></msub><mo id="S4.Ex1.m1.3.3.3.3.1.1.1.3.1" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.1.cmml">â¢</mo><mrow id="S4.Ex1.m1.3.3.3.3.1.1.1.3.3.2" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.3.1.cmml"><mo id="S4.Ex1.m1.3.3.3.3.1.1.1.3.3.2.1" maxsize="90%" minsize="90%" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.3.1.1.cmml">[</mo><mi id="S4.Ex1.m1.2.2.2.2" mathsize="90%" xref="S4.Ex1.m1.2.2.2.2.cmml">n</mi><mo id="S4.Ex1.m1.3.3.3.3.1.1.1.3.3.2.2" maxsize="90%" minsize="90%" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.3.1.1.cmml">]</mo></mrow></mrow></mrow><mo id="S4.Ex1.m1.3.3.3.3.1.1.3" xref="S4.Ex1.m1.3.3.3.3.1.2.1.cmml">|</mo></mrow><mn id="S4.Ex1.m1.3.3.3.3.3" mathsize="90%" xref="S4.Ex1.m1.3.3.3.3.3.cmml">2</mn></msup></mrow><mrow id="S4.Ex1.m1.5.5.5" xref="S4.Ex1.m1.5.5.5.cmml"><msubsup id="S4.Ex1.m1.5.5.5.3" xref="S4.Ex1.m1.5.5.5.3.cmml"><mo id="S4.Ex1.m1.5.5.5.3.2.2" maxsize="90%" minsize="90%" stretchy="true" xref="S4.Ex1.m1.5.5.5.3.2.2.cmml">âˆ‘</mo><mrow id="S4.Ex1.m1.5.5.5.3.2.3" xref="S4.Ex1.m1.5.5.5.3.2.3.cmml"><mi id="S4.Ex1.m1.5.5.5.3.2.3.2" mathsize="90%" xref="S4.Ex1.m1.5.5.5.3.2.3.2.cmml">n</mi><mo id="S4.Ex1.m1.5.5.5.3.2.3.1" mathsize="90%" xref="S4.Ex1.m1.5.5.5.3.2.3.1.cmml">=</mo><mn id="S4.Ex1.m1.5.5.5.3.2.3.3" mathsize="90%" xref="S4.Ex1.m1.5.5.5.3.2.3.3.cmml">0</mn></mrow><mrow id="S4.Ex1.m1.5.5.5.3.3" xref="S4.Ex1.m1.5.5.5.3.3.cmml"><mi id="S4.Ex1.m1.5.5.5.3.3.2" mathsize="90%" xref="S4.Ex1.m1.5.5.5.3.3.2.cmml">N</mi><mo id="S4.Ex1.m1.5.5.5.3.3.1" mathsize="90%" xref="S4.Ex1.m1.5.5.5.3.3.1.cmml">âˆ’</mo><mn id="S4.Ex1.m1.5.5.5.3.3.3" mathsize="90%" xref="S4.Ex1.m1.5.5.5.3.3.3.cmml">1</mn></mrow></msubsup><msup id="S4.Ex1.m1.5.5.5.2" xref="S4.Ex1.m1.5.5.5.2.cmml"><mrow id="S4.Ex1.m1.5.5.5.2.1.1" xref="S4.Ex1.m1.5.5.5.2.1.2.cmml"><mo id="S4.Ex1.m1.5.5.5.2.1.1.2" lspace="0em" xref="S4.Ex1.m1.5.5.5.2.1.2.1.cmml">|</mo><mrow id="S4.Ex1.m1.5.5.5.2.1.1.1" xref="S4.Ex1.m1.5.5.5.2.1.1.1.cmml"><msub id="S4.Ex1.m1.5.5.5.2.1.1.1.2" xref="S4.Ex1.m1.5.5.5.2.1.1.1.2.cmml"><mi id="S4.Ex1.m1.5.5.5.2.1.1.1.2.2" mathsize="90%" xref="S4.Ex1.m1.5.5.5.2.1.1.1.2.2.cmml">ğ²</mi><mi id="S4.Ex1.m1.5.5.5.2.1.1.1.2.3" mathsize="90%" xref="S4.Ex1.m1.5.5.5.2.1.1.1.2.3.cmml">p</mi></msub><mo id="S4.Ex1.m1.5.5.5.2.1.1.1.1" xref="S4.Ex1.m1.5.5.5.2.1.1.1.1.cmml">â¢</mo><mrow id="S4.Ex1.m1.5.5.5.2.1.1.1.3.2" xref="S4.Ex1.m1.5.5.5.2.1.1.1.3.1.cmml"><mo id="S4.Ex1.m1.5.5.5.2.1.1.1.3.2.1" maxsize="90%" minsize="90%" xref="S4.Ex1.m1.5.5.5.2.1.1.1.3.1.1.cmml">[</mo><mi id="S4.Ex1.m1.4.4.4.1" mathsize="90%" xref="S4.Ex1.m1.4.4.4.1.cmml">n</mi><mo id="S4.Ex1.m1.5.5.5.2.1.1.1.3.2.2" maxsize="90%" minsize="90%" xref="S4.Ex1.m1.5.5.5.2.1.1.1.3.1.1.cmml">]</mo></mrow></mrow><mo id="S4.Ex1.m1.5.5.5.2.1.1.3" xref="S4.Ex1.m1.5.5.5.2.1.2.1.cmml">|</mo></mrow><mn id="S4.Ex1.m1.5.5.5.2.3" mathsize="90%" xref="S4.Ex1.m1.5.5.5.2.3.cmml">2</mn></msup></mrow></mfrac></mrow><mo id="S4.Ex1.m1.6.6.1.2" lspace="0.170em" mathsize="90%" xref="S4.Ex1.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.6b"><apply id="S4.Ex1.m1.6.6.1.1.cmml" xref="S4.Ex1.m1.6.6.1"><eq id="S4.Ex1.m1.6.6.1.1.1.cmml" xref="S4.Ex1.m1.6.6.1.1.1"></eq><ci id="S4.Ex1.m1.6.6.1.1.2.cmml" xref="S4.Ex1.m1.6.6.1.1.2">ESR</ci><apply id="S4.Ex1.m1.5.5.cmml" xref="S4.Ex1.m1.5.5"><divide id="S4.Ex1.m1.5.5.6.cmml" xref="S4.Ex1.m1.5.5"></divide><apply id="S4.Ex1.m1.3.3.3.cmml" xref="S4.Ex1.m1.3.3.3"><apply id="S4.Ex1.m1.3.3.3.4.cmml" xref="S4.Ex1.m1.3.3.3.4"><csymbol cd="ambiguous" id="S4.Ex1.m1.3.3.3.4.1.cmml" xref="S4.Ex1.m1.3.3.3.4">superscript</csymbol><apply id="S4.Ex1.m1.3.3.3.4.2.cmml" xref="S4.Ex1.m1.3.3.3.4"><csymbol cd="ambiguous" id="S4.Ex1.m1.3.3.3.4.2.1.cmml" xref="S4.Ex1.m1.3.3.3.4">subscript</csymbol><sum id="S4.Ex1.m1.3.3.3.4.2.2.cmml" xref="S4.Ex1.m1.3.3.3.4.2.2"></sum><apply id="S4.Ex1.m1.3.3.3.4.2.3.cmml" xref="S4.Ex1.m1.3.3.3.4.2.3"><eq id="S4.Ex1.m1.3.3.3.4.2.3.1.cmml" xref="S4.Ex1.m1.3.3.3.4.2.3.1"></eq><ci id="S4.Ex1.m1.3.3.3.4.2.3.2.cmml" xref="S4.Ex1.m1.3.3.3.4.2.3.2">ğ‘›</ci><cn id="S4.Ex1.m1.3.3.3.4.2.3.3.cmml" type="integer" xref="S4.Ex1.m1.3.3.3.4.2.3.3">0</cn></apply></apply><apply id="S4.Ex1.m1.3.3.3.4.3.cmml" xref="S4.Ex1.m1.3.3.3.4.3"><minus id="S4.Ex1.m1.3.3.3.4.3.1.cmml" xref="S4.Ex1.m1.3.3.3.4.3.1"></minus><ci id="S4.Ex1.m1.3.3.3.4.3.2.cmml" xref="S4.Ex1.m1.3.3.3.4.3.2">ğ‘</ci><cn id="S4.Ex1.m1.3.3.3.4.3.3.cmml" type="integer" xref="S4.Ex1.m1.3.3.3.4.3.3">1</cn></apply></apply><apply id="S4.Ex1.m1.3.3.3.3.cmml" xref="S4.Ex1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.3.3.3.3.2.cmml" xref="S4.Ex1.m1.3.3.3.3">superscript</csymbol><apply id="S4.Ex1.m1.3.3.3.3.1.2.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1"><abs id="S4.Ex1.m1.3.3.3.3.1.2.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.2"></abs><apply id="S4.Ex1.m1.3.3.3.3.1.1.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1"><minus id="S4.Ex1.m1.3.3.3.3.1.1.1.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.1"></minus><apply id="S4.Ex1.m1.3.3.3.3.1.1.1.2.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2"><times id="S4.Ex1.m1.3.3.3.3.1.1.1.2.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.1"></times><apply id="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.2">subscript</csymbol><ci id="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.2.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.2">ğ²</ci><ci id="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.3.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.2.3">ğ‘</ci></apply><apply id="S4.Ex1.m1.3.3.3.3.1.1.1.2.3.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.3.2"><csymbol cd="latexml" id="S4.Ex1.m1.3.3.3.3.1.1.1.2.3.1.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.2.3.2.1">delimited-[]</csymbol><ci id="S4.Ex1.m1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1">ğ‘›</ci></apply></apply><apply id="S4.Ex1.m1.3.3.3.3.1.1.1.3.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3"><times id="S4.Ex1.m1.3.3.3.3.1.1.1.3.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.1"></times><apply id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2">subscript</csymbol><apply id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2"><ci id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2.1">^</ci><ci id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2.2.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.2.2">ğ²</ci></apply><ci id="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.3.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.2.3">ğ‘</ci></apply><apply id="S4.Ex1.m1.3.3.3.3.1.1.1.3.3.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.3.2"><csymbol cd="latexml" id="S4.Ex1.m1.3.3.3.3.1.1.1.3.3.1.1.cmml" xref="S4.Ex1.m1.3.3.3.3.1.1.1.3.3.2.1">delimited-[]</csymbol><ci id="S4.Ex1.m1.2.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2.2">ğ‘›</ci></apply></apply></apply></apply><cn id="S4.Ex1.m1.3.3.3.3.3.cmml" type="integer" xref="S4.Ex1.m1.3.3.3.3.3">2</cn></apply></apply><apply id="S4.Ex1.m1.5.5.5.cmml" xref="S4.Ex1.m1.5.5.5"><apply id="S4.Ex1.m1.5.5.5.3.cmml" xref="S4.Ex1.m1.5.5.5.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.5.5.5.3.1.cmml" xref="S4.Ex1.m1.5.5.5.3">superscript</csymbol><apply id="S4.Ex1.m1.5.5.5.3.2.cmml" xref="S4.Ex1.m1.5.5.5.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.5.5.5.3.2.1.cmml" xref="S4.Ex1.m1.5.5.5.3">subscript</csymbol><sum id="S4.Ex1.m1.5.5.5.3.2.2.cmml" xref="S4.Ex1.m1.5.5.5.3.2.2"></sum><apply id="S4.Ex1.m1.5.5.5.3.2.3.cmml" xref="S4.Ex1.m1.5.5.5.3.2.3"><eq id="S4.Ex1.m1.5.5.5.3.2.3.1.cmml" xref="S4.Ex1.m1.5.5.5.3.2.3.1"></eq><ci id="S4.Ex1.m1.5.5.5.3.2.3.2.cmml" xref="S4.Ex1.m1.5.5.5.3.2.3.2">ğ‘›</ci><cn id="S4.Ex1.m1.5.5.5.3.2.3.3.cmml" type="integer" xref="S4.Ex1.m1.5.5.5.3.2.3.3">0</cn></apply></apply><apply id="S4.Ex1.m1.5.5.5.3.3.cmml" xref="S4.Ex1.m1.5.5.5.3.3"><minus id="S4.Ex1.m1.5.5.5.3.3.1.cmml" xref="S4.Ex1.m1.5.5.5.3.3.1"></minus><ci id="S4.Ex1.m1.5.5.5.3.3.2.cmml" xref="S4.Ex1.m1.5.5.5.3.3.2">ğ‘</ci><cn id="S4.Ex1.m1.5.5.5.3.3.3.cmml" type="integer" xref="S4.Ex1.m1.5.5.5.3.3.3">1</cn></apply></apply><apply id="S4.Ex1.m1.5.5.5.2.cmml" xref="S4.Ex1.m1.5.5.5.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.5.5.5.2.2.cmml" xref="S4.Ex1.m1.5.5.5.2">superscript</csymbol><apply id="S4.Ex1.m1.5.5.5.2.1.2.cmml" xref="S4.Ex1.m1.5.5.5.2.1.1"><abs id="S4.Ex1.m1.5.5.5.2.1.2.1.cmml" xref="S4.Ex1.m1.5.5.5.2.1.1.2"></abs><apply id="S4.Ex1.m1.5.5.5.2.1.1.1.cmml" xref="S4.Ex1.m1.5.5.5.2.1.1.1"><times id="S4.Ex1.m1.5.5.5.2.1.1.1.1.cmml" xref="S4.Ex1.m1.5.5.5.2.1.1.1.1"></times><apply id="S4.Ex1.m1.5.5.5.2.1.1.1.2.cmml" xref="S4.Ex1.m1.5.5.5.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.5.5.5.2.1.1.1.2.1.cmml" xref="S4.Ex1.m1.5.5.5.2.1.1.1.2">subscript</csymbol><ci id="S4.Ex1.m1.5.5.5.2.1.1.1.2.2.cmml" xref="S4.Ex1.m1.5.5.5.2.1.1.1.2.2">ğ²</ci><ci id="S4.Ex1.m1.5.5.5.2.1.1.1.2.3.cmml" xref="S4.Ex1.m1.5.5.5.2.1.1.1.2.3">ğ‘</ci></apply><apply id="S4.Ex1.m1.5.5.5.2.1.1.1.3.1.cmml" xref="S4.Ex1.m1.5.5.5.2.1.1.1.3.2"><csymbol cd="latexml" id="S4.Ex1.m1.5.5.5.2.1.1.1.3.1.1.cmml" xref="S4.Ex1.m1.5.5.5.2.1.1.1.3.2.1">delimited-[]</csymbol><ci id="S4.Ex1.m1.4.4.4.1.cmml" xref="S4.Ex1.m1.4.4.4.1">ğ‘›</ci></apply></apply></apply><cn id="S4.Ex1.m1.5.5.5.2.3.cmml" type="integer" xref="S4.Ex1.m1.5.5.5.2.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.6c">{\mathrm{ESR}}=\frac{\sum_{n=0}^{N-1}\left|\mathbf{y}_{p}[n]-\widehat{\mathbf{%
y}}_{p}[n]\right|^{2}}{\sum_{n=0}^{N-1}\left|\mathbf{y}_{p}[n]\right|^{2}}\,.</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m1.6d">roman_ESR = divide start_ARG âˆ‘ start_POSTSUBSCRIPT italic_n = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N - 1 end_POSTSUPERSCRIPT | bold_y start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT [ italic_n ] - over^ start_ARG bold_y end_ARG start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT [ italic_n ] | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG âˆ‘ start_POSTSUBSCRIPT italic_n = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N - 1 end_POSTSUPERSCRIPT | bold_y start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT [ italic_n ] | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p2.4"><span class="ltx_text" id="S4.SS2.p2.4.1" style="font-size:90%;">The denominator of target signal itself is used to prevent the high energy segments from dominating the result.</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.2"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.2.1" style="font-size:90%;">Mel-spectrum loss</span><span class="ltx_text" id="S4.SS2.p3.2.2" style="font-size:90%;"> (</span><math alttext="L1_{mel}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" mathsize="90%" xref="S4.SS2.p3.1.m1.1.1.2.cmml">L</mi><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><msub id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml"><mn id="S4.SS2.p3.1.m1.1.1.3.2" mathsize="90%" xref="S4.SS2.p3.1.m1.1.1.3.2.cmml">1</mn><mrow id="S4.SS2.p3.1.m1.1.1.3.3" xref="S4.SS2.p3.1.m1.1.1.3.3.cmml"><mi id="S4.SS2.p3.1.m1.1.1.3.3.2" mathsize="90%" xref="S4.SS2.p3.1.m1.1.1.3.3.2.cmml">m</mi><mo id="S4.SS2.p3.1.m1.1.1.3.3.1" xref="S4.SS2.p3.1.m1.1.1.3.3.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.3.3.3" mathsize="90%" xref="S4.SS2.p3.1.m1.1.1.3.3.3.cmml">e</mi><mo id="S4.SS2.p3.1.m1.1.1.3.3.1a" xref="S4.SS2.p3.1.m1.1.1.3.3.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.3.3.4" mathsize="90%" xref="S4.SS2.p3.1.m1.1.1.3.3.4.cmml">l</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">ğ¿</ci><apply id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3">subscript</csymbol><cn id="S4.SS2.p3.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS2.p3.1.m1.1.1.3.2">1</cn><apply id="S4.SS2.p3.1.m1.1.1.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3"><times id="S4.SS2.p3.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.1"></times><ci id="S4.SS2.p3.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.2">ğ‘š</ci><ci id="S4.SS2.p3.1.m1.1.1.3.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.3">ğ‘’</ci><ci id="S4.SS2.p3.1.m1.1.1.3.3.4.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.4">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">L1_{mel}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_L 1 start_POSTSUBSCRIPT italic_m italic_e italic_l end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS2.p3.2.3" style="font-size:90%;">)

measures the difference between the ground-truth and predicted audio in the spectral domain. Denoting </span><math alttext="\phi(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.2" xref="S4.SS2.p3.2.m2.1.2.cmml"><mi id="S4.SS2.p3.2.m2.1.2.2" mathsize="90%" xref="S4.SS2.p3.2.m2.1.2.2.cmml">Ï•</mi><mo id="S4.SS2.p3.2.m2.1.2.1" xref="S4.SS2.p3.2.m2.1.2.1.cmml">â¢</mo><mrow id="S4.SS2.p3.2.m2.1.2.3.2" xref="S4.SS2.p3.2.m2.1.2.cmml"><mo id="S4.SS2.p3.2.m2.1.2.3.2.1" maxsize="90%" minsize="90%" xref="S4.SS2.p3.2.m2.1.2.cmml">(</mo><mo id="S4.SS2.p3.2.m2.1.1" lspace="0em" mathsize="90%" rspace="0em" xref="S4.SS2.p3.2.m2.1.1.cmml">â‹…</mo><mo id="S4.SS2.p3.2.m2.1.2.3.2.2" maxsize="90%" minsize="90%" xref="S4.SS2.p3.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.2.cmml" xref="S4.SS2.p3.2.m2.1.2"><times id="S4.SS2.p3.2.m2.1.2.1.cmml" xref="S4.SS2.p3.2.m2.1.2.1"></times><ci id="S4.SS2.p3.2.m2.1.2.2.cmml" xref="S4.SS2.p3.2.m2.1.2.2">italic-Ï•</ci><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\phi(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">italic_Ï• ( â‹… )</annotation></semantics></math><span class="ltx_text" id="S4.SS2.p3.2.4" style="font-size:90%;"> as the function that converts an audio waveform into a Mel spectrogram, this loss can be calculated as follows,</span></p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L1_{{mel}}=\mathbb{E}_{(\mathbf{x})}\left[\|\phi(\mathbf{x})-\phi(\hat{\mathbf%
{x}})\|_{1}\right]\,." class="ltx_Math" display="block" id="S4.Ex2.m1.4"><semantics id="S4.Ex2.m1.4a"><mrow id="S4.Ex2.m1.4.4.1" xref="S4.Ex2.m1.4.4.1.1.cmml"><mrow id="S4.Ex2.m1.4.4.1.1" xref="S4.Ex2.m1.4.4.1.1.cmml"><mrow id="S4.Ex2.m1.4.4.1.1.3" xref="S4.Ex2.m1.4.4.1.1.3.cmml"><mi id="S4.Ex2.m1.4.4.1.1.3.2" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.3.2.cmml">L</mi><mo id="S4.Ex2.m1.4.4.1.1.3.1" xref="S4.Ex2.m1.4.4.1.1.3.1.cmml">â¢</mo><msub id="S4.Ex2.m1.4.4.1.1.3.3" xref="S4.Ex2.m1.4.4.1.1.3.3.cmml"><mn id="S4.Ex2.m1.4.4.1.1.3.3.2" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.3.3.2.cmml">1</mn><mrow id="S4.Ex2.m1.4.4.1.1.3.3.3" xref="S4.Ex2.m1.4.4.1.1.3.3.3.cmml"><mi id="S4.Ex2.m1.4.4.1.1.3.3.3.2" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.3.3.3.2.cmml">m</mi><mo id="S4.Ex2.m1.4.4.1.1.3.3.3.1" xref="S4.Ex2.m1.4.4.1.1.3.3.3.1.cmml">â¢</mo><mi id="S4.Ex2.m1.4.4.1.1.3.3.3.3" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.3.3.3.3.cmml">e</mi><mo id="S4.Ex2.m1.4.4.1.1.3.3.3.1a" xref="S4.Ex2.m1.4.4.1.1.3.3.3.1.cmml">â¢</mo><mi id="S4.Ex2.m1.4.4.1.1.3.3.3.4" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.3.3.3.4.cmml">l</mi></mrow></msub></mrow><mo id="S4.Ex2.m1.4.4.1.1.2" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.2.cmml">=</mo><mrow id="S4.Ex2.m1.4.4.1.1.1" xref="S4.Ex2.m1.4.4.1.1.1.cmml"><msub id="S4.Ex2.m1.4.4.1.1.1.3" xref="S4.Ex2.m1.4.4.1.1.1.3.cmml"><mi id="S4.Ex2.m1.4.4.1.1.1.3.2" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.3.2.cmml">ğ”¼</mi><mrow id="S4.Ex2.m1.1.1.1.3" xref="S4.Ex2.m1.4.4.1.1.1.3.cmml"><mo id="S4.Ex2.m1.1.1.1.3.1" maxsize="90%" minsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.3.cmml">(</mo><mi id="S4.Ex2.m1.1.1.1.1" mathsize="90%" xref="S4.Ex2.m1.1.1.1.1.cmml">ğ±</mi><mo id="S4.Ex2.m1.1.1.1.3.2" maxsize="90%" minsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.3.cmml">)</mo></mrow></msub><mo id="S4.Ex2.m1.4.4.1.1.1.2" xref="S4.Ex2.m1.4.4.1.1.1.2.cmml">â¢</mo><mrow id="S4.Ex2.m1.4.4.1.1.1.1.1" xref="S4.Ex2.m1.4.4.1.1.1.1.2.cmml"><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.2" xref="S4.Ex2.m1.4.4.1.1.1.1.2.1.cmml">[</mo><msub id="S4.Ex2.m1.4.4.1.1.1.1.1.1" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml">Ï•</mi><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.1" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml">â¢</mo><mrow id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.2" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.2.1" maxsize="90%" minsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S4.Ex2.m1.2.2" mathsize="90%" xref="S4.Ex2.m1.2.2.cmml">ğ±</mi><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.2.2" maxsize="90%" minsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.cmml">Ï•</mi><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.1" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.1.cmml">â¢</mo><mrow id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.2" xref="S4.Ex2.m1.3.3.cmml"><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.2.1" maxsize="90%" minsize="90%" xref="S4.Ex2.m1.3.3.cmml">(</mo><mover accent="true" id="S4.Ex2.m1.3.3" xref="S4.Ex2.m1.3.3.cmml"><mi id="S4.Ex2.m1.3.3.2" mathsize="90%" xref="S4.Ex2.m1.3.3.2.cmml">ğ±</mi><mo id="S4.Ex2.m1.3.3.1" mathsize="90%" xref="S4.Ex2.m1.3.3.1.cmml">^</mo></mover><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.2.2" maxsize="90%" minsize="90%" xref="S4.Ex2.m1.3.3.cmml">)</mo></mrow></mrow></mrow><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S4.Ex2.m1.4.4.1.1.1.1.1.1.3" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.Ex2.m1.4.4.1.1.1.1.1.3" xref="S4.Ex2.m1.4.4.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S4.Ex2.m1.4.4.1.2" lspace="0.170em" mathsize="90%" xref="S4.Ex2.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex2.m1.4b"><apply id="S4.Ex2.m1.4.4.1.1.cmml" xref="S4.Ex2.m1.4.4.1"><eq id="S4.Ex2.m1.4.4.1.1.2.cmml" xref="S4.Ex2.m1.4.4.1.1.2"></eq><apply id="S4.Ex2.m1.4.4.1.1.3.cmml" xref="S4.Ex2.m1.4.4.1.1.3"><times id="S4.Ex2.m1.4.4.1.1.3.1.cmml" xref="S4.Ex2.m1.4.4.1.1.3.1"></times><ci id="S4.Ex2.m1.4.4.1.1.3.2.cmml" xref="S4.Ex2.m1.4.4.1.1.3.2">ğ¿</ci><apply id="S4.Ex2.m1.4.4.1.1.3.3.cmml" xref="S4.Ex2.m1.4.4.1.1.3.3"><csymbol cd="ambiguous" id="S4.Ex2.m1.4.4.1.1.3.3.1.cmml" xref="S4.Ex2.m1.4.4.1.1.3.3">subscript</csymbol><cn id="S4.Ex2.m1.4.4.1.1.3.3.2.cmml" type="integer" xref="S4.Ex2.m1.4.4.1.1.3.3.2">1</cn><apply id="S4.Ex2.m1.4.4.1.1.3.3.3.cmml" xref="S4.Ex2.m1.4.4.1.1.3.3.3"><times id="S4.Ex2.m1.4.4.1.1.3.3.3.1.cmml" xref="S4.Ex2.m1.4.4.1.1.3.3.3.1"></times><ci id="S4.Ex2.m1.4.4.1.1.3.3.3.2.cmml" xref="S4.Ex2.m1.4.4.1.1.3.3.3.2">ğ‘š</ci><ci id="S4.Ex2.m1.4.4.1.1.3.3.3.3.cmml" xref="S4.Ex2.m1.4.4.1.1.3.3.3.3">ğ‘’</ci><ci id="S4.Ex2.m1.4.4.1.1.3.3.3.4.cmml" xref="S4.Ex2.m1.4.4.1.1.3.3.3.4">ğ‘™</ci></apply></apply></apply><apply id="S4.Ex2.m1.4.4.1.1.1.cmml" xref="S4.Ex2.m1.4.4.1.1.1"><times id="S4.Ex2.m1.4.4.1.1.1.2.cmml" xref="S4.Ex2.m1.4.4.1.1.1.2"></times><apply id="S4.Ex2.m1.4.4.1.1.1.3.cmml" xref="S4.Ex2.m1.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex2.m1.4.4.1.1.1.3.1.cmml" xref="S4.Ex2.m1.4.4.1.1.1.3">subscript</csymbol><ci id="S4.Ex2.m1.4.4.1.1.1.3.2.cmml" xref="S4.Ex2.m1.4.4.1.1.1.3.2">ğ”¼</ci><ci id="S4.Ex2.m1.1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1.1">ğ±</ci></apply><apply id="S4.Ex2.m1.4.4.1.1.1.1.2.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S4.Ex2.m1.4.4.1.1.1.1.2.1.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.Ex2.m1.4.4.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1">subscript</csymbol><apply id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1"><minus id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2"><times id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.1"></times><ci id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2">italic-Ï•</ci><ci id="S4.Ex2.m1.2.2.cmml" xref="S4.Ex2.m1.2.2">ğ±</ci></apply><apply id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3"><times id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2">italic-Ï•</ci><apply id="S4.Ex2.m1.3.3.cmml" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.2"><ci id="S4.Ex2.m1.3.3.1.cmml" xref="S4.Ex2.m1.3.3.1">^</ci><ci id="S4.Ex2.m1.3.3.2.cmml" xref="S4.Ex2.m1.3.3.2">ğ±</ci></apply></apply></apply></apply><cn id="S4.Ex2.m1.4.4.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.Ex2.m1.4.4.1.1.1.1.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex2.m1.4c">L1_{{mel}}=\mathbb{E}_{(\mathbf{x})}\left[\|\phi(\mathbf{x})-\phi(\hat{\mathbf%
{x}})\|_{1}\right]\,.</annotation><annotation encoding="application/x-llamapun" id="S4.Ex2.m1.4d">italic_L 1 start_POSTSUBSCRIPT italic_m italic_e italic_l end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT ( bold_x ) end_POSTSUBSCRIPT [ âˆ¥ italic_Ï• ( bold_x ) - italic_Ï• ( over^ start_ARG bold_x end_ARG ) âˆ¥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p3.3"><span class="ltx_text" id="S4.SS2.p3.3.1" style="font-size:90%;color:#000000;">Since the characteristics of certain effects such as overdrive and distortion are more easily observed from the perspective of spectral domain, we conjecture that Mel-spectrum loss offers a more suitable measure than ESR for evaluating the performance of modeling overdrive and distortion.</span><span class="ltx_text" id="S4.SS2.p3.3.2" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1" style="font-size:90%;">FrÃ©chet Audio Distance</span><span class="ltx_text" id="S4.SS2.p4.1.2" style="font-size:90%;"> (FAD) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p4.1.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib32" title="">32</a><span class="ltx_text" id="S4.SS2.p4.1.4.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS2.p4.1.5" style="font-size:90%;"> measures
the FrÃ©chet distance between the distribution of embedding from a set of reference audios and those from the generated audios.
It was first proposed to evaluate a music enhancement task and was found to correlate well with human perception. The metric has been later applied to music generation tasks (e.g., </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p4.1.6.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib35" title="">35</a><span class="ltx_text" id="S4.SS2.p4.1.7.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS2.p4.1.8" style="font-size:90%;">) to indicate if the generated audio is plausible. We accordingly adopt it here as well.
To provide different insights than traditional alignment-based metrics, we report the FAD</span><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>To compute FAD, we use an open-source implementation: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/gudgud96/frechet-audio-distance" title="">https://github.com/gudgud96/frechet-audio-distance</a></span></span></span><span class="ltx_text" id="S4.SS2.p4.1.9" style="font-size:90%;"> score of all models with the VGGish model. Samples with a low FAD score are expected to be more plausible.
</span><span class="ltx_text" id="S4.SS2.p4.1.10" style="font-size:90%;color:#000000;">We note that there might be better alternatives than the VGGish model for computing the FAD scores <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib36" title="">36</a>]</cite>, but we leave that as a future work.</span><span class="ltx_text" id="S4.SS2.p4.1.11" style="font-size:90%;"></span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Implementation details</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text" id="S4.SS3.p1.1.1" style="font-size:90%;">For detailed information on the generator and discriminator, please refer to Sections </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S3.SS1" style="font-size:90%;" title="3.1 Generator â€£ 3 Methods â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">3.1</span></a><span class="ltx_text" id="S4.SS3.p1.1.2" style="font-size:90%;"> and </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S3.SS2" style="font-size:90%;" title="3.2 Discriminator â€£ 3 Methods â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">3.2</span></a><span class="ltx_text" id="S4.SS3.p1.1.3" style="font-size:90%;">. It is important to note that we use the same structures and settings for all generator models, including the supervised method, for fair comparison.</span></p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text" id="S4.SS3.p2.1.1" style="font-size:90%;">During training, we split every audio into two-second segments. Each model is trained using a single RTX 3090 GPU. The supervised and MelGAN discriminator models are trained with the settings described in their original papers.</span></p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text" id="S4.SS3.p3.1.1" style="font-size:90%;">For our discriminators, MSD and MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mo id="S4.SS3.p3.1.m1.1.1" mathsize="90%" xref="S4.SS3.p3.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><plus id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S4.SS3.p3.1.2" style="font-size:90%;">MPD, we employ the AdamW </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p3.1.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib37" title="">37</a><span class="ltx_text" id="S4.SS3.p3.1.4.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS3.p3.1.5" style="font-size:90%;"> optimizer with an initial generator learning rate of 5eâ€“5 and discriminator learning rate of 1eâ€“5</span><span class="ltx_text" id="S4.SS3.p3.1.6" style="font-size:90%;color:#000000;">, along with a weight decay of 0.01.</span><span class="ltx_text" id="S4.SS3.p3.1.7" style="font-size:90%;"> We set the generator learning rate empirically to a higher value to prevent an imbalance in the training process between the generator and discriminator, as our generator model sizes are much smaller than those of the proposed discriminators.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluation settings</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1"><span class="ltx_text" id="S4.SS4.p1.1.1" style="font-size:90%;">We conduct two experimental scenarios to evaluate the introduced three metrics mentioned in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.SS2" style="font-size:90%;" title="4.2 Metrics â€£ 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">4.2</span></a><span class="ltx_text" id="S4.SS4.p1.1.2" style="font-size:90%;">.
First, we compare our method with the supervised approach of DamskÃ¤gg </span><em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.1.3" style="font-size:90%;">et al.</em><span class="ltx_text" id="S4.SS4.p1.1.4" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS4.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a><span class="ltx_text" id="S4.SS4.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS4.p1.1.7" style="font-size:90%;"> and the unsupervised GAN-based approach proposed by Wright </span><em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.1.8" style="font-size:90%;">et al.</em><span class="ltx_text" id="S4.SS4.p1.1.9" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS4.p1.1.10.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a><span class="ltx_text" id="S4.SS4.p1.1.11.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS4.p1.1.12" style="font-size:90%;">.
Second, to validate the advantage of unsupervised learning, we combine the clean audio from EGDB and EGFxset as the input to our generator (marked as â€œbothâ€ in TableÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.T4" style="font-size:90%;" title="Table 4 â€£ 4.4 Evaluation settings â€£ 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S4.SS4.p1.1.13" style="font-size:90%;">), no matter whether the target tone is from EGDB or EGFxset.
The generator for all the aforementioned models used the same architecture; the difference in each model setting lies in the training method and the loss functions applied (e.g., supervised or adversarial).</span></p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T4.11" style="width:433.6pt;height:407.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(53.8pt,-50.5pt) scale(1.33027386366679,1.33027386366679) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.11.11">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S4.T4.3.3.3.4"><span class="ltx_text" id="S4.T4.3.3.3.4.1" style="font-size:90%;">Target tone</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T4.3.3.3.5"><span class="ltx_text" id="S4.T4.3.3.3.5.1" style="font-size:90%;">Input</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S4.T4.3.3.3.6"><span class="ltx_text" id="S4.T4.3.3.3.6.1" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T4.1.1.1.1"><math alttext="L1_{{mel}}\downarrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><mrow id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml"><mrow id="S4.T4.1.1.1.1.m1.1.1.2" xref="S4.T4.1.1.1.1.m1.1.1.2.cmml"><mi id="S4.T4.1.1.1.1.m1.1.1.2.2" mathsize="90%" xref="S4.T4.1.1.1.1.m1.1.1.2.2.cmml">L</mi><mo id="S4.T4.1.1.1.1.m1.1.1.2.1" xref="S4.T4.1.1.1.1.m1.1.1.2.1.cmml">â¢</mo><msub id="S4.T4.1.1.1.1.m1.1.1.2.3" xref="S4.T4.1.1.1.1.m1.1.1.2.3.cmml"><mn id="S4.T4.1.1.1.1.m1.1.1.2.3.2" mathsize="90%" xref="S4.T4.1.1.1.1.m1.1.1.2.3.2.cmml">1</mn><mrow id="S4.T4.1.1.1.1.m1.1.1.2.3.3" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3.cmml"><mi id="S4.T4.1.1.1.1.m1.1.1.2.3.3.2" mathsize="90%" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3.2.cmml">m</mi><mo id="S4.T4.1.1.1.1.m1.1.1.2.3.3.1" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3.1.cmml">â¢</mo><mi id="S4.T4.1.1.1.1.m1.1.1.2.3.3.3" mathsize="90%" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3.3.cmml">e</mi><mo id="S4.T4.1.1.1.1.m1.1.1.2.3.3.1a" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3.1.cmml">â¢</mo><mi id="S4.T4.1.1.1.1.m1.1.1.2.3.3.4" mathsize="90%" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3.4.cmml">l</mi></mrow></msub></mrow><mo id="S4.T4.1.1.1.1.m1.1.1.1" mathsize="90%" stretchy="false" xref="S4.T4.1.1.1.1.m1.1.1.1.cmml">â†“</mo><mi id="S4.T4.1.1.1.1.m1.1.1.3" xref="S4.T4.1.1.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1"><ci id="S4.T4.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1.1">â†“</ci><apply id="S4.T4.1.1.1.1.m1.1.1.2.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2"><times id="S4.T4.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.1"></times><ci id="S4.T4.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.2">ğ¿</ci><apply id="S4.T4.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S4.T4.1.1.1.1.m1.1.1.2.3.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.3">subscript</csymbol><cn id="S4.T4.1.1.1.1.m1.1.1.2.3.2.cmml" type="integer" xref="S4.T4.1.1.1.1.m1.1.1.2.3.2">1</cn><apply id="S4.T4.1.1.1.1.m1.1.1.2.3.3.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3"><times id="S4.T4.1.1.1.1.m1.1.1.2.3.3.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3.1"></times><ci id="S4.T4.1.1.1.1.m1.1.1.2.3.3.2.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3.2">ğ‘š</ci><ci id="S4.T4.1.1.1.1.m1.1.1.2.3.3.3.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3.3">ğ‘’</ci><ci id="S4.T4.1.1.1.1.m1.1.1.2.3.3.4.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.3.3.4">ğ‘™</ci></apply></apply></apply><csymbol cd="latexml" id="S4.T4.1.1.1.1.m1.1.1.3.cmml" xref="S4.T4.1.1.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">L1_{{mel}}\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">italic_L 1 start_POSTSUBSCRIPT italic_m italic_e italic_l end_POSTSUBSCRIPT â†“</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T4.2.2.2.2">
<span class="ltx_text" id="S4.T4.2.2.2.2.1" style="font-size:90%;">ESR</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T4.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.2.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T4.3.3.3.3">
<span class="ltx_text" id="S4.T4.3.3.3.3.1" style="font-size:90%;">FAD</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.3.3.3.3.m1.1"><semantics id="S4.T4.3.3.3.3.m1.1a"><mo id="S4.T4.3.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T4.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.3.m1.1b"><ci id="S4.T4.3.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.11.11.12.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.11.11.12.1.1"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.12.1.2"><span class="ltx_text" id="S4.T4.11.11.12.1.2.1" style="font-size:90%;">EGFxset</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.11.11.12.1.3">
<span class="ltx_text" id="S4.T4.11.11.12.1.3.1" style="font-size:90%;">SupervisedÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T4.11.11.12.1.3.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a><span class="ltx_text" id="S4.T4.11.11.12.1.3.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.12.1.4"><span class="ltx_text" id="S4.T4.11.11.12.1.4.1" style="font-size:90%;">4.041</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.12.1.5"><span class="ltx_text" id="S4.T4.11.11.12.1.5.1" style="font-size:90%;">0.106</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.12.1.6"><span class="ltx_text" id="S4.T4.11.11.12.1.6.1" style="font-size:90%;">5.256</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.11.13.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.11.11.13.2.1"><span class="ltx_text" id="S4.T4.11.11.13.2.1.1" style="font-size:90%;">BD-2</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.13.2.2"><span class="ltx_text" id="S4.T4.11.11.13.2.2.1" style="font-size:90%;">EGFxset</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.11.11.13.2.3"><span class="ltx_text" id="S4.T4.11.11.13.2.3.1" style="font-size:90%;">MSD</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.13.2.4"><span class="ltx_text" id="S4.T4.11.11.13.2.4.1" style="font-size:90%;">1.874</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.13.2.5"><span class="ltx_text" id="S4.T4.11.11.13.2.5.1" style="font-size:90%;">0.164</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.13.2.6"><span class="ltx_text" id="S4.T4.11.11.13.2.6.1" style="font-size:90%;">1.900</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.4.4.4.2"><span class="ltx_text" id="S4.T4.4.4.4.2.1" style="font-size:90%;">(EGFxset)</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.4.4.3"><span class="ltx_text" id="S4.T4.4.4.4.3.1" style="font-size:90%;">EGFxset</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.4.4.4.1">
<span class="ltx_text" id="S4.T4.4.4.4.1.1" style="font-size:90%;">MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S4.T4.4.4.4.1.m1.1"><semantics id="S4.T4.4.4.4.1.m1.1a"><mo id="S4.T4.4.4.4.1.m1.1.1" mathsize="90%" xref="S4.T4.4.4.4.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.1.m1.1b"><plus id="S4.T4.4.4.4.1.m1.1.1.cmml" xref="S4.T4.4.4.4.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S4.T4.4.4.4.1.2" style="font-size:90%;">MPD</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.4.4.4"><span class="ltx_text" id="S4.T4.4.4.4.4.1" style="font-size:90%;">1.535</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.4.4.5"><span class="ltx_text" id="S4.T4.4.4.4.5.1" style="font-size:90%;">0.052</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.4.4.6"><span class="ltx_text" id="S4.T4.4.4.4.6.1" style="font-size:90%;">0.983</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.5.5">
<td class="ltx_td ltx_border_r" id="S4.T4.5.5.5.2"></td>
<td class="ltx_td ltx_align_left" id="S4.T4.5.5.5.3"><span class="ltx_text" id="S4.T4.5.5.5.3.1" style="font-size:90%;">both</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.5.5.5.1">
<span class="ltx_text" id="S4.T4.5.5.5.1.1" style="font-size:90%;">MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S4.T4.5.5.5.1.m1.1"><semantics id="S4.T4.5.5.5.1.m1.1a"><mo id="S4.T4.5.5.5.1.m1.1.1" mathsize="90%" xref="S4.T4.5.5.5.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.1.m1.1b"><plus id="S4.T4.5.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.5.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S4.T4.5.5.5.1.2" style="font-size:90%;">MPD</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T4.5.5.5.4"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.5.4.1" style="font-size:90%;">1.156</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.5.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.5.5.1" style="font-size:90%;">0.022</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.5.5.5.6"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.5.6.1" style="font-size:90%;">0.550</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.11.14.3">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.11.11.14.3.1"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.14.3.2"><span class="ltx_text" id="S4.T4.11.11.14.3.2.1" style="font-size:90%;">EGDB</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.11.11.14.3.3">
<span class="ltx_text" id="S4.T4.11.11.14.3.3.1" style="font-size:90%;">SupervisedÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T4.11.11.14.3.3.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a><span class="ltx_text" id="S4.T4.11.11.14.3.3.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.14.3.4"><span class="ltx_text" id="S4.T4.11.11.14.3.4.1" style="font-size:90%;">2.342</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.14.3.5"><span class="ltx_text ltx_font_bold" id="S4.T4.11.11.14.3.5.1" style="font-size:90%;">0.019</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.14.3.6"><span class="ltx_text" id="S4.T4.11.11.14.3.6.1" style="font-size:90%;">1.657</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.11.15.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.11.11.15.4.1"><span class="ltx_text" id="S4.T4.11.11.15.4.1.1" style="font-size:90%;">Marshall</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.15.4.2"><span class="ltx_text" id="S4.T4.11.11.15.4.2.1" style="font-size:90%;">EGDB</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.11.11.15.4.3"><span class="ltx_text" id="S4.T4.11.11.15.4.3.1" style="font-size:90%;">MSD</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.15.4.4"><span class="ltx_text" id="S4.T4.11.11.15.4.4.1" style="font-size:90%;">2.660</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.15.4.5"><span class="ltx_text" id="S4.T4.11.11.15.4.5.1" style="font-size:90%;">0.229</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.15.4.6"><span class="ltx_text" id="S4.T4.11.11.15.4.6.1" style="font-size:90%;">1.410</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.6.6.6.2"><span class="ltx_text" id="S4.T4.6.6.6.2.1" style="font-size:90%;">(EGDB)</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.6.6.6.3"><span class="ltx_text" id="S4.T4.6.6.6.3.1" style="font-size:90%;">EGDB</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.6.6.6.1">
<span class="ltx_text" id="S4.T4.6.6.6.1.1" style="font-size:90%;">MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S4.T4.6.6.6.1.m1.1"><semantics id="S4.T4.6.6.6.1.m1.1a"><mo id="S4.T4.6.6.6.1.m1.1.1" mathsize="90%" xref="S4.T4.6.6.6.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.1.m1.1b"><plus id="S4.T4.6.6.6.1.m1.1.1.cmml" xref="S4.T4.6.6.6.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.6.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S4.T4.6.6.6.1.2" style="font-size:90%;">MPD</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T4.6.6.6.4"><span class="ltx_text ltx_font_bold" id="S4.T4.6.6.6.4.1" style="font-size:90%;">2.315</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.6.6.6.5"><span class="ltx_text" id="S4.T4.6.6.6.5.1" style="font-size:90%;">0.028</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.6.6.6.6"><span class="ltx_text ltx_font_bold" id="S4.T4.6.6.6.6.1" style="font-size:90%;">0.994</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.7.7">
<td class="ltx_td ltx_border_r" id="S4.T4.7.7.7.2"></td>
<td class="ltx_td ltx_align_left" id="S4.T4.7.7.7.3"><span class="ltx_text" id="S4.T4.7.7.7.3.1" style="font-size:90%;">both</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.7.7.7.1">
<span class="ltx_text" id="S4.T4.7.7.7.1.1" style="font-size:90%;">MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S4.T4.7.7.7.1.m1.1"><semantics id="S4.T4.7.7.7.1.m1.1a"><mo id="S4.T4.7.7.7.1.m1.1.1" mathsize="90%" xref="S4.T4.7.7.7.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.1.m1.1b"><plus id="S4.T4.7.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.7.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.7.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S4.T4.7.7.7.1.2" style="font-size:90%;">MPD</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T4.7.7.7.4"><span class="ltx_text" id="S4.T4.7.7.7.4.1" style="font-size:90%;">2.458</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.7.7.7.5"><span class="ltx_text" id="S4.T4.7.7.7.5.1" style="font-size:90%;">0.029</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.7.7.7.6"><span class="ltx_text" id="S4.T4.7.7.7.6.1" style="font-size:90%;">1.054</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.11.16.5">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.11.11.16.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.16.5.2"><span class="ltx_text" id="S4.T4.11.11.16.5.2.1" style="font-size:90%;">EGDB</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.11.11.16.5.3">
<span class="ltx_text" id="S4.T4.11.11.16.5.3.1" style="font-size:90%;">SupervisedÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T4.11.11.16.5.3.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a><span class="ltx_text" id="S4.T4.11.11.16.5.3.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.16.5.4"><span class="ltx_text ltx_font_bold" id="S4.T4.11.11.16.5.4.1" style="font-size:90%;">1.953</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.16.5.5"><span class="ltx_text ltx_font_bold" id="S4.T4.11.11.16.5.5.1" style="font-size:90%;">0.014</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.16.5.6"><span class="ltx_text" id="S4.T4.11.11.16.5.6.1" style="font-size:90%;">1.126</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.11.17.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.11.11.17.6.1"><span class="ltx_text" id="S4.T4.11.11.17.6.1.1" style="font-size:90%;">FTwin</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.17.6.2"><span class="ltx_text" id="S4.T4.11.11.17.6.2.1" style="font-size:90%;">EGDB</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.11.11.17.6.3"><span class="ltx_text" id="S4.T4.11.11.17.6.3.1" style="font-size:90%;">MSD</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.17.6.4"><span class="ltx_text" id="S4.T4.11.11.17.6.4.1" style="font-size:90%;">2.302</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.17.6.5"><span class="ltx_text" id="S4.T4.11.11.17.6.5.1" style="font-size:90%;">0.072</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.17.6.6"><span class="ltx_text" id="S4.T4.11.11.17.6.6.1" style="font-size:90%;">0.878</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.8.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.8.8.8.2"><span class="ltx_text" id="S4.T4.8.8.8.2.1" style="font-size:90%;">(EGDB)</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.8.8.8.3"><span class="ltx_text" id="S4.T4.8.8.8.3.1" style="font-size:90%;">EGDB</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.8.8.8.1">
<span class="ltx_text" id="S4.T4.8.8.8.1.1" style="font-size:90%;">MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S4.T4.8.8.8.1.m1.1"><semantics id="S4.T4.8.8.8.1.m1.1a"><mo id="S4.T4.8.8.8.1.m1.1.1" mathsize="90%" xref="S4.T4.8.8.8.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.1.m1.1b"><plus id="S4.T4.8.8.8.1.m1.1.1.cmml" xref="S4.T4.8.8.8.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.T4.8.8.8.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S4.T4.8.8.8.1.2" style="font-size:90%;">MPD</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T4.8.8.8.4"><span class="ltx_text" id="S4.T4.8.8.8.4.1" style="font-size:90%;">1.960</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.8.8.8.5"><span class="ltx_text" id="S4.T4.8.8.8.5.1" style="font-size:90%;">0.021</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.8.8.8.6"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.8.6.1" style="font-size:90%;">0.346</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.9.9.9">
<td class="ltx_td ltx_border_r" id="S4.T4.9.9.9.2"></td>
<td class="ltx_td ltx_align_left" id="S4.T4.9.9.9.3"><span class="ltx_text" id="S4.T4.9.9.9.3.1" style="font-size:90%;">both</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.9.9.9.1">
<span class="ltx_text" id="S4.T4.9.9.9.1.1" style="font-size:90%;">MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S4.T4.9.9.9.1.m1.1"><semantics id="S4.T4.9.9.9.1.m1.1a"><mo id="S4.T4.9.9.9.1.m1.1.1" mathsize="90%" xref="S4.T4.9.9.9.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.9.1.m1.1b"><plus id="S4.T4.9.9.9.1.m1.1.1.cmml" xref="S4.T4.9.9.9.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.9.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.T4.9.9.9.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S4.T4.9.9.9.1.2" style="font-size:90%;">MPD</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T4.9.9.9.4"><span class="ltx_text" id="S4.T4.9.9.9.4.1" style="font-size:90%;">2.267</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.9.9.9.5"><span class="ltx_text" id="S4.T4.9.9.9.5.1" style="font-size:90%;">0.020</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.9.9.9.6"><span class="ltx_text" id="S4.T4.9.9.9.6.1" style="font-size:90%;">0.434</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.11.18.7">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.11.11.18.7.1"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.18.7.2"><span class="ltx_text" id="S4.T4.11.11.18.7.2.1" style="font-size:90%;">EGDB</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.11.11.18.7.3">
<span class="ltx_text" id="S4.T4.11.11.18.7.3.1" style="font-size:90%;">SupervisedÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T4.11.11.18.7.3.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a><span class="ltx_text" id="S4.T4.11.11.18.7.3.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.18.7.4"><span class="ltx_text" id="S4.T4.11.11.18.7.4.1" style="font-size:90%;">1.705</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.18.7.5"><span class="ltx_text ltx_font_bold" id="S4.T4.11.11.18.7.5.1" style="font-size:90%;">0.012</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.11.18.7.6"><span class="ltx_text" id="S4.T4.11.11.18.7.6.1" style="font-size:90%;">1.923</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.11.19.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.11.11.19.8.1"><span class="ltx_text" id="S4.T4.11.11.19.8.1.1" style="font-size:90%;">Mesa</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.19.8.2"><span class="ltx_text" id="S4.T4.11.11.19.8.2.1" style="font-size:90%;">EGDB</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.11.11.19.8.3"><span class="ltx_text" id="S4.T4.11.11.19.8.3.1" style="font-size:90%;">MSD</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.19.8.4"><span class="ltx_text" id="S4.T4.11.11.19.8.4.1" style="font-size:90%;">2.158</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.19.8.5"><span class="ltx_text" id="S4.T4.11.11.19.8.5.1" style="font-size:90%;">0.137</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.19.8.6"><span class="ltx_text" id="S4.T4.11.11.19.8.6.1" style="font-size:90%;">1.674</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.10.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.10.10.10.2"><span class="ltx_text" id="S4.T4.10.10.10.2.1" style="font-size:90%;">(EGDB)</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.10.10.10.3"><span class="ltx_text" id="S4.T4.10.10.10.3.1" style="font-size:90%;">EGDB</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.10.10.10.1">
<span class="ltx_text" id="S4.T4.10.10.10.1.1" style="font-size:90%;">MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S4.T4.10.10.10.1.m1.1"><semantics id="S4.T4.10.10.10.1.m1.1a"><mo id="S4.T4.10.10.10.1.m1.1.1" mathsize="90%" xref="S4.T4.10.10.10.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.1.m1.1b"><plus id="S4.T4.10.10.10.1.m1.1.1.cmml" xref="S4.T4.10.10.10.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.T4.10.10.10.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S4.T4.10.10.10.1.2" style="font-size:90%;">MPD</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T4.10.10.10.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.10.10.4.1" style="font-size:90%;">1.633</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.10.10.10.5"><span class="ltx_text" id="S4.T4.10.10.10.5.1" style="font-size:90%;">0.014</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.10.10.10.6"><span class="ltx_text" id="S4.T4.10.10.10.6.1" style="font-size:90%;">1.748</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.11.11">
<td class="ltx_td ltx_border_r" id="S4.T4.11.11.11.2"></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.11.3"><span class="ltx_text" id="S4.T4.11.11.11.3.1" style="font-size:90%;">both</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.11.11.11.1">
<span class="ltx_text" id="S4.T4.11.11.11.1.1" style="font-size:90%;">MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S4.T4.11.11.11.1.m1.1"><semantics id="S4.T4.11.11.11.1.m1.1a"><mo id="S4.T4.11.11.11.1.m1.1.1" mathsize="90%" xref="S4.T4.11.11.11.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.11.1.m1.1b"><plus id="S4.T4.11.11.11.1.m1.1.1.cmml" xref="S4.T4.11.11.11.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.11.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.T4.11.11.11.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S4.T4.11.11.11.1.2" style="font-size:90%;">MPD</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.11.4"><span class="ltx_text" id="S4.T4.11.11.11.4.1" style="font-size:90%;">1.694</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.11.5"><span class="ltx_text" id="S4.T4.11.11.11.5.1" style="font-size:90%;">0.041</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.11.6"><span class="ltx_text ltx_font_bold" id="S4.T4.11.11.11.6.1" style="font-size:90%;">1.400</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Objective evaluation result of the supervised modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a>]</cite> and GAN-based models (i.e., MSD alone and ours) for different target tones (i.e., an extremely high-gain tone from EGFxset and three tones from EGDB), using source signals from different datasets as the model input (i.e., using target-aligned audio, or using â€œbothâ€ target-aligned audio and target-unaligned audio [i.e., EGDB+EFGxset]). All the three objective metrics are the lower the better; best results highlighted in bold.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F2.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="151" id="S4.F2.1.g1" src="extracted/5669049/DAFx24_Templates_LaTeX/Image/BD-2_target.png" width="200"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="151" id="S4.F2.2.g1" src="extracted/5669049/DAFx24_Templates_LaTeX/Image/BD-2_supervised.png" width="200"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F2.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="151" id="S4.F2.3.g1" src="extracted/5669049/DAFx24_Templates_LaTeX/Image/BD-2_msd_mpd.png" width="200"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Mel-spectrogram of the target audio signal, along with the ones generated by the supervised baselineÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a>]</cite> and the proposed MSD<math alttext="+" class="ltx_Math" display="inline" id="S4.F2.5.m1.1"><semantics id="S4.F2.5.m1.1b"><mo id="S4.F2.5.m1.1.1" xref="S4.F2.5.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.F2.5.m1.1c"><plus id="S4.F2.5.m1.1.1.cmml" xref="S4.F2.5.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.5.m1.1d">+</annotation><annotation encoding="application/x-llamapun" id="S4.F2.5.m1.1e">+</annotation></semantics></math>MPD GAN-based model given the corresponding clean audio signal. The target audio is sampled from the test set of EGFxsetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib16" title="">16</a>]</cite>, with BD-2 being the target tone.
We see missing high-frequency harmonics from the top-right corner of the middle Mel-spectrogram, the one generated by the supervised baseline. </figcaption>
</figure>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="151" id="S4.F3.1.g1" src="extracted/5669049/DAFx24_Templates_LaTeX/Image/ftwin_target.png" width="200"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="151" id="S4.F3.2.g1" src="extracted/5669049/DAFx24_Templates_LaTeX/Image/ftwin_supervised.png" width="200"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="151" id="S4.F3.3.g1" src="extracted/5669049/DAFx24_Templates_LaTeX/Image/ftwin_msd_mpd.png" width="200"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The mel-spectrogram between target audio , supervised approach and MSD<math alttext="+" class="ltx_Math" display="inline" id="S4.F3.6.m1.1"><semantics id="S4.F3.6.m1.1b"><mo id="S4.F3.6.m1.1.1" xref="S4.F3.6.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.F3.6.m1.1c"><plus id="S4.F3.6.m1.1.1.cmml" xref="S4.F3.6.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m1.1d">+</annotation><annotation encoding="application/x-llamapun" id="S4.F3.6.m1.1e">+</annotation></semantics></math>MPD sampled from the EGDB Fender test set. Both of supervised baseline and our MSD<math alttext="+" class="ltx_Math" display="inline" id="S4.F3.7.m2.1"><semantics id="S4.F3.7.m2.1b"><mo id="S4.F3.7.m2.1.1" xref="S4.F3.7.m2.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.F3.7.m2.1c"><plus id="S4.F3.7.m2.1.1.cmml" xref="S4.F3.7.m2.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.7.m2.1d">+</annotation><annotation encoding="application/x-llamapun" id="S4.F3.7.m2.1e">+</annotation></semantics></math>MPD exhibit artifacts. These artifacts manifest as the generation of non-existent high-frequency information in the target mel-spectrogram. </figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Result</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison with Baseline Methods</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text" id="S5.SS1.p1.1.1" style="font-size:90%;">Table </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.T4" style="font-size:90%;" title="Table 4 â€£ 4.4 Evaluation settings â€£ 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S5.SS1.p1.1.2" style="font-size:90%;"> shows the results of modeling different target tones.
We consider firstly the case when the input audio are from the same dataset as the target audio, namely using EGFxset input when the target tone is BD-2, and using EGDB input when the target tone is the other three. We will consider the result for the case when using input from â€œbothâ€ EGFxset and EGDB in the next subsection.</span></p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.2"><span class="ltx_text" id="S5.SS1.p2.2.1" style="font-size:90%;color:#000000;">As we address a challenging case with BD-2 as the target tone, which is characterized by a highly distorted tone, we opt for using the MelGAN discriminator as our baseline. This choice is based on its superior performance in handling heavy distortion settings, as demonstrated the experiments of Wright <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.2.1.1">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a>]</cite>, viewing the MelGAN discriminator used by them as a variant of MSD. </span><span class="ltx_text" id="S5.SS1.p2.2.2" style="font-size:90%;">
Table </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.T4" style="font-size:90%;" title="Table 4 â€£ 4.4 Evaluation settings â€£ 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S5.SS1.p2.2.3" style="font-size:90%;"> shows that the proposed GAN-based approach (i.e., MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mo id="S5.SS1.p2.1.m1.1.1" mathsize="90%" xref="S5.SS1.p2.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><plus id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S5.SS1.p2.2.4" style="font-size:90%;">MPD) consistently outperforms the existing GAN-based approach (i.e., MSD) across all the three objective metrics.
Notably, when MPD and MSD were applied in GAN training, these discriminators, originally designed for capturing the diverse periodic patterns, helped the generator produce a more realistic waveform.
Although our GAN-based model does not include any spectral-based discriminators, for </span><math alttext="L1_{mel}" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mrow id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><mi id="S5.SS1.p2.2.m2.1.1.2" mathsize="90%" xref="S5.SS1.p2.2.m2.1.1.2.cmml">L</mi><mo id="S5.SS1.p2.2.m2.1.1.1" xref="S5.SS1.p2.2.m2.1.1.1.cmml">â¢</mo><msub id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml"><mn id="S5.SS1.p2.2.m2.1.1.3.2" mathsize="90%" xref="S5.SS1.p2.2.m2.1.1.3.2.cmml">1</mn><mrow id="S5.SS1.p2.2.m2.1.1.3.3" xref="S5.SS1.p2.2.m2.1.1.3.3.cmml"><mi id="S5.SS1.p2.2.m2.1.1.3.3.2" mathsize="90%" xref="S5.SS1.p2.2.m2.1.1.3.3.2.cmml">m</mi><mo id="S5.SS1.p2.2.m2.1.1.3.3.1" xref="S5.SS1.p2.2.m2.1.1.3.3.1.cmml">â¢</mo><mi id="S5.SS1.p2.2.m2.1.1.3.3.3" mathsize="90%" xref="S5.SS1.p2.2.m2.1.1.3.3.3.cmml">e</mi><mo id="S5.SS1.p2.2.m2.1.1.3.3.1a" xref="S5.SS1.p2.2.m2.1.1.3.3.1.cmml">â¢</mo><mi id="S5.SS1.p2.2.m2.1.1.3.3.4" mathsize="90%" xref="S5.SS1.p2.2.m2.1.1.3.3.4.cmml">l</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><times id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1.1"></times><ci id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2">ğ¿</ci><apply id="S5.SS1.p2.2.m2.1.1.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.3.1.cmml" xref="S5.SS1.p2.2.m2.1.1.3">subscript</csymbol><cn id="S5.SS1.p2.2.m2.1.1.3.2.cmml" type="integer" xref="S5.SS1.p2.2.m2.1.1.3.2">1</cn><apply id="S5.SS1.p2.2.m2.1.1.3.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3"><times id="S5.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.1"></times><ci id="S5.SS1.p2.2.m2.1.1.3.3.2.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.2">ğ‘š</ci><ci id="S5.SS1.p2.2.m2.1.1.3.3.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.3">ğ‘’</ci><ci id="S5.SS1.p2.2.m2.1.1.3.3.4.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.4">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">L1_{mel}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">italic_L 1 start_POSTSUBSCRIPT italic_m italic_e italic_l end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S5.SS1.p2.2.5" style="font-size:90%;">, our method shows a slight improvement on the low-gain tones from EGDB. In general, this result suggests that the combination of MSD and MPD leads to better VA modeling than MSD alone.</span></p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.3"><span class="ltx_text" id="S5.SS1.p3.3.1" style="font-size:90%;">Table </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.T4" style="font-size:90%;" title="Table 4 â€£ 4.4 Evaluation settings â€£ 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S5.SS1.p3.3.2" style="font-size:90%;"> also shows that, compared to the supervised baselineÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.SS1.p3.3.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a><span class="ltx_text" id="S5.SS1.p3.3.4.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S5.SS1.p3.3.5" style="font-size:90%;">,
the proposed GAN-based approach (MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mo id="S5.SS1.p3.1.m1.1.1" mathsize="90%" xref="S5.SS1.p3.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><plus id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S5.SS1.p3.3.6" style="font-size:90%;">MPD) does not lead to better results in ESR. However, for the challenging case of the high-gain tone BD-2 from the EGFxset, the proposed GAN-based approach outperforms the supervised baselineÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.SS1.p3.3.7.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a><span class="ltx_text" id="S5.SS1.p3.3.8.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S5.SS1.p3.3.9" style="font-size:90%;"> greatly in all the three metrics, especially for </span><math alttext="L1_{mel}" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mi id="S5.SS1.p3.2.m2.1.1.2" mathsize="90%" xref="S5.SS1.p3.2.m2.1.1.2.cmml">L</mi><mo id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">â¢</mo><msub id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml"><mn id="S5.SS1.p3.2.m2.1.1.3.2" mathsize="90%" xref="S5.SS1.p3.2.m2.1.1.3.2.cmml">1</mn><mrow id="S5.SS1.p3.2.m2.1.1.3.3" xref="S5.SS1.p3.2.m2.1.1.3.3.cmml"><mi id="S5.SS1.p3.2.m2.1.1.3.3.2" mathsize="90%" xref="S5.SS1.p3.2.m2.1.1.3.3.2.cmml">m</mi><mo id="S5.SS1.p3.2.m2.1.1.3.3.1" xref="S5.SS1.p3.2.m2.1.1.3.3.1.cmml">â¢</mo><mi id="S5.SS1.p3.2.m2.1.1.3.3.3" mathsize="90%" xref="S5.SS1.p3.2.m2.1.1.3.3.3.cmml">e</mi><mo id="S5.SS1.p3.2.m2.1.1.3.3.1a" xref="S5.SS1.p3.2.m2.1.1.3.3.1.cmml">â¢</mo><mi id="S5.SS1.p3.2.m2.1.1.3.3.4" mathsize="90%" xref="S5.SS1.p3.2.m2.1.1.3.3.4.cmml">l</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><times id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1"></times><ci id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2">ğ¿</ci><apply id="S5.SS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p3.2.m2.1.1.3.1.cmml" xref="S5.SS1.p3.2.m2.1.1.3">subscript</csymbol><cn id="S5.SS1.p3.2.m2.1.1.3.2.cmml" type="integer" xref="S5.SS1.p3.2.m2.1.1.3.2">1</cn><apply id="S5.SS1.p3.2.m2.1.1.3.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3.3"><times id="S5.SS1.p3.2.m2.1.1.3.3.1.cmml" xref="S5.SS1.p3.2.m2.1.1.3.3.1"></times><ci id="S5.SS1.p3.2.m2.1.1.3.3.2.cmml" xref="S5.SS1.p3.2.m2.1.1.3.3.2">ğ‘š</ci><ci id="S5.SS1.p3.2.m2.1.1.3.3.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3.3.3">ğ‘’</ci><ci id="S5.SS1.p3.2.m2.1.1.3.3.4.cmml" xref="S5.SS1.p3.2.m2.1.1.3.3.4">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">L1_{mel}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">italic_L 1 start_POSTSUBSCRIPT italic_m italic_e italic_l end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S5.SS1.p3.3.10" style="font-size:90%;"> and FAD.
Informal listening to the generation result (examples available on the demo page) also shows that the proposed model performs perceptually better.</span><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>While we have not conducted a formal subjective evaluation of the implemented models, the listening test reported by the prior work of Wright <em class="ltx_emph ltx_font_italic" id="footnote3.1">et al.</em>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib14" title="">14</a>]</cite> has suggested that their GAN-based model (i.e., using MSD alone for the discriminator) outscores supervised models in human evaluation.</span></span></span><span class="ltx_text" id="S5.SS1.p3.3.11" style="font-size:90%;"> Using MSD alone also outperforms the supervised baseline in </span><math alttext="L1_{mel}" class="ltx_Math" display="inline" id="S5.SS1.p3.3.m3.1"><semantics id="S5.SS1.p3.3.m3.1a"><mrow id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml"><mi id="S5.SS1.p3.3.m3.1.1.2" mathsize="90%" xref="S5.SS1.p3.3.m3.1.1.2.cmml">L</mi><mo id="S5.SS1.p3.3.m3.1.1.1" xref="S5.SS1.p3.3.m3.1.1.1.cmml">â¢</mo><msub id="S5.SS1.p3.3.m3.1.1.3" xref="S5.SS1.p3.3.m3.1.1.3.cmml"><mn id="S5.SS1.p3.3.m3.1.1.3.2" mathsize="90%" xref="S5.SS1.p3.3.m3.1.1.3.2.cmml">1</mn><mrow id="S5.SS1.p3.3.m3.1.1.3.3" xref="S5.SS1.p3.3.m3.1.1.3.3.cmml"><mi id="S5.SS1.p3.3.m3.1.1.3.3.2" mathsize="90%" xref="S5.SS1.p3.3.m3.1.1.3.3.2.cmml">m</mi><mo id="S5.SS1.p3.3.m3.1.1.3.3.1" xref="S5.SS1.p3.3.m3.1.1.3.3.1.cmml">â¢</mo><mi id="S5.SS1.p3.3.m3.1.1.3.3.3" mathsize="90%" xref="S5.SS1.p3.3.m3.1.1.3.3.3.cmml">e</mi><mo id="S5.SS1.p3.3.m3.1.1.3.3.1a" xref="S5.SS1.p3.3.m3.1.1.3.3.1.cmml">â¢</mo><mi id="S5.SS1.p3.3.m3.1.1.3.3.4" mathsize="90%" xref="S5.SS1.p3.3.m3.1.1.3.3.4.cmml">l</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><apply id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"><times id="S5.SS1.p3.3.m3.1.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1.1"></times><ci id="S5.SS1.p3.3.m3.1.1.2.cmml" xref="S5.SS1.p3.3.m3.1.1.2">ğ¿</ci><apply id="S5.SS1.p3.3.m3.1.1.3.cmml" xref="S5.SS1.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p3.3.m3.1.1.3.1.cmml" xref="S5.SS1.p3.3.m3.1.1.3">subscript</csymbol><cn id="S5.SS1.p3.3.m3.1.1.3.2.cmml" type="integer" xref="S5.SS1.p3.3.m3.1.1.3.2">1</cn><apply id="S5.SS1.p3.3.m3.1.1.3.3.cmml" xref="S5.SS1.p3.3.m3.1.1.3.3"><times id="S5.SS1.p3.3.m3.1.1.3.3.1.cmml" xref="S5.SS1.p3.3.m3.1.1.3.3.1"></times><ci id="S5.SS1.p3.3.m3.1.1.3.3.2.cmml" xref="S5.SS1.p3.3.m3.1.1.3.3.2">ğ‘š</ci><ci id="S5.SS1.p3.3.m3.1.1.3.3.3.cmml" xref="S5.SS1.p3.3.m3.1.1.3.3.3">ğ‘’</ci><ci id="S5.SS1.p3.3.m3.1.1.3.3.4.cmml" xref="S5.SS1.p3.3.m3.1.1.3.3.4">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">L1_{mel}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.3.m3.1d">italic_L 1 start_POSTSUBSCRIPT italic_m italic_e italic_l end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S5.SS1.p3.3.12" style="font-size:90%;"> and FAD.
Together, this suggests the advantage of the GAN-based loss for high-gain tones.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Clean Audio Combination</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text" id="S5.SS2.p1.1.1" style="font-size:90%;">Next, we consider the case where we use the clean audio from both dataset as the input to our generator, no matter whether the target tone is from EGDB or EGFxset.
TableÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.T4" style="font-size:90%;" title="Table 4 â€£ 4.4 Evaluation settings â€£ 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S5.SS2.p1.1.2" style="font-size:90%;"> shows that applying this â€œbothâ€ input data setting can further boost the performance of all metrics using a paired dataset in a high-gain BD-2 target tone. In other words, the incorporation of EGDB clean tones contributes positively to the modeling of the target tone from EGFxset. This result provides more empirical evidences of the advantage of the GAN-based approach.</span></p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text" id="S5.SS2.p2.1.1" style="font-size:90%;color:#000000;">Interestingly, in contrast, we observe that incorporating clean data from EGFxset does not significantly contribute to modeling any of the target tones from EGDB.
We conjecture that this is due to the differences in music content between the two datasets (e.g., licks versus single notes).
Since EGFxset does not have note sequences and transitions between individual notes, adding inputs from EGDB could offer advantages.
However, this is not the case when considering the reverse scenario.
</span><span class="ltx_text" id="S5.SS2.p2.1.2" style="font-size:90%;"></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Benefit of the GAN-based Approach for VA Modeling</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1"><span class="ltx_text" id="S6.SS1.p1.1.1" style="font-size:90%;">Due to the limits in the amount of available paired data and computation resource, we have only considered two datasets and four tones in total in our experiments.
However, the experimental result has already revealed potentials of the GAN-based approach over the prevailing supervised approach for VA modeling. To illustrate this point further, we consider the tone that the supervised baselineÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a><span class="ltx_text" id="S6.SS1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S6.SS1.p1.1.4" style="font-size:90%;"> does not perform well, namely, the EGFxset BD-2, and conduct a case analysis.</span></p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1"><span class="ltx_text" id="S6.SS1.p2.1.1" style="font-size:90%;">FigureÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.F2" style="font-size:90%;" title="Figure 2 â€£ 4.4 Evaluation settings â€£ 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S6.SS1.p2.1.2" style="font-size:90%;"> plots the mel-spectrogram of a sampled target audio rendered with the BD-2 tone from EGFxset, along with the mel-spectrograms of the generation result of the supervised baselineÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS1.p2.1.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib6" title="">6</a><span class="ltx_text" id="S6.SS1.p2.1.4.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S6.SS1.p2.1.5" style="font-size:90%;"> and the proposed model given the corresponding clean signal.
We can see that many high-frequency harmonics are missing in the result of the supervised baseline. In contrast, they are </span><span class="ltx_text" id="S6.SS1.p2.1.6" style="font-size:90%;color:#000000;">effectively</span><span class="ltx_text" id="S6.SS1.p2.1.7" style="font-size:90%;">
captured by the proposed model throughout the time axis. </span><span class="ltx_text" id="S6.SS1.p2.1.8" style="font-size:90%;color:#000000;">Similar observations can be found for other samples for the BD-2 tone.</span><span class="ltx_text" id="S6.SS1.p2.1.9" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1"><span class="ltx_text" id="S6.SS1.p3.1.1" style="font-size:90%;">From the viewpoint of digital signal processing (DSP), higher gain value implies more non-harmonic high-frequencies in the audio.
While such non-harmonic high-frequencies may not be well captured by supervised loss functions such as the ESR, they can be better dealt with by adversarial losses such as MSD.
Compared to MSD only, MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1.1"><semantics id="S6.SS1.p3.1.m1.1a"><mo id="S6.SS1.p3.1.m1.1.1" mathsize="90%" xref="S6.SS1.p3.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><plus id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S6.SS1.p3.1.2" style="font-size:90%;">MPD can perform even better for such high-gain tones.
We speculate that adding MPD helps, for MPD operates directly on equally-spaced sample points of the audio waveform in its original temporal resolution (e.g., 44.1â€‰kHz), while MSD involves downsampling operations of the waveform. The downsampling operations of MSD may have limited its strength in assessing high-frequency components.</span></p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1"><span class="ltx_text" id="S6.SS1.p4.1.1" style="font-size:90%;">Informal listening also shows that the supervised baseline can already model the tones in EGDB well, and that for these tones the GAN-based models do not offer obvious advantages. MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S6.SS1.p4.1.m1.1"><semantics id="S6.SS1.p4.1.m1.1a"><mo id="S6.SS1.p4.1.m1.1.1" mathsize="90%" xref="S6.SS1.p4.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.1.m1.1b"><plus id="S6.SS1.p4.1.m1.1.1.cmml" xref="S6.SS1.p4.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p4.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S6.SS1.p4.1.2" style="font-size:90%;">MPD only performs better on ESR for the Marshall tone. However, we note that the GAN-based models are trained under an unpaired setting, which makes it easier to scale up the training data. Future work can further exploit this advantage by applying data augmentation methods or devising more advanced training framework.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Artifacts Generated by the Proposed Model
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><span class="ltx_text" id="S6.SS2.p1.1.1" style="font-size:90%;">During case analysis, we found that the proposed model is still not perfect and there is room for improvement. In particular, the audio generated by the proposed model may exhibit some artifacts. From FigureÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#S4.F3" style="font-size:90%;" title="Figure 3 â€£ 4.4 Evaluation settings â€£ 4 Experimental Setup â€£ Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S6.SS2.p1.1.2" style="font-size:90%;">, MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S6.SS2.p1.1.m1.1"><semantics id="S6.SS2.p1.1.m1.1a"><mo id="S6.SS2.p1.1.m1.1.1" mathsize="90%" xref="S6.SS2.p1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><plus id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S6.SS2.p1.1.3" style="font-size:90%;">MPD generate a splash of harmonics that does not exist in the target mel-spectrogram. It is unclear why the combination of MSD and MPD discriminators cannot detect such artifacts and accordingly prevent the generator from generating them.
However, as both MSD and MPD are discriminators operating directly on the audio waveforms, it might be interesting to incorporate spectral-based discriminators that operate on time-frequency representations to seek possible improvement. For example, in neural audio compression taskÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.15751v1#bib.bib9" title="">9</a><span class="ltx_text" id="S6.SS2.p1.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S6.SS2.p1.1.6" style="font-size:90%;">, researchers have shown that splitting the STFT into multi sub-bands allows each sub-discriminator focus on specific frequency bands, providing stronger gradient signals to the generator.</span></p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text" id="S6.SS2.p2.1.1" style="font-size:90%;">Another key observation is that, while both the supervised baseline model and the proposed MSD</span><math alttext="+" class="ltx_Math" display="inline" id="S6.SS2.p2.1.m1.1"><semantics id="S6.SS2.p2.1.m1.1a"><mo id="S6.SS2.p2.1.m1.1.1" mathsize="90%" xref="S6.SS2.p2.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><plus id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.1.m1.1d">+</annotation></semantics></math><span class="ltx_text" id="S6.SS2.p2.1.2" style="font-size:90%;">MPD model can result in low ESR values, both of them do not yet model the high-frequency components perfectly, especially for the sustain of notes.
This suggests that ESR may not be good enough either as a training objective or an objective evaluation metric for amplifier modeling. Future work can explore other auxiliary losses either for the supervised approach or the GAN-based approach.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1"><span class="ltx_text" id="S7.p1.1.1" style="font-size:90%;">In this paper, we have proposed a new GAN-based model for VA modeling by incorporating the MPD discriminator developed in research on neural vocoders.
With experiments on two datasets, we showed that the new model leads to improvement across a range of objective metrics over existing supervised and GAN-based models. Moreover, we demonstrated the benefit of a new scenario where combining clean audio from different datasets enhances GAN training, leading to further performance improvement. Following this light, future work can explore more advanced discriminator architectures to reduce model size, speed up training time, or further reduce artifacts. It would also be interesting to apply the GAN-based approach to datasets with greater diversity in musical content, guitar tone, and recording conditions.
</span></p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">8 </span>ACKNOWLEDGEMENTS</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1"><span class="ltx_text" id="S8.p1.1.1" style="font-size:90%;">The authors would like to thank the support from the Featured Area Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education of Taiwan (113L900901 /113L900902 /113L900903).</span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Alec Wright, Eero-Pekka DamskÃ¤gg, Lauri Juvela, and Vesa VÃ¤limÃ¤ki,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">â€œReal-time guitar amplifier emulation with deep learning,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">Applied Sciences</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, vol. 10, no. 3, pp. 766, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Eero-Pekka DamskÃ¤gg, Lauri Juvela, Etienne Thuillier, and Vesa VÃ¤limÃ¤ki,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">â€œDeep learning for tube amplifier emulation,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">, 2019, pp. 471â€“475.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
ChristianÂ J. Steinmetz and JoshuaÂ D. Reiss,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">â€œSteerable discovery of neural audio effects,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1" style="font-size:90%;">arXiv preprint arXiv:2112.02926</span><span class="ltx_text" id="bib.bib3.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Alec Wright, Eero-Pekka DamskÃ¤gg, and Vesa VÃ¤limÃ¤ki,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">â€œReal-time black-box modelling with recurrent neural networks,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib4.4.2" style="font-size:90%;">International Conference on Digital Audio Effects</span><span class="ltx_text" id="bib.bib4.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Lauri Juvela, Eero-Pekka DamskÃ¤gg, Aleksi Peussa, Jaakko MÃ¤kinen, Thomas Sherson, StylianosÂ I Mimilakis, Kimmo Rauhanen, and Athanasios Gotsopoulos,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">â€œEnd-to-end amp modeling: from data to controllable guitar amplifier models,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib5.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib5.5.3" style="font-size:90%;">, 2023, pp. 1â€“5.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Eero-Pekka DamskÃ¤gg, Lauri Juvela, Etienne Thuillier, and Vesa VÃ¤limÃ¤ki,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">â€œDeep learning for tube amplifier emulation,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib6.4.2" style="font-size:90%;">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</span><span class="ltx_text" id="bib.bib6.5.3" style="font-size:90%;">, 2019, pp. 471â€“475.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Johannes Imort, Giorgio Fabbro, Marco AÂ MartÃ­nez RamÃ­rez, Stefan Uhlich, Yuichiro Koyama, and Yuki Mitsufuji,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">â€œDistortion audio effects: Learning how to recover the clean signal,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">arXiv preprint arXiv:2202.01664</span><span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Kundan Kumar, Rithesh Kumar, Thibault DeÂ Boissiere, Lucas Gestin, WeiÂ Zhen Teoh, Jose Sotelo, Alexandre DeÂ Brebisson, Yoshua Bengio, and AaronÂ C. Courville,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">â€œMelGAN: Generative adversarial networks for conditional waveform synthesis,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">Advances in Neural Information Processing systems</span><span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">, vol. 32, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">â€œHigh-fidelity audio compression with improved rvqgan,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, vol. 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, and Nobukatsu Hojo,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">â€œCycleGAN-VC2: Improved cyclegan-based non-parallel voice conversion,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib10.5.3" style="font-size:90%;">, 2019, pp. 6820â€“6824.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
YinghaoÂ Aaron Li, Ali Zare, and Nima Mesgarani,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">â€œStarGANv2-VC: A diverse, unsupervised, non-parallel framework for natural-sounding voice conversion,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">arXiv preprint arXiv:2107.10394</span><span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">â€œGenerative adversarial networks,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">Communications of the ACM</span><span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">, vol. 63, no. 11, pp. 139â€“144, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">â€œHifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">, vol. 33, pp. 17022â€“17033, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Alec Wright, Vesa VÃ¤limÃ¤ki, and Lauri Juvela,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">â€œAdversarial guitar amplifier modelling with unpaired data,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib14.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib14.5.3" style="font-size:90%;">, 2023, pp. 1â€“5.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Yu-Hua Chen, Wen-Yi Hsiao, Tsu-Kuang Hsieh, Jyh-ShingÂ Roger Jang, and Yi-Hsuan Yang,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">â€œTowards automatic transcription of polyphonic electric guitar music: A new dataset and a multi-loss transformer model,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib15.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib15.5.3" style="font-size:90%;">, 2022, pp. 786â€“790.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Hegel Pedroza, Gerardo Meza, and IranÂ R Roman,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">â€œEGFxset: Electric guitar tones processed through real effects of distortion, modulation, delay and reverb,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">International Society for Music Information Retrieval Conference (ISMIR)</span><span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Alec Wright and Vesa VÃ¤limÃ¤ki,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">â€œGrey-box modelling of dynamic range compression,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">International Conference on Digital Audio Effects</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, 2022, pp. 304â€“311.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Felix Eichas and Udo ZÃ¶lzer,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">â€œGray-box modeling of guitar amplifiers,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.3.1" style="font-size:90%;">Journal of the Audio Engineering Society</span><span class="ltx_text" id="bib.bib18.4.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
John Covert and DavidÂ L. Livingston,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">â€œA vacuum-tube guitar amplifier model using a recurrent neural network,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib19.4.2" style="font-size:90%;">Proceedings of IEEE Southeastcon</span><span class="ltx_text" id="bib.bib19.5.3" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Zhichen Zhang, Edward Olbrych, Joseph Bruchalski, ThomasÂ J. McCormick, and DavidÂ L. Livingston,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">â€œA vacuum-tube guitar amplifier model using long/short-term memory networks,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.3.1" style="font-size:90%;">SoutheastCon</span><span class="ltx_text" id="bib.bib20.4.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Felix Eichas and Udo ZÃ¶lzer,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">â€œVirtual analog modeling of guitar amplifiers with Wiener-Hammerstein models,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib21.4.2" style="font-size:90%;">Proceedings of Annual Convention on Acoustics</span><span class="ltx_text" id="bib.bib21.5.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Aaron vanÂ den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">â€œWaveNet: A generative model for raw audio,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.3.1" style="font-size:90%;">arXiv preprint arXiv:1609.03499</span><span class="ltx_text" id="bib.bib22.4.2" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Alexandre DÃ©fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">â€œHigh fidelity neural audio compression,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.3.1" style="font-size:90%;">arXiv preprint arXiv:2210.13438</span><span class="ltx_text" id="bib.bib23.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Qingyang Xi, RachelÂ M Bittner, Johan Pauwels, Xuzhou Ye, and JuanÂ Pablo Bello,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">â€œGuitarSet: A dataset for guitar transcription,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib24.4.2" style="font-size:90%;">International Society for Music Information Retrieval Conference (ISMIR)</span><span class="ltx_text" id="bib.bib24.5.3" style="font-size:90%;">, 2018, pp. 453â€“460.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Marco ComunitÃ , Dan Stowell, and JoshuaÂ D Reiss,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">â€œGuitar effects recognition and parameter estimation with convolutional neural networks,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">arXiv preprint arXiv:2012.03216</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Christian Kehling, Jakob AbeÃŸer, Christian Dittmar, and Gerald Schuller,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">â€œAutomatic tablature transcription of electric guitar recordings by estimation of score-and instrument-related parameters.,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib26.4.2" style="font-size:90%;">DAFx</span><span class="ltx_text" id="bib.bib26.5.3" style="font-size:90%;">, 2014, pp. 219â€“226.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kim, and Juntae Kim,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">â€œUnivnet: A neural vocoder with multi-resolution spectrogram discriminators for high-fidelity waveform generation,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">arXiv preprint arXiv:2106.07889</span><span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Jinhyeok Yang, Junmo Lee, Youngik Kim, Hoonyoung Cho, and Injung Kim,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">â€œVocgan: A high-fidelity real-time vocoder with a hierarchically-nested adversarial network,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.3.1" style="font-size:90%;">arXiv preprint arXiv:2007.15256</span><span class="ltx_text" id="bib.bib28.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
JaeÂ Hyun Lim and JongÂ Chul Ye,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">â€œGeometric GAN,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.3.1" style="font-size:90%;">arXiv preprint arXiv:1705.02894</span><span class="ltx_text" id="bib.bib29.4.2" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Anders BoesenÂ Lindbo Larsen, SÃ¸renÂ Kaae SÃ¸nderby, Hugo Larochelle, and Ole Winther,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">â€œAutoencoding beyond pixels using a learned similarity metric,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib30.4.2" style="font-size:90%;">International conference on machine learning</span><span class="ltx_text" id="bib.bib30.5.3" style="font-size:90%;">, 2016, pp. 1558â€“1566.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
ChristianÂ J. Steinmetz and JoshuaÂ D. Reiss,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">â€œpyloudnorm: A simple yet flexible loudness meter in python,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib31.4.2" style="font-size:90%;">150th AES Convention</span><span class="ltx_text" id="bib.bib31.5.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">â€œFrÃ©chet audio distance: A metric for evaluating music enhancement algorithms,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="font-size:90%;">arXiv preprint arXiv:1812.08466</span><span class="ltx_text" id="bib.bib32.4.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Yen-Tung Yeh, Bo-Yu Chen, and Yi-Hsuan Yang,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">â€œExploiting pre-trained feature networks for generative adversarial networks in audio-domain loop generation,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib33.4.2" style="font-size:90%;">International Society for Music Information Retrieval Conference (ISMIR)</span><span class="ltx_text" id="bib.bib33.5.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Andrea Agostinelli, TimoÂ I Denk, ZalÃ¡n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">â€œMusicLM: Generating music from text,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.3.1" style="font-size:90%;">arXiv preprint arXiv:2301.11325</span><span class="ltx_text" id="bib.bib34.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre DÃ©fossez,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">â€œSimple and controllable music generation,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">, vol. 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Azalea Gui, Hannes Gamper, Sebastian Braun, and Dimitra Emmanouilidou,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">â€œAdapting FrÃ©chet audio distance for generative music evaluation,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib36.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib36.5.3" style="font-size:90%;">, 2024, pp. 1331â€“1335.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">â€œDecoupled weight decay regularization,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">arXiv preprint arXiv:1711.05101</span><span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Xudong Mao, Qing Li, Haoran Xie, RaymondÂ YK Lau, Zhen Wang, and Stephen PaulÂ Smolley,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">â€œLeast squares generative adversarial networks,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib38.4.2" style="font-size:90%;">IEEE International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib38.5.3" style="font-size:90%;">, 2017, pp. 2794â€“2802.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">â€œParallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,â€
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib39.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib39.5.3" style="font-size:90%;">, 2020, pp. 6199â€“6203.
</span>
</span>
</li>
</ul>
</section><div about="" class="ltx_rdf" content="Yu-Hua Chen" property="dcterms:creator"></div>
<div about="" class="ltx_rdf" content="Proceedings of the 27th International Conference on Digital Audio Effects (DAFx24)" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation Using GANs and Integrated Unaligned Clean Data" property="dcterms:title"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"></p>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Jun 22 06:30:51 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
