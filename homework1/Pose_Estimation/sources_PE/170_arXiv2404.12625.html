<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers</title>
<!--Generated on Wed May  1 13:54:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Markerless Motion Capture,  Pose and Shape Estimation,  Multi-view Pose Estimation,  Inverse-kinematics,  Skeletal Transformers
" lang="en" name="keywords"/>
<base href="/html/2404.12625v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S1" title="In SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S2" title="In SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S2.SS1" title="In II Related Work ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Keypoint Detection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S2.SS2" title="In II Related Work ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Pose and Shape Estimation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S2.SS3" title="In II Related Work ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Inverse-kinematics Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S2.SS4" title="In II Related Work ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Skeletal Neural Networks</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3" title="In SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3.SS1" title="In III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">3D Keypoint Estimation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3.SS2" title="In III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Skeletal Transformer</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3.SS3" title="In III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Joint Regressor</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3.SS4" title="In III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Data Preparation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3.SS5" title="In III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Training</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4" title="In SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.SS1" title="In IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Datasets</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.SS2" title="In IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Evaluation Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.SS3" title="In IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Implementation Details</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.SS4" title="In IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.SS4.SSS1" title="In IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>1 </span>Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.SS4.SSS2" title="In IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>2 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.SS4.SSS3" title="In IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>3 </span>Robustness to Noise and Occlusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.SS4.SSS4" title="In IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>4 </span>Qualitative Assessment</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S5" title="In SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Limitations and Future Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S6" title="In SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vandad¬†Davoodnia,
Saeed¬†Ghorbani,
Alexandre¬†Messier,
and¬†Ali¬†Etemad
</span><span class="ltx_author_notes">This work was performed during an internship at Ubisoft Laforge partially funded by Mitacs through the Accelerate program.V. Davoodnia and A. Etemad were with the Department of Electrical and Computer Engineering, Queen‚Äôs University, Kingston, Ontario, Canada. e-mail: {vandad.davoodnia,ali.etemad}@queensu.ca.V. Davoodnia and S. Ghorbani were with Ubisoft LaForge, Toronto, Ontario, Canada. e-mail: saeed.ghorbani@ubisoft.comA. Messier was with Ubisoft LaForge, Montreal, Qu√©bec, Canada. e-mail: alexandre.messier@ubisoft.com</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">We introduce SkelFormer, a novel markerless motion capture pipeline for multi-view human pose and shape estimation. Our method first uses off-the-shelf 2D keypoint estimators, pre-trained on large-scale in-the-wild data, to obtain 3D joint positions. Next, we design a regression-based inverse-kinematic skeletal transformer that maps the joint positions to pose and shape representations from heavily noisy observations. This module integrates prior knowledge about pose space and infers the full pose state at runtime. Separating the 3D keypoint detection and inverse-kinematic problems, along with the expressive representations learned by our skeletal transformer, enhance the generalization of our method to unseen noisy data. We evaluate our method on three public datasets in both in-distribution and out-of-distribution settings using three datasets, and observe strong performance with respect to prior works. Moreover, ablation experiments demonstrate the impact of each of the modules of our architecture. Finally, we study the performance of our method in dealing with noise and heavy occlusions and find considerable robustness with respect to other solutions.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Markerless Motion Capture, Pose and Shape Estimation, Multi-view Pose Estimation, Inverse-kinematics, Skeletal Transformers

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Motion capture is an active field of research with applications in sports, entertainment, health, and human-computer interaction. Currently, optical motion capture technology offers the most reliable and accurate solution by using a large number of cameras that detect markers attached to the actor‚Äôs body. As a result, optical motion capture is costly and has a time-consuming setup process, preventing its effective application in low-budget or outdoor settings. In contrast, markerless optical motion capture offers a more convenient and portable solution for capturing the pose, generally at the cost of accuracy. Therefore, a significant amount of research is dedicated to improving markerless motion capture in recent years, delivering high-quality animations by using only a few RGB cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib2" title="">2</a>]</cite>. Yet, markerless approaches either take a long time to process, <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">e.g</em>.<span class="ltx_text" id="S1.p1.1.2"></span>, 6 minutes for a 30-second video <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib3" title="">3</a>]</cite>, or they struggle to perform well in in-the-wild environments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib4" title="">4</a>]</cite>. Optimization-based solutions that fit a parametric pose or motion model to the detected keypoints often exhibit long run-times, while regression models trained on controlled and in-studio datasets lack generalization due to low-diversity backgrounds, appearance, and lighting conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this paper, we propose a novel pipeline for markerless motion capture, which we name SkelFormer. At a high level, SkelFormer consists of two main modules: a 3D keypoint estimator and a skeletal transformer. First, in order to simplify 3D keypoint detection while maintaining generalizability to a wider distribution of scenarios, our method uses a Direct-Linear-Transformation (DLT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib6" title="">6</a>]</cite> triangulation method on the output of off-the-shelf 2D keypoint estimators trained on in-the-wild data.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Next, we propose a skeletal transformer motivated by Inverse-Kinematics (IK) approaches to generate body pose and shape parameters rather than relying on the commonly used optimization methods. This module significantly reduces computational overhead while exhibiting more accurate performance. However, the misalignment between the estimated 3D keypoints and the body joint configuration of motion capture data makes the integration of keypoint estimators and our IK module challenging. To address this, we propose a simple joint regressor, trained on a small set of synthetic, and use it to generate synthetic keypoints from motion capture data that are aligned with 2D keypoint estimators. Lastly, we apply several augmentations on the acquired keypoints and train our IK component to predict the human body and shape from the noisy data. To rigorously test the performance of our method, we evaluate SkelFormer in both in-distribution (InD) and out-of-distribution (OoD) settings against prior works while noting that the majority of prior works have been tested in InD settings. Next, detailed ablation experiments demonstrate the impact of each component. Finally, we study the performance of our method and examine its robustness to highly noisy and occluded data.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In summary, we propose SkelFormer, a regression-based IK solution that converts 3D body keypoint positions to a full-body pose and shape. Our model is able to bridge the gap between the most accurate 3D keypoint detection algorithms and human pose and body mesh estimation with negligible performance degradation. SkelFormer achieves strong results, where we observe that our method outperforms others in InD scenarios. Next, we find strong cross-dataset generalizability through OoD evaluation on two unseen datasets, achieving competitive performance to InD multi-view solutions. Additionally, we show that our method exhibits a higher robustness (less than half of the error of optimization-based solutions), in severe noisy and occluded scenarios.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Keypoint Detection</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The 2D keypoint estimation field has seen substantial progress in recent years. Generally, 2D keypoint estimation models are categorized into top-down and bottom-up approaches, each with their trade-offs in speed and accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib9" title="">9</a>]</cite>. Due to the availability of large-scale datasets, such as COCO WholeBody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib11" title="">11</a>]</cite> and Halpe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib12" title="">12</a>]</cite>, 2D estimators have expanded into whole-body keypoints, potentially impacting 3D human pose and shape estimation. Previous works have proposed several strategies to infer the 3D keypoints of a subject, including semi-supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib14" title="">14</a>]</cite>, temporal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib16" title="">16</a>]</cite>, and multi-view <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib18" title="">18</a>]</cite> modeling. PoseBert <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib15" title="">15</a>]</cite> and volumetric Learnable Triangulation (LT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib17" title="">17</a>]</cite> are notable examples of temporal and multi-view methods, respectively, reporting 3D keypoint estimation with an error of below pixel-level accuracy. Our method is able to leverage the advances in 3D keypoint estimation by integrating off-the-shelf models into its pipeline.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Pose and Shape Estimation</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">Regression-based</span> methods generally predict the parameters of a body model, <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.2">e.g</em>.<span class="ltx_text" id="S2.SS2.p1.1.3"></span>, SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib20" title="">20</a>]</cite> represented by body shape and pose components. The research on body pose and shape regression can be categorized into single-view and multi-view problems. Single-view approaches generally suffer from the inherent 2D image to 3D pose ambiguities, resulting in worse performance compared to multi-view methods. For instance, Pose2Mesh <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib21" title="">21</a>]</cite> uses a GraphCNN, consisting of a mesh coarsening encoder-decoder architecture, to regress the human body and shape from a single image. Similarly, GTRS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib22" title="">22</a>]</cite> proposes a lightweight graph-based transformer network to uplift 2D keypoints to 3D pose and shape parameters. In another work, PyMAF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib23" title="">23</a>]</cite> also explores feature maps of the visual encoder to perform the regression task. Since our goal is to design a multi-view markerless motion capture pipeline, we only take inspiration from the advances in single-view research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In the context of markerless motion capture via multiple views, the majority of methods are supervised, utilizing strategies such as collaborative learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib24" title="">24</a>]</cite>, volumetric feature aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib25" title="">25</a>]</cite>, multi-view feature fusion via attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib2" title="">2</a>]</cite>, and pixel-aligned feedback fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib4" title="">4</a>]</cite>. Since regression models typically rely heavily on the availability of annotated data and the diversity of postures in the training data, they tend to be limited to in-studio quality and do not perform well on OoD evaluations with background and appearance shifts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib4" title="">4</a>]</cite>. Although a common solution to this problem is to pre-train the network on in-the-wild datasets, it does not guarantee better generalizability as the neural networks are susceptible to over-fitting and catastrophic forgetting. This is also evident by the best estimation error of 93 <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.1">mm</span> on the in-the-wild Ski-Pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib26" title="">26</a>]</cite> dataset reported in a recent work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib2" title="">2</a>]</cite>, which is three times bigger than their 33 <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.2">mm</span> error on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib5" title="">5</a>]</cite> dataset. Our work addresses this limitation in multi-view approaches by incorporating prior knowledge of human pose into the solution using an IK solver trained on a large set of motion capture data, thus improving its generalization to OoD observations.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">Optimization-based</span> approaches fit the parameter of the SMPL model to features extracted from an image, such as 2D/3D keypoints and silhouettes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib3" title="">3</a>]</cite>. Simplify-x <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib27" title="">27</a>]</cite> introduced VPoser, a human variational pose prior trained on a large collection of motion capture data, to reduce the complexity of the optimization space. Subsequently, the majority of recent works rely on VPoser to fit the SMPL model to 3D predictions, which is a time-consuming process that can take up to a day to process a one-hour-long video <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib3" title="">3</a>]</cite>. Furthermore, variational Gaussian models are prone to mean collapse due to the prior distribution assumption. Additionally, as the optimization process is sensitive to initialization, their potential to fit noisy data accurately is hindered <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib29" title="">29</a>]</cite>. To address this challenge, several works like ProHMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib30" title="">30</a>]</cite> have proposed using regression-based models to initialize the optimization variables. Although this approach can speed up and improve accuracy, optimization algorithms remain susceptible to reaching undesirable local minima, especially on noisy, exotic, or unseen poses as discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib29" title="">29</a>]</cite>. In summary, though capable of modeling complex movements and interactions, optimization-based models often require careful hyper-parameter tuning for each recording sequence, making them difficult to use in the face of multiple constraints and impractical for fast-paced production. In this paper, we show that compared to optimization solutions, our skeletal transformer is able to obtain a more accurate solutions and exhibits higher robustness to noise and occlusion.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Inverse-kinematics Models</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">IK is the task of obtaining body joint rotations given several pose constraints, with applications in robotics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib31" title="">31</a>]</cite> and animation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib32" title="">32</a>]</cite>. In the context of human pose and shape estimation, HybrIK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib1" title="">1</a>]</cite> proposed a hybrid analytical-neural IK solution that obtains the SMPL body rotations given the 3D keypoints estimated from monocular images. They designed their model to disambiguate the 2D image to 3D pose estimation by considering the shape of the human body and breaking the joint rotations to their swing and twist components. However, to our knowledge, IK applications of neural networks have not been explored for multi-view pose and shape estimation. This may be due to the superior performance of optimization methods, such as VPoser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib27" title="">27</a>]</cite>, yet at a significant computational cost.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S2.F1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of the proposed skeletal transformer pipeline is demonstrated. During training, noisy 3D keypoints are generated using our joint regressor, while during inference, 3D keypoint are provided by off-the-shelf models. Our proposed skeletal transformer then maps the keypoints onto the SMPL pose and shape parameters.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Skeletal Neural Networks</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Previous research has reported superior performance for human motion modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib33" title="">33</a>]</cite>, 3D keypoint refinement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib34" title="">34</a>]</cite>, and 2D to 3D uplifting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib15" title="">15</a>]</cite> using skeletal neural networks. In these approaches, the human body structure is modeled as a skeletal graph, and the neural networks exploit the graph structure by learning local and global pose features. Similar techniques have been proposed for hand pose estimation, where the margins for error are much lower than human body pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib35" title="">35</a>]</cite>. Motivated by the recent success of transformers in several fields, such as natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib36" title="">36</a>]</cite> and computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib37" title="">37</a>]</cite>, they have been used for motion inbetweening and completion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib39" title="">39</a>]</cite> of human poses, achieving high-quality results. In this work, we design our model based on a skeletal transformer architecture that can capture the full pose state by providing contextualized latent representations from 3D joint positions.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.p1.1.1">Overview.</span>
As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S2.F1" title="In II-C Inverse-kinematics Models ‚Ä£ II Related Work ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, our pipeline starts with a 3D keypoint estimator consisting of different sub-modules for tracking, 2D keypoint estimation, and triangulation, for which we use off-the-shelf models. Our proposed skeletal transformer then maps the estimated 3D keypoints onto the SMPL pose and shape parameters. The details of each part are given below.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">3D Keypoint Estimation</span>
</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Human Tracking.</span> We employ Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib40" title="">40</a>]</cite>, a well-established object detection model, to track the subjects in the input frames. Although more advanced methods, such as 3D skeleton tracking modules for crowded scenes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib41" title="">41</a>]</cite>, can be used, we did not observe any misidentification during single-person experiments.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">2D Pose Estimation.</span> To estimate the 2D joints within each frame, we employ HRNet-W48<span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.2">+</span>Dark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib9" title="">9</a>]</cite> trained on COCO WholeBody dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib11" title="">11</a>]</cite>. Since this model is trained on in-the-wild datasets, it helps with the generalizability of our pipeline.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Triangulation.</span> We choose a simple triangulation method by employing DLT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib6" title="">6</a>]</cite> on 2D keypoints given the extrinsic camera parameters. For this purpose, we consider 2D detection scores for assigning point occlusions.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Detailed architectures of our joint encoder, pose decoder, and shape decoder modules are presented.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Skeletal Transformer</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Traditional IK solvers often assume noise-free constraints, which is not always the case for observations, <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.1">e.g</em>.<span class="ltx_text" id="S3.SS2.p1.1.2"></span>, 3D keypoints in markerless motion capture. As a result, iterative IK solvers generally perform better than regression models in reaching the local minima given the ground-truth joints, at the cost of additional computations. However, noise and occlusions can cause iterative IK solvers to reach sub-optimal solutions. To address this issue and speed up the process, we introduce an end-to-end learnable pose reconstruction model as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S2.F1" title="In II-C Inverse-kinematics Models ‚Ä£ II Related Work ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>. Following, we present the detailed components of our skeletal transformer.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Joint Encoder.</span>
As depicted in <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S2.F1" title="In II-C Inverse-kinematics Models ‚Ä£ II Related Work ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, our network consists of a 3D joint encoder followed by pose and shape decoders. To account for the order of the joints in a skeleton, we first inject information about their ordering by concatenating a positional embedding (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.2">i.e</em>.<span class="ltx_text" id="S3.SS2.p2.1.3"></span>, joint ID embedding) vector to each joint input <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib42" title="">42</a>]</cite>. The results are then passed through an embedding layer to match the dimensionality of the transformer‚Äôs hidden layers. Next, we pass the embedded data through a series of transformer encoder blocks consisting of self-attention and feed-forward layers, followed by layer normalization, to obtain the skeletal tokens (see <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3.F2" title="In III-A 3D Keypoint Estimation ‚Ä£ III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>). To model joint occlusions, we mask out corresponding connections, encouraging the network to use contextual information embedded within the rest of the joints. The result is a latent representation of the joint positions, which is passed to the following decoders.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Pose Decoder.</span>
Our pose decoder aims to estimate the body pose given the latent representations of joint positions. A positional embedding is fed into the decoder to relay information about all 52 joints of the SMPL<span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.2">+</span>H skeleton configuration. Then, the skeletal tokens and occlusion masks are passed through the multi-head attention layers in the decoder. Specifically, the decoder consists of several blocks of self-attention, multi-head attention, and feed-forward layers followed by layer normalization (see <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3.F2" title="In III-A 3D Keypoint Estimation ‚Ä£ III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>). Additionally, to block the unwanted correlations between far-away joints (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p3.1.3">e.g</em>.<span class="ltx_text" id="S3.SS2.p3.1.4"></span>, left and right hands) that existed in the training set,
we set the attention weights such that each joint only attends to the other joints that are in a distance of less than 4 nodes in the kinematic tree.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.3"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.3.1">Symmetric Orthogonalization.</span>
The next step is to obtain the joint rotations from the decoder‚Äôs output. Although previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib30" title="">30</a>]</cite> have suggested using 6-DoF representations of rotation matrices, we find that SVD-based symmetric orthogonalization proposed in a recent work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib44" title="">44</a>]</cite> yields more accurate results and converges faster. Therefore, the output of the pose decoder is passed through a fully connected residual layer that outputs a square matrix <math alttext="M_{3\times 3}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">M</mi><mrow id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml"><mn id="S3.SS2.p4.1.m1.1.1.3.2" xref="S3.SS2.p4.1.m1.1.1.3.2.cmml">3</mn><mo id="S3.SS2.p4.1.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p4.1.m1.1.1.3.1.cmml">√ó</mo><mn id="S3.SS2.p4.1.m1.1.1.3.3" xref="S3.SS2.p4.1.m1.1.1.3.3.cmml">3</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">ùëÄ</ci><apply id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3"><times id="S3.SS2.p4.1.m1.1.1.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.3.1"></times><cn id="S3.SS2.p4.1.m1.1.1.3.2.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1.3.2">3</cn><cn id="S3.SS2.p4.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">M_{3\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_M start_POSTSUBSCRIPT 3 √ó 3 end_POSTSUBSCRIPT</annotation></semantics></math> with SVD of <math alttext="U\Sigma V^{T}" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">U</mi><mo id="S3.SS2.p4.2.m2.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.cmml">‚Å¢</mo><mi id="S3.SS2.p4.2.m2.1.1.3" mathvariant="normal" xref="S3.SS2.p4.2.m2.1.1.3.cmml">Œ£</mi><mo id="S3.SS2.p4.2.m2.1.1.1a" xref="S3.SS2.p4.2.m2.1.1.1.cmml">‚Å¢</mo><msup id="S3.SS2.p4.2.m2.1.1.4" xref="S3.SS2.p4.2.m2.1.1.4.cmml"><mi id="S3.SS2.p4.2.m2.1.1.4.2" xref="S3.SS2.p4.2.m2.1.1.4.2.cmml">V</mi><mi id="S3.SS2.p4.2.m2.1.1.4.3" xref="S3.SS2.p4.2.m2.1.1.4.3.cmml">T</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><times id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1"></times><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">ùëà</ci><ci id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">Œ£</ci><apply id="S3.SS2.p4.2.m2.1.1.4.cmml" xref="S3.SS2.p4.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.4.1.cmml" xref="S3.SS2.p4.2.m2.1.1.4">superscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.4.2.cmml" xref="S3.SS2.p4.2.m2.1.1.4.2">ùëâ</ci><ci id="S3.SS2.p4.2.m2.1.1.4.3.cmml" xref="S3.SS2.p4.2.m2.1.1.4.3">ùëá</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">U\Sigma V^{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">italic_U roman_Œ£ italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math>. Then, its symmetric orthogonalization <math alttext="\Theta\in SO(3)" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mrow id="S3.SS2.p4.3.m3.1.2" xref="S3.SS2.p4.3.m3.1.2.cmml"><mi id="S3.SS2.p4.3.m3.1.2.2" mathvariant="normal" xref="S3.SS2.p4.3.m3.1.2.2.cmml">Œò</mi><mo id="S3.SS2.p4.3.m3.1.2.1" xref="S3.SS2.p4.3.m3.1.2.1.cmml">‚àà</mo><mrow id="S3.SS2.p4.3.m3.1.2.3" xref="S3.SS2.p4.3.m3.1.2.3.cmml"><mi id="S3.SS2.p4.3.m3.1.2.3.2" xref="S3.SS2.p4.3.m3.1.2.3.2.cmml">S</mi><mo id="S3.SS2.p4.3.m3.1.2.3.1" xref="S3.SS2.p4.3.m3.1.2.3.1.cmml">‚Å¢</mo><mi id="S3.SS2.p4.3.m3.1.2.3.3" xref="S3.SS2.p4.3.m3.1.2.3.3.cmml">O</mi><mo id="S3.SS2.p4.3.m3.1.2.3.1a" xref="S3.SS2.p4.3.m3.1.2.3.1.cmml">‚Å¢</mo><mrow id="S3.SS2.p4.3.m3.1.2.3.4.2" xref="S3.SS2.p4.3.m3.1.2.3.cmml"><mo id="S3.SS2.p4.3.m3.1.2.3.4.2.1" stretchy="false" xref="S3.SS2.p4.3.m3.1.2.3.cmml">(</mo><mn id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">3</mn><mo id="S3.SS2.p4.3.m3.1.2.3.4.2.2" stretchy="false" xref="S3.SS2.p4.3.m3.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.2.cmml" xref="S3.SS2.p4.3.m3.1.2"><in id="S3.SS2.p4.3.m3.1.2.1.cmml" xref="S3.SS2.p4.3.m3.1.2.1"></in><ci id="S3.SS2.p4.3.m3.1.2.2.cmml" xref="S3.SS2.p4.3.m3.1.2.2">Œò</ci><apply id="S3.SS2.p4.3.m3.1.2.3.cmml" xref="S3.SS2.p4.3.m3.1.2.3"><times id="S3.SS2.p4.3.m3.1.2.3.1.cmml" xref="S3.SS2.p4.3.m3.1.2.3.1"></times><ci id="S3.SS2.p4.3.m3.1.2.3.2.cmml" xref="S3.SS2.p4.3.m3.1.2.3.2">ùëÜ</ci><ci id="S3.SS2.p4.3.m3.1.2.3.3.cmml" xref="S3.SS2.p4.3.m3.1.2.3.3">ùëÇ</ci><cn id="S3.SS2.p4.3.m3.1.1.cmml" type="integer" xref="S3.SS2.p4.3.m3.1.1">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\Theta\in SO(3)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">roman_Œò ‚àà italic_S italic_O ( 3 )</annotation></semantics></math> is obtained by:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Theta=U\Sigma_{o}V^{T},where\&gt;\Sigma_{o}=diag(1,1,det(UV^{T}))." class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1"><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.3.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2" mathvariant="normal" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">Œò</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.3.2.cmml">U</mi><mo id="S3.E1.m1.3.3.1.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.1.3.1.cmml">‚Å¢</mo><msub id="S3.E1.m1.3.3.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3.3.2" mathvariant="normal" xref="S3.E1.m1.3.3.1.1.1.1.3.3.2.cmml">Œ£</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3.3.3" xref="S3.E1.m1.3.3.1.1.1.1.3.3.3.cmml">o</mi></msub><mo id="S3.E1.m1.3.3.1.1.1.1.3.1a" xref="S3.E1.m1.3.3.1.1.1.1.3.1.cmml">‚Å¢</mo><msup id="S3.E1.m1.3.3.1.1.1.1.3.4" xref="S3.E1.m1.3.3.1.1.1.1.3.4.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3.4.2" xref="S3.E1.m1.3.3.1.1.1.1.3.4.2.cmml">V</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3.4.3" xref="S3.E1.m1.3.3.1.1.1.1.3.4.3.cmml">T</mi></msup></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.2.3" xref="S3.E1.m1.3.3.1.1.3a.cmml">,</mo><mrow id="S3.E1.m1.3.3.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.cmml"><mrow id="S3.E1.m1.3.3.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.3.2" xref="S3.E1.m1.3.3.1.1.2.2.3.2.cmml">w</mi><mo id="S3.E1.m1.3.3.1.1.2.2.3.1" xref="S3.E1.m1.3.3.1.1.2.2.3.1.cmml">‚Å¢</mo><mi id="S3.E1.m1.3.3.1.1.2.2.3.3" xref="S3.E1.m1.3.3.1.1.2.2.3.3.cmml">h</mi><mo id="S3.E1.m1.3.3.1.1.2.2.3.1a" xref="S3.E1.m1.3.3.1.1.2.2.3.1.cmml">‚Å¢</mo><mi id="S3.E1.m1.3.3.1.1.2.2.3.4" xref="S3.E1.m1.3.3.1.1.2.2.3.4.cmml">e</mi><mo id="S3.E1.m1.3.3.1.1.2.2.3.1b" xref="S3.E1.m1.3.3.1.1.2.2.3.1.cmml">‚Å¢</mo><mi id="S3.E1.m1.3.3.1.1.2.2.3.5" xref="S3.E1.m1.3.3.1.1.2.2.3.5.cmml">r</mi><mo id="S3.E1.m1.3.3.1.1.2.2.3.1c" xref="S3.E1.m1.3.3.1.1.2.2.3.1.cmml">‚Å¢</mo><mi id="S3.E1.m1.3.3.1.1.2.2.3.6" xref="S3.E1.m1.3.3.1.1.2.2.3.6.cmml">e</mi><mo id="S3.E1.m1.3.3.1.1.2.2.3.1d" lspace="0.220em" xref="S3.E1.m1.3.3.1.1.2.2.3.1.cmml">‚Å¢</mo><msub id="S3.E1.m1.3.3.1.1.2.2.3.7" xref="S3.E1.m1.3.3.1.1.2.2.3.7.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.3.7.2" mathvariant="normal" xref="S3.E1.m1.3.3.1.1.2.2.3.7.2.cmml">Œ£</mi><mi id="S3.E1.m1.3.3.1.1.2.2.3.7.3" xref="S3.E1.m1.3.3.1.1.2.2.3.7.3.cmml">o</mi></msub></mrow><mo id="S3.E1.m1.3.3.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1" xref="S3.E1.m1.3.3.1.1.2.2.1.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.1.3" xref="S3.E1.m1.3.3.1.1.2.2.1.3.cmml">d</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.2" xref="S3.E1.m1.3.3.1.1.2.2.1.2.cmml">‚Å¢</mo><mi id="S3.E1.m1.3.3.1.1.2.2.1.4" xref="S3.E1.m1.3.3.1.1.2.2.1.4.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.2a" xref="S3.E1.m1.3.3.1.1.2.2.1.2.cmml">‚Å¢</mo><mi id="S3.E1.m1.3.3.1.1.2.2.1.5" xref="S3.E1.m1.3.3.1.1.2.2.1.5.cmml">a</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.2b" xref="S3.E1.m1.3.3.1.1.2.2.1.2.cmml">‚Å¢</mo><mi id="S3.E1.m1.3.3.1.1.2.2.1.6" xref="S3.E1.m1.3.3.1.1.2.2.1.6.cmml">g</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.2c" xref="S3.E1.m1.3.3.1.1.2.2.1.2.cmml">‚Å¢</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.2.1.1.2.cmml">(</mo><mn id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">1</mn><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.2.1.1.2.cmml">,</mo><mn id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">1</mn><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.4" xref="S3.E1.m1.3.3.1.1.2.2.1.1.2.cmml">,</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.3.cmml">d</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.cmml">‚Å¢</mo><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.4" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.4.cmml">e</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2a" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.cmml">‚Å¢</mo><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.5" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.5.cmml">t</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2b" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.2.cmml">U</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.cmml">‚Å¢</mo><msup id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.2.cmml">V</mi><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.3.cmml">T</mi></msup></mrow><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.5" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3a.cmml" xref="S3.E1.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><eq id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"></eq><ci id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">Œò</ci><apply id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3"><times id="S3.E1.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1"></times><ci id="S3.E1.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2">ùëà</ci><apply id="S3.E1.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.3.2">Œ£</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.3.3">ùëú</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.3.4.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.3.4.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.4">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.3.4.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.4.2">ùëâ</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.4.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.4.3">ùëá</ci></apply></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2"><eq id="S3.E1.m1.3.3.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2"></eq><apply id="S3.E1.m1.3.3.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3"><times id="S3.E1.m1.3.3.1.1.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.1"></times><ci id="S3.E1.m1.3.3.1.1.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2">ùë§</ci><ci id="S3.E1.m1.3.3.1.1.2.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.3">‚Ñé</ci><ci id="S3.E1.m1.3.3.1.1.2.2.3.4.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.4">ùëí</ci><ci id="S3.E1.m1.3.3.1.1.2.2.3.5.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.5">ùëü</ci><ci id="S3.E1.m1.3.3.1.1.2.2.3.6.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.6">ùëí</ci><apply id="S3.E1.m1.3.3.1.1.2.2.3.7.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.7"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.3.7.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.7">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.3.7.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.7.2">Œ£</ci><ci id="S3.E1.m1.3.3.1.1.2.2.3.7.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.7.3">ùëú</ci></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1"><times id="S3.E1.m1.3.3.1.1.2.2.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.2"></times><ci id="S3.E1.m1.3.3.1.1.2.2.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.3">ùëë</ci><ci id="S3.E1.m1.3.3.1.1.2.2.1.4.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.4">ùëñ</ci><ci id="S3.E1.m1.3.3.1.1.2.2.1.5.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.5">ùëé</ci><ci id="S3.E1.m1.3.3.1.1.2.2.1.6.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.6">ùëî</ci><vector id="S3.E1.m1.3.3.1.1.2.2.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1"><cn id="S3.E1.m1.1.1.cmml" type="integer" xref="S3.E1.m1.1.1">1</cn><cn id="S3.E1.m1.2.2.cmml" type="integer" xref="S3.E1.m1.2.2">1</cn><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2"></times><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.3">ùëë</ci><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.4">ùëí</ci><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.5.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.5">ùë°</ci><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.2">ùëà</ci><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.2">ùëâ</ci><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3.3">ùëá</ci></apply></apply></apply></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\Theta=U\Sigma_{o}V^{T},where\&gt;\Sigma_{o}=diag(1,1,det(UV^{T})).</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">roman_Œò = italic_U roman_Œ£ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , italic_w italic_h italic_e italic_r italic_e roman_Œ£ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = italic_d italic_i italic_a italic_g ( 1 , 1 , italic_d italic_e italic_t ( italic_U italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">Shape Decoder.</span>
To obtain the shape parameters, we design a shape decoder with a similar architecture to our pose decoder. Since the order of joints does not affect the shape decoder, we remove the self-attention layers and set the sequence length of the shape decoder to one, effectively reducing the decoder to a feed-forward network with multi-head attention (see <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3.F2" title="In III-A 3D Keypoint Estimation ‚Ä£ III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Joint Regressor</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Our goal is to train our skeletal transformer on a large collection of motion capture data consisting of samples represented by SMPL shape and pose parameters. However, the skeleton configuration extracted from whole-body keypoint detectors is not aligned with the SMPL skeletal configuration. To address this, a joint regressor converts SMPL representations to other skeletal configurations. Currently available joint regressors have been reported to be either inaccurate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib45" title="">45</a>]</cite>, or simply obtained by selecting vertices on the <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">surface</span> of the body, which is not biomechanically correct nor in accordance with existing body models. Therefore, we propose a novel joint regressor and training scheme to align the SMPL representations to the 3D keypoint configuration using a small amount of synthetic data.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.4">The joint regressor, defined as a linear layer <math alttext="K=\mathcal{J}V" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">K</mi><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">ùí•</mi><mo id="S3.SS3.p2.1.m1.1.1.3.1" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><eq id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></eq><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ùêæ</ci><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><times id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.1"></times><ci id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">ùí•</ci><ci id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3">ùëâ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">K=\mathcal{J}V</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_K = caligraphic_J italic_V</annotation></semantics></math>, is trained to map the body mesh vertices <math alttext="V\in\mathbb{R}^{6890\times 3}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">V</mi><mo id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.cmml">‚àà</mo><msup id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml"><mn id="S3.SS3.p2.2.m2.1.1.3.3.2" xref="S3.SS3.p2.2.m2.1.1.3.3.2.cmml">6890</mn><mo id="S3.SS3.p2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.2.m2.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS3.p2.2.m2.1.1.3.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><in id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"></in><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">ùëâ</ci><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">‚Ñù</ci><apply id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3"><times id="S3.SS3.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.1"></times><cn id="S3.SS3.p2.2.m2.1.1.3.3.2.cmml" type="integer" xref="S3.SS3.p2.2.m2.1.1.3.3.2">6890</cn><cn id="S3.SS3.p2.2.m2.1.1.3.3.3.cmml" type="integer" xref="S3.SS3.p2.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">V\in\mathbb{R}^{6890\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_V ‚àà blackboard_R start_POSTSUPERSCRIPT 6890 √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math> to 3D keypoints <math alttext="K\in\mathbb{R}^{J\times 3}" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">K</mi><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">‚àà</mo><msup id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.2" xref="S3.SS3.p2.3.m3.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS3.p2.3.m3.1.1.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.3.2" xref="S3.SS3.p2.3.m3.1.1.3.3.2.cmml">J</mi><mo id="S3.SS3.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.3.m3.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS3.p2.3.m3.1.1.3.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><in id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></in><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">ùêæ</ci><apply id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2">‚Ñù</ci><apply id="S3.SS3.p2.3.m3.1.1.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3"><times id="S3.SS3.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.1"></times><ci id="S3.SS3.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.2">ùêΩ</ci><cn id="S3.SS3.p2.3.m3.1.1.3.3.3.cmml" type="integer" xref="S3.SS3.p2.3.m3.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">K\in\mathbb{R}^{J\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_K ‚àà blackboard_R start_POSTSUPERSCRIPT italic_J √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="J" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">ùêΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">J</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">italic_J</annotation></semantics></math> is the number of joints in the 3D keypoint configuration. In order to train the joint regressor, we randomly take 10,000 SMPL body samples from the AMASS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib46" title="">46</a>]</cite> and render them from four orthogonal views after removing the root translation and adding random root rotations augmentation. Next, following the process explained in <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3" title="III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">III</span></a>, we use HRNet-W48<span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.4.1">+</span>Dark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib9" title="">9</a>]</cite> for whole-body keypoint estimation followed by the DLT triangulation algorithm (<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3" title="III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">III</span></a>) to obtain the 3D keypoints for each sample.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.3">To encourage sparsity and avoid out-of-body predictions, previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib45" title="">45</a>]</cite> have suggested using an <math alttext="L_{2}" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">L</mi><mn id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">ùêø</ci><cn id="S3.SS3.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">L_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> regularization on the joint regressor with the goal of (a) enforcing a sum of 1 for vertex weights of each 3D joint; and (b) encouraging all of the weights of the joint regressor to lie between 0 and 1. However, doing so creates a trade-off between regularization and accuracy. In order to solve this issue, we apply a temperature-scaled Softmax function over the trainable parameters of the joint regressor <math alttext="\phi" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">œï</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">italic-œï</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_œï</annotation></semantics></math>, thus automatically satisfying both constraints. Doing so also gives us control over the vertex sparsity for each of the 3D joints. The joint regressor‚Äôs weights for the <math alttext="i_{th}" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.1"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">i</mi><mrow id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml"><mi id="S3.SS3.p3.3.m3.1.1.3.2" xref="S3.SS3.p3.3.m3.1.1.3.2.cmml">t</mi><mo id="S3.SS3.p3.3.m3.1.1.3.1" xref="S3.SS3.p3.3.m3.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS3.p3.3.m3.1.1.3.3" xref="S3.SS3.p3.3.m3.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">ùëñ</ci><apply id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3"><times id="S3.SS3.p3.3.m3.1.1.3.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3.1"></times><ci id="S3.SS3.p3.3.m3.1.1.3.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3.2">ùë°</ci><ci id="S3.SS3.p3.3.m3.1.1.3.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">i_{th}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.1d">italic_i start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math> keypoint are computed as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Sx1.EGx1">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{J}_{i}(\phi)=\frac{e^{\phi_{i}/T}}{\sum_{j=1}^{J}e^{\phi%
_{j}/T}}," class="ltx_Math" display="inline" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><msub id="S3.E2.m1.2.2.1.1.2.2" xref="S3.E2.m1.2.2.1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.cmml">ùí•</mi><mi id="S3.E2.m1.2.2.1.1.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1.1.2.1" xref="S3.E2.m1.2.2.1.1.2.1.cmml">‚Å¢</mo><mrow id="S3.E2.m1.2.2.1.1.2.3.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.2.3.2.1" stretchy="false" xref="S3.E2.m1.2.2.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">œï</mi><mo id="S3.E2.m1.2.2.1.1.2.3.2.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mfrac id="S3.E2.m1.2.2.1.1.3a" xref="S3.E2.m1.2.2.1.1.3.cmml"><msup id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.3.2.2.cmml">e</mi><mrow id="S3.E2.m1.2.2.1.1.3.2.3" xref="S3.E2.m1.2.2.1.1.3.2.3.cmml"><msub id="S3.E2.m1.2.2.1.1.3.2.3.2" xref="S3.E2.m1.2.2.1.1.3.2.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2.3.2.2" xref="S3.E2.m1.2.2.1.1.3.2.3.2.2.cmml">œï</mi><mi id="S3.E2.m1.2.2.1.1.3.2.3.2.3" xref="S3.E2.m1.2.2.1.1.3.2.3.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1.1.3.2.3.1" xref="S3.E2.m1.2.2.1.1.3.2.3.1.cmml">/</mo><mi id="S3.E2.m1.2.2.1.1.3.2.3.3" xref="S3.E2.m1.2.2.1.1.3.2.3.3.cmml">T</mi></mrow></msup><mrow id="S3.E2.m1.2.2.1.1.3.3" xref="S3.E2.m1.2.2.1.1.3.3.cmml"><msubsup id="S3.E2.m1.2.2.1.1.3.3.1" xref="S3.E2.m1.2.2.1.1.3.3.1.cmml"><mo id="S3.E2.m1.2.2.1.1.3.3.1.2.2" xref="S3.E2.m1.2.2.1.1.3.3.1.2.2.cmml">‚àë</mo><mrow id="S3.E2.m1.2.2.1.1.3.3.1.2.3" xref="S3.E2.m1.2.2.1.1.3.3.1.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.1.2.3.2" xref="S3.E2.m1.2.2.1.1.3.3.1.2.3.2.cmml">j</mi><mo id="S3.E2.m1.2.2.1.1.3.3.1.2.3.1" xref="S3.E2.m1.2.2.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E2.m1.2.2.1.1.3.3.1.2.3.3" xref="S3.E2.m1.2.2.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.2.2.1.1.3.3.1.3" xref="S3.E2.m1.2.2.1.1.3.3.1.3.cmml">J</mi></msubsup><msup id="S3.E2.m1.2.2.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.3.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.2.2" xref="S3.E2.m1.2.2.1.1.3.3.2.2.cmml">e</mi><mrow id="S3.E2.m1.2.2.1.1.3.3.2.3" xref="S3.E2.m1.2.2.1.1.3.3.2.3.cmml"><msub id="S3.E2.m1.2.2.1.1.3.3.2.3.2" xref="S3.E2.m1.2.2.1.1.3.3.2.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.2.3.2.2" xref="S3.E2.m1.2.2.1.1.3.3.2.3.2.2.cmml">œï</mi><mi id="S3.E2.m1.2.2.1.1.3.3.2.3.2.3" xref="S3.E2.m1.2.2.1.1.3.3.2.3.2.3.cmml">j</mi></msub><mo id="S3.E2.m1.2.2.1.1.3.3.2.3.1" xref="S3.E2.m1.2.2.1.1.3.3.2.3.1.cmml">/</mo><mi id="S3.E2.m1.2.2.1.1.3.3.2.3.3" xref="S3.E2.m1.2.2.1.1.3.3.2.3.3.cmml">T</mi></mrow></msup></mrow></mfrac></mstyle></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"></eq><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><times id="S3.E2.m1.2.2.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.1"></times><apply id="S3.E2.m1.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2">ùí•</ci><ci id="S3.E2.m1.2.2.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.3">ùëñ</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">italic-œï</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><divide id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3"></divide><apply id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.2">ùëí</ci><apply id="S3.E2.m1.2.2.1.1.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3"><divide id="S3.E2.m1.2.2.1.1.3.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.1"></divide><apply id="S3.E2.m1.2.2.1.1.3.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.2.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.2.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.2.2">italic-œï</ci><ci id="S3.E2.m1.2.2.1.1.3.2.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.2.3">ùëñ</ci></apply><ci id="S3.E2.m1.2.2.1.1.3.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.3">ùëá</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3"><apply id="S3.E2.m1.2.2.1.1.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.1.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.3.3.1.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.3.3.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1.2.2"></sum><apply id="S3.E2.m1.2.2.1.1.3.3.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1.2.3"><eq id="S3.E2.m1.2.2.1.1.3.3.1.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1.2.3.1"></eq><ci id="S3.E2.m1.2.2.1.1.3.3.1.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1.2.3.2">ùëó</ci><cn id="S3.E2.m1.2.2.1.1.3.3.1.2.3.3.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.2.2.1.1.3.3.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1.3">ùêΩ</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.2">ùëí</ci><apply id="S3.E2.m1.2.2.1.1.3.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3"><divide id="S3.E2.m1.2.2.1.1.3.3.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3.1"></divide><apply id="S3.E2.m1.2.2.1.1.3.3.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.2.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.3.2.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3.2.2">italic-œï</ci><ci id="S3.E2.m1.2.2.1.1.3.3.2.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3.2.3">ùëó</ci></apply><ci id="S3.E2.m1.2.2.1.1.3.3.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3.3">ùëá</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\displaystyle\mathcal{J}_{i}(\phi)=\frac{e^{\phi_{i}/T}}{\sum_{j=1}^{J}e^{\phi%
_{j}/T}},</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">caligraphic_J start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_œï ) = divide start_ARG italic_e start_POSTSUPERSCRIPT italic_œï start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_T end_POSTSUPERSCRIPT end_ARG start_ARG ‚àë start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_J end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT italic_œï start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / italic_T end_POSTSUPERSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p3.5">where <math alttext="T" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m1.1"><semantics id="S3.SS3.p3.4.m1.1a"><mi id="S3.SS3.p3.4.m1.1.1" xref="S3.SS3.p3.4.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m1.1b"><ci id="S3.SS3.p3.4.m1.1.1.cmml" xref="S3.SS3.p3.4.m1.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m1.1d">italic_T</annotation></semantics></math> controls the sharpness of the distribution of the vertex weights. We use an L-BFGS optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib47" title="">47</a>]</cite> to increase the training efficiency. We empirically set the temperature to <math alttext="T=10" class="ltx_Math" display="inline" id="S3.SS3.p3.5.m2.1"><semantics id="S3.SS3.p3.5.m2.1a"><mrow id="S3.SS3.p3.5.m2.1.1" xref="S3.SS3.p3.5.m2.1.1.cmml"><mi id="S3.SS3.p3.5.m2.1.1.2" xref="S3.SS3.p3.5.m2.1.1.2.cmml">T</mi><mo id="S3.SS3.p3.5.m2.1.1.1" xref="S3.SS3.p3.5.m2.1.1.1.cmml">=</mo><mn id="S3.SS3.p3.5.m2.1.1.3" xref="S3.SS3.p3.5.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m2.1b"><apply id="S3.SS3.p3.5.m2.1.1.cmml" xref="S3.SS3.p3.5.m2.1.1"><eq id="S3.SS3.p3.5.m2.1.1.1.cmml" xref="S3.SS3.p3.5.m2.1.1.1"></eq><ci id="S3.SS3.p3.5.m2.1.1.2.cmml" xref="S3.SS3.p3.5.m2.1.1.2">ùëá</ci><cn id="S3.SS3.p3.5.m2.1.1.3.cmml" type="integer" xref="S3.SS3.p3.5.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m2.1c">T=10</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.5.m2.1d">italic_T = 10</annotation></semantics></math> so that 3 to 10 vertices contribute to each joint.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Data Preparation</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.7">We extract pairs of 3D keypoints <math alttext="K" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">italic_K</annotation></semantics></math> (using our joint regressor), body joint rotation matrices <math alttext="\Theta\in\mathbb{R}^{52\times 3\times 3}" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><mrow id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" mathvariant="normal" xref="S3.SS4.p1.2.m2.1.1.2.cmml">Œò</mi><mo id="S3.SS4.p1.2.m2.1.1.1" xref="S3.SS4.p1.2.m2.1.1.1.cmml">‚àà</mo><msup id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml"><mi id="S3.SS4.p1.2.m2.1.1.3.2" xref="S3.SS4.p1.2.m2.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS4.p1.2.m2.1.1.3.3" xref="S3.SS4.p1.2.m2.1.1.3.3.cmml"><mn id="S3.SS4.p1.2.m2.1.1.3.3.2" xref="S3.SS4.p1.2.m2.1.1.3.3.2.cmml">52</mn><mo id="S3.SS4.p1.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p1.2.m2.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS4.p1.2.m2.1.1.3.3.3" xref="S3.SS4.p1.2.m2.1.1.3.3.3.cmml">3</mn><mo id="S3.SS4.p1.2.m2.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p1.2.m2.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS4.p1.2.m2.1.1.3.3.4" xref="S3.SS4.p1.2.m2.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><in id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1.1"></in><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">Œò</ci><apply id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.3.1.cmml" xref="S3.SS4.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.3.2.cmml" xref="S3.SS4.p1.2.m2.1.1.3.2">‚Ñù</ci><apply id="S3.SS4.p1.2.m2.1.1.3.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3.3"><times id="S3.SS4.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS4.p1.2.m2.1.1.3.3.1"></times><cn id="S3.SS4.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="S3.SS4.p1.2.m2.1.1.3.3.2">52</cn><cn id="S3.SS4.p1.2.m2.1.1.3.3.3.cmml" type="integer" xref="S3.SS4.p1.2.m2.1.1.3.3.3">3</cn><cn id="S3.SS4.p1.2.m2.1.1.3.3.4.cmml" type="integer" xref="S3.SS4.p1.2.m2.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\Theta\in\mathbb{R}^{52\times 3\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.1d">roman_Œò ‚àà blackboard_R start_POSTSUPERSCRIPT 52 √ó 3 √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, and shape parameters <math alttext="\beta\in\mathbb{R}^{16}" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><mrow id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">Œ≤</mi><mo id="S3.SS4.p1.3.m3.1.1.1" xref="S3.SS4.p1.3.m3.1.1.1.cmml">‚àà</mo><msup id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml"><mi id="S3.SS4.p1.3.m3.1.1.3.2" xref="S3.SS4.p1.3.m3.1.1.3.2.cmml">‚Ñù</mi><mn id="S3.SS4.p1.3.m3.1.1.3.3" xref="S3.SS4.p1.3.m3.1.1.3.3.cmml">16</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><in id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1"></in><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">ùõΩ</ci><apply id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.3.1.cmml" xref="S3.SS4.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.3.2.cmml" xref="S3.SS4.p1.3.m3.1.1.3.2">‚Ñù</ci><cn id="S3.SS4.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS4.p1.3.m3.1.1.3.3">16</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\beta\in\mathbb{R}^{16}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">italic_Œ≤ ‚àà blackboard_R start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT</annotation></semantics></math> from human motion capture data. Next, we measure the average joint rotations across the dataset to normalize <math alttext="\Theta" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4.1"><semantics id="S3.SS4.p1.4.m4.1a"><mi id="S3.SS4.p1.4.m4.1.1" mathvariant="normal" xref="S3.SS4.p1.4.m4.1.1.cmml">Œò</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><ci id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">Œò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m4.1d">roman_Œò</annotation></semantics></math>. As some layers in the skeletal transformer share weights across all the joints, the symmetry of the left and right rotations is important. Yet, the mean pose <math alttext="\Theta_{m}" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m5.1"><semantics id="S3.SS4.p1.5.m5.1a"><msub id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><mi id="S3.SS4.p1.5.m5.1.1.2" mathvariant="normal" xref="S3.SS4.p1.5.m5.1.1.2.cmml">Œò</mi><mi id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2">Œò</ci><ci id="S3.SS4.p1.5.m5.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">\Theta_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.m5.1d">roman_Œò start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> provided and used in previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib30" title="">30</a>]</cite> is not symmetrical. To address this, we add the mediolateral mirrored samples to the dataset and use quaternion averaging as described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib49" title="">49</a>]</cite> to measure <math alttext="\Theta_{m}" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m6.1"><semantics id="S3.SS4.p1.6.m6.1a"><msub id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml"><mi id="S3.SS4.p1.6.m6.1.1.2" mathvariant="normal" xref="S3.SS4.p1.6.m6.1.1.2.cmml">Œò</mi><mi id="S3.SS4.p1.6.m6.1.1.3" xref="S3.SS4.p1.6.m6.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><apply id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p1.6.m6.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.2">Œò</ci><ci id="S3.SS4.p1.6.m6.1.1.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">\Theta_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m6.1d">roman_Œò start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math>, subsequently normalizing the pose by <math alttext="\Delta\Theta=\Theta_{m}^{-1}\Theta" class="ltx_Math" display="inline" id="S3.SS4.p1.7.m7.1"><semantics id="S3.SS4.p1.7.m7.1a"><mrow id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml"><mrow id="S3.SS4.p1.7.m7.1.1.2" xref="S3.SS4.p1.7.m7.1.1.2.cmml"><mi id="S3.SS4.p1.7.m7.1.1.2.2" mathvariant="normal" xref="S3.SS4.p1.7.m7.1.1.2.2.cmml">Œî</mi><mo id="S3.SS4.p1.7.m7.1.1.2.1" xref="S3.SS4.p1.7.m7.1.1.2.1.cmml">‚Å¢</mo><mi id="S3.SS4.p1.7.m7.1.1.2.3" mathvariant="normal" xref="S3.SS4.p1.7.m7.1.1.2.3.cmml">Œò</mi></mrow><mo id="S3.SS4.p1.7.m7.1.1.1" xref="S3.SS4.p1.7.m7.1.1.1.cmml">=</mo><mrow id="S3.SS4.p1.7.m7.1.1.3" xref="S3.SS4.p1.7.m7.1.1.3.cmml"><msubsup id="S3.SS4.p1.7.m7.1.1.3.2" xref="S3.SS4.p1.7.m7.1.1.3.2.cmml"><mi id="S3.SS4.p1.7.m7.1.1.3.2.2.2" mathvariant="normal" xref="S3.SS4.p1.7.m7.1.1.3.2.2.2.cmml">Œò</mi><mi id="S3.SS4.p1.7.m7.1.1.3.2.2.3" xref="S3.SS4.p1.7.m7.1.1.3.2.2.3.cmml">m</mi><mrow id="S3.SS4.p1.7.m7.1.1.3.2.3" xref="S3.SS4.p1.7.m7.1.1.3.2.3.cmml"><mo id="S3.SS4.p1.7.m7.1.1.3.2.3a" xref="S3.SS4.p1.7.m7.1.1.3.2.3.cmml">‚àí</mo><mn id="S3.SS4.p1.7.m7.1.1.3.2.3.2" xref="S3.SS4.p1.7.m7.1.1.3.2.3.2.cmml">1</mn></mrow></msubsup><mo id="S3.SS4.p1.7.m7.1.1.3.1" xref="S3.SS4.p1.7.m7.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS4.p1.7.m7.1.1.3.3" mathvariant="normal" xref="S3.SS4.p1.7.m7.1.1.3.3.cmml">Œò</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><apply id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1"><eq id="S3.SS4.p1.7.m7.1.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1.1"></eq><apply id="S3.SS4.p1.7.m7.1.1.2.cmml" xref="S3.SS4.p1.7.m7.1.1.2"><times id="S3.SS4.p1.7.m7.1.1.2.1.cmml" xref="S3.SS4.p1.7.m7.1.1.2.1"></times><ci id="S3.SS4.p1.7.m7.1.1.2.2.cmml" xref="S3.SS4.p1.7.m7.1.1.2.2">Œî</ci><ci id="S3.SS4.p1.7.m7.1.1.2.3.cmml" xref="S3.SS4.p1.7.m7.1.1.2.3">Œò</ci></apply><apply id="S3.SS4.p1.7.m7.1.1.3.cmml" xref="S3.SS4.p1.7.m7.1.1.3"><times id="S3.SS4.p1.7.m7.1.1.3.1.cmml" xref="S3.SS4.p1.7.m7.1.1.3.1"></times><apply id="S3.SS4.p1.7.m7.1.1.3.2.cmml" xref="S3.SS4.p1.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m7.1.1.3.2.1.cmml" xref="S3.SS4.p1.7.m7.1.1.3.2">superscript</csymbol><apply id="S3.SS4.p1.7.m7.1.1.3.2.2.cmml" xref="S3.SS4.p1.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m7.1.1.3.2.2.1.cmml" xref="S3.SS4.p1.7.m7.1.1.3.2">subscript</csymbol><ci id="S3.SS4.p1.7.m7.1.1.3.2.2.2.cmml" xref="S3.SS4.p1.7.m7.1.1.3.2.2.2">Œò</ci><ci id="S3.SS4.p1.7.m7.1.1.3.2.2.3.cmml" xref="S3.SS4.p1.7.m7.1.1.3.2.2.3">ùëö</ci></apply><apply id="S3.SS4.p1.7.m7.1.1.3.2.3.cmml" xref="S3.SS4.p1.7.m7.1.1.3.2.3"><minus id="S3.SS4.p1.7.m7.1.1.3.2.3.1.cmml" xref="S3.SS4.p1.7.m7.1.1.3.2.3"></minus><cn id="S3.SS4.p1.7.m7.1.1.3.2.3.2.cmml" type="integer" xref="S3.SS4.p1.7.m7.1.1.3.2.3.2">1</cn></apply></apply><ci id="S3.SS4.p1.7.m7.1.1.3.3.cmml" xref="S3.SS4.p1.7.m7.1.1.3.3">Œò</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">\Delta\Theta=\Theta_{m}^{-1}\Theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.7.m7.1d">roman_Œî roman_Œò = roman_Œò start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_Œò</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">During training, we apply a range of online augmentations to diversify the data and enhance the robustness of our model. These augmentations include:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">Masking.</span> We apply random masking on each joint with a 20% chance to model partially occluded inputs, simulating scenarios where joints are not visible or considered outliers. For this task, we exploit the inner mechanics of transformers by masking the attention of the occluded joints in the encoder and between the encoder and decoders.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.1.1">Rotation.</span> We randomly rotate the 3D keypoints around the mid-hip point to simulate various orientations of the human body. Specifically, we rotate the points by <math alttext="\pm 180^{\circ}" class="ltx_Math" display="inline" id="S3.SS4.p4.1.m1.1"><semantics id="S3.SS4.p4.1.m1.1a"><mrow id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml"><mo id="S3.SS4.p4.1.m1.1.1a" xref="S3.SS4.p4.1.m1.1.1.cmml">¬±</mo><msup id="S3.SS4.p4.1.m1.1.1.2" xref="S3.SS4.p4.1.m1.1.1.2.cmml"><mn id="S3.SS4.p4.1.m1.1.1.2.2" xref="S3.SS4.p4.1.m1.1.1.2.2.cmml">180</mn><mo id="S3.SS4.p4.1.m1.1.1.2.3" xref="S3.SS4.p4.1.m1.1.1.2.3.cmml">‚àò</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><apply id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.SS4.p4.1.m1.1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">plus-or-minus</csymbol><apply id="S3.SS4.p4.1.m1.1.1.2.cmml" xref="S3.SS4.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p4.1.m1.1.1.2.1.cmml" xref="S3.SS4.p4.1.m1.1.1.2">superscript</csymbol><cn id="S3.SS4.p4.1.m1.1.1.2.2.cmml" type="integer" xref="S3.SS4.p4.1.m1.1.1.2.2">180</cn><compose id="S3.SS4.p4.1.m1.1.1.2.3.cmml" xref="S3.SS4.p4.1.m1.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">\pm 180^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.1.m1.1d">¬± 180 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPT</annotation></semantics></math> along the vertical axis to enhance the model‚Äôs ability to predict the root rotation accurately. Additionally, we occasionally rotate the body by 90¬∞¬† to simulate lying and sleeping postures, which are often considered a challenging posture in markerless motion capture applications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p5.1.1">Noise Addition.</span> To model the faulty predictions in the 3D keypoint estimation module, we add random Gaussian noise with a standard deviation proportional to 5% of the joint annotation confidences provided in the COCO WholeBody dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p6">
<p class="ltx_p" id="S3.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p6.1.1">Left-Right Mirroring.</span> We increase the robustness of our model to mediolateral flips by mirroring the keypoints along the YZ plane by 50% chance. Since the SMPL model is not symmetrical, we apply the mirroring directly on the inputs and the output of the model before the SMPL FK layer.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p7">
<p class="ltx_p" id="S3.SS4.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p7.1.1">Shape Augmentation.</span> Although AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib46" title="">46</a>]</cite> training set has a large variety of common and exotic poses, it lacks in body shapes variety and includes only 300 different body shapes. In order to mitigate this issue, we randomly augment the body shape parameters before keypoint corruption by an additive Gaussian noise with a standard deviation equal to the standard deviation of all available body shapes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p8">
<p class="ltx_p" id="S3.SS4.p8.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p8.1.1">Outliers.</span> A common source of noise in 3D triangulation pipelines is heavy 2D keypoint shifts caused by faulty detection in one or more views. Therefore, a straightforward approach is to consider such heavy shifts as outliers and mask those inputs in the network. However, the outlier detection algorithms might fail to detect such instances. In order to increase the robustness of our model to such noises, we apply a large additive Gaussian noise with a standard deviation of one meter to each keypoint with a chance of 1% during training to increase the robustness of our model to outliers.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.5.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.6.2">Training</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.2">In order to train our skeletal transformer, we feed the model with augmented data <math alttext="K_{a}" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><msub id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">K</mi><mi id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">ùêæ</ci><ci id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">K_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_K start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> to obtain pose and shape parameters, which are then passed through the SMPL Forward Kinematics (FK) layer. We then use a combination of rotational, positional, and shape losses to leverage different scopes and granularities to ultimately help with training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib50" title="">50</a>]</cite>. Our final loss is calculated as <math alttext="L=L_{R}+L_{P}+L_{S}" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><mrow id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">L</mi><mo id="S3.SS5.p1.2.m2.1.1.1" xref="S3.SS5.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml"><msub id="S3.SS5.p1.2.m2.1.1.3.2" xref="S3.SS5.p1.2.m2.1.1.3.2.cmml"><mi id="S3.SS5.p1.2.m2.1.1.3.2.2" xref="S3.SS5.p1.2.m2.1.1.3.2.2.cmml">L</mi><mi id="S3.SS5.p1.2.m2.1.1.3.2.3" xref="S3.SS5.p1.2.m2.1.1.3.2.3.cmml">R</mi></msub><mo id="S3.SS5.p1.2.m2.1.1.3.1" xref="S3.SS5.p1.2.m2.1.1.3.1.cmml">+</mo><msub id="S3.SS5.p1.2.m2.1.1.3.3" xref="S3.SS5.p1.2.m2.1.1.3.3.cmml"><mi id="S3.SS5.p1.2.m2.1.1.3.3.2" xref="S3.SS5.p1.2.m2.1.1.3.3.2.cmml">L</mi><mi id="S3.SS5.p1.2.m2.1.1.3.3.3" xref="S3.SS5.p1.2.m2.1.1.3.3.3.cmml">P</mi></msub><mo id="S3.SS5.p1.2.m2.1.1.3.1a" xref="S3.SS5.p1.2.m2.1.1.3.1.cmml">+</mo><msub id="S3.SS5.p1.2.m2.1.1.3.4" xref="S3.SS5.p1.2.m2.1.1.3.4.cmml"><mi id="S3.SS5.p1.2.m2.1.1.3.4.2" xref="S3.SS5.p1.2.m2.1.1.3.4.2.cmml">L</mi><mi id="S3.SS5.p1.2.m2.1.1.3.4.3" xref="S3.SS5.p1.2.m2.1.1.3.4.3.cmml">S</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><eq id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1.1"></eq><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">ùêø</ci><apply id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3"><plus id="S3.SS5.p1.2.m2.1.1.3.1.cmml" xref="S3.SS5.p1.2.m2.1.1.3.1"></plus><apply id="S3.SS5.p1.2.m2.1.1.3.2.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.3.2.1.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.3.2.2.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2.2">ùêø</ci><ci id="S3.SS5.p1.2.m2.1.1.3.2.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2.3">ùëÖ</ci></apply><apply id="S3.SS5.p1.2.m2.1.1.3.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS5.p1.2.m2.1.1.3.3">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS5.p1.2.m2.1.1.3.3.2">ùêø</ci><ci id="S3.SS5.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3.3.3">ùëÉ</ci></apply><apply id="S3.SS5.p1.2.m2.1.1.3.4.cmml" xref="S3.SS5.p1.2.m2.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.3.4.1.cmml" xref="S3.SS5.p1.2.m2.1.1.3.4">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.3.4.2.cmml" xref="S3.SS5.p1.2.m2.1.1.3.4.2">ùêø</ci><ci id="S3.SS5.p1.2.m2.1.1.3.4.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3.4.3">ùëÜ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">L=L_{R}+L_{P}+L_{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">italic_L = italic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT + italic_L start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT + italic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> with details as follows.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.2"><span class="ltx_text ltx_font_bold" id="S3.SS5.p2.2.1">Rotation Loss.</span> We define our rotational loss as the sum of the global and local geodesic distances between the predicted <math alttext="\hat{\Theta}" class="ltx_Math" display="inline" id="S3.SS5.p2.1.m1.1"><semantics id="S3.SS5.p2.1.m1.1a"><mover accent="true" id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><mi id="S3.SS5.p2.1.m1.1.1.2" mathvariant="normal" xref="S3.SS5.p2.1.m1.1.1.2.cmml">Œò</mi><mo id="S3.SS5.p2.1.m1.1.1.1" xref="S3.SS5.p2.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><ci id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1.1">^</ci><ci id="S3.SS5.p2.1.m1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2">Œò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">\hat{\Theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.1.m1.1d">over^ start_ARG roman_Œò end_ARG</annotation></semantics></math> and ground-truth rotations <math alttext="\Theta" class="ltx_Math" display="inline" id="S3.SS5.p2.2.m2.1"><semantics id="S3.SS5.p2.2.m2.1a"><mi id="S3.SS5.p2.2.m2.1.1" mathvariant="normal" xref="S3.SS5.p2.2.m2.1.1.cmml">Œò</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><ci id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">Œò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.2.m2.1d">roman_Œò</annotation></semantics></math> for each joint:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{R}(\Theta,\hat{\Theta})=\arccos\left(\frac{\operatorname{tr}(\Theta\hat{%
\Theta}^{\intercal})-1}{2}\right)." class="ltx_Math" display="block" id="S3.E3.m1.6"><semantics id="S3.E3.m1.6a"><mrow id="S3.E3.m1.6.6.1" xref="S3.E3.m1.6.6.1.1.cmml"><mrow id="S3.E3.m1.6.6.1.1" xref="S3.E3.m1.6.6.1.1.cmml"><mrow id="S3.E3.m1.6.6.1.1.2" xref="S3.E3.m1.6.6.1.1.2.cmml"><msub id="S3.E3.m1.6.6.1.1.2.2" xref="S3.E3.m1.6.6.1.1.2.2.cmml"><mi id="S3.E3.m1.6.6.1.1.2.2.2" xref="S3.E3.m1.6.6.1.1.2.2.2.cmml">L</mi><mi id="S3.E3.m1.6.6.1.1.2.2.3" xref="S3.E3.m1.6.6.1.1.2.2.3.cmml">R</mi></msub><mo id="S3.E3.m1.6.6.1.1.2.1" xref="S3.E3.m1.6.6.1.1.2.1.cmml">‚Å¢</mo><mrow id="S3.E3.m1.6.6.1.1.2.3.2" xref="S3.E3.m1.6.6.1.1.2.3.1.cmml"><mo id="S3.E3.m1.6.6.1.1.2.3.2.1" stretchy="false" xref="S3.E3.m1.6.6.1.1.2.3.1.cmml">(</mo><mi id="S3.E3.m1.3.3" mathvariant="normal" xref="S3.E3.m1.3.3.cmml">Œò</mi><mo id="S3.E3.m1.6.6.1.1.2.3.2.2" xref="S3.E3.m1.6.6.1.1.2.3.1.cmml">,</mo><mover accent="true" id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mi id="S3.E3.m1.4.4.2" mathvariant="normal" xref="S3.E3.m1.4.4.2.cmml">Œò</mi><mo id="S3.E3.m1.4.4.1" xref="S3.E3.m1.4.4.1.cmml">^</mo></mover><mo id="S3.E3.m1.6.6.1.1.2.3.2.3" stretchy="false" xref="S3.E3.m1.6.6.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.6.6.1.1.1" xref="S3.E3.m1.6.6.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.6.6.1.1.3.2" xref="S3.E3.m1.6.6.1.1.3.1.cmml"><mi id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml">arccos</mi><mo id="S3.E3.m1.6.6.1.1.3.2a" xref="S3.E3.m1.6.6.1.1.3.1.cmml">‚Å°</mo><mrow id="S3.E3.m1.6.6.1.1.3.2.1" xref="S3.E3.m1.6.6.1.1.3.1.cmml"><mo id="S3.E3.m1.6.6.1.1.3.2.1.1" xref="S3.E3.m1.6.6.1.1.3.1.cmml">(</mo><mfrac id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">tr</mi><mo id="S3.E3.m1.2.2.2.2.1a" xref="S3.E3.m1.2.2.2.2.2.cmml">‚Å°</mo><mrow id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.2.cmml"><mo id="S3.E3.m1.2.2.2.2.1.1.2" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.cmml">(</mo><mrow id="S3.E3.m1.2.2.2.2.1.1.1" xref="S3.E3.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.1.2" mathvariant="normal" xref="S3.E3.m1.2.2.2.2.1.1.1.2.cmml">Œò</mi><mo id="S3.E3.m1.2.2.2.2.1.1.1.1" xref="S3.E3.m1.2.2.2.2.1.1.1.1.cmml">‚Å¢</mo><msup id="S3.E3.m1.2.2.2.2.1.1.1.3" xref="S3.E3.m1.2.2.2.2.1.1.1.3.cmml"><mover accent="true" id="S3.E3.m1.2.2.2.2.1.1.1.3.2" xref="S3.E3.m1.2.2.2.2.1.1.1.3.2.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.1.3.2.2" mathvariant="normal" xref="S3.E3.m1.2.2.2.2.1.1.1.3.2.2.cmml">Œò</mi><mo id="S3.E3.m1.2.2.2.2.1.1.1.3.2.1" xref="S3.E3.m1.2.2.2.2.1.1.1.3.2.1.cmml">^</mo></mover><mo id="S3.E3.m1.2.2.2.2.1.1.1.3.3" xref="S3.E3.m1.2.2.2.2.1.1.1.3.3.cmml">‚ä∫</mo></msup></mrow><mo id="S3.E3.m1.2.2.2.2.1.1.3" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">‚àí</mo><mn id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.4.cmml">1</mn></mrow><mn id="S3.E3.m1.2.2.4" xref="S3.E3.m1.2.2.4.cmml">2</mn></mfrac><mo id="S3.E3.m1.6.6.1.1.3.2.1.2" xref="S3.E3.m1.6.6.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.6.6.1.2" lspace="0em" xref="S3.E3.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.6b"><apply id="S3.E3.m1.6.6.1.1.cmml" xref="S3.E3.m1.6.6.1"><eq id="S3.E3.m1.6.6.1.1.1.cmml" xref="S3.E3.m1.6.6.1.1.1"></eq><apply id="S3.E3.m1.6.6.1.1.2.cmml" xref="S3.E3.m1.6.6.1.1.2"><times id="S3.E3.m1.6.6.1.1.2.1.cmml" xref="S3.E3.m1.6.6.1.1.2.1"></times><apply id="S3.E3.m1.6.6.1.1.2.2.cmml" xref="S3.E3.m1.6.6.1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.1.1.2.2.1.cmml" xref="S3.E3.m1.6.6.1.1.2.2">subscript</csymbol><ci id="S3.E3.m1.6.6.1.1.2.2.2.cmml" xref="S3.E3.m1.6.6.1.1.2.2.2">ùêø</ci><ci id="S3.E3.m1.6.6.1.1.2.2.3.cmml" xref="S3.E3.m1.6.6.1.1.2.2.3">ùëÖ</ci></apply><interval closure="open" id="S3.E3.m1.6.6.1.1.2.3.1.cmml" xref="S3.E3.m1.6.6.1.1.2.3.2"><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">Œò</ci><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4"><ci id="S3.E3.m1.4.4.1.cmml" xref="S3.E3.m1.4.4.1">^</ci><ci id="S3.E3.m1.4.4.2.cmml" xref="S3.E3.m1.4.4.2">Œò</ci></apply></interval></apply><apply id="S3.E3.m1.6.6.1.1.3.1.cmml" xref="S3.E3.m1.6.6.1.1.3.2"><arccos id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5"></arccos><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><divide id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2"></divide><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><minus id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3"></minus><apply id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.1"><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">tr</ci><apply id="S3.E3.m1.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1"><times id="S3.E3.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.1"></times><ci id="S3.E3.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.2">Œò</ci><apply id="S3.E3.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.1.1.3.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.3">superscript</csymbol><apply id="S3.E3.m1.2.2.2.2.1.1.1.3.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.3.2"><ci id="S3.E3.m1.2.2.2.2.1.1.1.3.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.3.2.1">^</ci><ci id="S3.E3.m1.2.2.2.2.1.1.1.3.2.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.3.2.2">Œò</ci></apply><ci id="S3.E3.m1.2.2.2.2.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.3.3">‚ä∫</ci></apply></apply></apply><cn id="S3.E3.m1.2.2.2.4.cmml" type="integer" xref="S3.E3.m1.2.2.2.4">1</cn></apply><cn id="S3.E3.m1.2.2.4.cmml" type="integer" xref="S3.E3.m1.2.2.4">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.6c">L_{R}(\Theta,\hat{\Theta})=\arccos\left(\frac{\operatorname{tr}(\Theta\hat{%
\Theta}^{\intercal})-1}{2}\right).</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.6d">italic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( roman_Œò , over^ start_ARG roman_Œò end_ARG ) = roman_arccos ( divide start_ARG roman_tr ( roman_Œò over^ start_ARG roman_Œò end_ARG start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT ) - 1 end_ARG start_ARG 2 end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.9"><span class="ltx_text ltx_font_bold" id="S3.SS5.p3.9.1">Position Loss.</span>
Our model uses the SMPL FK layer to obtain the keypoints <math alttext="\hat{K}" class="ltx_Math" display="inline" id="S3.SS5.p3.1.m1.1"><semantics id="S3.SS5.p3.1.m1.1a"><mover accent="true" id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml"><mi id="S3.SS5.p3.1.m1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.cmml">K</mi><mo id="S3.SS5.p3.1.m1.1.1.1" xref="S3.SS5.p3.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"><ci id="S3.SS5.p3.1.m1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1.1">^</ci><ci id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2">ùêæ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">\hat{K}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.1.m1.1d">over^ start_ARG italic_K end_ARG</annotation></semantics></math> and vertices <math alttext="\hat{V}" class="ltx_Math" display="inline" id="S3.SS5.p3.2.m2.1"><semantics id="S3.SS5.p3.2.m2.1a"><mover accent="true" id="S3.SS5.p3.2.m2.1.1" xref="S3.SS5.p3.2.m2.1.1.cmml"><mi id="S3.SS5.p3.2.m2.1.1.2" xref="S3.SS5.p3.2.m2.1.1.2.cmml">V</mi><mo id="S3.SS5.p3.2.m2.1.1.1" xref="S3.SS5.p3.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.2.m2.1b"><apply id="S3.SS5.p3.2.m2.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1"><ci id="S3.SS5.p3.2.m2.1.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1.1">^</ci><ci id="S3.SS5.p3.2.m2.1.1.2.cmml" xref="S3.SS5.p3.2.m2.1.1.2">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.2.m2.1c">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.2.m2.1d">over^ start_ARG italic_V end_ARG</annotation></semantics></math>. We then define a positional loss using <math alttext="L_{P}=L_{1;s}(\hat{K},K)+L_{1;s}(\hat{V},V)" class="ltx_Math" display="inline" id="S3.SS5.p3.3.m3.8"><semantics id="S3.SS5.p3.3.m3.8a"><mrow id="S3.SS5.p3.3.m3.8.9" xref="S3.SS5.p3.3.m3.8.9.cmml"><msub id="S3.SS5.p3.3.m3.8.9.2" xref="S3.SS5.p3.3.m3.8.9.2.cmml"><mi id="S3.SS5.p3.3.m3.8.9.2.2" xref="S3.SS5.p3.3.m3.8.9.2.2.cmml">L</mi><mi id="S3.SS5.p3.3.m3.8.9.2.3" xref="S3.SS5.p3.3.m3.8.9.2.3.cmml">P</mi></msub><mo id="S3.SS5.p3.3.m3.8.9.1" xref="S3.SS5.p3.3.m3.8.9.1.cmml">=</mo><mrow id="S3.SS5.p3.3.m3.8.9.3" xref="S3.SS5.p3.3.m3.8.9.3.cmml"><mrow id="S3.SS5.p3.3.m3.8.9.3.2" xref="S3.SS5.p3.3.m3.8.9.3.2.cmml"><msub id="S3.SS5.p3.3.m3.8.9.3.2.2" xref="S3.SS5.p3.3.m3.8.9.3.2.2.cmml"><mi id="S3.SS5.p3.3.m3.8.9.3.2.2.2" xref="S3.SS5.p3.3.m3.8.9.3.2.2.2.cmml">L</mi><mrow id="S3.SS5.p3.3.m3.2.2.2.4" xref="S3.SS5.p3.3.m3.2.2.2.3.cmml"><mn id="S3.SS5.p3.3.m3.1.1.1.1" xref="S3.SS5.p3.3.m3.1.1.1.1.cmml">1</mn><mo id="S3.SS5.p3.3.m3.2.2.2.4.1" xref="S3.SS5.p3.3.m3.2.2.2.3.cmml">;</mo><mi id="S3.SS5.p3.3.m3.2.2.2.2" xref="S3.SS5.p3.3.m3.2.2.2.2.cmml">s</mi></mrow></msub><mo id="S3.SS5.p3.3.m3.8.9.3.2.1" xref="S3.SS5.p3.3.m3.8.9.3.2.1.cmml">‚Å¢</mo><mrow id="S3.SS5.p3.3.m3.8.9.3.2.3.2" xref="S3.SS5.p3.3.m3.8.9.3.2.3.1.cmml"><mo id="S3.SS5.p3.3.m3.8.9.3.2.3.2.1" stretchy="false" xref="S3.SS5.p3.3.m3.8.9.3.2.3.1.cmml">(</mo><mover accent="true" id="S3.SS5.p3.3.m3.5.5" xref="S3.SS5.p3.3.m3.5.5.cmml"><mi id="S3.SS5.p3.3.m3.5.5.2" xref="S3.SS5.p3.3.m3.5.5.2.cmml">K</mi><mo id="S3.SS5.p3.3.m3.5.5.1" xref="S3.SS5.p3.3.m3.5.5.1.cmml">^</mo></mover><mo id="S3.SS5.p3.3.m3.8.9.3.2.3.2.2" xref="S3.SS5.p3.3.m3.8.9.3.2.3.1.cmml">,</mo><mi id="S3.SS5.p3.3.m3.6.6" xref="S3.SS5.p3.3.m3.6.6.cmml">K</mi><mo id="S3.SS5.p3.3.m3.8.9.3.2.3.2.3" stretchy="false" xref="S3.SS5.p3.3.m3.8.9.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS5.p3.3.m3.8.9.3.1" xref="S3.SS5.p3.3.m3.8.9.3.1.cmml">+</mo><mrow id="S3.SS5.p3.3.m3.8.9.3.3" xref="S3.SS5.p3.3.m3.8.9.3.3.cmml"><msub id="S3.SS5.p3.3.m3.8.9.3.3.2" xref="S3.SS5.p3.3.m3.8.9.3.3.2.cmml"><mi id="S3.SS5.p3.3.m3.8.9.3.3.2.2" xref="S3.SS5.p3.3.m3.8.9.3.3.2.2.cmml">L</mi><mrow id="S3.SS5.p3.3.m3.4.4.2.4" xref="S3.SS5.p3.3.m3.4.4.2.3.cmml"><mn id="S3.SS5.p3.3.m3.3.3.1.1" xref="S3.SS5.p3.3.m3.3.3.1.1.cmml">1</mn><mo id="S3.SS5.p3.3.m3.4.4.2.4.1" xref="S3.SS5.p3.3.m3.4.4.2.3.cmml">;</mo><mi id="S3.SS5.p3.3.m3.4.4.2.2" xref="S3.SS5.p3.3.m3.4.4.2.2.cmml">s</mi></mrow></msub><mo id="S3.SS5.p3.3.m3.8.9.3.3.1" xref="S3.SS5.p3.3.m3.8.9.3.3.1.cmml">‚Å¢</mo><mrow id="S3.SS5.p3.3.m3.8.9.3.3.3.2" xref="S3.SS5.p3.3.m3.8.9.3.3.3.1.cmml"><mo id="S3.SS5.p3.3.m3.8.9.3.3.3.2.1" stretchy="false" xref="S3.SS5.p3.3.m3.8.9.3.3.3.1.cmml">(</mo><mover accent="true" id="S3.SS5.p3.3.m3.7.7" xref="S3.SS5.p3.3.m3.7.7.cmml"><mi id="S3.SS5.p3.3.m3.7.7.2" xref="S3.SS5.p3.3.m3.7.7.2.cmml">V</mi><mo id="S3.SS5.p3.3.m3.7.7.1" xref="S3.SS5.p3.3.m3.7.7.1.cmml">^</mo></mover><mo id="S3.SS5.p3.3.m3.8.9.3.3.3.2.2" xref="S3.SS5.p3.3.m3.8.9.3.3.3.1.cmml">,</mo><mi id="S3.SS5.p3.3.m3.8.8" xref="S3.SS5.p3.3.m3.8.8.cmml">V</mi><mo id="S3.SS5.p3.3.m3.8.9.3.3.3.2.3" stretchy="false" xref="S3.SS5.p3.3.m3.8.9.3.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.3.m3.8b"><apply id="S3.SS5.p3.3.m3.8.9.cmml" xref="S3.SS5.p3.3.m3.8.9"><eq id="S3.SS5.p3.3.m3.8.9.1.cmml" xref="S3.SS5.p3.3.m3.8.9.1"></eq><apply id="S3.SS5.p3.3.m3.8.9.2.cmml" xref="S3.SS5.p3.3.m3.8.9.2"><csymbol cd="ambiguous" id="S3.SS5.p3.3.m3.8.9.2.1.cmml" xref="S3.SS5.p3.3.m3.8.9.2">subscript</csymbol><ci id="S3.SS5.p3.3.m3.8.9.2.2.cmml" xref="S3.SS5.p3.3.m3.8.9.2.2">ùêø</ci><ci id="S3.SS5.p3.3.m3.8.9.2.3.cmml" xref="S3.SS5.p3.3.m3.8.9.2.3">ùëÉ</ci></apply><apply id="S3.SS5.p3.3.m3.8.9.3.cmml" xref="S3.SS5.p3.3.m3.8.9.3"><plus id="S3.SS5.p3.3.m3.8.9.3.1.cmml" xref="S3.SS5.p3.3.m3.8.9.3.1"></plus><apply id="S3.SS5.p3.3.m3.8.9.3.2.cmml" xref="S3.SS5.p3.3.m3.8.9.3.2"><times id="S3.SS5.p3.3.m3.8.9.3.2.1.cmml" xref="S3.SS5.p3.3.m3.8.9.3.2.1"></times><apply id="S3.SS5.p3.3.m3.8.9.3.2.2.cmml" xref="S3.SS5.p3.3.m3.8.9.3.2.2"><csymbol cd="ambiguous" id="S3.SS5.p3.3.m3.8.9.3.2.2.1.cmml" xref="S3.SS5.p3.3.m3.8.9.3.2.2">subscript</csymbol><ci id="S3.SS5.p3.3.m3.8.9.3.2.2.2.cmml" xref="S3.SS5.p3.3.m3.8.9.3.2.2.2">ùêø</ci><list id="S3.SS5.p3.3.m3.2.2.2.3.cmml" xref="S3.SS5.p3.3.m3.2.2.2.4"><cn id="S3.SS5.p3.3.m3.1.1.1.1.cmml" type="integer" xref="S3.SS5.p3.3.m3.1.1.1.1">1</cn><ci id="S3.SS5.p3.3.m3.2.2.2.2.cmml" xref="S3.SS5.p3.3.m3.2.2.2.2">ùë†</ci></list></apply><interval closure="open" id="S3.SS5.p3.3.m3.8.9.3.2.3.1.cmml" xref="S3.SS5.p3.3.m3.8.9.3.2.3.2"><apply id="S3.SS5.p3.3.m3.5.5.cmml" xref="S3.SS5.p3.3.m3.5.5"><ci id="S3.SS5.p3.3.m3.5.5.1.cmml" xref="S3.SS5.p3.3.m3.5.5.1">^</ci><ci id="S3.SS5.p3.3.m3.5.5.2.cmml" xref="S3.SS5.p3.3.m3.5.5.2">ùêæ</ci></apply><ci id="S3.SS5.p3.3.m3.6.6.cmml" xref="S3.SS5.p3.3.m3.6.6">ùêæ</ci></interval></apply><apply id="S3.SS5.p3.3.m3.8.9.3.3.cmml" xref="S3.SS5.p3.3.m3.8.9.3.3"><times id="S3.SS5.p3.3.m3.8.9.3.3.1.cmml" xref="S3.SS5.p3.3.m3.8.9.3.3.1"></times><apply id="S3.SS5.p3.3.m3.8.9.3.3.2.cmml" xref="S3.SS5.p3.3.m3.8.9.3.3.2"><csymbol cd="ambiguous" id="S3.SS5.p3.3.m3.8.9.3.3.2.1.cmml" xref="S3.SS5.p3.3.m3.8.9.3.3.2">subscript</csymbol><ci id="S3.SS5.p3.3.m3.8.9.3.3.2.2.cmml" xref="S3.SS5.p3.3.m3.8.9.3.3.2.2">ùêø</ci><list id="S3.SS5.p3.3.m3.4.4.2.3.cmml" xref="S3.SS5.p3.3.m3.4.4.2.4"><cn id="S3.SS5.p3.3.m3.3.3.1.1.cmml" type="integer" xref="S3.SS5.p3.3.m3.3.3.1.1">1</cn><ci id="S3.SS5.p3.3.m3.4.4.2.2.cmml" xref="S3.SS5.p3.3.m3.4.4.2.2">ùë†</ci></list></apply><interval closure="open" id="S3.SS5.p3.3.m3.8.9.3.3.3.1.cmml" xref="S3.SS5.p3.3.m3.8.9.3.3.3.2"><apply id="S3.SS5.p3.3.m3.7.7.cmml" xref="S3.SS5.p3.3.m3.7.7"><ci id="S3.SS5.p3.3.m3.7.7.1.cmml" xref="S3.SS5.p3.3.m3.7.7.1">^</ci><ci id="S3.SS5.p3.3.m3.7.7.2.cmml" xref="S3.SS5.p3.3.m3.7.7.2">ùëâ</ci></apply><ci id="S3.SS5.p3.3.m3.8.8.cmml" xref="S3.SS5.p3.3.m3.8.8">ùëâ</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.3.m3.8c">L_{P}=L_{1;s}(\hat{K},K)+L_{1;s}(\hat{V},V)</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.3.m3.8d">italic_L start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT = italic_L start_POSTSUBSCRIPT 1 ; italic_s end_POSTSUBSCRIPT ( over^ start_ARG italic_K end_ARG , italic_K ) + italic_L start_POSTSUBSCRIPT 1 ; italic_s end_POSTSUBSCRIPT ( over^ start_ARG italic_V end_ARG , italic_V )</annotation></semantics></math>, where the <math alttext="L_{1;s}" class="ltx_Math" display="inline" id="S3.SS5.p3.4.m4.2"><semantics id="S3.SS5.p3.4.m4.2a"><msub id="S3.SS5.p3.4.m4.2.3" xref="S3.SS5.p3.4.m4.2.3.cmml"><mi id="S3.SS5.p3.4.m4.2.3.2" xref="S3.SS5.p3.4.m4.2.3.2.cmml">L</mi><mrow id="S3.SS5.p3.4.m4.2.2.2.4" xref="S3.SS5.p3.4.m4.2.2.2.3.cmml"><mn id="S3.SS5.p3.4.m4.1.1.1.1" xref="S3.SS5.p3.4.m4.1.1.1.1.cmml">1</mn><mo id="S3.SS5.p3.4.m4.2.2.2.4.1" xref="S3.SS5.p3.4.m4.2.2.2.3.cmml">;</mo><mi id="S3.SS5.p3.4.m4.2.2.2.2" xref="S3.SS5.p3.4.m4.2.2.2.2.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.4.m4.2b"><apply id="S3.SS5.p3.4.m4.2.3.cmml" xref="S3.SS5.p3.4.m4.2.3"><csymbol cd="ambiguous" id="S3.SS5.p3.4.m4.2.3.1.cmml" xref="S3.SS5.p3.4.m4.2.3">subscript</csymbol><ci id="S3.SS5.p3.4.m4.2.3.2.cmml" xref="S3.SS5.p3.4.m4.2.3.2">ùêø</ci><list id="S3.SS5.p3.4.m4.2.2.2.3.cmml" xref="S3.SS5.p3.4.m4.2.2.2.4"><cn id="S3.SS5.p3.4.m4.1.1.1.1.cmml" type="integer" xref="S3.SS5.p3.4.m4.1.1.1.1">1</cn><ci id="S3.SS5.p3.4.m4.2.2.2.2.cmml" xref="S3.SS5.p3.4.m4.2.2.2.2">ùë†</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.4.m4.2c">L_{1;s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.4.m4.2d">italic_L start_POSTSUBSCRIPT 1 ; italic_s end_POSTSUBSCRIPT</annotation></semantics></math> represents a smoothed <math alttext="L_{1}" class="ltx_Math" display="inline" id="S3.SS5.p3.5.m5.1"><semantics id="S3.SS5.p3.5.m5.1a"><msub id="S3.SS5.p3.5.m5.1.1" xref="S3.SS5.p3.5.m5.1.1.cmml"><mi id="S3.SS5.p3.5.m5.1.1.2" xref="S3.SS5.p3.5.m5.1.1.2.cmml">L</mi><mn id="S3.SS5.p3.5.m5.1.1.3" xref="S3.SS5.p3.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.5.m5.1b"><apply id="S3.SS5.p3.5.m5.1.1.cmml" xref="S3.SS5.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS5.p3.5.m5.1.1.1.cmml" xref="S3.SS5.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS5.p3.5.m5.1.1.2.cmml" xref="S3.SS5.p3.5.m5.1.1.2">ùêø</ci><cn id="S3.SS5.p3.5.m5.1.1.3.cmml" type="integer" xref="S3.SS5.p3.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.5.m5.1c">L_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.5.m5.1d">italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib51" title="">51</a>]</cite>. This loss can be seen as a combination of <math alttext="L_{1}" class="ltx_Math" display="inline" id="S3.SS5.p3.6.m6.1"><semantics id="S3.SS5.p3.6.m6.1a"><msub id="S3.SS5.p3.6.m6.1.1" xref="S3.SS5.p3.6.m6.1.1.cmml"><mi id="S3.SS5.p3.6.m6.1.1.2" xref="S3.SS5.p3.6.m6.1.1.2.cmml">L</mi><mn id="S3.SS5.p3.6.m6.1.1.3" xref="S3.SS5.p3.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.6.m6.1b"><apply id="S3.SS5.p3.6.m6.1.1.cmml" xref="S3.SS5.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS5.p3.6.m6.1.1.1.cmml" xref="S3.SS5.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS5.p3.6.m6.1.1.2.cmml" xref="S3.SS5.p3.6.m6.1.1.2">ùêø</ci><cn id="S3.SS5.p3.6.m6.1.1.3.cmml" type="integer" xref="S3.SS5.p3.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.6.m6.1c">L_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.6.m6.1d">italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="L_{2}" class="ltx_Math" display="inline" id="S3.SS5.p3.7.m7.1"><semantics id="S3.SS5.p3.7.m7.1a"><msub id="S3.SS5.p3.7.m7.1.1" xref="S3.SS5.p3.7.m7.1.1.cmml"><mi id="S3.SS5.p3.7.m7.1.1.2" xref="S3.SS5.p3.7.m7.1.1.2.cmml">L</mi><mn id="S3.SS5.p3.7.m7.1.1.3" xref="S3.SS5.p3.7.m7.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.7.m7.1b"><apply id="S3.SS5.p3.7.m7.1.1.cmml" xref="S3.SS5.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS5.p3.7.m7.1.1.1.cmml" xref="S3.SS5.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS5.p3.7.m7.1.1.2.cmml" xref="S3.SS5.p3.7.m7.1.1.2">ùêø</ci><cn id="S3.SS5.p3.7.m7.1.1.3.cmml" type="integer" xref="S3.SS5.p3.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.7.m7.1c">L_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.7.m7.1d">italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> distances, which is less susceptible to outliers compared to <math alttext="L_{2}" class="ltx_Math" display="inline" id="S3.SS5.p3.8.m8.1"><semantics id="S3.SS5.p3.8.m8.1a"><msub id="S3.SS5.p3.8.m8.1.1" xref="S3.SS5.p3.8.m8.1.1.cmml"><mi id="S3.SS5.p3.8.m8.1.1.2" xref="S3.SS5.p3.8.m8.1.1.2.cmml">L</mi><mn id="S3.SS5.p3.8.m8.1.1.3" xref="S3.SS5.p3.8.m8.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.8.m8.1b"><apply id="S3.SS5.p3.8.m8.1.1.cmml" xref="S3.SS5.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS5.p3.8.m8.1.1.1.cmml" xref="S3.SS5.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS5.p3.8.m8.1.1.2.cmml" xref="S3.SS5.p3.8.m8.1.1.2">ùêø</ci><cn id="S3.SS5.p3.8.m8.1.1.3.cmml" type="integer" xref="S3.SS5.p3.8.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.8.m8.1c">L_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.8.m8.1d">italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> with less near-zero penalty compared to the <math alttext="L_{1}" class="ltx_Math" display="inline" id="S3.SS5.p3.9.m9.1"><semantics id="S3.SS5.p3.9.m9.1a"><msub id="S3.SS5.p3.9.m9.1.1" xref="S3.SS5.p3.9.m9.1.1.cmml"><mi id="S3.SS5.p3.9.m9.1.1.2" xref="S3.SS5.p3.9.m9.1.1.2.cmml">L</mi><mn id="S3.SS5.p3.9.m9.1.1.3" xref="S3.SS5.p3.9.m9.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.9.m9.1b"><apply id="S3.SS5.p3.9.m9.1.1.cmml" xref="S3.SS5.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS5.p3.9.m9.1.1.1.cmml" xref="S3.SS5.p3.9.m9.1.1">subscript</csymbol><ci id="S3.SS5.p3.9.m9.1.1.2.cmml" xref="S3.SS5.p3.9.m9.1.1.2">ùêø</ci><cn id="S3.SS5.p3.9.m9.1.1.3.cmml" type="integer" xref="S3.SS5.p3.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.9.m9.1c">L_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.9.m9.1d">italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> distance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.3"><span class="ltx_text ltx_font_bold" id="S3.SS5.p4.3.1">Shape Loss.</span>
Finally, we minimize the <math alttext="L_{S}=L_{2}(\hat{\beta},\beta)" class="ltx_Math" display="inline" id="S3.SS5.p4.1.m1.2"><semantics id="S3.SS5.p4.1.m1.2a"><mrow id="S3.SS5.p4.1.m1.2.3" xref="S3.SS5.p4.1.m1.2.3.cmml"><msub id="S3.SS5.p4.1.m1.2.3.2" xref="S3.SS5.p4.1.m1.2.3.2.cmml"><mi id="S3.SS5.p4.1.m1.2.3.2.2" xref="S3.SS5.p4.1.m1.2.3.2.2.cmml">L</mi><mi id="S3.SS5.p4.1.m1.2.3.2.3" xref="S3.SS5.p4.1.m1.2.3.2.3.cmml">S</mi></msub><mo id="S3.SS5.p4.1.m1.2.3.1" xref="S3.SS5.p4.1.m1.2.3.1.cmml">=</mo><mrow id="S3.SS5.p4.1.m1.2.3.3" xref="S3.SS5.p4.1.m1.2.3.3.cmml"><msub id="S3.SS5.p4.1.m1.2.3.3.2" xref="S3.SS5.p4.1.m1.2.3.3.2.cmml"><mi id="S3.SS5.p4.1.m1.2.3.3.2.2" xref="S3.SS5.p4.1.m1.2.3.3.2.2.cmml">L</mi><mn id="S3.SS5.p4.1.m1.2.3.3.2.3" xref="S3.SS5.p4.1.m1.2.3.3.2.3.cmml">2</mn></msub><mo id="S3.SS5.p4.1.m1.2.3.3.1" xref="S3.SS5.p4.1.m1.2.3.3.1.cmml">‚Å¢</mo><mrow id="S3.SS5.p4.1.m1.2.3.3.3.2" xref="S3.SS5.p4.1.m1.2.3.3.3.1.cmml"><mo id="S3.SS5.p4.1.m1.2.3.3.3.2.1" stretchy="false" xref="S3.SS5.p4.1.m1.2.3.3.3.1.cmml">(</mo><mover accent="true" id="S3.SS5.p4.1.m1.1.1" xref="S3.SS5.p4.1.m1.1.1.cmml"><mi id="S3.SS5.p4.1.m1.1.1.2" xref="S3.SS5.p4.1.m1.1.1.2.cmml">Œ≤</mi><mo id="S3.SS5.p4.1.m1.1.1.1" xref="S3.SS5.p4.1.m1.1.1.1.cmml">^</mo></mover><mo id="S3.SS5.p4.1.m1.2.3.3.3.2.2" xref="S3.SS5.p4.1.m1.2.3.3.3.1.cmml">,</mo><mi id="S3.SS5.p4.1.m1.2.2" xref="S3.SS5.p4.1.m1.2.2.cmml">Œ≤</mi><mo id="S3.SS5.p4.1.m1.2.3.3.3.2.3" stretchy="false" xref="S3.SS5.p4.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.1.m1.2b"><apply id="S3.SS5.p4.1.m1.2.3.cmml" xref="S3.SS5.p4.1.m1.2.3"><eq id="S3.SS5.p4.1.m1.2.3.1.cmml" xref="S3.SS5.p4.1.m1.2.3.1"></eq><apply id="S3.SS5.p4.1.m1.2.3.2.cmml" xref="S3.SS5.p4.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS5.p4.1.m1.2.3.2.1.cmml" xref="S3.SS5.p4.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS5.p4.1.m1.2.3.2.2.cmml" xref="S3.SS5.p4.1.m1.2.3.2.2">ùêø</ci><ci id="S3.SS5.p4.1.m1.2.3.2.3.cmml" xref="S3.SS5.p4.1.m1.2.3.2.3">ùëÜ</ci></apply><apply id="S3.SS5.p4.1.m1.2.3.3.cmml" xref="S3.SS5.p4.1.m1.2.3.3"><times id="S3.SS5.p4.1.m1.2.3.3.1.cmml" xref="S3.SS5.p4.1.m1.2.3.3.1"></times><apply id="S3.SS5.p4.1.m1.2.3.3.2.cmml" xref="S3.SS5.p4.1.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.SS5.p4.1.m1.2.3.3.2.1.cmml" xref="S3.SS5.p4.1.m1.2.3.3.2">subscript</csymbol><ci id="S3.SS5.p4.1.m1.2.3.3.2.2.cmml" xref="S3.SS5.p4.1.m1.2.3.3.2.2">ùêø</ci><cn id="S3.SS5.p4.1.m1.2.3.3.2.3.cmml" type="integer" xref="S3.SS5.p4.1.m1.2.3.3.2.3">2</cn></apply><interval closure="open" id="S3.SS5.p4.1.m1.2.3.3.3.1.cmml" xref="S3.SS5.p4.1.m1.2.3.3.3.2"><apply id="S3.SS5.p4.1.m1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1"><ci id="S3.SS5.p4.1.m1.1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1.1">^</ci><ci id="S3.SS5.p4.1.m1.1.1.2.cmml" xref="S3.SS5.p4.1.m1.1.1.2">ùõΩ</ci></apply><ci id="S3.SS5.p4.1.m1.2.2.cmml" xref="S3.SS5.p4.1.m1.2.2">ùõΩ</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.1.m1.2c">L_{S}=L_{2}(\hat{\beta},\beta)</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p4.1.m1.2d">italic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( over^ start_ARG italic_Œ≤ end_ARG , italic_Œ≤ )</annotation></semantics></math> distance between the estimated <math alttext="\hat{\beta}" class="ltx_Math" display="inline" id="S3.SS5.p4.2.m2.1"><semantics id="S3.SS5.p4.2.m2.1a"><mover accent="true" id="S3.SS5.p4.2.m2.1.1" xref="S3.SS5.p4.2.m2.1.1.cmml"><mi id="S3.SS5.p4.2.m2.1.1.2" xref="S3.SS5.p4.2.m2.1.1.2.cmml">Œ≤</mi><mo id="S3.SS5.p4.2.m2.1.1.1" xref="S3.SS5.p4.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.2.m2.1b"><apply id="S3.SS5.p4.2.m2.1.1.cmml" xref="S3.SS5.p4.2.m2.1.1"><ci id="S3.SS5.p4.2.m2.1.1.1.cmml" xref="S3.SS5.p4.2.m2.1.1.1">^</ci><ci id="S3.SS5.p4.2.m2.1.1.2.cmml" xref="S3.SS5.p4.2.m2.1.1.2">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.2.m2.1c">\hat{\beta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p4.2.m2.1d">over^ start_ARG italic_Œ≤ end_ARG</annotation></semantics></math> and ground-truth shape parameters <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS5.p4.3.m3.1"><semantics id="S3.SS5.p4.3.m3.1a"><mi id="S3.SS5.p4.3.m3.1.1" xref="S3.SS5.p4.3.m3.1.1.cmml">Œ≤</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.3.m3.1b"><ci id="S3.SS5.p4.3.m3.1.1.cmml" xref="S3.SS5.p4.3.m3.1.1">ùõΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.3.m3.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p4.3.m3.1d">italic_Œ≤</annotation></semantics></math> to train our shape decoder.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Datasets</span>
</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">AMASS.</span>
The Archive of Motion Capture as Surface Shapes (AMASS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib46" title="">46</a>]</cite> is a collection of 3D human pose and shape information collected from multiple motion capture databases. It contains over 40 hours of motion capture data from more than 300 subjects and spans over 11,000 actions. We follow the standard train, test, and validation splits used in prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib27" title="">27</a>]</cite>. This dataset is used solely for training the skeletal transformer IK solver.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Human3.6m.</span>
Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib5" title="">5</a>]</cite> is the standard benchmark for evaluating 3D human pose, shape, and body estimation in multi-view and single-view approaches. Following previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib52" title="">52</a>]</cite>, we perform our evaluations using Protocol-I, where the root-centered MPJPE and PA-MPJPE on subjects 9 and 11 are measured. We use this dataset for <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p2.1.2">InD</span> evaluation of our method.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">RICH.</span>
Real scenes, Interaction, Contact and Humans (RICH) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib53" title="">53</a>]</cite> is a recently published dataset of multi-view videos with accurate markerless motion-captured bodies and scenes. The test set contains one withheld scene and 7 unseen subjects in 52 scenarios, captured using four cameras. We use this dataset for <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p3.1.2">OoD</span> evaluation of our method in outdoor settings.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">MPI-INF-3DHP.</span>
The Max Planck Institute for Informatics 3D Human Pose dataset (MPI-INF-3DHP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib54" title="">54</a>]</cite> is a collection of over 1.5 million frames captured from eight angles, featuring eight actors performing various actions like sitting, jumping, dancing, and exercising. We follow the previous works by evaluating our model on subject 8 of the training set <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib55" title="">55</a>]</cite>. We use this dataset for <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p4.1.2">OoD</span> evaluation of our method.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Evaluation Metrics</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In order to evaluate the performance of our method, we employ standard evaluation metrics in 3D pose estimation literature. Mean-Per-Joint-Position-Error (<span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">MPJPE</span>) measures the Euclidean distance between the estimated joint positions and the ground-truth joint positions, averaged over all joints in the skeleton.
<span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.2">PA-MPJPE</span> is an extension of MPJPE, where a rigid alignment between the estimated and ground-truth keypoints is applied prior to error measurement. This metric shows how well the skeleton is estimated, regardless of scaling and rotation. Additionally, we report <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.3">MPVPE</span> and <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.4">PA-MPVPE</span> to show the error between ground-truth and predicted vertices of body mesh. Additionally, we report <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.5">AUC</span> and <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.6">PCK</span> at a threshold of 150 <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.7">mm</span> according to MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib54" title="">54</a>]</cite> evaluation criteria. Finally, <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.8">Rotation Error</span> is measured by the geodesic distance between the ground-truth and predicted poses.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Implementation Details</span>
</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Hyper-parameters.</span>
We select 65 joints from the 133 joints of the whole-body skeleton configuration, excluding most facial landmarks while keeping the eyes, ears, and nose. We choose two transformer blocks for the encoder and each of the decoders. As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S3.F2" title="In III-A 3D Keypoint Estimation ‚Ä£ III Methodology ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>, We use a positional embedding of size 64 in the encoder and decoder while setting hidden layer dimensions to 128 within all layers. The shape and pose decoder heads inherit 1024 dimensional residual layers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Optimization.</span>
We train our model using AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib56" title="">56</a>]</cite> with a batch size of 1024 on an NVIDIA A4000 GPU. We chose a learning rate of 1e-3, which is warmed up with a factor of 1e-4 for the first 2000 iterations and gradually reduced with a cosine annealing scheduler over 50000 iterations until it reaches 1e-7. The training of the network takes less than 18 hours to complete.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Mirror Test.</span>
Similar to the flipping test performed in 2D keypoint estimation methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib7" title="">7</a>]</cite>, we perform a mirroring test during inference to reduce the model‚Äôs biases towards left and right body parts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">Computation Cost.</span>
Our model contains 6.631¬†<span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.2">M</span> parameters with 159.482¬†<span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.3">M</span> FLOPs for a single input. As a result, our skeletal transformer can solve the IK problem in 66¬†<span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.4">ms</span> for a batch size of 512 using approximately 4G of GPU memory. Consequently, our SkelFormer pipeline takes 274¬†<span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.5">ms</span> to predict pose and shape parameters from one frame of four cameras.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>The comparison of our method in InD settings against prior multi-view works on the full test set of the Human3.6m dataset. * denotes the results from using ground-truth 3D keypoints as input</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1" style="font-size:70%;">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.1" style="font-size:70%;">PA-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.1.m1.1"><semantics id="S4.T1.2.2.2.1.m1.1a"><mo id="S4.T1.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.1" style="font-size:70%;">Output</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.3.4.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.4.1.1.1" style="font-size:70%;color:#808080;">CPN<span class="ltx_text ltx_font_typewriter" id="S4.T1.3.4.1.1.1.1">+</span>DLT</span><span class="ltx_text" id="S4.T1.3.4.1.1.2" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.4.1.1.3.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib57" title="">57</a><span class="ltx_text" id="S4.T1.3.4.1.1.4.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.3.4.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.4.1.2.1" style="font-size:70%;color:#808080;">32.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.3.4.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.4.1.3.1" style="font-size:70%;color:#808080;">27.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.3.4.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.4.1.4.1" style="font-size:70%;color:#808080;">Joint Pos. Only</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.5.2">
<td class="ltx_td ltx_align_left" id="S4.T1.3.5.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.5.2.1.1" style="font-size:70%;color:#808080;">LT</span><span class="ltx_text" id="S4.T1.3.5.2.1.2" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.5.2.1.3.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib17" title="">17</a><span class="ltx_text" id="S4.T1.3.5.2.1.4.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.5.2.2.1" style="font-size:70%;color:#808080;">20.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.5.2.3.1" style="font-size:70%;color:#808080;">17.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.5.2.4.1" style="font-size:70%;color:#808080;">Joint Pos. Only</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.6.3">
<td class="ltx_td ltx_align_left" id="S4.T1.3.6.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.6.3.1.1" style="font-size:70%;color:#808080;">Pose2Mesh</span><span class="ltx_text" id="S4.T1.3.6.3.1.2" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.6.3.1.3.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib21" title="">21</a><span class="ltx_text" id="S4.T1.3.6.3.1.4.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T1.3.6.3.1.5" style="font-size:70%;">*</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.6.3.2.1" style="font-size:70%;color:#808080;">29.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.6.3.3.1" style="font-size:70%;color:#808080;">23.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.6.3.4.1" style="font-size:70%;color:#808080;">Mesh Only</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.7.4">
<td class="ltx_td ltx_align_left" id="S4.T1.3.7.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.7.4.1.1" style="font-size:70%;">Huang </span><em class="ltx_emph ltx_font_italic" id="S4.T1.3.7.4.1.2" style="font-size:70%;">et al</em><span class="ltx_text" id="S4.T1.3.7.4.1.3" style="font-size:70%;">.</span><span class="ltx_text" id="S4.T1.3.7.4.1.4"></span><span class="ltx_text" id="S4.T1.3.7.4.1.5" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.7.4.1.6.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib58" title="">58</a><span class="ltx_text" id="S4.T1.3.7.4.1.7.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.4.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.7.4.2.1" style="font-size:70%;">58.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.4.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.7.4.3.1" style="font-size:70%;">47.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.7.4.4.1" style="font-size:70%;">Joint Rot.</span><span class="ltx_text ltx_font_typewriter" id="S4.T1.3.7.4.4.2" style="font-size:70%;">+</span><span class="ltx_text" id="S4.T1.3.7.4.4.3" style="font-size:70%;">Mesh</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3">
<td class="ltx_td ltx_align_left" id="S4.T1.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.3.1.1" style="font-size:70%;">Shin and Halilaj (SPIN</span><sup class="ltx_sup" id="S4.T1.3.3.1.2"><span class="ltx_text ltx_font_italic" id="S4.T1.3.3.1.2.1" style="font-size:70%;">4,cal</span></sup><span class="ltx_text" id="S4.T1.3.3.1.3" style="font-size:70%;">) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.3.1.4.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib25" title="">25</a><span class="ltx_text" id="S4.T1.3.3.1.5.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.3.2.1" style="font-size:70%;">49.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.3.3.1" style="font-size:70%;">35.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.3.4.1" style="font-size:70%;">Joint Rot.</span><span class="ltx_text ltx_font_typewriter" id="S4.T1.3.3.4.2" style="font-size:70%;">+</span><span class="ltx_text" id="S4.T1.3.3.4.3" style="font-size:70%;">Mesh</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.8.5">
<td class="ltx_td ltx_align_left" id="S4.T1.3.8.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.8.5.1.1" style="font-size:70%;">Shin and Halilaj (main) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.8.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib25" title="">25</a><span class="ltx_text" id="S4.T1.3.8.5.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.8.5.2.1" style="font-size:70%;">46.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.8.5.3.1" style="font-size:70%;">32.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.8.5.4.1" style="font-size:70%;">Joint Rot.</span><span class="ltx_text ltx_font_typewriter" id="S4.T1.3.8.5.4.2" style="font-size:70%;">+</span><span class="ltx_text" id="S4.T1.3.8.5.4.3" style="font-size:70%;">Mesh</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.9.6">
<td class="ltx_td ltx_align_left" id="S4.T1.3.9.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.9.6.1.1" style="font-size:70%;">Gong </span><em class="ltx_emph ltx_font_italic" id="S4.T1.3.9.6.1.2" style="font-size:70%;">et al</em><span class="ltx_text" id="S4.T1.3.9.6.1.3" style="font-size:70%;">.</span><span class="ltx_text" id="S4.T1.3.9.6.1.4"></span><span class="ltx_text" id="S4.T1.3.9.6.1.5" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.9.6.1.6.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib59" title="">59</a><span class="ltx_text" id="S4.T1.3.9.6.1.7.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.6.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.9.6.2.1" style="font-size:70%;">53.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.9.6.3.1" style="font-size:70%;">42.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.9.6.4.1" style="font-size:70%;">Joint Rot.</span><span class="ltx_text ltx_font_typewriter" id="S4.T1.3.9.6.4.2" style="font-size:70%;">+</span><span class="ltx_text" id="S4.T1.3.9.6.4.3" style="font-size:70%;">Mesh</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.10.7">
<td class="ltx_td ltx_align_left" id="S4.T1.3.10.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.10.7.1.1" style="font-size:70%;">Jiang </span><em class="ltx_emph ltx_font_italic" id="S4.T1.3.10.7.1.2" style="font-size:70%;">et al</em><span class="ltx_text" id="S4.T1.3.10.7.1.3" style="font-size:70%;">.</span><span class="ltx_text" id="S4.T1.3.10.7.1.4"></span><span class="ltx_text" id="S4.T1.3.10.7.1.5" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.10.7.1.6.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib60" title="">60</a><span class="ltx_text" id="S4.T1.3.10.7.1.7.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.10.7.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.10.7.2.1" style="font-size:70%;">50.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.10.7.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.10.7.3.1" style="font-size:70%;">37.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.10.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.10.7.4.1" style="font-size:70%;">Joint Rot.</span><span class="ltx_text ltx_font_typewriter" id="S4.T1.3.10.7.4.2" style="font-size:70%;">+</span><span class="ltx_text" id="S4.T1.3.10.7.4.3" style="font-size:70%;">Mesh</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.11.8">
<td class="ltx_td ltx_align_left" id="S4.T1.3.11.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.11.8.1.1" style="font-size:70%;">Jia </span><em class="ltx_emph ltx_font_italic" id="S4.T1.3.11.8.1.2" style="font-size:70%;">et al</em><span class="ltx_text" id="S4.T1.3.11.8.1.3" style="font-size:70%;">.</span><span class="ltx_text" id="S4.T1.3.11.8.1.4"></span><span class="ltx_text" id="S4.T1.3.11.8.1.5" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.11.8.1.6.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib4" title="">4</a><span class="ltx_text" id="S4.T1.3.11.8.1.7.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.11.8.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.11.8.2.1" style="font-size:70%;">33.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.11.8.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.11.8.3.1" style="font-size:70%;">26.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.11.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.11.8.4.1" style="font-size:70%;">Joint Rot.</span><span class="ltx_text ltx_font_typewriter" id="S4.T1.3.11.8.4.2" style="font-size:70%;">+</span><span class="ltx_text" id="S4.T1.3.11.8.4.3" style="font-size:70%;">Mesh</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.12.9">
<td class="ltx_td ltx_align_left" id="S4.T1.3.12.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.12.9.1.1" style="font-size:70%;">SMPLify-X (LT) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.12.9.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib27" title="">27</a><span class="ltx_text" id="S4.T1.3.12.9.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.12.9.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.12.9.2.1" style="font-size:70%;">26.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.12.9.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.12.9.3.1" style="font-size:70%;">21.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.12.9.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.12.9.4.1" style="font-size:70%;">Joint Rot.</span><span class="ltx_text ltx_font_typewriter" id="S4.T1.3.12.9.4.2" style="font-size:70%;">+</span><span class="ltx_text" id="S4.T1.3.12.9.4.3" style="font-size:70%;">Mesh</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.13.10">
<td class="ltx_td ltx_align_left" id="S4.T1.3.13.10.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.13.10.1.1" style="font-size:70%;">SkelFormer (CPN)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.13.10.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.13.10.2.1" style="font-size:70%;">33.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.13.10.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.13.10.3.1" style="font-size:70%;">27.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.13.10.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.13.10.4.1" style="font-size:70%;">Joint Rot.</span><span class="ltx_text ltx_font_typewriter" id="S4.T1.3.13.10.4.2" style="font-size:70%;">+</span><span class="ltx_text" id="S4.T1.3.13.10.4.3" style="font-size:70%;">Mesh</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.14.11">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.3.14.11.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.14.11.1.1" style="font-size:70%;">SkelFormer (LT)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.3.14.11.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.14.11.2.1" style="font-size:70%;">25.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.3.14.11.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.14.11.3.1" style="font-size:70%;">20.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.3.14.11.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.14.11.4.1" style="font-size:70%;">Joint Rot.</span><span class="ltx_text ltx_font_typewriter" id="S4.T1.3.14.11.4.2" style="font-size:70%;">+</span><span class="ltx_text" id="S4.T1.3.14.11.4.3" style="font-size:70%;">Mesh</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of our method in OoD setting against prior works. </figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.5.5.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.5.6.1" style="font-size:70%;">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1" style="font-size:70%;">MPVPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.1" style="font-size:70%;">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.1.m1.1"><semantics id="S4.T2.2.2.2.1.m1.1a"><mo id="S4.T2.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T2.2.2.2.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><ci id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.1.m1.1d">‚Üì</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.1" style="font-size:70%;">PA-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.1.m1.1"><semantics id="S4.T2.3.3.3.1.m1.1a"><mo id="S4.T2.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T2.3.3.3.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><ci id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.1.m1.1d">‚Üì</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.1" style="font-size:70%;">PCK<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.4.4.4.1.m1.1"><semantics id="S4.T2.4.4.4.1.m1.1a"><mo id="S4.T2.4.4.4.1.m1.1.1" stretchy="false" xref="S4.T2.4.4.4.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><ci id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.1.m1.1d">‚Üë</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.5.5.1" style="font-size:70%;">AUC<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.5.5.5.1.m1.1"><semantics id="S4.T2.5.5.5.1.m1.1a"><mo id="S4.T2.5.5.5.1.m1.1.1" stretchy="false" xref="S4.T2.5.5.5.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.1.m1.1b"><ci id="S4.T2.5.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.5.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.1.m1.1d">‚Üë</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.5.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.5.7.1" style="font-size:70%;">OoD</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.6.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="7" id="S4.T2.5.6.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.6.1.1.1" style="font-size:70%;">RICH Dataset</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.5.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.5.7.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T2.5.7.2.1.1" style="font-size:70%;">METRO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.7.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib61" title="">61</a><span class="ltx_text" id="S4.T2.5.7.2.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.7.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.7.2.2.1" style="font-size:70%;">134.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.7.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.7.2.3.1" style="font-size:70%;">129.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.7.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.7.2.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.7.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.7.2.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.7.2.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.7.2.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.7.2.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.7.2.7.1" style="font-size:70%;">‚úì</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.8.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T2.5.8.3.1.1" style="font-size:70%;">METRO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.8.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib61" title="">61</a><span class="ltx_text" id="S4.T2.5.8.3.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.8.3.2.1" style="font-size:70%;">107.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.8.3.3.1" style="font-size:70%;">98.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.8.3.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.8.3.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.3.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.8.3.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.3.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.8.3.7.1" style="font-size:70%;">‚úó</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.9.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.9.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T2.5.9.4.1.1" style="font-size:70%;">SA-HMR </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.9.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib62" title="">62</a><span class="ltx_text" id="S4.T2.5.9.4.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.4.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.9.4.2.1" style="font-size:70%;">103.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.4.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.9.4.3.1" style="font-size:70%;">93.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.9.4.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.4.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.9.4.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.4.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.9.4.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.4.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.9.4.7.1" style="font-size:70%;">‚úó</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.10.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.10.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T2.5.10.5.1.1" style="font-size:70%;">IPMAN-R </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.10.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib63" title="">63</a><span class="ltx_text" id="S4.T2.5.10.5.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.10.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.10.5.2.1" style="font-size:70%;">89.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.10.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.10.5.3.1" style="font-size:70%;">79.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.10.5.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.10.5.4.1" style="font-size:70%;">47.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.10.5.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.10.5.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.10.5.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.10.5.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.10.5.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.10.5.7.1" style="font-size:70%;">‚úó</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.11.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.11.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T2.5.11.6.1.1" style="font-size:70%;">SPIN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.11.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib43" title="">43</a><span class="ltx_text" id="S4.T2.5.11.6.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.6.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.11.6.2.1" style="font-size:70%;">129.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.11.6.3.1" style="font-size:70%;">112.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.11.6.4.1" style="font-size:70%;">71.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.6.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.11.6.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.11.6.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.6.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.11.6.7.1" style="font-size:70%;">‚úì</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.12.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.12.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T2.5.12.7.1.1" style="font-size:70%;">PARE </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.12.7.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib64" title="">64</a><span class="ltx_text" id="S4.T2.5.12.7.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.12.7.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.12.7.2.1" style="font-size:70%;">125.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.12.7.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.12.7.3.1" style="font-size:70%;">107.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.12.7.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.12.7.4.1" style="font-size:70%;">73.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.12.7.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.12.7.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.12.7.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.12.7.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.12.7.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.12.7.7.1" style="font-size:70%;">‚úì</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.13.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.13.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T2.5.13.8.1.1" style="font-size:70%;">CLIFF </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.13.8.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib65" title="">65</a><span class="ltx_text" id="S4.T2.5.13.8.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.13.8.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.13.8.2.1" style="font-size:70%;">122.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.13.8.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.13.8.3.1" style="font-size:70%;">107.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.13.8.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.13.8.4.1" style="font-size:70%;">67.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.13.8.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.13.8.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.13.8.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.13.8.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.13.8.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.13.8.7.1" style="font-size:70%;">‚úì</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.14.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.14.9.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.14.9.1.1" style="font-size:70%;">SkelFormer (HRNet)</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.14.9.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.14.9.2.1" style="font-size:70%;">39.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.14.9.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.14.9.3.1" style="font-size:70%;">44.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.14.9.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.14.9.4.1" style="font-size:70%;">35.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.14.9.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.14.9.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.14.9.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.14.9.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.14.9.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.14.9.7.1" style="font-size:70%;">‚úì</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.15.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S4.T2.5.15.10.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.15.10.1.1" style="font-size:70%;">MPI-INF-3DHP Dataset</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.5.16.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.5.16.11.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T2.5.16.11.1.1" style="font-size:70%;">Liang and Lin </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.16.11.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib55" title="">55</a><span class="ltx_text" id="S4.T2.5.16.11.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.16.11.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.16.11.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.16.11.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.16.11.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.16.11.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.16.11.4.1" style="font-size:70%;">59.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.16.11.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.16.11.5.1" style="font-size:70%;">95.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.16.11.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.16.11.6.1" style="font-size:70%;">65.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.16.11.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.16.11.7.1" style="font-size:70%;">‚úó</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.17.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.17.12.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T2.5.17.12.1.1" style="font-size:70%;">Shin and Halilaj </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.17.12.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib25" title="">25</a><span class="ltx_text" id="S4.T2.5.17.12.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.17.12.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.17.12.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.17.12.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.17.12.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.17.12.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.17.12.4.1" style="font-size:70%;">50.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.17.12.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.17.12.5.1" style="font-size:70%;">97.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.17.12.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.17.12.6.1" style="font-size:70%;">65.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.17.12.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.17.12.7.1" style="font-size:70%;">‚úó</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.18.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.18.13.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T2.5.18.13.1.1" style="font-size:70%;">Jia </span><em class="ltx_emph ltx_font_italic" id="S4.T2.5.18.13.1.2" style="font-size:70%;">et al</em><span class="ltx_text" id="S4.T2.5.18.13.1.3" style="font-size:70%;">.</span><span class="ltx_text" id="S4.T2.5.18.13.1.4"></span><span class="ltx_text" id="S4.T2.5.18.13.1.5" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.18.13.1.6.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib4" title="">4</a><span class="ltx_text" id="S4.T2.5.18.13.1.7.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.18.13.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.18.13.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.18.13.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.18.13.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.18.13.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.18.13.4.1" style="font-size:70%;">48.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.18.13.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.18.13.5.1" style="font-size:70%;">98.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.18.13.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.18.13.6.1" style="font-size:70%;">67.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.18.13.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.18.13.7.1" style="font-size:70%;">‚úó</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.19.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.5.19.14.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.19.14.1.1" style="font-size:70%;">SkelFormer (HRNet)</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.5.19.14.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.19.14.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.5.19.14.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.19.14.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.5.19.14.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.19.14.4.1" style="font-size:70%;">54.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.5.19.14.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.19.14.5.1" style="font-size:70%;">97.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.5.19.14.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.19.14.6.1" style="font-size:70%;">67.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.5.19.14.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.19.14.7.1" style="font-size:70%;">‚úì</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Results</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS1.5.1.1">IV-D</span>1 </span>Performance</h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.p1.1.1">InD Testing.</span>
To evaluate SkelFormer, we first compare its performance to prior works on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib5" title="">5</a>]</cite> dataset. In this experiment, most benchmarks pre-train their models on multiple datasets and fine-tune them on the Human3.6m training set. Accordingly, we report the performance of our model using keypoint estimators, <em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS1.p1.1.2">i.e</em>.<span class="ltx_text" id="S4.SS4.SSS1.p1.1.3"></span>, CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib57" title="">57</a>]</cite> and LT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib17" title="">17</a>]</cite>, trained on Human3.6m dataset (InD). To this end, we use 3D predictions from CPN (followed by DLT triangulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib6" title="">6</a>]</cite>) and LT, both of which are trained following the evaluation protocol described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib5" title="">5</a>]</cite>. Moreover, we train our skeletal transformer on the AMASS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib46" title="">46</a>]</cite> using a 17-joint configuration as per Human3.6M. Delving deep into the results in <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.T1" title="In IV-C Implementation Details ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">I</span></a>, we observe that our method outperforms the best regression model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib4" title="">4</a>]</cite>. Additionally, SkelFormer significantly outperforms optimization solutions from 2D multi-view keypoints and from 3D keypoint reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib58" title="">58</a>]</cite> and SMPLify-X (LT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib27" title="">27</a>]</cite>. A notable solution is Pose2Mesh <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib21" title="">21</a>]</cite>, another regression-based approach that exploits the 3D ground-truth joints as input. However, our method can still outperform Pose2Mesh despite not using the ground-truth information. Finally, although not directly comparable, our method can achieve the closest performance to solutions that only predict joint positions, namely, LT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib17" title="">17</a>]</cite> and CPN<span class="ltx_text ltx_font_typewriter" id="S4.SS4.SSS1.p1.1.4">+</span>DLT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib57" title="">57</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.p2.1.1">OoD Testing.</span>
Next, to test our method in OoD settings, we use HRNet-W48<span class="ltx_text ltx_font_typewriter" id="S4.SS4.SSS1.p2.1.2">+</span>Dark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib9" title="">9</a>]</cite> as the keypoint extractor, which has not been trained on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib53" title="">53</a>]</cite> or the MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib54" title="">54</a>]</cite> datasets. Moreover, we keep our entire pipeline frozen and do not fine-tune any of its components on any portion of these datasets. The results are presented in <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.T2" title="In IV-C Implementation Details ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">II</span></a>. It should be noted that while prior works on the RICH dataset are monocular pose estimation approaches, all prior works on MPI-INF-3DHP are multi-view solutions and use all available views. On the RICH dataset, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.T2" title="In IV-C Implementation Details ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">II</span></a> shows that other solutions, such as SPIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib43" title="">43</a>]</cite> and CLIFF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib65" title="">65</a>]</cite>, suffer greatly in OoD setups (obtaining 172% and 127% additional error w.r.t. their InD performance on Human3.6m dataset). In contrast, our method shows overall competitive results compared to InD solutions by outperforming prior works on RICH while showing relatively minor degradation on the MPI-INF-3DHP dataset. Finally, we demonstrate the fitting quality of our model compared to the pseudo-ground-truth provided in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib52" title="">52</a>]</cite> on the Human3.6m and the MPI-INF-3DHP datasets in <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.F3" title="In IV-D1 Performance ‚Ä£ IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>, showing improvements in the feet and hands regions overall.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F3.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A visual comparison with the pseudo-ground-truth from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib52" title="">52</a>]</cite> is provided, presenting the realism and accuracy of our SkelFormer.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Ablation Study results in the presence of 20% occlusion and a Gaussian noise of <math alttext="\sigma=20" class="ltx_Math" display="inline" id="S4.T3.2.m1.1"><semantics id="S4.T3.2.m1.1b"><mrow id="S4.T3.2.m1.1.1" xref="S4.T3.2.m1.1.1.cmml"><mi id="S4.T3.2.m1.1.1.2" xref="S4.T3.2.m1.1.1.2.cmml">œÉ</mi><mo id="S4.T3.2.m1.1.1.1" xref="S4.T3.2.m1.1.1.1.cmml">=</mo><mn id="S4.T3.2.m1.1.1.3" xref="S4.T3.2.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><apply id="S4.T3.2.m1.1.1.cmml" xref="S4.T3.2.m1.1.1"><eq id="S4.T3.2.m1.1.1.1.cmml" xref="S4.T3.2.m1.1.1.1"></eq><ci id="S4.T3.2.m1.1.1.2.cmml" xref="S4.T3.2.m1.1.1.2">ùúé</ci><cn id="S4.T3.2.m1.1.1.3.cmml" type="integer" xref="S4.T3.2.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">\sigma=20</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.m1.1e">italic_œÉ = 20</annotation></semantics></math> mm.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.5.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.4.1" style="font-size:70%;">Experiments</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.1.1" style="font-size:70%;">MPVPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.3.1.1.1.m1.1"><semantics id="S4.T3.3.1.1.1.m1.1a"><mo id="S4.T3.3.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.3.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.1.1.1.m1.1b"><ci id="S4.T3.3.1.1.1.m1.1.1.cmml" xref="S4.T3.3.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.1.1.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.4.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.2.2.1" style="font-size:70%;">PA-MPVPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.4.2.2.1.m1.1"><semantics id="S4.T3.4.2.2.1.m1.1a"><mo id="S4.T3.4.2.2.1.m1.1.1" stretchy="false" xref="S4.T3.4.2.2.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.2.2.1.m1.1b"><ci id="S4.T3.4.2.2.1.m1.1.1.cmml" xref="S4.T3.4.2.2.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.2.2.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.5.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.3.1" style="font-size:70%;">Rot. Error<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.5.3.3.1.m1.1"><semantics id="S4.T3.5.3.3.1.m1.1a"><mo id="S4.T3.5.3.3.1.m1.1.1" stretchy="false" xref="S4.T3.5.3.3.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.3.3.1.m1.1b"><ci id="S4.T3.5.3.3.1.m1.1.1.cmml" xref="S4.T3.5.3.3.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.3.3.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.10.9.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.10.9.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.10.9.1.1.1" style="font-size:70%;">SVD Symmetric Orthogonalization</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.10.9.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.10.9.1.2.1" style="font-size:70%;">18.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.10.9.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.10.9.1.3.1" style="font-size:70%;">12.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.10.9.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.10.9.1.4.1" style="font-size:70%;">2.94</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.10.2">
<td class="ltx_td ltx_align_left" id="S4.T3.10.10.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.10.2.1.1" style="font-size:70%;">6-DoF Symmetric Orthogonalization</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.10.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.10.2.2.1" style="font-size:70%;">19.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.10.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.10.2.3.1" style="font-size:70%;">12.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.10.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.10.2.4.1" style="font-size:70%;">3.13</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.11.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.10.11.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.11.3.1.1" style="font-size:70%;">Local Rot. Loss</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.11.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.11.3.2.1" style="font-size:70%;">19.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.11.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.11.3.3.1" style="font-size:70%;">13.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.11.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.11.3.4.1" style="font-size:70%;">3.01</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.12.4">
<td class="ltx_td ltx_align_left" id="S4.T3.10.12.4.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.12.4.1.1" style="font-size:70%;">Global Rot. Loss</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.12.4.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.12.4.2.1" style="font-size:70%;">19.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.12.4.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.12.4.3.1" style="font-size:70%;">13.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.12.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.12.4.4.1" style="font-size:70%;">3.63</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.13.5">
<td class="ltx_td ltx_align_left" id="S4.T3.10.13.5.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.10.13.5.1.1" style="font-size:70%;">Global<span class="ltx_text ltx_font_typewriter" id="S4.T3.10.13.5.1.1.1">+</span>Local Rot. Loss</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.13.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.10.13.5.2.1" style="font-size:70%;">18.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.13.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.10.13.5.3.1" style="font-size:70%;">12.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.13.5.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.10.13.5.4.1" style="font-size:70%;">2.94</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.14.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.10.14.6.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.14.6.1.1" style="font-size:70%;">Part-based Decoder Att. Weights</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.14.6.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.14.6.2.1" style="font-size:70%;">18.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.14.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.14.6.3.1" style="font-size:70%;">12.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.14.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.14.6.4.1" style="font-size:70%;">2.96</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.4">
<td class="ltx_td ltx_align_left" id="S4.T3.6.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<math alttext="d=1" class="ltx_Math" display="inline" id="S4.T3.6.4.1.m1.1"><semantics id="S4.T3.6.4.1.m1.1a"><mrow id="S4.T3.6.4.1.m1.1.1" xref="S4.T3.6.4.1.m1.1.1.cmml"><mi id="S4.T3.6.4.1.m1.1.1.2" mathsize="70%" xref="S4.T3.6.4.1.m1.1.1.2.cmml">d</mi><mo id="S4.T3.6.4.1.m1.1.1.1" mathsize="70%" xref="S4.T3.6.4.1.m1.1.1.1.cmml">=</mo><mn id="S4.T3.6.4.1.m1.1.1.3" mathsize="70%" xref="S4.T3.6.4.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.6.4.1.m1.1b"><apply id="S4.T3.6.4.1.m1.1.1.cmml" xref="S4.T3.6.4.1.m1.1.1"><eq id="S4.T3.6.4.1.m1.1.1.1.cmml" xref="S4.T3.6.4.1.m1.1.1.1"></eq><ci id="S4.T3.6.4.1.m1.1.1.2.cmml" xref="S4.T3.6.4.1.m1.1.1.2">ùëë</ci><cn id="S4.T3.6.4.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.6.4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.4.1.m1.1c">d=1</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.4.1.m1.1d">italic_d = 1</annotation></semantics></math><span class="ltx_text" id="S4.T3.6.4.1.1" style="font-size:70%;"> Decoder Att. Weights</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.4.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.6.4.2.1" style="font-size:70%;">19.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.4.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.6.4.3.1" style="font-size:70%;">12.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.6.4.4.1" style="font-size:70%;">2.97</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.5">
<td class="ltx_td ltx_align_left" id="S4.T3.7.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<math alttext="d=2" class="ltx_Math" display="inline" id="S4.T3.7.5.1.m1.1"><semantics id="S4.T3.7.5.1.m1.1a"><mrow id="S4.T3.7.5.1.m1.1.1" xref="S4.T3.7.5.1.m1.1.1.cmml"><mi id="S4.T3.7.5.1.m1.1.1.2" mathsize="70%" xref="S4.T3.7.5.1.m1.1.1.2.cmml">d</mi><mo id="S4.T3.7.5.1.m1.1.1.1" mathsize="70%" xref="S4.T3.7.5.1.m1.1.1.1.cmml">=</mo><mn id="S4.T3.7.5.1.m1.1.1.3" mathsize="70%" xref="S4.T3.7.5.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.7.5.1.m1.1b"><apply id="S4.T3.7.5.1.m1.1.1.cmml" xref="S4.T3.7.5.1.m1.1.1"><eq id="S4.T3.7.5.1.m1.1.1.1.cmml" xref="S4.T3.7.5.1.m1.1.1.1"></eq><ci id="S4.T3.7.5.1.m1.1.1.2.cmml" xref="S4.T3.7.5.1.m1.1.1.2">ùëë</ci><cn id="S4.T3.7.5.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.7.5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.5.1.m1.1c">d=2</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.5.1.m1.1d">italic_d = 2</annotation></semantics></math><span class="ltx_text" id="S4.T3.7.5.1.1" style="font-size:70%;"> Decoder Att. Weights</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.7.5.2.1" style="font-size:70%;">19.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.7.5.3.1" style="font-size:70%;">12.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.5.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.7.5.4.1" style="font-size:70%;">2.93</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.6">
<td class="ltx_td ltx_align_left" id="S4.T3.8.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<math alttext="d=3" class="ltx_Math" display="inline" id="S4.T3.8.6.1.m1.1"><semantics id="S4.T3.8.6.1.m1.1a"><mrow id="S4.T3.8.6.1.m1.1.1" xref="S4.T3.8.6.1.m1.1.1.cmml"><mi id="S4.T3.8.6.1.m1.1.1.2" mathsize="70%" xref="S4.T3.8.6.1.m1.1.1.2.cmml">d</mi><mo id="S4.T3.8.6.1.m1.1.1.1" mathsize="70%" xref="S4.T3.8.6.1.m1.1.1.1.cmml">=</mo><mn id="S4.T3.8.6.1.m1.1.1.3" mathsize="70%" xref="S4.T3.8.6.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.8.6.1.m1.1b"><apply id="S4.T3.8.6.1.m1.1.1.cmml" xref="S4.T3.8.6.1.m1.1.1"><eq id="S4.T3.8.6.1.m1.1.1.1.cmml" xref="S4.T3.8.6.1.m1.1.1.1"></eq><ci id="S4.T3.8.6.1.m1.1.1.2.cmml" xref="S4.T3.8.6.1.m1.1.1.2">ùëë</ci><cn id="S4.T3.8.6.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.8.6.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.6.1.m1.1c">d=3</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.6.1.m1.1d">italic_d = 3</annotation></semantics></math><span class="ltx_text" id="S4.T3.8.6.1.1" style="font-size:70%;"> Decoder Att. Weights</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.6.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.8.6.2.1" style="font-size:70%;">19.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.8.6.3.1" style="font-size:70%;">12.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.8.6.4.1" style="font-size:70%;">2.96</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.9.7">
<td class="ltx_td ltx_align_left" id="S4.T3.9.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<math alttext="d=4" class="ltx_Math" display="inline" id="S4.T3.9.7.1.m1.1"><semantics id="S4.T3.9.7.1.m1.1a"><mrow id="S4.T3.9.7.1.m1.1.1" xref="S4.T3.9.7.1.m1.1.1.cmml"><mi id="S4.T3.9.7.1.m1.1.1.2" mathsize="70%" xref="S4.T3.9.7.1.m1.1.1.2.cmml">d</mi><mo id="S4.T3.9.7.1.m1.1.1.1" mathsize="70%" xref="S4.T3.9.7.1.m1.1.1.1.cmml">=</mo><mn id="S4.T3.9.7.1.m1.1.1.3" mathsize="70%" xref="S4.T3.9.7.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.9.7.1.m1.1b"><apply id="S4.T3.9.7.1.m1.1.1.cmml" xref="S4.T3.9.7.1.m1.1.1"><eq id="S4.T3.9.7.1.m1.1.1.1.cmml" xref="S4.T3.9.7.1.m1.1.1.1"></eq><ci id="S4.T3.9.7.1.m1.1.1.2.cmml" xref="S4.T3.9.7.1.m1.1.1.2">ùëë</ci><cn id="S4.T3.9.7.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.9.7.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.7.1.m1.1c">d=4</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.7.1.m1.1d">italic_d = 4</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T3.9.7.1.1" style="font-size:70%;"> Decoder Att. Weights</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.9.7.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.9.7.2.1" style="font-size:70%;">18.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.9.7.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.9.7.3.1" style="font-size:70%;">12.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.9.7.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.9.7.4.1" style="font-size:70%;">2.94</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.8">
<td class="ltx_td ltx_align_left" id="S4.T3.10.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<math alttext="d=5" class="ltx_Math" display="inline" id="S4.T3.10.8.1.m1.1"><semantics id="S4.T3.10.8.1.m1.1a"><mrow id="S4.T3.10.8.1.m1.1.1" xref="S4.T3.10.8.1.m1.1.1.cmml"><mi id="S4.T3.10.8.1.m1.1.1.2" mathsize="70%" xref="S4.T3.10.8.1.m1.1.1.2.cmml">d</mi><mo id="S4.T3.10.8.1.m1.1.1.1" mathsize="70%" xref="S4.T3.10.8.1.m1.1.1.1.cmml">=</mo><mn id="S4.T3.10.8.1.m1.1.1.3" mathsize="70%" xref="S4.T3.10.8.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.10.8.1.m1.1b"><apply id="S4.T3.10.8.1.m1.1.1.cmml" xref="S4.T3.10.8.1.m1.1.1"><eq id="S4.T3.10.8.1.m1.1.1.1.cmml" xref="S4.T3.10.8.1.m1.1.1.1"></eq><ci id="S4.T3.10.8.1.m1.1.1.2.cmml" xref="S4.T3.10.8.1.m1.1.1.2">ùëë</ci><cn id="S4.T3.10.8.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.10.8.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.8.1.m1.1c">d=5</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.8.1.m1.1d">italic_d = 5</annotation></semantics></math><span class="ltx_text" id="S4.T3.10.8.1.1" style="font-size:70%;"> Decoder Att. Weights</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.8.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.8.2.1" style="font-size:70%;">18.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.8.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.8.3.1" style="font-size:70%;">12.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.8.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.8.4.1" style="font-size:70%;">2.97</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.15.7">
<td class="ltx_td ltx_align_left" id="S4.T3.10.15.7.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.15.7.1.1" style="font-size:70%;">w/o Decoder Att. Weights</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.15.7.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.15.7.2.1" style="font-size:70%;">19.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.15.7.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.15.7.3.1" style="font-size:70%;">12.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.15.7.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.15.7.4.1" style="font-size:70%;">3.04</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.16.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.10.16.8.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.16.8.1.1" style="font-size:70%;">w/o Rotation Normalization</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.16.8.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.16.8.2.1" style="font-size:70%;">19.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.16.8.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.16.8.3.1" style="font-size:70%;">13.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.16.8.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.16.8.4.1" style="font-size:70%;">3.01</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.17.9">
<td class="ltx_td ltx_align_left" id="S4.T3.10.17.9.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.17.9.1.1" style="font-size:70%;">w/o Mirror Test</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.17.9.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.17.9.2.1" style="font-size:70%;">27.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.17.9.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.17.9.3.1" style="font-size:70%;">19.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.17.9.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.17.9.4.1" style="font-size:70%;">3.56</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.18.10">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.10.18.10.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.18.10.1.1" style="font-size:70%;">w/o Shape Aug.</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.10.18.10.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.18.10.2.1" style="font-size:70%;">19.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.10.18.10.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.18.10.3.1" style="font-size:70%;">12.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.10.18.10.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.10.18.10.4.1" style="font-size:70%;">2.92</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS2.5.1.1">IV-D</span>2 </span>Ablation Study</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.5">We test the importance of different network components and report the results in <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.T3" title="In IV-D1 Performance ‚Ä£ IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">III</span></a>. Given our goal of increasing generalizability in the presence of noise and occlusions, we conduct experiments on motion capture data from the AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib46" title="">46</a>]</cite> testing set at the presence of 20% occlusion and additive Gaussian noise with <math alttext="\sigma=20" class="ltx_Math" display="inline" id="S4.SS4.SSS2.p1.1.m1.1"><semantics id="S4.SS4.SSS2.p1.1.m1.1a"><mrow id="S4.SS4.SSS2.p1.1.m1.1.1" xref="S4.SS4.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS4.SSS2.p1.1.m1.1.1.2" xref="S4.SS4.SSS2.p1.1.m1.1.1.2.cmml">œÉ</mi><mo id="S4.SS4.SSS2.p1.1.m1.1.1.1" xref="S4.SS4.SSS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS4.SSS2.p1.1.m1.1.1.3" xref="S4.SS4.SSS2.p1.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.1.m1.1b"><apply id="S4.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1"><eq id="S4.SS4.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1.1"></eq><ci id="S4.SS4.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1.2">ùúé</ci><cn id="S4.SS4.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS4.SSS2.p1.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.1.m1.1c">\sigma=20</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS2.p1.1.m1.1d">italic_œÉ = 20</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.5.1">mm</span>. First, we demonstrate the effectiveness of symmetric orthogonalization by replacing our SVD operation with the commonly used 6-DoF representation, showing a drop of 0.8 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.5.2">mm</span> and 0.19¬∞¬†of MPVPE and rotational error when SVD is removed. Next, we experiment with different combinations of local and global (after FK) rotational loss functions to train our model. We observe that combining global and local rotation losses results in a better performance. Next, we showcase the effectiveness of our decoder masking strategy using the attention weights in three experiments: <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.5.3">i</span>) part-based experiment, where we restrict the attention within upper right, upper left, lower right, lower left, and center regions of the body; <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.5.4">ii</span>) node distance experiments, where we restrict the attention based on the node distance <math alttext="d" class="ltx_Math" display="inline" id="S4.SS4.SSS2.p1.2.m2.1"><semantics id="S4.SS4.SSS2.p1.2.m2.1a"><mi id="S4.SS4.SSS2.p1.2.m2.1.1" xref="S4.SS4.SSS2.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.2.m2.1b"><ci id="S4.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS2.p1.2.m2.1.1">ùëë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS2.p1.2.m2.1d">italic_d</annotation></semantics></math> in the skeleton kinematic tree with values between <math alttext="1" class="ltx_Math" display="inline" id="S4.SS4.SSS2.p1.3.m3.1"><semantics id="S4.SS4.SSS2.p1.3.m3.1a"><mn id="S4.SS4.SSS2.p1.3.m3.1.1" xref="S4.SS4.SSS2.p1.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.3.m3.1b"><cn id="S4.SS4.SSS2.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS4.SSS2.p1.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.3.m3.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS2.p1.3.m3.1d">1</annotation></semantics></math> and <math alttext="5" class="ltx_Math" display="inline" id="S4.SS4.SSS2.p1.4.m4.1"><semantics id="S4.SS4.SSS2.p1.4.m4.1a"><mn id="S4.SS4.SSS2.p1.4.m4.1.1" xref="S4.SS4.SSS2.p1.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.4.m4.1b"><cn id="S4.SS4.SSS2.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS4.SSS2.p1.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.4.m4.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS2.p1.4.m4.1d">5</annotation></semantics></math>; and <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.5.5">iii</span>) without skeleton-aware attention masks, where we allow each joint to attend to any other joints in the decoder. We observe that the best results are obtained for <math alttext="d=4" class="ltx_Math" display="inline" id="S4.SS4.SSS2.p1.5.m5.1"><semantics id="S4.SS4.SSS2.p1.5.m5.1a"><mrow id="S4.SS4.SSS2.p1.5.m5.1.1" xref="S4.SS4.SSS2.p1.5.m5.1.1.cmml"><mi id="S4.SS4.SSS2.p1.5.m5.1.1.2" xref="S4.SS4.SSS2.p1.5.m5.1.1.2.cmml">d</mi><mo id="S4.SS4.SSS2.p1.5.m5.1.1.1" xref="S4.SS4.SSS2.p1.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS4.SSS2.p1.5.m5.1.1.3" xref="S4.SS4.SSS2.p1.5.m5.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.5.m5.1b"><apply id="S4.SS4.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS4.SSS2.p1.5.m5.1.1"><eq id="S4.SS4.SSS2.p1.5.m5.1.1.1.cmml" xref="S4.SS4.SSS2.p1.5.m5.1.1.1"></eq><ci id="S4.SS4.SSS2.p1.5.m5.1.1.2.cmml" xref="S4.SS4.SSS2.p1.5.m5.1.1.2">ùëë</ci><cn id="S4.SS4.SSS2.p1.5.m5.1.1.3.cmml" type="integer" xref="S4.SS4.SSS2.p1.5.m5.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.5.m5.1c">d=4</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS2.p1.5.m5.1d">italic_d = 4</annotation></semantics></math>, improving over the vanilla transformer decoder by 1.1 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.5.6">mm</span> MPVPE and 0.2¬∞¬†rotation error. Lastly, we show the impact of rotation normalization, mirror testing, and shape augmentation, where a significant drop in performance is seen in the individual absence of each of these components, highlighting their significance.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.1">Finally, we perform a qualitative experiment on our joint regressor. To this end, we use VPoser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib27" title="">27</a>]</cite> to fit onto 3D keypoints using our joint regressor and the one provided in prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib27" title="">27</a>]</cite> on Human3.6m dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib5" title="">5</a>]</cite>. In <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.F4" title="In IV-D2 Ablation Study ‚Ä£ IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>, we show the visual fidelity of the effect of our joint regressor in comparison to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib52" title="">52</a>]</cite>, specifically, in better fitting to the feet regions.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F4.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The fitting performance of VPoser is demonstrated while using (a) our proposed joint regressor; and (b) the joint regressor from Moon <em class="ltx_emph ltx_font_italic" id="S4.F4.3.1">et al</em>.<span class="ltx_text" id="S4.F4.4.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib52" title="">52</a>]</cite>.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F5.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Robustness of our skeletal transformer is highlighted in the presence of different levels of noise and occlusion by comparing it against VPoser and VPoser-t.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS3.5.1.1">IV-D</span>3 </span>Robustness to Noise and Occlusions</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.1">In order to evaluate the performance of our model on motion capture data, we perform experiments by simulating noise and occlusion. For these experiments, we compare our skeletal transformer with VPoser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib27" title="">27</a>]</cite> and its temporal version, called VPoser-t <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib3" title="">3</a>]</cite>, which tries to maximize temporal consistency during optimization. <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.F5" title="In IV-D2 Ablation Study ‚Ä£ IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a> demonstrates the performance of different models on the AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib46" title="">46</a>]</cite> testing set. In the first experiment, we introduce varying noise levels to the input data and evaluate the robustness of our method. More specifically, Gaussian noise with varying standard deviations up to 50 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS3.p1.1.1">mm</span> is added to the input, effectively increasing the MPJPE of ground truth up to 80 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS3.p1.1.2">mm</span> (referred to as GT noise). However, our skeletal transformer predicts body pose and shape parameters, which result in lower error after <math alttext="\sigma=15" class="ltx_Math" display="inline" id="S4.SS4.SSS3.p1.1.m1.1"><semantics id="S4.SS4.SSS3.p1.1.m1.1a"><mrow id="S4.SS4.SSS3.p1.1.m1.1.1" xref="S4.SS4.SSS3.p1.1.m1.1.1.cmml"><mi id="S4.SS4.SSS3.p1.1.m1.1.1.2" xref="S4.SS4.SSS3.p1.1.m1.1.1.2.cmml">œÉ</mi><mo id="S4.SS4.SSS3.p1.1.m1.1.1.1" xref="S4.SS4.SSS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS4.SSS3.p1.1.m1.1.1.3" xref="S4.SS4.SSS3.p1.1.m1.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p1.1.m1.1b"><apply id="S4.SS4.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS3.p1.1.m1.1.1"><eq id="S4.SS4.SSS3.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS3.p1.1.m1.1.1.1"></eq><ci id="S4.SS4.SSS3.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS3.p1.1.m1.1.1.2">ùúé</ci><cn id="S4.SS4.SSS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS4.SSS3.p1.1.m1.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p1.1.m1.1c">\sigma=15</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.p1.1.m1.1d">italic_œÉ = 15</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS3.p1.1.3">mm</span>. Additionally, its performance only degrades by 19.7 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS3.p1.1.4">mm</span> at maximum noise level, while VPoser fails to predict less noisy poses. Lastly, by comparing our method to a temporal model (VPoser-t), we demonstrate the model‚Äôs robustness to noisy scenarios.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F6.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Sample results on RICH dataset are presented.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F7.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Sample results on Human3.6m, MPI-INF-3DHP, and our collected videos are presented.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F8.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>We demonstrate the predictions of SkelFormer on its worst-performing sequences during the occlusion and noise experiments. The red points represent the model‚Äôs inputs.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.SSS3.p2">
<p class="ltx_p" id="S4.SS4.SSS3.p2.1">Next, we report the performance of our model in the presence of occlusions, where each joint is randomly masked with varying occlusion amounts of up to 50%. <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.F5" title="In IV-D2 Ablation Study ‚Ä£ IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a> shows that the performance of our skeletal transformer barely changes between 0 and 50% occlusion, thus showing the model‚Äôs capability to exploit local and global joint information. In contrast, VPoser‚Äôs performance drops by more than 20 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS3.p2.1.1">mm</span> in the presence of only 20% occlusion. We finally report the performance in an extreme occlusion scenario, often seen in IK applications, where only 7 end-point keypoints are provided. We observe that our model outperforms other solutions while maintaining a reasonable accuracy.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS4.5.1.1">IV-D</span>4 </span>Qualitative Assessment</h4>
<div class="ltx_para" id="S4.SS4.SSS4.p1">
<p class="ltx_p" id="S4.SS4.SSS4.p1.1">We present a qualitative assessment of our model to Out-of-Distribution (OoD) data to highlight our model‚Äôs robustness and generalization. <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.F6" title="In IV-D3 Robustness to Noise and Occlusions ‚Ä£ IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">6</span></a> demonstrates the visual quality of SkelFormer on RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib53" title="">53</a>]</cite> dataset in equally-sampled frames from the testing set. Our method yields the correct pose and shape with high overlaps with the subject. Some failure cases, in the first sequence when two of four cameras are occluded, are also demonstrated where the network has failed to predict the correct shape or pose.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS4.p2">
<p class="ltx_p" id="S4.SS4.SSS4.p2.1">In <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.F7" title="In IV-D3 Robustness to Noise and Occlusions ‚Ä£ IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">7</span></a>, we showcase the visual fidelity of SkelFormer by presenting more images on public datasets and in-house recording sessions using 6 GoPro cameras. Our collected videos include a running sequence in a large volume and a dying and sitting sequence in a small volume.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS4.p3">
<p class="ltx_p" id="S4.SS4.SSS4.p3.1"><a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#S4.F8" title="In IV-D3 Robustness to Noise and Occlusions ‚Ä£ IV-D Results ‚Ä£ IV Experiments ‚Ä£ SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">8</span></a> visualizes SkelFormer‚Äôs fitting capabilities in noisy and occlusion experiments on the sequences with its highest error from AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib46" title="">46</a>]</cite> testing set. Interestingly, in very noisy circumstances, the model tries to predict a plausible pose while adhering to the input as much as possible. It also generates viable and relaxed poses in the end-point experiments. Consistent with our results, we see almost no changes to the prediction in the presence of occlusions. Please refer to our video demo to see more animations containing noise and occlusion experiments and a comparison to VPoser-t, which is a temporal optimization method.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Limitations and Future Work</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The major limitation of this work is the accumulation of errors in our backbones. Although our network mitigates jitters and occlusions better than other approaches, it is still prone to artifacts caused by mediolateral mix-ups and incorrect tracking. It also produces jitters in the presence of sudden high-level occlusions. One solution to remedy such issues is to use more accurate human trackers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib40" title="">40</a>]</cite>, 2D keypoint estimators, and triangulation techniques. However, while this can be potentially effective in most cases, it does not address the main problem at a fundamental level. Another approach could be to use temporal information to understand anomalies and noisy keypoints via more sophisticated networks. To this end, SkelFormer can be used to initialize generative models such as DMMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib28" title="">28</a>]</cite> and HuMoR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.12625v1#bib.bib3" title="">3</a>]</cite> to refine the motions.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we presented SkelFormer, a novel multi-stage pipeline consisting of keypoint estimators and a skeletal transformer for markerless human motion capture. Our method leverages large amounts of motion capture data to address the poor generalization of multi-view human shape and pose estimation approaches while outperforming optimization approaches in accuracy. Through extensive experiments, we demonstrated the effectiveness of SkelFormer in several challenging conditions, including InD and OoD settings. Specifically, We achieve the best results in our InD experiments among prior multi-view pose and shape estimation approaches and show competitive performance in OoD settings, demonstrating the generalization of our pipeline. Furthermore, we show the effectiveness of the proposed elements in our skeletal transformer through our ablation study. Finally, we show that our single-frame skeletal transformer exhibits higher robustness to noise and occlusion compared to optimization approaches that rely on temporal data.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was partially funded by Mitacs through the Accelerate program.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J.¬†Li, C.¬†Xu, Z.¬†Chen, S.¬†Bian, L.¬†Yang, and C.¬†Lu, ‚ÄúHybrik: A hybrid
analytical-neural inverse kinematics solution for 3d human pose and shape
estimation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2021, pp. 3383‚Äì3393.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Z.¬†Yu, L.¬†Zhang, Y.¬†Xu, C.¬†Tang, L.¬†Tran, C.¬†Keskin, and H.¬†S. Park,
‚ÄúMultiview human body reconstruction from uncalibrated cameras,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D.¬†Rempe, T.¬†Birdal, A.¬†Hertzmann, J.¬†Yang, S.¬†Sridhar, and L.¬†J. Guibas,
‚ÄúHumor: 3d human motion model for robust pose estimation,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2021, pp.
11‚Äâ488‚Äì11‚Äâ499.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K.¬†Jia, H.¬†Zhang, L.¬†An, and Y.¬†Liu, ‚ÄúDelving deep into pixel alignment
feature for accurate multi-view human mesh recovery,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings
of the AAAI Conference on Artificial Intelligence</em>, vol.¬†37, no.¬†1, 2023, pp.
989‚Äì997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C.¬†Ionescu, D.¬†Papava, V.¬†Olaru, and C.¬†Sminchisescu, ‚ÄúHuman3.6m: Large scale
datasets and predictive methods for 3d human sensing in natural
environments,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, vol.¬†36, no.¬†7, pp. 1325‚Äì1339, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R.¬†Hartley and A.¬†Zisserman, <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Multiple view geometry in computer vision</em>,
2nd¬†ed.¬†¬†¬†Cambridge University Press,
2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K.¬†Sun, B.¬†Xiao, D.¬†Liu, and J.¬†Wang, ‚ÄúDeep high-resolution representation
learning for human pose estimation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, 2019, pp. 5693‚Äì5703.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Z.¬†Cao, T.¬†Simon, S.-E. Wei, and Y.¬†Sheikh, ‚ÄúRealtime multi-person 2d pose
estimation using part affinity fields,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, 2017, pp. 7291‚Äì7299.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
F.¬†Zhang, X.¬†Zhu, H.¬†Dai, M.¬†Ye, and C.¬†Zhu, ‚ÄúDistribution-aware coordinate
representation for human pose estimation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, 2020, pp. 7093‚Äì7102.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S.¬†Jin, L.¬†Xu, J.¬†Xu, C.¬†Wang, W.¬†Liu, C.¬†Qian, W.¬†Ouyang, and P.¬†Luo,
‚ÄúWhole-body human pose estimation in the wild,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">European
Conference on Computer Vision (ECCV)</em>.¬†¬†¬†Springer, 2020, pp. 196‚Äì214.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M.¬†Maire, S.¬†Belongie, J.¬†Hays, P.¬†Perona, D.¬†Ramanan,
P.¬†Doll√°r, and C.¬†L. Zitnick, ‚ÄúMicrosoft coco: Common objects in
context,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">European Conference on Computer Vision (ECCV)</em>.¬†¬†¬†Springer, 2014, pp. 740‚Äì755.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H.-S. Fang, J.¬†Li, H.¬†Tang, C.¬†Xu, H.¬†Zhu, Y.¬†Xiu, Y.-L. Li, and C.¬†Lu,
‚ÄúAlphapose: Whole-body regional multi-person pose estimation and tracking in
real-time,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R.¬†Mitra, N.¬†B. Gundavarapu, A.¬†Sharma, and A.¬†Jain, ‚ÄúMultiview-consistent
semi-supervised learning for 3d human pose estimation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020, pp.
6907‚Äì6916.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
V.¬†Davoodnia, S.¬†Ghorbani, and A.¬†Etemad, ‚ÄúEstimating pose from pressure data
for smart beds with deep image-based pose estimators,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Applied
Intelligence</em>, vol.¬†52, no.¬†2, pp. 2119‚Äì2133, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
W.¬†Zhu, X.¬†Ma, Z.¬†Liu, L.¬†Liu, W.¬†Wu, and Y.¬†Wang, ‚ÄúMotionbert: A unified
perspective on learning human motion representations,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">IEEE/CVF
International Conference on Computer Vision (ICCV)</em>.¬†¬†¬†IEEE, 2023, pp. 15‚Äâ085‚Äì15‚Äâ099.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
V.¬†Davoodnia and A.¬†Etemad, ‚ÄúHuman pose estimation from ambiguous pressure
recordings with spatio-temporal masked transformers,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em>, 2023, pp. 1‚Äì5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K.¬†Iskakov, E.¬†Burkov, V.¬†Lempitsky, and Y.¬†Malkov, ‚ÄúLearnable triangulation
of human pose,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">IEEE/CVF International Conference on Computer
Vision (ICCV)</em>, 2019, pp. 7718‚Äì7727.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Z.¬†Zhang, C.¬†Wang, W.¬†Qiu, W.¬†Qin, and W.¬†Zeng, ‚ÄúAdafuse: Adaptive multiview
fusion for accurate human pose estimation in the wild,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">International
Journal of Computer Vision</em>, vol. 129, pp. 703‚Äì718, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M.¬†Loper, N.¬†Mahmood, J.¬†Romero, G.¬†Pons-Moll, and M.¬†J. Black, ‚ÄúSmpl: A
skinned multi-person linear model,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ACM Transactions on Graphics</em>,
vol.¬†34, no.¬†6, pp. 1‚Äì16, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J.¬†Romero, D.¬†Tzionas, and M.¬†J. Black, ‚ÄúEmbodied hands: Modeling and
capturing hands and bodies together,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">ACM Transactions on Graphics</em>,
vol.¬†36, no.¬†6, pp. 1‚Äì17, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
H.¬†Choi, G.¬†Moon, and K.¬†M. Lee, ‚ÄúPose2mesh: Graph convolutional network for
3d human pose and mesh recovery from a 2d human pose,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">European
Conference on Computer Vision (ECCV)</em>.¬†¬†¬†Springer, 2020, pp. 769‚Äì787.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C.¬†Zheng, M.¬†Mendieta, P.¬†Wang, A.¬†Lu, and C.¬†Chen, ‚ÄúA lightweight graph
transformer network for human mesh reconstruction from 2d human pose,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">ACM International Conference on Multimedia (ACMMM)</em>, 2022, pp.
5496‚Äì5507.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H.¬†Zhang, Y.¬†Tian, X.¬†Zhou, W.¬†Ouyang, Y.¬†Liu, L.¬†Wang, and Z.¬†Sun, ‚ÄúPymaf: 3d
human pose and shape regression with pyramidal mesh alignment feedback
loop,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">IEEE/CVF International Conference on Computer Vision
(ICCV)</em>, 2021, pp. 11‚Äâ446‚Äì11‚Äâ456.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Z.¬†Li, M.¬†Oskarsson, and A.¬†Heyden, ‚Äú3d human pose and shape estimation
through collaborative learning and multi-view model-fitting,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>,
2021, pp. 1888‚Äì1897.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
S.¬†Shin and E.¬†Halilaj, ‚ÄúMulti-view human pose and shape estimation using
learnable volumetric aggregation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2011.13427</em>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J.¬†Sp√∂rri, ‚ÄúReasearch dedicated to sports injury prevention-the‚Äôsequence
of prevention‚Äôon the example of alpine ski racing,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Habilitation
with Venia Docendi in Biomechanics</em>, vol.¬†1, no.¬†2, p.¬†7, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
G.¬†Pavlakos, V.¬†Choutas, N.¬†Ghorbani, T.¬†Bolkart, A.¬†A. Osman, D.¬†Tzionas, and
M.¬†J. Black, ‚ÄúExpressive body capture: 3d hands, face, and body from a
single image,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2019, pp. 10‚Äâ975‚Äì10‚Äâ985.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
B.¬†Huang, Y.¬†Shu, T.¬†Zhang, and Y.¬†Wang, ‚ÄúDynamic multi-person mesh recovery
from uncalibrated multi-view cameras,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">IEEE International
Conference on 3D Vision (3DV)</em>, 2021, pp. 710‚Äì720.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
L.¬†Metz, C.¬†D. Freeman, S.¬†S. Schoenholz, and T.¬†Kachman, ‚ÄúGradients are not
all you need,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2111.05803</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
N.¬†Kolotouros, G.¬†Pavlakos, D.¬†Jayaraman, and K.¬†Daniilidis, ‚ÄúProbabilistic
modeling for human mesh recovery,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">IEEE/CVF International
Conference on Computer Vision (ICCV)</em>, 2021, pp. 11‚Äâ605‚Äì11‚Äâ614.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A.¬†Csiszar, J.¬†Eilers, and A.¬†Verl, ‚ÄúOn solving the inverse kinematics problem
using neural networks,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">IEEE International Conference on
Mechatronics and Machine Vision in Practice (M2VIP)</em>, 2017, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
R.¬†Villegas, J.¬†Yang, D.¬†Ceylan, and H.¬†Lee, ‚ÄúNeural kinematic networks for
unsupervised motion retargetting,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>, 2018, pp. 8639‚Äì8648.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S.¬†Raab, I.¬†Leibovitch, P.¬†Li, K.¬†Aberman, O.¬†Sorkine-Hornung, and D.¬†Cohen-Or,
‚ÄúModi: Unconditional motion synthesis from diverse data,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023, pp.
13‚Äâ873‚Äì13‚Äâ883.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
T.¬†Jiang, N.¬†C. Camgoz, and R.¬†Bowden, ‚ÄúSkeletor: Skeletal transformers for
robust body-pose estimation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>, 2021, pp. 3394‚Äì3402.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A.¬†Zeng, X.¬†Sun, L.¬†Yang, N.¬†Zhao, M.¬†Liu, and Q.¬†Xu, ‚ÄúLearning skeletal graph
neural networks for hard 3d pose estimation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">IEEE/CVF
International Conference on Computer Vision (ICCV)</em>, 2021, pp.
11‚Äâ436‚Äì11‚Äâ445.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
L.¬†Floridi and M.¬†Chiriatti, ‚ÄúGpt-3: Its nature, scope, limits, and
consequences,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Minds and Machines</em>, vol.¬†30, pp. 681‚Äì694, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y.¬†Xu, J.¬†Zhang, Q.¬†Zhang, and D.¬†Tao, ‚ÄúVitpose: Simple vision transformer
baselines for human pose estimation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Advances in Neural Information
Processing Systems (NeurIPS)</em>, vol.¬†35, pp. 38‚Äâ571‚Äì38‚Äâ584, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Y.¬†Duan, Y.¬†Lin, Z.¬†Zou, Y.¬†Yuan, Z.¬†Qian, and B.¬†Zhang, ‚ÄúA unified framework
for real time motion completion,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, vol.¬†36, 2022, pp. 4459‚Äì4467.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
J.¬†Qin, Y.¬†Zheng, and K.¬†Zhou, ‚ÄúMotion in-betweening via two-stage
transformers,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">ACM Transactions on Graphics</em>, vol.¬†41, no.¬†6, pp.
1‚Äì16, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
S.¬†Ren, K.¬†He, R.¬†Girshick, and J.¬†Sun, ‚ÄúFaster r-cnn: Towards real-time
object detection with region proposal networks,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Advances in Neural
Information Processing Systems (NeurIPS)</em>, vol.¬†28, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
L.¬†Bridgeman, M.¬†Volino, J.-Y. Guillemaut, and A.¬†Hilton, ‚ÄúMulti-person 3d
pose estimation and tracking in sports,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) workshops</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A.¬†Vaswani, N.¬†Shazeer, N.¬†Parmar, J.¬†Uszkoreit, L.¬†Jones, A.¬†N. Gomez,
≈Å.¬†Kaiser, and I.¬†Polosukhin, ‚ÄúAttention is all you need,‚Äù
<em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Advances in Neural Information Processing Systems (NeurIPS)</em>, vol.¬†30,
2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
N.¬†Kolotouros, G.¬†Pavlakos, M.¬†J. Black, and K.¬†Daniilidis, ‚ÄúLearning to
reconstruct 3d human pose and shape via model-fitting in the loop,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2019, pp.
2252‚Äì2261.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
J.¬†Levinson, C.¬†Esteves, K.¬†Chen, N.¬†Snavely, A.¬†Kanazawa, A.¬†Rostamizadeh, and
A.¬†Makadia, ‚ÄúAn analysis of svd for deep rotation estimation,‚Äù
<em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Advances in Neural Information Processing Systems (NeurIPS)</em>, vol.¬†33,
pp. 22‚Äâ554‚Äì22‚Äâ565, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
E.¬†Hedlin, H.¬†Rhodin, and K.¬†M. Yi, ‚ÄúA simple method to boost human pose
estimation accuracy by correcting the joint regressor for the human3. 6m
dataset,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">IEEE Conference on Robots and Vision (CRV)</em>, 2022, pp.
1‚Äì7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
N.¬†Mahmood, N.¬†Ghorbani, N.¬†F. Troje, G.¬†Pons-Moll, and M.¬†J. Black, ‚ÄúAmass:
Archive of motion capture as surface shapes,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">IEEE/CVF
International Conference on Computer Vision (ICCV)</em>, 2019, pp. 5442‚Äì5451.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
D.¬†C. Liu and J.¬†Nocedal, ‚ÄúOn the limited memory bfgs method for large scale
optimization,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Mathematical Programming</em>, vol.¬†45, no. 1-3, pp.
503‚Äì528, 1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
A.¬†Kanazawa, M.¬†J. Black, D.¬†W. Jacobs, and J.¬†Malik, ‚ÄúEnd-to-end recovery of
human shape and pose,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, 2018, pp. 7122‚Äì7131.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
F.¬†L. Markley, Y.¬†Cheng, J.¬†L. Crassidis, and Y.¬†Oshman, ‚ÄúAveraging
quaternions,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Journal of Guidance, Control, and Dynamics</em>, vol.¬†30,
no.¬†4, pp. 1193‚Äì1197, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
A.¬†Sengupta, I.¬†Budvytis, and R.¬†Cipolla, ‚ÄúSynthetic training for accurate 3d
human pose and shape estimation in the wild,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint
arXiv:2009.10013</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
R.¬†Girshick, ‚ÄúFast r-cnn,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">IEEE/CVF International Conference on
Computer Vision (ICCV)</em>, 2015, pp. 1440‚Äì1448.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
G.¬†Moon, H.¬†Choi, and K.¬†M. Lee, ‚ÄúNeuralannot: Neural annotator for 3d human
mesh training sets,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, 2022, pp. 2299‚Äì2307.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
C.-H.¬†P. Huang, H.¬†Yi, M.¬†H√∂schle, M.¬†Safroshkin, T.¬†Alexiadis,
S.¬†Polikovsky, D.¬†Scharstein, and M.¬†J. Black, ‚ÄúCapturing and inferring
dense full-body human-scene contact,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, 2022, pp. 13‚Äâ274‚Äì13‚Äâ285.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
D.¬†Mehta, H.¬†Rhodin, D.¬†Casas, P.¬†Fua, O.¬†Sotnychenko, W.¬†Xu, and C.¬†Theobalt,
‚ÄúMonocular 3d human pose estimation in the wild using improved cnn
supervision,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">IEEE International Conference on 3D Vision (3DV)</em>,
2017, pp. 506‚Äì516.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
J.¬†Liang and M.¬†C. Lin, ‚ÄúShape-aware human pose and shape reconstruction using
multi-view images,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">IEEE/CVF International Conference on Computer
Vision (ICCV)</em>, 2019, pp. 4352‚Äì4362.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
I.¬†Loshchilov and F.¬†Hutter, ‚ÄúDecoupled weight decay regularization,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">International Conference on Learning Representations (ICLR)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Y.¬†Chen, Z.¬†Wang, Y.¬†Peng, Z.¬†Zhang, G.¬†Yu, and J.¬†Sun, ‚ÄúCascaded pyramid
network for multi-person pose estimation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, 2018, pp. 7103‚Äì7112.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Y.¬†Huang, F.¬†Bogo, C.¬†Lassner, A.¬†Kanazawa, P.¬†V. Gehler, J.¬†Romero, I.¬†Akhter,
and M.¬†J. Black, ‚ÄúTowards accurate marker-less human shape and pose
estimation over time,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">IEEE International Conference on 3D Vision
(3DV)</em>, 2017, pp. 421‚Äì430.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
X.¬†Gong, L.¬†Song, M.¬†Zheng, B.¬†Planche, T.¬†Chen, J.¬†Yuan, D.¬†Doermann, and
Z.¬†Wu, ‚ÄúProgressive multi-view human mesh recovery with self-supervision,‚Äù
in <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>,
vol.¬†37, no.¬†1, 2023, pp. 676‚Äì684.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
X.¬†Jiang, X.¬†Nie, Z.¬†Wang, L.¬†Liu, and S.¬†Liu, ‚ÄúMulti-view human body mesh
translator,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2210.01886</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
K.¬†Lin, L.¬†Wang, and Z.¬†Liu, ‚ÄúEnd-to-end human pose and mesh reconstruction
with transformers,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, 2021, pp. 1954‚Äì1963.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Z.¬†Shen, Z.¬†Cen, S.¬†Peng, Q.¬†Shuai, H.¬†Bao, and X.¬†Zhou, ‚ÄúLearning human mesh
recovery in 3d scenes,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, 2023, pp. 17‚Äâ038‚Äì17‚Äâ047.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
S.¬†Tripathi, L.¬†M√ºller, C.-H.¬†P. Huang, O.¬†Taheri, M.¬†J. Black, and
D.¬†Tzionas, ‚Äú3d human pose estimation via intuitive physics,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>,
2023, pp. 4713‚Äì4725.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
M.¬†Kocabas, C.-H.¬†P. Huang, O.¬†Hilliges, and M.¬†J. Black, ‚ÄúPare: Part
attention regressor for 3d human body estimation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">IEEE/CVF
International Conference on Computer Vision (ICCV)</em>, 2021, pp.
11‚Äâ127‚Äì11‚Äâ137.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Z.¬†Li, J.¬†Liu, Z.¬†Zhang, S.¬†Xu, and Y.¬†Yan, ‚ÄúCliff: Carrying location
information in full frames into human pose and shape estimation,‚Äù in
<em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">European Conference on Computer Vision (ECCV)</em>.¬†¬†¬†Springer, 2022, pp. 590‚Äì606.

</span>
</li>
</ul>
</section><div about="" class="ltx_rdf" content="Vandad Davoodnia" property="dcterms:creator"></div>
<div about="" class="ltx_rdf" content="SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers" property="dcterms:title"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed May  1 13:54:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
