<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views</title>
<!--Generated on Thu Aug 15 08:15:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Sparse-View 3D Reconstruction Pose Estimation Open-World Generation" lang="en" name="keywords"/>
<base href="/html/2408.10195v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S1" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S2" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S2.SS1" title="In 2 Related Work ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Sparse-View 3D Reconstruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S2.SS2" title="In 2 Related Work ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Pose-Free Reconstruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S2.SS3" title="In 2 Related Work ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Open-World 3D Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS1" title="In 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Tiling Sparse View Images as Input Condition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS2" title="In 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Image-to-NOCS Diffusion as a Pose Estimator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS3" title="In 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Multi-View Prediction for 3D Reconstruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS4" title="In 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Pose Refinement with Reconstructed 3D Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4.SS1" title="In 4 Experiments ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4.SS2" title="In 4 Experiments ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experiment Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4.SS3" title="In 4 Experiments ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S5" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A1" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A </span>Additional Real-World Examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A2" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.B </span>Ablation Studies on Number of Experts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A3" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.C </span>Robustness to Varying Camera Intrinsics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A4" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.D </span>Sparse-View Reconstruction using the Estimated Poses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A5" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.E </span>Evaluation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A5.SS1" title="In Appendix 0.E Evaluation Details ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.E.1 </span>3D Reconstruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A5.SS2" title="In Appendix 0.E Evaluation Details ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.E.2 </span>Pose Estimation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A6" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.F </span>Details of Dataset Curation</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>UCLA </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Hillbot Inc. </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Stanford University 
<br class="ltx_break"/></span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Zhejiang University </span></span></span><span class="ltx_note ltx_role_institutetext" id="id5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>UC San Diego 
<br class="ltx_break"/></span></span></span>
<h1 class="ltx_title ltx_title_document">SpaRP: Fast 3D Object Reconstruction and 
<br class="ltx_break"/>Pose Estimation from Sparse Views </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chao Xu<sup class="ltx_sup" id="id3.1.id1">‚Ä†</sup><span class="ltx_ERROR undefined" id="id4.2.id2">\orcidlink</span>0009-0001-0574-5357
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ang Li
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Linghao Chen<sup class="ltx_sup" id="id5.1.id1">‚Ä†</sup>
</span><span class="ltx_author_notes">2244</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yulin Liu
</span><span class="ltx_author_notes">55</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/>Ruoxi Shi<sup class="ltx_sup" id="id6.1.id1">‚Ä†</sup>
</span><span class="ltx_author_notes">2255</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Su<sup class="ltx_sup" id="id1.1.1"><math alttext="\ddagger" class="ltx_Math" display="inline" id="id1.1.1.m1.1"><semantics id="id1.1.1.m1.1a"><mo id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml">‚Ä°</mo><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><ci id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">‚Ä°</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="id1.1.1.m1.1d">‚Ä°</annotation></semantics></math> </sup>
</span><span class="ltx_author_notes">2255</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minghua Liu<sup class="ltx_sup" id="id2.1.1">‚Ä†<math alttext="\ddagger" class="ltx_Math" display="inline" id="id2.1.1.m1.1"><semantics id="id2.1.1.m1.1a"><mo id="id2.1.1.m1.1.1" xref="id2.1.1.m1.1.1.cmml">‚Ä°</mo><annotation-xml encoding="MathML-Content" id="id2.1.1.m1.1b"><ci id="id2.1.1.m1.1.1.cmml" xref="id2.1.1.m1.1.1">‚Ä°</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.1.1.m1.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="id2.1.1.m1.1d">‚Ä°</annotation></semantics></math> </sup>
</span><span class="ltx_author_notes">2255</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Open-world 3D generation has recently attracted considerable attention. While many single-image-to-3D methods have yielded visually appealing outcomes, they often lack sufficient controllability and tend to produce hallucinated regions that may not align with users‚Äô expectations. In this paper, we explore an important scenario in which the input consists of one or a few unposed 2D images of a single object, with little or no overlap. We propose a novel method, SpaRP, to reconstruct a 3D textured mesh and estimate the relative camera poses for these sparse-view images. SpaRP distills knowledge from 2D diffusion models and finetunes them to implicitly deduce the 3D spatial relationships between the sparse views. The diffusion model is trained to jointly predict surrogate representations for camera poses and multi-view images of the object under known poses, integrating all information from the input sparse views. These predictions are then leveraged to accomplish 3D reconstruction and pose estimation, and the reconstructed 3D model can be used to further refine the camera poses of input views. Through extensive experiments on three datasets, we demonstrate that our method not only significantly outperforms baseline methods in terms of 3D reconstruction quality and pose prediction accuracy but also exhibits strong efficiency. It requires only about 20 seconds to produce a textured mesh and camera poses for the input views. Project page: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chaoxu.xyz/sparp" title="">https://chaoxu.xyz/sparp</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Sparse-View 3D Reconstruction Pose Estimation Open-World Generation
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">footnotetext: </span>This work is done while the author is an intern at Hillbot Inc. <sup class="ltx_sup" id="footnotex1.1"><math alttext="\ddagger" class="ltx_Math" display="inline" id="footnotex1.1.m1.1"><semantics id="footnotex1.1.m1.1b"><mo id="footnotex1.1.m1.1.1" xref="footnotex1.1.m1.1.1.cmml">‚Ä°</mo><annotation-xml encoding="MathML-Content" id="footnotex1.1.m1.1c"><ci id="footnotex1.1.m1.1.1.cmml" xref="footnotex1.1.m1.1.1">‚Ä°</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex1.1.m1.1d">\ddagger</annotation><annotation encoding="application/x-llamapun" id="footnotex1.1.m1.1e">‚Ä°</annotation></semantics></math></sup> Equal advisory.</span></span></span>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="508" id="S0.F1.g1" src="extracted/5793183/figures/sparp_teaser.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.3.2" style="font-size:90%;">SpaRP handles open-world 3D reconstruction and pose estimation from unposed sparse-view images, delivering results in approximately 20 seconds.</span></figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">3D object reconstruction is a long-standing problem with applications spanning 3D content creation, augmented reality, virtual reality, and robotics, among others. Although traditional photogrammetry¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib64" title="">64</a>]</cite> and recent neural field methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib77" title="">77</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib91" title="">91</a>]</cite> have made significant strides in reconstructing high-fidelity geometry and appearance, they typically require dense view inputs. However, in many practical scenarios, such as in e-commerce and consumer capture situations, acquiring a comprehensive set of high-resolution images along with precise camera data is not always feasible.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">On the other end of the spectrum, the tasks of converting a single image to 3D and text to 3D have recently seen substantial progress¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib80" title="">80</a>]</cite>, thanks to the rich priors embedded in 2D diffusion models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib53" title="">53</a>]</cite> and pre-training on extensive 3D datasets¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib9" title="">9</a>]</cite>. These methods may achieve high-quality geometry and texture that matches the input view, but they also introduce ambiguities in the regions not visible in the input image (such as the back view). Although these methods attempt to hallucinate reasonable interpretations of these invisible areas, the generated regions may not always align with users‚Äô expectations, and users often lack sufficient control over these ambiguous regions.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we explore a critical scenario where the input consists of one or a few unposed 2D images of a single object. The images are captured from arbitrarily distributed camera poses, often with little to no overlap. We tackle both the 3D reconstruction and pose estimation of input images under this sparse view setting. Note that, in dense view setting, traditional Structure-from-Motion (SfM) solvers (e.g., COLMAP¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib58" title="">58</a>]</cite>) are typically employed for pose estimation. However, with sparse view inputs, these solvers often become unreliable and tend to fail due to insufficient overlapping visual cues. This issue is the main reason why existing sparse view reconstruction methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib97" title="">97</a>]</cite> generally require known camera poses as input. While some recent methods have attempted pose-free reconstruction and pose estimation for sparse views¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib95" title="">95</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib18" title="">18</a>]</cite>, they are usually trained on a predefined small set of object categories and exhibit poor generalization to unseen object categories.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In response, we propose an innovative class-agnostic approach called SpaRP, capable of processing arbitrary object categories with unposed sparse views. Our inspiration comes from recent breakthroughs in open-domain single-image-to-3D methods. They leverage 2D diffusion models (e.g., Stable Diffusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib53" title="">53</a>]</cite>) to generate novel viewpoints of an object¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib35" title="">35</a>]</cite>, and even consistent multi-view images from a single input image¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib38" title="">38</a>]</cite>, by finetuning the diffusion models with corresponding multi-view image pairs. These discoveries imply that 2D diffusion models harbor rich priors concerning 3D objects. Instead of merely producing multi-view images, we contemplate leveraging 2D diffusion models to examine a set of unposed input images from sparse viewpoints, infer their spatial interrelationships, and recover relative camera poses and underlying 3D shapes.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Specifically, we finetune a 2D diffusion model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib53" title="">53</a>]</cite> to process sparse input views by compositing them into a single image for conditioning. The diffusion model is concurrently tuned to deduce the relative poses of the input images and the underlying 3D objects. For the relative pose estimation branch, instead of outputting camera poses as scalars, we task 2D diffusion models to produce a surrogate representation: the NOCS maps¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib74" title="">74</a>]</cite> that embed pixel-wise correspondences across different views and are more suitable for 2D diffusion models. From these maps, we extract the relative camera poses for the sparse views using the traditional PnP algorithm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib2" title="">2</a>]</cite>, assuming known camera intrinsics. For the reconstruction branch, the diffusion model is tasked to produce multi-view images of the object from fixed known camera poses, covering the entire 3D object. This task requires the models to incorporate all information from input sparse views and hallucinate invisible regions. We then feed the generated images with fixed known poses into a pre-trained 3D reconstruction module¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib32" title="">32</a>]</cite> to create a textured 3D mesh. We can further refine the estimated camera poses by aligning the input views with the generated mesh through differentiable rendering¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">We train SpaRP on the Objaverse¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib9" title="">9</a>]</cite> dataset with 1‚Äì6 unposed input views. Unlike some previous methods that rely on costly per-shape optimization¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib83" title="">83</a>]</cite>, our method delivers 3D textured meshes along with camera poses in a much more efficient manner, requiring only <math alttext="\sim" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><csymbol cd="latexml" id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">‚àº</annotation></semantics></math>16 seconds. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S0.F1" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, our approach can faithfully generate 3D assets that closely follow the reference unposed images, effectively overcoming the ambiguity issue of single-image-to-3D. Extensive evaluation on three datasets demonstrates the superior performance of our method over baselines in reconstructing 3D meshes with vivid appearance and high-fidelity geometry, alongside precise pose estimation of the input images.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Sparse-View 3D Reconstruction</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Reconstructing 3D objects from sparse-view images is challenging due to the lack of visual correspondence and clues. When a small baseline between images is assumed, several methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib93" title="">93</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib89" title="">89</a>]</cite> have pretrained generalizable models to infer surface positions by establishing pixel correspondences and learning generalizable priors across scenes. However, these methods often fail to produce satisfactory results when the sparse-view images have a large baseline. Some studies have attempted to alleviate the dependence on dense views by incorporating priors or adding regularization¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib45" title="">45</a>]</cite> into the NeRF optimization process. Others have employed 2D diffusion priors to generate novel-view images as additional input for the NeRF model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib21" title="">21</a>]</cite>. For example, ReconFusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib84" title="">84</a>]</cite> trains a NeRF from sparse-view images and uses a denoising UNet to infer some novel view images as support for the NeRF model. EscherNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib23" title="">23</a>]</cite> utilizes Stable Diffusion for novel view synthesis and designs a camera positional encoding module to yield more consistent images. Furthermore, some recent works¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib62" title="">62</a>]</cite> have integrated specialized loss functions and additional modalities as inputs into NeRF-based per-scene optimization.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In contrast to these methods, our approach does not require camera poses for the input sparse views. It is not limited to small baselines and is capable of generating 360-degree meshes. Furthermore, without the need for per-shape optimization, our method can quickly produce both textured meshes and camera poses in about 20 seconds.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pose-Free Reconstruction</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Unlike the methods mentioned above, which assume known camera poses, many studies have aimed to solve the pose-free reconstruction challenge. When provided with dense images, some approaches ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib86" title="">86</a>]</cite> jointly optimize the NeRF representation along with camera parameters. However, due to the highly non-convex nature of this optimization problem, such methods are susceptible to initial pose guesses and can become trapped in local minima. This issue worsens when input images are sparse, with increasing ambiguity and reduced constraint availability. In response, numerous proposals have attempted to enhance optimization robustness. For example, SpaRF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib71" title="">71</a>]</cite> uses dense image matches as explicit optimization constraints, while FvOR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib90" title="">90</a>]</cite> starts with coarse predictions of camera poses and alternated updates between shape and pose.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In contrast to the optimization-based methods, there is a body of research proposing generalizable solutions for this problem. VideoAE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib25" title="">25</a>]</cite> infers scene geometry from the first frame in a video series and estimates camera poses relative to that frame, which allows for warping scene geometry to decode new viewpoints. SparsePose¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib63" title="">63</a>]</cite> first regresses and then iteratively refines camera poses. FORGE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib17" title="">17</a>]</cite> designs neural networks to infer initial camera poses, fuse multi-view features, and decode spatial densities and colors. GRNN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib72" title="">72</a>]</cite> offers a GRU-based reconstruction method estimating the relative pose for each input view against a global feature volume. The RelPose series¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib95" title="">95</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib29" title="">29</a>]</cite> use probabilistic modeling for relative rotation estimation between images. Other works¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib18" title="">18</a>]</cite> eschew explicit camera pose estimations, instead employing transformers to encode input views into latent scene representations for novel view synthesis.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">More recently, leveraging large vision models and diffusion models, which have shown significant promise, new efforts have emerged for camera pose estimation. PoseDiffusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib75" title="">75</a>]</cite> implements a diffusion model guided by 2D keypoint matches to estimate poses. PF-LRM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib78" title="">78</a>]</cite> adapts the LRM model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib14" title="">14</a>]</cite> to predict a point cloud for each input image, then utilizes differentiable PnP for pose estimation. iFusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib83" title="">83</a>]</cite> employs an optimization pipeline to assess relative elevations and azimuths. It utilizes Zero123¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib35" title="">35</a>]</cite> predictions as a basis and optimizes the relative pose between two images by minimizing the reconstruction loss between the predicted and target images.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">In contrast to these existing approaches, our proposal capitalizes on the extensive priors inherent in pre-trained 2D diffusion models, thereby providing exceptional generalizability to handle a diverse range of open-world categories. Our method predicts camera poses and 3D mesh geometry in a single feedforward pass, negating the need for per-shape optimization.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Open-World 3D Generation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Open-world single-image-to-3D and text-to-3D tasks have recently undergone significant advancements. Recent 2D generative models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib53" title="">53</a>]</cite> and vision-language models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib48" title="">48</a>]</cite> have supplied valuable priors about the 3D world, sparking a surge in research on 3D generation. Notably, models such as DreamFusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib46" title="">46</a>]</cite>, Magic3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib30" title="">30</a>]</cite>, and ProlificDreamer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib80" title="">80</a>]</cite> have pioneered a line of approach to per-shape optimization¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib94" title="">94</a>]</cite>. These models optimize a 3D representation (e.g., NeRF) for each unique text or image input, utilizing the 2D prior models for gradient guidance. Although they produce impressive results, these methods are hampered by prolonged optimization times, often extending to several hours, and ‚Äúmulti-face issue‚Äù problems.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Moreover, beyond optimization-based methods, exemplified by Zero123¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib35" title="">35</a>]</cite>, numerous recent studies have investigated the employment of pre-trained 2D diffusion models for synthesizing novel views from single images or text¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib92" title="">92</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib60" title="">60</a>]</cite>. They have introduced varied strategies to foster 3D-consistent multi-view generation. The resulting multi-view images can then serve for 3D reconstruction, utilizing either optimization-based methods ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib38" title="">38</a>]</cite> or feedforward models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib28" title="">28</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">While most existing works focus on single-image-to-3D or text-to-3D, they often hallucinate regions that are invisible in the input image, which provides users with limited control over those areas. In this paper, we seek to broaden the input to encompass unposed sparse views and address both the 3D reconstruction and pose estimation challenges in a time-efficient way‚Äîwithin tens of seconds.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="160" id="S3.F2.g1" src="extracted/5793183/figures/pipeline-sparp.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.4.2" style="font-size:90%;">Pipeline Overview of SpaRP.<span class="ltx_text ltx_font_medium" id="S3.F2.4.2.1"> We begin by taking a sparse set of unposed images as input, which we tile into a single composite image. This composite image is subsequently provided to the Stable Diffusion UNet to serve as the conditioning input. The 2D diffusion model is simultaneously finetuned to predict NOCS maps for the input sparse views and multi-view images under known camera poses. From the NOCS maps, we extract the camera poses corresponding to the input views. The multi-view images are then processed by a reconstruction module to generate textured 3D meshes. Optionally, the camera poses can be further refined using the generated mesh for improved accuracy.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.4">Given <math alttext="n" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_n</annotation></semantics></math> unposed input images <math alttext="\{\mathbf{I}_{i}\mid i=1,\dots,n;\ 1\leq n\leq 6\}" class="ltx_Math" display="inline" id="S3.p1.2.m2.5"><semantics id="S3.p1.2.m2.5a"><mrow id="S3.p1.2.m2.5.5.2" xref="S3.p1.2.m2.5.5.3.cmml"><mo id="S3.p1.2.m2.5.5.2.3" stretchy="false" xref="S3.p1.2.m2.5.5.3.1.cmml">{</mo><msub id="S3.p1.2.m2.4.4.1.1" xref="S3.p1.2.m2.4.4.1.1.cmml"><mi id="S3.p1.2.m2.4.4.1.1.2" xref="S3.p1.2.m2.4.4.1.1.2.cmml">ùêà</mi><mi id="S3.p1.2.m2.4.4.1.1.3" xref="S3.p1.2.m2.4.4.1.1.3.cmml">i</mi></msub><mo fence="true" id="S3.p1.2.m2.5.5.2.4" lspace="0em" rspace="0em" xref="S3.p1.2.m2.5.5.3.1.cmml">‚à£</mo><mrow id="S3.p1.2.m2.5.5.2.2.2" xref="S3.p1.2.m2.5.5.2.2.3.cmml"><mrow id="S3.p1.2.m2.5.5.2.2.1.1" xref="S3.p1.2.m2.5.5.2.2.1.1.cmml"><mi id="S3.p1.2.m2.5.5.2.2.1.1.2" xref="S3.p1.2.m2.5.5.2.2.1.1.2.cmml">i</mi><mo id="S3.p1.2.m2.5.5.2.2.1.1.1" xref="S3.p1.2.m2.5.5.2.2.1.1.1.cmml">=</mo><mrow id="S3.p1.2.m2.5.5.2.2.1.1.3.2" xref="S3.p1.2.m2.5.5.2.2.1.1.3.1.cmml"><mn id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">1</mn><mo id="S3.p1.2.m2.5.5.2.2.1.1.3.2.1" xref="S3.p1.2.m2.5.5.2.2.1.1.3.1.cmml">,</mo><mi id="S3.p1.2.m2.2.2" mathvariant="normal" xref="S3.p1.2.m2.2.2.cmml">‚Ä¶</mi><mo id="S3.p1.2.m2.5.5.2.2.1.1.3.2.2" xref="S3.p1.2.m2.5.5.2.2.1.1.3.1.cmml">,</mo><mi id="S3.p1.2.m2.3.3" xref="S3.p1.2.m2.3.3.cmml">n</mi></mrow></mrow><mo id="S3.p1.2.m2.5.5.2.2.2.3" xref="S3.p1.2.m2.5.5.2.2.3a.cmml">;</mo><mrow id="S3.p1.2.m2.5.5.2.2.2.2" xref="S3.p1.2.m2.5.5.2.2.2.2.cmml"><mn id="S3.p1.2.m2.5.5.2.2.2.2.2" xref="S3.p1.2.m2.5.5.2.2.2.2.2.cmml"> 1</mn><mo id="S3.p1.2.m2.5.5.2.2.2.2.3" xref="S3.p1.2.m2.5.5.2.2.2.2.3.cmml">‚â§</mo><mi id="S3.p1.2.m2.5.5.2.2.2.2.4" xref="S3.p1.2.m2.5.5.2.2.2.2.4.cmml">n</mi><mo id="S3.p1.2.m2.5.5.2.2.2.2.5" xref="S3.p1.2.m2.5.5.2.2.2.2.5.cmml">‚â§</mo><mn id="S3.p1.2.m2.5.5.2.2.2.2.6" xref="S3.p1.2.m2.5.5.2.2.2.2.6.cmml">6</mn></mrow></mrow><mo id="S3.p1.2.m2.5.5.2.5" stretchy="false" xref="S3.p1.2.m2.5.5.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.5b"><apply id="S3.p1.2.m2.5.5.3.cmml" xref="S3.p1.2.m2.5.5.2"><csymbol cd="latexml" id="S3.p1.2.m2.5.5.3.1.cmml" xref="S3.p1.2.m2.5.5.2.3">conditional-set</csymbol><apply id="S3.p1.2.m2.4.4.1.1.cmml" xref="S3.p1.2.m2.4.4.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.4.4.1.1.1.cmml" xref="S3.p1.2.m2.4.4.1.1">subscript</csymbol><ci id="S3.p1.2.m2.4.4.1.1.2.cmml" xref="S3.p1.2.m2.4.4.1.1.2">ùêà</ci><ci id="S3.p1.2.m2.4.4.1.1.3.cmml" xref="S3.p1.2.m2.4.4.1.1.3">ùëñ</ci></apply><apply id="S3.p1.2.m2.5.5.2.2.3.cmml" xref="S3.p1.2.m2.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.p1.2.m2.5.5.2.2.3a.cmml" xref="S3.p1.2.m2.5.5.2.2.2.3">formulae-sequence</csymbol><apply id="S3.p1.2.m2.5.5.2.2.1.1.cmml" xref="S3.p1.2.m2.5.5.2.2.1.1"><eq id="S3.p1.2.m2.5.5.2.2.1.1.1.cmml" xref="S3.p1.2.m2.5.5.2.2.1.1.1"></eq><ci id="S3.p1.2.m2.5.5.2.2.1.1.2.cmml" xref="S3.p1.2.m2.5.5.2.2.1.1.2">ùëñ</ci><list id="S3.p1.2.m2.5.5.2.2.1.1.3.1.cmml" xref="S3.p1.2.m2.5.5.2.2.1.1.3.2"><cn id="S3.p1.2.m2.1.1.cmml" type="integer" xref="S3.p1.2.m2.1.1">1</cn><ci id="S3.p1.2.m2.2.2.cmml" xref="S3.p1.2.m2.2.2">‚Ä¶</ci><ci id="S3.p1.2.m2.3.3.cmml" xref="S3.p1.2.m2.3.3">ùëõ</ci></list></apply><apply id="S3.p1.2.m2.5.5.2.2.2.2.cmml" xref="S3.p1.2.m2.5.5.2.2.2.2"><and id="S3.p1.2.m2.5.5.2.2.2.2a.cmml" xref="S3.p1.2.m2.5.5.2.2.2.2"></and><apply id="S3.p1.2.m2.5.5.2.2.2.2b.cmml" xref="S3.p1.2.m2.5.5.2.2.2.2"><leq id="S3.p1.2.m2.5.5.2.2.2.2.3.cmml" xref="S3.p1.2.m2.5.5.2.2.2.2.3"></leq><cn id="S3.p1.2.m2.5.5.2.2.2.2.2.cmml" type="integer" xref="S3.p1.2.m2.5.5.2.2.2.2.2">1</cn><ci id="S3.p1.2.m2.5.5.2.2.2.2.4.cmml" xref="S3.p1.2.m2.5.5.2.2.2.2.4">ùëõ</ci></apply><apply id="S3.p1.2.m2.5.5.2.2.2.2c.cmml" xref="S3.p1.2.m2.5.5.2.2.2.2"><leq id="S3.p1.2.m2.5.5.2.2.2.2.5.cmml" xref="S3.p1.2.m2.5.5.2.2.2.2.5"></leq><share href="https://arxiv.org/html/2408.10195v1#S3.p1.2.m2.5.5.2.2.2.2.4.cmml" id="S3.p1.2.m2.5.5.2.2.2.2d.cmml" xref="S3.p1.2.m2.5.5.2.2.2.2"></share><cn id="S3.p1.2.m2.5.5.2.2.2.2.6.cmml" type="integer" xref="S3.p1.2.m2.5.5.2.2.2.2.6">6</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.5c">\{\mathbf{I}_{i}\mid i=1,\dots,n;\ 1\leq n\leq 6\}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.5d">{ bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚à£ italic_i = 1 , ‚Ä¶ , italic_n ; 1 ‚â§ italic_n ‚â§ 6 }</annotation></semantics></math>, which illustrate a single object from arbitrary categories, we predict their relative camera poses <math alttext="{\boldsymbol{\xi}_{ij}}" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><msub id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">ùùÉ</mi><mrow id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml"><mi id="S3.p1.3.m3.1.1.3.2" xref="S3.p1.3.m3.1.1.3.2.cmml">i</mi><mo id="S3.p1.3.m3.1.1.3.1" xref="S3.p1.3.m3.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.p1.3.m3.1.1.3.3" xref="S3.p1.3.m3.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">ùùÉ</ci><apply id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3"><times id="S3.p1.3.m3.1.1.3.1.cmml" xref="S3.p1.3.m3.1.1.3.1"></times><ci id="S3.p1.3.m3.1.1.3.2.cmml" xref="S3.p1.3.m3.1.1.3.2">ùëñ</ci><ci id="S3.p1.3.m3.1.1.3.3.cmml" xref="S3.p1.3.m3.1.1.3.3">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">{\boldsymbol{\xi}_{ij}}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">bold_italic_Œæ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> and reconstruct the 3D model <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">‚Ñ≥</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">‚Ñ≥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">caligraphic_M</annotation></semantics></math> of the object. As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.F2" title="In 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>, we first finetune a 2D diffusion model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib53" title="">53</a>]</cite> to process the unposed sparse input images (<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS1" title="3.1 Tiling Sparse View Images as Input Condition ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">3.1</span></a>). The 2D diffusion model is responsible for jointly generating grid images for both the NOCS maps of the input views, as well as multi-view images with known camera poses. We use the predicted NOCS maps to estimate the camera poses for the input views (<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS2" title="3.2 Image-to-NOCS Diffusion as a Pose Estimator ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">3.2</span></a>). The resulting multi-view images are fed into a two-stage 3D diffusion model for a coarse-to-fine generation of a 3D textured mesh (<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS3" title="3.3 Multi-View Prediction for 3D Reconstruction ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">3.3</span></a>). This joint training strategy allows the two branches to complement each other. It enhances the understanding of both the input sparse views and the intrinsic properties of the 3D objects, thereby improving the performance of both pose estimation and 3D reconstruction. Optionally, the generated 3D mesh can also be used to further refine the camera poses (<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS4" title="3.4 Pose Refinement with Reconstructed 3D Model ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="S3.F3.g1" src="extracted/5793183/figures/inputoutput-sparp.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.10.5.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.8.4" style="font-size:90%;">(a) Regardless of the poses of the sparse input views (in black), the output multiviews are uniformly distributed (in red) and encompass the entire 3D object. (b) The Normalized Object Coordinate Space (NOCS) of the object, whose orientation is aligned with the azimuth of the first input view. (c) An example of input and output tiled images. The elevation and azimuth of the first input view are denoted by <math alttext="\theta_{0}" class="ltx_Math" display="inline" id="S3.F3.5.1.m1.1"><semantics id="S3.F3.5.1.m1.1b"><msub id="S3.F3.5.1.m1.1.1" xref="S3.F3.5.1.m1.1.1.cmml"><mi id="S3.F3.5.1.m1.1.1.2" xref="S3.F3.5.1.m1.1.1.2.cmml">Œ∏</mi><mn id="S3.F3.5.1.m1.1.1.3" xref="S3.F3.5.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F3.5.1.m1.1c"><apply id="S3.F3.5.1.m1.1.1.cmml" xref="S3.F3.5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F3.5.1.m1.1.1.1.cmml" xref="S3.F3.5.1.m1.1.1">subscript</csymbol><ci id="S3.F3.5.1.m1.1.1.2.cmml" xref="S3.F3.5.1.m1.1.1.2">ùúÉ</ci><cn id="S3.F3.5.1.m1.1.1.3.cmml" type="integer" xref="S3.F3.5.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.5.1.m1.1d">\theta_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.5.1.m1.1e">italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\phi_{0}" class="ltx_Math" display="inline" id="S3.F3.6.2.m2.1"><semantics id="S3.F3.6.2.m2.1b"><msub id="S3.F3.6.2.m2.1.1" xref="S3.F3.6.2.m2.1.1.cmml"><mi id="S3.F3.6.2.m2.1.1.2" xref="S3.F3.6.2.m2.1.1.2.cmml">œï</mi><mn id="S3.F3.6.2.m2.1.1.3" xref="S3.F3.6.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F3.6.2.m2.1c"><apply id="S3.F3.6.2.m2.1.1.cmml" xref="S3.F3.6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F3.6.2.m2.1.1.1.cmml" xref="S3.F3.6.2.m2.1.1">subscript</csymbol><ci id="S3.F3.6.2.m2.1.1.2.cmml" xref="S3.F3.6.2.m2.1.1.2">italic-œï</ci><cn id="S3.F3.6.2.m2.1.1.3.cmml" type="integer" xref="S3.F3.6.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.6.2.m2.1d">\phi_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.6.2.m2.1e">italic_œï start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, respectively. The camera poses of the output multiview images are determined by <math alttext="\phi_{0}" class="ltx_Math" display="inline" id="S3.F3.7.3.m3.1"><semantics id="S3.F3.7.3.m3.1b"><msub id="S3.F3.7.3.m3.1.1" xref="S3.F3.7.3.m3.1.1.cmml"><mi id="S3.F3.7.3.m3.1.1.2" xref="S3.F3.7.3.m3.1.1.2.cmml">œï</mi><mn id="S3.F3.7.3.m3.1.1.3" xref="S3.F3.7.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F3.7.3.m3.1c"><apply id="S3.F3.7.3.m3.1.1.cmml" xref="S3.F3.7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F3.7.3.m3.1.1.1.cmml" xref="S3.F3.7.3.m3.1.1">subscript</csymbol><ci id="S3.F3.7.3.m3.1.1.2.cmml" xref="S3.F3.7.3.m3.1.1.2">italic-œï</ci><cn id="S3.F3.7.3.m3.1.1.3.cmml" type="integer" xref="S3.F3.7.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.7.3.m3.1d">\phi_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.7.3.m3.1e">italic_œï start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. The output NOCS maps correspond to the input sparse views, and the orientation of the coordinate frame is also determined by <math alttext="\phi_{0}" class="ltx_Math" display="inline" id="S3.F3.8.4.m4.1"><semantics id="S3.F3.8.4.m4.1b"><msub id="S3.F3.8.4.m4.1.1" xref="S3.F3.8.4.m4.1.1.cmml"><mi id="S3.F3.8.4.m4.1.1.2" xref="S3.F3.8.4.m4.1.1.2.cmml">œï</mi><mn id="S3.F3.8.4.m4.1.1.3" xref="S3.F3.8.4.m4.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F3.8.4.m4.1c"><apply id="S3.F3.8.4.m4.1.1.cmml" xref="S3.F3.8.4.m4.1.1"><csymbol cd="ambiguous" id="S3.F3.8.4.m4.1.1.1.cmml" xref="S3.F3.8.4.m4.1.1">subscript</csymbol><ci id="S3.F3.8.4.m4.1.1.2.cmml" xref="S3.F3.8.4.m4.1.1.2">italic-œï</ci><cn id="S3.F3.8.4.m4.1.1.3.cmml" type="integer" xref="S3.F3.8.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.8.4.m4.1d">\phi_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.8.4.m4.1e">italic_œï start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Tiling Sparse View Images as Input Condition</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Recently, numerous studies have shown that 2D diffusion models not only possess robust open-world capabilities but also learn rich 3D geometric priors. For instance, Stable Diffusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib53" title="">53</a>]</cite>, can be finetuned to include camera view control¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib38" title="">38</a>]</cite>, enabling it to predict novel views of objects‚Äîa task that necessitates significant 3D spatial reasoning. Consequently, we are inspired to utilize the rich priors inherent in 2D diffusion models for the tasks of sparse view 3D reconstruction and pose estimation.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.2">Unlike most existing approaches that use a single RGB image as the condition and focus on synthesizing multi-view images, our goal is to take a sparse set of input images and stimulate Stable Diffusion to infer the spatial relationships among the input views implicitly. To accomplish this, given <math alttext="1\sim 6" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mn id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">‚àº</mo><mn id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1">similar-to</csymbol><cn id="S3.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.2">1</cn><cn id="S3.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">1\sim 6</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">1 ‚àº 6</annotation></semantics></math> sparse views from arbitrary camera poses, we tile them into a <math alttext="3\times 2" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mn id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">3</mn><mo id="S3.SS1.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.2.m2.1.1.1.cmml">√ó</mo><mn id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><times id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></times><cn id="S3.SS1.p2.2.m2.1.1.2.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.2">3</cn><cn id="S3.SS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">3\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">3 √ó 2</annotation></semantics></math> multi-view grid, as illustrated in Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.F3" title="Figure 3 ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">3</span></a> (c). The image in the first grid cell determines a canonical frame (to be discussed later), while the order of the other views is inconsequential. When there are fewer than 6 sparse views, we use empty padding for the remaining grid cells. This composite image then serves as the condition for Stable Diffusion, which is expected to assimilate all information from the input sparse views during the diffusion process.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">We employ Stable Diffusion 2.1 as our base model. To adapt the original text-conditioning to our tiled multi-view image condition, we follow¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib60" title="">60</a>]</cite> to apply both local and global conditioning strategies. For local conditioning, we use the reference-only attention mechanism¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib96" title="">96</a>]</cite>, where we process the reference tiled image with the denoising UNet model and append the attention keys and values from this image to corresponding layers in the denoising model for the target images. This mechanism facilitates implicit yet effective interactions between the diffusion model and the sparse views. For global conditioning, we integrate the mean-pooled CLIP embedding of all input images‚Äîmodulated by learnable token weights‚Äîinto the diffusion process, enhancing the model‚Äôs ability to grasp the overarching semantics and structure of the sparse views.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">As depicted in¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.F2" title="In 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Figs.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a> and¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.F3" title="Figure 3 ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">3</span></a>, our objective is to concurrently generate grid images for both NOCS maps of the input views and multi-view images from known camera poses. To achieve this, we utilize a domain switcher¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib38" title="">38</a>]</cite> that enables flexible toggling between the two domains. The switcher consists of two learnable embeddings, one for each domain, which are then injected into the UNet of the stable diffusion models by being added to its time embedding.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Image-to-NOCS Diffusion as a Pose Estimator</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Conventional Structure-from-Motion (SfM) solvers, such as COLMAP¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib56" title="">56</a>]</cite>, rely on feature matching for pose estimation. However, in scenarios with sparse views, there may be little to no overlap between input views. The lack of sufficient visual correspondence cues often renders the solvers unreliable and prone to failure. Consequently, instead of relying on local correspondences, we leverage the rich semantic priors embedded in 2D diffusion models for pose estimation.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">One of the primary challenges is to enable 2D diffusion models to output camera poses. While camera poses can be represented in various scalar formats (e.g., 6-dimensional vector, four-by-four matrix, etc.), they are not native representations for a 2D diffusion model to generate. Inspired by recent works demonstrating that 2D diffusion models can be used to predict normal maps¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib38" title="">38</a>]</cite>‚Äîa domain different from natural images‚Äîwe propose using a surrogate representation: the Normalized Object Coordinate Space (NOCS)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib74" title="">74</a>]</cite>. We finetune Stable Diffusion to predict NOCS maps for each input view.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">As depicted in Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.F3" title="Figure 3 ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">3</span></a>(b), a NOCS frame is determined for each set of input sparse view images and the underlying 3D object. Specifically, the 3D shape is normalized into a unit cube, i.e., <math alttext="{x,y,z}\in[0,1]" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.5"><semantics id="S3.SS2.p3.1.m1.5a"><mrow id="S3.SS2.p3.1.m1.5.6" xref="S3.SS2.p3.1.m1.5.6.cmml"><mrow id="S3.SS2.p3.1.m1.5.6.2.2" xref="S3.SS2.p3.1.m1.5.6.2.1.cmml"><mi id="S3.SS2.p3.1.m1.3.3" xref="S3.SS2.p3.1.m1.3.3.cmml">x</mi><mo id="S3.SS2.p3.1.m1.5.6.2.2.1" xref="S3.SS2.p3.1.m1.5.6.2.1.cmml">,</mo><mi id="S3.SS2.p3.1.m1.4.4" xref="S3.SS2.p3.1.m1.4.4.cmml">y</mi><mo id="S3.SS2.p3.1.m1.5.6.2.2.2" xref="S3.SS2.p3.1.m1.5.6.2.1.cmml">,</mo><mi id="S3.SS2.p3.1.m1.5.5" xref="S3.SS2.p3.1.m1.5.5.cmml">z</mi></mrow><mo id="S3.SS2.p3.1.m1.5.6.1" xref="S3.SS2.p3.1.m1.5.6.1.cmml">‚àà</mo><mrow id="S3.SS2.p3.1.m1.5.6.3.2" xref="S3.SS2.p3.1.m1.5.6.3.1.cmml"><mo id="S3.SS2.p3.1.m1.5.6.3.2.1" stretchy="false" xref="S3.SS2.p3.1.m1.5.6.3.1.cmml">[</mo><mn id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.p3.1.m1.5.6.3.2.2" xref="S3.SS2.p3.1.m1.5.6.3.1.cmml">,</mo><mn id="S3.SS2.p3.1.m1.2.2" xref="S3.SS2.p3.1.m1.2.2.cmml">1</mn><mo id="S3.SS2.p3.1.m1.5.6.3.2.3" stretchy="false" xref="S3.SS2.p3.1.m1.5.6.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.5b"><apply id="S3.SS2.p3.1.m1.5.6.cmml" xref="S3.SS2.p3.1.m1.5.6"><in id="S3.SS2.p3.1.m1.5.6.1.cmml" xref="S3.SS2.p3.1.m1.5.6.1"></in><list id="S3.SS2.p3.1.m1.5.6.2.1.cmml" xref="S3.SS2.p3.1.m1.5.6.2.2"><ci id="S3.SS2.p3.1.m1.3.3.cmml" xref="S3.SS2.p3.1.m1.3.3">ùë•</ci><ci id="S3.SS2.p3.1.m1.4.4.cmml" xref="S3.SS2.p3.1.m1.4.4">ùë¶</ci><ci id="S3.SS2.p3.1.m1.5.5.cmml" xref="S3.SS2.p3.1.m1.5.5">ùëß</ci></list><interval closure="closed" id="S3.SS2.p3.1.m1.5.6.3.1.cmml" xref="S3.SS2.p3.1.m1.5.6.3.2"><cn id="S3.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p3.1.m1.1.1">0</cn><cn id="S3.SS2.p3.1.m1.2.2.cmml" type="integer" xref="S3.SS2.p3.1.m1.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.5c">{x,y,z}\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.5d">italic_x , italic_y , italic_z ‚àà [ 0 , 1 ]</annotation></semantics></math>. The shape‚Äôs upward axis aligns with the dataset‚Äôs inherent upward axis of the 3D object, typically the gravity axis. Predicting the object‚Äôs forward-facing direction may be ambiguous, so we rotate the 3D shape in the NOCS frame to align its forward direction (zero azimuth) with that of the first input view, thus unambiguously establishing the NOCS frame. For each input view, we then render a NOCS map, where each 2D pixel (<span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.1">r,g,b</span>) represents the corresponding 3D point‚Äôs position (<span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.2">x,y,z</span>) in the defined NOCS frame, as shown in Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.F3" title="Figure 3 ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">3</span></a>(c). These NOCS maps align with the operational domain of 2D diffusion models, similar to the normal maps in previous work¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">To facilitate interactions between NOCS maps from different views and generate more 3D-consistent NOCS maps, we tile all NOCS maps into a <math alttext="3\times 2" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mn id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">3</mn><mo id="S3.SS2.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p4.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><times id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1"></times><cn id="S3.SS2.p4.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1.2">3</cn><cn id="S3.SS2.p4.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">3\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">3 √ó 2</annotation></semantics></math> grid image as the input condition (see¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS1" title="3.1 Tiling Sparse View Images as Input Condition ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">3.1</span></a>), following the same tiling order and the empty padding convention. We finetune Stable Diffusion to generate these multi-view tiled NOCS maps, so the 2D diffusion model can attend to both the input sparse views and their NOCS maps during the diffusion process.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">After generating the NOCS maps for the input sparse views, we employ a traditional Perspective-n-Point (PnP) solver¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib2" title="">2</a>]</cite> to compute the poses <math alttext="\{\boldsymbol{\xi}_{i}^{\text{pnp}}\}" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.2.cmml"><mo id="S3.SS2.p5.1.m1.1.1.1.2" stretchy="false" xref="S3.SS2.p5.1.m1.1.1.2.cmml">{</mo><msubsup id="S3.SS2.p5.1.m1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.cmml">ùùÉ</mi><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.3.cmml">i</mi><mtext id="S3.SS2.p5.1.m1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.3a.cmml">pnp</mtext></msubsup><mo id="S3.SS2.p5.1.m1.1.1.1.3" stretchy="false" xref="S3.SS2.p5.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><set id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1"><apply id="S3.SS2.p5.1.m1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1">superscript</csymbol><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2">ùùÉ</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.3">ùëñ</ci></apply><ci id="S3.SS2.p5.1.m1.1.1.1.1.3a.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3"><mtext id="S3.SS2.p5.1.m1.1.1.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p5.1.m1.1.1.1.1.3">pnp</mtext></ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\{\boldsymbol{\xi}_{i}^{\text{pnp}}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">{ bold_italic_Œæ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT pnp end_POSTSUPERSCRIPT }</annotation></semantics></math> from the NOCS frame to the camera frames of each input view by minimizing the reprojection error:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\boldsymbol{\xi}_{i}^{\text{pnp}}=\arg\min_{\boldsymbol{\xi}_{i}\in\text{SE(3)%
}}\sum_{j=1}^{m_{i}}\|\mathbf{p}_{i,j}-\text{proj}(\mathbf{q}_{i,j},%
\boldsymbol{\xi}_{i})\|_{2}^{2},\vspace{-.5em}" class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><msubsup id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.3.2.2" xref="S3.E1.m1.5.5.1.1.3.2.2.cmml">ùùÉ</mi><mi id="S3.E1.m1.5.5.1.1.3.2.3" xref="S3.E1.m1.5.5.1.1.3.2.3.cmml">i</mi><mtext id="S3.E1.m1.5.5.1.1.3.3" xref="S3.E1.m1.5.5.1.1.3.3a.cmml">pnp</mtext></msubsup><mo id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.1.3.1" xref="S3.E1.m1.5.5.1.1.1.3.1.cmml">arg</mi><mo id="S3.E1.m1.5.5.1.1.1.3a" lspace="0.167em" xref="S3.E1.m1.5.5.1.1.1.3.cmml">‚Å°</mo><munder id="S3.E1.m1.5.5.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.1.3.2.cmml"><mi id="S3.E1.m1.5.5.1.1.1.3.2.2" xref="S3.E1.m1.5.5.1.1.1.3.2.2.cmml">min</mi><mrow id="S3.E1.m1.5.5.1.1.1.3.2.3" xref="S3.E1.m1.5.5.1.1.1.3.2.3.cmml"><msub id="S3.E1.m1.5.5.1.1.1.3.2.3.2" xref="S3.E1.m1.5.5.1.1.1.3.2.3.2.cmml"><mi id="S3.E1.m1.5.5.1.1.1.3.2.3.2.2" xref="S3.E1.m1.5.5.1.1.1.3.2.3.2.2.cmml">ùùÉ</mi><mi id="S3.E1.m1.5.5.1.1.1.3.2.3.2.3" xref="S3.E1.m1.5.5.1.1.1.3.2.3.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.5.5.1.1.1.3.2.3.1" xref="S3.E1.m1.5.5.1.1.1.3.2.3.1.cmml">‚àà</mo><mtext id="S3.E1.m1.5.5.1.1.1.3.2.3.3" xref="S3.E1.m1.5.5.1.1.1.3.2.3.3a.cmml">SE(3)</mtext></mrow></munder></mrow><mo id="S3.E1.m1.5.5.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><munderover id="S3.E1.m1.5.5.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S3.E1.m1.5.5.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.2.2.3.2" xref="S3.E1.m1.5.5.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S3.E1.m1.5.5.1.1.1.1.2.2.3.1" xref="S3.E1.m1.5.5.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.5.5.1.1.1.1.2.2.3.3" xref="S3.E1.m1.5.5.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><msub id="S3.E1.m1.5.5.1.1.1.1.2.3" xref="S3.E1.m1.5.5.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.2.3.2" xref="S3.E1.m1.5.5.1.1.1.1.2.3.2.cmml">m</mi><mi id="S3.E1.m1.5.5.1.1.1.1.2.3.3" xref="S3.E1.m1.5.5.1.1.1.1.2.3.3.cmml">i</mi></msub></munderover><msubsup id="S3.E1.m1.5.5.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.4.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.4.2.cmml">ùê©</mi><mrow id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.2.2.2.4.1" xref="S3.E1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml">‚àí</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml"><mtext id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.4" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.4a.cmml">proj</mtext><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.3.cmml">‚Å¢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">ùê™</mi><mrow id="S3.E1.m1.4.4.2.4" xref="S3.E1.m1.4.4.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">i</mi><mo id="S3.E1.m1.4.4.2.4.1" xref="S3.E1.m1.4.4.2.3.cmml">,</mo><mi id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.2.cmml">j</mi></mrow></msub><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.4" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">ùùÉ</mi><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.5" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E1.m1.5.5.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E1.m1.5.5.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"></eq><apply id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3">superscript</csymbol><apply id="S3.E1.m1.5.5.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3.2.1.cmml" xref="S3.E1.m1.5.5.1.1.3">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.3.2.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2.2">ùùÉ</ci><ci id="S3.E1.m1.5.5.1.1.3.2.3.cmml" xref="S3.E1.m1.5.5.1.1.3.2.3">ùëñ</ci></apply><ci id="S3.E1.m1.5.5.1.1.3.3a.cmml" xref="S3.E1.m1.5.5.1.1.3.3"><mtext id="S3.E1.m1.5.5.1.1.3.3.cmml" mathsize="70%" xref="S3.E1.m1.5.5.1.1.3.3">pnp</mtext></ci></apply><apply id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.2"></times><apply id="S3.E1.m1.5.5.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.3"><arg id="S3.E1.m1.5.5.1.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.3.1"></arg><apply id="S3.E1.m1.5.5.1.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.3.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2">subscript</csymbol><min id="S3.E1.m1.5.5.1.1.1.3.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2.2"></min><apply id="S3.E1.m1.5.5.1.1.1.3.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2.3"><in id="S3.E1.m1.5.5.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2.3.1"></in><apply id="S3.E1.m1.5.5.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.3.2.3.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2.3.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.3.2.3.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2.3.2.2">ùùÉ</ci><ci id="S3.E1.m1.5.5.1.1.1.3.2.3.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2.3.2.3">ùëñ</ci></apply><ci id="S3.E1.m1.5.5.1.1.1.3.2.3.3a.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2.3.3"><mtext id="S3.E1.m1.5.5.1.1.1.3.2.3.3.cmml" mathsize="70%" xref="S3.E1.m1.5.5.1.1.1.3.2.3.3">SE(3)</mtext></ci></apply></apply></apply><apply id="S3.E1.m1.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1"><apply id="S3.E1.m1.5.5.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.5.5.1.1.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2">subscript</csymbol><sum id="S3.E1.m1.5.5.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.2.2"></sum><apply id="S3.E1.m1.5.5.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.2.3"><eq id="S3.E1.m1.5.5.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.5.5.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.2.3.2">ùëó</ci><cn id="S3.E1.m1.5.5.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.5.5.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E1.m1.5.5.1.1.1.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.3.2">ùëö</ci><ci id="S3.E1.m1.5.5.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.3.3">ùëñ</ci></apply></apply><apply id="S3.E1.m1.5.5.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3"></minus><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.4.2">ùê©</ci><list id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.4"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ùëñ</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">ùëó</ci></list></apply><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2"><times id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.3"></times><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.4a.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.4"><mtext id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.4">proj</mtext></ci><interval closure="open" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2"><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ùê™</ci><list id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.4"><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">ùëñ</ci><ci id="S3.E1.m1.4.4.2.2.cmml" xref="S3.E1.m1.4.4.2.2">ùëó</ci></list></apply><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.2">ùùÉ</ci><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.2.2.3">ùëñ</ci></apply></interval></apply></apply></apply><cn id="S3.E1.m1.5.5.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.5.5.1.1.1.1.1.1.3">2</cn></apply><cn id="S3.E1.m1.5.5.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.5.5.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\boldsymbol{\xi}_{i}^{\text{pnp}}=\arg\min_{\boldsymbol{\xi}_{i}\in\text{SE(3)%
}}\sum_{j=1}^{m_{i}}\|\mathbf{p}_{i,j}-\text{proj}(\mathbf{q}_{i,j},%
\boldsymbol{\xi}_{i})\|_{2}^{2},\vspace{-.5em}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">bold_italic_Œæ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT pnp end_POSTSUPERSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT bold_italic_Œæ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚àà SE(3) end_POSTSUBSCRIPT ‚àë start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ‚à• bold_p start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT - proj ( bold_q start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT , bold_italic_Œæ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p5.11">where <math alttext="\mathbf{p}_{i,j}" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m1.2"><semantics id="S3.SS2.p5.2.m1.2a"><msub id="S3.SS2.p5.2.m1.2.3" xref="S3.SS2.p5.2.m1.2.3.cmml"><mi id="S3.SS2.p5.2.m1.2.3.2" xref="S3.SS2.p5.2.m1.2.3.2.cmml">ùê©</mi><mrow id="S3.SS2.p5.2.m1.2.2.2.4" xref="S3.SS2.p5.2.m1.2.2.2.3.cmml"><mi id="S3.SS2.p5.2.m1.1.1.1.1" xref="S3.SS2.p5.2.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p5.2.m1.2.2.2.4.1" xref="S3.SS2.p5.2.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p5.2.m1.2.2.2.2" xref="S3.SS2.p5.2.m1.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m1.2b"><apply id="S3.SS2.p5.2.m1.2.3.cmml" xref="S3.SS2.p5.2.m1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m1.2.3.1.cmml" xref="S3.SS2.p5.2.m1.2.3">subscript</csymbol><ci id="S3.SS2.p5.2.m1.2.3.2.cmml" xref="S3.SS2.p5.2.m1.2.3.2">ùê©</ci><list id="S3.SS2.p5.2.m1.2.2.2.3.cmml" xref="S3.SS2.p5.2.m1.2.2.2.4"><ci id="S3.SS2.p5.2.m1.1.1.1.1.cmml" xref="S3.SS2.p5.2.m1.1.1.1.1">ùëñ</ci><ci id="S3.SS2.p5.2.m1.2.2.2.2.cmml" xref="S3.SS2.p5.2.m1.2.2.2.2">ùëó</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m1.2c">\mathbf{p}_{i,j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m1.2d">bold_p start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math> represents the <math alttext="j^{\text{th}}" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m2.1"><semantics id="S3.SS2.p5.3.m2.1a"><msup id="S3.SS2.p5.3.m2.1.1" xref="S3.SS2.p5.3.m2.1.1.cmml"><mi id="S3.SS2.p5.3.m2.1.1.2" xref="S3.SS2.p5.3.m2.1.1.2.cmml">j</mi><mtext id="S3.SS2.p5.3.m2.1.1.3" xref="S3.SS2.p5.3.m2.1.1.3a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m2.1b"><apply id="S3.SS2.p5.3.m2.1.1.cmml" xref="S3.SS2.p5.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m2.1.1.1.cmml" xref="S3.SS2.p5.3.m2.1.1">superscript</csymbol><ci id="S3.SS2.p5.3.m2.1.1.2.cmml" xref="S3.SS2.p5.3.m2.1.1.2">ùëó</ci><ci id="S3.SS2.p5.3.m2.1.1.3a.cmml" xref="S3.SS2.p5.3.m2.1.1.3"><mtext id="S3.SS2.p5.3.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p5.3.m2.1.1.3">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m2.1c">j^{\text{th}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.3.m2.1d">italic_j start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> pixel‚Äôs location in the <math alttext="i^{\text{th}}" class="ltx_Math" display="inline" id="S3.SS2.p5.4.m3.1"><semantics id="S3.SS2.p5.4.m3.1a"><msup id="S3.SS2.p5.4.m3.1.1" xref="S3.SS2.p5.4.m3.1.1.cmml"><mi id="S3.SS2.p5.4.m3.1.1.2" xref="S3.SS2.p5.4.m3.1.1.2.cmml">i</mi><mtext id="S3.SS2.p5.4.m3.1.1.3" xref="S3.SS2.p5.4.m3.1.1.3a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m3.1b"><apply id="S3.SS2.p5.4.m3.1.1.cmml" xref="S3.SS2.p5.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m3.1.1.1.cmml" xref="S3.SS2.p5.4.m3.1.1">superscript</csymbol><ci id="S3.SS2.p5.4.m3.1.1.2.cmml" xref="S3.SS2.p5.4.m3.1.1.2">ùëñ</ci><ci id="S3.SS2.p5.4.m3.1.1.3a.cmml" xref="S3.SS2.p5.4.m3.1.1.3"><mtext id="S3.SS2.p5.4.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p5.4.m3.1.1.3">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m3.1c">i^{\text{th}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.4.m3.1d">italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> NOCS map; <math alttext="\mathbf{q}_{i,j}" class="ltx_Math" display="inline" id="S3.SS2.p5.5.m4.2"><semantics id="S3.SS2.p5.5.m4.2a"><msub id="S3.SS2.p5.5.m4.2.3" xref="S3.SS2.p5.5.m4.2.3.cmml"><mi id="S3.SS2.p5.5.m4.2.3.2" xref="S3.SS2.p5.5.m4.2.3.2.cmml">ùê™</mi><mrow id="S3.SS2.p5.5.m4.2.2.2.4" xref="S3.SS2.p5.5.m4.2.2.2.3.cmml"><mi id="S3.SS2.p5.5.m4.1.1.1.1" xref="S3.SS2.p5.5.m4.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p5.5.m4.2.2.2.4.1" xref="S3.SS2.p5.5.m4.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p5.5.m4.2.2.2.2" xref="S3.SS2.p5.5.m4.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m4.2b"><apply id="S3.SS2.p5.5.m4.2.3.cmml" xref="S3.SS2.p5.5.m4.2.3"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m4.2.3.1.cmml" xref="S3.SS2.p5.5.m4.2.3">subscript</csymbol><ci id="S3.SS2.p5.5.m4.2.3.2.cmml" xref="S3.SS2.p5.5.m4.2.3.2">ùê™</ci><list id="S3.SS2.p5.5.m4.2.2.2.3.cmml" xref="S3.SS2.p5.5.m4.2.2.2.4"><ci id="S3.SS2.p5.5.m4.1.1.1.1.cmml" xref="S3.SS2.p5.5.m4.1.1.1.1">ùëñ</ci><ci id="S3.SS2.p5.5.m4.2.2.2.2.cmml" xref="S3.SS2.p5.5.m4.2.2.2.2">ùëó</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m4.2c">\mathbf{q}_{i,j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.5.m4.2d">bold_q start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the corresponding 3D point location in the NOCS frame; <math alttext="m_{i}" class="ltx_Math" display="inline" id="S3.SS2.p5.6.m5.1"><semantics id="S3.SS2.p5.6.m5.1a"><msub id="S3.SS2.p5.6.m5.1.1" xref="S3.SS2.p5.6.m5.1.1.cmml"><mi id="S3.SS2.p5.6.m5.1.1.2" xref="S3.SS2.p5.6.m5.1.1.2.cmml">m</mi><mi id="S3.SS2.p5.6.m5.1.1.3" xref="S3.SS2.p5.6.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m5.1b"><apply id="S3.SS2.p5.6.m5.1.1.cmml" xref="S3.SS2.p5.6.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m5.1.1.1.cmml" xref="S3.SS2.p5.6.m5.1.1">subscript</csymbol><ci id="S3.SS2.p5.6.m5.1.1.2.cmml" xref="S3.SS2.p5.6.m5.1.1.2">ùëö</ci><ci id="S3.SS2.p5.6.m5.1.1.3.cmml" xref="S3.SS2.p5.6.m5.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m5.1c">m_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.6.m5.1d">italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the number of pixels for the <math alttext="i^{\text{th}}" class="ltx_Math" display="inline" id="S3.SS2.p5.7.m6.1"><semantics id="S3.SS2.p5.7.m6.1a"><msup id="S3.SS2.p5.7.m6.1.1" xref="S3.SS2.p5.7.m6.1.1.cmml"><mi id="S3.SS2.p5.7.m6.1.1.2" xref="S3.SS2.p5.7.m6.1.1.2.cmml">i</mi><mtext id="S3.SS2.p5.7.m6.1.1.3" xref="S3.SS2.p5.7.m6.1.1.3a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m6.1b"><apply id="S3.SS2.p5.7.m6.1.1.cmml" xref="S3.SS2.p5.7.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.7.m6.1.1.1.cmml" xref="S3.SS2.p5.7.m6.1.1">superscript</csymbol><ci id="S3.SS2.p5.7.m6.1.1.2.cmml" xref="S3.SS2.p5.7.m6.1.1.2">ùëñ</ci><ci id="S3.SS2.p5.7.m6.1.1.3a.cmml" xref="S3.SS2.p5.7.m6.1.1.3"><mtext id="S3.SS2.p5.7.m6.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p5.7.m6.1.1.3">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m6.1c">i^{\text{th}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.7.m6.1d">italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> view, and <span class="ltx_text ltx_markedasmath" id="S3.SS2.p5.11.1">proj</span> is the perspective projection operation. Note that the PnP algorithm assumes known camera intrinsics and optimizes only for the camera extrinsics. A RANSAC scheme is applied during the PnP computation for outlier removal, enhancing the robustness of the pose prediction to boundary noises and errors from the 2D diffusion model. As all NOCS maps share a common NOCS frame, we can thus determine the relative camera poses between views <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p5.9.m8.1"><semantics id="S3.SS2.p5.9.m8.1a"><mi id="S3.SS2.p5.9.m8.1.1" xref="S3.SS2.p5.9.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.9.m8.1b"><ci id="S3.SS2.p5.9.m8.1.1.cmml" xref="S3.SS2.p5.9.m8.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.9.m8.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.9.m8.1d">italic_i</annotation></semantics></math> and <math alttext="i^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p5.10.m9.1"><semantics id="S3.SS2.p5.10.m9.1a"><msup id="S3.SS2.p5.10.m9.1.1" xref="S3.SS2.p5.10.m9.1.1.cmml"><mi id="S3.SS2.p5.10.m9.1.1.2" xref="S3.SS2.p5.10.m9.1.1.2.cmml">i</mi><mo id="S3.SS2.p5.10.m9.1.1.3" xref="S3.SS2.p5.10.m9.1.1.3.cmml">‚Ä≤</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.10.m9.1b"><apply id="S3.SS2.p5.10.m9.1.1.cmml" xref="S3.SS2.p5.10.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.10.m9.1.1.1.cmml" xref="S3.SS2.p5.10.m9.1.1">superscript</csymbol><ci id="S3.SS2.p5.10.m9.1.1.2.cmml" xref="S3.SS2.p5.10.m9.1.1.2">ùëñ</ci><ci id="S3.SS2.p5.10.m9.1.1.3.cmml" xref="S3.SS2.p5.10.m9.1.1.3">‚Ä≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.10.m9.1c">i^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.10.m9.1d">italic_i start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT</annotation></semantics></math> through <math alttext="\boldsymbol{\xi}_{i}^{-1}\boldsymbol{\xi}_{i^{\prime}}" class="ltx_Math" display="inline" id="S3.SS2.p5.11.m10.1"><semantics id="S3.SS2.p5.11.m10.1a"><mrow id="S3.SS2.p5.11.m10.1.1" xref="S3.SS2.p5.11.m10.1.1.cmml"><msubsup id="S3.SS2.p5.11.m10.1.1.2" xref="S3.SS2.p5.11.m10.1.1.2.cmml"><mi id="S3.SS2.p5.11.m10.1.1.2.2.2" xref="S3.SS2.p5.11.m10.1.1.2.2.2.cmml">ùùÉ</mi><mi id="S3.SS2.p5.11.m10.1.1.2.2.3" xref="S3.SS2.p5.11.m10.1.1.2.2.3.cmml">i</mi><mrow id="S3.SS2.p5.11.m10.1.1.2.3" xref="S3.SS2.p5.11.m10.1.1.2.3.cmml"><mo id="S3.SS2.p5.11.m10.1.1.2.3a" xref="S3.SS2.p5.11.m10.1.1.2.3.cmml">‚àí</mo><mn id="S3.SS2.p5.11.m10.1.1.2.3.2" xref="S3.SS2.p5.11.m10.1.1.2.3.2.cmml">1</mn></mrow></msubsup><mo id="S3.SS2.p5.11.m10.1.1.1" xref="S3.SS2.p5.11.m10.1.1.1.cmml">‚Å¢</mo><msub id="S3.SS2.p5.11.m10.1.1.3" xref="S3.SS2.p5.11.m10.1.1.3.cmml"><mi id="S3.SS2.p5.11.m10.1.1.3.2" xref="S3.SS2.p5.11.m10.1.1.3.2.cmml">ùùÉ</mi><msup id="S3.SS2.p5.11.m10.1.1.3.3" xref="S3.SS2.p5.11.m10.1.1.3.3.cmml"><mi id="S3.SS2.p5.11.m10.1.1.3.3.2" xref="S3.SS2.p5.11.m10.1.1.3.3.2.cmml">i</mi><mo id="S3.SS2.p5.11.m10.1.1.3.3.3" xref="S3.SS2.p5.11.m10.1.1.3.3.3.cmml">‚Ä≤</mo></msup></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.11.m10.1b"><apply id="S3.SS2.p5.11.m10.1.1.cmml" xref="S3.SS2.p5.11.m10.1.1"><times id="S3.SS2.p5.11.m10.1.1.1.cmml" xref="S3.SS2.p5.11.m10.1.1.1"></times><apply id="S3.SS2.p5.11.m10.1.1.2.cmml" xref="S3.SS2.p5.11.m10.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p5.11.m10.1.1.2.1.cmml" xref="S3.SS2.p5.11.m10.1.1.2">superscript</csymbol><apply id="S3.SS2.p5.11.m10.1.1.2.2.cmml" xref="S3.SS2.p5.11.m10.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p5.11.m10.1.1.2.2.1.cmml" xref="S3.SS2.p5.11.m10.1.1.2">subscript</csymbol><ci id="S3.SS2.p5.11.m10.1.1.2.2.2.cmml" xref="S3.SS2.p5.11.m10.1.1.2.2.2">ùùÉ</ci><ci id="S3.SS2.p5.11.m10.1.1.2.2.3.cmml" xref="S3.SS2.p5.11.m10.1.1.2.2.3">ùëñ</ci></apply><apply id="S3.SS2.p5.11.m10.1.1.2.3.cmml" xref="S3.SS2.p5.11.m10.1.1.2.3"><minus id="S3.SS2.p5.11.m10.1.1.2.3.1.cmml" xref="S3.SS2.p5.11.m10.1.1.2.3"></minus><cn id="S3.SS2.p5.11.m10.1.1.2.3.2.cmml" type="integer" xref="S3.SS2.p5.11.m10.1.1.2.3.2">1</cn></apply></apply><apply id="S3.SS2.p5.11.m10.1.1.3.cmml" xref="S3.SS2.p5.11.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.11.m10.1.1.3.1.cmml" xref="S3.SS2.p5.11.m10.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.11.m10.1.1.3.2.cmml" xref="S3.SS2.p5.11.m10.1.1.3.2">ùùÉ</ci><apply id="S3.SS2.p5.11.m10.1.1.3.3.cmml" xref="S3.SS2.p5.11.m10.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p5.11.m10.1.1.3.3.1.cmml" xref="S3.SS2.p5.11.m10.1.1.3.3">superscript</csymbol><ci id="S3.SS2.p5.11.m10.1.1.3.3.2.cmml" xref="S3.SS2.p5.11.m10.1.1.3.3.2">ùëñ</ci><ci id="S3.SS2.p5.11.m10.1.1.3.3.3.cmml" xref="S3.SS2.p5.11.m10.1.1.3.3.3">‚Ä≤</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.11.m10.1c">\boldsymbol{\xi}_{i}^{-1}\boldsymbol{\xi}_{i^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.11.m10.1d">bold_italic_Œæ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_Œæ start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multi-View Prediction for 3D Reconstruction</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We follow the paradigm of recent single-image-to-3D methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib28" title="">28</a>]</cite> by initially generating multi-view images and subsequently using a feed-forward 3D reconstruction module to convert these images into a 3D representation. It is noteworthy that the input sparse views might not encompass the entire 3D objects, nor provide adequate information for 3D reconstruction. Therefore, we propose to predict multi-view images at uniformly distributed camera poses first, and then use these predicted images for 3D reconstruction.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.3">Unlike traditional novel view synthesis¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib35" title="">35</a>]</cite>, our approach employs a fixed camera configuration for target multi-views. As depicted in¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.F3" title="In 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>, our target multi-view images consist of six views with alternating <math alttext="20^{\circ}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><msup id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">20</mn><mo id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">superscript</csymbol><cn id="S3.SS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.p2.1.m1.1.1.2">20</cn><compose id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">20 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="-10^{\circ}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mo id="S3.SS3.p2.2.m2.1.1a" xref="S3.SS3.p2.2.m2.1.1.cmml">‚àí</mo><msup id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml"><mn id="S3.SS3.p2.2.m2.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.2.2.cmml">10</mn><mo id="S3.SS3.p2.2.m2.1.1.2.3" xref="S3.SS3.p2.2.m2.1.1.2.3.cmml">‚àò</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><minus id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"></minus><apply id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1.2">superscript</csymbol><cn id="S3.SS3.p2.2.m2.1.1.2.2.cmml" type="integer" xref="S3.SS3.p2.2.m2.1.1.2.2">10</cn><compose id="S3.SS3.p2.2.m2.1.1.2.3.cmml" xref="S3.SS3.p2.2.m2.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">-10^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">- 10 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPT</annotation></semantics></math> elevations, and <math alttext="60^{\circ}" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><msup id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mn id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">60</mn><mo id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">superscript</csymbol><cn id="S3.SS3.p2.3.m3.1.1.2.cmml" type="integer" xref="S3.SS3.p2.3.m3.1.1.2">60</cn><compose id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">60^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">60 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPT</annotation></semantics></math>-spaced azimuths relative to the first input view. Although the elevation angles are set absolutely, the azimuth angles are relative to the azimuth of the first input sparse view to resolve the ambiguity in face-forwarding directions. Furthermore, We maintain consistent camera intrinsics across target views, independent of input views. These strategies mitigate challenges in predicting camera intrinsics and elevation during the 3D reconstruction process. Existing methods hindered by this issue may be sensitive to intrinsic variations and often depend on predicting¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib34" title="">34</a>]</cite> or requiring user-specified¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib65" title="">65</a>]</cite> input image elevations.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Similar to NOCS map prediction, we tile all six views into a <math alttext="3\times 2" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mn id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">3</mn><mo id="S3.SS3.p3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p3.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><times id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></times><cn id="S3.SS3.p3.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.p3.1.m1.1.1.2">3</cn><cn id="S3.SS3.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">3\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">3 √ó 2</annotation></semantics></math> grid image and finetune Stable Diffusion to generate this tiled image. The 2D diffusion model, conditioned on the input sparse views, aims to incorporate all information from input views, deduce the underlying 3D objects, and predict the multi-view images at the predetermined camera poses. Although the predicted poses of input sparse views (<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS2" title="3.2 Image-to-NOCS Diffusion as a Pose Estimator ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">3.2</span></a>) are not directly employed in the 3D reconstruction, the joint training of NOCS prediction and multi-view prediction branches implicitly complement each other and boost the performance of both tasks.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.2">Upon generating the multi-view images at known camera poses, we utilize the multi-view to 3D reconstruction module proposed in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib32" title="">32</a>]</cite> to lift these images to 3D. The reconstruction module adopts a two-stage coarse-to-fine approach, which involves initially extracting the 2D features of the generated multi-view images, aggregating them with the known camera poses, and constructing a 3D cost volume. This 3D cost volume acts as the condition for the 3D diffusion networks. In the coarse stage, a low-resolution <math alttext="64^{3}" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.1"><semantics id="S3.SS3.p4.1.m1.1a"><msup id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mn id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">64</mn><mn id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">superscript</csymbol><cn id="S3.SS3.p4.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.p4.1.m1.1.1.2">64</cn><cn id="S3.SS3.p4.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p4.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">64^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.1d">64 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> 3D occupancy volume is produced. This is subsequently refined to yield a high-resolution <math alttext="128^{3}" class="ltx_Math" display="inline" id="S3.SS3.p4.2.m2.1"><semantics id="S3.SS3.p4.2.m2.1a"><msup id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mn id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml">128</mn><mn id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">superscript</csymbol><cn id="S3.SS3.p4.2.m2.1.1.2.cmml" type="integer" xref="S3.SS3.p4.2.m2.1.1.2">128</cn><cn id="S3.SS3.p4.2.m2.1.1.3.cmml" type="integer" xref="S3.SS3.p4.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">128^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.2.m2.1d">128 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> SDF (Signed Distance Field) volume with colors. Finally, a textured mesh is derived from the SDF volume employing the marching cubes algorithm.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Pose Refinement with Reconstructed 3D Model</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In Section¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S3.SS2" title="3.2 Image-to-NOCS Diffusion as a Pose Estimator ‚Ä£ 3 Method ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we finetune diffusion models for NOCS map prediction and camera pose estimation. However, due to the hallucinatory and stochastic nature of diffusion models, unavoidable errors may exist. The generated 3D mesh <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">‚Ñ≥</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">‚Ñ≥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">caligraphic_M</annotation></semantics></math>, though not perfect, provides a multi-view consistent and explicit 3D structure. We can further refine the coarse poses predicted from the NOCS maps by leveraging the reconstructed 3D shape.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.6"><span class="ltx_text ltx_font_bold" id="S3.SS4.p2.6.1">Pose Refinement via Differentiable Rendering.</span>
Starting with initial poses <math alttext="\{\boldsymbol{\xi}_{i}^{\text{pnp}}\}" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.1"><semantics id="S3.SS4.p2.1.m1.1a"><mrow id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.2.cmml"><mo id="S3.SS4.p2.1.m1.1.1.1.2" stretchy="false" xref="S3.SS4.p2.1.m1.1.1.2.cmml">{</mo><msubsup id="S3.SS4.p2.1.m1.1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.1.1.2.2" xref="S3.SS4.p2.1.m1.1.1.1.1.2.2.cmml">ùùÉ</mi><mi id="S3.SS4.p2.1.m1.1.1.1.1.2.3" xref="S3.SS4.p2.1.m1.1.1.1.1.2.3.cmml">i</mi><mtext id="S3.SS4.p2.1.m1.1.1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.1.1.3a.cmml">pnp</mtext></msubsup><mo id="S3.SS4.p2.1.m1.1.1.1.3" stretchy="false" xref="S3.SS4.p2.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><set id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.1"><apply id="S3.SS4.p2.1.m1.1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1">superscript</csymbol><apply id="S3.SS4.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.2.2">ùùÉ</ci><ci id="S3.SS4.p2.1.m1.1.1.1.1.2.3.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.2.3">ùëñ</ci></apply><ci id="S3.SS4.p2.1.m1.1.1.1.1.3a.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.3"><mtext id="S3.SS4.p2.1.m1.1.1.1.1.3.cmml" mathsize="70%" xref="S3.SS4.p2.1.m1.1.1.1.1.3">pnp</mtext></ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\{\boldsymbol{\xi}_{i}^{\text{pnp}}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.1.m1.1d">{ bold_italic_Œæ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT pnp end_POSTSUPERSCRIPT }</annotation></semantics></math> extracted from the predicted NOCS maps, we refine them through differentiable rendering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib26" title="">26</a>]</cite>. Specifically, we render the generated mesh <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.SS4.p2.2.m2.1"><semantics id="S3.SS4.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">‚Ñ≥</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">‚Ñ≥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.2.m2.1d">caligraphic_M</annotation></semantics></math> at optimizing camera poses <math alttext="\boldsymbol{\xi}_{i}" class="ltx_Math" display="inline" id="S3.SS4.p2.3.m3.1"><semantics id="S3.SS4.p2.3.m3.1a"><msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">ùùÉ</mi><mi id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">ùùÉ</ci><ci id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">\boldsymbol{\xi}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.3.m3.1d">bold_italic_Œæ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. We minimize the rendering loss between the rendered image <math alttext="\mathbf{I}^{r}_{i}=\mathcal{R}(\mathcal{M},\boldsymbol{\xi}_{i})" class="ltx_Math" display="inline" id="S3.SS4.p2.4.m4.2"><semantics id="S3.SS4.p2.4.m4.2a"><mrow id="S3.SS4.p2.4.m4.2.2" xref="S3.SS4.p2.4.m4.2.2.cmml"><msubsup id="S3.SS4.p2.4.m4.2.2.3" xref="S3.SS4.p2.4.m4.2.2.3.cmml"><mi id="S3.SS4.p2.4.m4.2.2.3.2.2" xref="S3.SS4.p2.4.m4.2.2.3.2.2.cmml">ùêà</mi><mi id="S3.SS4.p2.4.m4.2.2.3.3" xref="S3.SS4.p2.4.m4.2.2.3.3.cmml">i</mi><mi id="S3.SS4.p2.4.m4.2.2.3.2.3" xref="S3.SS4.p2.4.m4.2.2.3.2.3.cmml">r</mi></msubsup><mo id="S3.SS4.p2.4.m4.2.2.2" xref="S3.SS4.p2.4.m4.2.2.2.cmml">=</mo><mrow id="S3.SS4.p2.4.m4.2.2.1" xref="S3.SS4.p2.4.m4.2.2.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.4.m4.2.2.1.3" xref="S3.SS4.p2.4.m4.2.2.1.3.cmml">‚Ñõ</mi><mo id="S3.SS4.p2.4.m4.2.2.1.2" xref="S3.SS4.p2.4.m4.2.2.1.2.cmml">‚Å¢</mo><mrow id="S3.SS4.p2.4.m4.2.2.1.1.1" xref="S3.SS4.p2.4.m4.2.2.1.1.2.cmml"><mo id="S3.SS4.p2.4.m4.2.2.1.1.1.2" stretchy="false" xref="S3.SS4.p2.4.m4.2.2.1.1.2.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml">‚Ñ≥</mi><mo id="S3.SS4.p2.4.m4.2.2.1.1.1.3" xref="S3.SS4.p2.4.m4.2.2.1.1.2.cmml">,</mo><msub id="S3.SS4.p2.4.m4.2.2.1.1.1.1" xref="S3.SS4.p2.4.m4.2.2.1.1.1.1.cmml"><mi id="S3.SS4.p2.4.m4.2.2.1.1.1.1.2" xref="S3.SS4.p2.4.m4.2.2.1.1.1.1.2.cmml">ùùÉ</mi><mi id="S3.SS4.p2.4.m4.2.2.1.1.1.1.3" xref="S3.SS4.p2.4.m4.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS4.p2.4.m4.2.2.1.1.1.4" stretchy="false" xref="S3.SS4.p2.4.m4.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.2b"><apply id="S3.SS4.p2.4.m4.2.2.cmml" xref="S3.SS4.p2.4.m4.2.2"><eq id="S3.SS4.p2.4.m4.2.2.2.cmml" xref="S3.SS4.p2.4.m4.2.2.2"></eq><apply id="S3.SS4.p2.4.m4.2.2.3.cmml" xref="S3.SS4.p2.4.m4.2.2.3"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.2.2.3.1.cmml" xref="S3.SS4.p2.4.m4.2.2.3">subscript</csymbol><apply id="S3.SS4.p2.4.m4.2.2.3.2.cmml" xref="S3.SS4.p2.4.m4.2.2.3"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.2.2.3.2.1.cmml" xref="S3.SS4.p2.4.m4.2.2.3">superscript</csymbol><ci id="S3.SS4.p2.4.m4.2.2.3.2.2.cmml" xref="S3.SS4.p2.4.m4.2.2.3.2.2">ùêà</ci><ci id="S3.SS4.p2.4.m4.2.2.3.2.3.cmml" xref="S3.SS4.p2.4.m4.2.2.3.2.3">ùëü</ci></apply><ci id="S3.SS4.p2.4.m4.2.2.3.3.cmml" xref="S3.SS4.p2.4.m4.2.2.3.3">ùëñ</ci></apply><apply id="S3.SS4.p2.4.m4.2.2.1.cmml" xref="S3.SS4.p2.4.m4.2.2.1"><times id="S3.SS4.p2.4.m4.2.2.1.2.cmml" xref="S3.SS4.p2.4.m4.2.2.1.2"></times><ci id="S3.SS4.p2.4.m4.2.2.1.3.cmml" xref="S3.SS4.p2.4.m4.2.2.1.3">‚Ñõ</ci><interval closure="open" id="S3.SS4.p2.4.m4.2.2.1.1.2.cmml" xref="S3.SS4.p2.4.m4.2.2.1.1.1"><ci id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">‚Ñ≥</ci><apply id="S3.SS4.p2.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS4.p2.4.m4.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.2.2.1.1.1.1.1.cmml" xref="S3.SS4.p2.4.m4.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p2.4.m4.2.2.1.1.1.1.2.cmml" xref="S3.SS4.p2.4.m4.2.2.1.1.1.1.2">ùùÉ</ci><ci id="S3.SS4.p2.4.m4.2.2.1.1.1.1.3.cmml" xref="S3.SS4.p2.4.m4.2.2.1.1.1.1.3">ùëñ</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.2c">\mathbf{I}^{r}_{i}=\mathcal{R}(\mathcal{M},\boldsymbol{\xi}_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.4.m4.2d">bold_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = caligraphic_R ( caligraphic_M , bold_italic_Œæ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> and the input image <math alttext="\mathbf{I}_{i}" class="ltx_Math" display="inline" id="S3.SS4.p2.5.m5.1"><semantics id="S3.SS4.p2.5.m5.1a"><msub id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml"><mi id="S3.SS4.p2.5.m5.1.1.2" xref="S3.SS4.p2.5.m5.1.1.2.cmml">ùêà</mi><mi id="S3.SS4.p2.5.m5.1.1.3" xref="S3.SS4.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.1b"><apply id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p2.5.m5.1.1.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2">ùêà</ci><ci id="S3.SS4.p2.5.m5.1.1.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">\mathbf{I}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.5.m5.1d">bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to obtain the optimally fitted camera pose <math alttext="\boldsymbol{\xi}^{*}_{i}" class="ltx_Math" display="inline" id="S3.SS4.p2.6.m6.1"><semantics id="S3.SS4.p2.6.m6.1a"><msubsup id="S3.SS4.p2.6.m6.1.1" xref="S3.SS4.p2.6.m6.1.1.cmml"><mi id="S3.SS4.p2.6.m6.1.1.2.2" xref="S3.SS4.p2.6.m6.1.1.2.2.cmml">ùùÉ</mi><mi id="S3.SS4.p2.6.m6.1.1.3" xref="S3.SS4.p2.6.m6.1.1.3.cmml">i</mi><mo id="S3.SS4.p2.6.m6.1.1.2.3" xref="S3.SS4.p2.6.m6.1.1.2.3.cmml">‚àó</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m6.1b"><apply id="S3.SS4.p2.6.m6.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.6.m6.1.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1">subscript</csymbol><apply id="S3.SS4.p2.6.m6.1.1.2.cmml" xref="S3.SS4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.6.m6.1.1.2.1.cmml" xref="S3.SS4.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS4.p2.6.m6.1.1.2.2.cmml" xref="S3.SS4.p2.6.m6.1.1.2.2">ùùÉ</ci><times id="S3.SS4.p2.6.m6.1.1.2.3.cmml" xref="S3.SS4.p2.6.m6.1.1.2.3"></times></apply><ci id="S3.SS4.p2.6.m6.1.1.3.cmml" xref="S3.SS4.p2.6.m6.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m6.1c">\boldsymbol{\xi}^{*}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.6.m6.1d">bold_italic_Œæ start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. The optimization process can be formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\boldsymbol{\xi}^{*}_{i}=\arg\min_{\boldsymbol{\xi}_{i}\in\text{SE(3)}}\,(%
\lambda\cdot\mathcal{L}_{\text{mask}}(\mathbf{I}^{r}_{i},\mathbf{I}_{i})+\mu%
\cdot\mathcal{L}_{\text{rgb}}(\mathbf{I}^{r}_{i},\mathbf{I}_{i}))," class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml"><mi id="S3.E2.m1.1.1.1.1.4.2.2" xref="S3.E2.m1.1.1.1.1.4.2.2.cmml">ùùÉ</mi><mi id="S3.E2.m1.1.1.1.1.4.3" xref="S3.E2.m1.1.1.1.1.4.3.cmml">i</mi><mo id="S3.E2.m1.1.1.1.1.4.2.3" xref="S3.E2.m1.1.1.1.1.4.2.3.cmml">‚àó</mo></msubsup><mo id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">arg</mi><mo id="S3.E2.m1.1.1.1.1.2a" lspace="0.167em" xref="S3.E2.m1.1.1.1.1.2.cmml">‚Å°</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml"><munder id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml">min</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">ùùÉ</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml">‚àà</mo><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3a.cmml">SE(3)</mtext></mrow></munder><mo id="S3.E2.m1.1.1.1.1.2.2.2a" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">‚Å°</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml"><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.2" stretchy="false" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.1" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.cmml"><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.2.cmml">Œª</mi><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.1" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.1.cmml">‚ãÖ</mo><msub id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.2.cmml">‚Ñí</mi><mtext id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.3a.cmml">mask</mtext></msub></mrow><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.3.cmml">‚Å¢</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.3.cmml"><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.3.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.2.2.cmml">ùêà</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.3.cmml">i</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.2.3.cmml">r</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.3.cmml">,</mo><msub id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.2.cmml">ùêà</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.5" stretchy="false" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.5" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.5.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.cmml"><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.2.cmml">Œº</mi><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.1" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.1.cmml">‚ãÖ</mo><msub id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.2.cmml">‚Ñí</mi><mtext id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.3a.cmml">rgb</mtext></msub></mrow><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.3.cmml">‚Å¢</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.3.cmml"><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.3.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.2.2.cmml">ùêà</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.3.cmml">i</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.2.3.cmml">r</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.3.cmml">,</mo><msub id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.2.cmml">ùêà</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.5" stretchy="false" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"></eq><apply id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.1.4">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.2.1.cmml" xref="S3.E2.m1.1.1.1.1.4">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.4.2.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2">ùùÉ</ci><times id="S3.E2.m1.1.1.1.1.4.2.3.cmml" xref="S3.E2.m1.1.1.1.1.4.2.3"></times></apply><ci id="S3.E2.m1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.1.4.3">ùëñ</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><arg id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3"></arg><apply id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">subscript</csymbol><min id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2"></min><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3"><in id="S3.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.1"></in><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2">ùùÉ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.3">ùëñ</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3">SE(3)</mtext></ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1"><plus id="S3.E2.m1.1.1.1.1.2.2.2.2.1.5.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.5"></plus><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2"><times id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.3"></times><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4"><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.1">‚ãÖ</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.2">ùúÜ</ci><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.2">‚Ñí</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.3a.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.3"><mtext id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.4.3.3">mask</mtext></ci></apply></apply><interval closure="open" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2"><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.2.2">ùêà</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.2.3">ùëü</ci></apply><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.3">ùëñ</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.2">ùêà</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.2.2.2.2.3">ùëñ</ci></apply></interval></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4"><times id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.3"></times><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4"><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.1">‚ãÖ</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.2">ùúá</ci><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.2">‚Ñí</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.3a.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.3"><mtext id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.4.3.3">rgb</mtext></ci></apply></apply><interval closure="open" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2"><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.2.2">ùêà</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.2.3">ùëü</ci></apply><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.3.1.1.1.3">ùëñ</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.2">ùêà</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.4.2.2.2.3">ùëñ</ci></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\boldsymbol{\xi}^{*}_{i}=\arg\min_{\boldsymbol{\xi}_{i}\in\text{SE(3)}}\,(%
\lambda\cdot\mathcal{L}_{\text{mask}}(\mathbf{I}^{r}_{i},\mathbf{I}_{i})+\mu%
\cdot\mathcal{L}_{\text{rgb}}(\mathbf{I}^{r}_{i},\mathbf{I}_{i})),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">bold_italic_Œæ start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT bold_italic_Œæ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚àà SE(3) end_POSTSUBSCRIPT ( italic_Œª ‚ãÖ caligraphic_L start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT ( bold_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_Œº ‚ãÖ caligraphic_L start_POSTSUBSCRIPT rgb end_POSTSUBSCRIPT ( bold_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.4">where <math alttext="\mathcal{L}_{\text{mask}}" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">‚Ñí</mi><mtext id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3a.cmml">mask</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">‚Ñí</ci><ci id="S3.SS4.p3.1.m1.1.1.3a.cmml" xref="S3.SS4.p3.1.m1.1.1.3"><mtext id="S3.SS4.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS4.p3.1.m1.1.1.3">mask</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\mathcal{L}_{\text{mask}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{\text{rgb}}" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><msub id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">‚Ñí</mi><mtext id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3a.cmml">rgb</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">‚Ñí</ci><ci id="S3.SS4.p3.2.m2.1.1.3a.cmml" xref="S3.SS4.p3.2.m2.1.1.3"><mtext id="S3.SS4.p3.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS4.p3.2.m2.1.1.3">rgb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">\mathcal{L}_{\text{rgb}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT rgb end_POSTSUBSCRIPT</annotation></semantics></math> are the cross-entropy and MSE losses computed for the foreground masks and the RGB values, respectively, and <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS4.p3.3.m3.1"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.3.m3.1d">italic_Œª</annotation></semantics></math> and <math alttext="\mu" class="ltx_Math" display="inline" id="S3.SS4.p3.4.m4.1"><semantics id="S3.SS4.p3.4.m4.1a"><mi id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">Œº</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><ci id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">ùúá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.4.m4.1d">italic_Œº</annotation></semantics></math> are two weighting coefficients. The refinement process is lightweight and can be completed in just one second, given the generated mesh.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.1.1">Mixture of Experts (MoE).</span> The NOCS pose predictions are inherently stochastic and may not produce an accurate pose in a single pass. For instance, with objects possessing certain symmetries, the diffusion model may predict only one of the possible symmetric poses. We employ a Mixture of Experts (MoE) strategy to further refine the pose, which is simple but effective. Specifically, we generate multiple NOCS maps for each input view using different seeds. We then select the pose that minimizes the rendering loss based on the refinement results with the generated 3D mesh. This technique effectively reduces pose estimation error, as quantitatively validated by the ablation study in the Appendix.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Settings</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.44.2.1" style="font-size:129%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.2.1" style="font-size:129%;">
<span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1">Evaluation Results for Pose Estimation.</span> We compare our method with RelPose++¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib29" title="">29</a>]</cite>, FORGE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib17" title="">17</a>]</cite>, and iFusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib83" title="">83</a>]</cite> on three unseen datasets: OmniObject3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib85" title="">85</a>]</cite>, GSO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite>, and ABO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib7" title="">7</a>]</cite>. <math alttext="500" class="ltx_Math" display="inline" id="S4.T1.2.1.m1.1"><semantics id="S4.T1.2.1.m1.1b"><mn id="S4.T1.2.1.m1.1.1" xref="S4.T1.2.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S4.T1.2.1.m1.1c"><cn id="S4.T1.2.1.m1.1.1.cmml" type="integer" xref="S4.T1.2.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.1.m1.1d">500</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.1.m1.1e">500</annotation></semantics></math> objects are sampled for each dataset.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.19">
<tr class="ltx_tr" id="S4.T1.7.5">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.7.5.6"><span class="ltx_text" id="S4.T1.7.5.6.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.7.5.7"><span class="ltx_text" id="S4.T1.7.5.7.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.3.1.1">
<span class="ltx_text" id="S4.T1.3.1.1.1" style="font-size:70%;">Rot. Err</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.3.1.1.m1.1"><semantics id="S4.T1.3.1.1.m1.1a"><mo id="S4.T1.3.1.1.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T1.3.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.1.m1.1b"><ci id="S4.T1.3.1.1.m1.1.1.cmml" xref="S4.T1.3.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.1.1.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.4.2.2">
<span class="ltx_text" id="S4.T1.4.2.2.1" style="font-size:70%;">Acc.@15</span><math alttext="{}^{\circ}\uparrow" class="ltx_math_unparsed" display="inline" id="S4.T1.4.2.2.m1.1"><semantics id="S4.T1.4.2.2.m1.1a"><mmultiscripts id="S4.T1.4.2.2.m1.1.1"><mo id="S4.T1.4.2.2.m1.1.1.2" mathsize="70%" stretchy="false">‚Üë</mo><mprescripts id="S4.T1.4.2.2.m1.1.1a"></mprescripts><mrow id="S4.T1.4.2.2.m1.1.1b"></mrow><mo id="S4.T1.4.2.2.m1.1.1.3" mathsize="70%">‚àò</mo></mmultiscripts><annotation encoding="application/x-tex" id="S4.T1.4.2.2.m1.1b">{}^{\circ}\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.2.2.m1.1c">start_FLOATSUPERSCRIPT ‚àò end_FLOATSUPERSCRIPT ‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.3.3">
<span class="ltx_text" id="S4.T1.5.3.3.1" style="font-size:70%;">Acc.@30</span><math alttext="{}^{\circ}\uparrow" class="ltx_math_unparsed" display="inline" id="S4.T1.5.3.3.m1.1"><semantics id="S4.T1.5.3.3.m1.1a"><mmultiscripts id="S4.T1.5.3.3.m1.1.1"><mo id="S4.T1.5.3.3.m1.1.1.2" mathsize="70%" stretchy="false">‚Üë</mo><mprescripts id="S4.T1.5.3.3.m1.1.1a"></mprescripts><mrow id="S4.T1.5.3.3.m1.1.1b"></mrow><mo id="S4.T1.5.3.3.m1.1.1.3" mathsize="70%">‚àò</mo></mmultiscripts><annotation encoding="application/x-tex" id="S4.T1.5.3.3.m1.1b">{}^{\circ}\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.3.3.m1.1c">start_FLOATSUPERSCRIPT ‚àò end_FLOATSUPERSCRIPT ‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.6.4.4">
<span class="ltx_text" id="S4.T1.6.4.4.1" style="font-size:70%;">Trans. Err</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.6.4.4.m1.1"><semantics id="S4.T1.6.4.4.m1.1a"><mo id="S4.T1.6.4.4.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T1.6.4.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.4.4.m1.1b"><ci id="S4.T1.6.4.4.m1.1.1.cmml" xref="S4.T1.6.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.4.4.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.7.5.5">
<span class="ltx_text" id="S4.T1.7.5.5.1" style="font-size:70%;">Time</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.7.5.5.m1.1"><semantics id="S4.T1.7.5.5.m1.1a"><mo id="S4.T1.7.5.5.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T1.7.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.5.5.m1.1b"><ci id="S4.T1.7.5.5.m1.1.1.cmml" xref="S4.T1.7.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.5.5.m1.1d">‚Üì</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.18">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.19.18.1" rowspan="7"><span class="ltx_text" id="S4.T1.19.18.1.1" style="font-size:70%;">GSO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.19.18.2"><span class="ltx_text" id="S4.T1.19.18.2.1" style="font-size:70%;">RelPose++</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.18.3"><span class="ltx_text" id="S4.T1.19.18.3.1" style="font-size:70%;">103.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.18.4"><span class="ltx_text" id="S4.T1.19.18.4.1" style="font-size:70%;">0.011</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.18.5"><span class="ltx_text" id="S4.T1.19.18.5.1" style="font-size:70%;">0.033</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.18.6"><span class="ltx_text" id="S4.T1.19.18.6.1" style="font-size:70%;">4.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.18.7"><span class="ltx_text" id="S4.T1.19.18.7.1" style="font-size:70%;">3.6s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.19">
<td class="ltx_td ltx_align_left" id="S4.T1.19.19.1"><span class="ltx_text" id="S4.T1.19.19.1.1" style="font-size:70%;">FORGE</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.19.2"><span class="ltx_text" id="S4.T1.19.19.2.1" style="font-size:70%;">111.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.19.3"><span class="ltx_text" id="S4.T1.19.19.3.1" style="font-size:70%;">0.004</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.19.4"><span class="ltx_text" id="S4.T1.19.19.4.1" style="font-size:70%;">0.020</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.19.5"><span class="ltx_text" id="S4.T1.19.19.5.1" style="font-size:70%;">4.21</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.19.6"><span class="ltx_text" id="S4.T1.19.19.6.1" style="font-size:70%;">440s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.6">
<td class="ltx_td ltx_align_left" id="S4.T1.8.6.1">
<span class="ltx_text" id="S4.T1.8.6.1.1" style="font-size:70%;">iFusion (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.8.6.1.m1.1"><semantics id="S4.T1.8.6.1.m1.1a"><msub id="S4.T1.8.6.1.m1.1.1" xref="S4.T1.8.6.1.m1.1.1.cmml"><mi id="S4.T1.8.6.1.m1.1.1.2" mathsize="70%" xref="S4.T1.8.6.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.8.6.1.m1.1.1.3" xref="S4.T1.8.6.1.m1.1.1.3.cmml"><mi id="S4.T1.8.6.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.8.6.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.8.6.1.m1.1.1.3.1" xref="S4.T1.8.6.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.8.6.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.8.6.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.8.6.1.m1.1.1.3.1a" xref="S4.T1.8.6.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.8.6.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.8.6.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.8.6.1.m1.1.1.3.1b" xref="S4.T1.8.6.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.8.6.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.8.6.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.8.6.1.m1.1b"><apply id="S4.T1.8.6.1.m1.1.1.cmml" xref="S4.T1.8.6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.8.6.1.m1.1.1.1.cmml" xref="S4.T1.8.6.1.m1.1.1">subscript</csymbol><ci id="S4.T1.8.6.1.m1.1.1.2.cmml" xref="S4.T1.8.6.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.8.6.1.m1.1.1.3.cmml" xref="S4.T1.8.6.1.m1.1.1.3"><times id="S4.T1.8.6.1.m1.1.1.3.1.cmml" xref="S4.T1.8.6.1.m1.1.1.3.1"></times><ci id="S4.T1.8.6.1.m1.1.1.3.2.cmml" xref="S4.T1.8.6.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.8.6.1.m1.1.1.3.3.cmml" xref="S4.T1.8.6.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.8.6.1.m1.1.1.3.4.cmml" xref="S4.T1.8.6.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.8.6.1.m1.1.1.3.5.cmml" xref="S4.T1.8.6.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.6.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.6.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.8.6.1.2" style="font-size:70%;">=1)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.2"><span class="ltx_text" id="S4.T1.8.6.2.1" style="font-size:70%;">95.15</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.3"><span class="ltx_text" id="S4.T1.8.6.3.1" style="font-size:70%;">0.208</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.4"><span class="ltx_text" id="S4.T1.8.6.4.1" style="font-size:70%;">0.258</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.5"><span class="ltx_text" id="S4.T1.8.6.5.1" style="font-size:70%;">3.65</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.6"><span class="ltx_text" id="S4.T1.8.6.6.1" style="font-size:70%;">64s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.9.7">
<td class="ltx_td ltx_align_left" id="S4.T1.9.7.1">
<span class="ltx_text" id="S4.T1.9.7.1.1" style="font-size:70%;">iFusion (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.9.7.1.m1.1"><semantics id="S4.T1.9.7.1.m1.1a"><msub id="S4.T1.9.7.1.m1.1.1" xref="S4.T1.9.7.1.m1.1.1.cmml"><mi id="S4.T1.9.7.1.m1.1.1.2" mathsize="70%" xref="S4.T1.9.7.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.9.7.1.m1.1.1.3" xref="S4.T1.9.7.1.m1.1.1.3.cmml"><mi id="S4.T1.9.7.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.9.7.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.9.7.1.m1.1.1.3.1" xref="S4.T1.9.7.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.9.7.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.9.7.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.9.7.1.m1.1.1.3.1a" xref="S4.T1.9.7.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.9.7.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.9.7.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.9.7.1.m1.1.1.3.1b" xref="S4.T1.9.7.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.9.7.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.9.7.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.9.7.1.m1.1b"><apply id="S4.T1.9.7.1.m1.1.1.cmml" xref="S4.T1.9.7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.9.7.1.m1.1.1.1.cmml" xref="S4.T1.9.7.1.m1.1.1">subscript</csymbol><ci id="S4.T1.9.7.1.m1.1.1.2.cmml" xref="S4.T1.9.7.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.9.7.1.m1.1.1.3.cmml" xref="S4.T1.9.7.1.m1.1.1.3"><times id="S4.T1.9.7.1.m1.1.1.3.1.cmml" xref="S4.T1.9.7.1.m1.1.1.3.1"></times><ci id="S4.T1.9.7.1.m1.1.1.3.2.cmml" xref="S4.T1.9.7.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.9.7.1.m1.1.1.3.3.cmml" xref="S4.T1.9.7.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.9.7.1.m1.1.1.3.4.cmml" xref="S4.T1.9.7.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.9.7.1.m1.1.1.3.5.cmml" xref="S4.T1.9.7.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.7.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.7.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.9.7.1.2" style="font-size:70%;">=4)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.2"><span class="ltx_text" id="S4.T1.9.7.2.1" style="font-size:70%;">8.61</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.3"><span class="ltx_text" id="S4.T1.9.7.3.1" style="font-size:70%;">0.651</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.4"><span class="ltx_text" id="S4.T1.9.7.4.1" style="font-size:70%;">0.759</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.5"><span class="ltx_text" id="S4.T1.9.7.5.1" style="font-size:70%;">0.49</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.6"><span class="ltx_text" id="S4.T1.9.7.6.1" style="font-size:70%;">256s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.20">
<td class="ltx_td ltx_align_left" id="S4.T1.19.20.1"><span class="ltx_text" id="S4.T1.19.20.1.1" style="font-size:70%;">Ours (w/o refine)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.20.2"><span class="ltx_text" id="S4.T1.19.20.2.1" style="font-size:70%;">13.02</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.20.3"><span class="ltx_text" id="S4.T1.19.20.3.1" style="font-size:70%;">0.537</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.20.4"><span class="ltx_text" id="S4.T1.19.20.4.1" style="font-size:70%;">0.616</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.20.5"><span class="ltx_text" id="S4.T1.19.20.5.1" style="font-size:70%;">0.58</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.20.6"><span class="ltx_text" id="S4.T1.19.20.6.1" style="font-size:70%;">10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.8">
<td class="ltx_td ltx_align_left" id="S4.T1.10.8.1">
<span class="ltx_text" id="S4.T1.10.8.1.1" style="font-size:70%;">Ours (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.10.8.1.m1.1"><semantics id="S4.T1.10.8.1.m1.1a"><msub id="S4.T1.10.8.1.m1.1.1" xref="S4.T1.10.8.1.m1.1.1.cmml"><mi id="S4.T1.10.8.1.m1.1.1.2" mathsize="70%" xref="S4.T1.10.8.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.10.8.1.m1.1.1.3" xref="S4.T1.10.8.1.m1.1.1.3.cmml"><mi id="S4.T1.10.8.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.10.8.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.10.8.1.m1.1.1.3.1" xref="S4.T1.10.8.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.10.8.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.10.8.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.10.8.1.m1.1.1.3.1a" xref="S4.T1.10.8.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.10.8.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.10.8.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.10.8.1.m1.1.1.3.1b" xref="S4.T1.10.8.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.10.8.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.10.8.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.10.8.1.m1.1b"><apply id="S4.T1.10.8.1.m1.1.1.cmml" xref="S4.T1.10.8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.10.8.1.m1.1.1.1.cmml" xref="S4.T1.10.8.1.m1.1.1">subscript</csymbol><ci id="S4.T1.10.8.1.m1.1.1.2.cmml" xref="S4.T1.10.8.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.10.8.1.m1.1.1.3.cmml" xref="S4.T1.10.8.1.m1.1.1.3"><times id="S4.T1.10.8.1.m1.1.1.3.1.cmml" xref="S4.T1.10.8.1.m1.1.1.3.1"></times><ci id="S4.T1.10.8.1.m1.1.1.3.2.cmml" xref="S4.T1.10.8.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.10.8.1.m1.1.1.3.3.cmml" xref="S4.T1.10.8.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.10.8.1.m1.1.1.3.4.cmml" xref="S4.T1.10.8.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.10.8.1.m1.1.1.3.5.cmml" xref="S4.T1.10.8.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.8.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.8.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.10.8.1.2" style="font-size:70%;">=1)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.2"><span class="ltx_text" id="S4.T1.10.8.2.1" style="font-size:70%;">9.87</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.3"><span class="ltx_text" id="S4.T1.10.8.3.1" style="font-size:70%;">0.563</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.4"><span class="ltx_text" id="S4.T1.10.8.4.1" style="font-size:70%;">0.617</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.5"><span class="ltx_text" id="S4.T1.10.8.5.1" style="font-size:70%;">0.42</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.6"><span class="ltx_text" id="S4.T1.10.8.6.1" style="font-size:70%;">27s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.11.9">
<td class="ltx_td ltx_align_left" id="S4.T1.11.9.1">
<span class="ltx_text" id="S4.T1.11.9.1.1" style="font-size:70%;">Ours (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.11.9.1.m1.1"><semantics id="S4.T1.11.9.1.m1.1a"><msub id="S4.T1.11.9.1.m1.1.1" xref="S4.T1.11.9.1.m1.1.1.cmml"><mi id="S4.T1.11.9.1.m1.1.1.2" mathsize="70%" xref="S4.T1.11.9.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.11.9.1.m1.1.1.3" xref="S4.T1.11.9.1.m1.1.1.3.cmml"><mi id="S4.T1.11.9.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.11.9.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.11.9.1.m1.1.1.3.1" xref="S4.T1.11.9.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.11.9.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.11.9.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.11.9.1.m1.1.1.3.1a" xref="S4.T1.11.9.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.11.9.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.11.9.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.11.9.1.m1.1.1.3.1b" xref="S4.T1.11.9.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.11.9.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.11.9.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.11.9.1.m1.1b"><apply id="S4.T1.11.9.1.m1.1.1.cmml" xref="S4.T1.11.9.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.11.9.1.m1.1.1.1.cmml" xref="S4.T1.11.9.1.m1.1.1">subscript</csymbol><ci id="S4.T1.11.9.1.m1.1.1.2.cmml" xref="S4.T1.11.9.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.11.9.1.m1.1.1.3.cmml" xref="S4.T1.11.9.1.m1.1.1.3"><times id="S4.T1.11.9.1.m1.1.1.3.1.cmml" xref="S4.T1.11.9.1.m1.1.1.3.1"></times><ci id="S4.T1.11.9.1.m1.1.1.3.2.cmml" xref="S4.T1.11.9.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.11.9.1.m1.1.1.3.3.cmml" xref="S4.T1.11.9.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.11.9.1.m1.1.1.3.4.cmml" xref="S4.T1.11.9.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.11.9.1.m1.1.1.3.5.cmml" xref="S4.T1.11.9.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.9.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.9.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.11.9.1.2" style="font-size:70%;">=4)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.11.9.2"><span class="ltx_text ltx_font_bold" id="S4.T1.11.9.2.1" style="font-size:70%;">5.28</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.11.9.3"><span class="ltx_text ltx_font_bold" id="S4.T1.11.9.3.1" style="font-size:70%;">0.750</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.11.9.4"><span class="ltx_text ltx_font_bold" id="S4.T1.11.9.4.1" style="font-size:70%;">0.787</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.11.9.5"><span class="ltx_text ltx_font_bold" id="S4.T1.11.9.5.1" style="font-size:70%;">0.23</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.11.9.6"><span class="ltx_text" id="S4.T1.11.9.6.1" style="font-size:70%;">57s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.21">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.19.21.1" rowspan="7"><span class="ltx_text" id="S4.T1.19.21.1.1" style="font-size:70%;">OO3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib85" title="">85</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.19.21.2"><span class="ltx_text" id="S4.T1.19.21.2.1" style="font-size:70%;">RelPose++</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.21.3"><span class="ltx_text" id="S4.T1.19.21.3.1" style="font-size:70%;">105.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.21.4"><span class="ltx_text" id="S4.T1.19.21.4.1" style="font-size:70%;">0.008</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.21.5"><span class="ltx_text" id="S4.T1.19.21.5.1" style="font-size:70%;">0.046</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.21.6"><span class="ltx_text" id="S4.T1.19.21.6.1" style="font-size:70%;">7.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.21.7"><span class="ltx_text" id="S4.T1.19.21.7.1" style="font-size:70%;">3.6s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.22">
<td class="ltx_td ltx_align_left" id="S4.T1.19.22.1"><span class="ltx_text" id="S4.T1.19.22.1.1" style="font-size:70%;">FORGE</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.22.2"><span class="ltx_text" id="S4.T1.19.22.2.1" style="font-size:70%;">99.27</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.22.3"><span class="ltx_text" id="S4.T1.19.22.3.1" style="font-size:70%;">0.014</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.22.4"><span class="ltx_text" id="S4.T1.19.22.4.1" style="font-size:70%;">0.063</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.22.5"><span class="ltx_text" id="S4.T1.19.22.5.1" style="font-size:70%;">7.27</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.22.6"><span class="ltx_text" id="S4.T1.19.22.6.1" style="font-size:70%;">440s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.12.10">
<td class="ltx_td ltx_align_left" id="S4.T1.12.10.1">
<span class="ltx_text" id="S4.T1.12.10.1.1" style="font-size:70%;">iFusion (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.12.10.1.m1.1"><semantics id="S4.T1.12.10.1.m1.1a"><msub id="S4.T1.12.10.1.m1.1.1" xref="S4.T1.12.10.1.m1.1.1.cmml"><mi id="S4.T1.12.10.1.m1.1.1.2" mathsize="70%" xref="S4.T1.12.10.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.12.10.1.m1.1.1.3" xref="S4.T1.12.10.1.m1.1.1.3.cmml"><mi id="S4.T1.12.10.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.12.10.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.12.10.1.m1.1.1.3.1" xref="S4.T1.12.10.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.12.10.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.12.10.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.12.10.1.m1.1.1.3.1a" xref="S4.T1.12.10.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.12.10.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.12.10.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.12.10.1.m1.1.1.3.1b" xref="S4.T1.12.10.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.12.10.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.12.10.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.12.10.1.m1.1b"><apply id="S4.T1.12.10.1.m1.1.1.cmml" xref="S4.T1.12.10.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.12.10.1.m1.1.1.1.cmml" xref="S4.T1.12.10.1.m1.1.1">subscript</csymbol><ci id="S4.T1.12.10.1.m1.1.1.2.cmml" xref="S4.T1.12.10.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.12.10.1.m1.1.1.3.cmml" xref="S4.T1.12.10.1.m1.1.1.3"><times id="S4.T1.12.10.1.m1.1.1.3.1.cmml" xref="S4.T1.12.10.1.m1.1.1.3.1"></times><ci id="S4.T1.12.10.1.m1.1.1.3.2.cmml" xref="S4.T1.12.10.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.12.10.1.m1.1.1.3.3.cmml" xref="S4.T1.12.10.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.12.10.1.m1.1.1.3.4.cmml" xref="S4.T1.12.10.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.12.10.1.m1.1.1.3.5.cmml" xref="S4.T1.12.10.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.10.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.10.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.12.10.1.2" style="font-size:70%;">=1)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.2"><span class="ltx_text" id="S4.T1.12.10.2.1" style="font-size:70%;">91.15</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.3"><span class="ltx_text" id="S4.T1.12.10.3.1" style="font-size:70%;">0.166</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.4"><span class="ltx_text" id="S4.T1.12.10.4.1" style="font-size:70%;">0.271</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.5"><span class="ltx_text" id="S4.T1.12.10.5.1" style="font-size:70%;">4.77</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.6"><span class="ltx_text" id="S4.T1.12.10.6.1" style="font-size:70%;">64s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.13.11">
<td class="ltx_td ltx_align_left" id="S4.T1.13.11.1">
<span class="ltx_text" id="S4.T1.13.11.1.1" style="font-size:70%;">iFusion (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.13.11.1.m1.1"><semantics id="S4.T1.13.11.1.m1.1a"><msub id="S4.T1.13.11.1.m1.1.1" xref="S4.T1.13.11.1.m1.1.1.cmml"><mi id="S4.T1.13.11.1.m1.1.1.2" mathsize="70%" xref="S4.T1.13.11.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.13.11.1.m1.1.1.3" xref="S4.T1.13.11.1.m1.1.1.3.cmml"><mi id="S4.T1.13.11.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.13.11.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.13.11.1.m1.1.1.3.1" xref="S4.T1.13.11.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.13.11.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.13.11.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.13.11.1.m1.1.1.3.1a" xref="S4.T1.13.11.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.13.11.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.13.11.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.13.11.1.m1.1.1.3.1b" xref="S4.T1.13.11.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.13.11.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.13.11.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.13.11.1.m1.1b"><apply id="S4.T1.13.11.1.m1.1.1.cmml" xref="S4.T1.13.11.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.13.11.1.m1.1.1.1.cmml" xref="S4.T1.13.11.1.m1.1.1">subscript</csymbol><ci id="S4.T1.13.11.1.m1.1.1.2.cmml" xref="S4.T1.13.11.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.13.11.1.m1.1.1.3.cmml" xref="S4.T1.13.11.1.m1.1.1.3"><times id="S4.T1.13.11.1.m1.1.1.3.1.cmml" xref="S4.T1.13.11.1.m1.1.1.3.1"></times><ci id="S4.T1.13.11.1.m1.1.1.3.2.cmml" xref="S4.T1.13.11.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.13.11.1.m1.1.1.3.3.cmml" xref="S4.T1.13.11.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.13.11.1.m1.1.1.3.4.cmml" xref="S4.T1.13.11.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.13.11.1.m1.1.1.3.5.cmml" xref="S4.T1.13.11.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.11.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.13.11.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.13.11.1.2" style="font-size:70%;">=4)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.2"><span class="ltx_text" id="S4.T1.13.11.2.1" style="font-size:70%;">15.08</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.3"><span class="ltx_text" id="S4.T1.13.11.3.1" style="font-size:70%;">0.498</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.4"><span class="ltx_text" id="S4.T1.13.11.4.1" style="font-size:70%;">0.721</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.5"><span class="ltx_text" id="S4.T1.13.11.5.1" style="font-size:70%;">1.12</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.6"><span class="ltx_text" id="S4.T1.13.11.6.1" style="font-size:70%;">256s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.23">
<td class="ltx_td ltx_align_left" id="S4.T1.19.23.1"><span class="ltx_text" id="S4.T1.19.23.1.1" style="font-size:70%;">Ours (w/o refine)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.23.2"><span class="ltx_text" id="S4.T1.19.23.2.1" style="font-size:70%;">14.75</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.23.3"><span class="ltx_text" id="S4.T1.19.23.3.1" style="font-size:70%;">0.508</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.23.4"><span class="ltx_text" id="S4.T1.19.23.4.1" style="font-size:70%;">0.725</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.23.5"><span class="ltx_text" id="S4.T1.19.23.5.1" style="font-size:70%;">0.90</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.23.6"><span class="ltx_text" id="S4.T1.19.23.6.1" style="font-size:70%;">10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.12">
<td class="ltx_td ltx_align_left" id="S4.T1.14.12.1">
<span class="ltx_text" id="S4.T1.14.12.1.1" style="font-size:70%;">Ours (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.14.12.1.m1.1"><semantics id="S4.T1.14.12.1.m1.1a"><msub id="S4.T1.14.12.1.m1.1.1" xref="S4.T1.14.12.1.m1.1.1.cmml"><mi id="S4.T1.14.12.1.m1.1.1.2" mathsize="70%" xref="S4.T1.14.12.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.14.12.1.m1.1.1.3" xref="S4.T1.14.12.1.m1.1.1.3.cmml"><mi id="S4.T1.14.12.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.14.12.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.14.12.1.m1.1.1.3.1" xref="S4.T1.14.12.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.14.12.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.14.12.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.14.12.1.m1.1.1.3.1a" xref="S4.T1.14.12.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.14.12.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.14.12.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.14.12.1.m1.1.1.3.1b" xref="S4.T1.14.12.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.14.12.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.14.12.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.14.12.1.m1.1b"><apply id="S4.T1.14.12.1.m1.1.1.cmml" xref="S4.T1.14.12.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.14.12.1.m1.1.1.1.cmml" xref="S4.T1.14.12.1.m1.1.1">subscript</csymbol><ci id="S4.T1.14.12.1.m1.1.1.2.cmml" xref="S4.T1.14.12.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.14.12.1.m1.1.1.3.cmml" xref="S4.T1.14.12.1.m1.1.1.3"><times id="S4.T1.14.12.1.m1.1.1.3.1.cmml" xref="S4.T1.14.12.1.m1.1.1.3.1"></times><ci id="S4.T1.14.12.1.m1.1.1.3.2.cmml" xref="S4.T1.14.12.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.14.12.1.m1.1.1.3.3.cmml" xref="S4.T1.14.12.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.14.12.1.m1.1.1.3.4.cmml" xref="S4.T1.14.12.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.14.12.1.m1.1.1.3.5.cmml" xref="S4.T1.14.12.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.12.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.14.12.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.14.12.1.2" style="font-size:70%;">=1)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.2"><span class="ltx_text" id="S4.T1.14.12.2.1" style="font-size:70%;">13.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.3"><span class="ltx_text" id="S4.T1.14.12.3.1" style="font-size:70%;">0.544</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.4"><span class="ltx_text" id="S4.T1.14.12.4.1" style="font-size:70%;">0.730</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.5"><span class="ltx_text" id="S4.T1.14.12.5.1" style="font-size:70%;">0.89</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.6"><span class="ltx_text" id="S4.T1.14.12.6.1" style="font-size:70%;">27s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.15.13">
<td class="ltx_td ltx_align_left" id="S4.T1.15.13.1">
<span class="ltx_text" id="S4.T1.15.13.1.1" style="font-size:70%;">Ours (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.15.13.1.m1.1"><semantics id="S4.T1.15.13.1.m1.1a"><msub id="S4.T1.15.13.1.m1.1.1" xref="S4.T1.15.13.1.m1.1.1.cmml"><mi id="S4.T1.15.13.1.m1.1.1.2" mathsize="70%" xref="S4.T1.15.13.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.15.13.1.m1.1.1.3" xref="S4.T1.15.13.1.m1.1.1.3.cmml"><mi id="S4.T1.15.13.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.15.13.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.15.13.1.m1.1.1.3.1" xref="S4.T1.15.13.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.15.13.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.15.13.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.15.13.1.m1.1.1.3.1a" xref="S4.T1.15.13.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.15.13.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.15.13.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.15.13.1.m1.1.1.3.1b" xref="S4.T1.15.13.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.15.13.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.15.13.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.15.13.1.m1.1b"><apply id="S4.T1.15.13.1.m1.1.1.cmml" xref="S4.T1.15.13.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.15.13.1.m1.1.1.1.cmml" xref="S4.T1.15.13.1.m1.1.1">subscript</csymbol><ci id="S4.T1.15.13.1.m1.1.1.2.cmml" xref="S4.T1.15.13.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.15.13.1.m1.1.1.3.cmml" xref="S4.T1.15.13.1.m1.1.1.3"><times id="S4.T1.15.13.1.m1.1.1.3.1.cmml" xref="S4.T1.15.13.1.m1.1.1.3.1"></times><ci id="S4.T1.15.13.1.m1.1.1.3.2.cmml" xref="S4.T1.15.13.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.15.13.1.m1.1.1.3.3.cmml" xref="S4.T1.15.13.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.15.13.1.m1.1.1.3.4.cmml" xref="S4.T1.15.13.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.15.13.1.m1.1.1.3.5.cmml" xref="S4.T1.15.13.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.13.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.15.13.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.15.13.1.2" style="font-size:70%;">=4)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.2"><span class="ltx_text ltx_font_bold" id="S4.T1.15.13.2.1" style="font-size:70%;">10.07</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.3"><span class="ltx_text ltx_font_bold" id="S4.T1.15.13.3.1" style="font-size:70%;">0.668</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.4"><span class="ltx_text ltx_font_bold" id="S4.T1.15.13.4.1" style="font-size:70%;">0.849</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.5"><span class="ltx_text ltx_font_bold" id="S4.T1.15.13.5.1" style="font-size:70%;">0.63</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.6"><span class="ltx_text" id="S4.T1.15.13.6.1" style="font-size:70%;">57s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.24">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.19.24.1" rowspan="7"><span class="ltx_text" id="S4.T1.19.24.1.1" style="font-size:70%;">ABO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib7" title="">7</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.19.24.2"><span class="ltx_text" id="S4.T1.19.24.2.1" style="font-size:70%;">RelPose++</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.24.3"><span class="ltx_text" id="S4.T1.19.24.3.1" style="font-size:70%;">103.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.24.4"><span class="ltx_text" id="S4.T1.19.24.4.1" style="font-size:70%;">0.017</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.24.5"><span class="ltx_text" id="S4.T1.19.24.5.1" style="font-size:70%;">0.039</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.24.6"><span class="ltx_text" id="S4.T1.19.24.6.1" style="font-size:70%;">5.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.19.24.7"><span class="ltx_text" id="S4.T1.19.24.7.1" style="font-size:70%;">3.6s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.25">
<td class="ltx_td ltx_align_left" id="S4.T1.19.25.1"><span class="ltx_text" id="S4.T1.19.25.1.1" style="font-size:70%;">FORGE</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.25.2"><span class="ltx_text" id="S4.T1.19.25.2.1" style="font-size:70%;">110.64</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.25.3"><span class="ltx_text" id="S4.T1.19.25.3.1" style="font-size:70%;">0.005</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.25.4"><span class="ltx_text" id="S4.T1.19.25.4.1" style="font-size:70%;">0.023</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.25.5"><span class="ltx_text" id="S4.T1.19.25.5.1" style="font-size:70%;">4.18</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.25.6"><span class="ltx_text" id="S4.T1.19.25.6.1" style="font-size:70%;">440s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.16.14">
<td class="ltx_td ltx_align_left" id="S4.T1.16.14.1">
<span class="ltx_text" id="S4.T1.16.14.1.1" style="font-size:70%;">iFusion (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.16.14.1.m1.1"><semantics id="S4.T1.16.14.1.m1.1a"><msub id="S4.T1.16.14.1.m1.1.1" xref="S4.T1.16.14.1.m1.1.1.cmml"><mi id="S4.T1.16.14.1.m1.1.1.2" mathsize="70%" xref="S4.T1.16.14.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.16.14.1.m1.1.1.3" xref="S4.T1.16.14.1.m1.1.1.3.cmml"><mi id="S4.T1.16.14.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.16.14.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.16.14.1.m1.1.1.3.1" xref="S4.T1.16.14.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.16.14.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.16.14.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.16.14.1.m1.1.1.3.1a" xref="S4.T1.16.14.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.16.14.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.16.14.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.16.14.1.m1.1.1.3.1b" xref="S4.T1.16.14.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.16.14.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.16.14.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.16.14.1.m1.1b"><apply id="S4.T1.16.14.1.m1.1.1.cmml" xref="S4.T1.16.14.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.16.14.1.m1.1.1.1.cmml" xref="S4.T1.16.14.1.m1.1.1">subscript</csymbol><ci id="S4.T1.16.14.1.m1.1.1.2.cmml" xref="S4.T1.16.14.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.16.14.1.m1.1.1.3.cmml" xref="S4.T1.16.14.1.m1.1.1.3"><times id="S4.T1.16.14.1.m1.1.1.3.1.cmml" xref="S4.T1.16.14.1.m1.1.1.3.1"></times><ci id="S4.T1.16.14.1.m1.1.1.3.2.cmml" xref="S4.T1.16.14.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.16.14.1.m1.1.1.3.3.cmml" xref="S4.T1.16.14.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.16.14.1.m1.1.1.3.4.cmml" xref="S4.T1.16.14.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.16.14.1.m1.1.1.3.5.cmml" xref="S4.T1.16.14.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.14.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.16.14.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.16.14.1.2" style="font-size:70%;">=1)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.16.14.2"><span class="ltx_text" id="S4.T1.16.14.2.1" style="font-size:70%;">96.65</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.16.14.3"><span class="ltx_text" id="S4.T1.16.14.3.1" style="font-size:70%;">0.186</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.16.14.4"><span class="ltx_text" id="S4.T1.16.14.4.1" style="font-size:70%;">0.219</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.16.14.5"><span class="ltx_text" id="S4.T1.16.14.5.1" style="font-size:70%;">3.88</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.16.14.6"><span class="ltx_text" id="S4.T1.16.14.6.1" style="font-size:70%;">64s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.17.15">
<td class="ltx_td ltx_align_left" id="S4.T1.17.15.1">
<span class="ltx_text" id="S4.T1.17.15.1.1" style="font-size:70%;">iFusion (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.17.15.1.m1.1"><semantics id="S4.T1.17.15.1.m1.1a"><msub id="S4.T1.17.15.1.m1.1.1" xref="S4.T1.17.15.1.m1.1.1.cmml"><mi id="S4.T1.17.15.1.m1.1.1.2" mathsize="70%" xref="S4.T1.17.15.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.17.15.1.m1.1.1.3" xref="S4.T1.17.15.1.m1.1.1.3.cmml"><mi id="S4.T1.17.15.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.17.15.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.17.15.1.m1.1.1.3.1" xref="S4.T1.17.15.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.17.15.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.17.15.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.17.15.1.m1.1.1.3.1a" xref="S4.T1.17.15.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.17.15.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.17.15.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.17.15.1.m1.1.1.3.1b" xref="S4.T1.17.15.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.17.15.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.17.15.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.17.15.1.m1.1b"><apply id="S4.T1.17.15.1.m1.1.1.cmml" xref="S4.T1.17.15.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.17.15.1.m1.1.1.1.cmml" xref="S4.T1.17.15.1.m1.1.1">subscript</csymbol><ci id="S4.T1.17.15.1.m1.1.1.2.cmml" xref="S4.T1.17.15.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.17.15.1.m1.1.1.3.cmml" xref="S4.T1.17.15.1.m1.1.1.3"><times id="S4.T1.17.15.1.m1.1.1.3.1.cmml" xref="S4.T1.17.15.1.m1.1.1.3.1"></times><ci id="S4.T1.17.15.1.m1.1.1.3.2.cmml" xref="S4.T1.17.15.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.17.15.1.m1.1.1.3.3.cmml" xref="S4.T1.17.15.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.17.15.1.m1.1.1.3.4.cmml" xref="S4.T1.17.15.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.17.15.1.m1.1.1.3.5.cmml" xref="S4.T1.17.15.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.15.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.17.15.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.17.15.1.2" style="font-size:70%;">=4)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.17.15.2"><span class="ltx_text" id="S4.T1.17.15.2.1" style="font-size:70%;">8.55</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.17.15.3"><span class="ltx_text" id="S4.T1.17.15.3.1" style="font-size:70%;">0.578</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.17.15.4"><span class="ltx_text" id="S4.T1.17.15.4.1" style="font-size:70%;">0.631</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.17.15.5"><span class="ltx_text" id="S4.T1.17.15.5.1" style="font-size:70%;">0.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.17.15.6"><span class="ltx_text" id="S4.T1.17.15.6.1" style="font-size:70%;">256s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.26">
<td class="ltx_td ltx_align_left" id="S4.T1.19.26.1"><span class="ltx_text" id="S4.T1.19.26.1.1" style="font-size:70%;">Ours (w/o refine)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.26.2"><span class="ltx_text" id="S4.T1.19.26.2.1" style="font-size:70%;">10.87</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.26.3"><span class="ltx_text" id="S4.T1.19.26.3.1" style="font-size:70%;">0.554</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.26.4"><span class="ltx_text" id="S4.T1.19.26.4.1" style="font-size:70%;">0.597</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.26.5"><span class="ltx_text" id="S4.T1.19.26.5.1" style="font-size:70%;">0.49</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.26.6"><span class="ltx_text" id="S4.T1.19.26.6.1" style="font-size:70%;">10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.18.16">
<td class="ltx_td ltx_align_left" id="S4.T1.18.16.1">
<span class="ltx_text" id="S4.T1.18.16.1.1" style="font-size:70%;">Ours (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.18.16.1.m1.1"><semantics id="S4.T1.18.16.1.m1.1a"><msub id="S4.T1.18.16.1.m1.1.1" xref="S4.T1.18.16.1.m1.1.1.cmml"><mi id="S4.T1.18.16.1.m1.1.1.2" mathsize="70%" xref="S4.T1.18.16.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.18.16.1.m1.1.1.3" xref="S4.T1.18.16.1.m1.1.1.3.cmml"><mi id="S4.T1.18.16.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.18.16.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.18.16.1.m1.1.1.3.1" xref="S4.T1.18.16.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.18.16.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.18.16.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.18.16.1.m1.1.1.3.1a" xref="S4.T1.18.16.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.18.16.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.18.16.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.18.16.1.m1.1.1.3.1b" xref="S4.T1.18.16.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.18.16.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.18.16.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.18.16.1.m1.1b"><apply id="S4.T1.18.16.1.m1.1.1.cmml" xref="S4.T1.18.16.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.18.16.1.m1.1.1.1.cmml" xref="S4.T1.18.16.1.m1.1.1">subscript</csymbol><ci id="S4.T1.18.16.1.m1.1.1.2.cmml" xref="S4.T1.18.16.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.18.16.1.m1.1.1.3.cmml" xref="S4.T1.18.16.1.m1.1.1.3"><times id="S4.T1.18.16.1.m1.1.1.3.1.cmml" xref="S4.T1.18.16.1.m1.1.1.3.1"></times><ci id="S4.T1.18.16.1.m1.1.1.3.2.cmml" xref="S4.T1.18.16.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.18.16.1.m1.1.1.3.3.cmml" xref="S4.T1.18.16.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.18.16.1.m1.1.1.3.4.cmml" xref="S4.T1.18.16.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.18.16.1.m1.1.1.3.5.cmml" xref="S4.T1.18.16.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.16.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.18.16.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.18.16.1.2" style="font-size:70%;">=1)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.18.16.2"><span class="ltx_text" id="S4.T1.18.16.2.1" style="font-size:70%;">9.30</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.18.16.3"><span class="ltx_text" id="S4.T1.18.16.3.1" style="font-size:70%;">0.565</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.18.16.4"><span class="ltx_text" id="S4.T1.18.16.4.1" style="font-size:70%;">0.600</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.18.16.5"><span class="ltx_text" id="S4.T1.18.16.5.1" style="font-size:70%;">0.43</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.18.16.6"><span class="ltx_text" id="S4.T1.18.16.6.1" style="font-size:70%;">27s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.17">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.19.17.1">
<span class="ltx_text" id="S4.T1.19.17.1.1" style="font-size:70%;">Ours (</span><math alttext="n_{init}" class="ltx_Math" display="inline" id="S4.T1.19.17.1.m1.1"><semantics id="S4.T1.19.17.1.m1.1a"><msub id="S4.T1.19.17.1.m1.1.1" xref="S4.T1.19.17.1.m1.1.1.cmml"><mi id="S4.T1.19.17.1.m1.1.1.2" mathsize="70%" xref="S4.T1.19.17.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T1.19.17.1.m1.1.1.3" xref="S4.T1.19.17.1.m1.1.1.3.cmml"><mi id="S4.T1.19.17.1.m1.1.1.3.2" mathsize="70%" xref="S4.T1.19.17.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T1.19.17.1.m1.1.1.3.1" xref="S4.T1.19.17.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.19.17.1.m1.1.1.3.3" mathsize="70%" xref="S4.T1.19.17.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T1.19.17.1.m1.1.1.3.1a" xref="S4.T1.19.17.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.19.17.1.m1.1.1.3.4" mathsize="70%" xref="S4.T1.19.17.1.m1.1.1.3.4.cmml">i</mi><mo id="S4.T1.19.17.1.m1.1.1.3.1b" xref="S4.T1.19.17.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T1.19.17.1.m1.1.1.3.5" mathsize="70%" xref="S4.T1.19.17.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.19.17.1.m1.1b"><apply id="S4.T1.19.17.1.m1.1.1.cmml" xref="S4.T1.19.17.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.19.17.1.m1.1.1.1.cmml" xref="S4.T1.19.17.1.m1.1.1">subscript</csymbol><ci id="S4.T1.19.17.1.m1.1.1.2.cmml" xref="S4.T1.19.17.1.m1.1.1.2">ùëõ</ci><apply id="S4.T1.19.17.1.m1.1.1.3.cmml" xref="S4.T1.19.17.1.m1.1.1.3"><times id="S4.T1.19.17.1.m1.1.1.3.1.cmml" xref="S4.T1.19.17.1.m1.1.1.3.1"></times><ci id="S4.T1.19.17.1.m1.1.1.3.2.cmml" xref="S4.T1.19.17.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T1.19.17.1.m1.1.1.3.3.cmml" xref="S4.T1.19.17.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T1.19.17.1.m1.1.1.3.4.cmml" xref="S4.T1.19.17.1.m1.1.1.3.4">ùëñ</ci><ci id="S4.T1.19.17.1.m1.1.1.3.5.cmml" xref="S4.T1.19.17.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.17.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.19.17.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.19.17.1.2" style="font-size:70%;">=4)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.19.17.2"><span class="ltx_text ltx_font_bold" id="S4.T1.19.17.2.1" style="font-size:70%;">5.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.19.17.3"><span class="ltx_text ltx_font_bold" id="S4.T1.19.17.3.1" style="font-size:70%;">0.675</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.19.17.4"><span class="ltx_text ltx_font_bold" id="S4.T1.19.17.4.1" style="font-size:70%;">0.701</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.19.17.5"><span class="ltx_text ltx_font_bold" id="S4.T1.19.17.5.1" style="font-size:70%;">0.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.19.17.6"><span class="ltx_text" id="S4.T1.19.17.6.1" style="font-size:70%;">57s</span></td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Training Datasets and Details.</span>
We train our models on a curated subset of 100k shapes from the Objaverse dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib9" title="">9</a>]</cite>. Considering the variable quality of the original Objaverse dataset, we opted to filter out higher-quality data by initially manually annotating 8,000 3D objects based on overall geometry quality and texture preferences. Subsequently, we train MLP models for quality rating classification and texture score regression, utilizing their multimodal features¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib33" title="">33</a>]</cite>. Based on the predictions of these models, we select shapes that are rated as high-quality and have top texture scores. Further details about the data filtering process are included in the Appendix.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">For each 3D shape, we render 10 sets of images using BlenderProc¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib11" title="">11</a>]</cite>. Each set comprises 6 input images, 6 output multi-view images, and 6 NOCS maps. To mimic real-world conditions and ensure model robustness, we randomly sample camera intrinsics and extrinsics, as well as environment maps for the input images. For the output multi-view images, their intrinsics remain constant, while the extrinsics are derived from a fixed delta pose and the azimuth of the input images. Each set of input and output images shares the same environment map. During training, we randomly selected between 1 to 6 views as sparse input views, with the first view of each set always being included. We train the model utilizing 8 A100 GPUs for approximately 3 days.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="509" id="S4.F4.g1" src="extracted/5793183/figures/sparp_comp_3d_new.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F4.4.2" style="font-size:90%;">Qualitative Results on 3D Reconstruction.<span class="ltx_text ltx_font_medium" id="S4.F4.4.2.1"> Zero123XL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib8" title="">8</a>]</cite>, One2345¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib34" title="">34</a>]</cite>, and TripoSR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib69" title="">69</a>]</cite> are single-image-to-3D methods, each utilizing only the first input image. iFusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib83" title="">83</a>]</cite>, EscherNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib23" title="">23</a>]</cite>, and our approach take all input images (the first row). Textured meshes and mesh normal renderings are shown. Shapes come from the OmniObject3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib85" title="">85</a>]</cite> and GSO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite> datasets.
</span></span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Baselines.</span>
For 3D reconstruction, we compare our method with both state-of-the-art single-image-to-3D and sparse-view-to-3D baselines. Single-view-to-3D methods we evaluate include optimization-based approaches, such as Zero123 XL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib8" title="">8</a>]</cite>, SyncDreamer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib36" title="">36</a>]</cite>, and DreamGaussian¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib66" title="">66</a>]</cite>, as well as feed-forward methods like One-2-3-45¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib34" title="">34</a>]</cite> and Shap-E¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib20" title="">20</a>]</cite>. For sparse-view methods, we consider two recent open-source approaches as baselines: iFusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib83" title="">83</a>]</cite> and EscherNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib23" title="">23</a>]</cite>. We utilize ThreeStudio¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib13" title="">13</a>]</cite>‚Äôs implementation for Zero123 XL and the official implementations for the other baselines. Specifically, for iFusion, we use their official reconstruction pipeline integrated with Zero123 XL.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">For sparse-view pose estimation, we compare our method with state-of-the-art approaches including RelPose++¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib29" title="">29</a>]</cite>, FORGE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib17" title="">17</a>]</cite>, and iFusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib83" title="">83</a>]</cite>. The latter two are optimization-based while <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib29" title="">29</a>]</cite> is a feed-forward method.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Evaluation Datasets.</span>
For 3D reconstruction, we evaluate the methods on the entire GSO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite> dataset, which comprises 1,030 3D shapes; none of these shapes were seen during our training. For each 3D shape, we randomly render six views as input images. For single-image-to-3D methods, a fixed-view image is taken as input following¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib34" title="">34</a>]</cite>. We carefully align the predictions with the ground truth meshes before calculating the metrics. Please refer to the Appendix for detailed information on shape alignment and the evaluation metrics.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">For pose estimation, we evaluate the approaches on three datasets: OmniObject3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib85" title="">85</a>]</cite> and GSO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite>, both captured from real scans, and ABO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib7" title="">7</a>]</cite>, a synthetic dataset created by artists. For each dataset, we randomly choose 500 objects and render five random sparse views per shape. We follow iFusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib83" title="">83</a>]</cite> to report the rotation accuracy and the median error in rotation and translation across all image pairs. More details are provided in the Appendix.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.20.1.1" style="font-size:129%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T2.21.2" style="font-size:129%;">Quantitative Comparison on 3D Reconstruction.<span class="ltx_text ltx_font_medium" id="S4.T2.21.2.1"> Evaluated on the complete GSO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite> dataset, which contains 1,030 3D objects. Five single-image-to-3D methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib66" title="">66</a>]</cite> and two sparse-view methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib23" title="">23</a>]</cite> are compared.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.6">
<tr class="ltx_tr" id="S4.T2.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="n_{input}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.m1.1.1.2" mathsize="70%" xref="S4.T2.1.1.1.m1.1.1.2.cmml">n</mi><mrow id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3.cmml"><mi id="S4.T2.1.1.1.m1.1.1.3.2" mathsize="70%" xref="S4.T2.1.1.1.m1.1.1.3.2.cmml">i</mi><mo id="S4.T2.1.1.1.m1.1.1.3.1" xref="S4.T2.1.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T2.1.1.1.m1.1.1.3.3" mathsize="70%" xref="S4.T2.1.1.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T2.1.1.1.m1.1.1.3.1a" xref="S4.T2.1.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T2.1.1.1.m1.1.1.3.4" mathsize="70%" xref="S4.T2.1.1.1.m1.1.1.3.4.cmml">p</mi><mo id="S4.T2.1.1.1.m1.1.1.3.1b" xref="S4.T2.1.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T2.1.1.1.m1.1.1.3.5" mathsize="70%" xref="S4.T2.1.1.1.m1.1.1.3.5.cmml">u</mi><mo id="S4.T2.1.1.1.m1.1.1.3.1c" xref="S4.T2.1.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.T2.1.1.1.m1.1.1.3.6" mathsize="70%" xref="S4.T2.1.1.1.m1.1.1.3.6.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2">ùëõ</ci><apply id="S4.T2.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3"><times id="S4.T2.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.1.1.1.m1.1.1.3.1"></times><ci id="S4.T2.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.1.1.1.m1.1.1.3.2">ùëñ</ci><ci id="S4.T2.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3.3">ùëõ</ci><ci id="S4.T2.1.1.1.m1.1.1.3.4.cmml" xref="S4.T2.1.1.1.m1.1.1.3.4">ùëù</ci><ci id="S4.T2.1.1.1.m1.1.1.3.5.cmml" xref="S4.T2.1.1.1.m1.1.1.3.5">ùë¢</ci><ci id="S4.T2.1.1.1.m1.1.1.3.6.cmml" xref="S4.T2.1.1.1.m1.1.1.3.6">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">n_{input}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_p italic_u italic_t end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.6.6.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.6.7.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.2.2.2.1" style="font-size:70%;">F-Score (%)</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T2.2.2.2.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.3.3.3.1" style="font-size:70%;">CLIP-Sim</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.m1.1a"><mo id="S4.T2.3.3.3.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T2.3.3.3.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.4.4.4.1" style="font-size:70%;">PSNR</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.4.4.4.m1.1"><semantics id="S4.T2.4.4.4.m1.1a"><mo id="S4.T2.4.4.4.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T2.4.4.4.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.5.5.5.1" style="font-size:70%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.5.5.5.m1.1"><semantics id="S4.T2.5.5.5.m1.1a"><mo id="S4.T2.5.5.5.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T2.5.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.m1.1b"><ci id="S4.T2.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.6.6.6.1" style="font-size:70%;">Time</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.6.6.6.m1.1"><semantics id="S4.T2.6.6.6.m1.1a"><mo id="S4.T2.6.6.6.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T2.6.6.6.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.m1.1b"><ci id="S4.T2.6.6.6.m1.1.1.cmml" xref="S4.T2.6.6.6.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.m1.1d">‚Üì</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.7.1" rowspan="5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.7.1.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.6.7.2.1" style="font-size:70%;">Zero123 XL¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.6.7.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib8" title="">8</a><span class="ltx_text" id="S4.T2.6.7.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.7.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.7.3.1" style="font-size:70%;">91.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.7.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.7.4.1" style="font-size:70%;">73.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.7.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.7.5.1" style="font-size:70%;">18.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.7.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.7.6.1" style="font-size:70%;">0.136</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.7.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.7.7.1" style="font-size:70%;">20min</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.6.8.1.1" style="font-size:70%;">Shap-E¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.6.8.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib20" title="">20</a><span class="ltx_text" id="S4.T2.6.8.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.8.2.1" style="font-size:70%;">91.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.8.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.8.3.1" style="font-size:70%;">73.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.8.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.8.4.1" style="font-size:70%;">18.96</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.8.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.8.5.1" style="font-size:70%;">0.140</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.8.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.8.6.1" style="font-size:70%;">27s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.6.9.1.1" style="font-size:70%;">One-2-3-45¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.6.9.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib34" title="">34</a><span class="ltx_text" id="S4.T2.6.9.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.9.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.9.2.1" style="font-size:70%;">90.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.9.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.9.3.1" style="font-size:70%;">70.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.9.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.9.4.1" style="font-size:70%;">19.07</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.9.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.9.5.1" style="font-size:70%;">0.133</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.9.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.9.6.1" style="font-size:70%;">45s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.6.10.1.1" style="font-size:70%;">SyncDreamer¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.6.10.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib36" title="">36</a><span class="ltx_text" id="S4.T2.6.10.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.10.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.10.2.1" style="font-size:70%;">84.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.10.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.10.3.1" style="font-size:70%;">68.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.10.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.10.4.1" style="font-size:70%;">16.86</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.10.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.10.5.1" style="font-size:70%;">0.145</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.10.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.10.6.1" style="font-size:70%;">6min</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.6.11.1.1" style="font-size:70%;">DreamGaussian¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.6.11.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib66" title="">66</a><span class="ltx_text" id="S4.T2.6.11.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.11.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.11.2.1" style="font-size:70%;">81.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.11.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.11.3.1" style="font-size:70%;">68.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.11.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.11.4.1" style="font-size:70%;">17.88</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.11.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.11.5.1" style="font-size:70%;">0.147</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.11.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.11.6.1" style="font-size:70%;">2min</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.12">
<td class="ltx_td ltx_border_r" id="S4.T2.6.12.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.12.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.12.2.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.12.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.12.3.1" style="font-size:70%;">95.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.12.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.12.4.1" style="font-size:70%;">78.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.12.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.12.5.1" style="font-size:70%;">19.87</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.12.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.12.6.1" style="font-size:70%;">0.124</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.12.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.12.7.1" style="font-size:70%;">16s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.13">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.6.13.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.13.1.1" style="font-size:70%;">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.13.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.6.13.2.1" style="font-size:70%;">iFusion¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.6.13.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib83" title="">83</a><span class="ltx_text" id="S4.T2.6.13.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.13.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.13.3.1" style="font-size:70%;">88.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.13.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.13.4.1" style="font-size:70%;">66.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.13.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.13.5.1" style="font-size:70%;">16.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.13.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.13.6.1" style="font-size:70%;">0.151</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.13.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.13.7.1" style="font-size:70%;">28min</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.14.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.6.14.1.1" style="font-size:70%;">EscherNet¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.6.14.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib23" title="">23</a><span class="ltx_text" id="S4.T2.6.14.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.14.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.14.2.1" style="font-size:70%;">94.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.14.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.14.3.1" style="font-size:70%;">65.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.14.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.14.4.1" style="font-size:70%;">16.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.14.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.14.5.1" style="font-size:70%;">0.139</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.14.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.14.6.1" style="font-size:70%;">9min</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.15">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.6.15.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.6.15.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.6.15.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.15.2.1" style="font-size:70%;">96.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.6.15.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.15.3.1" style="font-size:70%;">78.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.6.15.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.15.4.1" style="font-size:70%;">19.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.6.15.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.15.5.1" style="font-size:70%;">0.123</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.6.15.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.15.6.1" style="font-size:70%;">16s</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="244" id="S4.F5.g1" src="extracted/5793183/figures/sparp_single_multi.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F5.4.2" style="font-size:90%;">Single-View vs. Sparse-View for 3D Reconstruction.<span class="ltx_text ltx_font_medium" id="S4.F5.4.2.1"> We compare the results of our method when using single-view and sparse-view inputs. </span></span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.15.1.1" style="font-size:129%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T3.16.2" style="font-size:129%;">Ablation Study on the Number of Input Views.<span class="ltx_text ltx_font_medium" id="S4.T3.16.2.1"> Evaluated on the GSO dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite>.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.7">
<tr class="ltx_tr" id="S4.T3.7.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.1.1.1">
<math alttext="n" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.m1.1.1" mathsize="70%" xref="S4.T3.1.1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">italic_n</annotation></semantics></math><span class="ltx_text" id="S4.T3.1.1.1.1" style="font-size:70%;"> views</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.2">
<span class="ltx_text" id="S4.T3.2.2.2.1" style="font-size:70%;">Rot. Err</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.m1.1a"><mo id="S4.T3.2.2.2.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.3.3.3">
<span class="ltx_text" id="S4.T3.3.3.3.1" style="font-size:70%;">Acc.@5</span><math alttext="{}^{\circ}\uparrow" class="ltx_math_unparsed" display="inline" id="S4.T3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.m1.1a"><mmultiscripts id="S4.T3.3.3.3.m1.1.1"><mo id="S4.T3.3.3.3.m1.1.1.2" mathsize="70%" stretchy="false">‚Üë</mo><mprescripts id="S4.T3.3.3.3.m1.1.1a"></mprescripts><mrow id="S4.T3.3.3.3.m1.1.1b"></mrow><mo id="S4.T3.3.3.3.m1.1.1.3" mathsize="70%">‚àò</mo></mmultiscripts><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1b">{}^{\circ}\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.m1.1c">start_FLOATSUPERSCRIPT ‚àò end_FLOATSUPERSCRIPT ‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.4.4.4">
<span class="ltx_text" id="S4.T3.4.4.4.1" style="font-size:70%;">Trans. Err</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.4.4.4.m1.1"><semantics id="S4.T3.4.4.4.m1.1a"><mo id="S4.T3.4.4.4.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.4.4.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.m1.1b"><ci id="S4.T3.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.5.5.5">
<span class="ltx_text" id="S4.T3.5.5.5.1" style="font-size:70%;">F-Score (%)</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.5.5.5.m1.1"><semantics id="S4.T3.5.5.5.m1.1a"><mo id="S4.T3.5.5.5.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.5.5.5.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.m1.1b"><ci id="S4.T3.5.5.5.m1.1.1.cmml" xref="S4.T3.5.5.5.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.6.6.6">
<span class="ltx_text" id="S4.T3.6.6.6.1" style="font-size:70%;">CLIP-Sim</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.6.6.6.m1.1"><semantics id="S4.T3.6.6.6.m1.1a"><mo id="S4.T3.6.6.6.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.6.6.6.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.m1.1b"><ci id="S4.T3.6.6.6.m1.1.1.cmml" xref="S4.T3.6.6.6.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.6.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.7.7.7">
<span class="ltx_text" id="S4.T3.7.7.7.1" style="font-size:70%;">PSNR</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.7.7.7.m1.1"><semantics id="S4.T3.7.7.7.m1.1a"><mo id="S4.T3.7.7.7.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.7.7.7.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.m1.1b"><ci id="S4.T3.7.7.7.m1.1.1.cmml" xref="S4.T3.7.7.7.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.7.7.m1.1d">‚Üë</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.8.1"><span class="ltx_text" id="S4.T3.7.8.1.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.8.2"><span class="ltx_text" id="S4.T3.7.8.2.1" style="font-size:70%;">‚Äì</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.8.3"><span class="ltx_text" id="S4.T3.7.8.3.1" style="font-size:70%;">‚Äì</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.8.4"><span class="ltx_text" id="S4.T3.7.8.4.1" style="font-size:70%;">‚Äì</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.8.5"><span class="ltx_text" id="S4.T3.7.8.5.1" style="font-size:70%;">89.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.8.6"><span class="ltx_text" id="S4.T3.7.8.6.1" style="font-size:70%;">74.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.8.7"><span class="ltx_text" id="S4.T3.7.8.7.1" style="font-size:70%;">17.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.9.1"><span class="ltx_text" id="S4.T3.7.9.1.1" style="font-size:70%;">2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.9.2"><span class="ltx_text" id="S4.T3.7.9.2.1" style="font-size:70%;">8.56</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.9.3"><span class="ltx_text" id="S4.T3.7.9.3.1" style="font-size:70%;">0.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.9.4"><span class="ltx_text" id="S4.T3.7.9.4.1" style="font-size:70%;">0.42</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.9.5"><span class="ltx_text" id="S4.T3.7.9.5.1" style="font-size:70%;">93.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.9.6"><span class="ltx_text" id="S4.T3.7.9.6.1" style="font-size:70%;">76.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.9.7"><span class="ltx_text" id="S4.T3.7.9.7.1" style="font-size:70%;">18.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.10.1"><span class="ltx_text" id="S4.T3.7.10.1.1" style="font-size:70%;">4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.10.2"><span class="ltx_text" id="S4.T3.7.10.2.1" style="font-size:70%;">6.03</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.10.3"><span class="ltx_text" id="S4.T3.7.10.3.1" style="font-size:70%;">0.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.10.4"><span class="ltx_text" id="S4.T3.7.10.4.1" style="font-size:70%;">0.28</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.10.5"><span class="ltx_text" id="S4.T3.7.10.5.1" style="font-size:70%;">96.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.10.6"><span class="ltx_text" id="S4.T3.7.10.6.1" style="font-size:70%;">77.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.10.7"><span class="ltx_text" id="S4.T3.7.10.7.1" style="font-size:70%;">19.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.11">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.7.11.1"><span class="ltx_text" id="S4.T3.7.11.1.1" style="font-size:70%;">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.11.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.11.2.1" style="font-size:70%;">5.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.11.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.11.3.1" style="font-size:70%;">0.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.7.11.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.11.4.1" style="font-size:70%;">0.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.11.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.11.5.1" style="font-size:70%;">96.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.11.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.11.6.1" style="font-size:70%;">78.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.11.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.11.7.1" style="font-size:70%;">19.3</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="270" id="S4.F6.g1" src="extracted/5793183/figures/sparp_pose_quali.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.3.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F6.4.2" style="font-size:90%;">Ablation Study on Pose Refinement.<span class="ltx_text ltx_font_medium" id="S4.F6.4.2.1"> We showcase the input images, predicted NOCS maps, and converted poses. The ground truth poses are in black, while the predicted poses before and after refinement are in blue and red, respectively.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.15.1.1" style="font-size:129%;">Table 4</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T4.16.2" style="font-size:129%;">Effect of Joint Training.<span class="ltx_text ltx_font_medium" id="S4.T4.16.2.1"> Evaluated on 500 objects from GSO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite>.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.7">
<tr class="ltx_tr" id="S4.T4.7.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.7.7.8"><span class="ltx_text" id="S4.T4.7.7.8.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1">
<span class="ltx_text" id="S4.T4.1.1.1.1" style="font-size:70%;">Rot. Err</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T4.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.2.2">
<span class="ltx_text" id="S4.T4.2.2.2.1" style="font-size:70%;">Acc.@15</span><math alttext="{}^{\circ}\uparrow" class="ltx_math_unparsed" display="inline" id="S4.T4.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.m1.1a"><mmultiscripts id="S4.T4.2.2.2.m1.1.1"><mo id="S4.T4.2.2.2.m1.1.1.2" mathsize="70%" stretchy="false">‚Üë</mo><mprescripts id="S4.T4.2.2.2.m1.1.1a"></mprescripts><mrow id="S4.T4.2.2.2.m1.1.1b"></mrow><mo id="S4.T4.2.2.2.m1.1.1.3" mathsize="70%">‚àò</mo></mmultiscripts><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1b">{}^{\circ}\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.m1.1c">start_FLOATSUPERSCRIPT ‚àò end_FLOATSUPERSCRIPT ‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3">
<span class="ltx_text" id="S4.T4.3.3.3.1" style="font-size:70%;">Acc.@30</span><math alttext="{}^{\circ}\uparrow" class="ltx_math_unparsed" display="inline" id="S4.T4.3.3.3.m1.1"><semantics id="S4.T4.3.3.3.m1.1a"><mmultiscripts id="S4.T4.3.3.3.m1.1.1"><mo id="S4.T4.3.3.3.m1.1.1.2" mathsize="70%" stretchy="false">‚Üë</mo><mprescripts id="S4.T4.3.3.3.m1.1.1a"></mprescripts><mrow id="S4.T4.3.3.3.m1.1.1b"></mrow><mo id="S4.T4.3.3.3.m1.1.1.3" mathsize="70%">‚àò</mo></mmultiscripts><annotation encoding="application/x-tex" id="S4.T4.3.3.3.m1.1b">{}^{\circ}\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.m1.1c">start_FLOATSUPERSCRIPT ‚àò end_FLOATSUPERSCRIPT ‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.4.4.4">
<span class="ltx_text" id="S4.T4.4.4.4.1" style="font-size:70%;">Trans. Err</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.4.4.4.m1.1"><semantics id="S4.T4.4.4.4.m1.1a"><mo id="S4.T4.4.4.4.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T4.4.4.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.m1.1b"><ci id="S4.T4.4.4.4.m1.1.1.cmml" xref="S4.T4.4.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.5.5.5">
<span class="ltx_text" id="S4.T4.5.5.5.1" style="font-size:70%;">F-Score (%)</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.5.5.5.m1.1"><semantics id="S4.T4.5.5.5.m1.1a"><mo id="S4.T4.5.5.5.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T4.5.5.5.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.m1.1b"><ci id="S4.T4.5.5.5.m1.1.1.cmml" xref="S4.T4.5.5.5.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.6.6.6">
<span class="ltx_text" id="S4.T4.6.6.6.1" style="font-size:70%;">CLIP-Sim</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.6.6.6.m1.1"><semantics id="S4.T4.6.6.6.m1.1a"><mo id="S4.T4.6.6.6.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T4.6.6.6.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.m1.1b"><ci id="S4.T4.6.6.6.m1.1.1.cmml" xref="S4.T4.6.6.6.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.6.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.7.7.7">
<span class="ltx_text" id="S4.T4.7.7.7.1" style="font-size:70%;">PSNR</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.7.7.7.m1.1"><semantics id="S4.T4.7.7.7.m1.1a"><mo id="S4.T4.7.7.7.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T4.7.7.7.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.m1.1b"><ci id="S4.T4.7.7.7.m1.1.1.cmml" xref="S4.T4.7.7.7.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.7.m1.1d">‚Üë</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.7.8.1"><span class="ltx_text" id="S4.T4.7.8.1.1" style="font-size:70%;">Separate</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.2"><span class="ltx_text" id="S4.T4.7.8.2.1" style="font-size:70%;">9.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.3"><span class="ltx_text" id="S4.T4.7.8.3.1" style="font-size:70%;">0.563</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.4"><span class="ltx_text" id="S4.T4.7.8.4.1" style="font-size:70%;">0.617</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.7.8.5"><span class="ltx_text" id="S4.T4.7.8.5.1" style="font-size:70%;">0.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.6"><span class="ltx_text" id="S4.T4.7.8.6.1" style="font-size:70%;">96.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.7"><span class="ltx_text" id="S4.T4.7.8.7.1" style="font-size:70%;">78.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.8"><span class="ltx_text" id="S4.T4.7.8.8.1" style="font-size:70%;">19.03</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.9">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.7.9.1"><span class="ltx_text" id="S4.T4.7.9.1.1" style="font-size:70%;">Joint</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.9.2"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.2.1" style="font-size:70%;">8.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.9.3"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.3.1" style="font-size:70%;">0.601</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.9.4"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.4.1" style="font-size:70%;">0.677</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.7.9.5"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.5.1" style="font-size:70%;">0.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.9.6"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.6.1" style="font-size:70%;">97.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.9.7"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.7.1" style="font-size:70%;">78.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.9.8"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.8.1" style="font-size:70%;">19.42</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiment Results</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.2.1">Pose Prediction.</span>
We report the pose estimation results in <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4.T1" title="In 4.1 Evaluation Settings ‚Ä£ 4 Experiments ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, where it is evident that SpaRP outperforms all baseline methods by a significant margin. It is worth noting that RelPose++¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib29" title="">29</a>]</cite> and FORGE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib17" title="">17</a>]</cite> struggle to yield satisfactory results for our open-world evaluation images. iFusion, an optimization-based approach, is prone to becoming trapped in local minima. With only one initial pose (<math alttext="n_{init}=1" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><msub id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2.2" xref="S4.SS2.p1.1.m1.1.1.2.2.cmml">n</mi><mrow id="S4.SS2.p1.1.m1.1.1.2.3" xref="S4.SS2.p1.1.m1.1.1.2.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2.3.2" xref="S4.SS2.p1.1.m1.1.1.2.3.2.cmml">i</mi><mo id="S4.SS2.p1.1.m1.1.1.2.3.1" xref="S4.SS2.p1.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p1.1.m1.1.1.2.3.3" xref="S4.SS2.p1.1.m1.1.1.2.3.3.cmml">n</mi><mo id="S4.SS2.p1.1.m1.1.1.2.3.1a" xref="S4.SS2.p1.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p1.1.m1.1.1.2.3.4" xref="S4.SS2.p1.1.m1.1.1.2.3.4.cmml">i</mi><mo id="S4.SS2.p1.1.m1.1.1.2.3.1b" xref="S4.SS2.p1.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p1.1.m1.1.1.2.3.5" xref="S4.SS2.p1.1.m1.1.1.2.3.5.cmml">t</mi></mrow></msub><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></eq><apply id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2">ùëõ</ci><apply id="S4.SS2.p1.1.m1.1.1.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3"><times id="S4.SS2.p1.1.m1.1.1.2.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3.1"></times><ci id="S4.SS2.p1.1.m1.1.1.2.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3.2">ùëñ</ci><ci id="S4.SS2.p1.1.m1.1.1.2.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3.3">ùëõ</ci><ci id="S4.SS2.p1.1.m1.1.1.2.3.4.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3.4">ùëñ</ci><ci id="S4.SS2.p1.1.m1.1.1.2.3.5.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3.5">ùë°</ci></apply></apply><cn id="S4.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">n_{init}=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT = 1</annotation></semantics></math>), it also fails to produce adequate results. In contrast, our method leverages priors from 2D diffusion models and can generate acceptable results in a single forward pass. Even without any additional refinement (w/o refine), our method can already produce results similar to iFusion with four initial poses (<math alttext="n_{init}=4" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><msub id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2.2" xref="S4.SS2.p1.2.m2.1.1.2.2.cmml">n</mi><mrow id="S4.SS2.p1.2.m2.1.1.2.3" xref="S4.SS2.p1.2.m2.1.1.2.3.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2.3.2" xref="S4.SS2.p1.2.m2.1.1.2.3.2.cmml">i</mi><mo id="S4.SS2.p1.2.m2.1.1.2.3.1" xref="S4.SS2.p1.2.m2.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p1.2.m2.1.1.2.3.3" xref="S4.SS2.p1.2.m2.1.1.2.3.3.cmml">n</mi><mo id="S4.SS2.p1.2.m2.1.1.2.3.1a" xref="S4.SS2.p1.2.m2.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p1.2.m2.1.1.2.3.4" xref="S4.SS2.p1.2.m2.1.1.2.3.4.cmml">i</mi><mo id="S4.SS2.p1.2.m2.1.1.2.3.1b" xref="S4.SS2.p1.2.m2.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p1.2.m2.1.1.2.3.5" xref="S4.SS2.p1.2.m2.1.1.2.3.5.cmml">t</mi></mrow></msub><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><eq id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></eq><apply id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.2.1.cmml" xref="S4.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2.2">ùëõ</ci><apply id="S4.SS2.p1.2.m2.1.1.2.3.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3"><times id="S4.SS2.p1.2.m2.1.1.2.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3.1"></times><ci id="S4.SS2.p1.2.m2.1.1.2.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3.2">ùëñ</ci><ci id="S4.SS2.p1.2.m2.1.1.2.3.3.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3.3">ùëõ</ci><ci id="S4.SS2.p1.2.m2.1.1.2.3.4.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3.4">ùëñ</ci><ci id="S4.SS2.p1.2.m2.1.1.2.3.5.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3.5">ùë°</ci></apply></apply><cn id="S4.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">n_{init}=4</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT = 4</annotation></semantics></math>), while being far more efficient, requiring just 1/25 of the runtime. With the integration of further refinement through a mixture of experts, our method achieves even better performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">3D Reconstruction.</span>
We present the qualitative results in <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4.F4" title="In 4.1 Evaluation Settings ‚Ä£ 4 Experiments ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>. With only a single-view input, single-image-to-3D methods fail to produce meshes that faithfully match the entire structure and details of the ground truth mesh. For instance, most single-view baseline methods are unable to reconstruct the stems of the artichoke, the back of the firetruck, the red saddle on Yoshi, and the two separate legs of Kirby standing on the ground. In contrast, sparse-view methods yield results that are much closer to the ground truth by incorporating information from multiple sparse views. Compared to iFusion, EscherNet, our method generates meshes with higher-quality geometry and textures that more accurately match the input sparse views. We report the quantitative results in <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4.T2" title="In 4.1 Evaluation Settings ‚Ä£ 4 Experiments ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>, where our method significantly outperforms both single-view-to-3D and sparse-view approaches in terms of both 2D and 3D metrics. Moreover, our method exhibits superior efficiency, being much faster than the baseline methods.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Analysis</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Single View vs. Sparse Views.</span> In <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4.F5" title="In 4.1 Evaluation Settings ‚Ä£ 4 Experiments ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>, we present the results obtained by our method when provided with single-view and sparse-view inputs. With a single-view input, our method can still generate reasonable results, yet it may not accurately capture the structures and details of the regions that are not visible. Our method demonstrates the capability to effectively integrate information from all sparse-view inputs provided.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Number of Views.</span> In <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4.T3" title="In 4.1 Evaluation Settings ‚Ä£ 4 Experiments ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>, we quantitatively showcase the impact of the number of views on both 3D reconstruction and pose estimation. We observe that incorporating more input views enables the 2D diffusion network to better grasp their spatial relationships and underlying 3D objects, boosting both tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Pose Refinement.</span> While the predicted NOCS maps can be directly converted into camera poses, we have found that these poses can be further refined through alignment with the generated 3D meshes. <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4.F6" title="In 4.1 Evaluation Settings ‚Ä£ 4 Experiments ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">6</span></a> showcases the predicted poses before and after refinement. Although both are generally very close to the ground truth poses, refinement can further reduce the error.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">Number of Experts.</span> We employ a mixture-of-experts strategy to address the ambiguity issues related to NOCS prediction for symmetric objects. By using this strategy and increasing the number of experts, there is a substantial increase in pose estimation accuracy. Please refer to the Appendix for more details and quantitative ablation studies.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p5.1.1">Joint Training.</span> We finetune 2D diffusion models to jointly predict NOCS maps and multi-view images from sparse, unposed views by leveraging a domain switcher. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#S4.T4" title="In 4.1 Evaluation Settings ‚Ä£ 4 Experiments ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>, this joint training strategy enables the two branches to implicitly interact and complement each other, enhancing the interpretation of both the input sparse views and the intrinsic properties of the 3D objects, which in turn improves the performance of each task.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We present SpaRP, a novel method for 3D reconstruction and pose estimation using unposed sparse-view images. Our method leverages rich priors embedded in 2D diffusion models and exhibits strong open-world generalizability. Without the need for per-shape optimization, it can deliver high-quality textured meshes, along with accurate camera poses, in approximately 20 seconds.
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Acknowledgements: </span> We thank Chong Zeng, Xinyue Wei for the discussion and help with data processing, and Peng Wang for providing the evaluation set. We also extend our thanks to all annotators for their meticulous annotations.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: A randomized correspondence algorithm for structural image editing. ACM Trans. Graph. <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">28</span>(3), ¬†24 (2009)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bradski, G.: Perspective-n-point (pnp) pose computation (the opencv library). <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html" title="">https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html</a> (2000)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Chan, E.R., Nagano, K., Chan, M.A., Bergman, A.W., Park, J.J., Levy, A., Aittala, M., De¬†Mello, S., Karras, T., Wetzstein, G.: Genvs: Generative novel view synthesis with 3d-aware diffusion models (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, A., Xu, Z., Zhao, F., Zhang, X., Xiang, F., Yu, J., Su, H.: Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14124‚Äì14133 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. arXiv preprint arXiv:2309.16585 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Collins, J., Goel, S., Deng, K., Luthra, A., Xu, L., Gundogdu, E., Zhang, X., Vicente, T.F.Y., Dideriksen, T., Arora, H., et¬†al.: Abo: Dataset and benchmarks for real-world 3d object understanding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 21126‚Äì21136 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Deitke, M., Liu, R., Wallingford, M., Ngo, H., Michel, O., Kusupati, A., Fan, A., Laforte, C., Voleti, V., Gadre, S.Y., et¬†al.: Objaverse-xl: A universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13142‚Äì13153 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Deng, C., Jiang, C., Qi, C.R., Yan, X., Zhou, Y., Guibas, L., Anguelov, D., et¬†al.: Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 20637‚Äì20647 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Denninger, M., Sundermeyer, M., Winkelbauer, D., Zidan, Y., Olefir, D., Elbadrawy, M., Lodhi, A., Katam, H.: Blenderproc. arXiv preprint arXiv:1911.01911 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset of 3d scanned household items. In: 2022 International Conference on Robotics and Automation (ICRA). pp. 2553‚Äì2560. IEEE (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Guo, Y.C., Liu, Y.T., Shao, R., Laforte, C., Voleti, V., Luo, G., Chen, C.H., Zou, Z.X., Wang, C., Cao, Y.P., Zhang, S.H.: threestudio: A unified framework for 3d content generation. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/threestudio-project/threestudio" title="">https://github.com/threestudio-project/threestudio</a> (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided object generation with dream fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 867‚Äì876 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jain, A., Tancik, M., Abbeel, P.: Putting nerf on a diet: Semantically consistent few-shot view synthesis. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5885‚Äì5894 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jiang, H., Jiang, Z., Grauman, K., Zhu, Y.: Few-view object reconstruction with unknown categories and camera poses. arXiv preprint arXiv:2212.04492 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jiang, H., Jiang, Z., Zhao, Y., Huang, Q.: Leap: Liberate sparse-view 3d modeling from camera poses. arXiv preprint arXiv:2310.01410 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Johari, M.M., Lepoittevin, Y., Fleuret, F.: Geonerf: Generalizing nerf with geometry priors. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18365‚Äì18375 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Karnewar, A., Vedaldi, A., Novotny, D., Mitra, N.J.: Holodiffusion: Training a 3d diffusion model using 2d images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18423‚Äì18433 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Kim, M., Seo, S., Han, B.: Infonerf: Ray entropy minimization for few-shot neural volume rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12912‚Äì12921 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Kong, X., Liu, S., Lyu, X., Taher, M., Qi, X., Davison, A.J.: Eschernet: A generative model for scalable view synthesis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9503‚Äì9513 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Kulh√°nek, J., Derner, E., Sattler, T., Babu≈°ka, R.: Viewformer: Nerf-free neural rendering from few images using transformers. In: European Conference on Computer Vision. pp. 198‚Äì216. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Lai, Z., Liu, S., Efros, A.A., Wang, X.: Video autoencoder: self-supervised disentanglement of static 3d structure and motion. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9730‚Äì9740 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Laine, S., Hellsten, J., Karras, T., Seol, Y., Lehtinen, J., Aila, T.: Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">39</span>(6) (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Lee, H.H., Chang, A.X.: Understanding pure clip guidance for voxel grid nerf models. arXiv preprint arXiv:2209.15172 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K., Shakhnarovich, G., Bi, S.: Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Lin, A., Zhang, J.Y., Ramanan, D., Tulsiani, S.: Relpose++: Recovering 6d poses from sparse-view observations. arXiv preprint arXiv:2305.04926 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 300‚Äì309 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Lin, C.H., Ma, W.C., Torralba, A., Lucey, S.: Barf: Bundle-adjusting neural radiance fields. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5741‚Äì5751 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10072‚Äì10083 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Liu, M., Shi, R., Kuang, K., Zhu, Y., Li, X., Han, S., Cai, H., Porikli, F., Su, H.: Openshape: Scaling up 3d shape representation towards open-world understanding. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib33.1.1">36</span> (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Liu, M., Xu, C., Jin, H., Chen, L., Varma¬†T, M., Xu, Z., Su, H.: One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib34.1.1">36</span> (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Liu, R., Wu, R., Van¬†Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9298‚Äì9309 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Liu, Y., Peng, S., Liu, L., Wang, Q., Wang, P., Theobalt, C., Zhou, X., Wang, W.: Neural rays for occlusion-aware image-based rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7824‚Äì7833 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et¬†al.: Wonder3d: Single image to 3d using cross-domain diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9970‚Äì9980 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Long, X., Lin, C., Wang, P., Komura, T., Wang, W.: Sparseneus: Fast generalizable neural surface reconstruction from sparse views. In: European Conference on Computer Vision. pp. 210‚Äì227. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Melas-Kyriazi, L., Laina, I., Rupprecht, C., Vedaldi, A.: Realfusion: 360deg reconstruction of any object from a single image. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8446‚Äì8455 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Metzer, G., Richardson, E., Patashnik, O., Giryes, R., Cohen-Or, D.: Latent-nerf for shape-guided generation of 3d shapes and textures. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12663‚Äì12673 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Michel, O., Bar-On, R., Liu, R., Benaim, S., Hanocka, R.: Text2mesh: Text-driven neural stylization for meshes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13492‚Äì13502 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM <span class="ltx_text ltx_font_bold" id="bib.bib43.1.1">65</span>(1), 99‚Äì106 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Mohammad¬†Khalid, N., Xie, T., Belilovsky, E., Popa, T.: Clip-mesh: Generating textured meshes from text using pretrained image-text models. In: SIGGRAPH Asia 2022 conference papers. pp.¬†1‚Äì8 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Niemeyer, M., Barron, J.T., Mildenhall, B., Sajjadi, M.S., Geiger, A., Radwan, N.: Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5480‚Äì5490 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Qian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.Y., Skorokhodov, I., Wonka, P., Tulyakov, S., et¬†al.: Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et¬†al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748‚Äì8763. PMLR (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Raj, A., Kaza, S., Poole, B., Niemeyer, M., Ruiz, N., Mildenhall, B., Zada, S., Aberman, K., Rubinstein, M., Barron, J., et¬†al.: Dreambooth3d: Subject-driven text-to-3d generation. arXiv preprint arXiv:2303.13508 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: International Conference on Machine Learning. pp. 8821‚Äì8831. PMLR (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Rematas, K., Martin-Brualla, R., Ferrari, V.: Sharf: Shape-conditioned radiance fields from a single view. arXiv preprint arXiv:2102.08860 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Ren, Y., Zhang, T., Pollefeys, M., S√ºsstrunk, S., Wang, F.: Volrecon: Volume rendering of signed ray distance functions for generalizable multi-view reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16685‚Äì16695 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684‚Äì10695 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo¬†Lopes, R., Karagol¬†Ayan, B., Salimans, T., et¬†al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib54.1.1">35</span>, 36479‚Äì36494 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Sajjadi, M.S., Meyer, H., Pot, E., Bergmann, U., Greff, K., Radwan, N., Vora, S., Luƒçiƒá, M., Duckworth, D., Dosovitskiy, A., et¬†al.: Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6229‚Äì6238 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4104‚Äì4113 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Sch√∂nberger, J.L., Zheng, E., Frahm, J.M., Pollefeys, M.: Pixelwise view selection for unstructured multi-view stereo. In: Computer Vision‚ÄìECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14. pp. 501‚Äì518. Springer (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Sch√∂nberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Seo, J., Jang, W., Kwak, M.S., Ko, J., Kim, H., Kim, J., Kim, J.H., Lee, J., Kim, S.: Let 2d diffusion model know 3d-consistency for robust text-to-3d generation. arXiv preprint arXiv:2303.07937 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Shi, R., Wei, X., Wang, C., Su, H.: Zerorf: Fast sparse view 360 <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib61.1.m1.1"><semantics id="bib.bib61.1.m1.1a"><mo id="bib.bib61.1.m1.1.1" stretchy="false" xref="bib.bib61.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib61.1.m1.1b"><ci id="bib.bib61.1.m1.1.1.cmml" xref="bib.bib61.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib61.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib61.1.m1.1d">{</annotation></semantics></math><math alttext="\backslash" class="ltx_Math" display="inline" id="bib.bib61.2.m2.1"><semantics id="bib.bib61.2.m2.1a"><mo id="bib.bib61.2.m2.1.1" xref="bib.bib61.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib61.2.m2.1b"><ci id="bib.bib61.2.m2.1.1.cmml" xref="bib.bib61.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib61.2.m2.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="bib.bib61.2.m2.1d">\</annotation></semantics></math>deg<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib61.3.m3.1"><semantics id="bib.bib61.3.m3.1a"><mo id="bib.bib61.3.m3.1.1" stretchy="false" xref="bib.bib61.3.m3.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib61.3.m3.1b"><ci id="bib.bib61.3.m3.1.1.cmml" xref="bib.bib61.3.m3.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib61.3.m3.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib61.3.m3.1d">}</annotation></semantics></math> reconstruction with zero pretraining. arXiv preprint arXiv:2312.09249 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Sinha, S., Zhang, J.Y., Tagliasacchi, A., Gilitschenski, I., Lindell, D.B.: Sparsepose: Sparse-view camera pose regression and refinement. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 21349‚Äì21359 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Stereopsis, R.M.: Accurate, dense, and robust multiview stereopsis. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE <span class="ltx_text ltx_font_bold" id="bib.bib64.1.1">32</span>(8) (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Tang, J., Chen, Z., Chen, X., Wang, T., Zeng, G., Liu, Z.: Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. arXiv preprint arXiv:2303.14184 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Tewari, A., Yin, T., Cazenavette, G., Rezchikov, S., Tenenbaum, J.B., Durand, F., Freeman, W.T., Sitzmann, V.: Diffusion with forward models: Solving stochastic inverse problems without direct supervision. arXiv preprint arXiv:2306.11719 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Tochilkin, D., Pankratz, D., Liu, Z., Huang, Z., Letts, A., Li, Y., Liang, D., Laforte, C., Jampani, V., Cao, Y.P.: Triposr: Fast 3d object reconstruction from a single image (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Trevithick, A., Yang, B.: Grf: Learning a general radiance field for 3d representation and rendering. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15182‚Äì15192 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Truong, P., Rakotosaona, M.J., Manhardt, F., Tombari, F.: Sparf: Neural radiance fields from sparse and noisy poses. In: CVF Conference on Computer Vision and Pattern Recognition, CVPR. vol.¬†1 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Tung, H.Y.F., Cheng, R., Fragkiadaki, K.: Learning spatial common sense with geometry-aware recurrent networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2595‚Äì2603 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Wang, H., Du, X., Li, J., Yeh, R.A., Shakhnarovich, G.: Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12619‚Äì12629 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Wang, H., Sridhar, S., Huang, J., Valentin, J., Song, S., Guibas, L.J.: Normalized object coordinate space for category-level 6d object pose and size estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2642‚Äì2651 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Wang, J., Rupprecht, C., Novotny, D.: Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9773‚Äì9783 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Wang, P., Chen, X., Chen, T., Venugopalan, S., Wang, Z., et¬†al.: Is attention all nerf needs? arXiv preprint arXiv:2207.13298 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z., Zhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Wang, Q., Wang, Z., Genova, K., Srinivasan, P.P., Zhou, H., Barron, J.T., Martin-Brualla, R., Snavely, N., Funkhouser, T.: Ibrnet: Learning multi-view image-based rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4690‚Äì4699 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Wang, Z., Wu, S., Xie, W., Chen, M., Prisacariu, V.A.: Nerf‚Äì: Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Weng, H., Yang, T., Wang, J., Li, Y., Zhang, T., Chen, C., Zhang, L.: Consistent123: Improve consistency for one image to 3d object synthesis. arXiv preprint arXiv:2310.08092 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Wu, C.H., Chen, Y.C., Solarte, B., Yuan, L., Sun, M.: ifusion: Inverting diffusion for pose-free reconstruction from sparse views (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Wu, R., Mildenhall, B., Henzler, P., Park, K., Gao, R., Watson, D., Srinivasan, P.P., Verbin, D., Barron, J.T., Poole, B., et¬†al.: Reconfusion: 3d reconstruction with diffusion priors. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 21551‚Äì21561 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Wu, T., Zhang, J., Fu, X., Wang, Y., Ren, J., Pan, L., Wu, W., Yang, L., Wang, J., Qian, C., et¬†al.: Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 803‚Äì814 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Xia, Y., Tang, H., Timofte, R., Van¬†Gool, L.: Sinerf: Sinusoidal neural radiance fields for joint pose estimation and scene reconstruction. arXiv preprint arXiv:2210.04553 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Xu, D., Jiang, Y., Wang, P., Fan, Z., Wang, Y., Wang, Z.: Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with 360deg views. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4479‚Äì4489 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Xu, J., Wang, X., Cheng, W., Cao, Y.P., Shan, Y., Qie, X., Gao, S.: Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 20908‚Äì20918 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Yang, H., Hong, L., Li, A., Hu, T., Li, Z., Lee, G.H., Wang, L.: Contranerf: Generalizable neural radiance fields for synthetic-to-real novel view synthesis via contrastive learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16508‚Äì16517 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Yang, Z., Ren, Z., Bautista, M.A., Zhang, Z., Shan, Q., Huang, Q.: Fvor: Robust joint shape and pose optimization for few-view object reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2497‚Äì2507 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Yariv, L., Gu, J., Kasten, Y., Lipman, Y.: Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib91.1.1">34</span>, 4805‚Äì4815 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Ye, J., Wang, P., Li, K., Shi, Y., Wang, H.: Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models. arXiv preprint arXiv:2310.03020 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from one or few images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4578‚Äì4587 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Yu, C., Zhou, Q., Li, J., Zhang, Z., Wang, Z., Wang, F.: Points-to-3d: Bridging the gap between sparse points and shape-controllable text-to-3d generation. arXiv preprint arXiv:2307.13908 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Zhang, J.Y., Ramanan, D., Tulsiani, S.: Relpose: Predicting probabilistic relative rotation for single objects in the wild. In: European Conference on Computer Vision. pp. 592‚Äì611. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Zhang, L.: Reference-only control. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Mikubill/sd-webui-controlnet/discussions/1236" title="">https://github.com/Mikubill/sd-webui-controlnet/discussions/1236</a> (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Zhou, Z., Tulsiani, S.: Sparsefusion: Distilling view-conditioned diffusion for 3d reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12588‚Äì12597 (2023)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="Pt0.A0.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="567" id="Pt0.A0.F7.g1" src="extracted/5793183/figures/sparp-real.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A0.F7.3.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_bold" id="Pt0.A0.F7.4.2" style="font-size:90%;">Real-World Examples<span class="ltx_text ltx_font_medium" id="Pt0.A0.F7.4.2.1">: The input images are either sourced from amazon.com or captured using an iPhone.</span></span></figcaption>
</figure>
<section class="ltx_appendix" id="Pt0.A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>Additional Real-World Examples</h2>
<div class="ltx_para" id="Pt0.A1.p1">
<p class="ltx_p" id="Pt0.A1.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A0.F7" title="In SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">7</span></a>, we demonstrate that SpaRP can be applied to real-world sparse-view images without camera poses. This includes images captured by users with consumer devices (e.g., with an iPhone) or e-commerce product images (e.g., from amazon.com). SpaRP is capable of achieving commendable results in both pose estimation and 3D reconstruction.</p>
</div>
</section>
<section class="ltx_appendix" id="Pt0.A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.B </span>Ablation Studies on Number of Experts</h2>
<figure class="ltx_table" id="Pt0.A2.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A2.T5.10.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.11.2" style="font-size:90%;">Ablation Study on the Number of Experts for Pose Estimation.<span class="ltx_text ltx_font_medium" id="Pt0.A2.T5.11.2.1"> Evaluated on the OmniObject3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib85" title="">85</a>]</cite> and GSO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite> datasets.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Pt0.A2.T5.7">
<tr class="ltx_tr" id="Pt0.A2.T5.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="Pt0.A2.T5.1.1.1" rowspan="2"><span class="ltx_text" id="Pt0.A2.T5.1.1.1.1"><span class="ltx_text" id="Pt0.A2.T5.1.1.1.1.2"></span> <span class="ltx_text" id="Pt0.A2.T5.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="Pt0.A2.T5.1.1.1.1.1.1">
<span class="ltx_tr" id="Pt0.A2.T5.1.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Pt0.A2.T5.1.1.1.1.1.1.1.1"><math alttext="n_{init}" class="ltx_Math" display="inline" id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1"><semantics id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1a"><msub id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.2" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">n</mi><mrow id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">i</mi><mo id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.1" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">n</mi><mo id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.1a" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.4" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.4.cmml">i</mi><mo id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.1b" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.5" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1b"><apply id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.2">ùëõ</ci><apply id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3"><times id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.1"></times><ci id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.2">ùëñ</ci><ci id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.3">ùëõ</ci><ci id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.4.cmml" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.4">ùëñ</ci><ci id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.5.cmml" xref="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.1.1.1.1.1.1.1.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math></span></span>
<span class="ltx_tr" id="Pt0.A2.T5.1.1.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Pt0.A2.T5.1.1.1.1.1.1.2.1">experts</span></span>
</span></span> <span class="ltx_text" id="Pt0.A2.T5.1.1.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="Pt0.A2.T5.1.1.2">OmniObject3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib85" title="">85</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="Pt0.A2.T5.1.1.3">Google Scanned Objects¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.7.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A2.T5.2.2.1">Rot. Err <math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T5.2.2.1.m1.1"><semantics id="Pt0.A2.T5.2.2.1.m1.1a"><mo id="Pt0.A2.T5.2.2.1.m1.1.1" stretchy="false" xref="Pt0.A2.T5.2.2.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.2.2.1.m1.1b"><ci id="Pt0.A2.T5.2.2.1.m1.1.1.cmml" xref="Pt0.A2.T5.2.2.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.2.2.1.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A2.T5.3.3.2">Acc.@15<math alttext="{}^{\circ}\uparrow" class="ltx_math_unparsed" display="inline" id="Pt0.A2.T5.3.3.2.m1.1"><semantics id="Pt0.A2.T5.3.3.2.m1.1a"><mmultiscripts id="Pt0.A2.T5.3.3.2.m1.1.1"><mo id="Pt0.A2.T5.3.3.2.m1.1.1.2" stretchy="false">‚Üë</mo><mprescripts id="Pt0.A2.T5.3.3.2.m1.1.1a"></mprescripts><mrow id="Pt0.A2.T5.3.3.2.m1.1.1b"></mrow><mo id="Pt0.A2.T5.3.3.2.m1.1.1.3">‚àò</mo></mmultiscripts><annotation encoding="application/x-tex" id="Pt0.A2.T5.3.3.2.m1.1b">{}^{\circ}\uparrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.3.3.2.m1.1c">start_FLOATSUPERSCRIPT ‚àò end_FLOATSUPERSCRIPT ‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Pt0.A2.T5.4.4.3">Trans. Err<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T5.4.4.3.m1.1"><semantics id="Pt0.A2.T5.4.4.3.m1.1a"><mo id="Pt0.A2.T5.4.4.3.m1.1.1" stretchy="false" xref="Pt0.A2.T5.4.4.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.4.4.3.m1.1b"><ci id="Pt0.A2.T5.4.4.3.m1.1.1.cmml" xref="Pt0.A2.T5.4.4.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.4.4.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.4.4.3.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A2.T5.5.5.4">Rot. Err <math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T5.5.5.4.m1.1"><semantics id="Pt0.A2.T5.5.5.4.m1.1a"><mo id="Pt0.A2.T5.5.5.4.m1.1.1" stretchy="false" xref="Pt0.A2.T5.5.5.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.5.5.4.m1.1b"><ci id="Pt0.A2.T5.5.5.4.m1.1.1.cmml" xref="Pt0.A2.T5.5.5.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.5.5.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.5.5.4.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A2.T5.6.6.5">Acc.@15<math alttext="{}^{\circ}\uparrow" class="ltx_math_unparsed" display="inline" id="Pt0.A2.T5.6.6.5.m1.1"><semantics id="Pt0.A2.T5.6.6.5.m1.1a"><mmultiscripts id="Pt0.A2.T5.6.6.5.m1.1.1"><mo id="Pt0.A2.T5.6.6.5.m1.1.1.2" stretchy="false">‚Üë</mo><mprescripts id="Pt0.A2.T5.6.6.5.m1.1.1a"></mprescripts><mrow id="Pt0.A2.T5.6.6.5.m1.1.1b"></mrow><mo id="Pt0.A2.T5.6.6.5.m1.1.1.3">‚àò</mo></mmultiscripts><annotation encoding="application/x-tex" id="Pt0.A2.T5.6.6.5.m1.1b">{}^{\circ}\uparrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.6.6.5.m1.1c">start_FLOATSUPERSCRIPT ‚àò end_FLOATSUPERSCRIPT ‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A2.T5.7.7.6">Trans. Err<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T5.7.7.6.m1.1"><semantics id="Pt0.A2.T5.7.7.6.m1.1a"><mo id="Pt0.A2.T5.7.7.6.m1.1.1" stretchy="false" xref="Pt0.A2.T5.7.7.6.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.7.7.6.m1.1b"><ci id="Pt0.A2.T5.7.7.6.m1.1.1.cmml" xref="Pt0.A2.T5.7.7.6.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.7.7.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.7.7.6.m1.1d">‚Üì</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.7.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Pt0.A2.T5.7.8.1">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A2.T5.7.8.2">15.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A2.T5.7.8.3">0.495</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Pt0.A2.T5.7.8.4">1.04</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A2.T5.7.8.5">9.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A2.T5.7.8.6">0.562</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A2.T5.7.8.7">0.43</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.7.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="Pt0.A2.T5.7.9.1">2</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.7.9.2">12.05</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.7.9.3">0.585</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Pt0.A2.T5.7.9.4">0.76</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.7.9.5">6.03</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.7.9.6">0.687</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.7.9.7">0.28</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.7.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="Pt0.A2.T5.7.10.1">4</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.7.10.2">10.46</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.7.10.3">0.647</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Pt0.A2.T5.7.10.4">0.68</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.7.10.5">4.71</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.7.10.6">0.805</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.7.10.7">0.21</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.7.11">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Pt0.A2.T5.7.11.1">8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A2.T5.7.11.2"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.7.11.2.1">9.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A2.T5.7.11.3"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.7.11.3.1">0.690</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Pt0.A2.T5.7.11.4"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.7.11.4.1">0.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A2.T5.7.11.5"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.7.11.5.1">4.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A2.T5.7.11.6"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.7.11.6.1">0.853</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A2.T5.7.11.7"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.7.11.7.1">0.20</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="Pt0.A2.p1">
<p class="ltx_p" id="Pt0.A2.p1.2">Due to the inherently stochastic nature of diffusion models, our multi-view diffusion model may sometimes fail to accurately understand the spatial relationship between input images of objects and estimate their relative poses in a single diffusion pass, especially with objects that have some symmetry. We found that employing a Mixture of Experts (MoE) strategy effectively mitigates this issue. Specifically, we run the diffusion models <math alttext="n_{init}" class="ltx_Math" display="inline" id="Pt0.A2.p1.1.m1.1"><semantics id="Pt0.A2.p1.1.m1.1a"><msub id="Pt0.A2.p1.1.m1.1.1" xref="Pt0.A2.p1.1.m1.1.1.cmml"><mi id="Pt0.A2.p1.1.m1.1.1.2" xref="Pt0.A2.p1.1.m1.1.1.2.cmml">n</mi><mrow id="Pt0.A2.p1.1.m1.1.1.3" xref="Pt0.A2.p1.1.m1.1.1.3.cmml"><mi id="Pt0.A2.p1.1.m1.1.1.3.2" xref="Pt0.A2.p1.1.m1.1.1.3.2.cmml">i</mi><mo id="Pt0.A2.p1.1.m1.1.1.3.1" xref="Pt0.A2.p1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="Pt0.A2.p1.1.m1.1.1.3.3" xref="Pt0.A2.p1.1.m1.1.1.3.3.cmml">n</mi><mo id="Pt0.A2.p1.1.m1.1.1.3.1a" xref="Pt0.A2.p1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="Pt0.A2.p1.1.m1.1.1.3.4" xref="Pt0.A2.p1.1.m1.1.1.3.4.cmml">i</mi><mo id="Pt0.A2.p1.1.m1.1.1.3.1b" xref="Pt0.A2.p1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="Pt0.A2.p1.1.m1.1.1.3.5" xref="Pt0.A2.p1.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Pt0.A2.p1.1.m1.1b"><apply id="Pt0.A2.p1.1.m1.1.1.cmml" xref="Pt0.A2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Pt0.A2.p1.1.m1.1.1.1.cmml" xref="Pt0.A2.p1.1.m1.1.1">subscript</csymbol><ci id="Pt0.A2.p1.1.m1.1.1.2.cmml" xref="Pt0.A2.p1.1.m1.1.1.2">ùëõ</ci><apply id="Pt0.A2.p1.1.m1.1.1.3.cmml" xref="Pt0.A2.p1.1.m1.1.1.3"><times id="Pt0.A2.p1.1.m1.1.1.3.1.cmml" xref="Pt0.A2.p1.1.m1.1.1.3.1"></times><ci id="Pt0.A2.p1.1.m1.1.1.3.2.cmml" xref="Pt0.A2.p1.1.m1.1.1.3.2">ùëñ</ci><ci id="Pt0.A2.p1.1.m1.1.1.3.3.cmml" xref="Pt0.A2.p1.1.m1.1.1.3.3">ùëõ</ci><ci id="Pt0.A2.p1.1.m1.1.1.3.4.cmml" xref="Pt0.A2.p1.1.m1.1.1.3.4">ùëñ</ci><ci id="Pt0.A2.p1.1.m1.1.1.3.5.cmml" xref="Pt0.A2.p1.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.p1.1.m1.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.p1.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math> times with different random seeds to generate multiple
sets of NOCS maps for pose prediction, selecting the optimal one based on the minimum rendering loss from the pose refinement stage. As shown in¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A2.T5" title="In Appendix 0.B Ablation Studies on Number of Experts ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>, increasing the number of experts (<math alttext="n_{init}" class="ltx_Math" display="inline" id="Pt0.A2.p1.2.m2.1"><semantics id="Pt0.A2.p1.2.m2.1a"><msub id="Pt0.A2.p1.2.m2.1.1" xref="Pt0.A2.p1.2.m2.1.1.cmml"><mi id="Pt0.A2.p1.2.m2.1.1.2" xref="Pt0.A2.p1.2.m2.1.1.2.cmml">n</mi><mrow id="Pt0.A2.p1.2.m2.1.1.3" xref="Pt0.A2.p1.2.m2.1.1.3.cmml"><mi id="Pt0.A2.p1.2.m2.1.1.3.2" xref="Pt0.A2.p1.2.m2.1.1.3.2.cmml">i</mi><mo id="Pt0.A2.p1.2.m2.1.1.3.1" xref="Pt0.A2.p1.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="Pt0.A2.p1.2.m2.1.1.3.3" xref="Pt0.A2.p1.2.m2.1.1.3.3.cmml">n</mi><mo id="Pt0.A2.p1.2.m2.1.1.3.1a" xref="Pt0.A2.p1.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="Pt0.A2.p1.2.m2.1.1.3.4" xref="Pt0.A2.p1.2.m2.1.1.3.4.cmml">i</mi><mo id="Pt0.A2.p1.2.m2.1.1.3.1b" xref="Pt0.A2.p1.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="Pt0.A2.p1.2.m2.1.1.3.5" xref="Pt0.A2.p1.2.m2.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Pt0.A2.p1.2.m2.1b"><apply id="Pt0.A2.p1.2.m2.1.1.cmml" xref="Pt0.A2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="Pt0.A2.p1.2.m2.1.1.1.cmml" xref="Pt0.A2.p1.2.m2.1.1">subscript</csymbol><ci id="Pt0.A2.p1.2.m2.1.1.2.cmml" xref="Pt0.A2.p1.2.m2.1.1.2">ùëõ</ci><apply id="Pt0.A2.p1.2.m2.1.1.3.cmml" xref="Pt0.A2.p1.2.m2.1.1.3"><times id="Pt0.A2.p1.2.m2.1.1.3.1.cmml" xref="Pt0.A2.p1.2.m2.1.1.3.1"></times><ci id="Pt0.A2.p1.2.m2.1.1.3.2.cmml" xref="Pt0.A2.p1.2.m2.1.1.3.2">ùëñ</ci><ci id="Pt0.A2.p1.2.m2.1.1.3.3.cmml" xref="Pt0.A2.p1.2.m2.1.1.3.3">ùëõ</ci><ci id="Pt0.A2.p1.2.m2.1.1.3.4.cmml" xref="Pt0.A2.p1.2.m2.1.1.3.4">ùëñ</ci><ci id="Pt0.A2.p1.2.m2.1.1.3.5.cmml" xref="Pt0.A2.p1.2.m2.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.p1.2.m2.1c">n_{init}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.p1.2.m2.1d">italic_n start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math>) from 1 to 8 led to a significant improvement in the accuracy of relative pose predictions across both the OmniObject3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib85" title="">85</a>]</cite> and GSO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib12" title="">12</a>]</cite> datasets. This demonstrates that the MoE strategy is simple yet effective in improving the robustness of our pose prediction approach.</p>
</div>
</section>
<section class="ltx_appendix" id="Pt0.A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.C </span>Robustness to Varying Camera Intrinsics</h2>
<div class="ltx_para" id="Pt0.A3.p1">
<p class="ltx_p" id="Pt0.A3.p1.1">Our multi-view diffusion model demonstrates robust performance across varying input image camera intrinsics. During its training, we randomize both the focal length and optical center of input images. The input image field of view (FOV) follows a normal distribution <math alttext="\mathcal{N}(36^{\circ},9^{\circ})" class="ltx_Math" display="inline" id="Pt0.A3.p1.1.m1.2"><semantics id="Pt0.A3.p1.1.m1.2a"><mrow id="Pt0.A3.p1.1.m1.2.2" xref="Pt0.A3.p1.1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Pt0.A3.p1.1.m1.2.2.4" xref="Pt0.A3.p1.1.m1.2.2.4.cmml">ùí©</mi><mo id="Pt0.A3.p1.1.m1.2.2.3" xref="Pt0.A3.p1.1.m1.2.2.3.cmml">‚Å¢</mo><mrow id="Pt0.A3.p1.1.m1.2.2.2.2" xref="Pt0.A3.p1.1.m1.2.2.2.3.cmml"><mo id="Pt0.A3.p1.1.m1.2.2.2.2.3" stretchy="false" xref="Pt0.A3.p1.1.m1.2.2.2.3.cmml">(</mo><msup id="Pt0.A3.p1.1.m1.1.1.1.1.1" xref="Pt0.A3.p1.1.m1.1.1.1.1.1.cmml"><mn id="Pt0.A3.p1.1.m1.1.1.1.1.1.2" xref="Pt0.A3.p1.1.m1.1.1.1.1.1.2.cmml">36</mn><mo id="Pt0.A3.p1.1.m1.1.1.1.1.1.3" xref="Pt0.A3.p1.1.m1.1.1.1.1.1.3.cmml">‚àò</mo></msup><mo id="Pt0.A3.p1.1.m1.2.2.2.2.4" xref="Pt0.A3.p1.1.m1.2.2.2.3.cmml">,</mo><msup id="Pt0.A3.p1.1.m1.2.2.2.2.2" xref="Pt0.A3.p1.1.m1.2.2.2.2.2.cmml"><mn id="Pt0.A3.p1.1.m1.2.2.2.2.2.2" xref="Pt0.A3.p1.1.m1.2.2.2.2.2.2.cmml">9</mn><mo id="Pt0.A3.p1.1.m1.2.2.2.2.2.3" xref="Pt0.A3.p1.1.m1.2.2.2.2.2.3.cmml">‚àò</mo></msup><mo id="Pt0.A3.p1.1.m1.2.2.2.2.5" stretchy="false" xref="Pt0.A3.p1.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A3.p1.1.m1.2b"><apply id="Pt0.A3.p1.1.m1.2.2.cmml" xref="Pt0.A3.p1.1.m1.2.2"><times id="Pt0.A3.p1.1.m1.2.2.3.cmml" xref="Pt0.A3.p1.1.m1.2.2.3"></times><ci id="Pt0.A3.p1.1.m1.2.2.4.cmml" xref="Pt0.A3.p1.1.m1.2.2.4">ùí©</ci><interval closure="open" id="Pt0.A3.p1.1.m1.2.2.2.3.cmml" xref="Pt0.A3.p1.1.m1.2.2.2.2"><apply id="Pt0.A3.p1.1.m1.1.1.1.1.1.cmml" xref="Pt0.A3.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="Pt0.A3.p1.1.m1.1.1.1.1.1.1.cmml" xref="Pt0.A3.p1.1.m1.1.1.1.1.1">superscript</csymbol><cn id="Pt0.A3.p1.1.m1.1.1.1.1.1.2.cmml" type="integer" xref="Pt0.A3.p1.1.m1.1.1.1.1.1.2">36</cn><compose id="Pt0.A3.p1.1.m1.1.1.1.1.1.3.cmml" xref="Pt0.A3.p1.1.m1.1.1.1.1.1.3"></compose></apply><apply id="Pt0.A3.p1.1.m1.2.2.2.2.2.cmml" xref="Pt0.A3.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="Pt0.A3.p1.1.m1.2.2.2.2.2.1.cmml" xref="Pt0.A3.p1.1.m1.2.2.2.2.2">superscript</csymbol><cn id="Pt0.A3.p1.1.m1.2.2.2.2.2.2.cmml" type="integer" xref="Pt0.A3.p1.1.m1.2.2.2.2.2.2">9</cn><compose id="Pt0.A3.p1.1.m1.2.2.2.2.2.3.cmml" xref="Pt0.A3.p1.1.m1.2.2.2.2.2.3"></compose></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A3.p1.1.m1.2c">\mathcal{N}(36^{\circ},9^{\circ})</annotation><annotation encoding="application/x-llamapun" id="Pt0.A3.p1.1.m1.2d">caligraphic_N ( 36 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPT , 9 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPT )</annotation></semantics></math>, centered at 36 degrees. The optical center also follows a normal distribution centered at the image center.
As shown in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A3.F8" title="In Appendix 0.C Robustness to Varying Camera Intrinsics ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">8</span></a>,
we tested the model‚Äôs performance across input FOVs ranging from 5 to 65 degrees, covering common photographic focal lengths. Using 20 different objects, we calculated the average PSNR and LPIPS for predictions at various FOVs. Our model demonstrated consistently high performance across the tested range. This showcases its robustness to intrinsic variations in input images.</p>
</div>
<figure class="ltx_figure" id="Pt0.A3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="213" id="Pt0.A3.F8.g1" src="extracted/5793183/figures/supp_robust2intrinsics.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A3.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="Pt0.A3.F8.3.2" style="font-size:90%;">Our model achieves consistently high PSNR and low LPIPS across different input-view FOVs, with no significant performance degradation due to focal length variations.</span></figcaption>
</figure>
</section>
<section class="ltx_appendix" id="Pt0.A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.D </span>Sparse-View Reconstruction using the Estimated Poses</h2>
<div class="ltx_para" id="Pt0.A4.p1">
<p class="ltx_p" id="Pt0.A4.p1.1">Our estimated poses can benefit numerous downstream applications, including many existing sparse-view 3D reconstruction approaches that require camera poses. Here, we demonstrate how our estimated poses can be utilized with ZeroRF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib61" title="">61</a>]</cite>, a sparse-view 3D reconstruction method. ZeroRF is an optimization-based method that does not rely on pretrained priors and requires camera poses as input. As depicted in¬†<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#Pt0.A4.F9" title="In Appendix 0.D Sparse-View Reconstruction using the Estimated Poses ‚Ä£ SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">9</span></a>, by using only five images along with the corresponding predicted poses as input, ZeroRF is capable of generating a NeRF that synthesizes reasonable novel views. The resulting mesh also shows commendable global geometry, considering the challenging nature of the task.</p>
</div>
<figure class="ltx_figure" id="Pt0.A4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="387" id="Pt0.A4.F9.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="Pt0.A4.F9.3.2" style="font-size:90%;">The camera poses predicted by our method can be utilized in ZeroRF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib61" title="">61</a>]</cite>, which is an optimization-based sparse-view reconstruction method requiring camera poses as input. The input images are sourced from the ABO dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib7" title="">7</a>]</cite>.</span></figcaption>
</figure>
</section>
<section class="ltx_appendix" id="Pt0.A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.E </span>Evaluation Details</h2>
<section class="ltx_subsection" id="Pt0.A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.E.1 </span>3D Reconstruction</h3>
<div class="ltx_para" id="Pt0.A5.SS1.p1">
<p class="ltx_p" id="Pt0.A5.SS1.p1.1">To account for the scale and pose ambiguity of the generated mesh, we align the predicted mesh with the ground truth mesh prior to metric calculation. During the alignment process, we sample 12 rotations (<math alttext="30^{\circ}" class="ltx_Math" display="inline" id="Pt0.A5.SS1.p1.1.m1.1"><semantics id="Pt0.A5.SS1.p1.1.m1.1a"><msup id="Pt0.A5.SS1.p1.1.m1.1.1" xref="Pt0.A5.SS1.p1.1.m1.1.1.cmml"><mn id="Pt0.A5.SS1.p1.1.m1.1.1.2" xref="Pt0.A5.SS1.p1.1.m1.1.1.2.cmml">30</mn><mo id="Pt0.A5.SS1.p1.1.m1.1.1.3" xref="Pt0.A5.SS1.p1.1.m1.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="Pt0.A5.SS1.p1.1.m1.1b"><apply id="Pt0.A5.SS1.p1.1.m1.1.1.cmml" xref="Pt0.A5.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Pt0.A5.SS1.p1.1.m1.1.1.1.cmml" xref="Pt0.A5.SS1.p1.1.m1.1.1">superscript</csymbol><cn id="Pt0.A5.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="Pt0.A5.SS1.p1.1.m1.1.1.2">30</cn><compose id="Pt0.A5.SS1.p1.1.m1.1.1.3.cmml" xref="Pt0.A5.SS1.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A5.SS1.p1.1.m1.1c">30^{\circ}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A5.SS1.p1.1.m1.1d">30 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPT</annotation></semantics></math> apart) as initial positions and 10 scales from 0.6 to 1.4, which is dense enough in practice. We enumerate the combinations of these rotations and scales for initialization and subsequently refine the alignment with the Iterative Closest Point (ICP) algorithm. We select the alignment that yields the highest inlier ratio. Both the ground truth and predicted meshes are then scaled to fit within a unit bounding box.</p>
</div>
<div class="ltx_para" id="Pt0.A5.SS1.p2">
<p class="ltx_p" id="Pt0.A5.SS1.p2.1">We adopt the evaluation metrics from¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib32" title="">32</a>]</cite> to assess the reconstruction quality from two perspectives: (1) geometric quality and (2) texture quality. For geometric quality, we apply the F-score to quantify the discrepancy between the reconstructed and ground truth meshes, setting the F-score threshold at 0.05. To evaluate texture quality, we compute the CLIP-Similarity, PSNR, and LPIPS between images rendered from the reconstructed mesh and those of the ground truth. The meshes undergo rendering from 24 distinct viewpoints, encompassing a full 360-degree view around the object. The rendered images have a resolution of 512 <math alttext="\times" class="ltx_Math" display="inline" id="Pt0.A5.SS1.p2.1.m1.1"><semantics id="Pt0.A5.SS1.p2.1.m1.1a"><mo id="Pt0.A5.SS1.p2.1.m1.1.1" xref="Pt0.A5.SS1.p2.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="Pt0.A5.SS1.p2.1.m1.1b"><times id="Pt0.A5.SS1.p2.1.m1.1.1.cmml" xref="Pt0.A5.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A5.SS1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="Pt0.A5.SS1.p2.1.m1.1d">√ó</annotation></semantics></math> 512 pixels.</p>
</div>
</section>
<section class="ltx_subsection" id="Pt0.A5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.E.2 </span>Pose Estimation</h3>
<div class="ltx_para" id="Pt0.A5.SS2.p1">
<p class="ltx_p" id="Pt0.A5.SS2.p1.3">To evaluate pose estimation, we render five sparse views for each shape and assess the relative poses between all ten pairs of views. We convert the predicted poses to the OpenCV convention and report the median rotation error, rotation accuracy, and translation error across all pairs. The rotation error is the minimum angular deviation between the predicted and the ground truth poses. In contrast, the translation error is the absolute difference between the corresponding translation vectors. We present accuracies as the percentage of pose pairs with rotation errors below the thresholds of 15<sup class="ltx_sup" id="Pt0.A5.SS2.p1.3.1"><span class="ltx_text ltx_font_italic" id="Pt0.A5.SS2.p1.3.1.1">‚àò</span></sup> and 30<sup class="ltx_sup" id="Pt0.A5.SS2.p1.3.2"><span class="ltx_text ltx_font_italic" id="Pt0.A5.SS2.p1.3.2.1">‚àò</span></sup>.
It should be noted that iFusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib83" title="">83</a>]</cite> infers only the relative elevation, azimuth, and distance, and cannot provide the 4x4 camera matrix without the absolute camera pose of the reference image. For iFusion, we supplement the elevation angle of the reference image using an external elevation estimation method¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib34" title="">34</a>]</cite>, which has a median prediction error of 5<sup class="ltx_sup" id="Pt0.A5.SS2.p1.3.3"><span class="ltx_text ltx_font_italic" id="Pt0.A5.SS2.p1.3.3.1">‚àò</span></sup> on the GSO dataset. Additionally, many baseline methods do not require camera intrinsics as input, resulting in predicted poses with varying distances from the camera to the shape, as reflected by the magnitude of the translation vectors. To address this intrinsic ambiguity, we normalize the predicted translation vectors for each method by using a scale factor that aligns the first view‚Äôs predicted camera translation with the ground truth translation. After normalization, we report the absolute translation errors. Furthermore, in our ablation studies, we investigate the impact of the number of input views and the number of experts on pose estimation performance using subsets of 100 shapes.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="Pt0.A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.F </span>Details of Dataset Curation</h2>
<div class="ltx_para" id="Pt0.A6.p1">
<p class="ltx_p" id="Pt0.A6.p1.1">The Objaverse dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib9" title="">9</a>]</cite> contains about 800,000 shapes. However, this dataset includes numerous partial scans, scenes, and basic, textureless geometries that are unsuitable for our task of generating single objects. To optimize the training process in terms of efficacy and efficiency, we curate a high-quality subset consisting of single objects with high-fidelity geometry and vivid textural appearance. We begin by randomly selecting a subset of 3D models and then task annotators with assessing the overall geometry quality and evaluating texture aesthetic preferences. Subsequently, we train a simple network to predict such annotations.</p>
</div>
<div class="ltx_para" id="Pt0.A6.p2">
<p class="ltx_p" id="Pt0.A6.p2.1">For assessing overall geometry quality, annotators are required to assign one of three possible levels to each 3D model:</p>
</div>
<div class="ltx_para" id="Pt0.A6.p3">
<blockquote class="ltx_quote" id="Pt0.A6.p3.1">
<p class="ltx_p" id="Pt0.A6.p3.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A6.p3.1.1.1">High quality</span>: Objects that represent a single entity with a clear semantic meaning, such as avatars and animals.</p>
<p class="ltx_p" id="Pt0.A6.p3.1.2"><span class="ltx_text ltx_font_italic" id="Pt0.A6.p3.1.2.1">Medium quality</span>: Simple geometric shapes (e.g., cubes, spheres); geometries that are abstract or have unclear semantic meaning; and repetitive structures found in the Objaverse, such as skeletal frames of houses and staircases.</p>
<p class="ltx_p" id="Pt0.A6.p3.1.3"><span class="ltx_text ltx_font_italic" id="Pt0.A6.p3.1.3.1">Low quality</span>: Point clouds; scenes with multiple elements; incomplete, low-quality, or unidentifiable 3D scans.</p>
</blockquote>
</div>
<div class="ltx_para" id="Pt0.A6.p4">
<p class="ltx_p" id="Pt0.A6.p4.1">For texture preference, given the difficulty in defining absolute standards due to aesthetic subjectivity, we adopt a binary choice approach for annotation. This method presents annotators with pairs of 3D models, prompting them to select the one with superior texture quality or visual appeal.</p>
</div>
<div class="ltx_para" id="Pt0.A6.p5">
<p class="ltx_p" id="Pt0.A6.p5.1">Overall, we have recruited 10 annotators and collected labels for 4,000 pairs of shapes in total. Based on these annotations, we trained MLP networks to predict overall geometry quality ratings and texture scores, respectively. Both networks take the multimodal features of each shape as input, which include image, text, and 3D features, as encoded in OpenShape¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10195v1#bib.bib33" title="">33</a>]</cite>. The rating classification MLP predicts a one-hot encoded label across three levels, and is trained using the cross-entropy loss. Meanwhile, the texture scoring MLP regresses a score for each shape and is trained using a relative margin loss.</p>
</div>
<div class="ltx_para" id="Pt0.A6.p6">
<p class="ltx_p" id="Pt0.A6.p6.1">During the training of SpaRP, we utilized the trained MLPs to curate a subset of approximately 100,000 objects. These objects are rated as high-quality and possess texture scores within the top 20%.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 15 08:15:17 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
