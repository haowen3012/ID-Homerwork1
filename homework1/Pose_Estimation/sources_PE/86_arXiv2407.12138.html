<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Monocular pose estimation of articulated surgical instruments in open surgery</title>
<!--Generated on Tue Jul 16 19:36:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.12138v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S1" title="In Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#Sx1.SS1" title="In Related Work ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Object Pose Estimation Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#Sx1.SS2" title="In Related Work ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Pose Estimation Methods for Surgical Instruments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#Sx1.SS3" title="In Related Work ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Overview of Synthetic Data Generation for Object Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#Sx1.SS4" title="In Related Work ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Bridging the Domain Gap Between Synthetic and Real Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#Sx1.SS5" title="In Related Work ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5 </span>Data Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#Sx1.SS6" title="In Related Work ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.6 </span>Data Annotation for Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2" title="In Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS1" title="In 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Overview of the Proposed Pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS2" title="In 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Synthetic Data Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS2.SSS1" title="In 2.2 Synthetic Data Generation ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>3D Modeling of Surgical Instruments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS2.SSS2" title="In 2.2 Synthetic Data Generation ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Hand-Object Interaction Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS2.SSS3" title="In 2.2 Synthetic Data Generation ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Rendering Pipeline</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3" title="In 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Pose Estimation Framework</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS1" title="In 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Direct 6D Object Pose Estimation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS1.Px1" title="In 2.3.1 Direct 6D Object Pose Estimation ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">3D Rotation Parameterization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS1.Px2" title="In 2.3.1 Direct 6D Object Pose Estimation ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">3D Translation Parameterization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS1.Px3" title="In 2.3.1 Direct 6D Object Pose Estimation ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">2D-3D Dense Correspondence Maps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS1.Px4" title="In 2.3.1 Direct 6D Object Pose Estimation ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Patch-PnP</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS2" title="In 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Multitask Pose Losses</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS2.Px1" title="In 2.3.2 Multitask Pose Losses ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Pose Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS2.Px2" title="In 2.3.2 Multitask Pose Losses ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Geometry Loss (<math alttext="\mathcal{L}_{\text{Geom}}" class="ltx_Math" display="inline"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>Geom</mtext></msub><annotation-xml encoding="MathML-Content"><apply><csymbol cd="ambiguous">subscript</csymbol><ci>ℒ</ci><ci><mtext mathsize="70%">Geom</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex">\mathcal{L}_{\text{Geom}}</annotation><annotation encoding="application/x-llamapun">caligraphic_L start_POSTSUBSCRIPT Geom end_POSTSUBSCRIPT</annotation></semantics></math>)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS2.Px3" title="In 2.3.2 Multitask Pose Losses ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Articulation Loss (<math alttext="L_{\text{Art}}" class="ltx_Math" display="inline"><semantics><msub><mi>L</mi><mtext>Art</mtext></msub><annotation-xml encoding="MathML-Content"><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝐿</ci><ci><mtext mathsize="70%">Art</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex">L_{\text{Art}}</annotation><annotation encoding="application/x-llamapun">italic_L start_POSTSUBSCRIPT Art end_POSTSUBSCRIPT</annotation></semantics></math>)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS2.Px4" title="In 2.3.2 Multitask Pose Losses ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Object Category Loss (<math alttext="L_{\text{Cat}}" class="ltx_Math" display="inline"><semantics><msub><mi>L</mi><mtext>Cat</mtext></msub><annotation-xml encoding="MathML-Content"><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝐿</ci><ci><mtext mathsize="70%">Cat</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex">L_{\text{Cat}}</annotation><annotation encoding="application/x-llamapun">italic_L start_POSTSUBSCRIPT Cat end_POSTSUBSCRIPT</annotation></semantics></math>)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS3" title="In 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.3 </span>Pose Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS4" title="In 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.4 </span>Training Procedure</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS4.Px1" title="In 2.3.4 Training Procedure ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Stage 1: Training on Synthetic Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS4.Px2" title="In 2.3.4 Training Procedure ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Step 2: Object Detection Domain Adaptation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS4.Px3" title="In 2.3.4 Training Procedure ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Step 3: Pose Estimation Domain Adaptation</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3" title="In Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments, Results, and Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS1" title="In 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS1.SSS1" title="In 3.1 Experimental Setup ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Dataset and Labels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS1.SSS2" title="In 3.1 Experimental Setup ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Evaluation Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS1.SSS2.Px1" title="In 3.1.2 Evaluation Metrics ‣ 3.1 Experimental Setup ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">6D Pose Evaluation Metric</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS1.SSS2.Px2" title="In 3.1.2 Evaluation Metrics ‣ 3.1 Experimental Setup ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">2D Object Detection Metric</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS1.SSS3" title="In 3.1 Experimental Setup ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Results on Real-World Surgical Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS1.SSS3.Px1" title="In 3.1.3 Results on Real-World Surgical Data ‣ 3.1 Experimental Setup ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Pose Estimation Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS1.SSS3.Px2" title="In 3.1.3 Results on Real-World Surgical Data ‣ 3.1 Experimental Setup ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Object Detection Results</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS2" title="In 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS2.SSS1" title="In 3.2 Ablation Studies ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Impact of Synthetic Hand-Object Interactions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS2.SSS2" title="In 3.2 Ablation Studies ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Patch-PnP vs. PnP-RANSAC</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS2.SSS2.Px1" title="In 3.2.2 Patch-PnP vs. PnP-RANSAC ‣ 3.2 Ablation Studies ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Training strategy impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.SS2.SSS2.Px2" title="In 3.2.2 Patch-PnP vs. PnP-RANSAC ‣ 3.2 Ablation Studies ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Inference strategy impact</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S4" title="In Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Disscusion and Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Monocular pose estimation of
articulated surgical instruments
in open surgery</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Robert <span class="ltx_text" id="id4.1.id1" style="color:#FF0000;">Spektor</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:spektor@campus.technion.ac.il">spektor@campus.technion.ac.il</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tom <span class="ltx_text" id="id5.1.id1" style="color:#FF0000;">Friedman</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:t%CB%99friedman@rmabam.health.gov.il">t˙friedman@rmabam.health.gov.il</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Itay <span class="ltx_text" id="id6.1.id1" style="color:#FF0000;">Or</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:I%CB%99or@rmabam.health.gov.il">I˙or@rmabam.health.gov.il</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gil <span class="ltx_text" id="id7.1.id1" style="color:#FF0000;">Bolotin</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:g%CB%99bolotin@rmabam.health.gov.il">g˙bolotin@rmabam.health.gov.il</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shlomi <span class="ltx_text" id="id8.1.id1" style="color:#FF0000;">Laufer</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:laufer@technion.ac.il">laufer@technion.ac.il</a>
</span>
<span class="ltx_contact ltx_role_address">Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Haifa, Israel
</span>
<span class="ltx_contact ltx_role_address">Rambam Health Care Campus, Haifa, Israel
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.3">This work presents a novel approach to monocular 6D pose estimation of surgical instruments in open surgery, addressing challenges such as object articulations, symmetries, occlusions, and lack of annotated real-world data. The method leverages synthetic data generation and domain adaptation techniques to overcome these obstacles.
The proposed approach consists of three main components: <math alttext="(1)" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.2.2"><mo id="id1.1.m1.1.2.2.1" stretchy="false">(</mo><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">1</mn><mo id="id1.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn id="id1.1.m1.1.1.cmml" type="integer" xref="id1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">(1)</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">( 1 )</annotation></semantics></math> synthetic data generation using 3D modeling of surgical tools with articulation rigging and physically-based rendering; <math alttext="(2)" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1.2.2"><mo id="id2.2.m2.1.2.2.1" stretchy="false">(</mo><mn id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">2</mn><mo id="id2.2.m2.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><cn id="id2.2.m2.1.1.cmml" type="integer" xref="id2.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">(2)</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">( 2 )</annotation></semantics></math> a tailored pose estimation framework combining object detection with pose estimation and a hybrid geometric fusion strategy; and <math alttext="(3)" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mrow id="id3.3.m3.1.2.2"><mo id="id3.3.m3.1.2.2.1" stretchy="false">(</mo><mn id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">3</mn><mo id="id3.3.m3.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><cn id="id3.3.m3.1.1.cmml" type="integer" xref="id3.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">(3)</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">( 3 )</annotation></semantics></math> a training strategy that utilizes both synthetic and real unannotated data, employing domain adaptation on real video data using automatically generated pseudo-labels.
Evaluations conducted on videos of open surgery demonstrate the good performance and real-world applicability of the proposed method, highlighting its potential for integration into medical augmented reality and robotic systems. The approach eliminates the need for extensive manual annotation of real surgical data.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Object pose estimation is a fundamental problem in computer vision that aims to determine the position and orientation of an object in 3D space relative to a camera. It has a wide range of applications, including robotic manipulation <cite class="ltx_cite ltx_citemacro_citet">Tremblay et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib57" title="">57</a>]</cite>, autonomous navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib41" title="">41</a>]</cite>, and augmented reality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib50" title="">50</a>]</cite>. In the medical domain, accurate pose estimation of surgical instruments plays a crucial role in computer-assisted surgery, enabling advanced features such as surgical navigation, skill assessment, and robotic assistance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Traditional approaches to object pose estimation often rely on template matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib45" title="">45</a>]</cite> or key-point matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib31" title="">31</a>]</cite>. However, with the advent of deep learning, significant progress has been made in developing more robust and accurate pose estimation techniques using various input modalities, including RGB images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib47" title="">47</a>]</cite>, depth maps, and point clouds <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib59" title="">59</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Existing approaches to object pose estimation can be broadly categorized into two groups: marker-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib1" title="">1</a>]</cite> and markerless methods. Marker-based methods rely on attaching physical markers, such as color-coded fiducials <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib61" title="">61</a>]</cite> or passive reflective markers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib18" title="">18</a>]</cite>, to the instruments. These markers are then detected and tracked in the image space to estimate the pose. While marker-based methods can provide accurate pose estimates, they require modifying the instruments and may interfere with the surgical workflow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">On the other hand, markerless methods aim to estimate the pose directly from the visual appearance of the instruments in the image. These methods typically involve training deep neural networks on large datasets of annotated images. The networks learn to extract relevant features and estimate the 6D pose of the instruments using various techniques, such as landmark detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib46" title="">46</a>]</cite> or dense correspondence estimation with Perspective-n-Point (PnP) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib51" title="">51</a>]</cite>, or direct pose regression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib60" title="">60</a>]</cite>. Markerless methods offer a more practical and non-intrusive solution for surgical instrument pose estimation, as they do not require any modifications to the instruments.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">While markerless pose estimation methods have shown promising results, they face several challenges that hinder their widespread adoption in surgical settings. One of the primary challenges is the scarcity of annotated real-world data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib9" title="">9</a>]</cite>. Collecting and annotating large datasets of images with accurate 6D pose labels is time-consuming, expensive, and often impractical <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib57" title="">57</a>]</cite>. This challenge is particularly relevant in the medical domain, where privacy concerns and the difficulty of recording during surgeries due to camera placement restrictions (as some areas must remain sterile) further complicate the data collection process.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To address the data scarcity problem, researchers have explored the use of synthetic data generation techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib57" title="">57</a>]</cite>. By creating realistic virtual models of surgical instruments and rendering them in simulated environments, large amounts of annotated training data can be generated efficiently. However, models trained solely on synthetic data often suffer from the domain gap problem, where the learned features do not generalize well to real-world scenarios. To bridge this gap, domain adaptation techniques, such as domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib57" title="">57</a>]</cite> and adversarial training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib4" title="">4</a>]</cite>, have been proposed. These techniques aim to make the models more robust to the differences between synthetic and real data, improving their performance on real-world images.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="219" id="S1.F1.g1" src="extracted/5735855/graphics/2024-03-22_11-40.png" width="315"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 1: </span>Pose estimation of surgical instruments, showcasing the precision required in labeling articulated tools, such as needle holders and tweezers, during an open surgical procedure.</figcaption>
</figure>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Despite the significant progress in deep learning-based pose estimation methods, their application to surgical instruments poses additional challenges due to the unique characteristics of the surgical environment and the instruments themselves <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib15" title="">15</a>]</cite>. Surgical instruments often have metallic and reflective surfaces, which can cause severe specular reflections and make it difficult to extract reliable visual features. Moreover, articulated objects, such as surgical instruments with movable parts, introduces additional complexities. Articulated objects have multiple degrees of freedom, allowing their parts to move independently, which significantly increases the difficulty of accurately estimating their pose.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="120" id="S1.F2.g1" src="x1.png" width="161"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 2: </span>Surgical instruments with reflective surface</figcaption>
</figure>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In this study, we tackle the multiple challenges raised by of open surgery pose detection. Our key contributions include:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Creation of a new in-the-wild dataset of open surgery tool and hand segmentations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Development of a synthetic dataset for tool pose estimation that addresses issues like tool articulation and hand occlusions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Utilization of solely synthetic data to develop methods for both object detection and tool pose estimation. The pose estimation incorporates the standard six degrees of freedom as well as ool articulation (i.e., measuring how open the tools are).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Introduction of a sim2real domain adaptation technique that utilizes the temporal-spatial characteristics of video data to refine both object detection and pose estimation models, with each model helping to improve the other.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S1.p8.2">It is important to highlight that our approach eliminates the need for manual data labeling. New tools can be added to the dataset using only their 3D models (e.g., STL files) without manual labeling. Any labeling done in this study serves solely to validate our algorithms.
Addressing these challenges is crucial for developing accurate and reliable pose estimation methods for surgical instruments, which can ultimately lead to improved surgical outcomes and patient care.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Related Work</h2>
<section class="ltx_subsection" id="Sx1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Object Pose Estimation Methods</h3>
<div class="ltx_para" id="Sx1.SS1.p1">
<p class="ltx_p" id="Sx1.SS1.p1.1">Object pose estimation methods have seen significant advances in recent years, particularly with the rise of deep learning. In this section, we discuss state-of-the-art methods for both general object pose estimation and those specifically designed for surgical instruments.
<span class="ltx_text ltx_font_bold" id="Sx1.SS1.p1.1.1">Single-stage methods</span> directly regress the 6D pose of an object from the input image. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib62" title="">62</a>]</cite> predicts the 3D translation and rotation of an object in a single forward pass. Having separate branches for object classification and pose estimation. The pose estimation branch regresses the 3D translation and rotation in a quaternion representation.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib12" title="">12</a>]</cite>, extends the popular object detection framework Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib22" title="">22</a>]</cite> to jointly perform object detection and pose estimation. It introduces a pose estimation branch that operates on the region of interest(ROI) features extracted by the ROI pooling layer. It combines object detection and pose estimation in a single network, allowing pose estimation of multiple objects in an image. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib64" title="">64</a>]</cite> is a single-stage method that estimates 2D-3D dense correspondence maps on the full image and estimates the pose via PnP-RANSAC. Then, further refine the initial pose estimate using a custom deep learning model.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.SS1.p2">
<p class="ltx_p" id="Sx1.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="Sx1.SS1.p2.1.1">Two-stage methods</span> decouple object detection and pose estimation by first detecting the object of interest and then estimating its pose using a separate network. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib47" title="">47</a>]</cite> employs a segmentation-based approach to identify the 2D centers of the objects of interest. The segmentation network does not use a hourglass-shaped architecture, making it more efficient. It then predicts the 3D bounding box of the object and regresses the 6D pose from the cropped object image. The method employs a coarse-to-fine approach, where the initial pose estimate is refined using a more precise network.</p>
</div>
<div class="ltx_para" id="Sx1.SS1.p3">
<p class="ltx_p" id="Sx1.SS1.p3.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib32" title="">32</a>]</cite> disentangles the pose estimation process into predicting the object’s rotation and translation separately. This disentanglement reportedly leads to more accurate and robust pose estimation compared to previous methods.</p>
</div>
<div class="ltx_para" id="Sx1.SS1.p4">
<p class="ltx_p" id="Sx1.SS1.p4.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib44" title="">44</a>]</cite> employs a fully convolutional network to predict dense 2D-3D correspondences and utilizes a GAN to enhance the stability of these dense correspondences.</p>
</div>
<div class="ltx_para" id="Sx1.SS1.p5">
<p class="ltx_p" id="Sx1.SS1.p5.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib46" title="">46</a>]</cite> presents a voting-based approach to manage occluded or truncated keypoints. Each pixel votes for the object’s keypoints, and the pose is determined by solving a least-squares problem. The network generates pixel-wise heatmaps for each keypoint and employs a differentiable RANSAC layer to estimate the pose from the keypoint correspondences.</p>
</div>
<div class="ltx_para" id="Sx1.SS1.p6">
<p class="ltx_p" id="Sx1.SS1.p6.1">Several state-of-the-art deep learning architectures have been proposed to further improve the accuracy and robustness of object pose estimation.</p>
</div>
<div class="ltx_para" id="Sx1.SS1.p7">
<p class="ltx_p" id="Sx1.SS1.p7.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib51" title="">51</a>]</cite> focuses on surface encoding by assigning a unique code to each vertex on a 3D object model, starting with coarse fragments and refining to finer details. The method includes assigning these unique codes to 3D vertices, predicting their correspondence to image pixels, and estimating the object’s pose using a PnP-RANSAC-like approach.</p>
</div>
<div class="ltx_para" id="Sx1.SS1.p8">
<p class="ltx_p" id="Sx1.SS1.p8.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib60" title="">60</a>]</cite> employs a direct regression approach to efficiently and differentiably predict the 6D pose of an object in an end-to-end trainable manner. By utilizing intermediate geometric representations, the method guides and enhances the accuracy of pose prediction. It achieves competitive performance compared to state-of-the-art methods on standard benchmarks.</p>
</div>
<div class="ltx_para" id="Sx1.SS1.p9">
<p class="ltx_p" id="Sx1.SS1.p9.1">While these methods have shown impressive results on general object pose estimation benchmarks, they often struggle when applied directly to surgical instruments due to the unique challenges posed by the surgical environment.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Pose Estimation Methods for Surgical Instruments</h3>
<div class="ltx_para" id="Sx1.SS2.p1">
<p class="ltx_p" id="Sx1.SS2.p1.1">Several deep learning-based methods have been proposed specifically for surgical instrument pose estimation, addressing the unique challenges posed by the surgical environment. These methods can be further categorized based on the type of surgery they focus on, such as laparoscopic or open surgery.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.SS2.p2">
<p class="ltx_p" id="Sx1.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="Sx1.SS2.p2.1.1">Methods Focusing on Laparoscopic Instruments.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib21" title="">21</a>]</cite> present a method for detection, segmentation, and 3D pose estimation of laparoscopic instruments using CNN’s and algebraic geometry. They employ a two-stage approach, where the instrument is first detected and segmented using a CNN-based segmentation network. The segmented instrument is then processed by a separate CNN to predict its 3D pose using an algebraic geometry-based method. The authors demonstrate the effectiveness of their approach on a dataset of laparoscopic images, achieving high accuracy in instrument detection, segmentation, and pose estimation.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.SS2.p3">
<p class="ltx_p" id="Sx1.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="Sx1.SS2.p3.1.1">Methods addressing challenges in open surgery.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib23" title="">23</a>]</cite> introduce a markerless hand-tool tracking pipeline for open surgery, focusing on larger, non-articulated tools. They create a synthetic data generation pipeline using the MANO hand model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib49" title="">49</a>]</cite> and GraspIt! <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib40" title="">40</a>]</cite> simulator to produce realistic hand-tool interactions. The authors also capture real-world data with ground truth annotations in a mock operating room. They evaluate three baseline models (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib46" title="">46</a>]</cite>, HandObjectNet, and a combined model) trained on the synthetic dataset and refined on the real dataset. The best-performing model, HandObjectNet, achieves an average 3D vertex error of 16.7 mm on the synthetic test set and 13.8 mm on the real test set after refinement. This work demonstrates the potential of using synthetic data for pretraining and real-world data for fine-tuning pose estimation models in open surgery.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Overview of Synthetic Data Generation for Object Pose Estimation</h3>
<div class="ltx_para" id="Sx1.SS3.p1">
<p class="ltx_p" id="Sx1.SS3.p1.1">Synthetic data generation has gained significant attention in the object pose estimation community due to several reasons. First, annotating real-world data for object pose estimation is labor-intensive and error-prone, often requiring specialized equipment or manual labeling by experts. In contrast, synthetic data can be automatically generated with precise annotations, saving time and resources. Second, rendering synthetic data is fast and cheap compared to capturing real-world data, allowing researchers to create large-scale datasets with diverse object appearances, poses, backgrounds, and lighting conditions. This diversity can improve the robustness and generalization of trained object pose estimation models.</p>
</div>
<div class="ltx_para" id="Sx1.SS3.p2">
<p class="ltx_p" id="Sx1.SS3.p2.1">One of the most common approaches for generating synthetic data for object pose estimation is through 3D modeling and rendering. This involves creating 3D models of objects using computer graphics software and rendering them from different viewpoints to generate 2D images with corresponding pose annotations. The 3D models can be created manually using 3D modeling software or scanned from real objects using techniques like photogrammetry or depth sensing.</p>
</div>
<div class="ltx_para" id="Sx1.SS3.p3">
<p class="ltx_p" id="Sx1.SS3.p3.1">To create realistic synthetic images, physically-based rendering (PBR) techniques are often employed. PBR accurately simulates the flow of light energy in the scene by ray tracing, naturally accounting for complex illumination effects such as scattering, refraction, and reflection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib26" title="">26</a>]</cite>. The resulting rendered images look realistic and are often difficult to differentiate from real photographs.</p>
</div>
<div class="ltx_para" id="Sx1.SS3.p4">
<p class="ltx_p" id="Sx1.SS3.p4.1">In the context of object pose estimation, 3D models of objects are placed in virtual scenes and rendered from various viewpoints. The precise 3D poses of the objects in these rendered images can be automatically recorded, providing accurate ground-truth annotations for training and evaluating object pose estimation models. By varying the object poses, backgrounds, and lighting conditions in the virtual scenes, researchers can generate diverse synthetic datasets that cover a wide range of scenarios.</p>
</div>
<div class="ltx_para" id="Sx1.SS3.p5">
<p class="ltx_p" id="Sx1.SS3.p5.1">The effectiveness of using synthetic data generated through 3D modeling and PBR for object pose estimation has been demonstrated in various studies. In the BOP challenge 2020 on 6D object localization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib26" title="">26</a>]</cite>, methods trained on PBR images achieved significantly higher accuracy scores compared to those trained on ”render &amp; paste” images. While incorporating real training images further improved the scores, competitive results were obtained using only PBR images. Notably, the increased photorealism of PBR images led to clear improvements even for methods that apply strong data augmentation, such as CosyPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4 </span>Bridging the Domain Gap Between Synthetic and Real Data</h3>
<div class="ltx_para" id="Sx1.SS4.p1">
<p class="ltx_p" id="Sx1.SS4.p1.1">While synthetic data generation has shown great promise in training deep learning models for object pose estimation, there often exists a domain gap between synthetic and real-world data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib57" title="">57</a>]</cite>. This domain gap can limit the performance of models trained solely on synthetic data when applied to real-world scenarios. Various techniques have been proposed to address this issue to bridge the domain gap and improve the transferability of models trained on synthetic data to real-world applications.</p>
</div>
<div class="ltx_para" id="Sx1.SS4.p2">
<p class="ltx_p" id="Sx1.SS4.p2.1">Domain adaptation techniques:</p>
<ol class="ltx_enumerate" id="Sx1.I2">
<li class="ltx_item" id="Sx1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx1.I2.i1.p1">
<p class="ltx_p" id="Sx1.I2.i1.p1.1">Fine-tuning: One common approach is to fine-tune models that are pre-trained on synthetic data using a smaller dataset of real-world images. This allows the models to adapt to the characteristics of real-world data while leveraging the knowledge learned from synthetic data. Fine-tuning can be performed on the entire model or only on specific layers, depending on the similarity between the synthetic and real-world domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib56" title="">56</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx1.I2.i2.p1">
<p class="ltx_p" id="Sx1.I2.i2.p1.1">Unsupervised domain adaptation: Unsupervised domain adaptation techniques aim to align the feature distributions of synthetic and real-world data without requiring labeled real-world examples. These techniques often involve adversarial training, where a discriminator network is trained to distinguish between features from the synthetic and real-world domains, while the pose estimation network is trained to fool the discriminator. By minimizing the domain discrepancy, unsupervised domain adaptation can improve the performance of object pose estimation models on real-world data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="Sx1.SS4.p3">
<p class="ltx_p" id="Sx1.SS4.p3.1">Examples of bridging the domain gap:
Several works have successfully demonstrated the effectiveness of bridging the domain gap between synthetic and real data for object pose estimation. For instance, the work by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib23" title="">23</a>]</cite> on hand-tool interaction in open surgery utilizes a combination of synthetic pretraining and real-world fine-tuning. They first train their models on a large-scale synthetic dataset generated using a physically-based rendering pipeline and then fine-tune the models on a smaller dataset of real-world images captured in a mock operating room. This approach allows them to leverage the diversity and accuracy of synthetic data while adapting the models to the characteristics of real-world surgical scenes.</p>
</div>
<div class="ltx_para" id="Sx1.SS4.p4">
<p class="ltx_p" id="Sx1.SS4.p4.1">Another example is the work by Sundermeyer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib53" title="">53</a>]</cite>, where they propose a self-supervised domain adaptation method for object pose estimation. Their approach relies on a differentiable renderer to generate synthetic views of objects and align them with real-world images using a contrastive loss. By minimizing the domain discrepancy between synthetic and real-world images, their method can improve the performance of object pose estimation models on real-world data without requiring any labeled real-world examples.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.5 </span>Data Collection</h3>
<div class="ltx_para" id="Sx1.SS5.p1">
<p class="ltx_p" id="Sx1.SS5.p1.1">We collected real-world data from surgical procedures for the purpose of unsupervised sim-to-real domain adaptation and to validate our approach and ensure its effectiveness in practical surgical scenarios. Data were collected from seven different surgeries. Each surgery involved the removal of a vein from the leg as part of the “Coronary Artery Bypass Graft (CABG)” procedure. Our focus is specifically on the suturing part of the procedure, which typically lasts between 15 to 30 minutes. In total, four different surgeons performed the surgeries, with each suturing procedure carried out by a single surgeon. The study was approved by the Rambam Health Care Campus IRB committee.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.6 </span>Data Annotation for Evaluation</h3>
<div class="ltx_para" id="Sx1.SS6.p1">
<p class="ltx_p" id="Sx1.SS6.p1.1">Since annotating 6D pose is very complex, we instead annotate instance segmentation labels and evaluate on them. Approximately 100 images were selected from each procedure, covering a variety of tool poses and challenging scenarios. These frames were manually annotated.</p>
</div>
<div class="ltx_para" id="Sx1.SS6.p2">
<p class="ltx_p" id="Sx1.SS6.p2.1">For each selected frame, we created detailed segmentation masks of the surgical tools of interest (needle-holder and tweezers), as well as background surgical tools and hands. These segmentation masks provide pixel-level annotations of the tools and hands in the surgical scene. The annotation process was carried out using the SAM (Segment Anything) model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib28" title="">28</a>]</cite>, which generates high-quality object masks from input prompts such as points or boxes. Any masks that did not meet quality standards were manually adjusted. These annotated frames provide essential ground truth data for evaluating the effectiveness of our pose estimation network on actual surgical data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview of the Proposed Pipeline</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The proposed pipeline addresses the unique challenges of the surgical environment, such as the scarcity of annotated real-world data, the geometry of articulated instruments, and the presence of occlusions.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The core goal is to enable accurate pose estimation without extensive manual annotation of real surgical data. To achieve this, a three-component pipeline is proposed, consisting of synthetic data generation, a tailored pose estimation framework, and a training strategy that leverages both synthetic and real unannotated data.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic Data Generation</h3>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="686" id="S2.F3.g1" src="extracted/5735855/graphics/data_collection_photo.jpg" width="1220"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 3: </span>Needle-holder photographed on a spinning table for photogrammetry.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Synthetic data helps overcome the challenges associated with acquiring and annotating real-world data, which is particularly difficult in the surgical domain due to privacy concerns and the complexity of capturing ground truth poses.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>3D Modeling of Surgical Instruments</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">We decided to create our own CAD models of the surgical instruments instead of relying on existing CAD models. This decision was driven by the need for highly realistic and accurate representations, as well as the unavailability of CAD models.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">To prepare surgical instruments for photogrammetry, we spray them with a dust spray to minimize glare and reflections that could interfere with the scanning process. We found that dry shampoo worked better as a dust spray than specialized sprays for 3D scanning. It resulted in a better 3D reconstruction and is more cost-effective. The tools are then placed on a spinning table and photographed 360 degrees at three different heights. This ensures that every detail and surface of the instruments is accurately captured.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1">The captured images are processed using Reality Capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib14" title="">14</a>]</cite>, a professional photogrammetry software. Once the initial 3D meshes are generated, we perform a series of refinement steps to enhance their quality and realism. We use Blender <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib8" title="">8</a>]</cite>, a popular free and open source 3D modeling and animation software, to clean the meshes by removing noise and artifacts introduced during the photogrammetry process. Additionally, we employ Blender’s smoothing and hole-filling tools to improve the mesh surface quality.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p4">
<p class="ltx_p" id="S2.SS2.SSS1.p4.1">To simulate the articulation of the surgical instruments, we used two different techniques. For the needle holder, which consists of two rigid parts, we separated the model into two parts. For the tweezers, which are made from a single part, the opening and closing introduce bending. To simulate this bending, we used Blender’s rigging and weight-painting tools. Rigging involves creating a virtual skeleton within the 3D mesh, allowing us to define the movement of different parts. Weight painting assigns influence levels to the bones, which control how the mesh deforms when the bones move. This combination allows us to simulate the bending motion of the tweezers, ensuring realistic articulation.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p5">
<p class="ltx_p" id="S2.SS2.SSS1.p5.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.F4" title="Fig. 4 ‣ 2.2.1 3D Modeling of Surgical Instruments ‣ 2.2 Synthetic Data Generation ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">4</span></a> shows the tools we scanned at different articulation angles. After rigging the meshes, we exported them in different articulation angles. Specifically, we exported 15 different meshes for the needle holder and 10 meshes for the tweezers, each representing a different articulation angle.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="174" id="S2.F4.g1" src="extracted/5735855/graphics/tools_opening2.png" width="242"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 4: </span>Generated synthetic surgical tools in varying degrees of articulation</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Hand-Object Interaction Modeling</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Occlusions of hand grasps are commonly encountered in surgical scenarios. We simulate them with a generative hand-grasp model called ContactGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib36" title="">36</a>]</cite> in combination with the MANO hand mesh <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib48" title="">48</a>]</cite>. We used the model from the official implementation, which was trained on the GRAB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib11" title="">11</a>]</cite>. The GRAB dataset includes real human grasps for 51 objects from 10 different subjects.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">We generate a diverse set of hand-object grasps by varying the grip positions, hand orientations, and contact points between the hand and the surgical instruments. By incorporating these hand-object grasps into our synthetic data, we introduce realistic hand occlusions and interactions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Rendering Pipeline</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">With the rigged 3D models of the surgical instruments and the hand-object grasps, we proceed to render a large synthetic dataset using Blender. Our rendering pipeline generates diverse and realistic images with varying backgrounds and lighting conditions.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<p class="ltx_p" id="S2.SS2.SSS3.p2.1">We create virtual environments within Blender, including operating room scenes and other indoor environments. To enhance the realism and variability of the rendered images, we incorporate HDRI (High Dynamic Range Imaging) maps, which provide high-quality lighting and reflections. These HDRI maps cover a wide range of lighting scenarios, from surgical settings to other indoor environments, adding diversity to our synthetic dataset.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p3">
<p class="ltx_p" id="S2.SS2.SSS3.p3.1">During the rendering process, we randomize various aspects of the scene to increase the diversity of the synthetic dataset. This includes randomizing the positions and orientations of the surgical instruments, the hand-object grasps, and the camera viewpoints. We also apply random variations to the material properties of the instruments, such as their color and surface properties, to introduce visual variability.</p>
</div>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="621" id="S2.F5.g1" src="extracted/5735855/graphics/synthetic_data/synthetic_data2.png" width="998"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 5: </span>Synthetic data of surgical tools</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS3.p4">
<p class="ltx_p" id="S2.SS2.SSS3.p4.1">In total, we generate 80,000 synthetic images using our rendering pipeline, utilizing BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib16" title="">16</a>]</cite>. Each image is accompanied by precise ground-truth annotations, including the 6D pose of the surgical instruments and the articulation angles. This comprehensive synthetic dataset serves as a valuable resource for training our pose estimation network. A sample from this dataset is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.F5" title="Fig. 5 ‣ 2.2.3 Rendering Pipeline ‣ 2.2 Synthetic Data Generation ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Pose Estimation Framework</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Our pose estimation framework follows a two-stage approach, consisting of an object detection stage followed by a pose estimation stage. The first stage involves detecting the objects of interest in the input image. The second stage takes the detected object crops and estimates the 6D pose, object class, and articulation angle.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">The object detection stage is handled by YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib27" title="">27</a>]</cite>, which is trained to detect and localize the surgical tools in the input image. The detector provides bounding boxes around the detected tools, along with confidence scores indicating the likelihood of each surgical tool class.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">The pose estimation stage takes as input cropped object regions from the detector and processes them through the pose estimation network. The pose estimation network estimates the 6D pose, object class, and articulation angle of the surgical tool.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Direct 6D Object Pose Estimation</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">As briefly mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#Sx1" title="Related Work ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_title">Related Work</span></a>, our pose estimation method draws inspiration from GDR-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib60" title="">60</a>]</cite> for direct 6D object pose estimation, specifically the GDRNPP implementation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib37" title="">37</a>]</cite>, which incorporates several improvements over the original GDR-Net presented in the conference version. It is worth noting that GDRNPP won most awards in the 2022 BOP challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib52" title="">52</a>]</cite> for “6D localization of seen objects” and “2D detection of seen objects”, demonstrating its state-of-the-art performance. We adopt several key components from GDR-Net, including the 3D rotation parameterization, 3D translation parameterization, 2D-3D dense correspondence maps, Patch-PnP, and the multitask 6D pose loss, which are described in the following paragraphs.</p>
</div>
<section class="ltx_paragraph" id="S2.SS3.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">3D Rotation Parameterization</h5>
<div class="ltx_para" id="S2.SS3.SSS1.Px1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.Px1.p1.1">For the 3D rotation, we adopt the 6D rotation representation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib67" title="">67</a>]</cite> instead of commonly used representations like quaternions or Euler angles. Quaternions and Euler angles suffer from ambiguities and discontinuities, which can hinder the learning process and lead to suboptimal pose estimates. The 6D rotation representation provides a continuous and unambiguous way to represent rotations.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.Px1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.Px1.p2.4">The 6D rotation representation consists of two 3D vectors, denoted <math alttext="r_{1}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px1.p2.1.m1.1"><semantics id="S2.SS3.SSS1.Px1.p2.1.m1.1a"><msub id="S2.SS3.SSS1.Px1.p2.1.m1.1.1" xref="S2.SS3.SSS1.Px1.p2.1.m1.1.1.cmml"><mi id="S2.SS3.SSS1.Px1.p2.1.m1.1.1.2" xref="S2.SS3.SSS1.Px1.p2.1.m1.1.1.2.cmml">r</mi><mn id="S2.SS3.SSS1.Px1.p2.1.m1.1.1.3" xref="S2.SS3.SSS1.Px1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px1.p2.1.m1.1b"><apply id="S2.SS3.SSS1.Px1.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS1.Px1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px1.p2.1.m1.1.1.1.cmml" xref="S2.SS3.SSS1.Px1.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px1.p2.1.m1.1.1.2.cmml" xref="S2.SS3.SSS1.Px1.p2.1.m1.1.1.2">𝑟</ci><cn id="S2.SS3.SSS1.Px1.p2.1.m1.1.1.3.cmml" type="integer" xref="S2.SS3.SSS1.Px1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px1.p2.1.m1.1c">r_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px1.p2.1.m1.1d">italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="r_{2}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px1.p2.2.m2.1"><semantics id="S2.SS3.SSS1.Px1.p2.2.m2.1a"><msub id="S2.SS3.SSS1.Px1.p2.2.m2.1.1" xref="S2.SS3.SSS1.Px1.p2.2.m2.1.1.cmml"><mi id="S2.SS3.SSS1.Px1.p2.2.m2.1.1.2" xref="S2.SS3.SSS1.Px1.p2.2.m2.1.1.2.cmml">r</mi><mn id="S2.SS3.SSS1.Px1.p2.2.m2.1.1.3" xref="S2.SS3.SSS1.Px1.p2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px1.p2.2.m2.1b"><apply id="S2.SS3.SSS1.Px1.p2.2.m2.1.1.cmml" xref="S2.SS3.SSS1.Px1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px1.p2.2.m2.1.1.1.cmml" xref="S2.SS3.SSS1.Px1.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px1.p2.2.m2.1.1.2.cmml" xref="S2.SS3.SSS1.Px1.p2.2.m2.1.1.2">𝑟</ci><cn id="S2.SS3.SSS1.Px1.p2.2.m2.1.1.3.cmml" type="integer" xref="S2.SS3.SSS1.Px1.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px1.p2.2.m2.1c">r_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px1.p2.2.m2.1d">italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, which correspond to the first two columns of the rotation matrix <math alttext="\mathbf{R}{\in}\textit{SO}(3)" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px1.p2.3.m3.1"><semantics id="S2.SS3.SSS1.Px1.p2.3.m3.1a"><mrow id="S2.SS3.SSS1.Px1.p2.3.m3.1.2" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.cmml"><mi id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.2" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.2.cmml">𝐑</mi><mo id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.1" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.1.cmml">∈</mo><mrow id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.cmml"><mtext class="ltx_mathvariant_italic" id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.2" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.2a.cmml">SO</mtext><mo id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.1" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.1.cmml">⁢</mo><mrow id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.3.2" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.cmml"><mo id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.3.2.1" stretchy="false" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.cmml">(</mo><mn id="S2.SS3.SSS1.Px1.p2.3.m3.1.1" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.1.cmml">3</mn><mo id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.3.2.2" stretchy="false" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px1.p2.3.m3.1b"><apply id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.cmml" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2"><in id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.1.cmml" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.1"></in><ci id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.2.cmml" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.2">𝐑</ci><apply id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.cmml" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3"><times id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.1.cmml" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.1"></times><ci id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.2a.cmml" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.2"><mtext class="ltx_mathvariant_italic" id="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.2.cmml" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.2.3.2">SO</mtext></ci><cn id="S2.SS3.SSS1.Px1.p2.3.m3.1.1.cmml" type="integer" xref="S2.SS3.SSS1.Px1.p2.3.m3.1.1">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px1.p2.3.m3.1c">\mathbf{R}{\in}\textit{SO}(3)</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px1.p2.3.m3.1d">bold_R ∈ SO ( 3 )</annotation></semantics></math>. The network predicts these two columns using the Patch-PnP module. To ensure the orthogonality and validity of the rotation matrix, Gram-Schmidt process is applied to the predicted columns. This process orthogonalizes the columns and computes the third column to complete the rotation matrix. The resulting rotation matrix <math alttext="R=[r_{1},r_{2},r_{3}]" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px1.p2.4.m4.3"><semantics id="S2.SS3.SSS1.Px1.p2.4.m4.3a"><mrow id="S2.SS3.SSS1.Px1.p2.4.m4.3.3" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.cmml"><mi id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.5" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.5.cmml">R</mi><mo id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.4" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.4.cmml">=</mo><mrow id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.4.cmml"><mo id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.4" stretchy="false" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.4.cmml">[</mo><msub id="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1" xref="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.cmml"><mi id="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.2" xref="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.2.cmml">r</mi><mn id="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.3" xref="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.5" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.4.cmml">,</mo><msub id="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2" xref="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.cmml"><mi id="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.2" xref="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.2.cmml">r</mi><mn id="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.3" xref="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.6" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.4.cmml">,</mo><msub id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.cmml"><mi id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.2" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.2.cmml">r</mi><mn id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.3" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.3.cmml">3</mn></msub><mo id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.7" stretchy="false" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px1.p2.4.m4.3b"><apply id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3"><eq id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.4.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.4"></eq><ci id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.5.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.5">𝑅</ci><list id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.4.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3"><apply id="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.2.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.2">𝑟</ci><cn id="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.3.cmml" type="integer" xref="S2.SS3.SSS1.Px1.p2.4.m4.1.1.1.1.1.3">1</cn></apply><apply id="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.1.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.2">𝑟</ci><cn id="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.3.cmml" type="integer" xref="S2.SS3.SSS1.Px1.p2.4.m4.2.2.2.2.2.3">2</cn></apply><apply id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.1.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3">subscript</csymbol><ci id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.2.cmml" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.2">𝑟</ci><cn id="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.3.cmml" type="integer" xref="S2.SS3.SSS1.Px1.p2.4.m4.3.3.3.3.3.3">3</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px1.p2.4.m4.3c">R=[r_{1},r_{2},r_{3}]</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px1.p2.4.m4.3d">italic_R = [ italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ]</annotation></semantics></math> is guaranteed to be orthogonal and represents a valid 3D rotation.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.Px1.p3">
<p class="ltx_p" id="S2.SS3.SSS1.Px1.p3.1">In the context of working with object crops, we focus on predicting the allocentric representation of an object’s rotation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib29" title="">29</a>]</cite>. This approach is independent of the viewer’s perspective and assumes that the camera has rotated from its original position to point toward the center of the object’s Region of Interest (RoI). This representation eliminates the need to account for variable camera-object positioning, simplifying the rotational estimation process. Once the allocentric rotation is determined, it can be converted into the true 3D rotation (egocentric representation) by using the object’s 3D translation and the camera’s intrinsic parameters <math alttext="K" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px1.p3.1.m1.1"><semantics id="S2.SS3.SSS1.Px1.p3.1.m1.1a"><mi id="S2.SS3.SSS1.Px1.p3.1.m1.1.1" xref="S2.SS3.SSS1.Px1.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px1.p3.1.m1.1b"><ci id="S2.SS3.SSS1.Px1.p3.1.m1.1.1.cmml" xref="S2.SS3.SSS1.Px1.p3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px1.p3.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px1.p3.1.m1.1d">italic_K</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib29" title="">29</a>]</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">3D Translation Parameterization</h5>
<div class="ltx_para" id="S2.SS3.SSS1.Px2.p1">
<p class="ltx_p" id="S2.SS3.SSS1.Px2.p1.1">Direct prediction of translation from a Region of Interest (RoI) is impossible without additional information. To address this, we use a parametrization technique known as the scale-invariant translation estimate (SITE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib32" title="">32</a>]</cite>. SITE represents the translation as a combination of the 2D center coordinates of the object in the image plane and the depth of the object relative to the camera. The network predicts three components: <math alttext="(\delta_{x},\delta_{y},\delta_{z})" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p1.1.m1.3"><semantics id="S2.SS3.SSS1.Px2.p1.1.m1.3a"><mrow id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.4.cmml"><mo id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.4" stretchy="false" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.4.cmml">(</mo><msub id="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1" xref="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.cmml"><mi id="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.2" xref="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.2.cmml">δ</mi><mi id="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.3" xref="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.3.cmml">x</mi></msub><mo id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.5" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.4.cmml">,</mo><msub id="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2" xref="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.cmml"><mi id="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.2" xref="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.2.cmml">δ</mi><mi id="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.3" xref="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.3.cmml">y</mi></msub><mo id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.6" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.4.cmml">,</mo><msub id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.cmml"><mi id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.2" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.2.cmml">δ</mi><mi id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.3" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.3.cmml">z</mi></msub><mo id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.7" stretchy="false" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p1.1.m1.3b"><vector id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.4.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3"><apply id="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.2.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.2">𝛿</ci><ci id="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.3.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.1.1.1.1.3">𝑥</ci></apply><apply id="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.1.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.2">𝛿</ci><ci id="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.3.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.2.2.2.2.3">𝑦</ci></apply><apply id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.1.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.2.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.2">𝛿</ci><ci id="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.3.cmml" xref="S2.SS3.SSS1.Px2.p1.1.m1.3.3.3.3.3">𝑧</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p1.1.m1.3c">(\delta_{x},\delta_{y},\delta_{z})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p1.1.m1.3d">( italic_δ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_δ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , italic_δ start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.Px2.p2">
<p class="ltx_p" id="S2.SS3.SSS1.Px2.p2.6"><math alttext="\delta_{x}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p2.1.m1.1"><semantics id="S2.SS3.SSS1.Px2.p2.1.m1.1a"><msub id="S2.SS3.SSS1.Px2.p2.1.m1.1.1" xref="S2.SS3.SSS1.Px2.p2.1.m1.1.1.cmml"><mi id="S2.SS3.SSS1.Px2.p2.1.m1.1.1.2" xref="S2.SS3.SSS1.Px2.p2.1.m1.1.1.2.cmml">δ</mi><mi id="S2.SS3.SSS1.Px2.p2.1.m1.1.1.3" xref="S2.SS3.SSS1.Px2.p2.1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p2.1.m1.1b"><apply id="S2.SS3.SSS1.Px2.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p2.1.m1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p2.1.m1.1.1.2.cmml" xref="S2.SS3.SSS1.Px2.p2.1.m1.1.1.2">𝛿</ci><ci id="S2.SS3.SSS1.Px2.p2.1.m1.1.1.3.cmml" xref="S2.SS3.SSS1.Px2.p2.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p2.1.m1.1c">\delta_{x}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p2.1.m1.1d">italic_δ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\delta_{y}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p2.2.m2.1"><semantics id="S2.SS3.SSS1.Px2.p2.2.m2.1a"><msub id="S2.SS3.SSS1.Px2.p2.2.m2.1.1" xref="S2.SS3.SSS1.Px2.p2.2.m2.1.1.cmml"><mi id="S2.SS3.SSS1.Px2.p2.2.m2.1.1.2" xref="S2.SS3.SSS1.Px2.p2.2.m2.1.1.2.cmml">δ</mi><mi id="S2.SS3.SSS1.Px2.p2.2.m2.1.1.3" xref="S2.SS3.SSS1.Px2.p2.2.m2.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p2.2.m2.1b"><apply id="S2.SS3.SSS1.Px2.p2.2.m2.1.1.cmml" xref="S2.SS3.SSS1.Px2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p2.2.m2.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p2.2.m2.1.1.2.cmml" xref="S2.SS3.SSS1.Px2.p2.2.m2.1.1.2">𝛿</ci><ci id="S2.SS3.SSS1.Px2.p2.2.m2.1.1.3.cmml" xref="S2.SS3.SSS1.Px2.p2.2.m2.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p2.2.m2.1c">\delta_{y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p2.2.m2.1d">italic_δ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math> represent the offset of the object’s center with respect to the center of the bounding box in the image plane, normalized by the bounding size <math alttext="(w,h)" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p2.3.m3.2"><semantics id="S2.SS3.SSS1.Px2.p2.3.m3.2a"><mrow id="S2.SS3.SSS1.Px2.p2.3.m3.2.3.2" xref="S2.SS3.SSS1.Px2.p2.3.m3.2.3.1.cmml"><mo id="S2.SS3.SSS1.Px2.p2.3.m3.2.3.2.1" stretchy="false" xref="S2.SS3.SSS1.Px2.p2.3.m3.2.3.1.cmml">(</mo><mi id="S2.SS3.SSS1.Px2.p2.3.m3.1.1" xref="S2.SS3.SSS1.Px2.p2.3.m3.1.1.cmml">w</mi><mo id="S2.SS3.SSS1.Px2.p2.3.m3.2.3.2.2" xref="S2.SS3.SSS1.Px2.p2.3.m3.2.3.1.cmml">,</mo><mi id="S2.SS3.SSS1.Px2.p2.3.m3.2.2" xref="S2.SS3.SSS1.Px2.p2.3.m3.2.2.cmml">h</mi><mo id="S2.SS3.SSS1.Px2.p2.3.m3.2.3.2.3" stretchy="false" xref="S2.SS3.SSS1.Px2.p2.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p2.3.m3.2b"><interval closure="open" id="S2.SS3.SSS1.Px2.p2.3.m3.2.3.1.cmml" xref="S2.SS3.SSS1.Px2.p2.3.m3.2.3.2"><ci id="S2.SS3.SSS1.Px2.p2.3.m3.1.1.cmml" xref="S2.SS3.SSS1.Px2.p2.3.m3.1.1">𝑤</ci><ci id="S2.SS3.SSS1.Px2.p2.3.m3.2.2.cmml" xref="S2.SS3.SSS1.Px2.p2.3.m3.2.2">ℎ</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p2.3.m3.2c">(w,h)</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p2.3.m3.2d">( italic_w , italic_h )</annotation></semantics></math>. and <math alttext="\delta_{z}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p2.4.m4.1"><semantics id="S2.SS3.SSS1.Px2.p2.4.m4.1a"><msub id="S2.SS3.SSS1.Px2.p2.4.m4.1.1" xref="S2.SS3.SSS1.Px2.p2.4.m4.1.1.cmml"><mi id="S2.SS3.SSS1.Px2.p2.4.m4.1.1.2" xref="S2.SS3.SSS1.Px2.p2.4.m4.1.1.2.cmml">δ</mi><mi id="S2.SS3.SSS1.Px2.p2.4.m4.1.1.3" xref="S2.SS3.SSS1.Px2.p2.4.m4.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p2.4.m4.1b"><apply id="S2.SS3.SSS1.Px2.p2.4.m4.1.1.cmml" xref="S2.SS3.SSS1.Px2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p2.4.m4.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p2.4.m4.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p2.4.m4.1.1.2.cmml" xref="S2.SS3.SSS1.Px2.p2.4.m4.1.1.2">𝛿</ci><ci id="S2.SS3.SSS1.Px2.p2.4.m4.1.1.3.cmml" xref="S2.SS3.SSS1.Px2.p2.4.m4.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p2.4.m4.1c">\delta_{z}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p2.4.m4.1d">italic_δ start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT</annotation></semantics></math> represents the depth of the object relative to the camera, normalized by the bounding box zoom-in ratio <math alttext="r=s_{\text{zoom}}/\text{max}(w,h)" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p2.5.m5.2"><semantics id="S2.SS3.SSS1.Px2.p2.5.m5.2a"><mrow id="S2.SS3.SSS1.Px2.p2.5.m5.2.3" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.cmml"><mi id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.2" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.2.cmml">r</mi><mo id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.1" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.1.cmml">=</mo><mrow id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.cmml"><mrow id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.cmml"><msub id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.cmml"><mi id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.2" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.2.cmml">s</mi><mtext id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.3" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.3a.cmml">zoom</mtext></msub><mo id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.1" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.1.cmml">/</mo><mtext id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.3" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.3a.cmml">max</mtext></mrow><mo id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.1" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.1.cmml">⁢</mo><mrow id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.3.2" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.3.1.cmml"><mo id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.3.2.1" stretchy="false" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.3.1.cmml">(</mo><mi id="S2.SS3.SSS1.Px2.p2.5.m5.1.1" xref="S2.SS3.SSS1.Px2.p2.5.m5.1.1.cmml">w</mi><mo id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.3.2.2" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.3.1.cmml">,</mo><mi id="S2.SS3.SSS1.Px2.p2.5.m5.2.2" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.2.cmml">h</mi><mo id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.3.2.3" stretchy="false" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p2.5.m5.2b"><apply id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3"><eq id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.1.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.1"></eq><ci id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.2.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.2">𝑟</ci><apply id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3"><times id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.1.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.1"></times><apply id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2"><divide id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.1.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.1"></divide><apply id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.1.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.2">𝑠</ci><ci id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.3a.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.3"><mtext id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.3.cmml" mathsize="70%" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.2.3">zoom</mtext></ci></apply><ci id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.3a.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.3"><mtext id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.3.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.2.3">max</mtext></ci></apply><interval closure="open" id="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.3.1.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.3.3.3.2"><ci id="S2.SS3.SSS1.Px2.p2.5.m5.1.1.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.1.1">𝑤</ci><ci id="S2.SS3.SSS1.Px2.p2.5.m5.2.2.cmml" xref="S2.SS3.SSS1.Px2.p2.5.m5.2.2">ℎ</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p2.5.m5.2c">r=s_{\text{zoom}}/\text{max}(w,h)</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p2.5.m5.2d">italic_r = italic_s start_POSTSUBSCRIPT zoom end_POSTSUBSCRIPT / max ( italic_w , italic_h )</annotation></semantics></math> where <math alttext="s_{\text{zoom}}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p2.6.m6.1"><semantics id="S2.SS3.SSS1.Px2.p2.6.m6.1a"><msub id="S2.SS3.SSS1.Px2.p2.6.m6.1.1" xref="S2.SS3.SSS1.Px2.p2.6.m6.1.1.cmml"><mi id="S2.SS3.SSS1.Px2.p2.6.m6.1.1.2" xref="S2.SS3.SSS1.Px2.p2.6.m6.1.1.2.cmml">s</mi><mtext id="S2.SS3.SSS1.Px2.p2.6.m6.1.1.3" xref="S2.SS3.SSS1.Px2.p2.6.m6.1.1.3a.cmml">zoom</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p2.6.m6.1b"><apply id="S2.SS3.SSS1.Px2.p2.6.m6.1.1.cmml" xref="S2.SS3.SSS1.Px2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p2.6.m6.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p2.6.m6.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p2.6.m6.1.1.2.cmml" xref="S2.SS3.SSS1.Px2.p2.6.m6.1.1.2">𝑠</ci><ci id="S2.SS3.SSS1.Px2.p2.6.m6.1.1.3a.cmml" xref="S2.SS3.SSS1.Px2.p2.6.m6.1.1.3"><mtext id="S2.SS3.SSS1.Px2.p2.6.m6.1.1.3.cmml" mathsize="70%" xref="S2.SS3.SSS1.Px2.p2.6.m6.1.1.3">zoom</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p2.6.m6.1c">s_{\text{zoom}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p2.6.m6.1d">italic_s start_POSTSUBSCRIPT zoom end_POSTSUBSCRIPT</annotation></semantics></math> is the crop zoom-in size.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.Px2.p3">
<p class="ltx_p" id="S2.SS3.SSS1.Px2.p3.4">To obtain the final 3D translation <math alttext="t=(t_{x},t_{y},t_{z})" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p3.1.m1.3"><semantics id="S2.SS3.SSS1.Px2.p3.1.m1.3a"><mrow id="S2.SS3.SSS1.Px2.p3.1.m1.3.3" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.cmml"><mi id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.5" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.5.cmml">t</mi><mo id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.4" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.4.cmml">=</mo><mrow id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.4.cmml"><mo id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.4" stretchy="false" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.4.cmml">(</mo><msub id="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1" xref="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.2" xref="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.2.cmml">t</mi><mi id="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.3" xref="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.3.cmml">x</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.5" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.4.cmml">,</mo><msub id="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.cmml"><mi id="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.2.cmml">t</mi><mi id="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.3" xref="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.3.cmml">y</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.6" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.4.cmml">,</mo><msub id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.cmml"><mi id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.2" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.2.cmml">t</mi><mi id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.3" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.3.cmml">z</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.7" stretchy="false" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p3.1.m1.3b"><apply id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3"><eq id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.4.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.4"></eq><ci id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.5.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.5">𝑡</ci><vector id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.4.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3"><apply id="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.2">𝑡</ci><ci id="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.1.1.1.1.1.3">𝑥</ci></apply><apply id="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.1.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.2">𝑡</ci><ci id="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.3.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.2.2.2.2.2.3">𝑦</ci></apply><apply id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.1.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.2.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.2">𝑡</ci><ci id="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.3.cmml" xref="S2.SS3.SSS1.Px2.p3.1.m1.3.3.3.3.3.3">𝑧</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p3.1.m1.3c">t=(t_{x},t_{y},t_{z})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p3.1.m1.3d">italic_t = ( italic_t start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT )</annotation></semantics></math>, we first compute the 2D center coordinates of the object <math alttext="(o_{x},o_{y})" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p3.2.m2.2"><semantics id="S2.SS3.SSS1.Px2.p3.2.m2.2a"><mrow id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.3.cmml"><mo id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.3" stretchy="false" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.3.cmml">(</mo><msub id="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1" xref="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.cmml"><mi id="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.2" xref="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.2.cmml">o</mi><mi id="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.3" xref="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.3.cmml">x</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.4" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.3.cmml">,</mo><msub id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.cmml"><mi id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.2.cmml">o</mi><mi id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.3" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.3.cmml">y</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.5" stretchy="false" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p3.2.m2.2b"><interval closure="open" id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.3.cmml" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2"><apply id="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.2.cmml" xref="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.2">𝑜</ci><ci id="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.3.cmml" xref="S2.SS3.SSS1.Px2.p3.2.m2.1.1.1.1.3">𝑥</ci></apply><apply id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.1.cmml" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.2">𝑜</ci><ci id="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.3.cmml" xref="S2.SS3.SSS1.Px2.p3.2.m2.2.2.2.2.3">𝑦</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p3.2.m2.2c">(o_{x},o_{y})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p3.2.m2.2d">( italic_o start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )</annotation></semantics></math> by adding the predicted offsets <math alttext="(\delta_{x},\delta_{y})" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p3.3.m3.2"><semantics id="S2.SS3.SSS1.Px2.p3.3.m3.2a"><mrow id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.3.cmml"><mo id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.3" stretchy="false" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.3.cmml">(</mo><msub id="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1" xref="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.cmml"><mi id="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.2" xref="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.2.cmml">δ</mi><mi id="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.3" xref="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.3.cmml">x</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.4" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.3.cmml">,</mo><msub id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.cmml"><mi id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.2.cmml">δ</mi><mi id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.3" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.3.cmml">y</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.5" stretchy="false" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p3.3.m3.2b"><interval closure="open" id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.3.cmml" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2"><apply id="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.2.cmml" xref="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.2">𝛿</ci><ci id="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.3.cmml" xref="S2.SS3.SSS1.Px2.p3.3.m3.1.1.1.1.3">𝑥</ci></apply><apply id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.1.cmml" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.2">𝛿</ci><ci id="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.3.cmml" xref="S2.SS3.SSS1.Px2.p3.3.m3.2.2.2.2.3">𝑦</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p3.3.m3.2c">(\delta_{x},\delta_{y})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p3.3.m3.2d">( italic_δ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_δ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )</annotation></semantics></math> to the center of the bounding box <math alttext="(c_{x},c_{y})" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p3.4.m4.2"><semantics id="S2.SS3.SSS1.Px2.p3.4.m4.2a"><mrow id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.3.cmml"><mo id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.3" stretchy="false" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.3.cmml">(</mo><msub id="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1" xref="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.cmml"><mi id="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.2" xref="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.2.cmml">c</mi><mi id="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.3" xref="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.3.cmml">x</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.4" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.3.cmml">,</mo><msub id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.cmml"><mi id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.2.cmml">c</mi><mi id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.3" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.3.cmml">y</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.5" stretchy="false" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p3.4.m4.2b"><interval closure="open" id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.3.cmml" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2"><apply id="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.2.cmml" xref="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.2">𝑐</ci><ci id="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.3.cmml" xref="S2.SS3.SSS1.Px2.p3.4.m4.1.1.1.1.3">𝑥</ci></apply><apply id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.1.cmml" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.2">𝑐</ci><ci id="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.3.cmml" xref="S2.SS3.SSS1.Px2.p3.4.m4.2.2.2.2.3">𝑦</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p3.4.m4.2c">(c_{x},c_{y})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p3.4.m4.2d">( italic_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx1">
<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle o_{x}" class="ltx_Math" display="inline" id="S2.Ex1.m1.1"><semantics id="S2.Ex1.m1.1a"><msub id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml"><mi id="S2.Ex1.m1.1.1.2" xref="S2.Ex1.m1.1.1.2.cmml">o</mi><mi id="S2.Ex1.m1.1.1.3" xref="S2.Ex1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.1b"><apply id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.1.cmml" xref="S2.Ex1.m1.1.1">subscript</csymbol><ci id="S2.Ex1.m1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.2">𝑜</ci><ci id="S2.Ex1.m1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.1c">\displaystyle o_{x}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.1d">italic_o start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=c_{x}+\delta_{x}\cdot w" class="ltx_Math" display="inline" id="S2.Ex1.m2.1"><semantics id="S2.Ex1.m2.1a"><mrow id="S2.Ex1.m2.1.1" xref="S2.Ex1.m2.1.1.cmml"><mi id="S2.Ex1.m2.1.1.2" xref="S2.Ex1.m2.1.1.2.cmml"></mi><mo id="S2.Ex1.m2.1.1.1" xref="S2.Ex1.m2.1.1.1.cmml">=</mo><mrow id="S2.Ex1.m2.1.1.3" xref="S2.Ex1.m2.1.1.3.cmml"><msub id="S2.Ex1.m2.1.1.3.2" xref="S2.Ex1.m2.1.1.3.2.cmml"><mi id="S2.Ex1.m2.1.1.3.2.2" xref="S2.Ex1.m2.1.1.3.2.2.cmml">c</mi><mi id="S2.Ex1.m2.1.1.3.2.3" xref="S2.Ex1.m2.1.1.3.2.3.cmml">x</mi></msub><mo id="S2.Ex1.m2.1.1.3.1" xref="S2.Ex1.m2.1.1.3.1.cmml">+</mo><mrow id="S2.Ex1.m2.1.1.3.3" xref="S2.Ex1.m2.1.1.3.3.cmml"><msub id="S2.Ex1.m2.1.1.3.3.2" xref="S2.Ex1.m2.1.1.3.3.2.cmml"><mi id="S2.Ex1.m2.1.1.3.3.2.2" xref="S2.Ex1.m2.1.1.3.3.2.2.cmml">δ</mi><mi id="S2.Ex1.m2.1.1.3.3.2.3" xref="S2.Ex1.m2.1.1.3.3.2.3.cmml">x</mi></msub><mo id="S2.Ex1.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.Ex1.m2.1.1.3.3.1.cmml">⋅</mo><mi id="S2.Ex1.m2.1.1.3.3.3" xref="S2.Ex1.m2.1.1.3.3.3.cmml">w</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m2.1b"><apply id="S2.Ex1.m2.1.1.cmml" xref="S2.Ex1.m2.1.1"><eq id="S2.Ex1.m2.1.1.1.cmml" xref="S2.Ex1.m2.1.1.1"></eq><csymbol cd="latexml" id="S2.Ex1.m2.1.1.2.cmml" xref="S2.Ex1.m2.1.1.2">absent</csymbol><apply id="S2.Ex1.m2.1.1.3.cmml" xref="S2.Ex1.m2.1.1.3"><plus id="S2.Ex1.m2.1.1.3.1.cmml" xref="S2.Ex1.m2.1.1.3.1"></plus><apply id="S2.Ex1.m2.1.1.3.2.cmml" xref="S2.Ex1.m2.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m2.1.1.3.2.1.cmml" xref="S2.Ex1.m2.1.1.3.2">subscript</csymbol><ci id="S2.Ex1.m2.1.1.3.2.2.cmml" xref="S2.Ex1.m2.1.1.3.2.2">𝑐</ci><ci id="S2.Ex1.m2.1.1.3.2.3.cmml" xref="S2.Ex1.m2.1.1.3.2.3">𝑥</ci></apply><apply id="S2.Ex1.m2.1.1.3.3.cmml" xref="S2.Ex1.m2.1.1.3.3"><ci id="S2.Ex1.m2.1.1.3.3.1.cmml" xref="S2.Ex1.m2.1.1.3.3.1">⋅</ci><apply id="S2.Ex1.m2.1.1.3.3.2.cmml" xref="S2.Ex1.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m2.1.1.3.3.2.1.cmml" xref="S2.Ex1.m2.1.1.3.3.2">subscript</csymbol><ci id="S2.Ex1.m2.1.1.3.3.2.2.cmml" xref="S2.Ex1.m2.1.1.3.3.2.2">𝛿</ci><ci id="S2.Ex1.m2.1.1.3.3.2.3.cmml" xref="S2.Ex1.m2.1.1.3.3.2.3">𝑥</ci></apply><ci id="S2.Ex1.m2.1.1.3.3.3.cmml" xref="S2.Ex1.m2.1.1.3.3.3">𝑤</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m2.1c">\displaystyle=c_{x}+\delta_{x}\cdot w</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m2.1d">= italic_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT + italic_δ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ⋅ italic_w</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle o_{y}" class="ltx_Math" display="inline" id="S2.Ex2.m1.1"><semantics id="S2.Ex2.m1.1a"><msub id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml"><mi id="S2.Ex2.m1.1.1.2" xref="S2.Ex2.m1.1.1.2.cmml">o</mi><mi id="S2.Ex2.m1.1.1.3" xref="S2.Ex2.m1.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.1b"><apply id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.cmml" xref="S2.Ex2.m1.1.1">subscript</csymbol><ci id="S2.Ex2.m1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.2">𝑜</ci><ci id="S2.Ex2.m1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.1c">\displaystyle o_{y}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m1.1d">italic_o start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=c_{y}+\delta_{y}\cdot h" class="ltx_Math" display="inline" id="S2.Ex2.m2.1"><semantics id="S2.Ex2.m2.1a"><mrow id="S2.Ex2.m2.1.1" xref="S2.Ex2.m2.1.1.cmml"><mi id="S2.Ex2.m2.1.1.2" xref="S2.Ex2.m2.1.1.2.cmml"></mi><mo id="S2.Ex2.m2.1.1.1" xref="S2.Ex2.m2.1.1.1.cmml">=</mo><mrow id="S2.Ex2.m2.1.1.3" xref="S2.Ex2.m2.1.1.3.cmml"><msub id="S2.Ex2.m2.1.1.3.2" xref="S2.Ex2.m2.1.1.3.2.cmml"><mi id="S2.Ex2.m2.1.1.3.2.2" xref="S2.Ex2.m2.1.1.3.2.2.cmml">c</mi><mi id="S2.Ex2.m2.1.1.3.2.3" xref="S2.Ex2.m2.1.1.3.2.3.cmml">y</mi></msub><mo id="S2.Ex2.m2.1.1.3.1" xref="S2.Ex2.m2.1.1.3.1.cmml">+</mo><mrow id="S2.Ex2.m2.1.1.3.3" xref="S2.Ex2.m2.1.1.3.3.cmml"><msub id="S2.Ex2.m2.1.1.3.3.2" xref="S2.Ex2.m2.1.1.3.3.2.cmml"><mi id="S2.Ex2.m2.1.1.3.3.2.2" xref="S2.Ex2.m2.1.1.3.3.2.2.cmml">δ</mi><mi id="S2.Ex2.m2.1.1.3.3.2.3" xref="S2.Ex2.m2.1.1.3.3.2.3.cmml">y</mi></msub><mo id="S2.Ex2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.Ex2.m2.1.1.3.3.1.cmml">⋅</mo><mi id="S2.Ex2.m2.1.1.3.3.3" xref="S2.Ex2.m2.1.1.3.3.3.cmml">h</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m2.1b"><apply id="S2.Ex2.m2.1.1.cmml" xref="S2.Ex2.m2.1.1"><eq id="S2.Ex2.m2.1.1.1.cmml" xref="S2.Ex2.m2.1.1.1"></eq><csymbol cd="latexml" id="S2.Ex2.m2.1.1.2.cmml" xref="S2.Ex2.m2.1.1.2">absent</csymbol><apply id="S2.Ex2.m2.1.1.3.cmml" xref="S2.Ex2.m2.1.1.3"><plus id="S2.Ex2.m2.1.1.3.1.cmml" xref="S2.Ex2.m2.1.1.3.1"></plus><apply id="S2.Ex2.m2.1.1.3.2.cmml" xref="S2.Ex2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex2.m2.1.1.3.2.1.cmml" xref="S2.Ex2.m2.1.1.3.2">subscript</csymbol><ci id="S2.Ex2.m2.1.1.3.2.2.cmml" xref="S2.Ex2.m2.1.1.3.2.2">𝑐</ci><ci id="S2.Ex2.m2.1.1.3.2.3.cmml" xref="S2.Ex2.m2.1.1.3.2.3">𝑦</ci></apply><apply id="S2.Ex2.m2.1.1.3.3.cmml" xref="S2.Ex2.m2.1.1.3.3"><ci id="S2.Ex2.m2.1.1.3.3.1.cmml" xref="S2.Ex2.m2.1.1.3.3.1">⋅</ci><apply id="S2.Ex2.m2.1.1.3.3.2.cmml" xref="S2.Ex2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.Ex2.m2.1.1.3.3.2.1.cmml" xref="S2.Ex2.m2.1.1.3.3.2">subscript</csymbol><ci id="S2.Ex2.m2.1.1.3.3.2.2.cmml" xref="S2.Ex2.m2.1.1.3.3.2.2">𝛿</ci><ci id="S2.Ex2.m2.1.1.3.3.2.3.cmml" xref="S2.Ex2.m2.1.1.3.3.2.3">𝑦</ci></apply><ci id="S2.Ex2.m2.1.1.3.3.3.cmml" xref="S2.Ex2.m2.1.1.3.3.3">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m2.1c">\displaystyle=c_{y}+\delta_{y}\cdot h</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m2.1d">= italic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT + italic_δ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ⋅ italic_h</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS3.SSS1.Px2.p3.6">Then, using the camera intrinsic parameters (focal length <math alttext="f" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p3.5.m1.1"><semantics id="S2.SS3.SSS1.Px2.p3.5.m1.1a"><mi id="S2.SS3.SSS1.Px2.p3.5.m1.1.1" xref="S2.SS3.SSS1.Px2.p3.5.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p3.5.m1.1b"><ci id="S2.SS3.SSS1.Px2.p3.5.m1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.5.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p3.5.m1.1c">f</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p3.5.m1.1d">italic_f</annotation></semantics></math> and principal point <math alttext="(p_{x},p_{y})" class="ltx_Math" display="inline" id="S2.SS3.SSS1.Px2.p3.6.m2.2"><semantics id="S2.SS3.SSS1.Px2.p3.6.m2.2a"><mrow id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.3.cmml"><mo id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.3" stretchy="false" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.3.cmml">(</mo><msub id="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1" xref="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.cmml"><mi id="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.2" xref="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.2.cmml">p</mi><mi id="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.3" xref="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.3.cmml">x</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.4" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.3.cmml">,</mo><msub id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.cmml"><mi id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.2" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.2.cmml">p</mi><mi id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.3" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.3.cmml">y</mi></msub><mo id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.5" stretchy="false" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.Px2.p3.6.m2.2b"><interval closure="open" id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.3.cmml" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2"><apply id="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.2.cmml" xref="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.2">𝑝</ci><ci id="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.3.cmml" xref="S2.SS3.SSS1.Px2.p3.6.m2.1.1.1.1.3">𝑥</ci></apply><apply id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.1.cmml" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.2.cmml" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.2">𝑝</ci><ci id="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.3.cmml" xref="S2.SS3.SSS1.Px2.p3.6.m2.2.2.2.2.3">𝑦</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.Px2.p3.6.m2.2c">(p_{x},p_{y})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.Px2.p3.6.m2.2d">( italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )</annotation></semantics></math>), we compute the 3D translation:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx2">
<tbody id="S2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle t_{x}" class="ltx_Math" display="inline" id="S2.Ex3.m1.1"><semantics id="S2.Ex3.m1.1a"><msub id="S2.Ex3.m1.1.1" xref="S2.Ex3.m1.1.1.cmml"><mi id="S2.Ex3.m1.1.1.2" xref="S2.Ex3.m1.1.1.2.cmml">t</mi><mi id="S2.Ex3.m1.1.1.3" xref="S2.Ex3.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S2.Ex3.m1.1b"><apply id="S2.Ex3.m1.1.1.cmml" xref="S2.Ex3.m1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.cmml" xref="S2.Ex3.m1.1.1">subscript</csymbol><ci id="S2.Ex3.m1.1.1.2.cmml" xref="S2.Ex3.m1.1.1.2">𝑡</ci><ci id="S2.Ex3.m1.1.1.3.cmml" xref="S2.Ex3.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m1.1c">\displaystyle t_{x}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex3.m1.1d">italic_t start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(o_{x}-p_{x})\cdot\frac{t_{z}}{f}" class="ltx_Math" display="inline" id="S2.Ex3.m2.1"><semantics id="S2.Ex3.m2.1a"><mrow id="S2.Ex3.m2.1.1" xref="S2.Ex3.m2.1.1.cmml"><mi id="S2.Ex3.m2.1.1.3" xref="S2.Ex3.m2.1.1.3.cmml"></mi><mo id="S2.Ex3.m2.1.1.2" xref="S2.Ex3.m2.1.1.2.cmml">=</mo><mrow id="S2.Ex3.m2.1.1.1" xref="S2.Ex3.m2.1.1.1.cmml"><mrow id="S2.Ex3.m2.1.1.1.1.1" xref="S2.Ex3.m2.1.1.1.1.1.1.cmml"><mo id="S2.Ex3.m2.1.1.1.1.1.2" stretchy="false" xref="S2.Ex3.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex3.m2.1.1.1.1.1.1" xref="S2.Ex3.m2.1.1.1.1.1.1.cmml"><msub id="S2.Ex3.m2.1.1.1.1.1.1.2" xref="S2.Ex3.m2.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex3.m2.1.1.1.1.1.1.2.2" xref="S2.Ex3.m2.1.1.1.1.1.1.2.2.cmml">o</mi><mi id="S2.Ex3.m2.1.1.1.1.1.1.2.3" xref="S2.Ex3.m2.1.1.1.1.1.1.2.3.cmml">x</mi></msub><mo id="S2.Ex3.m2.1.1.1.1.1.1.1" xref="S2.Ex3.m2.1.1.1.1.1.1.1.cmml">−</mo><msub id="S2.Ex3.m2.1.1.1.1.1.1.3" xref="S2.Ex3.m2.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex3.m2.1.1.1.1.1.1.3.2" xref="S2.Ex3.m2.1.1.1.1.1.1.3.2.cmml">p</mi><mi id="S2.Ex3.m2.1.1.1.1.1.1.3.3" xref="S2.Ex3.m2.1.1.1.1.1.1.3.3.cmml">x</mi></msub></mrow><mo id="S2.Ex3.m2.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S2.Ex3.m2.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.Ex3.m2.1.1.1.2" rspace="0.222em" xref="S2.Ex3.m2.1.1.1.2.cmml">⋅</mo><mstyle displaystyle="true" id="S2.Ex3.m2.1.1.1.3" xref="S2.Ex3.m2.1.1.1.3.cmml"><mfrac id="S2.Ex3.m2.1.1.1.3a" xref="S2.Ex3.m2.1.1.1.3.cmml"><msub id="S2.Ex3.m2.1.1.1.3.2" xref="S2.Ex3.m2.1.1.1.3.2.cmml"><mi id="S2.Ex3.m2.1.1.1.3.2.2" xref="S2.Ex3.m2.1.1.1.3.2.2.cmml">t</mi><mi id="S2.Ex3.m2.1.1.1.3.2.3" xref="S2.Ex3.m2.1.1.1.3.2.3.cmml">z</mi></msub><mi id="S2.Ex3.m2.1.1.1.3.3" xref="S2.Ex3.m2.1.1.1.3.3.cmml">f</mi></mfrac></mstyle></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m2.1b"><apply id="S2.Ex3.m2.1.1.cmml" xref="S2.Ex3.m2.1.1"><eq id="S2.Ex3.m2.1.1.2.cmml" xref="S2.Ex3.m2.1.1.2"></eq><csymbol cd="latexml" id="S2.Ex3.m2.1.1.3.cmml" xref="S2.Ex3.m2.1.1.3">absent</csymbol><apply id="S2.Ex3.m2.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1"><ci id="S2.Ex3.m2.1.1.1.2.cmml" xref="S2.Ex3.m2.1.1.1.2">⋅</ci><apply id="S2.Ex3.m2.1.1.1.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1"><minus id="S2.Ex3.m2.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.1"></minus><apply id="S2.Ex3.m2.1.1.1.1.1.1.2.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex3.m2.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.Ex3.m2.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.2.2">𝑜</ci><ci id="S2.Ex3.m2.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.2.3">𝑥</ci></apply><apply id="S2.Ex3.m2.1.1.1.1.1.1.3.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex3.m2.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex3.m2.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.3.2">𝑝</ci><ci id="S2.Ex3.m2.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.3.3">𝑥</ci></apply></apply><apply id="S2.Ex3.m2.1.1.1.3.cmml" xref="S2.Ex3.m2.1.1.1.3"><divide id="S2.Ex3.m2.1.1.1.3.1.cmml" xref="S2.Ex3.m2.1.1.1.3"></divide><apply id="S2.Ex3.m2.1.1.1.3.2.cmml" xref="S2.Ex3.m2.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex3.m2.1.1.1.3.2.1.cmml" xref="S2.Ex3.m2.1.1.1.3.2">subscript</csymbol><ci id="S2.Ex3.m2.1.1.1.3.2.2.cmml" xref="S2.Ex3.m2.1.1.1.3.2.2">𝑡</ci><ci id="S2.Ex3.m2.1.1.1.3.2.3.cmml" xref="S2.Ex3.m2.1.1.1.3.2.3">𝑧</ci></apply><ci id="S2.Ex3.m2.1.1.1.3.3.cmml" xref="S2.Ex3.m2.1.1.1.3.3">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m2.1c">\displaystyle=(o_{x}-p_{x})\cdot\frac{t_{z}}{f}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex3.m2.1d">= ( italic_o start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) ⋅ divide start_ARG italic_t start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_ARG start_ARG italic_f end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
<tbody id="S2.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle t_{y}" class="ltx_Math" display="inline" id="S2.Ex4.m1.1"><semantics id="S2.Ex4.m1.1a"><msub id="S2.Ex4.m1.1.1" xref="S2.Ex4.m1.1.1.cmml"><mi id="S2.Ex4.m1.1.1.2" xref="S2.Ex4.m1.1.1.2.cmml">t</mi><mi id="S2.Ex4.m1.1.1.3" xref="S2.Ex4.m1.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S2.Ex4.m1.1b"><apply id="S2.Ex4.m1.1.1.cmml" xref="S2.Ex4.m1.1.1"><csymbol cd="ambiguous" id="S2.Ex4.m1.1.1.1.cmml" xref="S2.Ex4.m1.1.1">subscript</csymbol><ci id="S2.Ex4.m1.1.1.2.cmml" xref="S2.Ex4.m1.1.1.2">𝑡</ci><ci id="S2.Ex4.m1.1.1.3.cmml" xref="S2.Ex4.m1.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex4.m1.1c">\displaystyle t_{y}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex4.m1.1d">italic_t start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(o_{y}-p_{y})\cdot\frac{t_{z}}{f}" class="ltx_Math" display="inline" id="S2.Ex4.m2.1"><semantics id="S2.Ex4.m2.1a"><mrow id="S2.Ex4.m2.1.1" xref="S2.Ex4.m2.1.1.cmml"><mi id="S2.Ex4.m2.1.1.3" xref="S2.Ex4.m2.1.1.3.cmml"></mi><mo id="S2.Ex4.m2.1.1.2" xref="S2.Ex4.m2.1.1.2.cmml">=</mo><mrow id="S2.Ex4.m2.1.1.1" xref="S2.Ex4.m2.1.1.1.cmml"><mrow id="S2.Ex4.m2.1.1.1.1.1" xref="S2.Ex4.m2.1.1.1.1.1.1.cmml"><mo id="S2.Ex4.m2.1.1.1.1.1.2" stretchy="false" xref="S2.Ex4.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex4.m2.1.1.1.1.1.1" xref="S2.Ex4.m2.1.1.1.1.1.1.cmml"><msub id="S2.Ex4.m2.1.1.1.1.1.1.2" xref="S2.Ex4.m2.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex4.m2.1.1.1.1.1.1.2.2" xref="S2.Ex4.m2.1.1.1.1.1.1.2.2.cmml">o</mi><mi id="S2.Ex4.m2.1.1.1.1.1.1.2.3" xref="S2.Ex4.m2.1.1.1.1.1.1.2.3.cmml">y</mi></msub><mo id="S2.Ex4.m2.1.1.1.1.1.1.1" xref="S2.Ex4.m2.1.1.1.1.1.1.1.cmml">−</mo><msub id="S2.Ex4.m2.1.1.1.1.1.1.3" xref="S2.Ex4.m2.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex4.m2.1.1.1.1.1.1.3.2" xref="S2.Ex4.m2.1.1.1.1.1.1.3.2.cmml">p</mi><mi id="S2.Ex4.m2.1.1.1.1.1.1.3.3" xref="S2.Ex4.m2.1.1.1.1.1.1.3.3.cmml">y</mi></msub></mrow><mo id="S2.Ex4.m2.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S2.Ex4.m2.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.Ex4.m2.1.1.1.2" rspace="0.222em" xref="S2.Ex4.m2.1.1.1.2.cmml">⋅</mo><mstyle displaystyle="true" id="S2.Ex4.m2.1.1.1.3" xref="S2.Ex4.m2.1.1.1.3.cmml"><mfrac id="S2.Ex4.m2.1.1.1.3a" xref="S2.Ex4.m2.1.1.1.3.cmml"><msub id="S2.Ex4.m2.1.1.1.3.2" xref="S2.Ex4.m2.1.1.1.3.2.cmml"><mi id="S2.Ex4.m2.1.1.1.3.2.2" xref="S2.Ex4.m2.1.1.1.3.2.2.cmml">t</mi><mi id="S2.Ex4.m2.1.1.1.3.2.3" xref="S2.Ex4.m2.1.1.1.3.2.3.cmml">z</mi></msub><mi id="S2.Ex4.m2.1.1.1.3.3" xref="S2.Ex4.m2.1.1.1.3.3.cmml">f</mi></mfrac></mstyle></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex4.m2.1b"><apply id="S2.Ex4.m2.1.1.cmml" xref="S2.Ex4.m2.1.1"><eq id="S2.Ex4.m2.1.1.2.cmml" xref="S2.Ex4.m2.1.1.2"></eq><csymbol cd="latexml" id="S2.Ex4.m2.1.1.3.cmml" xref="S2.Ex4.m2.1.1.3">absent</csymbol><apply id="S2.Ex4.m2.1.1.1.cmml" xref="S2.Ex4.m2.1.1.1"><ci id="S2.Ex4.m2.1.1.1.2.cmml" xref="S2.Ex4.m2.1.1.1.2">⋅</ci><apply id="S2.Ex4.m2.1.1.1.1.1.1.cmml" xref="S2.Ex4.m2.1.1.1.1.1"><minus id="S2.Ex4.m2.1.1.1.1.1.1.1.cmml" xref="S2.Ex4.m2.1.1.1.1.1.1.1"></minus><apply id="S2.Ex4.m2.1.1.1.1.1.1.2.cmml" xref="S2.Ex4.m2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex4.m2.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex4.m2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.Ex4.m2.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex4.m2.1.1.1.1.1.1.2.2">𝑜</ci><ci id="S2.Ex4.m2.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex4.m2.1.1.1.1.1.1.2.3">𝑦</ci></apply><apply id="S2.Ex4.m2.1.1.1.1.1.1.3.cmml" xref="S2.Ex4.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex4.m2.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex4.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex4.m2.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex4.m2.1.1.1.1.1.1.3.2">𝑝</ci><ci id="S2.Ex4.m2.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex4.m2.1.1.1.1.1.1.3.3">𝑦</ci></apply></apply><apply id="S2.Ex4.m2.1.1.1.3.cmml" xref="S2.Ex4.m2.1.1.1.3"><divide id="S2.Ex4.m2.1.1.1.3.1.cmml" xref="S2.Ex4.m2.1.1.1.3"></divide><apply id="S2.Ex4.m2.1.1.1.3.2.cmml" xref="S2.Ex4.m2.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex4.m2.1.1.1.3.2.1.cmml" xref="S2.Ex4.m2.1.1.1.3.2">subscript</csymbol><ci id="S2.Ex4.m2.1.1.1.3.2.2.cmml" xref="S2.Ex4.m2.1.1.1.3.2.2">𝑡</ci><ci id="S2.Ex4.m2.1.1.1.3.2.3.cmml" xref="S2.Ex4.m2.1.1.1.3.2.3">𝑧</ci></apply><ci id="S2.Ex4.m2.1.1.1.3.3.cmml" xref="S2.Ex4.m2.1.1.1.3.3">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex4.m2.1c">\displaystyle=(o_{y}-p_{y})\cdot\frac{t_{z}}{f}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex4.m2.1d">= ( italic_o start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) ⋅ divide start_ARG italic_t start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_ARG start_ARG italic_f end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
<tbody id="S2.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle t_{z}" class="ltx_Math" display="inline" id="S2.Ex5.m1.1"><semantics id="S2.Ex5.m1.1a"><msub id="S2.Ex5.m1.1.1" xref="S2.Ex5.m1.1.1.cmml"><mi id="S2.Ex5.m1.1.1.2" xref="S2.Ex5.m1.1.1.2.cmml">t</mi><mi id="S2.Ex5.m1.1.1.3" xref="S2.Ex5.m1.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S2.Ex5.m1.1b"><apply id="S2.Ex5.m1.1.1.cmml" xref="S2.Ex5.m1.1.1"><csymbol cd="ambiguous" id="S2.Ex5.m1.1.1.1.cmml" xref="S2.Ex5.m1.1.1">subscript</csymbol><ci id="S2.Ex5.m1.1.1.2.cmml" xref="S2.Ex5.m1.1.1.2">𝑡</ci><ci id="S2.Ex5.m1.1.1.3.cmml" xref="S2.Ex5.m1.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex5.m1.1c">\displaystyle t_{z}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex5.m1.1d">italic_t start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\delta_{z}\cdot r" class="ltx_Math" display="inline" id="S2.Ex5.m2.1"><semantics id="S2.Ex5.m2.1a"><mrow id="S2.Ex5.m2.1.1" xref="S2.Ex5.m2.1.1.cmml"><mi id="S2.Ex5.m2.1.1.2" xref="S2.Ex5.m2.1.1.2.cmml"></mi><mo id="S2.Ex5.m2.1.1.1" xref="S2.Ex5.m2.1.1.1.cmml">=</mo><mrow id="S2.Ex5.m2.1.1.3" xref="S2.Ex5.m2.1.1.3.cmml"><msub id="S2.Ex5.m2.1.1.3.2" xref="S2.Ex5.m2.1.1.3.2.cmml"><mi id="S2.Ex5.m2.1.1.3.2.2" xref="S2.Ex5.m2.1.1.3.2.2.cmml">δ</mi><mi id="S2.Ex5.m2.1.1.3.2.3" xref="S2.Ex5.m2.1.1.3.2.3.cmml">z</mi></msub><mo id="S2.Ex5.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S2.Ex5.m2.1.1.3.1.cmml">⋅</mo><mi id="S2.Ex5.m2.1.1.3.3" xref="S2.Ex5.m2.1.1.3.3.cmml">r</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex5.m2.1b"><apply id="S2.Ex5.m2.1.1.cmml" xref="S2.Ex5.m2.1.1"><eq id="S2.Ex5.m2.1.1.1.cmml" xref="S2.Ex5.m2.1.1.1"></eq><csymbol cd="latexml" id="S2.Ex5.m2.1.1.2.cmml" xref="S2.Ex5.m2.1.1.2">absent</csymbol><apply id="S2.Ex5.m2.1.1.3.cmml" xref="S2.Ex5.m2.1.1.3"><ci id="S2.Ex5.m2.1.1.3.1.cmml" xref="S2.Ex5.m2.1.1.3.1">⋅</ci><apply id="S2.Ex5.m2.1.1.3.2.cmml" xref="S2.Ex5.m2.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex5.m2.1.1.3.2.1.cmml" xref="S2.Ex5.m2.1.1.3.2">subscript</csymbol><ci id="S2.Ex5.m2.1.1.3.2.2.cmml" xref="S2.Ex5.m2.1.1.3.2.2">𝛿</ci><ci id="S2.Ex5.m2.1.1.3.2.3.cmml" xref="S2.Ex5.m2.1.1.3.2.3">𝑧</ci></apply><ci id="S2.Ex5.m2.1.1.3.3.cmml" xref="S2.Ex5.m2.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex5.m2.1c">\displaystyle=\delta_{z}\cdot r</annotation><annotation encoding="application/x-llamapun" id="S2.Ex5.m2.1d">= italic_δ start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ⋅ italic_r</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">2D-3D Dense Correspondence Maps</h5>
<div class="ltx_para" id="S2.SS3.SSS1.Px3.p1">
<p class="ltx_p" id="S2.SS3.SSS1.Px3.p1.1">The network generates a dense 2D-3D correspondence map <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib24" title="">24</a>]</cite>, which is an intermediate geometric feature that establishes a relationship between each object pixel in the 2D image and its corresponding 3D coordinate on the object model. These 3D coordinates are normalized using the dimensions of a tight 3D bounding box that encloses the mesh. Utilizing this correspondence map, we can apply PnP-RANSAC algorithms to find a plausible 6D pose of the object.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS1.Px4">
<h5 class="ltx_title ltx_title_paragraph">Patch-PnP</h5>
<div class="ltx_para" id="S2.SS3.SSS1.Px4.p1">
<p class="ltx_p" id="S2.SS3.SSS1.Px4.p1.1">GDR-Net introduced a differentiable module named Patch-PnP, which is composed of convolutional layers followed by fully connected layers. This module processes geometric feature maps, such as 2D-3D Dense Correspondence Maps, and directly regresses the 6D object pose. Its primary aim is to replace the traditional PnP-RANSAC method with a more seamless and differentiable pipeline. The differentiable nature of Patch-PnP allows for end-to-end learning during the training of the pose estimation task.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Multitask Pose Losses</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">We utilize an ensemble of loss functions tailored to optimize different aspects of the 6D pose estimation task. The total loss comprises the following components:</p>
</div>
<section class="ltx_paragraph" id="S2.SS3.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Pose Loss</h5>
<div class="ltx_para" id="S2.SS3.SSS2.Px1.p1">
<p class="ltx_p" id="S2.SS3.SSS2.Px1.p1.1">(<math alttext="\mathcal{L}_{\text{Pose}}" class="ltx_Math" display="inline" id="S2.SS3.SSS2.Px1.p1.1.m1.1"><semantics id="S2.SS3.SSS2.Px1.p1.1.m1.1a"><msub id="S2.SS3.SSS2.Px1.p1.1.m1.1.1" xref="S2.SS3.SSS2.Px1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.SSS2.Px1.p1.1.m1.1.1.2" xref="S2.SS3.SSS2.Px1.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.SS3.SSS2.Px1.p1.1.m1.1.1.3" xref="S2.SS3.SSS2.Px1.p1.1.m1.1.1.3a.cmml">Pose</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.Px1.p1.1.m1.1b"><apply id="S2.SS3.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS2.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.Px1.p1.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.Px1.p1.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.Px1.p1.1.m1.1.1.2">ℒ</ci><ci id="S2.SS3.SSS2.Px1.p1.1.m1.1.1.3a.cmml" xref="S2.SS3.SSS2.Px1.p1.1.m1.1.1.3"><mtext id="S2.SS3.SSS2.Px1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS3.SSS2.Px1.p1.1.m1.1.1.3">Pose</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.Px1.p1.1.m1.1c">\mathcal{L}_{\text{Pose}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.Px1.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT Pose end_POSTSUBSCRIPT</annotation></semantics></math>) This loss is further disentangled into three components:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Rotation Loss (<math alttext="\mathcal{L}_{R}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.1.m1.1"><semantics id="S2.I1.i1.p1.1.m1.1a"><msub id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.I1.i1.p1.1.m1.1.1.2" xref="S2.I1.i1.p1.1.m1.1.1.2.cmml">ℒ</mi><mi id="S2.I1.i1.p1.1.m1.1.1.3" xref="S2.I1.i1.p1.1.m1.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><apply id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.1.m1.1.1.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2">ℒ</ci><ci id="S2.I1.i1.p1.1.m1.1.1.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">\mathcal{L}_{R}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT</annotation></semantics></math>): Measures the discrepancy between the predicted and ground-truth rotations using a point matching loss. The rotation loss is computed as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx3">
<tbody id="S2.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{R}=\frac{1}{N}\sum_{i=1}^{N}\|\hat{R}p_{i}-\bar{R}p_%
{i}\|_{1}" class="ltx_Math" display="inline" id="S2.Ex6.m1.1"><semantics id="S2.Ex6.m1.1a"><mrow id="S2.Ex6.m1.1.1" xref="S2.Ex6.m1.1.1.cmml"><msub id="S2.Ex6.m1.1.1.3" xref="S2.Ex6.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex6.m1.1.1.3.2" xref="S2.Ex6.m1.1.1.3.2.cmml">ℒ</mi><mi id="S2.Ex6.m1.1.1.3.3" xref="S2.Ex6.m1.1.1.3.3.cmml">R</mi></msub><mo id="S2.Ex6.m1.1.1.2" xref="S2.Ex6.m1.1.1.2.cmml">=</mo><mrow id="S2.Ex6.m1.1.1.1" xref="S2.Ex6.m1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.Ex6.m1.1.1.1.3" xref="S2.Ex6.m1.1.1.1.3.cmml"><mfrac id="S2.Ex6.m1.1.1.1.3a" xref="S2.Ex6.m1.1.1.1.3.cmml"><mn id="S2.Ex6.m1.1.1.1.3.2" xref="S2.Ex6.m1.1.1.1.3.2.cmml">1</mn><mi id="S2.Ex6.m1.1.1.1.3.3" xref="S2.Ex6.m1.1.1.1.3.3.cmml">N</mi></mfrac></mstyle><mo id="S2.Ex6.m1.1.1.1.2" xref="S2.Ex6.m1.1.1.1.2.cmml">⁢</mo><mrow id="S2.Ex6.m1.1.1.1.1" xref="S2.Ex6.m1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.Ex6.m1.1.1.1.1.2" xref="S2.Ex6.m1.1.1.1.1.2.cmml"><munderover id="S2.Ex6.m1.1.1.1.1.2a" xref="S2.Ex6.m1.1.1.1.1.2.cmml"><mo id="S2.Ex6.m1.1.1.1.1.2.2.2" movablelimits="false" xref="S2.Ex6.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.Ex6.m1.1.1.1.1.2.2.3" xref="S2.Ex6.m1.1.1.1.1.2.2.3.cmml"><mi id="S2.Ex6.m1.1.1.1.1.2.2.3.2" xref="S2.Ex6.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.Ex6.m1.1.1.1.1.2.2.3.1" xref="S2.Ex6.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.Ex6.m1.1.1.1.1.2.2.3.3" xref="S2.Ex6.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.Ex6.m1.1.1.1.1.2.3" xref="S2.Ex6.m1.1.1.1.1.2.3.cmml">N</mi></munderover></mstyle><msub id="S2.Ex6.m1.1.1.1.1.1" xref="S2.Ex6.m1.1.1.1.1.1.cmml"><mrow id="S2.Ex6.m1.1.1.1.1.1.1.1" xref="S2.Ex6.m1.1.1.1.1.1.1.2.cmml"><mo id="S2.Ex6.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex6.m1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.Ex6.m1.1.1.1.1.1.1.1.1" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">R</mi><mo id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mo id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.1" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">p</mi><mi id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S2.Ex6.m1.1.1.1.1.1.1.1.1.1" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">R</mi><mo id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2.1" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2.1.cmml">¯</mo></mover><mo id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.1" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><msub id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">p</mi><mi id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><mo id="S2.Ex6.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.Ex6.m1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.Ex6.m1.1.1.1.1.1.3" xref="S2.Ex6.m1.1.1.1.1.1.3.cmml">1</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex6.m1.1b"><apply id="S2.Ex6.m1.1.1.cmml" xref="S2.Ex6.m1.1.1"><eq id="S2.Ex6.m1.1.1.2.cmml" xref="S2.Ex6.m1.1.1.2"></eq><apply id="S2.Ex6.m1.1.1.3.cmml" xref="S2.Ex6.m1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex6.m1.1.1.3.1.cmml" xref="S2.Ex6.m1.1.1.3">subscript</csymbol><ci id="S2.Ex6.m1.1.1.3.2.cmml" xref="S2.Ex6.m1.1.1.3.2">ℒ</ci><ci id="S2.Ex6.m1.1.1.3.3.cmml" xref="S2.Ex6.m1.1.1.3.3">𝑅</ci></apply><apply id="S2.Ex6.m1.1.1.1.cmml" xref="S2.Ex6.m1.1.1.1"><times id="S2.Ex6.m1.1.1.1.2.cmml" xref="S2.Ex6.m1.1.1.1.2"></times><apply id="S2.Ex6.m1.1.1.1.3.cmml" xref="S2.Ex6.m1.1.1.1.3"><divide id="S2.Ex6.m1.1.1.1.3.1.cmml" xref="S2.Ex6.m1.1.1.1.3"></divide><cn id="S2.Ex6.m1.1.1.1.3.2.cmml" type="integer" xref="S2.Ex6.m1.1.1.1.3.2">1</cn><ci id="S2.Ex6.m1.1.1.1.3.3.cmml" xref="S2.Ex6.m1.1.1.1.3.3">𝑁</ci></apply><apply id="S2.Ex6.m1.1.1.1.1.cmml" xref="S2.Ex6.m1.1.1.1.1"><apply id="S2.Ex6.m1.1.1.1.1.2.cmml" xref="S2.Ex6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex6.m1.1.1.1.1.2.1.cmml" xref="S2.Ex6.m1.1.1.1.1.2">superscript</csymbol><apply id="S2.Ex6.m1.1.1.1.1.2.2.cmml" xref="S2.Ex6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex6.m1.1.1.1.1.2.2.1.cmml" xref="S2.Ex6.m1.1.1.1.1.2">subscript</csymbol><sum id="S2.Ex6.m1.1.1.1.1.2.2.2.cmml" xref="S2.Ex6.m1.1.1.1.1.2.2.2"></sum><apply id="S2.Ex6.m1.1.1.1.1.2.2.3.cmml" xref="S2.Ex6.m1.1.1.1.1.2.2.3"><eq id="S2.Ex6.m1.1.1.1.1.2.2.3.1.cmml" xref="S2.Ex6.m1.1.1.1.1.2.2.3.1"></eq><ci id="S2.Ex6.m1.1.1.1.1.2.2.3.2.cmml" xref="S2.Ex6.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S2.Ex6.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S2.Ex6.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.Ex6.m1.1.1.1.1.2.3.cmml" xref="S2.Ex6.m1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S2.Ex6.m1.1.1.1.1.1.cmml" xref="S2.Ex6.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex6.m1.1.1.1.1.1.2.cmml" xref="S2.Ex6.m1.1.1.1.1.1">subscript</csymbol><apply id="S2.Ex6.m1.1.1.1.1.1.1.2.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex6.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.Ex6.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1"><minus id="S2.Ex6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2"><times id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2"><ci id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.2.2">𝑅</ci></apply><apply id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.2">𝑝</ci><ci id="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3"><times id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.1"></times><apply id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2"><ci id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2.1">¯</ci><ci id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.2.2">𝑅</ci></apply><apply id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.2">𝑝</ci><ci id="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.Ex6.m1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply><cn id="S2.Ex6.m1.1.1.1.1.1.3.cmml" type="integer" xref="S2.Ex6.m1.1.1.1.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex6.m1.1c">\displaystyle\mathcal{L}_{R}=\frac{1}{N}\sum_{i=1}^{N}\|\hat{R}p_{i}-\bar{R}p_%
{i}\|_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex6.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ over^ start_ARG italic_R end_ARG italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_R end_ARG italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.I1.i1.p1.5">where <math alttext="\hat{R}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.2.m1.1"><semantics id="S2.I1.i1.p1.2.m1.1a"><mover accent="true" id="S2.I1.i1.p1.2.m1.1.1" xref="S2.I1.i1.p1.2.m1.1.1.cmml"><mi id="S2.I1.i1.p1.2.m1.1.1.2" xref="S2.I1.i1.p1.2.m1.1.1.2.cmml">R</mi><mo id="S2.I1.i1.p1.2.m1.1.1.1" xref="S2.I1.i1.p1.2.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.2.m1.1b"><apply id="S2.I1.i1.p1.2.m1.1.1.cmml" xref="S2.I1.i1.p1.2.m1.1.1"><ci id="S2.I1.i1.p1.2.m1.1.1.1.cmml" xref="S2.I1.i1.p1.2.m1.1.1.1">^</ci><ci id="S2.I1.i1.p1.2.m1.1.1.2.cmml" xref="S2.I1.i1.p1.2.m1.1.1.2">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.2.m1.1c">\hat{R}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.2.m1.1d">over^ start_ARG italic_R end_ARG</annotation></semantics></math> is the estimated rotation matrix, <math alttext="\bar{R}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.3.m2.1"><semantics id="S2.I1.i1.p1.3.m2.1a"><mover accent="true" id="S2.I1.i1.p1.3.m2.1.1" xref="S2.I1.i1.p1.3.m2.1.1.cmml"><mi id="S2.I1.i1.p1.3.m2.1.1.2" xref="S2.I1.i1.p1.3.m2.1.1.2.cmml">R</mi><mo id="S2.I1.i1.p1.3.m2.1.1.1" xref="S2.I1.i1.p1.3.m2.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.3.m2.1b"><apply id="S2.I1.i1.p1.3.m2.1.1.cmml" xref="S2.I1.i1.p1.3.m2.1.1"><ci id="S2.I1.i1.p1.3.m2.1.1.1.cmml" xref="S2.I1.i1.p1.3.m2.1.1.1">¯</ci><ci id="S2.I1.i1.p1.3.m2.1.1.2.cmml" xref="S2.I1.i1.p1.3.m2.1.1.2">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.3.m2.1c">\bar{R}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.3.m2.1d">over¯ start_ARG italic_R end_ARG</annotation></semantics></math> is the ground truth rotation matrix, <math alttext="p_{i}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.4.m3.1"><semantics id="S2.I1.i1.p1.4.m3.1a"><msub id="S2.I1.i1.p1.4.m3.1.1" xref="S2.I1.i1.p1.4.m3.1.1.cmml"><mi id="S2.I1.i1.p1.4.m3.1.1.2" xref="S2.I1.i1.p1.4.m3.1.1.2.cmml">p</mi><mi id="S2.I1.i1.p1.4.m3.1.1.3" xref="S2.I1.i1.p1.4.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.4.m3.1b"><apply id="S2.I1.i1.p1.4.m3.1.1.cmml" xref="S2.I1.i1.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.4.m3.1.1.1.cmml" xref="S2.I1.i1.p1.4.m3.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.4.m3.1.1.2.cmml" xref="S2.I1.i1.p1.4.m3.1.1.2">𝑝</ci><ci id="S2.I1.i1.p1.4.m3.1.1.3.cmml" xref="S2.I1.i1.p1.4.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.4.m3.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.4.m3.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are the 3D points in the object model, and <math alttext="N" class="ltx_Math" display="inline" id="S2.I1.i1.p1.5.m4.1"><semantics id="S2.I1.i1.p1.5.m4.1a"><mi id="S2.I1.i1.p1.5.m4.1.1" xref="S2.I1.i1.p1.5.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.5.m4.1b"><ci id="S2.I1.i1.p1.5.m4.1.1.cmml" xref="S2.I1.i1.p1.5.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.5.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.5.m4.1d">italic_N</annotation></semantics></math> is the total number of points.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.2">Center Loss (<math alttext="\mathcal{L}_{\text{center}}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.1"><semantics id="S2.I1.i2.p1.1.m1.1a"><msub id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3a.cmml">center</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">ℒ</ci><ci id="S2.I1.i2.p1.1.m1.1.1.3a.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3"><mtext id="S2.I1.i2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.I1.i2.p1.1.m1.1.1.3">center</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">\mathcal{L}_{\text{center}}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT center end_POSTSUBSCRIPT</annotation></semantics></math>): <math alttext="L1" class="ltx_Math" display="inline" id="S2.I1.i2.p1.2.m2.1"><semantics id="S2.I1.i2.p1.2.m2.1a"><mrow id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.2" xref="S2.I1.i2.p1.2.m2.1.1.2.cmml">L</mi><mo id="S2.I1.i2.p1.2.m2.1.1.1" xref="S2.I1.i2.p1.2.m2.1.1.1.cmml">⁢</mo><mn id="S2.I1.i2.p1.2.m2.1.1.3" xref="S2.I1.i2.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><times id="S2.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.1"></times><ci id="S2.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2">𝐿</ci><cn id="S2.I1.i2.p1.2.m2.1.1.3.cmml" type="integer" xref="S2.I1.i2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">L1</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.2.m2.1d">italic_L 1</annotation></semantics></math> loss between the predicted and ground truth 2D center SITE components:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx4">
<tbody id="S2.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{center}}=\|(\hat{\delta}_{x},\hat{\delta}_{y})%
-(\bar{\delta}_{x},\bar{\delta}_{y})\|_{1}" class="ltx_Math" display="inline" id="S2.Ex7.m1.1"><semantics id="S2.Ex7.m1.1a"><mrow id="S2.Ex7.m1.1.1" xref="S2.Ex7.m1.1.1.cmml"><msub id="S2.Ex7.m1.1.1.3" xref="S2.Ex7.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex7.m1.1.1.3.2" xref="S2.Ex7.m1.1.1.3.2.cmml">ℒ</mi><mtext id="S2.Ex7.m1.1.1.3.3" xref="S2.Ex7.m1.1.1.3.3a.cmml">center</mtext></msub><mo id="S2.Ex7.m1.1.1.2" xref="S2.Ex7.m1.1.1.2.cmml">=</mo><msub id="S2.Ex7.m1.1.1.1" xref="S2.Ex7.m1.1.1.1.cmml"><mrow id="S2.Ex7.m1.1.1.1.1.1" xref="S2.Ex7.m1.1.1.1.1.2.cmml"><mo id="S2.Ex7.m1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex7.m1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.Ex7.m1.1.1.1.1.1.1" xref="S2.Ex7.m1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex7.m1.1.1.1.1.1.1.2.2" xref="S2.Ex7.m1.1.1.1.1.1.1.2.3.cmml"><mo id="S2.Ex7.m1.1.1.1.1.1.1.2.2.3" stretchy="false" xref="S2.Ex7.m1.1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">δ</mi><mo id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2.1" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.3.cmml">x</mi></msub><mo id="S2.Ex7.m1.1.1.1.1.1.1.2.2.4" xref="S2.Ex7.m1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.cmml"><mover accent="true" id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">δ</mi><mo id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2.1" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2.1.cmml">^</mo></mover><mi id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.3" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.3.cmml">y</mi></msub><mo id="S2.Ex7.m1.1.1.1.1.1.1.2.2.5" stretchy="false" xref="S2.Ex7.m1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow><mo id="S2.Ex7.m1.1.1.1.1.1.1.5" xref="S2.Ex7.m1.1.1.1.1.1.1.5.cmml">−</mo><mrow id="S2.Ex7.m1.1.1.1.1.1.1.4.2" xref="S2.Ex7.m1.1.1.1.1.1.1.4.3.cmml"><mo id="S2.Ex7.m1.1.1.1.1.1.1.4.2.3" stretchy="false" xref="S2.Ex7.m1.1.1.1.1.1.1.4.3.cmml">(</mo><msub id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.cmml"><mover accent="true" id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2.cmml"><mi id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2.2" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2.2.cmml">δ</mi><mo id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2.1" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2.1.cmml">¯</mo></mover><mi id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.3" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.3.cmml">x</mi></msub><mo id="S2.Ex7.m1.1.1.1.1.1.1.4.2.4" xref="S2.Ex7.m1.1.1.1.1.1.1.4.3.cmml">,</mo><msub id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.cmml"><mover accent="true" id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2.cmml"><mi id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2.2" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2.2.cmml">δ</mi><mo id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2.1" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2.1.cmml">¯</mo></mover><mi id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.3" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.3.cmml">y</mi></msub><mo id="S2.Ex7.m1.1.1.1.1.1.1.4.2.5" stretchy="false" xref="S2.Ex7.m1.1.1.1.1.1.1.4.3.cmml">)</mo></mrow></mrow><mo id="S2.Ex7.m1.1.1.1.1.1.3" stretchy="false" xref="S2.Ex7.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.Ex7.m1.1.1.1.3" xref="S2.Ex7.m1.1.1.1.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex7.m1.1b"><apply id="S2.Ex7.m1.1.1.cmml" xref="S2.Ex7.m1.1.1"><eq id="S2.Ex7.m1.1.1.2.cmml" xref="S2.Ex7.m1.1.1.2"></eq><apply id="S2.Ex7.m1.1.1.3.cmml" xref="S2.Ex7.m1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex7.m1.1.1.3.1.cmml" xref="S2.Ex7.m1.1.1.3">subscript</csymbol><ci id="S2.Ex7.m1.1.1.3.2.cmml" xref="S2.Ex7.m1.1.1.3.2">ℒ</ci><ci id="S2.Ex7.m1.1.1.3.3a.cmml" xref="S2.Ex7.m1.1.1.3.3"><mtext id="S2.Ex7.m1.1.1.3.3.cmml" mathsize="70%" xref="S2.Ex7.m1.1.1.3.3">center</mtext></ci></apply><apply id="S2.Ex7.m1.1.1.1.cmml" xref="S2.Ex7.m1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex7.m1.1.1.1.2.cmml" xref="S2.Ex7.m1.1.1.1">subscript</csymbol><apply id="S2.Ex7.m1.1.1.1.1.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex7.m1.1.1.1.1.2.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.2">norm</csymbol><apply id="S2.Ex7.m1.1.1.1.1.1.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1"><minus id="S2.Ex7.m1.1.1.1.1.1.1.5.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.5"></minus><interval closure="open" id="S2.Ex7.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2"><apply id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2"><ci id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.2.2">𝛿</ci></apply><ci id="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.1.1.1.3">𝑥</ci></apply><apply id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2">subscript</csymbol><apply id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2"><ci id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2.1">^</ci><ci id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.2.2">𝛿</ci></apply><ci id="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.2.2.2.3">𝑦</ci></apply></interval><interval closure="open" id="S2.Ex7.m1.1.1.1.1.1.1.4.3.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2"><apply id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1"><csymbol cd="ambiguous" id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1">subscript</csymbol><apply id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2"><ci id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2.1">¯</ci><ci id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.2.2">𝛿</ci></apply><ci id="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.3.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.3.1.1.3">𝑥</ci></apply><apply id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2">subscript</csymbol><apply id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2"><ci id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2.1.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2.1">¯</ci><ci id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2.2.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.2.2">𝛿</ci></apply><ci id="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.3.cmml" xref="S2.Ex7.m1.1.1.1.1.1.1.4.2.2.3">𝑦</ci></apply></interval></apply></apply><cn id="S2.Ex7.m1.1.1.1.3.cmml" type="integer" xref="S2.Ex7.m1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex7.m1.1c">\displaystyle\mathcal{L}_{\text{center}}=\|(\hat{\delta}_{x},\hat{\delta}_{y})%
-(\bar{\delta}_{x},\bar{\delta}_{y})\|_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex7.m1.1d">caligraphic_L start_POSTSUBSCRIPT center end_POSTSUBSCRIPT = ∥ ( over^ start_ARG italic_δ end_ARG start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , over^ start_ARG italic_δ end_ARG start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) - ( over¯ start_ARG italic_δ end_ARG start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , over¯ start_ARG italic_δ end_ARG start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Depth Loss (<math alttext="\mathcal{L}_{z}" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><msub id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml">ℒ</mi><mi id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2">ℒ</ci><ci id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">\mathcal{L}_{z}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT</annotation></semantics></math>): L1 loss between the predicted and ground truth object centroid SITE component:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx5">
<tbody id="S2.Ex8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{z}=\|\hat{\delta}_{z}-\bar{\delta}_{z}\|_{1}" class="ltx_Math" display="inline" id="S2.Ex8.m1.1"><semantics id="S2.Ex8.m1.1a"><mrow id="S2.Ex8.m1.1.1" xref="S2.Ex8.m1.1.1.cmml"><msub id="S2.Ex8.m1.1.1.3" xref="S2.Ex8.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex8.m1.1.1.3.2" xref="S2.Ex8.m1.1.1.3.2.cmml">ℒ</mi><mi id="S2.Ex8.m1.1.1.3.3" xref="S2.Ex8.m1.1.1.3.3.cmml">z</mi></msub><mo id="S2.Ex8.m1.1.1.2" xref="S2.Ex8.m1.1.1.2.cmml">=</mo><msub id="S2.Ex8.m1.1.1.1" xref="S2.Ex8.m1.1.1.1.cmml"><mrow id="S2.Ex8.m1.1.1.1.1.1" xref="S2.Ex8.m1.1.1.1.1.2.cmml"><mo id="S2.Ex8.m1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex8.m1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.Ex8.m1.1.1.1.1.1.1" xref="S2.Ex8.m1.1.1.1.1.1.1.cmml"><msub id="S2.Ex8.m1.1.1.1.1.1.1.2" xref="S2.Ex8.m1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S2.Ex8.m1.1.1.1.1.1.1.2.2" xref="S2.Ex8.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.Ex8.m1.1.1.1.1.1.1.2.2.2" xref="S2.Ex8.m1.1.1.1.1.1.1.2.2.2.cmml">δ</mi><mo id="S2.Ex8.m1.1.1.1.1.1.1.2.2.1" xref="S2.Ex8.m1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S2.Ex8.m1.1.1.1.1.1.1.2.3" xref="S2.Ex8.m1.1.1.1.1.1.1.2.3.cmml">z</mi></msub><mo id="S2.Ex8.m1.1.1.1.1.1.1.1" xref="S2.Ex8.m1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S2.Ex8.m1.1.1.1.1.1.1.3" xref="S2.Ex8.m1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S2.Ex8.m1.1.1.1.1.1.1.3.2" xref="S2.Ex8.m1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.Ex8.m1.1.1.1.1.1.1.3.2.2" xref="S2.Ex8.m1.1.1.1.1.1.1.3.2.2.cmml">δ</mi><mo id="S2.Ex8.m1.1.1.1.1.1.1.3.2.1" xref="S2.Ex8.m1.1.1.1.1.1.1.3.2.1.cmml">¯</mo></mover><mi id="S2.Ex8.m1.1.1.1.1.1.1.3.3" xref="S2.Ex8.m1.1.1.1.1.1.1.3.3.cmml">z</mi></msub></mrow><mo id="S2.Ex8.m1.1.1.1.1.1.3" stretchy="false" xref="S2.Ex8.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.Ex8.m1.1.1.1.3" xref="S2.Ex8.m1.1.1.1.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex8.m1.1b"><apply id="S2.Ex8.m1.1.1.cmml" xref="S2.Ex8.m1.1.1"><eq id="S2.Ex8.m1.1.1.2.cmml" xref="S2.Ex8.m1.1.1.2"></eq><apply id="S2.Ex8.m1.1.1.3.cmml" xref="S2.Ex8.m1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex8.m1.1.1.3.1.cmml" xref="S2.Ex8.m1.1.1.3">subscript</csymbol><ci id="S2.Ex8.m1.1.1.3.2.cmml" xref="S2.Ex8.m1.1.1.3.2">ℒ</ci><ci id="S2.Ex8.m1.1.1.3.3.cmml" xref="S2.Ex8.m1.1.1.3.3">𝑧</ci></apply><apply id="S2.Ex8.m1.1.1.1.cmml" xref="S2.Ex8.m1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex8.m1.1.1.1.2.cmml" xref="S2.Ex8.m1.1.1.1">subscript</csymbol><apply id="S2.Ex8.m1.1.1.1.1.2.cmml" xref="S2.Ex8.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex8.m1.1.1.1.1.2.1.cmml" xref="S2.Ex8.m1.1.1.1.1.1.2">norm</csymbol><apply id="S2.Ex8.m1.1.1.1.1.1.1.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1"><minus id="S2.Ex8.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.1"></minus><apply id="S2.Ex8.m1.1.1.1.1.1.1.2.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex8.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S2.Ex8.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.2.2"><ci id="S2.Ex8.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S2.Ex8.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.2.2.2">𝛿</ci></apply><ci id="S2.Ex8.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.2.3">𝑧</ci></apply><apply id="S2.Ex8.m1.1.1.1.1.1.1.3.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex8.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S2.Ex8.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.3.2"><ci id="S2.Ex8.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.3.2.1">¯</ci><ci id="S2.Ex8.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.3.2.2">𝛿</ci></apply><ci id="S2.Ex8.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex8.m1.1.1.1.1.1.1.3.3">𝑧</ci></apply></apply></apply><cn id="S2.Ex8.m1.1.1.1.3.cmml" type="integer" xref="S2.Ex8.m1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex8.m1.1c">\displaystyle\mathcal{L}_{z}=\|\hat{\delta}_{z}-\bar{\delta}_{z}\|_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex8.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT = ∥ over^ start_ARG italic_δ end_ARG start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT - over¯ start_ARG italic_δ end_ARG start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Geometry Loss (<math alttext="\mathcal{L}_{\text{Geom}}" class="ltx_Math" display="inline" id="S2.SS3.SSS2.Px2.1.m1.1"><semantics id="S2.SS3.SSS2.Px2.1.m1.1b"><msub id="S2.SS3.SSS2.Px2.1.m1.1.1" xref="S2.SS3.SSS2.Px2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.SSS2.Px2.1.m1.1.1.2" xref="S2.SS3.SSS2.Px2.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.SS3.SSS2.Px2.1.m1.1.1.3" xref="S2.SS3.SSS2.Px2.1.m1.1.1.3a.cmml">Geom</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.Px2.1.m1.1c"><apply id="S2.SS3.SSS2.Px2.1.m1.1.1.cmml" xref="S2.SS3.SSS2.Px2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.Px2.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.Px2.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.Px2.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.Px2.1.m1.1.1.2">ℒ</ci><ci id="S2.SS3.SSS2.Px2.1.m1.1.1.3a.cmml" xref="S2.SS3.SSS2.Px2.1.m1.1.1.3"><mtext id="S2.SS3.SSS2.Px2.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS3.SSS2.Px2.1.m1.1.1.3">Geom</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.Px2.1.m1.1d">\mathcal{L}_{\text{Geom}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.Px2.1.m1.1e">caligraphic_L start_POSTSUBSCRIPT Geom end_POSTSUBSCRIPT</annotation></semantics></math>)</h5>
<div class="ltx_para" id="S2.SS3.SSS2.Px2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.Px2.p1.1">This loss combines the dense correspondence loss and the mask losses:</p>
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1">Mask Loss (<math alttext="\mathcal{L}_{\text{mask}}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.1.m1.1"><semantics id="S2.I2.i1.p1.1.m1.1a"><msub id="S2.I2.i1.p1.1.m1.1.1" xref="S2.I2.i1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.I2.i1.p1.1.m1.1.1.2" xref="S2.I2.i1.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.I2.i1.p1.1.m1.1.1.3" xref="S2.I2.i1.p1.1.m1.1.1.3a.cmml">mask</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.1.m1.1b"><apply id="S2.I2.i1.p1.1.m1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I2.i1.p1.1.m1.1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I2.i1.p1.1.m1.1.1.2.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2">ℒ</ci><ci id="S2.I2.i1.p1.1.m1.1.1.3a.cmml" xref="S2.I2.i1.p1.1.m1.1.1.3"><mtext id="S2.I2.i1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.I2.i1.p1.1.m1.1.1.3">mask</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.1.m1.1c">\mathcal{L}_{\text{mask}}</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT</annotation></semantics></math>): The mask loss consists of two terms, the visible mask loss and the amodal (full) mask loss:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx6">
<tbody id="S2.Ex9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{mask}}=\|\hat{M}_{\text{vis}}-\bar{M}_{\text{%
vis}}\|_{1}+\|\hat{M}_{\text{full}}-\bar{M}_{\text{full}}\|_{1}" class="ltx_Math" display="inline" id="S2.Ex9.m1.2"><semantics id="S2.Ex9.m1.2a"><mrow id="S2.Ex9.m1.2.2" xref="S2.Ex9.m1.2.2.cmml"><msub id="S2.Ex9.m1.2.2.4" xref="S2.Ex9.m1.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex9.m1.2.2.4.2" xref="S2.Ex9.m1.2.2.4.2.cmml">ℒ</mi><mtext id="S2.Ex9.m1.2.2.4.3" xref="S2.Ex9.m1.2.2.4.3a.cmml">mask</mtext></msub><mo id="S2.Ex9.m1.2.2.3" xref="S2.Ex9.m1.2.2.3.cmml">=</mo><mrow id="S2.Ex9.m1.2.2.2" xref="S2.Ex9.m1.2.2.2.cmml"><msub id="S2.Ex9.m1.1.1.1.1" xref="S2.Ex9.m1.1.1.1.1.cmml"><mrow id="S2.Ex9.m1.1.1.1.1.1.1" xref="S2.Ex9.m1.1.1.1.1.1.2.cmml"><mo id="S2.Ex9.m1.1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex9.m1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.Ex9.m1.1.1.1.1.1.1.1" xref="S2.Ex9.m1.1.1.1.1.1.1.1.cmml"><msub id="S2.Ex9.m1.1.1.1.1.1.1.1.2" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S2.Ex9.m1.1.1.1.1.1.1.1.2.2" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.Ex9.m1.1.1.1.1.1.1.1.2.2.2" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2.2.2.cmml">M</mi><mo id="S2.Ex9.m1.1.1.1.1.1.1.1.2.2.1" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mtext id="S2.Ex9.m1.1.1.1.1.1.1.1.2.3" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2.3a.cmml">vis</mtext></msub><mo id="S2.Ex9.m1.1.1.1.1.1.1.1.1" xref="S2.Ex9.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S2.Ex9.m1.1.1.1.1.1.1.1.3" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S2.Ex9.m1.1.1.1.1.1.1.1.3.2" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.Ex9.m1.1.1.1.1.1.1.1.3.2.2" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3.2.2.cmml">M</mi><mo id="S2.Ex9.m1.1.1.1.1.1.1.1.3.2.1" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3.2.1.cmml">¯</mo></mover><mtext id="S2.Ex9.m1.1.1.1.1.1.1.1.3.3" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3.3a.cmml">vis</mtext></msub></mrow><mo id="S2.Ex9.m1.1.1.1.1.1.1.3" stretchy="false" xref="S2.Ex9.m1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.Ex9.m1.1.1.1.1.3" xref="S2.Ex9.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.Ex9.m1.2.2.2.3" xref="S2.Ex9.m1.2.2.2.3.cmml">+</mo><msub id="S2.Ex9.m1.2.2.2.2" xref="S2.Ex9.m1.2.2.2.2.cmml"><mrow id="S2.Ex9.m1.2.2.2.2.1.1" xref="S2.Ex9.m1.2.2.2.2.1.2.cmml"><mo id="S2.Ex9.m1.2.2.2.2.1.1.2" stretchy="false" xref="S2.Ex9.m1.2.2.2.2.1.2.1.cmml">‖</mo><mrow id="S2.Ex9.m1.2.2.2.2.1.1.1" xref="S2.Ex9.m1.2.2.2.2.1.1.1.cmml"><msub id="S2.Ex9.m1.2.2.2.2.1.1.1.2" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2.cmml"><mover accent="true" id="S2.Ex9.m1.2.2.2.2.1.1.1.2.2" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2.2.cmml"><mi id="S2.Ex9.m1.2.2.2.2.1.1.1.2.2.2" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2.2.2.cmml">M</mi><mo id="S2.Ex9.m1.2.2.2.2.1.1.1.2.2.1" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2.2.1.cmml">^</mo></mover><mtext id="S2.Ex9.m1.2.2.2.2.1.1.1.2.3" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2.3a.cmml">full</mtext></msub><mo id="S2.Ex9.m1.2.2.2.2.1.1.1.1" xref="S2.Ex9.m1.2.2.2.2.1.1.1.1.cmml">−</mo><msub id="S2.Ex9.m1.2.2.2.2.1.1.1.3" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3.cmml"><mover accent="true" id="S2.Ex9.m1.2.2.2.2.1.1.1.3.2" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3.2.cmml"><mi id="S2.Ex9.m1.2.2.2.2.1.1.1.3.2.2" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3.2.2.cmml">M</mi><mo id="S2.Ex9.m1.2.2.2.2.1.1.1.3.2.1" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3.2.1.cmml">¯</mo></mover><mtext id="S2.Ex9.m1.2.2.2.2.1.1.1.3.3" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3.3a.cmml">full</mtext></msub></mrow><mo id="S2.Ex9.m1.2.2.2.2.1.1.3" stretchy="false" xref="S2.Ex9.m1.2.2.2.2.1.2.1.cmml">‖</mo></mrow><mn id="S2.Ex9.m1.2.2.2.2.3" xref="S2.Ex9.m1.2.2.2.2.3.cmml">1</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex9.m1.2b"><apply id="S2.Ex9.m1.2.2.cmml" xref="S2.Ex9.m1.2.2"><eq id="S2.Ex9.m1.2.2.3.cmml" xref="S2.Ex9.m1.2.2.3"></eq><apply id="S2.Ex9.m1.2.2.4.cmml" xref="S2.Ex9.m1.2.2.4"><csymbol cd="ambiguous" id="S2.Ex9.m1.2.2.4.1.cmml" xref="S2.Ex9.m1.2.2.4">subscript</csymbol><ci id="S2.Ex9.m1.2.2.4.2.cmml" xref="S2.Ex9.m1.2.2.4.2">ℒ</ci><ci id="S2.Ex9.m1.2.2.4.3a.cmml" xref="S2.Ex9.m1.2.2.4.3"><mtext id="S2.Ex9.m1.2.2.4.3.cmml" mathsize="70%" xref="S2.Ex9.m1.2.2.4.3">mask</mtext></ci></apply><apply id="S2.Ex9.m1.2.2.2.cmml" xref="S2.Ex9.m1.2.2.2"><plus id="S2.Ex9.m1.2.2.2.3.cmml" xref="S2.Ex9.m1.2.2.2.3"></plus><apply id="S2.Ex9.m1.1.1.1.1.cmml" xref="S2.Ex9.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex9.m1.1.1.1.1.2.cmml" xref="S2.Ex9.m1.1.1.1.1">subscript</csymbol><apply id="S2.Ex9.m1.1.1.1.1.1.2.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex9.m1.1.1.1.1.1.2.1.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.Ex9.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1"><minus id="S2.Ex9.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.1"></minus><apply id="S2.Ex9.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex9.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S2.Ex9.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2.2"><ci id="S2.Ex9.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S2.Ex9.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2.2.2">𝑀</ci></apply><ci id="S2.Ex9.m1.1.1.1.1.1.1.1.2.3a.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2.3"><mtext id="S2.Ex9.m1.1.1.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S2.Ex9.m1.1.1.1.1.1.1.1.2.3">vis</mtext></ci></apply><apply id="S2.Ex9.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex9.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S2.Ex9.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3.2"><ci id="S2.Ex9.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3.2.1">¯</ci><ci id="S2.Ex9.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3.2.2">𝑀</ci></apply><ci id="S2.Ex9.m1.1.1.1.1.1.1.1.3.3a.cmml" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3.3"><mtext id="S2.Ex9.m1.1.1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.Ex9.m1.1.1.1.1.1.1.1.3.3">vis</mtext></ci></apply></apply></apply><cn id="S2.Ex9.m1.1.1.1.1.3.cmml" type="integer" xref="S2.Ex9.m1.1.1.1.1.3">1</cn></apply><apply id="S2.Ex9.m1.2.2.2.2.cmml" xref="S2.Ex9.m1.2.2.2.2"><csymbol cd="ambiguous" id="S2.Ex9.m1.2.2.2.2.2.cmml" xref="S2.Ex9.m1.2.2.2.2">subscript</csymbol><apply id="S2.Ex9.m1.2.2.2.2.1.2.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1"><csymbol cd="latexml" id="S2.Ex9.m1.2.2.2.2.1.2.1.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.2">norm</csymbol><apply id="S2.Ex9.m1.2.2.2.2.1.1.1.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1"><minus id="S2.Ex9.m1.2.2.2.2.1.1.1.1.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.1"></minus><apply id="S2.Ex9.m1.2.2.2.2.1.1.1.2.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex9.m1.2.2.2.2.1.1.1.2.1.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2">subscript</csymbol><apply id="S2.Ex9.m1.2.2.2.2.1.1.1.2.2.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2.2"><ci id="S2.Ex9.m1.2.2.2.2.1.1.1.2.2.1.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2.2.1">^</ci><ci id="S2.Ex9.m1.2.2.2.2.1.1.1.2.2.2.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2.2.2">𝑀</ci></apply><ci id="S2.Ex9.m1.2.2.2.2.1.1.1.2.3a.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2.3"><mtext id="S2.Ex9.m1.2.2.2.2.1.1.1.2.3.cmml" mathsize="70%" xref="S2.Ex9.m1.2.2.2.2.1.1.1.2.3">full</mtext></ci></apply><apply id="S2.Ex9.m1.2.2.2.2.1.1.1.3.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex9.m1.2.2.2.2.1.1.1.3.1.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3">subscript</csymbol><apply id="S2.Ex9.m1.2.2.2.2.1.1.1.3.2.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3.2"><ci id="S2.Ex9.m1.2.2.2.2.1.1.1.3.2.1.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3.2.1">¯</ci><ci id="S2.Ex9.m1.2.2.2.2.1.1.1.3.2.2.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3.2.2">𝑀</ci></apply><ci id="S2.Ex9.m1.2.2.2.2.1.1.1.3.3a.cmml" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3.3"><mtext id="S2.Ex9.m1.2.2.2.2.1.1.1.3.3.cmml" mathsize="70%" xref="S2.Ex9.m1.2.2.2.2.1.1.1.3.3">full</mtext></ci></apply></apply></apply><cn id="S2.Ex9.m1.2.2.2.2.3.cmml" type="integer" xref="S2.Ex9.m1.2.2.2.2.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex9.m1.2c">\displaystyle\mathcal{L}_{\text{mask}}=\|\hat{M}_{\text{vis}}-\bar{M}_{\text{%
vis}}\|_{1}+\|\hat{M}_{\text{full}}-\bar{M}_{\text{full}}\|_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex9.m1.2d">caligraphic_L start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT = ∥ over^ start_ARG italic_M end_ARG start_POSTSUBSCRIPT vis end_POSTSUBSCRIPT - over¯ start_ARG italic_M end_ARG start_POSTSUBSCRIPT vis end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ∥ over^ start_ARG italic_M end_ARG start_POSTSUBSCRIPT full end_POSTSUBSCRIPT - over¯ start_ARG italic_M end_ARG start_POSTSUBSCRIPT full end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.I2.i1.p2">
<p class="ltx_p" id="S2.I2.i1.p2.2">where <math alttext="M_{\text{vis}}" class="ltx_Math" display="inline" id="S2.I2.i1.p2.1.m1.1"><semantics id="S2.I2.i1.p2.1.m1.1a"><msub id="S2.I2.i1.p2.1.m1.1.1" xref="S2.I2.i1.p2.1.m1.1.1.cmml"><mi id="S2.I2.i1.p2.1.m1.1.1.2" xref="S2.I2.i1.p2.1.m1.1.1.2.cmml">M</mi><mtext id="S2.I2.i1.p2.1.m1.1.1.3" xref="S2.I2.i1.p2.1.m1.1.1.3a.cmml">vis</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p2.1.m1.1b"><apply id="S2.I2.i1.p2.1.m1.1.1.cmml" xref="S2.I2.i1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I2.i1.p2.1.m1.1.1.1.cmml" xref="S2.I2.i1.p2.1.m1.1.1">subscript</csymbol><ci id="S2.I2.i1.p2.1.m1.1.1.2.cmml" xref="S2.I2.i1.p2.1.m1.1.1.2">𝑀</ci><ci id="S2.I2.i1.p2.1.m1.1.1.3a.cmml" xref="S2.I2.i1.p2.1.m1.1.1.3"><mtext id="S2.I2.i1.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.I2.i1.p2.1.m1.1.1.3">vis</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p2.1.m1.1c">M_{\text{vis}}</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p2.1.m1.1d">italic_M start_POSTSUBSCRIPT vis end_POSTSUBSCRIPT</annotation></semantics></math> is the visible object mask and <math alttext="M_{\text{full}}" class="ltx_Math" display="inline" id="S2.I2.i1.p2.2.m2.1"><semantics id="S2.I2.i1.p2.2.m2.1a"><msub id="S2.I2.i1.p2.2.m2.1.1" xref="S2.I2.i1.p2.2.m2.1.1.cmml"><mi id="S2.I2.i1.p2.2.m2.1.1.2" xref="S2.I2.i1.p2.2.m2.1.1.2.cmml">M</mi><mtext id="S2.I2.i1.p2.2.m2.1.1.3" xref="S2.I2.i1.p2.2.m2.1.1.3a.cmml">full</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p2.2.m2.1b"><apply id="S2.I2.i1.p2.2.m2.1.1.cmml" xref="S2.I2.i1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I2.i1.p2.2.m2.1.1.1.cmml" xref="S2.I2.i1.p2.2.m2.1.1">subscript</csymbol><ci id="S2.I2.i1.p2.2.m2.1.1.2.cmml" xref="S2.I2.i1.p2.2.m2.1.1.2">𝑀</ci><ci id="S2.I2.i1.p2.2.m2.1.1.3a.cmml" xref="S2.I2.i1.p2.2.m2.1.1.3"><mtext id="S2.I2.i1.p2.2.m2.1.1.3.cmml" mathsize="70%" xref="S2.I2.i1.p2.2.m2.1.1.3">full</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p2.2.m2.1c">M_{\text{full}}</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p2.2.m2.1d">italic_M start_POSTSUBSCRIPT full end_POSTSUBSCRIPT</annotation></semantics></math> is the full object mask.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.2">2D-3D Dense Correspondence Loss (<math alttext="\mathcal{L}_{\text{corr}}" class="ltx_Math" display="inline" id="S2.I2.i2.p1.1.m1.1"><semantics id="S2.I2.i2.p1.1.m1.1a"><msub id="S2.I2.i2.p1.1.m1.1.1" xref="S2.I2.i2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.I2.i2.p1.1.m1.1.1.2" xref="S2.I2.i2.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.I2.i2.p1.1.m1.1.1.3" xref="S2.I2.i2.p1.1.m1.1.1.3a.cmml">corr</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.1.m1.1b"><apply id="S2.I2.i2.p1.1.m1.1.1.cmml" xref="S2.I2.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I2.i2.p1.1.m1.1.1.1.cmml" xref="S2.I2.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I2.i2.p1.1.m1.1.1.2.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2">ℒ</ci><ci id="S2.I2.i2.p1.1.m1.1.1.3a.cmml" xref="S2.I2.i2.p1.1.m1.1.1.3"><mtext id="S2.I2.i2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.I2.i2.p1.1.m1.1.1.3">corr</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.1.m1.1c">\mathcal{L}_{\text{corr}}</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT corr end_POSTSUBSCRIPT</annotation></semantics></math>): <math alttext="L1" class="ltx_Math" display="inline" id="S2.I2.i2.p1.2.m2.1"><semantics id="S2.I2.i2.p1.2.m2.1a"><mrow id="S2.I2.i2.p1.2.m2.1.1" xref="S2.I2.i2.p1.2.m2.1.1.cmml"><mi id="S2.I2.i2.p1.2.m2.1.1.2" xref="S2.I2.i2.p1.2.m2.1.1.2.cmml">L</mi><mo id="S2.I2.i2.p1.2.m2.1.1.1" xref="S2.I2.i2.p1.2.m2.1.1.1.cmml">⁢</mo><mn id="S2.I2.i2.p1.2.m2.1.1.3" xref="S2.I2.i2.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.2.m2.1b"><apply id="S2.I2.i2.p1.2.m2.1.1.cmml" xref="S2.I2.i2.p1.2.m2.1.1"><times id="S2.I2.i2.p1.2.m2.1.1.1.cmml" xref="S2.I2.i2.p1.2.m2.1.1.1"></times><ci id="S2.I2.i2.p1.2.m2.1.1.2.cmml" xref="S2.I2.i2.p1.2.m2.1.1.2">𝐿</ci><cn id="S2.I2.i2.p1.2.m2.1.1.3.cmml" type="integer" xref="S2.I2.i2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.2.m2.1c">L1</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p1.2.m2.1d">italic_L 1</annotation></semantics></math> loss between the predicted and ground truth dense 2D-3D correspondence maps:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx7">
<tbody id="S2.Ex10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{corr}}=\frac{1}{\sum{\bar{M}_{\text{vis}}}}\|%
\bar{M}_{\text{vis}}\odot(\hat{D}-\bar{D})\|_{1}" class="ltx_Math" display="inline" id="S2.Ex10.m1.1"><semantics id="S2.Ex10.m1.1a"><mrow id="S2.Ex10.m1.1.1" xref="S2.Ex10.m1.1.1.cmml"><msub id="S2.Ex10.m1.1.1.3" xref="S2.Ex10.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex10.m1.1.1.3.2" xref="S2.Ex10.m1.1.1.3.2.cmml">ℒ</mi><mtext id="S2.Ex10.m1.1.1.3.3" xref="S2.Ex10.m1.1.1.3.3a.cmml">corr</mtext></msub><mo id="S2.Ex10.m1.1.1.2" xref="S2.Ex10.m1.1.1.2.cmml">=</mo><mrow id="S2.Ex10.m1.1.1.1" xref="S2.Ex10.m1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.Ex10.m1.1.1.1.3" xref="S2.Ex10.m1.1.1.1.3.cmml"><mfrac id="S2.Ex10.m1.1.1.1.3a" xref="S2.Ex10.m1.1.1.1.3.cmml"><mn id="S2.Ex10.m1.1.1.1.3.2" xref="S2.Ex10.m1.1.1.1.3.2.cmml">1</mn><mrow id="S2.Ex10.m1.1.1.1.3.3" xref="S2.Ex10.m1.1.1.1.3.3.cmml"><mo id="S2.Ex10.m1.1.1.1.3.3.1" xref="S2.Ex10.m1.1.1.1.3.3.1.cmml">∑</mo><msub id="S2.Ex10.m1.1.1.1.3.3.2" xref="S2.Ex10.m1.1.1.1.3.3.2.cmml"><mover accent="true" id="S2.Ex10.m1.1.1.1.3.3.2.2" xref="S2.Ex10.m1.1.1.1.3.3.2.2.cmml"><mi id="S2.Ex10.m1.1.1.1.3.3.2.2.2" xref="S2.Ex10.m1.1.1.1.3.3.2.2.2.cmml">M</mi><mo id="S2.Ex10.m1.1.1.1.3.3.2.2.1" xref="S2.Ex10.m1.1.1.1.3.3.2.2.1.cmml">¯</mo></mover><mtext id="S2.Ex10.m1.1.1.1.3.3.2.3" xref="S2.Ex10.m1.1.1.1.3.3.2.3a.cmml">vis</mtext></msub></mrow></mfrac></mstyle><mo id="S2.Ex10.m1.1.1.1.2" xref="S2.Ex10.m1.1.1.1.2.cmml">⁢</mo><msub id="S2.Ex10.m1.1.1.1.1" xref="S2.Ex10.m1.1.1.1.1.cmml"><mrow id="S2.Ex10.m1.1.1.1.1.1.1" xref="S2.Ex10.m1.1.1.1.1.1.2.cmml"><mo id="S2.Ex10.m1.1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex10.m1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.Ex10.m1.1.1.1.1.1.1.1" xref="S2.Ex10.m1.1.1.1.1.1.1.1.cmml"><msub id="S2.Ex10.m1.1.1.1.1.1.1.1.3" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S2.Ex10.m1.1.1.1.1.1.1.1.3.2" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.Ex10.m1.1.1.1.1.1.1.1.3.2.2" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3.2.2.cmml">M</mi><mo id="S2.Ex10.m1.1.1.1.1.1.1.1.3.2.1" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3.2.1.cmml">¯</mo></mover><mtext id="S2.Ex10.m1.1.1.1.1.1.1.1.3.3" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3.3a.cmml">vis</mtext></msub><mo id="S2.Ex10.m1.1.1.1.1.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S2.Ex10.m1.1.1.1.1.1.1.1.2.cmml">⊙</mo><mrow id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">D</mi><mo id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">D</mi><mo id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">¯</mo></mover></mrow><mo id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex10.m1.1.1.1.1.1.1.3" stretchy="false" xref="S2.Ex10.m1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.Ex10.m1.1.1.1.1.3" xref="S2.Ex10.m1.1.1.1.1.3.cmml">1</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex10.m1.1b"><apply id="S2.Ex10.m1.1.1.cmml" xref="S2.Ex10.m1.1.1"><eq id="S2.Ex10.m1.1.1.2.cmml" xref="S2.Ex10.m1.1.1.2"></eq><apply id="S2.Ex10.m1.1.1.3.cmml" xref="S2.Ex10.m1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex10.m1.1.1.3.1.cmml" xref="S2.Ex10.m1.1.1.3">subscript</csymbol><ci id="S2.Ex10.m1.1.1.3.2.cmml" xref="S2.Ex10.m1.1.1.3.2">ℒ</ci><ci id="S2.Ex10.m1.1.1.3.3a.cmml" xref="S2.Ex10.m1.1.1.3.3"><mtext id="S2.Ex10.m1.1.1.3.3.cmml" mathsize="70%" xref="S2.Ex10.m1.1.1.3.3">corr</mtext></ci></apply><apply id="S2.Ex10.m1.1.1.1.cmml" xref="S2.Ex10.m1.1.1.1"><times id="S2.Ex10.m1.1.1.1.2.cmml" xref="S2.Ex10.m1.1.1.1.2"></times><apply id="S2.Ex10.m1.1.1.1.3.cmml" xref="S2.Ex10.m1.1.1.1.3"><divide id="S2.Ex10.m1.1.1.1.3.1.cmml" xref="S2.Ex10.m1.1.1.1.3"></divide><cn id="S2.Ex10.m1.1.1.1.3.2.cmml" type="integer" xref="S2.Ex10.m1.1.1.1.3.2">1</cn><apply id="S2.Ex10.m1.1.1.1.3.3.cmml" xref="S2.Ex10.m1.1.1.1.3.3"><sum id="S2.Ex10.m1.1.1.1.3.3.1.cmml" xref="S2.Ex10.m1.1.1.1.3.3.1"></sum><apply id="S2.Ex10.m1.1.1.1.3.3.2.cmml" xref="S2.Ex10.m1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.Ex10.m1.1.1.1.3.3.2.1.cmml" xref="S2.Ex10.m1.1.1.1.3.3.2">subscript</csymbol><apply id="S2.Ex10.m1.1.1.1.3.3.2.2.cmml" xref="S2.Ex10.m1.1.1.1.3.3.2.2"><ci id="S2.Ex10.m1.1.1.1.3.3.2.2.1.cmml" xref="S2.Ex10.m1.1.1.1.3.3.2.2.1">¯</ci><ci id="S2.Ex10.m1.1.1.1.3.3.2.2.2.cmml" xref="S2.Ex10.m1.1.1.1.3.3.2.2.2">𝑀</ci></apply><ci id="S2.Ex10.m1.1.1.1.3.3.2.3a.cmml" xref="S2.Ex10.m1.1.1.1.3.3.2.3"><mtext id="S2.Ex10.m1.1.1.1.3.3.2.3.cmml" mathsize="70%" xref="S2.Ex10.m1.1.1.1.3.3.2.3">vis</mtext></ci></apply></apply></apply><apply id="S2.Ex10.m1.1.1.1.1.cmml" xref="S2.Ex10.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex10.m1.1.1.1.1.2.cmml" xref="S2.Ex10.m1.1.1.1.1">subscript</csymbol><apply id="S2.Ex10.m1.1.1.1.1.1.2.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex10.m1.1.1.1.1.1.2.1.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.Ex10.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex10.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.2">direct-product</csymbol><apply id="S2.Ex10.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex10.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S2.Ex10.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3.2"><ci id="S2.Ex10.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3.2.1">¯</ci><ci id="S2.Ex10.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3.2.2">𝑀</ci></apply><ci id="S2.Ex10.m1.1.1.1.1.1.1.1.3.3a.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3.3"><mtext id="S2.Ex10.m1.1.1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.Ex10.m1.1.1.1.1.1.1.1.3.3">vis</mtext></ci></apply><apply id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1"><minus id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.2.2">𝐷</ci></apply><apply id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3"><ci id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3.1">¯</ci><ci id="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex10.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝐷</ci></apply></apply></apply></apply><cn id="S2.Ex10.m1.1.1.1.1.3.cmml" type="integer" xref="S2.Ex10.m1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex10.m1.1c">\displaystyle\mathcal{L}_{\text{corr}}=\frac{1}{\sum{\bar{M}_{\text{vis}}}}\|%
\bar{M}_{\text{vis}}\odot(\hat{D}-\bar{D})\|_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex10.m1.1d">caligraphic_L start_POSTSUBSCRIPT corr end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG ∑ over¯ start_ARG italic_M end_ARG start_POSTSUBSCRIPT vis end_POSTSUBSCRIPT end_ARG ∥ over¯ start_ARG italic_M end_ARG start_POSTSUBSCRIPT vis end_POSTSUBSCRIPT ⊙ ( over^ start_ARG italic_D end_ARG - over¯ start_ARG italic_D end_ARG ) ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.I2.i2.p2">
<p class="ltx_p" id="S2.I2.i2.p2.2">where <math alttext="D" class="ltx_Math" display="inline" id="S2.I2.i2.p2.1.m1.1"><semantics id="S2.I2.i2.p2.1.m1.1a"><mi id="S2.I2.i2.p2.1.m1.1.1" xref="S2.I2.i2.p2.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p2.1.m1.1b"><ci id="S2.I2.i2.p2.1.m1.1.1.cmml" xref="S2.I2.i2.p2.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p2.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p2.1.m1.1d">italic_D</annotation></semantics></math> is the 2D-3D dense correspondence, and <math alttext="\bar{M}_{\text{vis}}" class="ltx_Math" display="inline" id="S2.I2.i2.p2.2.m2.1"><semantics id="S2.I2.i2.p2.2.m2.1a"><msub id="S2.I2.i2.p2.2.m2.1.1" xref="S2.I2.i2.p2.2.m2.1.1.cmml"><mover accent="true" id="S2.I2.i2.p2.2.m2.1.1.2" xref="S2.I2.i2.p2.2.m2.1.1.2.cmml"><mi id="S2.I2.i2.p2.2.m2.1.1.2.2" xref="S2.I2.i2.p2.2.m2.1.1.2.2.cmml">M</mi><mo id="S2.I2.i2.p2.2.m2.1.1.2.1" xref="S2.I2.i2.p2.2.m2.1.1.2.1.cmml">¯</mo></mover><mtext id="S2.I2.i2.p2.2.m2.1.1.3" xref="S2.I2.i2.p2.2.m2.1.1.3a.cmml">vis</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p2.2.m2.1b"><apply id="S2.I2.i2.p2.2.m2.1.1.cmml" xref="S2.I2.i2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I2.i2.p2.2.m2.1.1.1.cmml" xref="S2.I2.i2.p2.2.m2.1.1">subscript</csymbol><apply id="S2.I2.i2.p2.2.m2.1.1.2.cmml" xref="S2.I2.i2.p2.2.m2.1.1.2"><ci id="S2.I2.i2.p2.2.m2.1.1.2.1.cmml" xref="S2.I2.i2.p2.2.m2.1.1.2.1">¯</ci><ci id="S2.I2.i2.p2.2.m2.1.1.2.2.cmml" xref="S2.I2.i2.p2.2.m2.1.1.2.2">𝑀</ci></apply><ci id="S2.I2.i2.p2.2.m2.1.1.3a.cmml" xref="S2.I2.i2.p2.2.m2.1.1.3"><mtext id="S2.I2.i2.p2.2.m2.1.1.3.cmml" mathsize="70%" xref="S2.I2.i2.p2.2.m2.1.1.3">vis</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p2.2.m2.1c">\bar{M}_{\text{vis}}</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p2.2.m2.1d">over¯ start_ARG italic_M end_ARG start_POSTSUBSCRIPT vis end_POSTSUBSCRIPT</annotation></semantics></math> is the ground-truth visible object mask.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S2.SS3.SSS2.Px2.p1.2">The total geometry loss is the sum of the dense correspondence loss and the mask loss:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx8">
<tbody id="S2.Ex11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{Geom}}=\mathcal{L}_{\text{corr}}+\mathcal{L}_{%
\text{mask}}" class="ltx_Math" display="inline" id="S2.Ex11.m1.1"><semantics id="S2.Ex11.m1.1a"><mrow id="S2.Ex11.m1.1.1" xref="S2.Ex11.m1.1.1.cmml"><msub id="S2.Ex11.m1.1.1.2" xref="S2.Ex11.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex11.m1.1.1.2.2" xref="S2.Ex11.m1.1.1.2.2.cmml">ℒ</mi><mtext id="S2.Ex11.m1.1.1.2.3" xref="S2.Ex11.m1.1.1.2.3a.cmml">Geom</mtext></msub><mo id="S2.Ex11.m1.1.1.1" xref="S2.Ex11.m1.1.1.1.cmml">=</mo><mrow id="S2.Ex11.m1.1.1.3" xref="S2.Ex11.m1.1.1.3.cmml"><msub id="S2.Ex11.m1.1.1.3.2" xref="S2.Ex11.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex11.m1.1.1.3.2.2" xref="S2.Ex11.m1.1.1.3.2.2.cmml">ℒ</mi><mtext id="S2.Ex11.m1.1.1.3.2.3" xref="S2.Ex11.m1.1.1.3.2.3a.cmml">corr</mtext></msub><mo id="S2.Ex11.m1.1.1.3.1" xref="S2.Ex11.m1.1.1.3.1.cmml">+</mo><msub id="S2.Ex11.m1.1.1.3.3" xref="S2.Ex11.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex11.m1.1.1.3.3.2" xref="S2.Ex11.m1.1.1.3.3.2.cmml">ℒ</mi><mtext id="S2.Ex11.m1.1.1.3.3.3" xref="S2.Ex11.m1.1.1.3.3.3a.cmml">mask</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex11.m1.1b"><apply id="S2.Ex11.m1.1.1.cmml" xref="S2.Ex11.m1.1.1"><eq id="S2.Ex11.m1.1.1.1.cmml" xref="S2.Ex11.m1.1.1.1"></eq><apply id="S2.Ex11.m1.1.1.2.cmml" xref="S2.Ex11.m1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex11.m1.1.1.2.1.cmml" xref="S2.Ex11.m1.1.1.2">subscript</csymbol><ci id="S2.Ex11.m1.1.1.2.2.cmml" xref="S2.Ex11.m1.1.1.2.2">ℒ</ci><ci id="S2.Ex11.m1.1.1.2.3a.cmml" xref="S2.Ex11.m1.1.1.2.3"><mtext id="S2.Ex11.m1.1.1.2.3.cmml" mathsize="70%" xref="S2.Ex11.m1.1.1.2.3">Geom</mtext></ci></apply><apply id="S2.Ex11.m1.1.1.3.cmml" xref="S2.Ex11.m1.1.1.3"><plus id="S2.Ex11.m1.1.1.3.1.cmml" xref="S2.Ex11.m1.1.1.3.1"></plus><apply id="S2.Ex11.m1.1.1.3.2.cmml" xref="S2.Ex11.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex11.m1.1.1.3.2.1.cmml" xref="S2.Ex11.m1.1.1.3.2">subscript</csymbol><ci id="S2.Ex11.m1.1.1.3.2.2.cmml" xref="S2.Ex11.m1.1.1.3.2.2">ℒ</ci><ci id="S2.Ex11.m1.1.1.3.2.3a.cmml" xref="S2.Ex11.m1.1.1.3.2.3"><mtext id="S2.Ex11.m1.1.1.3.2.3.cmml" mathsize="70%" xref="S2.Ex11.m1.1.1.3.2.3">corr</mtext></ci></apply><apply id="S2.Ex11.m1.1.1.3.3.cmml" xref="S2.Ex11.m1.1.1.3.3"><csymbol cd="ambiguous" id="S2.Ex11.m1.1.1.3.3.1.cmml" xref="S2.Ex11.m1.1.1.3.3">subscript</csymbol><ci id="S2.Ex11.m1.1.1.3.3.2.cmml" xref="S2.Ex11.m1.1.1.3.3.2">ℒ</ci><ci id="S2.Ex11.m1.1.1.3.3.3a.cmml" xref="S2.Ex11.m1.1.1.3.3.3"><mtext id="S2.Ex11.m1.1.1.3.3.3.cmml" mathsize="70%" xref="S2.Ex11.m1.1.1.3.3.3">mask</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex11.m1.1c">\displaystyle\mathcal{L}_{\text{Geom}}=\mathcal{L}_{\text{corr}}+\mathcal{L}_{%
\text{mask}}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex11.m1.1d">caligraphic_L start_POSTSUBSCRIPT Geom end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT corr end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Articulation Loss (<math alttext="L_{\text{Art}}" class="ltx_Math" display="inline" id="S2.SS3.SSS2.Px3.1.m1.1"><semantics id="S2.SS3.SSS2.Px3.1.m1.1b"><msub id="S2.SS3.SSS2.Px3.1.m1.1.1" xref="S2.SS3.SSS2.Px3.1.m1.1.1.cmml"><mi id="S2.SS3.SSS2.Px3.1.m1.1.1.2" xref="S2.SS3.SSS2.Px3.1.m1.1.1.2.cmml">L</mi><mtext id="S2.SS3.SSS2.Px3.1.m1.1.1.3" xref="S2.SS3.SSS2.Px3.1.m1.1.1.3a.cmml">Art</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.Px3.1.m1.1c"><apply id="S2.SS3.SSS2.Px3.1.m1.1.1.cmml" xref="S2.SS3.SSS2.Px3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.Px3.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.Px3.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.Px3.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.Px3.1.m1.1.1.2">𝐿</ci><ci id="S2.SS3.SSS2.Px3.1.m1.1.1.3a.cmml" xref="S2.SS3.SSS2.Px3.1.m1.1.1.3"><mtext id="S2.SS3.SSS2.Px3.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS3.SSS2.Px3.1.m1.1.1.3">Art</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.Px3.1.m1.1d">L_{\text{Art}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.Px3.1.m1.1e">italic_L start_POSTSUBSCRIPT Art end_POSTSUBSCRIPT</annotation></semantics></math>)</h5>
<div class="ltx_para" id="S2.SS3.SSS2.Px3.p1">
<p class="ltx_p" id="S2.SS3.SSS2.Px3.p1.1">L1 loss between the predicted and ground truth articulation angles.</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx9">
<tbody id="S2.Ex12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{z}=\|\hat{A}-\bar{A}\|_{1}" class="ltx_Math" display="inline" id="S2.Ex12.m1.1"><semantics id="S2.Ex12.m1.1a"><mrow id="S2.Ex12.m1.1.1" xref="S2.Ex12.m1.1.1.cmml"><msub id="S2.Ex12.m1.1.1.3" xref="S2.Ex12.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex12.m1.1.1.3.2" xref="S2.Ex12.m1.1.1.3.2.cmml">ℒ</mi><mi id="S2.Ex12.m1.1.1.3.3" xref="S2.Ex12.m1.1.1.3.3.cmml">z</mi></msub><mo id="S2.Ex12.m1.1.1.2" xref="S2.Ex12.m1.1.1.2.cmml">=</mo><msub id="S2.Ex12.m1.1.1.1" xref="S2.Ex12.m1.1.1.1.cmml"><mrow id="S2.Ex12.m1.1.1.1.1.1" xref="S2.Ex12.m1.1.1.1.1.2.cmml"><mo id="S2.Ex12.m1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex12.m1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.Ex12.m1.1.1.1.1.1.1" xref="S2.Ex12.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S2.Ex12.m1.1.1.1.1.1.1.2" xref="S2.Ex12.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex12.m1.1.1.1.1.1.1.2.2" xref="S2.Ex12.m1.1.1.1.1.1.1.2.2.cmml">A</mi><mo id="S2.Ex12.m1.1.1.1.1.1.1.2.1" xref="S2.Ex12.m1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S2.Ex12.m1.1.1.1.1.1.1.1" xref="S2.Ex12.m1.1.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S2.Ex12.m1.1.1.1.1.1.1.3" xref="S2.Ex12.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex12.m1.1.1.1.1.1.1.3.2" xref="S2.Ex12.m1.1.1.1.1.1.1.3.2.cmml">A</mi><mo id="S2.Ex12.m1.1.1.1.1.1.1.3.1" xref="S2.Ex12.m1.1.1.1.1.1.1.3.1.cmml">¯</mo></mover></mrow><mo id="S2.Ex12.m1.1.1.1.1.1.3" stretchy="false" xref="S2.Ex12.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.Ex12.m1.1.1.1.3" xref="S2.Ex12.m1.1.1.1.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex12.m1.1b"><apply id="S2.Ex12.m1.1.1.cmml" xref="S2.Ex12.m1.1.1"><eq id="S2.Ex12.m1.1.1.2.cmml" xref="S2.Ex12.m1.1.1.2"></eq><apply id="S2.Ex12.m1.1.1.3.cmml" xref="S2.Ex12.m1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex12.m1.1.1.3.1.cmml" xref="S2.Ex12.m1.1.1.3">subscript</csymbol><ci id="S2.Ex12.m1.1.1.3.2.cmml" xref="S2.Ex12.m1.1.1.3.2">ℒ</ci><ci id="S2.Ex12.m1.1.1.3.3.cmml" xref="S2.Ex12.m1.1.1.3.3">𝑧</ci></apply><apply id="S2.Ex12.m1.1.1.1.cmml" xref="S2.Ex12.m1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex12.m1.1.1.1.2.cmml" xref="S2.Ex12.m1.1.1.1">subscript</csymbol><apply id="S2.Ex12.m1.1.1.1.1.2.cmml" xref="S2.Ex12.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex12.m1.1.1.1.1.2.1.cmml" xref="S2.Ex12.m1.1.1.1.1.1.2">norm</csymbol><apply id="S2.Ex12.m1.1.1.1.1.1.1.cmml" xref="S2.Ex12.m1.1.1.1.1.1.1"><minus id="S2.Ex12.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex12.m1.1.1.1.1.1.1.1"></minus><apply id="S2.Ex12.m1.1.1.1.1.1.1.2.cmml" xref="S2.Ex12.m1.1.1.1.1.1.1.2"><ci id="S2.Ex12.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex12.m1.1.1.1.1.1.1.2.1">^</ci><ci id="S2.Ex12.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex12.m1.1.1.1.1.1.1.2.2">𝐴</ci></apply><apply id="S2.Ex12.m1.1.1.1.1.1.1.3.cmml" xref="S2.Ex12.m1.1.1.1.1.1.1.3"><ci id="S2.Ex12.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex12.m1.1.1.1.1.1.1.3.1">¯</ci><ci id="S2.Ex12.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex12.m1.1.1.1.1.1.1.3.2">𝐴</ci></apply></apply></apply><cn id="S2.Ex12.m1.1.1.1.3.cmml" type="integer" xref="S2.Ex12.m1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex12.m1.1c">\displaystyle\mathcal{L}_{z}=\|\hat{A}-\bar{A}\|_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex12.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT = ∥ over^ start_ARG italic_A end_ARG - over¯ start_ARG italic_A end_ARG ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS3.SSS2.Px3.p1.2">where A is the normalized articulation angle.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS2.Px4">
<h5 class="ltx_title ltx_title_paragraph">Object Category Loss (<math alttext="L_{\text{Cat}}" class="ltx_Math" display="inline" id="S2.SS3.SSS2.Px4.1.m1.1"><semantics id="S2.SS3.SSS2.Px4.1.m1.1b"><msub id="S2.SS3.SSS2.Px4.1.m1.1.1" xref="S2.SS3.SSS2.Px4.1.m1.1.1.cmml"><mi id="S2.SS3.SSS2.Px4.1.m1.1.1.2" xref="S2.SS3.SSS2.Px4.1.m1.1.1.2.cmml">L</mi><mtext id="S2.SS3.SSS2.Px4.1.m1.1.1.3" xref="S2.SS3.SSS2.Px4.1.m1.1.1.3a.cmml">Cat</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.Px4.1.m1.1c"><apply id="S2.SS3.SSS2.Px4.1.m1.1.1.cmml" xref="S2.SS3.SSS2.Px4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.Px4.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.Px4.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.Px4.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.Px4.1.m1.1.1.2">𝐿</ci><ci id="S2.SS3.SSS2.Px4.1.m1.1.1.3a.cmml" xref="S2.SS3.SSS2.Px4.1.m1.1.1.3"><mtext id="S2.SS3.SSS2.Px4.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS3.SSS2.Px4.1.m1.1.1.3">Cat</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.Px4.1.m1.1d">L_{\text{Cat}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.Px4.1.m1.1e">italic_L start_POSTSUBSCRIPT Cat end_POSTSUBSCRIPT</annotation></semantics></math>)</h5>
<div class="ltx_para" id="S2.SS3.SSS2.Px4.p1">
<p class="ltx_p" id="S2.SS3.SSS2.Px4.p1.1">Cross-entropy loss of the object categories.</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx10">
<tbody id="S2.Ex13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{Cat}}=\text{CE}(\hat{\text{C}},\bar{\text{C}})" class="ltx_Math" display="inline" id="S2.Ex13.m1.2"><semantics id="S2.Ex13.m1.2a"><mrow id="S2.Ex13.m1.2.3" xref="S2.Ex13.m1.2.3.cmml"><msub id="S2.Ex13.m1.2.3.2" xref="S2.Ex13.m1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex13.m1.2.3.2.2" xref="S2.Ex13.m1.2.3.2.2.cmml">ℒ</mi><mtext id="S2.Ex13.m1.2.3.2.3" xref="S2.Ex13.m1.2.3.2.3a.cmml">Cat</mtext></msub><mo id="S2.Ex13.m1.2.3.1" xref="S2.Ex13.m1.2.3.1.cmml">=</mo><mrow id="S2.Ex13.m1.2.3.3" xref="S2.Ex13.m1.2.3.3.cmml"><mtext id="S2.Ex13.m1.2.3.3.2" xref="S2.Ex13.m1.2.3.3.2a.cmml">CE</mtext><mo id="S2.Ex13.m1.2.3.3.1" xref="S2.Ex13.m1.2.3.3.1.cmml">⁢</mo><mrow id="S2.Ex13.m1.2.3.3.3.2" xref="S2.Ex13.m1.2.3.3.3.1.cmml"><mo id="S2.Ex13.m1.2.3.3.3.2.1" stretchy="false" xref="S2.Ex13.m1.2.3.3.3.1.cmml">(</mo><mover accent="true" id="S2.Ex13.m1.1.1" xref="S2.Ex13.m1.1.1.cmml"><mtext id="S2.Ex13.m1.1.1.2" xref="S2.Ex13.m1.1.1.2a.cmml">C</mtext><mo id="S2.Ex13.m1.1.1.1" xref="S2.Ex13.m1.1.1.1.cmml">^</mo></mover><mo id="S2.Ex13.m1.2.3.3.3.2.2" xref="S2.Ex13.m1.2.3.3.3.1.cmml">,</mo><mover accent="true" id="S2.Ex13.m1.2.2" xref="S2.Ex13.m1.2.2.cmml"><mtext id="S2.Ex13.m1.2.2.2" xref="S2.Ex13.m1.2.2.2a.cmml">C</mtext><mo id="S2.Ex13.m1.2.2.1" xref="S2.Ex13.m1.2.2.1.cmml">¯</mo></mover><mo id="S2.Ex13.m1.2.3.3.3.2.3" stretchy="false" xref="S2.Ex13.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex13.m1.2b"><apply id="S2.Ex13.m1.2.3.cmml" xref="S2.Ex13.m1.2.3"><eq id="S2.Ex13.m1.2.3.1.cmml" xref="S2.Ex13.m1.2.3.1"></eq><apply id="S2.Ex13.m1.2.3.2.cmml" xref="S2.Ex13.m1.2.3.2"><csymbol cd="ambiguous" id="S2.Ex13.m1.2.3.2.1.cmml" xref="S2.Ex13.m1.2.3.2">subscript</csymbol><ci id="S2.Ex13.m1.2.3.2.2.cmml" xref="S2.Ex13.m1.2.3.2.2">ℒ</ci><ci id="S2.Ex13.m1.2.3.2.3a.cmml" xref="S2.Ex13.m1.2.3.2.3"><mtext id="S2.Ex13.m1.2.3.2.3.cmml" mathsize="70%" xref="S2.Ex13.m1.2.3.2.3">Cat</mtext></ci></apply><apply id="S2.Ex13.m1.2.3.3.cmml" xref="S2.Ex13.m1.2.3.3"><times id="S2.Ex13.m1.2.3.3.1.cmml" xref="S2.Ex13.m1.2.3.3.1"></times><ci id="S2.Ex13.m1.2.3.3.2a.cmml" xref="S2.Ex13.m1.2.3.3.2"><mtext id="S2.Ex13.m1.2.3.3.2.cmml" xref="S2.Ex13.m1.2.3.3.2">CE</mtext></ci><interval closure="open" id="S2.Ex13.m1.2.3.3.3.1.cmml" xref="S2.Ex13.m1.2.3.3.3.2"><apply id="S2.Ex13.m1.1.1.cmml" xref="S2.Ex13.m1.1.1"><ci id="S2.Ex13.m1.1.1.1.cmml" xref="S2.Ex13.m1.1.1.1">^</ci><ci id="S2.Ex13.m1.1.1.2a.cmml" xref="S2.Ex13.m1.1.1.2"><mtext id="S2.Ex13.m1.1.1.2.cmml" xref="S2.Ex13.m1.1.1.2">C</mtext></ci></apply><apply id="S2.Ex13.m1.2.2.cmml" xref="S2.Ex13.m1.2.2"><ci id="S2.Ex13.m1.2.2.1.cmml" xref="S2.Ex13.m1.2.2.1">¯</ci><ci id="S2.Ex13.m1.2.2.2a.cmml" xref="S2.Ex13.m1.2.2.2"><mtext id="S2.Ex13.m1.2.2.2.cmml" xref="S2.Ex13.m1.2.2.2">C</mtext></ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex13.m1.2c">\displaystyle\mathcal{L}_{\text{Cat}}=\text{CE}(\hat{\text{C}},\bar{\text{C}})</annotation><annotation encoding="application/x-llamapun" id="S2.Ex13.m1.2d">caligraphic_L start_POSTSUBSCRIPT Cat end_POSTSUBSCRIPT = CE ( over^ start_ARG C end_ARG , over¯ start_ARG C end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS3.SSS2.Px4.p1.2">where C is the is the object class.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.Px4.p2">
<p class="ltx_p" id="S2.SS3.SSS2.Px4.p2.1">The total loss is a weighted sum of these individual loss components:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathcal{L}_{\text{total}}=w_{\text{Pose}}\cdot\mathcal{L}_{\text{Pose}}+w_{%
\text{Geom}}\cdot\mathcal{L}_{\text{Geom}}+w_{\text{Cat}}\cdot\mathcal{L}_{%
\text{Cat}}+w_{\text{Art}}\cdot\mathcal{L}_{\text{Art}}" class="ltx_Math" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><msub id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.2.2" xref="S2.E1.m1.1.1.2.2.cmml">ℒ</mi><mtext id="S2.E1.m1.1.1.2.3" xref="S2.E1.m1.1.1.2.3a.cmml">total</mtext></msub><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mrow id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><msub id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2.2" xref="S2.E1.m1.1.1.3.2.2.2.cmml">w</mi><mtext id="S2.E1.m1.1.1.3.2.2.3" xref="S2.E1.m1.1.1.3.2.2.3a.cmml">Pose</mtext></msub><mo id="S2.E1.m1.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.1.1.3.2.1.cmml">⋅</mo><msub id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.3.2.3.2" xref="S2.E1.m1.1.1.3.2.3.2.cmml">ℒ</mi><mtext id="S2.E1.m1.1.1.3.2.3.3" xref="S2.E1.m1.1.1.3.2.3.3a.cmml">Pose</mtext></msub></mrow><mo id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><msub id="S2.E1.m1.1.1.3.3.2" xref="S2.E1.m1.1.1.3.3.2.cmml"><mi id="S2.E1.m1.1.1.3.3.2.2" xref="S2.E1.m1.1.1.3.3.2.2.cmml">w</mi><mtext id="S2.E1.m1.1.1.3.3.2.3" xref="S2.E1.m1.1.1.3.3.2.3a.cmml">Geom</mtext></msub><mo id="S2.E1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.1.1.3.3.1.cmml">⋅</mo><msub id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.3.3.3.2" xref="S2.E1.m1.1.1.3.3.3.2.cmml">ℒ</mi><mtext id="S2.E1.m1.1.1.3.3.3.3" xref="S2.E1.m1.1.1.3.3.3.3a.cmml">Geom</mtext></msub></mrow><mo id="S2.E1.m1.1.1.3.1a" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.3.4" xref="S2.E1.m1.1.1.3.4.cmml"><msub id="S2.E1.m1.1.1.3.4.2" xref="S2.E1.m1.1.1.3.4.2.cmml"><mi id="S2.E1.m1.1.1.3.4.2.2" xref="S2.E1.m1.1.1.3.4.2.2.cmml">w</mi><mtext id="S2.E1.m1.1.1.3.4.2.3" xref="S2.E1.m1.1.1.3.4.2.3a.cmml">Cat</mtext></msub><mo id="S2.E1.m1.1.1.3.4.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.1.1.3.4.1.cmml">⋅</mo><msub id="S2.E1.m1.1.1.3.4.3" xref="S2.E1.m1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.3.4.3.2" xref="S2.E1.m1.1.1.3.4.3.2.cmml">ℒ</mi><mtext id="S2.E1.m1.1.1.3.4.3.3" xref="S2.E1.m1.1.1.3.4.3.3a.cmml">Cat</mtext></msub></mrow><mo id="S2.E1.m1.1.1.3.1b" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.3.5" xref="S2.E1.m1.1.1.3.5.cmml"><msub id="S2.E1.m1.1.1.3.5.2" xref="S2.E1.m1.1.1.3.5.2.cmml"><mi id="S2.E1.m1.1.1.3.5.2.2" xref="S2.E1.m1.1.1.3.5.2.2.cmml">w</mi><mtext id="S2.E1.m1.1.1.3.5.2.3" xref="S2.E1.m1.1.1.3.5.2.3a.cmml">Art</mtext></msub><mo id="S2.E1.m1.1.1.3.5.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.1.1.3.5.1.cmml">⋅</mo><msub id="S2.E1.m1.1.1.3.5.3" xref="S2.E1.m1.1.1.3.5.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.3.5.3.2" xref="S2.E1.m1.1.1.3.5.3.2.cmml">ℒ</mi><mtext id="S2.E1.m1.1.1.3.5.3.3" xref="S2.E1.m1.1.1.3.5.3.3a.cmml">Art</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">ℒ</ci><ci id="S2.E1.m1.1.1.2.3a.cmml" xref="S2.E1.m1.1.1.2.3"><mtext id="S2.E1.m1.1.1.2.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.2.3">total</mtext></ci></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><plus id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></plus><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><ci id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.1">⋅</ci><apply id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.2.1.cmml" xref="S2.E1.m1.1.1.3.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.3.2.2.3a.cmml" xref="S2.E1.m1.1.1.3.2.2.3"><mtext id="S2.E1.m1.1.1.3.2.2.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.3.2.2.3">Pose</mtext></ci></apply><apply id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.3.1.cmml" xref="S2.E1.m1.1.1.3.2.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.3.2.cmml" xref="S2.E1.m1.1.1.3.2.3.2">ℒ</ci><ci id="S2.E1.m1.1.1.3.2.3.3a.cmml" xref="S2.E1.m1.1.1.3.2.3.3"><mtext id="S2.E1.m1.1.1.3.2.3.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.3.2.3.3">Pose</mtext></ci></apply></apply><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><ci id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.1">⋅</ci><apply id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.2.2.cmml" xref="S2.E1.m1.1.1.3.3.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.3.3.2.3a.cmml" xref="S2.E1.m1.1.1.3.3.2.3"><mtext id="S2.E1.m1.1.1.3.3.2.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.3.3.2.3">Geom</mtext></ci></apply><apply id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2">ℒ</ci><ci id="S2.E1.m1.1.1.3.3.3.3a.cmml" xref="S2.E1.m1.1.1.3.3.3.3"><mtext id="S2.E1.m1.1.1.3.3.3.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.3.3.3.3">Geom</mtext></ci></apply></apply><apply id="S2.E1.m1.1.1.3.4.cmml" xref="S2.E1.m1.1.1.3.4"><ci id="S2.E1.m1.1.1.3.4.1.cmml" xref="S2.E1.m1.1.1.3.4.1">⋅</ci><apply id="S2.E1.m1.1.1.3.4.2.cmml" xref="S2.E1.m1.1.1.3.4.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.4.2.1.cmml" xref="S2.E1.m1.1.1.3.4.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.4.2.2.cmml" xref="S2.E1.m1.1.1.3.4.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.3.4.2.3a.cmml" xref="S2.E1.m1.1.1.3.4.2.3"><mtext id="S2.E1.m1.1.1.3.4.2.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.3.4.2.3">Cat</mtext></ci></apply><apply id="S2.E1.m1.1.1.3.4.3.cmml" xref="S2.E1.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.4.3.1.cmml" xref="S2.E1.m1.1.1.3.4.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.4.3.2.cmml" xref="S2.E1.m1.1.1.3.4.3.2">ℒ</ci><ci id="S2.E1.m1.1.1.3.4.3.3a.cmml" xref="S2.E1.m1.1.1.3.4.3.3"><mtext id="S2.E1.m1.1.1.3.4.3.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.3.4.3.3">Cat</mtext></ci></apply></apply><apply id="S2.E1.m1.1.1.3.5.cmml" xref="S2.E1.m1.1.1.3.5"><ci id="S2.E1.m1.1.1.3.5.1.cmml" xref="S2.E1.m1.1.1.3.5.1">⋅</ci><apply id="S2.E1.m1.1.1.3.5.2.cmml" xref="S2.E1.m1.1.1.3.5.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.5.2.1.cmml" xref="S2.E1.m1.1.1.3.5.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.5.2.2.cmml" xref="S2.E1.m1.1.1.3.5.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.3.5.2.3a.cmml" xref="S2.E1.m1.1.1.3.5.2.3"><mtext id="S2.E1.m1.1.1.3.5.2.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.3.5.2.3">Art</mtext></ci></apply><apply id="S2.E1.m1.1.1.3.5.3.cmml" xref="S2.E1.m1.1.1.3.5.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.5.3.1.cmml" xref="S2.E1.m1.1.1.3.5.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.5.3.2.cmml" xref="S2.E1.m1.1.1.3.5.3.2">ℒ</ci><ci id="S2.E1.m1.1.1.3.5.3.3a.cmml" xref="S2.E1.m1.1.1.3.5.3.3"><mtext id="S2.E1.m1.1.1.3.5.3.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.3.5.3.3">Art</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\mathcal{L}_{\text{total}}=w_{\text{Pose}}\cdot\mathcal{L}_{\text{Pose}}+w_{%
\text{Geom}}\cdot\mathcal{L}_{\text{Geom}}+w_{\text{Cat}}\cdot\mathcal{L}_{%
\text{Cat}}+w_{\text{Art}}\cdot\mathcal{L}_{\text{Art}}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">caligraphic_L start_POSTSUBSCRIPT total end_POSTSUBSCRIPT = italic_w start_POSTSUBSCRIPT Pose end_POSTSUBSCRIPT ⋅ caligraphic_L start_POSTSUBSCRIPT Pose end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT Geom end_POSTSUBSCRIPT ⋅ caligraphic_L start_POSTSUBSCRIPT Geom end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT Cat end_POSTSUBSCRIPT ⋅ caligraphic_L start_POSTSUBSCRIPT Cat end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT Art end_POSTSUBSCRIPT ⋅ caligraphic_L start_POSTSUBSCRIPT Art end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>Pose Model Architecture</h4>
<div class="ltx_para" id="S2.SS3.SSS3.p1">
<p class="ltx_p" id="S2.SS3.SSS3.p1.1">Our pose estimation network simultaneously estimates object category, articulation angle, and pose from image crops. The network consists of two interconnected sections: (1) the intermediate feature regression stage and (2) the pose regression stage (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.F6" title="Fig. 6 ‣ 2.3.3 Pose Model Architecture ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="525" id="S2.F6.g1" src="extracted/5735855/graphics/network_new2.png" width="657"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 6: </span><span class="ltx_text ltx_font_bold" id="S2.F6.4.1">Pose Model Architecture:</span> The input image is cropped at the Region of Interest (RoI) and processed by the Multitask Feature Regression. The output, <math alttext="M_{2D-3D}" class="ltx_Math" display="inline" id="S2.F6.2.m1.1"><semantics id="S2.F6.2.m1.1b"><msub id="S2.F6.2.m1.1.1" xref="S2.F6.2.m1.1.1.cmml"><mi id="S2.F6.2.m1.1.1.2" xref="S2.F6.2.m1.1.1.2.cmml">M</mi><mrow id="S2.F6.2.m1.1.1.3" xref="S2.F6.2.m1.1.1.3.cmml"><mrow id="S2.F6.2.m1.1.1.3.2" xref="S2.F6.2.m1.1.1.3.2.cmml"><mn id="S2.F6.2.m1.1.1.3.2.2" xref="S2.F6.2.m1.1.1.3.2.2.cmml">2</mn><mo id="S2.F6.2.m1.1.1.3.2.1" xref="S2.F6.2.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S2.F6.2.m1.1.1.3.2.3" xref="S2.F6.2.m1.1.1.3.2.3.cmml">D</mi></mrow><mo id="S2.F6.2.m1.1.1.3.1" xref="S2.F6.2.m1.1.1.3.1.cmml">−</mo><mrow id="S2.F6.2.m1.1.1.3.3" xref="S2.F6.2.m1.1.1.3.3.cmml"><mn id="S2.F6.2.m1.1.1.3.3.2" xref="S2.F6.2.m1.1.1.3.3.2.cmml">3</mn><mo id="S2.F6.2.m1.1.1.3.3.1" xref="S2.F6.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S2.F6.2.m1.1.1.3.3.3" xref="S2.F6.2.m1.1.1.3.3.3.cmml">D</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.F6.2.m1.1c"><apply id="S2.F6.2.m1.1.1.cmml" xref="S2.F6.2.m1.1.1"><csymbol cd="ambiguous" id="S2.F6.2.m1.1.1.1.cmml" xref="S2.F6.2.m1.1.1">subscript</csymbol><ci id="S2.F6.2.m1.1.1.2.cmml" xref="S2.F6.2.m1.1.1.2">𝑀</ci><apply id="S2.F6.2.m1.1.1.3.cmml" xref="S2.F6.2.m1.1.1.3"><minus id="S2.F6.2.m1.1.1.3.1.cmml" xref="S2.F6.2.m1.1.1.3.1"></minus><apply id="S2.F6.2.m1.1.1.3.2.cmml" xref="S2.F6.2.m1.1.1.3.2"><times id="S2.F6.2.m1.1.1.3.2.1.cmml" xref="S2.F6.2.m1.1.1.3.2.1"></times><cn id="S2.F6.2.m1.1.1.3.2.2.cmml" type="integer" xref="S2.F6.2.m1.1.1.3.2.2">2</cn><ci id="S2.F6.2.m1.1.1.3.2.3.cmml" xref="S2.F6.2.m1.1.1.3.2.3">𝐷</ci></apply><apply id="S2.F6.2.m1.1.1.3.3.cmml" xref="S2.F6.2.m1.1.1.3.3"><times id="S2.F6.2.m1.1.1.3.3.1.cmml" xref="S2.F6.2.m1.1.1.3.3.1"></times><cn id="S2.F6.2.m1.1.1.3.3.2.cmml" type="integer" xref="S2.F6.2.m1.1.1.3.3.2">3</cn><ci id="S2.F6.2.m1.1.1.3.3.3.cmml" xref="S2.F6.2.m1.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F6.2.m1.1d">M_{2D-3D}</annotation><annotation encoding="application/x-llamapun" id="S2.F6.2.m1.1e">italic_M start_POSTSUBSCRIPT 2 italic_D - 3 italic_D end_POSTSUBSCRIPT</annotation></semantics></math>, is then forwarded to Patch-PnP during training or to PnP-RANSAC during testing.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.SSS3.p2">
<p class="ltx_p" id="S2.SS3.SSS3.p2.1">The intermediate feature regression stage is based on the Feature Pyramid Network (FPN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib33" title="">33</a>]</cite> with a Vision Transformer (ViT) backbone pretrained following the method described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib43" title="">43</a>]</cite> on the LVD-142M dataset. The features extracted by the ViT backbone are fed into a Pyramid Pooling Module (PPM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib66" title="">66</a>]</cite> before being fed back to the FPN. The object category head is attached to the PPM output for early branching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib19" title="">19</a>]</cite>. The fused feature map from the FPN is directed to three heads: articulation, mask, and 2D-3D Dense Correspondence (<math alttext="M_{2D-3D}" class="ltx_Math" display="inline" id="S2.SS3.SSS3.p2.1.m1.1"><semantics id="S2.SS3.SSS3.p2.1.m1.1a"><msub id="S2.SS3.SSS3.p2.1.m1.1.1" xref="S2.SS3.SSS3.p2.1.m1.1.1.cmml"><mi id="S2.SS3.SSS3.p2.1.m1.1.1.2" xref="S2.SS3.SSS3.p2.1.m1.1.1.2.cmml">M</mi><mrow id="S2.SS3.SSS3.p2.1.m1.1.1.3" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.cmml"><mrow id="S2.SS3.SSS3.p2.1.m1.1.1.3.2" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.2.cmml"><mn id="S2.SS3.SSS3.p2.1.m1.1.1.3.2.2" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.2.2.cmml">2</mn><mo id="S2.SS3.SSS3.p2.1.m1.1.1.3.2.1" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S2.SS3.SSS3.p2.1.m1.1.1.3.2.3" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.2.3.cmml">D</mi></mrow><mo id="S2.SS3.SSS3.p2.1.m1.1.1.3.1" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.1.cmml">−</mo><mrow id="S2.SS3.SSS3.p2.1.m1.1.1.3.3" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.3.cmml"><mn id="S2.SS3.SSS3.p2.1.m1.1.1.3.3.2" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.3.2.cmml">3</mn><mo id="S2.SS3.SSS3.p2.1.m1.1.1.3.3.1" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S2.SS3.SSS3.p2.1.m1.1.1.3.3.3" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.3.3.cmml">D</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS3.p2.1.m1.1b"><apply id="S2.SS3.SSS3.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS3.p2.1.m1.1.1.1.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS3.p2.1.m1.1.1.2.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1.2">𝑀</ci><apply id="S2.SS3.SSS3.p2.1.m1.1.1.3.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1.3"><minus id="S2.SS3.SSS3.p2.1.m1.1.1.3.1.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.1"></minus><apply id="S2.SS3.SSS3.p2.1.m1.1.1.3.2.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.2"><times id="S2.SS3.SSS3.p2.1.m1.1.1.3.2.1.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.2.1"></times><cn id="S2.SS3.SSS3.p2.1.m1.1.1.3.2.2.cmml" type="integer" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.2.2">2</cn><ci id="S2.SS3.SSS3.p2.1.m1.1.1.3.2.3.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.2.3">𝐷</ci></apply><apply id="S2.SS3.SSS3.p2.1.m1.1.1.3.3.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.3"><times id="S2.SS3.SSS3.p2.1.m1.1.1.3.3.1.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.3.1"></times><cn id="S2.SS3.SSS3.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.3.2">3</cn><ci id="S2.SS3.SSS3.p2.1.m1.1.1.3.3.3.cmml" xref="S2.SS3.SSS3.p2.1.m1.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS3.p2.1.m1.1c">M_{2D-3D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS3.p2.1.m1.1d">italic_M start_POSTSUBSCRIPT 2 italic_D - 3 italic_D end_POSTSUBSCRIPT</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS3.p3">
<p class="ltx_p" id="S2.SS3.SSS3.p3.3">The pose regression stage varies depending on the phase. During training, the <math alttext="M_{2D-3D}" class="ltx_Math" display="inline" id="S2.SS3.SSS3.p3.1.m1.1"><semantics id="S2.SS3.SSS3.p3.1.m1.1a"><msub id="S2.SS3.SSS3.p3.1.m1.1.1" xref="S2.SS3.SSS3.p3.1.m1.1.1.cmml"><mi id="S2.SS3.SSS3.p3.1.m1.1.1.2" xref="S2.SS3.SSS3.p3.1.m1.1.1.2.cmml">M</mi><mrow id="S2.SS3.SSS3.p3.1.m1.1.1.3" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.cmml"><mrow id="S2.SS3.SSS3.p3.1.m1.1.1.3.2" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.2.cmml"><mn id="S2.SS3.SSS3.p3.1.m1.1.1.3.2.2" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.2.2.cmml">2</mn><mo id="S2.SS3.SSS3.p3.1.m1.1.1.3.2.1" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S2.SS3.SSS3.p3.1.m1.1.1.3.2.3" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.2.3.cmml">D</mi></mrow><mo id="S2.SS3.SSS3.p3.1.m1.1.1.3.1" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.1.cmml">−</mo><mrow id="S2.SS3.SSS3.p3.1.m1.1.1.3.3" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.3.cmml"><mn id="S2.SS3.SSS3.p3.1.m1.1.1.3.3.2" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.3.2.cmml">3</mn><mo id="S2.SS3.SSS3.p3.1.m1.1.1.3.3.1" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S2.SS3.SSS3.p3.1.m1.1.1.3.3.3" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.3.3.cmml">D</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS3.p3.1.m1.1b"><apply id="S2.SS3.SSS3.p3.1.m1.1.1.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS3.p3.1.m1.1.1.1.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS3.p3.1.m1.1.1.2.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1.2">𝑀</ci><apply id="S2.SS3.SSS3.p3.1.m1.1.1.3.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1.3"><minus id="S2.SS3.SSS3.p3.1.m1.1.1.3.1.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.1"></minus><apply id="S2.SS3.SSS3.p3.1.m1.1.1.3.2.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.2"><times id="S2.SS3.SSS3.p3.1.m1.1.1.3.2.1.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.2.1"></times><cn id="S2.SS3.SSS3.p3.1.m1.1.1.3.2.2.cmml" type="integer" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.2.2">2</cn><ci id="S2.SS3.SSS3.p3.1.m1.1.1.3.2.3.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.2.3">𝐷</ci></apply><apply id="S2.SS3.SSS3.p3.1.m1.1.1.3.3.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.3"><times id="S2.SS3.SSS3.p3.1.m1.1.1.3.3.1.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.3.1"></times><cn id="S2.SS3.SSS3.p3.1.m1.1.1.3.3.2.cmml" type="integer" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.3.2">3</cn><ci id="S2.SS3.SSS3.p3.1.m1.1.1.3.3.3.cmml" xref="S2.SS3.SSS3.p3.1.m1.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS3.p3.1.m1.1c">M_{2D-3D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS3.p3.1.m1.1d">italic_M start_POSTSUBSCRIPT 2 italic_D - 3 italic_D end_POSTSUBSCRIPT</annotation></semantics></math> encoding is processed by Patch-PnP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib60" title="">60</a>]</cite>, a differentiable pose estimator. During testing, the <math alttext="M_{2D-3D}" class="ltx_Math" display="inline" id="S2.SS3.SSS3.p3.2.m2.1"><semantics id="S2.SS3.SSS3.p3.2.m2.1a"><msub id="S2.SS3.SSS3.p3.2.m2.1.1" xref="S2.SS3.SSS3.p3.2.m2.1.1.cmml"><mi id="S2.SS3.SSS3.p3.2.m2.1.1.2" xref="S2.SS3.SSS3.p3.2.m2.1.1.2.cmml">M</mi><mrow id="S2.SS3.SSS3.p3.2.m2.1.1.3" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.cmml"><mrow id="S2.SS3.SSS3.p3.2.m2.1.1.3.2" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.2.cmml"><mn id="S2.SS3.SSS3.p3.2.m2.1.1.3.2.2" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.2.2.cmml">2</mn><mo id="S2.SS3.SSS3.p3.2.m2.1.1.3.2.1" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.2.1.cmml">⁢</mo><mi id="S2.SS3.SSS3.p3.2.m2.1.1.3.2.3" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.2.3.cmml">D</mi></mrow><mo id="S2.SS3.SSS3.p3.2.m2.1.1.3.1" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.1.cmml">−</mo><mrow id="S2.SS3.SSS3.p3.2.m2.1.1.3.3" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.3.cmml"><mn id="S2.SS3.SSS3.p3.2.m2.1.1.3.3.2" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.3.2.cmml">3</mn><mo id="S2.SS3.SSS3.p3.2.m2.1.1.3.3.1" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S2.SS3.SSS3.p3.2.m2.1.1.3.3.3" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.3.3.cmml">D</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS3.p3.2.m2.1b"><apply id="S2.SS3.SSS3.p3.2.m2.1.1.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS3.p3.2.m2.1.1.1.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.SSS3.p3.2.m2.1.1.2.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1.2">𝑀</ci><apply id="S2.SS3.SSS3.p3.2.m2.1.1.3.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1.3"><minus id="S2.SS3.SSS3.p3.2.m2.1.1.3.1.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.1"></minus><apply id="S2.SS3.SSS3.p3.2.m2.1.1.3.2.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.2"><times id="S2.SS3.SSS3.p3.2.m2.1.1.3.2.1.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.2.1"></times><cn id="S2.SS3.SSS3.p3.2.m2.1.1.3.2.2.cmml" type="integer" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.2.2">2</cn><ci id="S2.SS3.SSS3.p3.2.m2.1.1.3.2.3.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.2.3">𝐷</ci></apply><apply id="S2.SS3.SSS3.p3.2.m2.1.1.3.3.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.3"><times id="S2.SS3.SSS3.p3.2.m2.1.1.3.3.1.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.3.1"></times><cn id="S2.SS3.SSS3.p3.2.m2.1.1.3.3.2.cmml" type="integer" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.3.2">3</cn><ci id="S2.SS3.SSS3.p3.2.m2.1.1.3.3.3.cmml" xref="S2.SS3.SSS3.p3.2.m2.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS3.p3.2.m2.1c">M_{2D-3D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS3.p3.2.m2.1d">italic_M start_POSTSUBSCRIPT 2 italic_D - 3 italic_D end_POSTSUBSCRIPT</annotation></semantics></math> encoding is converted to 2D-3D pairs, and the pose is estimated using a PnP-RANSAC variant. The final output consists of the pose parameters (<math alttext="R_{3x3},t_{3}" class="ltx_Math" display="inline" id="S2.SS3.SSS3.p3.3.m3.2"><semantics id="S2.SS3.SSS3.p3.3.m3.2a"><mrow id="S2.SS3.SSS3.p3.3.m3.2.2.2" xref="S2.SS3.SSS3.p3.3.m3.2.2.3.cmml"><msub id="S2.SS3.SSS3.p3.3.m3.1.1.1.1" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.cmml"><mi id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.2" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.2.cmml">R</mi><mrow id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.cmml"><mn id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.2" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.2.cmml">3</mn><mo id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.1" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.3" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.3.cmml">x</mi><mo id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.1a" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.1.cmml">⁢</mo><mn id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.4" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.4.cmml">3</mn></mrow></msub><mo id="S2.SS3.SSS3.p3.3.m3.2.2.2.3" xref="S2.SS3.SSS3.p3.3.m3.2.2.3.cmml">,</mo><msub id="S2.SS3.SSS3.p3.3.m3.2.2.2.2" xref="S2.SS3.SSS3.p3.3.m3.2.2.2.2.cmml"><mi id="S2.SS3.SSS3.p3.3.m3.2.2.2.2.2" xref="S2.SS3.SSS3.p3.3.m3.2.2.2.2.2.cmml">t</mi><mn id="S2.SS3.SSS3.p3.3.m3.2.2.2.2.3" xref="S2.SS3.SSS3.p3.3.m3.2.2.2.2.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS3.p3.3.m3.2b"><list id="S2.SS3.SSS3.p3.3.m3.2.2.3.cmml" xref="S2.SS3.SSS3.p3.3.m3.2.2.2"><apply id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.cmml" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.1.cmml" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.2.cmml" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.2">𝑅</ci><apply id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.cmml" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3"><times id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.1.cmml" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.1"></times><cn id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.2.cmml" type="integer" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.2">3</cn><ci id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.3.cmml" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.3">𝑥</ci><cn id="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.4.cmml" type="integer" xref="S2.SS3.SSS3.p3.3.m3.1.1.1.1.3.4">3</cn></apply></apply><apply id="S2.SS3.SSS3.p3.3.m3.2.2.2.2.cmml" xref="S2.SS3.SSS3.p3.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS3.p3.3.m3.2.2.2.2.1.cmml" xref="S2.SS3.SSS3.p3.3.m3.2.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS3.p3.3.m3.2.2.2.2.2.cmml" xref="S2.SS3.SSS3.p3.3.m3.2.2.2.2.2">𝑡</ci><cn id="S2.SS3.SSS3.p3.3.m3.2.2.2.2.3.cmml" type="integer" xref="S2.SS3.SSS3.p3.3.m3.2.2.2.2.3">3</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS3.p3.3.m3.2c">R_{3x3},t_{3}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS3.p3.3.m3.2d">italic_R start_POSTSUBSCRIPT 3 italic_x 3 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math>), object class, and articulation angle.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.4 </span>Training Procedure</h4>
<div class="ltx_para" id="S2.SS3.SSS4.p1">
<p class="ltx_p" id="S2.SS3.SSS4.p1.1">The training process involves the following steps:</p>
<ol class="ltx_enumerate" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1">Train the pose estimation network and the object detection network independently on synthetic data.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1">Create pseudo-labels for the object detector using the pose estimation network and retrain the object detector on the pseudo-labels.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I3.i3.p1">
<p class="ltx_p" id="S2.I3.i3.p1.1">Create pseudo-labels for the pose estimator using the updated object detector and retrain the pose estimator on the pseudo-labels.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I3.i4.p1">
<p class="ltx_p" id="S2.I3.i4.p1.1">Repeat steps 2-3 for multiple iterations to progressively refine the networks’ performance on real-world data.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S2.SS3.SSS4.p1.2">The details of each step are discussed in the following subsections.</p>
</div>
<section class="ltx_paragraph" id="S2.SS3.SSS4.Px1">
<h5 class="ltx_title ltx_title_paragraph">Stage 1: Training on Synthetic Data</h5>
<div class="ltx_para" id="S2.SS3.SSS4.Px1.p1">
<p class="ltx_p" id="S2.SS3.SSS4.Px1.p1.1">In the first stage, we train the pose estimation network and the object detection network independently on the synthetic dataset generated using the pipeline described in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS2" title="2.2 Synthetic Data Generation ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS4.Px2">
<h5 class="ltx_title ltx_title_paragraph">Step 2: Object Detection Domain Adaptation</h5>
<div class="ltx_para" id="S2.SS3.SSS4.Px2.p1">
<ol class="ltx_enumerate" id="S2.I4">
<li class="ltx_item" id="S2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I4.i1.p1">
<p class="ltx_p" id="S2.I4.i1.p1.1">We apply object tracking on the real video data using the BoT-SORT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib2" title="">2</a>]</cite> tracking algorithm.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I4.i2.p1">
<p class="ltx_p" id="S2.I4.i2.p1.1">We select frames with high detection confidence and at least three consecutive tracking detections as pseudo-labels for object detection.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I4.i3.p1">
<p class="ltx_p" id="S2.I4.i3.p1.1">The selected frames are passed to the pose estimator, which estimates the object pose, class, and articulation angle. The resulting mesh and pose are reprojected onto the image plane. The resulting reprojected bounding box is then used as the new pseudo-label for the object detection network. This process is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.F7" title="Fig. 7 ‣ Step 2: Object Detection Domain Adaptation ‣ 2.3.4 Training Procedure ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I4.i4.p1">
<p class="ltx_p" id="S2.I4.i4.p1.1">Retrain the object detector on the pseudo-labeled data and a fraction of synthetic data.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="96" id="S2.F7.g1" src="extracted/5735855/graphics/bbox_ref.png" width="334"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 7: </span>Bounding box refinement stage: This figure illustrates the process of refining bounding boxes for object detection. Initially, frames with high detection confidence and consecutive tracking detections are selected. These frames are then passed to the pose estimator, which estimates the object pose, class, and articulation angle. The resulting mesh and pose are reprojected onto the image plane, creating a reprojected bounding box that is used as the new pseudo-label for retraining the object detection network.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS4.Px3">
<h5 class="ltx_title ltx_title_paragraph">Step 3: Pose Estimation Domain Adaptation</h5>
<div class="ltx_para" id="S2.SS3.SSS4.Px3.p1">
<ol class="ltx_enumerate" id="S2.I5">
<li class="ltx_item" id="S2.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I5.i1.p1">
<p class="ltx_p" id="S2.I5.i1.p1.1">Using the refined object detection model, we obtain image crops of the detected objects from the real video data.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I5.i2.p1">
<p class="ltx_p" id="S2.I5.i2.p1.1">We run the pose estimation network on these image crops to predict the 6D pose, object class, and articulation angle.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I5.i3.p1">
<p class="ltx_p" id="S2.I5.i3.p1.1">We filter the pose predictions based on the following criteria:</p>
<ul class="ltx_itemize" id="S2.I5.i3.I1">
<li class="ltx_item" id="S2.I5.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3..1</span>
<div class="ltx_para" id="S2.I5.i3.I1.i1.p1">
<p class="ltx_p" id="S2.I5.i3.I1.i1.p1.1">High class confidence</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3..2</span>
<div class="ltx_para" id="S2.I5.i3.I1.i2.p1">
<p class="ltx_p" id="S2.I5.i3.I1.i2.p1.1">Low PnP-RANSAC outlier count</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3..3</span>
<div class="ltx_para" id="S2.I5.i3.I1.i3.p1">
<p class="ltx_p" id="S2.I5.i3.I1.i3.p1.1">Low reprojection error</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S2.I5.i3.p1.2">The pose predictions that satisfy these criteria are selected as pseudo-labels for pose estimation.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I5.i4.p1">
<p class="ltx_p" id="S2.I5.i4.p1.1">Retrain the pose estimator on the pseudo-labeled data and a fraction of synthetic data.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments, Results, and Ablation Studies</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Setup</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our experiments are implemented with the PyTorch deep learning framework. We train our models end-to-end using the Ranger optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib63" title="">63</a>]</cite>, which combines Rectified Adam (RAdam) with Lookahead. A batch size of 120 is used during training, along with a base learning rate of 1e-5. We employ a cosine annealing learning rate schedule <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib38" title="">38</a>]</cite>, which gradually reduces the learning rate after approximately 72% of the training iterations.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">During training, we apply various data augmentation techniques with a base probability of 0.8. These augmentations are taken from the GDRNPP implementation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib37" title="">37</a>]</cite> and include: Gaussian blur, Sharpness enhancement, Contrast enhancement ,Brightness adjustment ,Color enhancement, Addition of random values to pixel intensities, Channel-wise inversion, Multiplication of pixel intensities ,Addition of Gaussian noise, Linear contrast adjustment, and Grayscale conversion.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">For inference, we use the Progressive-X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib6" title="">6</a>]</cite> PnP-RANSAC scheme, which utilizes Graph-Cut RANSAC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Dataset and Labels</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#Sx1.SS6" title="1.6 Data Annotation for Evaluation ‣ Related Work ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">1.6</span></a>, our real-world dataset consists of annotated frames from seven different surgical procedures. For each surgery, approximately 100 images were selected and manually annotated with detailed segmentation masks of the surgical tools and hands. These annotations are used for testing only; they serve as the ground truth for evaluating our pose estimation method on real-world data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Evaluation Metrics</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Due to the lack of ground truth 6D pose annotations in our real-world dataset, we employ a 2D reprojection-based evaluation method. This approach compares the reprojected mask of the object, obtained using the estimated pose, class, and articulation angle, with the manually annotated segmentation masks. However, directly comparing the full reprojection masks to the annotated tool masks may not provide an accurate assessment of the pose estimation accuracy, as the annotated tool masks may be partially occluded by the surgeon’s hands. To account for these occlusions, we incorporate the hand masks into our evaluation process.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">6D Pose Evaluation Metric</h5>
<div class="ltx_para" id="S3.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px1.p1.1">We propose a hand-occlusion aware reprojection that focuses on evaluating the visible parts of the tools while considering the presence of hand occlusions. The steps involved in computing the metrics are as follows:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Given an input image and the estimated pose of the surgical tool, we project the 3D model of the tool onto the 2D image plane using the camera’s intrinsic parameters. This results in a reprojected binary mask representing the visible surface of the tool according to the estimated pose.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">We subtract the hand masks from both the annotated tool masks and the reprojected tool masks. This step removes the regions occluded by the hands, allowing us to focus on the visible parts of the tools.</p>
</div>
<div class="ltx_para" id="S3.I1.i2.p2">
<p class="ltx_p" id="S3.I1.i2.p2.5">We use the Average Precision (<math alttext="AP" class="ltx_Math" display="inline" id="S3.I1.i2.p2.1.m1.1"><semantics id="S3.I1.i2.p2.1.m1.1a"><mrow id="S3.I1.i2.p2.1.m1.1.1" xref="S3.I1.i2.p2.1.m1.1.1.cmml"><mi id="S3.I1.i2.p2.1.m1.1.1.2" xref="S3.I1.i2.p2.1.m1.1.1.2.cmml">A</mi><mo id="S3.I1.i2.p2.1.m1.1.1.1" xref="S3.I1.i2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.I1.i2.p2.1.m1.1.1.3" xref="S3.I1.i2.p2.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p2.1.m1.1b"><apply id="S3.I1.i2.p2.1.m1.1.1.cmml" xref="S3.I1.i2.p2.1.m1.1.1"><times id="S3.I1.i2.p2.1.m1.1.1.1.cmml" xref="S3.I1.i2.p2.1.m1.1.1.1"></times><ci id="S3.I1.i2.p2.1.m1.1.1.2.cmml" xref="S3.I1.i2.p2.1.m1.1.1.2">𝐴</ci><ci id="S3.I1.i2.p2.1.m1.1.1.3.cmml" xref="S3.I1.i2.p2.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p2.1.m1.1c">AP</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p2.1.m1.1d">italic_A italic_P</annotation></semantics></math>) metric for segmentation following the method used in the COCO 2020 Object Segmentation Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib34" title="">34</a>]</cite> and BOP 2022 Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib52" title="">52</a>]</cite>. For each object, we calculate a per-object Average Precision (<math alttext="AP_{O}" class="ltx_Math" display="inline" id="S3.I1.i2.p2.2.m2.1"><semantics id="S3.I1.i2.p2.2.m2.1a"><mrow id="S3.I1.i2.p2.2.m2.1.1" xref="S3.I1.i2.p2.2.m2.1.1.cmml"><mi id="S3.I1.i2.p2.2.m2.1.1.2" xref="S3.I1.i2.p2.2.m2.1.1.2.cmml">A</mi><mo id="S3.I1.i2.p2.2.m2.1.1.1" xref="S3.I1.i2.p2.2.m2.1.1.1.cmml">⁢</mo><msub id="S3.I1.i2.p2.2.m2.1.1.3" xref="S3.I1.i2.p2.2.m2.1.1.3.cmml"><mi id="S3.I1.i2.p2.2.m2.1.1.3.2" xref="S3.I1.i2.p2.2.m2.1.1.3.2.cmml">P</mi><mi id="S3.I1.i2.p2.2.m2.1.1.3.3" xref="S3.I1.i2.p2.2.m2.1.1.3.3.cmml">O</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p2.2.m2.1b"><apply id="S3.I1.i2.p2.2.m2.1.1.cmml" xref="S3.I1.i2.p2.2.m2.1.1"><times id="S3.I1.i2.p2.2.m2.1.1.1.cmml" xref="S3.I1.i2.p2.2.m2.1.1.1"></times><ci id="S3.I1.i2.p2.2.m2.1.1.2.cmml" xref="S3.I1.i2.p2.2.m2.1.1.2">𝐴</ci><apply id="S3.I1.i2.p2.2.m2.1.1.3.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i2.p2.2.m2.1.1.3.1.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3">subscript</csymbol><ci id="S3.I1.i2.p2.2.m2.1.1.3.2.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3.2">𝑃</ci><ci id="S3.I1.i2.p2.2.m2.1.1.3.3.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3.3">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p2.2.m2.1c">AP_{O}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p2.2.m2.1d">italic_A italic_P start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT</annotation></semantics></math>) by averaging the <math alttext="AP" class="ltx_Math" display="inline" id="S3.I1.i2.p2.3.m3.1"><semantics id="S3.I1.i2.p2.3.m3.1a"><mrow id="S3.I1.i2.p2.3.m3.1.1" xref="S3.I1.i2.p2.3.m3.1.1.cmml"><mi id="S3.I1.i2.p2.3.m3.1.1.2" xref="S3.I1.i2.p2.3.m3.1.1.2.cmml">A</mi><mo id="S3.I1.i2.p2.3.m3.1.1.1" xref="S3.I1.i2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.I1.i2.p2.3.m3.1.1.3" xref="S3.I1.i2.p2.3.m3.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p2.3.m3.1b"><apply id="S3.I1.i2.p2.3.m3.1.1.cmml" xref="S3.I1.i2.p2.3.m3.1.1"><times id="S3.I1.i2.p2.3.m3.1.1.1.cmml" xref="S3.I1.i2.p2.3.m3.1.1.1"></times><ci id="S3.I1.i2.p2.3.m3.1.1.2.cmml" xref="S3.I1.i2.p2.3.m3.1.1.2">𝐴</ci><ci id="S3.I1.i2.p2.3.m3.1.1.3.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p2.3.m3.1c">AP</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p2.3.m3.1d">italic_A italic_P</annotation></semantics></math> across the following Intersection over Union (IoU) thresholds: [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]. The IoU is evaluated between the manually subtracted annotated tool mask and the manually subtracted reprojected tool mask. The overall performance metric (<math alttext="AP" class="ltx_Math" display="inline" id="S3.I1.i2.p2.4.m4.1"><semantics id="S3.I1.i2.p2.4.m4.1a"><mrow id="S3.I1.i2.p2.4.m4.1.1" xref="S3.I1.i2.p2.4.m4.1.1.cmml"><mi id="S3.I1.i2.p2.4.m4.1.1.2" xref="S3.I1.i2.p2.4.m4.1.1.2.cmml">A</mi><mo id="S3.I1.i2.p2.4.m4.1.1.1" xref="S3.I1.i2.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.I1.i2.p2.4.m4.1.1.3" xref="S3.I1.i2.p2.4.m4.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p2.4.m4.1b"><apply id="S3.I1.i2.p2.4.m4.1.1.cmml" xref="S3.I1.i2.p2.4.m4.1.1"><times id="S3.I1.i2.p2.4.m4.1.1.1.cmml" xref="S3.I1.i2.p2.4.m4.1.1.1"></times><ci id="S3.I1.i2.p2.4.m4.1.1.2.cmml" xref="S3.I1.i2.p2.4.m4.1.1.2">𝐴</ci><ci id="S3.I1.i2.p2.4.m4.1.1.3.cmml" xref="S3.I1.i2.p2.4.m4.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p2.4.m4.1c">AP</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p2.4.m4.1d">italic_A italic_P</annotation></semantics></math>) is then computed by averaging the <math alttext="AP_{O}" class="ltx_Math" display="inline" id="S3.I1.i2.p2.5.m5.1"><semantics id="S3.I1.i2.p2.5.m5.1a"><mrow id="S3.I1.i2.p2.5.m5.1.1" xref="S3.I1.i2.p2.5.m5.1.1.cmml"><mi id="S3.I1.i2.p2.5.m5.1.1.2" xref="S3.I1.i2.p2.5.m5.1.1.2.cmml">A</mi><mo id="S3.I1.i2.p2.5.m5.1.1.1" xref="S3.I1.i2.p2.5.m5.1.1.1.cmml">⁢</mo><msub id="S3.I1.i2.p2.5.m5.1.1.3" xref="S3.I1.i2.p2.5.m5.1.1.3.cmml"><mi id="S3.I1.i2.p2.5.m5.1.1.3.2" xref="S3.I1.i2.p2.5.m5.1.1.3.2.cmml">P</mi><mi id="S3.I1.i2.p2.5.m5.1.1.3.3" xref="S3.I1.i2.p2.5.m5.1.1.3.3.cmml">O</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p2.5.m5.1b"><apply id="S3.I1.i2.p2.5.m5.1.1.cmml" xref="S3.I1.i2.p2.5.m5.1.1"><times id="S3.I1.i2.p2.5.m5.1.1.1.cmml" xref="S3.I1.i2.p2.5.m5.1.1.1"></times><ci id="S3.I1.i2.p2.5.m5.1.1.2.cmml" xref="S3.I1.i2.p2.5.m5.1.1.2">𝐴</ci><apply id="S3.I1.i2.p2.5.m5.1.1.3.cmml" xref="S3.I1.i2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i2.p2.5.m5.1.1.3.1.cmml" xref="S3.I1.i2.p2.5.m5.1.1.3">subscript</csymbol><ci id="S3.I1.i2.p2.5.m5.1.1.3.2.cmml" xref="S3.I1.i2.p2.5.m5.1.1.3.2">𝑃</ci><ci id="S3.I1.i2.p2.5.m5.1.1.3.3.cmml" xref="S3.I1.i2.p2.5.m5.1.1.3.3">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p2.5.m5.1c">AP_{O}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p2.5.m5.1d">italic_A italic_P start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT</annotation></semantics></math> scores for all objects.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">2D Object Detection Metric</h5>
<div class="ltx_para" id="S3.SS1.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px2.p1.3">Similarly to our segmentation approach, the <math alttext="AP" class="ltx_Math" display="inline" id="S3.SS1.SSS2.Px2.p1.1.m1.1"><semantics id="S3.SS1.SSS2.Px2.p1.1.m1.1a"><mrow id="S3.SS1.SSS2.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS2.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.Px2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.1.m1.1.1.2.cmml">A</mi><mo id="S3.SS1.SSS2.Px2.p1.1.m1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS2.Px2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.1.m1.1.1"><times id="S3.SS1.SSS2.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.1.m1.1.1.1"></times><ci id="S3.SS1.SSS2.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.1.m1.1.1.2">𝐴</ci><ci id="S3.SS1.SSS2.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.1.m1.1c">AP</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.Px2.p1.1.m1.1d">italic_A italic_P</annotation></semantics></math> is determined by calculating the precision at various IoU thresholds, yet here it specifically evaluates the accuracy with which objects are detected rather than segmented. The overall <math alttext="AP" class="ltx_Math" display="inline" id="S3.SS1.SSS2.Px2.p1.2.m2.1"><semantics id="S3.SS1.SSS2.Px2.p1.2.m2.1a"><mrow id="S3.SS1.SSS2.Px2.p1.2.m2.1.1" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.Px2.p1.2.m2.1.1.2" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.1.2.cmml">A</mi><mo id="S3.SS1.SSS2.Px2.p1.2.m2.1.1.1" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS2.Px2.p1.2.m2.1.1.3" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.Px2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.1"><times id="S3.SS1.SSS2.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.1.1"></times><ci id="S3.SS1.SSS2.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.1.2">𝐴</ci><ci id="S3.SS1.SSS2.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.2.m2.1c">AP</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.Px2.p1.2.m2.1d">italic_A italic_P</annotation></semantics></math> score reflects the aggregate precision across all detected objects in the dataset.
We evaluate only those object instances where at least <math alttext="10\%" class="ltx_Math" display="inline" id="S3.SS1.SSS2.Px2.p1.3.m3.1"><semantics id="S3.SS1.SSS2.Px2.p1.3.m3.1a"><mrow id="S3.SS1.SSS2.Px2.p1.3.m3.1.1" xref="S3.SS1.SSS2.Px2.p1.3.m3.1.1.cmml"><mn id="S3.SS1.SSS2.Px2.p1.3.m3.1.1.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.1.1.2.cmml">10</mn><mo id="S3.SS1.SSS2.Px2.p1.3.m3.1.1.1" xref="S3.SS1.SSS2.Px2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p1.3.m3.1b"><apply id="S3.SS1.SSS2.Px2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS1.SSS2.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.1.1.1">percent</csymbol><cn id="S3.SS1.SSS2.Px2.p1.3.m3.1.1.2.cmml" type="integer" xref="S3.SS1.SSS2.Px2.p1.3.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.3.m3.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.Px2.p1.3.m3.1d">10 %</annotation></semantics></math> of their projected surface area is visible, excluding detections of objects visible from less than 10% which are not counted towards false positives. Due to the lack of ground-truth poses and the minimal variation in camera-to-object distance, we estimate the visibility percentage of each object by counting the number of visible pixels in the ground truth segmentation.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Results on Real-World Surgical Data</h4>
<section class="ltx_paragraph" id="S3.SS1.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">Pose Estimation Results</h5>
<div class="ltx_para" id="S3.SS1.SSS3.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS3.Px1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.T1" title="Table 1 ‣ Pose Estimation Results ‣ 3.1.3 Results on Real-World Surgical Data ‣ 3.1 Experimental Setup ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">1</span></a> presents the performance of our pose estimation method on the real-world surgical dataset. We evaluate the Average Precision (AP) for two surgical tools: needle-holder and tweezers. The AP is computed based on the hand-occlusion-aware reprojection metric described in the previous section.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.Px1.p2">
<p class="ltx_p" id="S3.SS1.SSS3.Px1.p2.1">We compare three variations of our method: (1) Pose estimation trained only on synthetic data, (2) Pose estimation with real data refinement. These variations are evaluated using both ground truth bounding boxes (GT Bbox) and predicted bounding boxes from the object detection stage.
It is important to note that predicting pose on the ground truth bounding boxes isolates the pose estimation performance and does not introduce the errors caused by the bounding box prediction.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Pose Estimation Average Precision (AP) on the Real-World Surgical Data. ”Synth” refers to training on synthetic data, ”Real” refers to refinement on real data, ”GT RoI” refers to ground truth RoI, and ”Pred RoI” refers to predicted RoI.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1">RoI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2">Trained on</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.3">Needle-holder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.4">Tweezers</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.5">Mean</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.1">GT RoI</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.2">Synth</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.3">0.654</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.4">0.680</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.5">0.667</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.1">GT RoI</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.2">Real</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.3">0.784</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.4">0.805</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.5">0.795</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.4.3.1">Pred RoI</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.4.3.2">Real</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.4.3.3">0.754</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.4.3.4">0.752</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.4.3.5">0.753</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">Object Detection Results</h5>
<div class="ltx_para" id="S3.SS1.SSS3.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS3.Px2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.T2" title="Table 2 ‣ Object Detection Results ‣ 3.1.3 Results on Real-World Surgical Data ‣ 3.1 Experimental Setup ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">2</span></a> presents the object detection performance on the real-world surgical dataset. We evaluate the Average Precision (AP) for detecting the needle-holder and tweezers using our object detection method.
We present results when training the model only on synthetic data, after refinement on real data and pose refinement with the tuned pose model as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.F7" title="Fig. 7 ‣ Step 2: Object Detection Domain Adaptation ‣ 2.3.4 Training Procedure ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>YOLOv8 Object Detection Average Precision on the Real-World Surgical Data</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.1.1.1.1">Training Strategy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.2">Needle-holder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.3">Tweezers</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.4">Mean</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.2.1.1">trained on synthetic data</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.2">0.564</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.3">0.455</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T2.1.2.1.4">0.510</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.3.2.1">tuned on real data</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.2">0.731</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.3">0.807</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.3.2.4">0.769</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.1.4.3.1">pose refinement</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.4.3.2.1">0.841</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.4.3.3.1">0.877</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T2.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.4.3.4.1">0.859</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Ablation Studies</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Impact of Synthetic Hand-Object Interactions</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">To investigate the impact of synthetic hand-object interactions on the performance of our pose estimation and object detection models, we conduct a series of ablation studies. We train three variants of our models on different synthetic datasets. These datasets are a sub-sample of the whole synthetic data that we generated:</p>
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">Synthetic data with synthetic hands: This dataset includes realistic hand-object interactions, as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#Sx1.SS6" title="1.6 Data Annotation for Evaluation ‣ Related Work ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">1.6</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Synthetic data with masked tools: In this dataset, regions of the surgical tools that would have been occluded by hands are masked out to simulate occlusions without explicitly modeling the hands.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">Synthetic data without hands: Here, synthetic hands are excluded from the rendering process, leaving only the surgical tools visible in the scenes.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.F8" title="Fig. 8 ‣ 3.2.1 Impact of Synthetic Hand-Object Interactions ‣ 3.2 Ablation Studies ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">8</span></a> shows an example of the hand dataset.
We evaluate the performance of the pose estimation model and the object detection model trained on each of these datasets.</p>
</div>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="436" id="S3.F8.g1" src="extracted/5735855/graphics/hand_ablation_light.png" width="2124"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 8: </span>Example from hand ablation data: image with synthetic hands (left), image with masked tools (middle), image without hands (right).</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">We evaluate the pose estimation accuracy of the models trained on the three synthetic datasets when provided with ground truth to isolate the impact of the object detection stage.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>AP Metrics for Pose Model Evaluation with Ground-Truth Bounding Boxes</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.1.1.1.1">Synthetic Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.2">Needle-holder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.3">Tweezers</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.4">Mean</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.2.1.1">Without Hands</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.1.2">0.483</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.1.3">0.621</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.1.2.1.4">0.552</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.3.2.1">Masked Tools</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.2.2">0.574</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.2.3">0.727</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.3.2.4">0.651</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T3.1.4.3.1">With Hands</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.4.3.2.1">0.577</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.4.3.3.1">0.758</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T3.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.4.3.4.1">0.668</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Patch-PnP vs. PnP-RANSAC</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">This ablation study evaluates the performance of two different strategies for pose estimation: Patch-PnP and PnP-RANSAC.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">For PnP-RANSAC, we utilize the Progressive-x algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib6" title="">6</a>]</cite> in conjunction with Graph-cut RANSAC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#bib.bib5" title="">5</a>]</cite>. Hereafter, we refer to this combination as PnP-RANSAC. Progressive-x is a multi-model fitting algorithm that progressively fits model instances to data points, taking into account both the fit quality and the spatial coherence of the points. Graph-cut RANSAC is a variant of RANSAC that enhances the efficiency and robustness of model fitting through graph-based segmentation. It employs a graph-based representation of the data points and uses graph-cut optimization to identify the optimal set of inliers for each model hypothesis.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Training strategy impact</h5>
<div class="ltx_para" id="S3.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS2.Px1.p1.1">We evaluate the impact of training an end-to-end pose estimation model with Patch-PnP and all the associated pose losses as described in section <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S2.SS3.SSS2" title="2.3.2 Multitask Pose Losses ‣ 2.3 Pose Estimation Framework ‣ 2 Methodology ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">2.3.2</span></a>, vs training without the Patch-PnP module.
The inference strategy in both cases is with PnP-RANSAC. Results are shown in table <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.T4" title="Table 4 ‣ Training strategy impact ‣ 3.2.2 Patch-PnP vs. PnP-RANSAC ‣ 3.2 Ablation Studies ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of Patch-PnP and Prog-x with Graph-cut RANSAC for pose estimation inference.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.1.1.1.1">Strategy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.1.1.1.2">Needle-holder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.1.1.1.3">Tweezers</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.1.1.1.4">Mean</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.2.1.1">Patch-PnP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.2.1.2">0.567</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.1.3.1">0.803</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.2.1.4">0.685</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.3.2.1">Prog-x</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S3.T4.1.3.2.2.1">0.671</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.3.2.3">0.779</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S3.T4.1.3.2.4.1">0.725</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Inference strategy impact</h5>
<div class="ltx_para" id="S3.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.Px2.p1.1">In this experiment we evaluate PnP-RANSAC vs Patch-PnP inference on an end-to-end trained pose model. Results are shown in table <a class="ltx_ref" href="https://arxiv.org/html/2407.12138v1#S3.T5" title="Table 5 ‣ Inference strategy impact ‣ 3.2.2 Patch-PnP vs. PnP-RANSAC ‣ 3.2 Ablation Studies ‣ 3 Experiments, Results, and Ablation Studies ‣ Monocular pose estimation of articulated surgical instruments in open surgery"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of Patch-PnP and Prog-x with Graph-cut RANSAC for pose estimation inference.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.1">Strategy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.2">Needle-holder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.3">Tweezers</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.4">Mean</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T5.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.2.1.1">Patch-PnP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.2.1.2">0.622</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S3.T5.1.2.1.3.1">0.839</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.2.1.4">0.731</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.3.2.1">Prog-x</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S3.T5.1.3.2.2.1">0.784</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.3.2.3">0.805</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S3.T5.1.3.2.4.1">0.795</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS2.SSS2.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.Px2.p2.1">The results show that using PnP-RANSAC overall achieves better results than Patch-PnP, though on the Tweezers, Patch-PnP outperformed PnP-RANSAC.</p>
</div>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Disscusion and Conclusions</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This study introduced a novel approach for monocular pose estimation of articulated surgical instruments in open surgery, marking an ”in the wild” investigation in a relatively under-explored domain. Our approach involved creating a diverse synthetic dataset of surgical instruments with various articulation angles, enabling initial model training. These models were then refined through synthetic to real domain adaptation techniques, resulting in robust predictions of 6D pose, instrument class, and articulation angle in real surgical scenarios. The results demonstrate our method’s effectiveness in addressing unique surgical environment challenges, such as limited real-world data, complex instrument geometry, and occlusions. Notably, our approach shows promise not only in pose estimation but also in improving object detection accuracy through an iterative refinement process that leverages our pose estimation method, suggesting a potential pathway for enhancing overall surgical instrument recognition systems.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">While our approach shows promise, we acknowledge certain limitations that present opportunities for future research. Our current evaluation method focuses on the 2D reprojection of estimated poses, which does not fully capture the depth information of the instruments. Future work could incorporate depth information in the evaluation process to enhance 3D position accuracy. Additionally, our study was limited to two surgical tools and one site, and expanding the synthetic dataset to include a wider variety of instruments would further demonstrate the generalizability of our approach.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Looking ahead, this work opens up several directions for future research in surgical instrument pose estimation. Future studies could explore a wider range of surgical instruments and environments to further test and refine these techniques. The potential for improving both pose estimation and object detection accuracy through our iterative approach also merits further investigation.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">In conclusion, our study demonstrates a novel approach to pose estimation of articulated surgical instruments in open surgery using synthetic data and domain adaptation techniques. This work contributes to the ongoing efforts in computer-assisted surgery, offering a potential pathway to overcome the challenge of limited annotated real-world data. As research in this field progresses, we hope that this and similar approaches will continue to be refined, ultimately supporting the development of more effective tools for surgical assistance and training.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ababsa and Mallem [2004]</span>
<span class="ltx_bibblock">
Ababsa, F.e., Mallem, M., 2004.

</span>
<span class="ltx_bibblock">Robust camera pose estimation using 2d fiducials tracking for real-time augmented reality systems, in: Proceedings of the 2004 ACM SIGGRAPH international conference on Virtual Reality continuum and its applications in industry, pp. 431–435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharon et al. [2022]</span>
<span class="ltx_bibblock">
Aharon, N., Orfaig, R., Bobrovsky, B.Z., 2022.

</span>
<span class="ltx_bibblock">Bot-sort: Robust associations multi-pedestrian tracking.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2206.14651 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allan et al. [2019]</span>
<span class="ltx_bibblock">
Allan, M., Shvets, A., Kurmann, T., Zhang, Z., Duggal, R., Su, Y.H., Rieke, N., Laina, I., Kalavakonda, N., Bodenstedt, S., et al., 2019.

</span>
<span class="ltx_bibblock">2017 robotic instrument segmentation challenge.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1902.06426 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. [2021]</span>
<span class="ltx_bibblock">
Bai, T., Luo, J., Zhao, J., Wen, B., Wang, Q., 2021.

</span>
<span class="ltx_bibblock">Recent advances in adversarial training for adversarial robustness.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2102.01356 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barath and Matas [2018]</span>
<span class="ltx_bibblock">
Barath, D., Matas, J., 2018.

</span>
<span class="ltx_bibblock">Graph-cut ransac, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6733–6741.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barath and Matas [2019]</span>
<span class="ltx_bibblock">
Barath, D., Matas, J., 2019.

</span>
<span class="ltx_bibblock">Progressive-x: Efficient, anytime, multi-model fitting algorithm, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 3780–3788.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bkheet et al. [2023]</span>
<span class="ltx_bibblock">
Bkheet, E., D’Angelo, A.L., Goldbraikh, A., Laufer, S., 2023.

</span>
<span class="ltx_bibblock">Using hand pose estimation to automate open surgery training feedback.

</span>
<span class="ltx_bibblock">International Journal of Computer Assisted Radiology and Surgery 18, 1279–1285.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blender Foundation [2023]</span>
<span class="ltx_bibblock">
Blender Foundation, 2023.

</span>
<span class="ltx_bibblock">Blender.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.blender.org/" title="">https://www.blender.org/</a>. version 3.5.0.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bouget et al. [2017]</span>
<span class="ltx_bibblock">
Bouget, D., Allan, M., Stoyanov, D., Jannin, P., 2017.

</span>
<span class="ltx_bibblock">Vision-based and marker-less surgical tool detection and tracking: a review of the literature.

</span>
<span class="ltx_bibblock">Medical image analysis 35, 633–654.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bouget et al. [2015]</span>
<span class="ltx_bibblock">
Bouget, D., Benenson, R., Omran, M., Riffaud, L., Schiele, B., Jannin, P., 2015.

</span>
<span class="ltx_bibblock">Detecting surgical tools by modelling local appearance and global shape.

</span>
<span class="ltx_bibblock">IEEE transactions on medical imaging 34, 2603–2617.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brahmbhatt et al. [2019]</span>
<span class="ltx_bibblock">
Brahmbhatt, S., Ham, C., Kemp, C.C., Hays, J., 2019.

</span>
<span class="ltx_bibblock">ContactDB: Analyzing and predicting grasp contact via thermal imaging, in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://contactdb.cc.gatech.edu" title="">https://contactdb.cc.gatech.edu</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braun et al. [2016]</span>
<span class="ltx_bibblock">
Braun, M., Rao, Q., Wang, Y., Flohr, F., 2016.

</span>
<span class="ltx_bibblock">Pose-rcnn: Joint object detection and pose estimation using 3d object proposals, in: 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC), IEEE. pp. 1546–1551.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burton et al. [2023]</span>
<span class="ltx_bibblock">
Burton, W., Myers, C., Rutherford, M., Rullkoetter, P., 2023.

</span>
<span class="ltx_bibblock">Evaluation of single-stage vision models for pose estimation of surgical instruments.

</span>
<span class="ltx_bibblock">International Journal of Computer Assisted Radiology and Surgery 18, 2125–2142.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Capturing Reality [2023]</span>
<span class="ltx_bibblock">
Capturing Reality, 2023.

</span>
<span class="ltx_bibblock">Reality capture.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.capturingreality.com/" title="">https://www.capturingreality.com/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Colleoni et al. [2020]</span>
<span class="ltx_bibblock">
Colleoni, E., Edwards, P., Stoyanov, D., 2020.

</span>
<span class="ltx_bibblock">Synthetic and real inputs for tool segmentation in robotic surgery, in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 700–710.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denninger et al. [2023]</span>
<span class="ltx_bibblock">
Denninger, M., Winkelbauer, D., Sundermeyer, M., Boerdijk, W., Knauer, M., Strobl, K.H., Humt, M., Triebel, R., 2023.

</span>
<span class="ltx_bibblock">Blenderproc2: A procedural pipeline for photorealistic rendering.

</span>
<span class="ltx_bibblock">Journal of Open Source Software 8, 4901.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21105/joss.04901" title="">https://doi.org/10.21105/joss.04901</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.21105/joss.04901" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.21105/joss.04901</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doignon et al. [2008]</span>
<span class="ltx_bibblock">
Doignon, C., Nageotte, F., Maurin, B., Krupa, A., 2008.

</span>
<span class="ltx_bibblock">Pose estimation and feature tracking for robot assisted surgery with medical imaging.

</span>
<span class="ltx_bibblock">Unifying perspectives in computational and robot vision , 79–101.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elfring et al. [2010]</span>
<span class="ltx_bibblock">
Elfring, R., de la Fuente, M., Radermacher, K., 2010.

</span>
<span class="ltx_bibblock">Assessment of optical localizer accuracy for computer aided surgery systems.

</span>
<span class="ltx_bibblock">Computer Aided Surgery 15, 1–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elhoseiny et al. [2015]</span>
<span class="ltx_bibblock">
Elhoseiny, M., El-Gaaly, T., Bakry, A., Elgammal, A., 2015.

</span>
<span class="ltx_bibblock">Convolutional models for joint object categorization and pose estimation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1511.05175 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganin et al. [2016]</span>
<span class="ltx_bibblock">
Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., March, M., Lempitsky, V., 2016.

</span>
<span class="ltx_bibblock">Domain-adversarial training of neural networks.

</span>
<span class="ltx_bibblock">Journal of machine learning research 17, 1–35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasan et al. [2021]</span>
<span class="ltx_bibblock">
Hasan, M.K., Calvet, L., Rabbani, N., Bartoli, A., 2021.

</span>
<span class="ltx_bibblock">Detection, segmentation, and 3d pose estimation of surgical tools using convolutional neural networks and algebraic geometry.

</span>
<span class="ltx_bibblock">Medical Image Analysis 70, 101994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2017]</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017.

</span>
<span class="ltx_bibblock">Mask r-cnn, in: Proceedings of the IEEE international conference on computer vision, pp. 2961–2969.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hein et al. [2021]</span>
<span class="ltx_bibblock">
Hein, J., Seibold, M., Bogo, F., Farshad, M., Pollefeys, M., Fürnstahl, P., Navab, N., 2021.

</span>
<span class="ltx_bibblock">Towards markerless surgical tool and hand pose estimation.

</span>
<span class="ltx_bibblock">International journal of computer assisted radiology and surgery 16, 799–808.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodan et al. [2020]</span>
<span class="ltx_bibblock">
Hodan, T., Barath, D., Matas, J., 2020.

</span>
<span class="ltx_bibblock">Epos: Estimating 6d pose of objects with symmetries, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11703–11712.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodan et al. [2018]</span>
<span class="ltx_bibblock">
Hodan, T., Michel, F., Brachmann, E., Kehl, W., GlentBuch, A., Kraft, D., Drost, B., Vidal, J., Ihrke, S., Zabulis, X., et al., 2018.

</span>
<span class="ltx_bibblock">Bop: Benchmark for 6d object pose estimation, in: Proceedings of the European conference on computer vision (ECCV), pp. 19–34.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodaň et al. [2020]</span>
<span class="ltx_bibblock">
Hodaň, T., Sundermeyer, M., Drost, B., Labbé, Y., Brachmann, E., Michel, F., Rother, C., Matas, J., 2020.

</span>
<span class="ltx_bibblock">Bop challenge 2020 on 6d object localization, in: Computer Vision–ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, Springer. pp. 577–594.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jocher et al. [2023]</span>
<span class="ltx_bibblock">
Jocher, G., Chaurasia, A., Qiu, J., 2023.

</span>
<span class="ltx_bibblock">Ultralytics YOLO.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ultralytics/ultralytics" title="">https://github.com/ultralytics/ultralytics</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov et al. [2023]</span>
<span class="ltx_bibblock">
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R., 2023.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock">arXiv:2304.02643 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kundu et al. [2018]</span>
<span class="ltx_bibblock">
Kundu, A., Li, Y., Rehg, J.M., 2018.

</span>
<span class="ltx_bibblock">3d-rcnn: Instance-level 3d object reconstruction via render-and-compare, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3559–3568.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Labbé et al. [2020]</span>
<span class="ltx_bibblock">
Labbé, Y., Carpentier, J., Aubry, M., Sivic, J., 2020.

</span>
<span class="ltx_bibblock">Cosypose: Consistent multi-view multi-object 6d pose estimation, in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16, Springer. pp. 574–591.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lepetit et al. [2004]</span>
<span class="ltx_bibblock">
Lepetit, V., Pilet, J., Fua, P., 2004.

</span>
<span class="ltx_bibblock">Point matching as a classification problem for fast and robust object pose estimation, in: Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., IEEE. pp. II–II.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019]</span>
<span class="ltx_bibblock">
Li, Z., Wang, G., Ji, X., 2019.

</span>
<span class="ltx_bibblock">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7678–7687.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2017]</span>
<span class="ltx_bibblock">
Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., 2017.

</span>
<span class="ltx_bibblock">Feature pyramid networks for object detection, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2014]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context, in: Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, Springer. pp. 740–755.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019]</span>
<span class="ltx_bibblock">
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., Han, J., 2019.

</span>
<span class="ltx_bibblock">On the variance of the adaptive learning rate and beyond.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1908.03265 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023]</span>
<span class="ltx_bibblock">
Liu, S., Zhou, Y., Yang, J., Gupta, S., Wang, S., 2023.

</span>
<span class="ltx_bibblock">Contactgen: Generative contact modeling for grasp generation, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20609–20620.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2022]</span>
<span class="ltx_bibblock">
Liu, X., Zhang, R., Zhang, C., Fu, B., Tang, J., Liang, X., Tang, J., Cheng, X., Zhang, Y., Wang, G., Ji, X., 2022.

</span>
<span class="ltx_bibblock">Gdrnpp.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/shanice-l/gdrnpp_bop2022" title="">https://github.com/shanice-l/gdrnpp_bop2022</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter [2016]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F., 2016.

</span>
<span class="ltx_bibblock">Sgdr: Stochastic gradient descent with warm restarts.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1608.03983 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meza et al. [2021]</span>
<span class="ltx_bibblock">
Meza, J., Romero, L.A., Marrugo, A.G., 2021.

</span>
<span class="ltx_bibblock">Markerpose: robust real-time planar target tracking for accurate stereo pose estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1282–1290.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller and Allen [2004]</span>
<span class="ltx_bibblock">
Miller, A.T., Allen, P.K., 2004.

</span>
<span class="ltx_bibblock">Graspit! a versatile simulator for robotic grasping.

</span>
<span class="ltx_bibblock">IEEE Robotics &amp; Automation Magazine 11, 110–122.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mousavian et al. [2017]</span>
<span class="ltx_bibblock">
Mousavian, A., Anguelov, D., Flynn, J., Kosecka, J., 2017.

</span>
<span class="ltx_bibblock">3d bounding box estimation using deep learning and geometry, in: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 7074–7082.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nema and Vachhani [2022]</span>
<span class="ltx_bibblock">
Nema, S., Vachhani, L., 2022.

</span>
<span class="ltx_bibblock">Surgical instrument detection and tracking technologies: Automating dataset labeling for surgical skill assessment.

</span>
<span class="ltx_bibblock">Frontiers in Robotics and AI 9, 1030846.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oquab et al. [2023]</span>
<span class="ltx_bibblock">
Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al., 2023.

</span>
<span class="ltx_bibblock">Dinov2: Learning robust visual features without supervision.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.07193 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. [2019]</span>
<span class="ltx_bibblock">
Park, K., Patten, T., Vincze, M., 2019.

</span>
<span class="ltx_bibblock">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7668–7677.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Payet and Todorovic [2011]</span>
<span class="ltx_bibblock">
Payet, N., Todorovic, S., 2011.

</span>
<span class="ltx_bibblock">From contours to 3d object detection and pose estimation, in: 2011 International Conference on Computer Vision, IEEE. pp. 983–990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. [2019]</span>
<span class="ltx_bibblock">
Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H., 2019.

</span>
<span class="ltx_bibblock">Pvnet: Pixel-wise voting network for 6dof pose estimation, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4561–4570.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rad and Lepetit [2017]</span>
<span class="ltx_bibblock">
Rad, M., Lepetit, V., 2017.

</span>
<span class="ltx_bibblock">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth, in: Proceedings of the IEEE international conference on computer vision, pp. 3828–3836.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Romero et al. [2017]</span>
<span class="ltx_bibblock">
Romero, J., Tzionas, D., Black, M.J., 2017.

</span>
<span class="ltx_bibblock">Embodied hands: Modeling and capturing hands and bodies together.

</span>
<span class="ltx_bibblock">ACM Transactions on Graphics, (Proc. SIGGRAPH Asia) 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Romero et al. [2022]</span>
<span class="ltx_bibblock">
Romero, J., Tzionas, D., Black, M.J., 2022.

</span>
<span class="ltx_bibblock">Embodied hands: Modeling and capturing hands and bodies together.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2201.02610 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. [2019]</span>
<span class="ltx_bibblock">
Su, Y., Rambach, J., Minaskan, N., Lesur, P., Pagani, A., Stricker, D., 2019.

</span>
<span class="ltx_bibblock">Deep multi-state object pose estimation for augmented reality assembly, in: 2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), IEEE. pp. 222–227.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. [2022]</span>
<span class="ltx_bibblock">
Su, Y., Saleh, M., Fetzer, T., Rambach, J., Navab, N., Busam, B., Stricker, D., Tombari, F., 2022.

</span>
<span class="ltx_bibblock">Zebrapose: Coarse to fine surface encoding for 6dof object pose estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6738–6748.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundermeyer et al. [2023]</span>
<span class="ltx_bibblock">
Sundermeyer, M., Hodaň, T., Labbe, Y., Wang, G., Brachmann, E., Drost, B., Rother, C., Matas, J., 2023.

</span>
<span class="ltx_bibblock">Bop challenge 2022 on detection, segmentation and pose estimation of specific rigid objects, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2784–2793.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundermeyer et al. [2020]</span>
<span class="ltx_bibblock">
Sundermeyer, M., Marton, Z.C., Durner, M., Triebel, R., 2020.

</span>
<span class="ltx_bibblock">Augmented autoencoders: Implicit 3d orientation learning for 6d object detection.

</span>
<span class="ltx_bibblock">International Journal of Computer Vision 128, 714–729.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taheri et al. [2020]</span>
<span class="ltx_bibblock">
Taheri, O., Ghorbani, N., Black, M.J., Tzionas, D., 2020.

</span>
<span class="ltx_bibblock">GRAB: A dataset of whole-body human grasping of objects, in: European Conference on Computer Vision (ECCV).

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://grab.is.tue.mpg.de" title="">https://grab.is.tue.mpg.de</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tobin et al. [2017]</span>
<span class="ltx_bibblock">
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., Abbeel, P., 2017.

</span>
<span class="ltx_bibblock">Domain randomization for transferring deep neural networks from simulation to the real world, in: 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), IEEE. pp. 23–30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tremblay et al. [2018a]</span>
<span class="ltx_bibblock">
Tremblay, J., Prakash, A., Acuna, D., Brophy, M., Jampani, V., Anil, C., To, T., Cameracci, E., Boochoon, S., Birchfield, S., 2018a.

</span>
<span class="ltx_bibblock">Training deep networks with synthetic data: Bridging the reality gap by domain randomization, in: Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 969–977.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tremblay et al. [2018b]</span>
<span class="ltx_bibblock">
Tremblay, J., To, T., Sundaralingam, B., Xiang, Y., Fox, D., Birchfield, S., 2018b.

</span>
<span class="ltx_bibblock">Deep object pose estimation for semantic robotic grasping of household objects.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1809.10790 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsai et al. [2018]</span>
<span class="ltx_bibblock">
Tsai, C.Y., Hsu, K.J., Nisar, H., 2018.

</span>
<span class="ltx_bibblock">Efficient model-based object pose estimation based on multi-template tracking and pnp algorithms.

</span>
<span class="ltx_bibblock">Algorithms 11, 122.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019]</span>
<span class="ltx_bibblock">
Wang, C., Xu, D., Zhu, Y., Martín-Martín, R., Lu, C., Fei-Fei, L., Savarese, S., 2019.

</span>
<span class="ltx_bibblock">Densefusion: 6d object pose estimation by iterative dense fusion, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3343–3352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021]</span>
<span class="ltx_bibblock">
Wang, G., Manhardt, F., Tombari, F., Ji, X., 2021.

</span>
<span class="ltx_bibblock">Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16611–16621.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. [1997]</span>
<span class="ltx_bibblock">
Wei, G.Q., Arbter, K., Hirzinger, G., 1997.

</span>
<span class="ltx_bibblock">Real-time visual servoing for laparoscopic surgery. controlling robot motion with color image segmentation.

</span>
<span class="ltx_bibblock">IEEE Engineering in Medicine and Biology Magazine 16, 40–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiang et al. [2017]</span>
<span class="ltx_bibblock">
Xiang, Y., Schmidt, T., Narayanan, V., Fox, D., 2017.

</span>
<span class="ltx_bibblock">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1711.00199 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yong et al. [2020]</span>
<span class="ltx_bibblock">
Yong, H., Huang, J., Hua, X., Zhang, L., 2020.

</span>
<span class="ltx_bibblock">Gradient centralization: A new optimization technique for deep neural networks, in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, Springer. pp. 635–652.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zakharov et al. [2019]</span>
<span class="ltx_bibblock">
Zakharov, S., Shugurov, I., Ilic, S., 2019.

</span>
<span class="ltx_bibblock">Dpod: 6d pose object detector and refiner, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 1941–1950.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2019]</span>
<span class="ltx_bibblock">
Zhang, M., Lucas, J., Ba, J., Hinton, G.E., 2019.

</span>
<span class="ltx_bibblock">Lookahead optimizer: k steps forward, 1 step back.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2017]</span>
<span class="ltx_bibblock">
Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., 2017.

</span>
<span class="ltx_bibblock">Pyramid scene parsing network, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881–2890.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2019]</span>
<span class="ltx_bibblock">
Zhou, Y., Barnes, C., Lu, J., Yang, J., Li, H., 2019.

</span>
<span class="ltx_bibblock">On the continuity of rotation representations in neural networks, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5745–5753.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul 16 19:36:37 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
