<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects</title>
<!--Generated on Tue Aug  6 03:34:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.16428v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S1" title="In Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S2" title="In Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S3" title="In Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>HANDS23 challenge overview</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S3.SS1" title="In 3 HANDS23 challenge overview ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Workshop challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S3.SS2" title="In 3 HANDS23 challenge overview ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Evaluation criteria</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S4" title="In Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S4.SS1" title="In 4 Methods ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span><span class="ltx_text" style="color:#000000;">AssemblyHands</span> methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S4.SS2" title="In 4 Methods ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span><span class="ltx_text" style="color:#000000;">ARCTIC</span> methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5" title="In Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.SS1" title="In 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.SS2" title="In 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span><span class="ltx_text" style="color:#000000;">AssemblyHands</span> analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.SS3" title="In 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span><span class="ltx_text" style="color:#000000;">ARCTIC</span> analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S6" title="In Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S1a" title="In Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional results of <span class="ltx_text" style="color:#000000;">AssemblyHands</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">

Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zicong Fan<sup class="ltx_sup" id="id25.2.id1"><span class="ltx_text ltx_font_italic" id="id25.2.id1.1">1,2∗</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Takehiko Ohkawa<sup class="ltx_sup" id="id26.2.id1"><span class="ltx_text ltx_font_italic" id="id26.2.id1.1">3∗</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Linlin Yang<sup class="ltx_sup" id="id27.2.id1"><span class="ltx_text ltx_font_italic" id="id27.2.id1.1">4∗</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/> Nie Lin<sup class="ltx_sup" id="id28.2.id1">3</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
 Zhishan Zhou<sup class="ltx_sup" id="id29.2.id1">5</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Shihao Zhou<sup class="ltx_sup" id="id30.2.id1">5</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Jiajun Liang<sup class="ltx_sup" id="id31.2.id1">5</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/> Zhong Gao<sup class="ltx_sup" id="id32.2.id1">6</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
 Xuanyang Zhang<sup class="ltx_sup" id="id33.2.id1">6</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Xue Zhang<sup class="ltx_sup" id="id34.2.id1">7</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Fei Li<sup class="ltx_sup" id="id35.2.id1">7</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Zheng Liu<sup class="ltx_sup" id="id36.2.id1">8</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/> Feng Lu<sup class="ltx_sup" id="id37.2.id1">8</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
 Karim Abou Zeid<sup class="ltx_sup" id="id38.2.id1">9</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Bastian Leibe<sup class="ltx_sup" id="id39.2.id1">9</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Jeongwan On<sup class="ltx_sup" id="id40.2.id1"><span class="ltx_text ltx_font_italic" id="id40.2.id1.1">10</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/> Seungryul Baek<sup class="ltx_sup" id="id41.2.id1"><span class="ltx_text ltx_font_italic" id="id41.2.id1.1">10</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
 Aditya Prakash<sup class="ltx_sup" id="id42.2.id1"><span class="ltx_text ltx_font_italic" id="id42.2.id1.1">11</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Saurabh Gupta<sup class="ltx_sup" id="id43.2.id1"><span class="ltx_text ltx_font_italic" id="id43.2.id1.1">11</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Kun He<sup class="ltx_sup" id="id44.2.id1"><span class="ltx_text ltx_font_italic" id="id44.2.id1.1">12</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/> Yoichi Sato<sup class="ltx_sup" id="id45.2.id1">3</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
 Otmar Hilliges<sup class="ltx_sup" id="id46.2.id1">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Hyung Jin Chang<sup class="ltx_sup" id="id47.2.id1"><span class="ltx_text ltx_font_italic" id="id47.2.id1.1">13</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Angela Yao<sup class="ltx_sup" id="id48.2.id1"><span class="ltx_text ltx_font_italic" id="id48.2.id1.1">14</span></sup>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id49.id1">We interact with the world with our hands and see it through our own (egocentric) perspective.
A holistic 3D understanding of such interactions
from egocentric views
is important for tasks in robotics, AR/VR, action recognition and motion generation.
Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement.
To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits.
Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks.
Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from different views.
Our study further reveals challenging scenarios intractable with state-of-the-art methods, such as fast hand motion, object reconstruction from narrow egocentric views, and close contact between two hands and objects.
Our efforts will enrich the community’s knowledge foundation and facilitate future hand studies on egocentric hand-object interactions.</p>
</div>
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>
<sup class="ltx_sup" id="footnote1.1">∗</sup>Equal contribution,
<sup class="ltx_sup" id="footnote1.2">1</sup>ETH Zürich,
<sup class="ltx_sup" id="footnote1.3">2</sup>Max Planck Institute for Intelligent Systems, Tübingen, Germany,
<sup class="ltx_sup" id="footnote1.4">3</sup>The University of Tokyo,
<sup class="ltx_sup" id="footnote1.5">4</sup>Communication University of China,
<sup class="ltx_sup" id="footnote1.6">5</sup>Jiiov,
<sup class="ltx_sup" id="footnote1.7">6</sup>Bytedance,
<sup class="ltx_sup" id="footnote1.8">7</sup>Fujitsu Research &amp; Development Center Co., Ltd.,
<sup class="ltx_sup" id="footnote1.9">8</sup>Beihang University,
<sup class="ltx_sup" id="footnote1.10">9</sup>RWTH Aachen University,
<sup class="ltx_sup" id="footnote1.11"><span class="ltx_text ltx_font_italic" id="footnote1.11.1">10</span></sup>UNIST,
<sup class="ltx_sup" id="footnote1.12"><span class="ltx_text ltx_font_italic" id="footnote1.12.1">11</span></sup>UIUC,
<sup class="ltx_sup" id="footnote1.13"><span class="ltx_text ltx_font_italic" id="footnote1.13.1">12</span></sup>Meta,
<sup class="ltx_sup" id="footnote1.14"><span class="ltx_text ltx_font_italic" id="footnote1.14.1">13</span></sup>University of Birmingham,
<sup class="ltx_sup" id="footnote1.15"><span class="ltx_text ltx_font_italic" id="footnote1.15.1">14</span></sup>National University of Singapore.
</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="104" id="S1.F1.g1" src="extracted/5776199/0_figures/fig/teaser.jpg" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.5.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1.6.2" style="font-size:90%;">Tasks in HANDS23<span class="ltx_text ltx_font_medium" id="S1.F1.6.2.1">.
In <span class="ltx_text" id="S1.F1.6.2.1.1" style="color:#000000;">AssemblyHands</span>, from its multi-view headset (a), we estimate 3D hand poses from images (b); In <span class="ltx_text" id="S1.F1.6.2.1.2" style="color:#000000;">ARCTIC</span>, we estimate the poses of two hands and articulated objects from an image (c).</span></span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">We interact with the world with our hands
and see it through our eyes: we wake up and grab our phone to check the time;
we use tools when assembling parts of a car; we open the microwave door to heat food, to name a few.
An egocentric 3D understanding of our hand interactions with objects
will fundamentally impact areas such as robotics grasping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib11" title="">11</a>]</cite>,
augmented and virtual reality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib25" title="">25</a>]</cite>,
action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib7" title="">7</a>]</cite> and motion generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib79" title="">79</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, it is non-trivial to accurately reconstruct 3D hands and or objects due to its high degree of freedoms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib65" title="">65</a>]</cite>, ambiguous texture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib16" title="">16</a>]</cite>, and heavy occlusions.
These challenges are intensified in an egocentric view <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib51" title="">51</a>]</cite>, particularly with object interactions, due to significant camera distortion, rapid and varied changes in viewpoint caused by head movements and hand-object occlusion.
To better understand these challenges, we introduce a public challenge in conjunction with ICCV 2023 (<em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">i.e</em>.<span class="ltx_text" id="S1.p2.1.2"></span>, HANDS23) based on recent egocentric hand datasets, AssemblyHands <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib49" title="">49</a>]</cite> and ARCTIC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib17" title="">17</a>]</cite> (see <span class="ltx_text" id="S1.p2.1.3" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">1</span></a></span>). These two datasets are large-scale, multi-view, and provide monochrome or RGB egocentric videos of the hands dexterously manipulating objects.
Accordingly, we host two tasks: 1) egocentric 3D hand pose estimation from a single-view image based on AssemblyHands, and 2) consistent motion reconstruction based on ARCTIC.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We introduce new HANDS23 methods and recent dataset leaderboard baselines that substantially outperform initial baselines for both tasks, setting new benchmarks for subsequent comparisons on the datasets.
The two datasets are significantly larger and include a wider variety of bimanual manipulations compared to earlier datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib6" title="">6</a>]</cite>, enabling a more authentic assessment of real-world interactions.
With these benchmarks, we thoroughly analyze factors such as viewpoint, action types, hand position, model size, and object variations to determine their impact on 3D hand(-object) reconstruction.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our findings show the success of addressing the distortion of egocentric cameras with explicit perspective cropping or implicit learning for the distortion bias.
In addition, recent high-capacity vision transformers can learn complex hand-object interactions.
Adaptive fusion techniques for multi-view predictions further boost performance.
We also analyze the remaining challenges that are still difficult to handle with the recent methods, <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">e.g</em>.<span class="ltx_text" id="S1.p4.1.2"></span>, fast hand motion, object reconstruction from narrow and moving views, and intricate interactions and close contact between two hands and objects.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To summarize, we contribute state-of-the-art baselines and gather the submitted methods for <span class="ltx_text" id="S1.p5.1.1" style="color:#000000;">AssemblyHands</span> and <span class="ltx_text" id="S1.p5.1.2" style="color:#000000;">ARCTIC</span> to foster future research on egocentric hand-object interactions.
Furthermore, we thoroughly analyze the two benchmarks to provide insights for future directions in egocentric hand pose estimation and consistent motion reconstruction.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">3D hand pose estimation:</span>
Reconstructing 3D hand poses has a long history <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib48" title="">48</a>]</cite> ever since the important work led by Rehg and Kanade <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib57" title="">57</a>]</cite>.
A large body of research in this area focuses on single-hand reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib13" title="">13</a>]</cite>.
For example,
while a popular OpenPose library features 2D hand keypoints  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib63" title="">63</a>]</cite>, Zimmermann <em class="ltx_emph ltx_font_italic" id="S2.p1.1.2">et al</em>.<span class="ltx_text" id="S2.p1.1.3"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib88" title="">88</a>]</cite> initially extend to estimate 3D hand poses using deep convolutional networks.
Ever since the release of InterHand2.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib45" title="">45</a>]</cite> dataset, the community has increased focus on strongly interacting two hands reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib22" title="">22</a>]</cite>.
For example, Li <em class="ltx_emph ltx_font_italic" id="S2.p1.1.4">et al</em>.<span class="ltx_text" id="S2.p1.1.5"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib35" title="">35</a>]</cite> and Moon <em class="ltx_emph ltx_font_italic" id="S2.p1.1.6">et al</em>.<span class="ltx_text" id="S2.p1.1.7"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib44" title="">44</a>]</cite> use relighting techniques to augment InterHand2.6M with more natural lighting and diverse backgrounds.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Hand-object reconstruction:</span>
The holistic reconstruction of hands and objects have increased interest in the hand community in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib6" title="">6</a>]</cite>.
Methods in this area mostly assume a given object template and jointly estimate the hand poses and the object rigid poses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib74" title="">74</a>]</cite> or articulated poses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib17" title="">17</a>]</cite>.
For example, Cao <em class="ltx_emph ltx_font_italic" id="S2.p2.1.2">et al</em>.<span class="ltx_text" id="S2.p2.1.3"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib5" title="">5</a>]</cite> fits object templates to in-the-wild interaction videos.
Liu <em class="ltx_emph ltx_font_italic" id="S2.p2.1.4">et al</em>.<span class="ltx_text" id="S2.p2.1.5"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib39" title="">39</a>]</cite> introduce a semi-supervised learning framework via pseudo-groundtruth from temporal data to improve hand-object reconstruction.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">More recent methods do not assume object templates for hand-object reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib68" title="">68</a>]</cite>.
For example, Fan <em class="ltx_emph ltx_font_italic" id="S2.p3.1.1">et al</em>.<span class="ltx_text" id="S2.p3.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib15" title="">15</a>]</cite> introduced the first category-agnostic method that reconstructs an articulated hand and object jointly from a video. Nonetheless,our challenges and insights on hand-object occlusions and camera distortions are still applicable to these more challenging template-free reconstruction settings.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Public reports for the previous challenges (HANDS17 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib77" title="">77</a>]</cite> and HANDS19 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib2" title="">2</a>]</cite>)
distilled the insights from individual review papers and practical techniques into comprehensive summaries to enrich the community’s knowledge base.
These past challenges use benchmarks which include depth-based hand pose estimation from egocentric views.
Instead of depth sensors, the HANDS23 benchmarks are based on affordable and widely applicable image sensors, <em class="ltx_emph ltx_font_italic" id="S2.p4.1.1">i.e</em>.<span class="ltx_text" id="S2.p4.1.2"></span>, RGB and monochrome images.
This paper further advances the analysis with unique insights, such as multi-view egocentric cameras, object reconstruction in contact, and modeling with recent transformers beyond conventional CNNs.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>HANDS23 challenge overview</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The workshop contains two hand-related 3D reconstruction challenges in hand-object strongly interacting settings.
In this section, we introduce the two challenges and their evaluation criteria.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Workshop challenges</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">3D hand pose estimation in AssemblyHands:</span>
As illustrated in <span class="ltx_text" id="S3.SS1.p1.1.2" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">1</span></a></span>,
this task focuses on egocentric 3D hand pose estimation from a single-view image based on <span class="ltx_text" id="S3.SS1.p1.1.3" style="color:#000000;">AssemblyHands</span>. The dataset provides multi-view captured videos of hand-object interaction while assembling and disassembling toy vehicles.
In particular, it provides allocentric (fixed-view) and egocentric recordings and auxiliary cues like action, object, or context information for hand pose estimation.
We refer readers to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib49" title="">49</a>]</cite> for more dataset details.
The training, validation, and testing sets contain 383K, 32K, and 62K monochrome images, respectively, all captured from egocentric cameras.
During training, 3D hand keypoint coordinates, hand bounding boxes and camera intrinsic and extrinsic matrices for the four egocentric cameras attached to the headset are provided.
The same information is provided during testing, minus the 3D keypoints.
Unlike <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib49" title="">49</a>]</cite>,
given the availability of multi-view egocentric images, this task lets participants develop multi-view fusion based on the corresponding multi-view images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.2"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.2.1">Consistent motion reconstruction in ARCTIC:</span>
Given an RGB image, the goal of this task is to estimate poses of hands and articulated objects to recover the 3D surfaces of the interaction (see <span class="ltx_text" id="S3.SS1.p2.2.2" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">1</span></a></span>).
We refer readers to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib17" title="">17</a>]</cite> for more details.
The <span class="ltx_text" id="S3.SS1.p2.2.3" style="color:#000000;">ARCTIC</span> dataset contains data of hands dexterously manipulating articulated objects and videos from 8<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mo id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><times id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">×</annotation></semantics></math> allocentric views and 1<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mo id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><times id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">×</annotation></semantics></math> egocentric views.
The official splits of the ARCTIC dataset are used for training, validation, and testing.
There are two sub-tasks: allocentric task and egocentric task. In the former, only allocentric images can be used for training and evaluation. For the latter, all images from the training set can be used for training while only the egocentric view images are used during evaluation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation criteria</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.6"><span class="ltx_text ltx_font_bold" id="S3.SS2.p1.6.1" style="color:#000000;">AssemblyHands</span><span class="ltx_text ltx_font_bold" id="S3.SS2.p1.6.2"> evaluation:</span>
We use MPJPE as an evaluation metric in millimeters, comparing the model predictions against the ground-truth in world coordinates.
We provide the intrinsic and extrinsic of the egocentric cameras to construct submission results defined in the world coordinates.
Assuming that the human hand has a total <math alttext="{N}_{J}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">N</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">J</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑁</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝐽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">{N}_{J}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_N start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT</annotation></semantics></math> joints, we denote
wrist-relative coordinates of the prediction and ground-truth as <math alttext="\hat{J}\in\mathbb{R}^{{N}_{J}\times 3}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2.2" xref="S3.SS2.p1.2.m2.1.1.2.2.cmml">J</mi><mo id="S3.SS2.p1.2.m2.1.1.2.1" xref="S3.SS2.p1.2.m2.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml"><msub id="S3.SS2.p1.2.m2.1.1.3.3.2" xref="S3.SS2.p1.2.m2.1.1.3.3.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.3.2.2" xref="S3.SS2.p1.2.m2.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS2.p1.2.m2.1.1.3.3.2.3" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3.cmml">J</mi></msub><mo id="S3.SS2.p1.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS2.p1.2.m2.1.1.3.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><in id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></in><apply id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><ci id="S3.SS2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2.1">^</ci><ci id="S3.SS2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2">𝐽</ci></apply><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3"><times id="S3.SS2.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.1"></times><apply id="S3.SS2.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.3.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.3.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2.2">𝑁</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.2.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3">𝐽</ci></apply><cn id="S3.SS2.p1.2.m2.1.1.3.3.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\hat{J}\in\mathbb{R}^{{N}_{J}\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">over^ start_ARG italic_J end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT × 3 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="J\in\mathbb{R}^{{N}_{J}\times 3}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">J</mi><mo id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml"><msub id="S3.SS2.p1.3.m3.1.1.3.3.2" xref="S3.SS2.p1.3.m3.1.1.3.3.2.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.3.2.2" xref="S3.SS2.p1.3.m3.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS2.p1.3.m3.1.1.3.3.2.3" xref="S3.SS2.p1.3.m3.1.1.3.3.2.3.cmml">J</mi></msub><mo id="S3.SS2.p1.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.3.m3.1.1.3.3.1.cmml">×</mo><mn id="S3.SS2.p1.3.m3.1.1.3.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><in id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></in><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝐽</ci><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3"><times id="S3.SS2.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3.1"></times><apply id="S3.SS2.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.3.2.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3.2">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.3.3.2.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3.2.2">𝑁</ci><ci id="S3.SS2.p1.3.m3.1.1.3.3.2.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3.2.3">𝐽</ci></apply><cn id="S3.SS2.p1.3.m3.1.1.3.3.3.cmml" type="integer" xref="S3.SS2.p1.3.m3.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">J\in\mathbb{R}^{{N}_{J}\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_J ∈ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT × 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, respectively.
Given a joint visibility indicator <math alttext="\gamma_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">γ</mi><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝛾</ci><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\gamma_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_γ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> per joint <math alttext="J_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><msub id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">J</mi><mi id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">𝐽</ci><ci id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">J_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_J start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, we compute the Euclidean distance between predicted and ground-truth joints as <math alttext="\frac{1}{\sum_{i=1}^{N_{J}}\gamma_{i}}\sum_{i=1}^{N_{J}}\gamma_{i}\left\|\hat{%
J}_{i}-J_{i}\right\|_{2}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><mrow id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mfrac id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml"><mn id="S3.SS2.p1.6.m6.1.1.3.2" xref="S3.SS2.p1.6.m6.1.1.3.2.cmml">1</mn><mrow id="S3.SS2.p1.6.m6.1.1.3.3" xref="S3.SS2.p1.6.m6.1.1.3.3.cmml"><mstyle displaystyle="false" id="S3.SS2.p1.6.m6.1.1.3.3.1" xref="S3.SS2.p1.6.m6.1.1.3.3.1.cmml"><msubsup id="S3.SS2.p1.6.m6.1.1.3.3.1a" xref="S3.SS2.p1.6.m6.1.1.3.3.1.cmml"><mo id="S3.SS2.p1.6.m6.1.1.3.3.1.2.2" xref="S3.SS2.p1.6.m6.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.SS2.p1.6.m6.1.1.3.3.1.2.3" xref="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.2" xref="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.1" xref="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.3" xref="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><msub id="S3.SS2.p1.6.m6.1.1.3.3.1.3" xref="S3.SS2.p1.6.m6.1.1.3.3.1.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.3.3.1.3.2" xref="S3.SS2.p1.6.m6.1.1.3.3.1.3.2.cmml">N</mi><mi id="S3.SS2.p1.6.m6.1.1.3.3.1.3.3" xref="S3.SS2.p1.6.m6.1.1.3.3.1.3.3.cmml">J</mi></msub></msubsup></mstyle><msub id="S3.SS2.p1.6.m6.1.1.3.3.2" xref="S3.SS2.p1.6.m6.1.1.3.3.2.cmml"><mi id="S3.SS2.p1.6.m6.1.1.3.3.2.2" xref="S3.SS2.p1.6.m6.1.1.3.3.2.2.cmml">γ</mi><mi id="S3.SS2.p1.6.m6.1.1.3.3.2.3" xref="S3.SS2.p1.6.m6.1.1.3.3.2.3.cmml">i</mi></msub></mrow></mfrac><mo id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p1.6.m6.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.cmml"><msubsup id="S3.SS2.p1.6.m6.1.1.1.2" xref="S3.SS2.p1.6.m6.1.1.1.2.cmml"><mo id="S3.SS2.p1.6.m6.1.1.1.2.2.2" xref="S3.SS2.p1.6.m6.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.SS2.p1.6.m6.1.1.1.2.2.3" xref="S3.SS2.p1.6.m6.1.1.1.2.2.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.1.2.2.3.2" xref="S3.SS2.p1.6.m6.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.SS2.p1.6.m6.1.1.1.2.2.3.1" xref="S3.SS2.p1.6.m6.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.SS2.p1.6.m6.1.1.1.2.2.3.3" xref="S3.SS2.p1.6.m6.1.1.1.2.2.3.3.cmml">1</mn></mrow><msub id="S3.SS2.p1.6.m6.1.1.1.2.3" xref="S3.SS2.p1.6.m6.1.1.1.2.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.1.2.3.2" xref="S3.SS2.p1.6.m6.1.1.1.2.3.2.cmml">N</mi><mi id="S3.SS2.p1.6.m6.1.1.1.2.3.3" xref="S3.SS2.p1.6.m6.1.1.1.2.3.3.cmml">J</mi></msub></msubsup><mrow id="S3.SS2.p1.6.m6.1.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.1.cmml"><msub id="S3.SS2.p1.6.m6.1.1.1.1.3" xref="S3.SS2.p1.6.m6.1.1.1.1.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.1.1.3.2" xref="S3.SS2.p1.6.m6.1.1.1.1.3.2.cmml">γ</mi><mi id="S3.SS2.p1.6.m6.1.1.1.1.3.3" xref="S3.SS2.p1.6.m6.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S3.SS2.p1.6.m6.1.1.1.1.2" xref="S3.SS2.p1.6.m6.1.1.1.1.2.cmml">⁢</mo><msub id="S3.SS2.p1.6.m6.1.1.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.1.1.cmml"><mrow id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.2.cmml"><mo id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.2" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.cmml"><msub id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2.2" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2.2.cmml">J</mi><mo id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2.1" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.3" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.2" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.2.cmml">J</mi><mi id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.3" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.3" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.SS2.p1.6.m6.1.1.1.1.1.3" xref="S3.SS2.p1.6.m6.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><times id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2"></times><apply id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3"><divide id="S3.SS2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3"></divide><cn id="S3.SS2.p1.6.m6.1.1.3.2.cmml" type="integer" xref="S3.SS2.p1.6.m6.1.1.3.2">1</cn><apply id="S3.SS2.p1.6.m6.1.1.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3"><apply id="S3.SS2.p1.6.m6.1.1.3.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.3.3.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1">superscript</csymbol><apply id="S3.SS2.p1.6.m6.1.1.3.3.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.3.3.1.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1">subscript</csymbol><sum id="S3.SS2.p1.6.m6.1.1.3.3.1.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1.2.2"></sum><apply id="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1.2.3"><eq id="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.1"></eq><ci id="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.2">𝑖</ci><cn id="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.3.cmml" type="integer" xref="S3.SS2.p1.6.m6.1.1.3.3.1.2.3.3">1</cn></apply></apply><apply id="S3.SS2.p1.6.m6.1.1.3.3.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.3.3.1.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1.3">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.3.3.1.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1.3.2">𝑁</ci><ci id="S3.SS2.p1.6.m6.1.1.3.3.1.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.1.3.3">𝐽</ci></apply></apply><apply id="S3.SS2.p1.6.m6.1.1.3.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.3.3.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.2">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.3.3.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.2.2">𝛾</ci><ci id="S3.SS2.p1.6.m6.1.1.3.3.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.2.3">𝑖</ci></apply></apply></apply><apply id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1"><apply id="S3.SS2.p1.6.m6.1.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2">superscript</csymbol><apply id="S3.SS2.p1.6.m6.1.1.1.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.2.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2">subscript</csymbol><sum id="S3.SS2.p1.6.m6.1.1.1.2.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2.2.2"></sum><apply id="S3.SS2.p1.6.m6.1.1.1.2.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2.2.3"><eq id="S3.SS2.p1.6.m6.1.1.1.2.2.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2.2.3.1"></eq><ci id="S3.SS2.p1.6.m6.1.1.1.2.2.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.SS2.p1.6.m6.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.SS2.p1.6.m6.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.SS2.p1.6.m6.1.1.1.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.2.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2.3">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.1.2.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2.3.2">𝑁</ci><ci id="S3.SS2.p1.6.m6.1.1.1.2.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.1.2.3.3">𝐽</ci></apply></apply><apply id="S3.SS2.p1.6.m6.1.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1"><times id="S3.SS2.p1.6.m6.1.1.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.2"></times><apply id="S3.SS2.p1.6.m6.1.1.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.3.2">𝛾</ci><ci id="S3.SS2.p1.6.m6.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.SS2.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.p1.6.m6.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.6.m6.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1"><minus id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2"><ci id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.2.2">𝐽</ci></apply><ci id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.2">𝐽</ci><ci id="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><cn id="S3.SS2.p1.6.m6.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.p1.6.m6.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">\frac{1}{\sum_{i=1}^{N_{J}}\gamma_{i}}\sum_{i=1}^{N_{J}}\gamma_{i}\left\|\hat{%
J}_{i}-J_{i}\right\|_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">divide start_ARG 1 end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_γ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_γ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ over^ start_ARG italic_J end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_J start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>.
The visibility indicator offers per-joint binary labels, representing whether the annotated keypoints are visible from the given egocentric view.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.7"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.7.1" style="color:#000000;">ARCTIC</span><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.7.2"> evaluation:</span>
Since the original <span class="ltx_text" id="S3.SS2.p2.7.3" style="color:#000000;">ARCTIC</span> paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib17" title="">17</a>]</cite> has a heavy focus on the quality of hand-object contact in the reconstructed hand and object meshes, we use Contact Deviation (CDev) introduced in the <span class="ltx_text" id="S3.SS2.p2.7.4" style="color:#000000;">ARCTIC</span> dataset as the main metric for the competition. In particular, this metric measures the extent to which a hand vertex deviates from the supposed contact object vertex in the prediction. Concretely,
suppose that for a given frame, <math alttext="\{(\mathbf{h}_{i},\mathbf{o}_{i})\}_{i=1}^{C}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msubsup id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mrow id="S3.SS2.p2.1.m1.1.1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.1.2.cmml"><mo id="S3.SS2.p2.1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.1.m1.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.3.cmml"><mo id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.3" stretchy="false" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.2.cmml">𝐡</mi><mi id="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.4" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.2" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.2.cmml">𝐨</mi><mi id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.3" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.5" stretchy="false" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS2.p2.1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.1.m1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p2.1.m1.1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p2.1.m1.1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.1.m1.1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">C</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1">superscript</csymbol><apply id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><set id="S3.SS2.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1"><interval closure="open" id="S3.SS2.p2.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2"><apply id="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.2">𝐡</ci><ci id="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.2">𝐨</ci><ci id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply></interval></set><apply id="S3.SS2.p2.1.m1.1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.1.3"><eq id="S3.SS2.p2.1.m1.1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.3.1"></eq><ci id="S3.SS2.p2.1.m1.1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.3.2">𝑖</ci><cn id="S3.SS2.p2.1.m1.1.1.1.3.3.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\{(\mathbf{h}_{i},\mathbf{o}_{i})\}_{i=1}^{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">{ ( bold_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> are <math alttext="C" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_C</annotation></semantics></math> pairs of in-contact (<math alttext="&lt;3mm" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml"></mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">&lt;</mo><mrow id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml"><mn id="S3.SS2.p2.3.m3.1.1.3.2" xref="S3.SS2.p2.3.m3.1.1.3.2.cmml">3</mn><mo id="S3.SS2.p2.3.m3.1.1.3.1" xref="S3.SS2.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.1.1.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.cmml">m</mi><mo id="S3.SS2.p2.3.m3.1.1.3.1a" xref="S3.SS2.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.1.1.3.4" xref="S3.SS2.p2.3.m3.1.1.3.4.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><lt id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></lt><csymbol cd="latexml" id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">absent</csymbol><apply id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3"><times id="S3.SS2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3.1"></times><cn id="S3.SS2.p2.3.m3.1.1.3.2.cmml" type="integer" xref="S3.SS2.p2.3.m3.1.1.3.2">3</cn><ci id="S3.SS2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3">𝑚</ci><ci id="S3.SS2.p2.3.m3.1.1.3.4.cmml" xref="S3.SS2.p2.3.m3.1.1.3.4">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">&lt;3mm</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">&lt; 3 italic_m italic_m</annotation></semantics></math> distance) hand-object vertices according to ground-truth, and <math alttext="\{(\hat{\mathbf{h}}_{i},\hat{\mathbf{o}}_{i})\}_{i=1}^{C}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><msubsup id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mrow id="S3.SS2.p2.4.m4.1.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.2.cmml"><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.3.cmml"><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.3" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.2.cmml">𝐡</mi><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.4" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.cmml"><mover accent="true" id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2.2.cmml">𝐨</mi><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2.1.cmml">^</mo></mover><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.3" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.5" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p2.4.m4.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p2.4.m4.1.1.1.3.1" xref="S3.SS2.p2.4.m4.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.4.m4.1.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">C</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1">superscript</csymbol><apply id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><set id="S3.SS2.p2.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1"><interval closure="open" id="S3.SS2.p2.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2"><apply id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2"><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.2">𝐡</ci></apply><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2">subscript</csymbol><apply id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2"><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2.1">^</ci><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.2.2">𝐨</ci></apply><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.2.3">𝑖</ci></apply></interval></set><apply id="S3.SS2.p2.4.m4.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3"><eq id="S3.SS2.p2.4.m4.1.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.1"></eq><ci id="S3.SS2.p2.4.m4.1.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.2">𝑖</ci><cn id="S3.SS2.p2.4.m4.1.1.1.3.3.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\{(\hat{\mathbf{h}}_{i},\hat{\mathbf{o}}_{i})\}_{i=1}^{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">{ ( over^ start_ARG bold_h end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG bold_o end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> are the predictions respectively.
The CDev metric is the average distance between <math alttext="\hat{\mathbf{h}}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mover accent="true" id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2.2" xref="S3.SS2.p2.5.m5.1.1.2.2.cmml">𝐡</mi><mo id="S3.SS2.p2.5.m5.1.1.2.1" xref="S3.SS2.p2.5.m5.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><apply id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2"><ci id="S3.SS2.p2.5.m5.1.1.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.2.1">^</ci><ci id="S3.SS2.p2.5.m5.1.1.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2.2">𝐡</ci></apply><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\hat{\mathbf{h}}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">over^ start_ARG bold_h end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\hat{\mathbf{o}}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mover accent="true" id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2.2" xref="S3.SS2.p2.6.m6.1.1.2.2.cmml">𝐨</mi><mo id="S3.SS2.p2.6.m6.1.1.2.1" xref="S3.SS2.p2.6.m6.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><apply id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2"><ci id="S3.SS2.p2.6.m6.1.1.2.1.cmml" xref="S3.SS2.p2.6.m6.1.1.2.1">^</ci><ci id="S3.SS2.p2.6.m6.1.1.2.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2.2">𝐨</ci></apply><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\hat{\mathbf{o}}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">over^ start_ARG bold_o end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in millimeters, <math alttext="\frac{1}{C}\sum_{i=1}^{C}||\hat{\mathbf{h}}_{i}-\hat{\mathbf{o}}_{i}||" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m7.1"><semantics id="S3.SS2.p2.7.m7.1a"><mrow id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mfrac id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml"><mn id="S3.SS2.p2.7.m7.1.1.3.2" xref="S3.SS2.p2.7.m7.1.1.3.2.cmml">1</mn><mi id="S3.SS2.p2.7.m7.1.1.3.3" xref="S3.SS2.p2.7.m7.1.1.3.3.cmml">C</mi></mfrac><mo id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p2.7.m7.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.cmml"><msubsup id="S3.SS2.p2.7.m7.1.1.1.2" xref="S3.SS2.p2.7.m7.1.1.1.2.cmml"><mo id="S3.SS2.p2.7.m7.1.1.1.2.2.2" rspace="0em" xref="S3.SS2.p2.7.m7.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.SS2.p2.7.m7.1.1.1.2.2.3" xref="S3.SS2.p2.7.m7.1.1.1.2.2.3.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.2.2.3.2" xref="S3.SS2.p2.7.m7.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.SS2.p2.7.m7.1.1.1.2.2.3.1" xref="S3.SS2.p2.7.m7.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.SS2.p2.7.m7.1.1.1.2.2.3.3" xref="S3.SS2.p2.7.m7.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.SS2.p2.7.m7.1.1.1.2.3" xref="S3.SS2.p2.7.m7.1.1.1.2.3.cmml">C</mi></msubsup><mrow id="S3.SS2.p2.7.m7.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.2.cmml"><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.7.m7.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.SS2.p2.7.m7.1.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.cmml"><msub id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2.2.cmml">𝐡</mi><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.2.cmml">𝐨</mi><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.7.m7.1.1.1.1.2.1.cmml">‖</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><times id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2"></times><apply id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3"><divide id="S3.SS2.p2.7.m7.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.3"></divide><cn id="S3.SS2.p2.7.m7.1.1.3.2.cmml" type="integer" xref="S3.SS2.p2.7.m7.1.1.3.2">1</cn><ci id="S3.SS2.p2.7.m7.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3">𝐶</ci></apply><apply id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"><apply id="S3.SS2.p2.7.m7.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.2.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.2">superscript</csymbol><apply id="S3.SS2.p2.7.m7.1.1.1.2.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.2.2.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.2">subscript</csymbol><sum id="S3.SS2.p2.7.m7.1.1.1.2.2.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.2.2.2"></sum><apply id="S3.SS2.p2.7.m7.1.1.1.2.2.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.2.2.3"><eq id="S3.SS2.p2.7.m7.1.1.1.2.2.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.2.2.3.1"></eq><ci id="S3.SS2.p2.7.m7.1.1.1.2.2.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.SS2.p2.7.m7.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.SS2.p2.7.m7.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.SS2.p2.7.m7.1.1.1.2.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.2.3">𝐶</ci></apply><apply id="S3.SS2.p2.7.m7.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p2.7.m7.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.2">norm</csymbol><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1"><minus id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1"></minus><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2"><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2.1">^</ci><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.2.2">𝐡</ci></apply><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2"><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.2">𝐨</ci></apply><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\frac{1}{C}\sum_{i=1}^{C}||\hat{\mathbf{h}}_{i}-\hat{\mathbf{o}}_{i}||</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">divide start_ARG 1 end_ARG start_ARG italic_C end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT | | over^ start_ARG bold_h end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG bold_o end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | |</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">For completeness, we report all metrics introduced in <span class="ltx_text" id="S3.SS2.p3.1.1" style="color:#000000;">ARCTIC</span>.
In particular, the task requires the reconstructed meshes to have accurate hand-object contact (CDev) and smooth motion (ACC). Additionally, during articulation or when carrying an object, it is crucial that the vertices of the hand and object which are in contact maintain synchronized movement (MDev).
Moreover, we assess hand and object poses, alongside their relative movements, using metrics like MPJPE, AAE, Success Rate, and MRRPE. For detailed information, see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib17" title="">17</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This section presents the methods in the two challenges and other competing methods on the leaderboards.
Four methods outperform the baseline in both <span class="ltx_text" id="S4.p1.1.1" style="color:#000000;">AssemblyHands</span> and <span class="ltx_text" id="S4.p1.1.2" style="color:#000000;">ARCTIC</span>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span class="ltx_text" id="S4.SS1.1.1" style="color:#000000;">AssemblyHands</span> methods</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Participants develop methods that learn the mapping from egocentric images to 3D keypoints.
The methods are categorized into: <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">heatmap-based</span> and <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">regression-based</span> approaches.
Given the presence of complex hand-object interactions in the egocentric scenes, high-capacity transformer models and attention mechanisms addressing occluded regions have been proposed as the backbone networks.
<span class="ltx_text" id="S4.SS1.p1.1.3" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S4.T1" title="Table 1 ‣ 4.1 AssemblyHands methods ‣ 4 Methods ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">1</span></a></span> summarizes the methods based on the learning, preprocessing, multi-view fusion, and post-processing approaches.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1" style="color:#000000;">Base</span><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.2">:</span> This method uses a <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.3">heatmap-based</span> framework based on heatmaps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib45" title="">45</a>]</cite> with 2.5D representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib32" title="">32</a>]</cite> and a ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib29" title="">29</a>]</cite> backbone.
The implementation can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib47" title="">47</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1" style="color:#000000;">JHands</span><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.2"> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib85" title="">85</a>]</cite>:</span> This method employs a <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.3">regression-based</span> approach with simple MLP heads for regressing 2D keypoints, root-relative 3D keypoints, and the global root depth.
The regression training is empowered by a recent fast and strong vision transformer, Hiera <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib60" title="">60</a>]</cite>, pre-trained with masked auto-encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib30" title="">30</a>]</cite>.
A multi-level feature fusion that concatenates the features of different layers is adopted for better feature extraction at different scales.
The method additionally uses other publicly available datasets for training, namely FreiHAND <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib88" title="">88</a>]</cite>, DexYCB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib6" title="">6</a>]</cite>, and CompHand <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib8" title="">8</a>]</cite>.
The implementation is available in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib83" title="">83</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1" style="color:#000000;">PICO-AI</span><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.2">:</span>
This method proposes a heatmap voting scheme in addition to the 2.5D heatmaps.
Due to their sparsity, the conventional heatmaps pose an imbalance problem between positive and negative samples in the loss function.
Hence, the proposed voting mechanism aims to spread the loss evenly across the entire heatmaps.
Given the initial guess of keypoints obtained from the heatmaps, the method defines a local region centered on the joint position and operates the soft-argmax within the region to obtain refined keypoint coordinates.
This restricts the impact of background points, leading to more reliable optimization.
The training is facilitated by CNN-based RegNety320 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib56" title="">56</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T1.4.2" style="font-size:90%;">Method and preprocessing summary in <span class="ltx_text" id="S4.T1.4.2.1" style="color:#000000;">AssemblyHands</span>.<span class="ltx_text ltx_font_medium" id="S4.T1.4.2.2">
We summarize submitted methods in terms of learning methods, architecture, preprocessing, and multi-view fusion techniques. The tuple (views, phase) indicates the number of views used in either train or test time.
</span></span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.5" style="width:433.6pt;height:125.4pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-174.1pt,50.1pt) scale(0.554568968935714,0.554568968935714) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.5.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.5.1.1.1.2">Learning methods</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.5.1.1.1.3">Architecture</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.5.1.1.1.4">Preprocessing</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.5.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.1.1.5.1">
<tr class="ltx_tr" id="S4.T1.5.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.1.1.5.1.1.1">Multi-view fusion</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.1.1.5.1.2.1">(views, phase)</td>
</tr>
</table>
</th>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.5.1.2.2.1"><span class="ltx_text" id="S4.T1.5.1.2.2.1.1" style="color:#000000;">Base</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.5.1.2.2.2">2.5D heatmaps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib45" title="">45</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.5.1.2.2.3">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib29" title="">29</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.5.1.2.2.4">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.5.1.2.2.5">Simple average (4, test)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.5.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.3.1.1">
<span class="ltx_text" id="S4.T1.5.1.3.1.1.1" style="color:#000000;">JHands</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib85" title="">85</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.3.1.2">Regression</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.3.1.3">Hiera <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib60" title="">60</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.3.1.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.3.1.4.1">
<tr class="ltx_tr" id="S4.T1.5.1.3.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.3.1.4.1.1.1">Warp perspective,</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.3.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.3.1.4.1.2.1">color jitter, random mask</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.3.1.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.3.1.5.1">
<tr class="ltx_tr" id="S4.T1.5.1.3.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.3.1.5.1.1.1">Adaptive view selection</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.3.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.3.1.5.1.2.1">and average (2, test)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.4.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.4.2.1"><span class="ltx_text" id="S4.T1.5.1.4.2.1.1" style="color:#000000;">PICO-AI</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.4.2.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.4.2.2.1">
<tr class="ltx_tr" id="S4.T1.5.1.4.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.4.2.2.1.1.1">2.5D heatmaps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib45" title="">45</a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.4.2.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.4.2.2.1.2.1">Heatmap voting</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.4.2.3">RegNety320 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib56" title="">56</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.4.2.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.4.2.4.1">
<tr class="ltx_tr" id="S4.T1.5.1.4.2.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.4.2.4.1.1.1">Scale, rotate,</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.4.2.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.4.2.4.1.2.1">flip, tanslate</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.4.2.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.4.2.5.1">
<tr class="ltx_tr" id="S4.T1.5.1.4.2.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.4.2.5.1.1.1">Adaptive view selection</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.4.2.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.4.2.5.1.2.1">FTL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib58" title="">58</a>]</cite> (2, train)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.5.3.1"><span class="ltx_text" id="S4.T1.5.1.5.3.1.1" style="color:#000000;">FRDC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.5.3.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.5.3.2.1">
<tr class="ltx_tr" id="S4.T1.5.1.5.3.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.5.3.2.1.1.1">Regression</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.5.3.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.5.3.2.1.2.1">2D heatmaps</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.5.3.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.5.3.3.1">
<tr class="ltx_tr" id="S4.T1.5.1.5.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.5.3.3.1.1.1">HandOccNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib53" title="">53</a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.5.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.5.3.3.1.2.1">with ConvNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib41" title="">41</a>]</cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.5.3.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.5.3.4.1">
<tr class="ltx_tr" id="S4.T1.5.1.5.3.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.5.3.4.1.1.1">Scale, rotate,</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.5.3.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.5.3.4.1.2.1">color jitter</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.5.3.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.5.3.5.1">
<tr class="ltx_tr" id="S4.T1.5.1.5.3.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.5.3.5.1.1.1">Weighted average</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.5.3.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.5.3.5.1.2.1">(4, test)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.6.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.6.4.1"><span class="ltx_text" id="S4.T1.5.1.6.4.1.1" style="color:#000000;">Phi-AI</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.6.4.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.6.4.2.1">
<tr class="ltx_tr" id="S4.T1.5.1.6.4.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.6.4.2.1.1.1">2D heatmaps and</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.6.4.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.6.4.2.1.2.1">3D location maps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib82" title="">82</a>]</cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.6.4.3">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib29" title="">29</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.6.4.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.6.4.4.1">
<tr class="ltx_tr" id="S4.T1.5.1.6.4.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.6.4.4.1.1.1">Scale, rotate, translate,</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.6.4.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.6.4.4.1.2.1">color jitter, gaussian blur</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.6.4.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.6.4.5.1">
<tr class="ltx_tr" id="S4.T1.5.1.6.4.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.6.4.5.1.1.1">Weighted average</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.6.4.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.5.1.6.4.5.1.2.1">(4, test)</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1" style="color:#000000;">Phi-AI</span><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.2">:</span>
While following the heatmap-based approach, this method adapts MinimalHand <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib82" title="">82</a>]</cite> with the ResNet50 backbone, where 2D heatmaps and 3D location maps are regressed.
Instead of selecting 3D keypoint coordinates from the location maps,
the proposed method modifies it by using heatmap values to weight 3D keypoint coordinates, achieving a more robust estimation.
Moreover, the method adds a residual structured layer after the original three-tier cascade networks to refine the calculated location maps.
The method further applies the ensemble of final keypoint outputs combined with the <span class="ltx_text" id="S4.SS1.p5.1.3" style="color:#000000;">Base</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1" style="color:#000000;">FRDC</span><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.2">:</span>
This method adopts a hybrid approach by combining <span class="ltx_text ltx_font_italic" id="S4.SS1.p6.1.3">regression</span> with <span class="ltx_text ltx_font_italic" id="S4.SS1.p6.1.4">heatmap</span> for training.
HandOccNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib53" title="">53</a>]</cite> is modified to regress 3D keypoint coordinates and integrated with an additional branch of 2D heatmap regression.
HandOccNet enriches feature extraction with spatial attention mechanisms for occluded regions, making it robust under hand-object occlusions.
The method further utilizes a stronger ConvNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib41" title="">41</a>]</cite> backbone and feature fusion from the 2D keypoint regressor.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p7.1.1">Preprocessing of egocentric images:</span>
Compared to conventional static camera setups, egocentric images exhibit unique properties and biases, such as distortion, head camera motion, and different color representations.
Thus, it is vital to preprocess egocentric images to alleviate these effects during training.
Augmentation techniques are detailed in <span class="ltx_text" id="S4.SS1.p7.1.2" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S4.T1" title="Table 1 ‣ 4.1 AssemblyHands methods ‣ 4 Methods ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">1</span></a></span>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1">The method <span class="ltx_text" id="S4.SS1.p8.1.1" style="color:#000000;">JHands</span> addresses the distortion issue with a warp perspective operation to make the hands near the edge less stretched.
While AssemblyHands provides rectified images converted from fisheye cameras to a pinhole camera model, they often include excessively stretched areas near the edges.
To address this, the method calculates a virtual camera and corresponding perspective transformation matrix based on the pixel coordinates of the crop and the camera parameters.
The generated crops can be found in the analysis of <span class="ltx_text" id="S4.SS1.p8.1.2" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F3" title="Figure 3 ‣ 5.2 AssemblyHands analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">3</span></a></span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p9">
<p class="ltx_p" id="S4.SS1.p9.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p9.1.1">Multi-view fusion:</span>
Since AssemblyHands offers multi-view egocentric videos,
participants can optionally use the constraint of multi-view geometry and fusion techniques during training or inference.</p>
</div>
<div class="ltx_para" id="S4.SS1.p10">
<p class="ltx_p" id="S4.SS1.p10.1">While <span class="ltx_text" id="S4.SS1.p10.1.1" style="color:#000000;">Base</span> uses a simple average of predicted keypoints from all four camera views in the test time,
<span class="ltx_text" id="S4.SS1.p10.1.2" style="color:#000000;">PICO-AI</span> proposes multi-view feature fusion during training using Feature Transform Layers (FTL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib58" title="">58</a>]</cite>.
This FTL training requires fusing two out of four views; thus, the method chooses the most suitable views for every frame.
In cases with multiple candidates, the Intersection over Union (IoU) is computed between hand boxes from per-view predictions and 2D keypoints from previous 3D predictions. The two views with the highest IoUs are selected for their superior prediction reliability.</p>
</div>
<div class="ltx_para" id="S4.SS1.p11">
<p class="ltx_p" id="S4.SS1.p11.1">The methods <span class="ltx_text" id="S4.SS1.p11.1.1" style="color:#000000;">JHands</span>, <span class="ltx_text" id="S4.SS1.p11.1.2" style="color:#000000;">FRDC</span>, and <span class="ltx_text" id="S4.SS1.p11.1.3" style="color:#000000;">Phi-AI</span> apply adaptive fusion in predicted keypoints during testing.
The method <span class="ltx_text" id="S4.SS1.p11.1.4" style="color:#000000;">JHands</span> computes the MPJPE with each other view and selects two results of views with the lowest MPJPE, excluding noisy predictions in the average.
If the MPJPE is lower than a threshold, the mean of the two results is calculated as the final result.
Otherwise, the result with a lower PA-MPJPE with the predictions in the previous frame is chosen.
The methods <span class="ltx_text" id="S4.SS1.p11.1.5" style="color:#000000;">FRDC</span> and <span class="ltx_text" id="S4.SS1.p11.1.6" style="color:#000000;">Phi-AI</span> use a weighted average for each view prediction, assigning weights based on each view’s validation performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p12">
<p class="ltx_p" id="S4.SS1.p12.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p12.1.1">Postprocessing:</span>
Several postprocessing techniques, including test-time augmentation, smoothing, and model ensembling are used to enhance inference outcomes.
In particular, the method <span class="ltx_text" id="S4.SS1.p12.1.2" style="color:#000000;">JHands</span> applies an offline smooth (Savitzky-Golay) filter on each video sequence.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span class="ltx_text" id="S4.SS2.1.1" style="color:#000000;">ARCTIC</span> methods</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text" id="S4.SS2.p1.1.1" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S4.T2" title="Table 2 ‣ 4.2 ARCTIC methods ‣ 4 Methods ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">2</span></a></span> summarizes the details for each method in terms of the input image dimensions, image backbones, learning rate scheduling, the number of training epochs, batch size, and cropping strategies.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.17.3.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T2.4.2" style="font-size:90%;">Method and preprocessing summary in <span class="ltx_text" id="S4.T2.4.2.3" style="color:#000000;">ARCTIC</span>.<span class="ltx_text ltx_font_medium" id="S4.T2.4.2.2">
We summarize baselines on <span class="ltx_text" id="S4.T2.4.2.2.1" style="color:#000000;">ARCTIC</span> in terms of input dimensions, image backbones, learning rate scheduling, training epochs, batch size and the cropping used for input.
<sup class="ltx_sup" id="S4.T2.4.2.2.2">∗</sup>Method trains 50 epochs for decoder and 36 for backbone.
<sup class="ltx_sup" id="S4.T2.4.2.2.3">+</sup>Learning rate is 1e-7 to 1e-4 with linear warmup for first 5% step, and 1e-4 to 1e-7 with cosine decay for rest.
</span></span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.11" style="width:433.6pt;height:147pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-116.7pt,39.4pt) scale(0.650139683720331,0.650139683720331) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.11.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.11.7.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.11.7.8.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.11.7.8.1.2">Input size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.11.7.8.1.3">Backbones</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.11.7.8.1.4">Learning rate schedule</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.11.7.8.1.5">Training epochs</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.11.7.8.1.6">Batch size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.11.7.8.1.7">Cropping</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.1.1.2">ArcticNet-SF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib17" title="">17</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.1.1.1"><math alttext="224\times 224" class="ltx_Math" display="inline" id="S4.T2.5.1.1.1.m1.1"><semantics id="S4.T2.5.1.1.1.m1.1a"><mrow id="S4.T2.5.1.1.1.m1.1.1" xref="S4.T2.5.1.1.1.m1.1.1.cmml"><mn id="S4.T2.5.1.1.1.m1.1.1.2" xref="S4.T2.5.1.1.1.m1.1.1.2.cmml">224</mn><mo id="S4.T2.5.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.5.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.5.1.1.1.m1.1.1.3" xref="S4.T2.5.1.1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.1.1.1.m1.1b"><apply id="S4.T2.5.1.1.1.m1.1.1.cmml" xref="S4.T2.5.1.1.1.m1.1.1"><times id="S4.T2.5.1.1.1.m1.1.1.1.cmml" xref="S4.T2.5.1.1.1.m1.1.1.1"></times><cn id="S4.T2.5.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.5.1.1.1.m1.1.1.2">224</cn><cn id="S4.T2.5.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.5.1.1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.1.1.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.1.1.1.m1.1d">224 × 224</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.1.1.3">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.1.1.4">1e-5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.5.1.1.5.1">
<tr class="ltx_tr" id="S4.T2.5.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.5.1.1.5.1.1.1">allocentric: 20</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.5.1.1.5.1.2.1">egocentric: 50</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.1.1.6">64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.1.1.7">object</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.2.2">DIGIT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib16" title="">16</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.2.1"><math alttext="224\times 224" class="ltx_Math" display="inline" id="S4.T2.6.2.2.1.m1.1"><semantics id="S4.T2.6.2.2.1.m1.1a"><mrow id="S4.T2.6.2.2.1.m1.1.1" xref="S4.T2.6.2.2.1.m1.1.1.cmml"><mn id="S4.T2.6.2.2.1.m1.1.1.2" xref="S4.T2.6.2.2.1.m1.1.1.2.cmml">224</mn><mo id="S4.T2.6.2.2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.6.2.2.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.6.2.2.1.m1.1.1.3" xref="S4.T2.6.2.2.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.2.2.1.m1.1b"><apply id="S4.T2.6.2.2.1.m1.1.1.cmml" xref="S4.T2.6.2.2.1.m1.1.1"><times id="S4.T2.6.2.2.1.m1.1.1.1.cmml" xref="S4.T2.6.2.2.1.m1.1.1.1"></times><cn id="S4.T2.6.2.2.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.6.2.2.1.m1.1.1.2">224</cn><cn id="S4.T2.6.2.2.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.6.2.2.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.2.2.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.2.2.1.m1.1d">224 × 224</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.2.3">HRNet-W32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.2.4">1e-5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.2.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.6.2.2.5.1">
<tr class="ltx_tr" id="S4.T2.6.2.2.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.6.2.2.5.1.1.1">allocentric: 20</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.2.2.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.6.2.2.5.1.2.1">egocentric: 50</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.2.6">64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.2.7">object</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.3.3.2">AmbiguousHands</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.3.3.1"><math alttext="224\times 224" class="ltx_Math" display="inline" id="S4.T2.7.3.3.1.m1.1"><semantics id="S4.T2.7.3.3.1.m1.1a"><mrow id="S4.T2.7.3.3.1.m1.1.1" xref="S4.T2.7.3.3.1.m1.1.1.cmml"><mn id="S4.T2.7.3.3.1.m1.1.1.2" xref="S4.T2.7.3.3.1.m1.1.1.2.cmml">224</mn><mo id="S4.T2.7.3.3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.7.3.3.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.7.3.3.1.m1.1.1.3" xref="S4.T2.7.3.3.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.3.3.1.m1.1b"><apply id="S4.T2.7.3.3.1.m1.1.1.cmml" xref="S4.T2.7.3.3.1.m1.1.1"><times id="S4.T2.7.3.3.1.m1.1.1.1.cmml" xref="S4.T2.7.3.3.1.m1.1.1.1"></times><cn id="S4.T2.7.3.3.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.7.3.3.1.m1.1.1.2">224</cn><cn id="S4.T2.7.3.3.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.7.3.3.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.3.3.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.3.3.1.m1.1d">224 × 224</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.3.3.3">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.3.3.4">1e-5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.3.3.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.7.3.3.5.1">
<tr class="ltx_tr" id="S4.T2.7.3.3.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.7.3.3.5.1.1.1">allocentric: 20</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.3.3.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.7.3.3.5.1.2.1">egocentric: 100</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.3.3.6">32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.3.3.7">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.7.3.3.7.1">
<tr class="ltx_tr" id="S4.T2.7.3.3.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.7.3.3.7.1.1.1">hand</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.3.3.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.7.3.3.7.1.2.1">object</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.5.5.3"><span class="ltx_text" id="S4.T2.9.5.5.3.1" style="color:#000000;">UVHand</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.8.4.4.1"><math alttext="384\times 384" class="ltx_Math" display="inline" id="S4.T2.8.4.4.1.m1.1"><semantics id="S4.T2.8.4.4.1.m1.1a"><mrow id="S4.T2.8.4.4.1.m1.1.1" xref="S4.T2.8.4.4.1.m1.1.1.cmml"><mn id="S4.T2.8.4.4.1.m1.1.1.2" xref="S4.T2.8.4.4.1.m1.1.1.2.cmml">384</mn><mo id="S4.T2.8.4.4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.8.4.4.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.8.4.4.1.m1.1.1.3" xref="S4.T2.8.4.4.1.m1.1.1.3.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.4.4.1.m1.1b"><apply id="S4.T2.8.4.4.1.m1.1.1.cmml" xref="S4.T2.8.4.4.1.m1.1.1"><times id="S4.T2.8.4.4.1.m1.1.1.1.cmml" xref="S4.T2.8.4.4.1.m1.1.1.1"></times><cn id="S4.T2.8.4.4.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.8.4.4.1.m1.1.1.2">384</cn><cn id="S4.T2.8.4.4.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.8.4.4.1.m1.1.1.3">384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.4.4.1.m1.1c">384\times 384</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.4.4.1.m1.1d">384 × 384</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.5.5.4">Swin-L</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.5.5.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.5.5.1">
<tr class="ltx_tr" id="S4.T2.9.5.5.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.5.5.1.1.1">2e-4 (backbone)</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.5.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.5.5.1.2.1">1e-7 (others)</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.5.5.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.5.2.1">
<tr class="ltx_tr" id="S4.T2.9.5.5.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.5.2.1.2.1">allocentric: N/A</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.5.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.5.2.1.1.1">egocentric: 50/36<sup class="ltx_sup" id="S4.T2.9.5.5.2.1.1.1.1">∗</sup>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.5.5.6">48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.5.5.7">object</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.7.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.11.7.7.3">
<span class="ltx_text" id="S4.T2.11.7.7.3.1" style="color:#000000;">JointTransformer</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib1" title="">1</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.10.6.6.1"><math alttext="224\times 224" class="ltx_Math" display="inline" id="S4.T2.10.6.6.1.m1.1"><semantics id="S4.T2.10.6.6.1.m1.1a"><mrow id="S4.T2.10.6.6.1.m1.1.1" xref="S4.T2.10.6.6.1.m1.1.1.cmml"><mn id="S4.T2.10.6.6.1.m1.1.1.2" xref="S4.T2.10.6.6.1.m1.1.1.2.cmml">224</mn><mo id="S4.T2.10.6.6.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.10.6.6.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.10.6.6.1.m1.1.1.3" xref="S4.T2.10.6.6.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.6.6.1.m1.1b"><apply id="S4.T2.10.6.6.1.m1.1.1.cmml" xref="S4.T2.10.6.6.1.m1.1.1"><times id="S4.T2.10.6.6.1.m1.1.1.1.cmml" xref="S4.T2.10.6.6.1.m1.1.1.1"></times><cn id="S4.T2.10.6.6.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.10.6.6.1.m1.1.1.2">224</cn><cn id="S4.T2.10.6.6.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.10.6.6.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.6.6.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.6.6.1.m1.1d">224 × 224</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.11.7.7.4">ViT-G</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.11.7.7.2">1e-7/1e-5<sup class="ltx_sup" id="S4.T2.11.7.7.2.1">+</sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.11.7.7.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.11.7.7.5.1">
<tr class="ltx_tr" id="S4.T2.11.7.7.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.11.7.7.5.1.1.1">allocentric: 20</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.7.7.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.11.7.7.5.1.2.1">egocentric: 100</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.11.7.7.6">64</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.11.7.7.7">object</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.8"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.8.1">Preliminary:</span>
All methods below are regression-based and predict two-hand <span class="ltx_text" id="S4.SS2.p2.8.2">MANO</span>  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib59" title="">59</a>]</cite> parameters <math alttext="\mathbf{\Theta}=\{\mathbf{\theta},\mathbf{\beta}\}" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.2"><semantics id="S4.SS2.p2.1.m1.2a"><mrow id="S4.SS2.p2.1.m1.2.3" xref="S4.SS2.p2.1.m1.2.3.cmml"><mi id="S4.SS2.p2.1.m1.2.3.2" xref="S4.SS2.p2.1.m1.2.3.2.cmml">𝚯</mi><mo id="S4.SS2.p2.1.m1.2.3.1" xref="S4.SS2.p2.1.m1.2.3.1.cmml">=</mo><mrow id="S4.SS2.p2.1.m1.2.3.3.2" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml"><mo id="S4.SS2.p2.1.m1.2.3.3.2.1" stretchy="false" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml">{</mo><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">θ</mi><mo id="S4.SS2.p2.1.m1.2.3.3.2.2" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml">,</mo><mi id="S4.SS2.p2.1.m1.2.2" xref="S4.SS2.p2.1.m1.2.2.cmml">β</mi><mo id="S4.SS2.p2.1.m1.2.3.3.2.3" stretchy="false" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.2b"><apply id="S4.SS2.p2.1.m1.2.3.cmml" xref="S4.SS2.p2.1.m1.2.3"><eq id="S4.SS2.p2.1.m1.2.3.1.cmml" xref="S4.SS2.p2.1.m1.2.3.1"></eq><ci id="S4.SS2.p2.1.m1.2.3.2.cmml" xref="S4.SS2.p2.1.m1.2.3.2">𝚯</ci><set id="S4.SS2.p2.1.m1.2.3.3.1.cmml" xref="S4.SS2.p2.1.m1.2.3.3.2"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝜃</ci><ci id="S4.SS2.p2.1.m1.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2">𝛽</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.2c">\mathbf{\Theta}=\{\mathbf{\theta},\mathbf{\beta}\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.2d">bold_Θ = { italic_θ , italic_β }</annotation></semantics></math> and articulated object parameters <math alttext="\mathbf{\Omega}" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">𝛀</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝛀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\mathbf{\Omega}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">bold_Ω</annotation></semantics></math>.
In particular, with the MANO pose and shape parameters <math alttext="\mathbf{\theta},\mathbf{\beta}" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.2"><semantics id="S4.SS2.p2.3.m3.2a"><mrow id="S4.SS2.p2.3.m3.2.3.2" xref="S4.SS2.p2.3.m3.2.3.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">θ</mi><mo id="S4.SS2.p2.3.m3.2.3.2.1" xref="S4.SS2.p2.3.m3.2.3.1.cmml">,</mo><mi id="S4.SS2.p2.3.m3.2.2" xref="S4.SS2.p2.3.m3.2.2.cmml">β</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.2b"><list id="S4.SS2.p2.3.m3.2.3.1.cmml" xref="S4.SS2.p2.3.m3.2.3.2"><ci id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">𝜃</ci><ci id="S4.SS2.p2.3.m3.2.2.cmml" xref="S4.SS2.p2.3.m3.2.2">𝛽</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.2c">\mathbf{\theta},\mathbf{\beta}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.2d">italic_θ , italic_β</annotation></semantics></math>, the <span class="ltx_text" id="S4.SS2.p2.8.3">MANO</span> model <math alttext="\mathcal{H}" class="ltx_Math" display="inline" id="S4.SS2.p2.4.m4.1"><semantics id="S4.SS2.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">ℋ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">ℋ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\mathcal{H}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.4.m4.1d">caligraphic_H</annotation></semantics></math> returns a mesh with vertices via <math alttext="\mathcal{H}(\mathbf{\theta},\mathbf{\beta})\in\rm I\!R^{778\times 3}" class="ltx_Math" display="inline" id="S4.SS2.p2.5.m5.2"><semantics id="S4.SS2.p2.5.m5.2a"><mrow id="S4.SS2.p2.5.m5.2.3" xref="S4.SS2.p2.5.m5.2.3.cmml"><mrow id="S4.SS2.p2.5.m5.2.3.2" xref="S4.SS2.p2.5.m5.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.5.m5.2.3.2.2" xref="S4.SS2.p2.5.m5.2.3.2.2.cmml">ℋ</mi><mo id="S4.SS2.p2.5.m5.2.3.2.1" xref="S4.SS2.p2.5.m5.2.3.2.1.cmml">⁢</mo><mrow id="S4.SS2.p2.5.m5.2.3.2.3.2" xref="S4.SS2.p2.5.m5.2.3.2.3.1.cmml"><mo id="S4.SS2.p2.5.m5.2.3.2.3.2.1" stretchy="false" xref="S4.SS2.p2.5.m5.2.3.2.3.1.cmml">(</mo><mi id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">θ</mi><mo id="S4.SS2.p2.5.m5.2.3.2.3.2.2" xref="S4.SS2.p2.5.m5.2.3.2.3.1.cmml">,</mo><mi id="S4.SS2.p2.5.m5.2.2" xref="S4.SS2.p2.5.m5.2.2.cmml">β</mi><mo id="S4.SS2.p2.5.m5.2.3.2.3.2.3" stretchy="false" xref="S4.SS2.p2.5.m5.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p2.5.m5.2.3.1" xref="S4.SS2.p2.5.m5.2.3.1.cmml">∈</mo><mrow id="S4.SS2.p2.5.m5.2.3.3" xref="S4.SS2.p2.5.m5.2.3.3.cmml"><mpadded width="0.348em"><mi id="S4.SS2.p2.5.m5.2.3.3.2" mathvariant="normal" xref="S4.SS2.p2.5.m5.2.3.3.2.cmml">I</mi></mpadded><mo id="S4.SS2.p2.5.m5.2.3.3.1" xref="S4.SS2.p2.5.m5.2.3.3.1.cmml">⁢</mo><msup id="S4.SS2.p2.5.m5.2.3.3.3" xref="S4.SS2.p2.5.m5.2.3.3.3.cmml"><mi id="S4.SS2.p2.5.m5.2.3.3.3.2" mathvariant="normal" xref="S4.SS2.p2.5.m5.2.3.3.3.2.cmml">R</mi><mrow id="S4.SS2.p2.5.m5.2.3.3.3.3" xref="S4.SS2.p2.5.m5.2.3.3.3.3.cmml"><mn id="S4.SS2.p2.5.m5.2.3.3.3.3.2" xref="S4.SS2.p2.5.m5.2.3.3.3.3.2.cmml">778</mn><mo id="S4.SS2.p2.5.m5.2.3.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p2.5.m5.2.3.3.3.3.1.cmml">×</mo><mn id="S4.SS2.p2.5.m5.2.3.3.3.3.3" xref="S4.SS2.p2.5.m5.2.3.3.3.3.3.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.2b"><apply id="S4.SS2.p2.5.m5.2.3.cmml" xref="S4.SS2.p2.5.m5.2.3"><in id="S4.SS2.p2.5.m5.2.3.1.cmml" xref="S4.SS2.p2.5.m5.2.3.1"></in><apply id="S4.SS2.p2.5.m5.2.3.2.cmml" xref="S4.SS2.p2.5.m5.2.3.2"><times id="S4.SS2.p2.5.m5.2.3.2.1.cmml" xref="S4.SS2.p2.5.m5.2.3.2.1"></times><ci id="S4.SS2.p2.5.m5.2.3.2.2.cmml" xref="S4.SS2.p2.5.m5.2.3.2.2">ℋ</ci><interval closure="open" id="S4.SS2.p2.5.m5.2.3.2.3.1.cmml" xref="S4.SS2.p2.5.m5.2.3.2.3.2"><ci id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">𝜃</ci><ci id="S4.SS2.p2.5.m5.2.2.cmml" xref="S4.SS2.p2.5.m5.2.2">𝛽</ci></interval></apply><apply id="S4.SS2.p2.5.m5.2.3.3.cmml" xref="S4.SS2.p2.5.m5.2.3.3"><times id="S4.SS2.p2.5.m5.2.3.3.1.cmml" xref="S4.SS2.p2.5.m5.2.3.3.1"></times><ci id="S4.SS2.p2.5.m5.2.3.3.2.cmml" xref="S4.SS2.p2.5.m5.2.3.3.2">I</ci><apply id="S4.SS2.p2.5.m5.2.3.3.3.cmml" xref="S4.SS2.p2.5.m5.2.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.2.3.3.3.1.cmml" xref="S4.SS2.p2.5.m5.2.3.3.3">superscript</csymbol><ci id="S4.SS2.p2.5.m5.2.3.3.3.2.cmml" xref="S4.SS2.p2.5.m5.2.3.3.3.2">R</ci><apply id="S4.SS2.p2.5.m5.2.3.3.3.3.cmml" xref="S4.SS2.p2.5.m5.2.3.3.3.3"><times id="S4.SS2.p2.5.m5.2.3.3.3.3.1.cmml" xref="S4.SS2.p2.5.m5.2.3.3.3.3.1"></times><cn id="S4.SS2.p2.5.m5.2.3.3.3.3.2.cmml" type="integer" xref="S4.SS2.p2.5.m5.2.3.3.3.3.2">778</cn><cn id="S4.SS2.p2.5.m5.2.3.3.3.3.3.cmml" type="integer" xref="S4.SS2.p2.5.m5.2.3.3.3.3.3">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.2c">\mathcal{H}(\mathbf{\theta},\mathbf{\beta})\in\rm I\!R^{778\times 3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.5.m5.2d">caligraphic_H ( italic_θ , italic_β ) ∈ roman_I roman_R start_POSTSUPERSCRIPT 778 × 3 end_POSTSUPERSCRIPT</annotation></semantics></math>.
3D joints are obtained via a linear regressor.
The articulated object model <math alttext="\mathcal{O}" class="ltx_Math" display="inline" id="S4.SS2.p2.6.m6.1"><semantics id="S4.SS2.p2.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml">𝒪</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><ci id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">𝒪</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">\mathcal{O}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.6.m6.1d">caligraphic_O</annotation></semantics></math> was introduced in <span class="ltx_text" id="S4.SS2.p2.8.4" style="color:#000000;">ARCTIC</span> to provide an articulated mesh with vertices via <math alttext="\mathcal{O}(\mathbf{\Omega})\in\rm I\!R^{V\times 3}" class="ltx_Math" display="inline" id="S4.SS2.p2.7.m7.1"><semantics id="S4.SS2.p2.7.m7.1a"><mrow id="S4.SS2.p2.7.m7.1.2" xref="S4.SS2.p2.7.m7.1.2.cmml"><mrow id="S4.SS2.p2.7.m7.1.2.2" xref="S4.SS2.p2.7.m7.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.7.m7.1.2.2.2" xref="S4.SS2.p2.7.m7.1.2.2.2.cmml">𝒪</mi><mo id="S4.SS2.p2.7.m7.1.2.2.1" xref="S4.SS2.p2.7.m7.1.2.2.1.cmml">⁢</mo><mrow id="S4.SS2.p2.7.m7.1.2.2.3.2" xref="S4.SS2.p2.7.m7.1.2.2.cmml"><mo id="S4.SS2.p2.7.m7.1.2.2.3.2.1" stretchy="false" xref="S4.SS2.p2.7.m7.1.2.2.cmml">(</mo><mi id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">𝛀</mi><mo id="S4.SS2.p2.7.m7.1.2.2.3.2.2" stretchy="false" xref="S4.SS2.p2.7.m7.1.2.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p2.7.m7.1.2.1" xref="S4.SS2.p2.7.m7.1.2.1.cmml">∈</mo><mrow id="S4.SS2.p2.7.m7.1.2.3" xref="S4.SS2.p2.7.m7.1.2.3.cmml"><mpadded width="0.348em"><mi id="S4.SS2.p2.7.m7.1.2.3.2" mathvariant="normal" xref="S4.SS2.p2.7.m7.1.2.3.2.cmml">I</mi></mpadded><mo id="S4.SS2.p2.7.m7.1.2.3.1" xref="S4.SS2.p2.7.m7.1.2.3.1.cmml">⁢</mo><msup id="S4.SS2.p2.7.m7.1.2.3.3" xref="S4.SS2.p2.7.m7.1.2.3.3.cmml"><mi id="S4.SS2.p2.7.m7.1.2.3.3.2" mathvariant="normal" xref="S4.SS2.p2.7.m7.1.2.3.3.2.cmml">R</mi><mrow id="S4.SS2.p2.7.m7.1.2.3.3.3" xref="S4.SS2.p2.7.m7.1.2.3.3.3.cmml"><mi id="S4.SS2.p2.7.m7.1.2.3.3.3.2" mathvariant="normal" xref="S4.SS2.p2.7.m7.1.2.3.3.3.2.cmml">V</mi><mo id="S4.SS2.p2.7.m7.1.2.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p2.7.m7.1.2.3.3.3.1.cmml">×</mo><mn id="S4.SS2.p2.7.m7.1.2.3.3.3.3" xref="S4.SS2.p2.7.m7.1.2.3.3.3.3.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><apply id="S4.SS2.p2.7.m7.1.2.cmml" xref="S4.SS2.p2.7.m7.1.2"><in id="S4.SS2.p2.7.m7.1.2.1.cmml" xref="S4.SS2.p2.7.m7.1.2.1"></in><apply id="S4.SS2.p2.7.m7.1.2.2.cmml" xref="S4.SS2.p2.7.m7.1.2.2"><times id="S4.SS2.p2.7.m7.1.2.2.1.cmml" xref="S4.SS2.p2.7.m7.1.2.2.1"></times><ci id="S4.SS2.p2.7.m7.1.2.2.2.cmml" xref="S4.SS2.p2.7.m7.1.2.2.2">𝒪</ci><ci id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">𝛀</ci></apply><apply id="S4.SS2.p2.7.m7.1.2.3.cmml" xref="S4.SS2.p2.7.m7.1.2.3"><times id="S4.SS2.p2.7.m7.1.2.3.1.cmml" xref="S4.SS2.p2.7.m7.1.2.3.1"></times><ci id="S4.SS2.p2.7.m7.1.2.3.2.cmml" xref="S4.SS2.p2.7.m7.1.2.3.2">I</ci><apply id="S4.SS2.p2.7.m7.1.2.3.3.cmml" xref="S4.SS2.p2.7.m7.1.2.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.7.m7.1.2.3.3.1.cmml" xref="S4.SS2.p2.7.m7.1.2.3.3">superscript</csymbol><ci id="S4.SS2.p2.7.m7.1.2.3.3.2.cmml" xref="S4.SS2.p2.7.m7.1.2.3.3.2">R</ci><apply id="S4.SS2.p2.7.m7.1.2.3.3.3.cmml" xref="S4.SS2.p2.7.m7.1.2.3.3.3"><times id="S4.SS2.p2.7.m7.1.2.3.3.3.1.cmml" xref="S4.SS2.p2.7.m7.1.2.3.3.3.1"></times><ci id="S4.SS2.p2.7.m7.1.2.3.3.3.2.cmml" xref="S4.SS2.p2.7.m7.1.2.3.3.3.2">V</ci><cn id="S4.SS2.p2.7.m7.1.2.3.3.3.3.cmml" type="integer" xref="S4.SS2.p2.7.m7.1.2.3.3.3.3">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">\mathcal{O}(\mathbf{\Omega})\in\rm I\!R^{V\times 3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.7.m7.1d">caligraphic_O ( bold_Ω ) ∈ roman_I roman_R start_POSTSUPERSCRIPT roman_V × 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="\mathbf{\Omega}\in\rm I\!R^{7}" class="ltx_Math" display="inline" id="S4.SS2.p2.8.m8.1"><semantics id="S4.SS2.p2.8.m8.1a"><mrow id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml"><mi id="S4.SS2.p2.8.m8.1.1.2" xref="S4.SS2.p2.8.m8.1.1.2.cmml">𝛀</mi><mo id="S4.SS2.p2.8.m8.1.1.1" xref="S4.SS2.p2.8.m8.1.1.1.cmml">∈</mo><mrow id="S4.SS2.p2.8.m8.1.1.3" xref="S4.SS2.p2.8.m8.1.1.3.cmml"><mpadded width="0.348em"><mi id="S4.SS2.p2.8.m8.1.1.3.2" mathvariant="normal" xref="S4.SS2.p2.8.m8.1.1.3.2.cmml">I</mi></mpadded><mo id="S4.SS2.p2.8.m8.1.1.3.1" xref="S4.SS2.p2.8.m8.1.1.3.1.cmml">⁢</mo><msup id="S4.SS2.p2.8.m8.1.1.3.3" xref="S4.SS2.p2.8.m8.1.1.3.3.cmml"><mi id="S4.SS2.p2.8.m8.1.1.3.3.2" mathvariant="normal" xref="S4.SS2.p2.8.m8.1.1.3.3.2.cmml">R</mi><mn id="S4.SS2.p2.8.m8.1.1.3.3.3" xref="S4.SS2.p2.8.m8.1.1.3.3.3.cmml">7</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.1b"><apply id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1"><in id="S4.SS2.p2.8.m8.1.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1.1"></in><ci id="S4.SS2.p2.8.m8.1.1.2.cmml" xref="S4.SS2.p2.8.m8.1.1.2">𝛀</ci><apply id="S4.SS2.p2.8.m8.1.1.3.cmml" xref="S4.SS2.p2.8.m8.1.1.3"><times id="S4.SS2.p2.8.m8.1.1.3.1.cmml" xref="S4.SS2.p2.8.m8.1.1.3.1"></times><ci id="S4.SS2.p2.8.m8.1.1.3.2.cmml" xref="S4.SS2.p2.8.m8.1.1.3.2">I</ci><apply id="S4.SS2.p2.8.m8.1.1.3.3.cmml" xref="S4.SS2.p2.8.m8.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.8.m8.1.1.3.3.1.cmml" xref="S4.SS2.p2.8.m8.1.1.3.3">superscript</csymbol><ci id="S4.SS2.p2.8.m8.1.1.3.3.2.cmml" xref="S4.SS2.p2.8.m8.1.1.3.3.2">R</ci><cn id="S4.SS2.p2.8.m8.1.1.3.3.3.cmml" type="integer" xref="S4.SS2.p2.8.m8.1.1.3.3.3">7</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.1c">\mathbf{\Omega}\in\rm I\!R^{7}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.8.m8.1d">bold_Ω ∈ roman_I roman_R start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT</annotation></semantics></math> contains the global orientation, global translation, and object articulation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.2"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.2.1" style="color:#000000;">ArcticNet-SF</span><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.2.2"> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib17" title="">17</a>]</cite>:</span> Introduced in <span class="ltx_text" id="S4.SS2.p3.2.3" style="color:#000000;">ARCTIC</span>, it is a single-frame baseline. It first extracts an image feature vector from the input image; then, it regresses hand and object parameters with simple MLPs.
The hand and object meshes can then be extracted via <math alttext="\mathcal{H}(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.2" xref="S4.SS2.p3.1.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.1.m1.1.2.2" xref="S4.SS2.p3.1.m1.1.2.2.cmml">ℋ</mi><mo id="S4.SS2.p3.1.m1.1.2.1" xref="S4.SS2.p3.1.m1.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.p3.1.m1.1.2.3.2" xref="S4.SS2.p3.1.m1.1.2.cmml"><mo id="S4.SS2.p3.1.m1.1.2.3.2.1" stretchy="false" xref="S4.SS2.p3.1.m1.1.2.cmml">(</mo><mo id="S4.SS2.p3.1.m1.1.1" lspace="0em" rspace="0em" xref="S4.SS2.p3.1.m1.1.1.cmml">⋅</mo><mo id="S4.SS2.p3.1.m1.1.2.3.2.2" stretchy="false" xref="S4.SS2.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.2"><times id="S4.SS2.p3.1.m1.1.2.1.cmml" xref="S4.SS2.p3.1.m1.1.2.1"></times><ci id="S4.SS2.p3.1.m1.1.2.2.cmml" xref="S4.SS2.p3.1.m1.1.2.2">ℋ</ci><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathcal{H}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">caligraphic_H ( ⋅ )</annotation></semantics></math> and <math alttext="\mathcal{O}(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.2" xref="S4.SS2.p3.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.2.m2.1.2.2" xref="S4.SS2.p3.2.m2.1.2.2.cmml">𝒪</mi><mo id="S4.SS2.p3.2.m2.1.2.1" xref="S4.SS2.p3.2.m2.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.p3.2.m2.1.2.3.2" xref="S4.SS2.p3.2.m2.1.2.cmml"><mo id="S4.SS2.p3.2.m2.1.2.3.2.1" stretchy="false" xref="S4.SS2.p3.2.m2.1.2.cmml">(</mo><mo id="S4.SS2.p3.2.m2.1.1" lspace="0em" rspace="0em" xref="S4.SS2.p3.2.m2.1.1.cmml">⋅</mo><mo id="S4.SS2.p3.2.m2.1.2.3.2.2" stretchy="false" xref="S4.SS2.p3.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.2.cmml" xref="S4.SS2.p3.2.m2.1.2"><times id="S4.SS2.p3.2.m2.1.2.1.cmml" xref="S4.SS2.p3.2.m2.1.2.1"></times><ci id="S4.SS2.p3.2.m2.1.2.2.cmml" xref="S4.SS2.p3.2.m2.1.2.2">𝒪</ci><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\mathcal{O}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">caligraphic_O ( ⋅ )</annotation></semantics></math>.
For more details, see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1" style="color:#000000;">JointTransformer</span><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.2"> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib1" title="">1</a>]</cite>:</span>
<span class="ltx_text" id="S4.SS2.p4.1.3" style="color:#000000;">JointTransformer</span> enhances <span class="ltx_text" id="S4.SS2.p4.1.4" style="color:#000000;">ArcticNet-SF</span> by integrating a transformer decoder instead of the MLP regressors for hand and object parameter estimation.
The decoder employs learned queries for the angle of each joint, the shape and translation of each hand, and the translation, rotation, and articulation of the object.
It alternates between self-attention between queries and cross-attention of queries to the elements of the backbone feature map, followed by linear layers that regress the final parameters.
Specifically, separate linear layers are dedicated to regressing joint angles, hand shape, hand translation, object translation, object orientation, and object articulation.
The best model uses a ViT-G <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib78" title="">78</a>]</cite> backbone with frozen DINOv2 weights <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib52" title="">52</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1" style="color:#000000;">AmbiguousHands</span><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.2"> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib55" title="">55</a>]</cite>:</span> The method addresses scale ambiguity, resulting from bounding box cropping in data augmentation and camera intrinsics, by employing positional encoding of these elements to mitigate scale issues. This leads to improved spatial alignment. Subsequently, the approach enhances network visibility by integrating local features through distinct hand and object crops. They follow the general approach of <span class="ltx_text" id="S4.SS2.p5.1.3" style="color:#000000;">ArcticNet-SF</span> to regress hand/object parameters.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p6.1.1" style="color:#000000;">UVHand</span><span class="ltx_text ltx_font_bold" id="S4.SS2.p6.1.2">:</span> Since <span class="ltx_text" id="S4.SS2.p6.1.3" style="color:#000000;">ArcticNet-SF</span> only leverages a global feature vector to estimate hand and object parameters, the image features lack local context. To address this, <span class="ltx_text" id="S4.SS2.p6.1.4" style="color:#000000;">UVHand</span> leverages Swin-L transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib40" title="">40</a>]</cite> to extract image features. They then further leverage Deformable DETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib86" title="">86</a>]</cite> to encode the multiple-scale feature maps. The encoded feature maps are then aggregated via self- and cross-attention before regresssing hand and object parameters.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p7.1.1" style="color:#000000;">DIGIT</span><span class="ltx_text ltx_font_bold" id="S4.SS2.p7.1.2"> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib16" title="">16</a>]</cite>:</span> The method was introduced to estimate strongly interacting hands in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib16" title="">16</a>]</cite>. Since <span class="ltx_text" id="S4.SS2.p7.1.3" style="color:#000000;">ArcticNet-SF</span> is sensitive when the hands are interacting with objects, <span class="ltx_text" id="S4.SS2.p7.1.4" style="color:#000000;">DIGIT</span> was extended to the <span class="ltx_text" id="S4.SS2.p7.1.5" style="color:#000000;">ARCTIC</span> setting.
Given an image, it first estimates hand part-wise segmentation masks and object masks.
The mask predictions are fused with the image features to perform parameter estimation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p8.1.1">Implementation details:</span>
<span class="ltx_text" id="S4.SS2.p8.1.2" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S4.T2" title="Table 2 ‣ 4.2 ARCTIC methods ‣ 4 Methods ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">2</span></a></span> shows that all methods use the default cropping as in <span class="ltx_text" id="S4.SS2.p8.1.3" style="color:#000000;">ARCTIC</span> to crop around the object, while <span class="ltx_text" id="S4.SS2.p8.1.4" style="color:#000000;">AmbiguousHands</span> performs three crops (around two hands and the object).
<span class="ltx_text" id="S4.SS2.p8.1.5" style="color:#000000;">DIGIT</span> uses the HRNet-W32 backbone <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib67" title="">67</a>]</cite> and trains with a batch size of 64 with the same learning rate for all iterations.
<span class="ltx_text" id="S4.SS2.p8.1.6" style="color:#000000;">UVHand</span> takes as input a <math alttext="384\times 384" class="ltx_Math" display="inline" id="S4.SS2.p8.1.m1.1"><semantics id="S4.SS2.p8.1.m1.1a"><mrow id="S4.SS2.p8.1.m1.1.1" xref="S4.SS2.p8.1.m1.1.1.cmml"><mn id="S4.SS2.p8.1.m1.1.1.2" xref="S4.SS2.p8.1.m1.1.1.2.cmml">384</mn><mo id="S4.SS2.p8.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p8.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p8.1.m1.1.1.3" xref="S4.SS2.p8.1.m1.1.1.3.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.1.m1.1b"><apply id="S4.SS2.p8.1.m1.1.1.cmml" xref="S4.SS2.p8.1.m1.1.1"><times id="S4.SS2.p8.1.m1.1.1.1.cmml" xref="S4.SS2.p8.1.m1.1.1.1"></times><cn id="S4.SS2.p8.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.p8.1.m1.1.1.2">384</cn><cn id="S4.SS2.p8.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.p8.1.m1.1.1.3">384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.1.m1.1c">384\times 384</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.1.m1.1d">384 × 384</annotation></semantics></math> image cropped around the object and encodes it with the Swin-L transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib40" title="">40</a>]</cite> backbone. It was trained with a batch size of 48 with a learning rate of 2e-4 for the backbone and 1e-7 for other weights.
Due to computational cost, they train 50 epochs for the decoder and 36 for the backbone.
<span class="ltx_text" id="S4.SS2.p8.1.7" style="color:#000000;">JointTransformer</span> uses ViT-G <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib78" title="">78</a>]</cite> backbone with frozen DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib52" title="">52</a>]</cite> weights to train with a batch size of 64. It performs a linear warmup from 1e-7 to 1e-4 in the first 5% steps and uses cosine decay from 1e-4 to 1e-7 for the rest of the steps.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and analysis</h2>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.4.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T3.5.2" style="font-size:90%;">Method performance in <span class="ltx_text" id="S5.T3.5.2.1" style="color:#000000;">AssemblyHands</span>.<span class="ltx_text ltx_font_medium" id="S5.T3.5.2.2">
We compare <span class="ltx_text" id="S5.T3.5.2.2.1" style="color:#000000;">AssemblyHands</span> method performance on egocentric test data.
We show the final MPJPE on the test set as the metrics (lower better).
We also provide detailed evaluations, regarding the varying distances of hand position from the image center and different verb action categories.
The hand distance is computed by the distance from the image center to the hand center position per image, and averaged over the lower two views of the headset.
Verb classes of “attempt to X” are merged to “X” for simplicity.
The higher and lower three verbs are color-coded in red and blue, respectively.
</span></span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T3.6" style="width:433.6pt;height:225.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-71.3pt,37.0pt) scale(0.75248493532453,0.75248493532453) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.6.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.6.1.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.6.1.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3" id="S5.T3.6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.6.1.1.1.3.1">Hand distance (px)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" colspan="7" id="S5.T3.6.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.6.1.1.1.4.1">Verb class</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S5.T3.6.1.2.2.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.6.1.2.2.2">Score</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.2.2.3">-200</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.2.2.4">200-250</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.6.1.2.2.5">250-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.2.2.6">clap</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.2.2.7">inspect</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.2.2.8">pass</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.2.2.9">pick up</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.2.2.10">position</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.2.2.11">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.6.1.2.2.11.1">
<tr class="ltx_tr" id="S5.T3.6.1.2.2.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.6.1.2.2.11.1.1.1">position</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.2.2.11.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.6.1.2.2.11.1.2.1">screw on</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr" id="S5.T3.6.1.2.2.12">pull</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.6.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.6.1.3.1.1"><span class="ltx_text" id="S5.T3.6.1.3.1.1.1" style="color:#000000;">Base</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.1.3.1.2">20.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.3.1.3">20.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.3.1.4">21.97</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.1.3.1.5">24.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.3.1.6">19.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.3.1.7" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.3.1.7.1" style="background-color:#F4CCCC;">22.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.3.1.8">22.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.3.1.9">21.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.3.1.10">22.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.3.1.11">21.62</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S5.T3.6.1.3.1.12">18.74</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.4.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.6.1.4.2.1"><span class="ltx_text" id="S5.T3.6.1.4.2.1.1" style="color:#000000;">JHands</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.1.4.2.2">12.21</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.4.2.3">12.35</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.4.2.4">11.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.1.4.2.5">13.72</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.4.2.6" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.4.2.6.1" style="background-color:#C9DAF8;">10.65</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.4.2.7" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.4.2.7.1" style="background-color:#F4CCCC;">16.27</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.4.2.8">12.86</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.4.2.9">13.67</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.4.2.10" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.4.2.10.1" style="background-color:#F4CCCC;">14.58</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.4.2.11">12.8</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T3.6.1.4.2.12">11.06</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.5.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.6.1.5.3.1"><span class="ltx_text" id="S5.T3.6.1.5.3.1.1" style="color:#000000;">PICO-AI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.1.5.3.2">12.46</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.5.3.3">12.51</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.5.3.4">11.62</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.1.5.3.5">12.95</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.5.3.6">12.98</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.5.3.7" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.5.3.7.1" style="background-color:#F4CCCC;">15.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.5.3.8" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.5.3.8.1" style="background-color:#C9DAF8;">11.37</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.5.3.9">13.2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.5.3.10">13.18</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.5.3.11">11.39</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T3.6.1.5.3.12">15.13</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.6.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.6.1.6.4.1"><span class="ltx_text" id="S5.T3.6.1.6.4.1.1" style="color:#000000;">FRDC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.1.6.4.2">16.48</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.6.4.3">16.39</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.6.4.4">15.89</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.1.6.4.5">18.69</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.6.4.6">15.24</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.6.4.7" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.6.4.7.1" style="background-color:#F4CCCC;">21.33</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.6.4.8">17.86</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.6.4.9">18.26</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.6.4.10" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.6.4.10.1" style="background-color:#F4CCCC;">19.21</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.6.4.11">18.03</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T3.6.1.6.4.12" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.6.4.12.1" style="background-color:#C9DAF8;">12.83</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.7.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.6.1.7.5.1"><span class="ltx_text" id="S5.T3.6.1.7.5.1.1" style="color:#000000;">Phi-AI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.1.7.5.2">17.26</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.7.5.3">17.24</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.7.5.4">15.81</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.1.7.5.5">19.51</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.7.5.6" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.7.5.6.1" style="background-color:#F4CCCC;">19.86</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.7.5.7" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.7.5.7.1" style="background-color:#F4CCCC;">20.93</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.7.5.8">17.91</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.7.5.9">19.01</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.7.5.10">19.7</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.7.5.11">19.35</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T3.6.1.7.5.12">17.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.8.6">
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T3.6.1.8.6.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_ll ltx_border_t" colspan="11" id="S5.T3.6.1.8.6.2"><span class="ltx_text ltx_font_bold" id="S5.T3.6.1.8.6.2.1">Verb class (continue)</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.9.7">
<th class="ltx_td ltx_th ltx_th_column" id="S5.T3.6.1.9.7.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_ll" id="S5.T3.6.1.9.7.2">push</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.9.7.3">put down</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.9.7.4">remove</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.9.7.5">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.6.1.9.7.5.1">
<tr class="ltx_tr" id="S5.T3.6.1.9.7.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.6.1.9.7.5.1.1.1">remove</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.9.7.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.6.1.9.7.5.1.2.1">screw from</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.9.7.6">rotate</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.9.7.7">screw</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.9.7.8">tilt down</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.9.7.9">tilt up</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.9.7.10">unscrew</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.1.9.7.11">none</th>
<th class="ltx_td ltx_th ltx_th_column" id="S5.T3.6.1.9.7.12"></th>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.10.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.6.1.10.8.1"><span class="ltx_text" id="S5.T3.6.1.10.8.1.1" style="color:#000000;">Base</span></td>
<td class="ltx_td ltx_align_center ltx_border_ll ltx_border_t" id="S5.T3.6.1.10.8.2">19.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.10.8.3">20.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.10.8.4">19.99</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.10.8.5" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.10.8.5.1" style="background-color:#C9DAF8;">16.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.10.8.6" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.10.8.6.1" style="background-color:#F4CCCC;">22.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.10.8.7" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.10.8.7.1" style="background-color:#F4CCCC;">22.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.10.8.8" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.10.8.8.1" style="background-color:#C9DAF8;">13.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.10.8.9" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.10.8.9.1" style="background-color:#C9DAF8;">15.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.10.8.10">20.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.1.10.8.11">19.82</td>
<td class="ltx_td" id="S5.T3.6.1.10.8.12"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.11.9">
<td class="ltx_td ltx_align_left" id="S5.T3.6.1.11.9.1"><span class="ltx_text" id="S5.T3.6.1.11.9.1.1" style="color:#000000;">JHands</span></td>
<td class="ltx_td ltx_align_center ltx_border_ll" id="S5.T3.6.1.11.9.2">13.96</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.11.9.3">13.72</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.11.9.4">13.11</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.11.9.5">11.72</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.11.9.6">12.26</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.11.9.7" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.11.9.7.1" style="background-color:#F4CCCC;">14.11</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.11.9.8" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.11.9.8.1" style="background-color:#C9DAF8;">9.61</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.11.9.9" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.11.9.9.1" style="background-color:#C9DAF8;">9.92</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.11.9.10">12.25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.11.9.11">10.99</td>
<td class="ltx_td" id="S5.T3.6.1.11.9.12"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.12.10">
<td class="ltx_td ltx_align_left" id="S5.T3.6.1.12.10.1"><span class="ltx_text" id="S5.T3.6.1.12.10.1.1" style="color:#000000;">PICO-AI</span></td>
<td class="ltx_td ltx_align_center ltx_border_ll" id="S5.T3.6.1.12.10.2">12.29</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.12.10.3">13.41</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.12.10.4">12.83</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.12.10.5" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.12.10.5.1" style="background-color:#C9DAF8;">9.99</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.12.10.6" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.12.10.6.1" style="background-color:#F4CCCC;">13.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.12.10.7" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.12.10.7.1" style="background-color:#F4CCCC;">13.72</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.12.10.8" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.12.10.8.1" style="background-color:#C9DAF8;">10.44</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.12.10.9">11.56</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.12.10.10">12.87</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.12.10.11">11.71</td>
<td class="ltx_td" id="S5.T3.6.1.12.10.12"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.13.11">
<td class="ltx_td ltx_align_left" id="S5.T3.6.1.13.11.1"><span class="ltx_text" id="S5.T3.6.1.13.11.1.1" style="color:#000000;">FRDC</span></td>
<td class="ltx_td ltx_align_center ltx_border_ll" id="S5.T3.6.1.13.11.2" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.13.11.2.1" style="background-color:#F4CCCC;">19.52</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.13.11.3">17.87</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.13.11.4">18.55</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.13.11.5">14.47</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.13.11.6">16.44</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.13.11.7">18.81</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.13.11.8" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.13.11.8.1" style="background-color:#C9DAF8;">14.03</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.13.11.9" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.13.11.9.1" style="background-color:#C9DAF8;">13.37</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.13.11.10">16.41</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.1.13.11.11">15.01</td>
<td class="ltx_td" id="S5.T3.6.1.13.11.12"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.1.14.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.6.1.14.12.1"><span class="ltx_text" id="S5.T3.6.1.14.12.1.1" style="color:#000000;">Phi-AI</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_ll" id="S5.T3.6.1.14.12.2">19.12</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.1.14.12.3">18.29</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.1.14.12.4">18.18</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.1.14.12.5" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.14.12.5.1" style="background-color:#C9DAF8;">13.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.1.14.12.6">18.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.1.14.12.7" style="background-color:#F4CCCC;"><span class="ltx_text" id="S5.T3.6.1.14.12.7.1" style="background-color:#F4CCCC;">19.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.1.14.12.8" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.14.12.8.1" style="background-color:#C9DAF8;">13.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.1.14.12.9" style="background-color:#C9DAF8;"><span class="ltx_text" id="S5.T3.6.1.14.12.9.1" style="background-color:#C9DAF8;">15.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.1.14.12.10">17.29</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.1.14.12.11">15.8</td>
<td class="ltx_td ltx_border_bb" id="S5.T3.6.1.14.12.12"></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.25.4.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S5.T4.6.3" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S5.T4.6.3.1">Method performance in <span class="ltx_text" id="S5.T4.6.3.1.1" style="color:#000000;">ARCTIC</span></span>.
We compare performance in both allocentric (top half) and egocentric (bottom half) views.
We evaluate using metrics for contact and relative position (measuring hand-object contact and prediction of relative root position), motion (assessing temporally-consistent contact and smoothness), and hand and object metrics (indicating root-relative reconstruction error).
We use the CDev score as the main metric for this competition.
We denote left and right hands as <math alttext="l" class="ltx_Math" display="inline" id="S5.T4.4.1.m1.1"><semantics id="S5.T4.4.1.m1.1b"><mi id="S5.T4.4.1.m1.1.1" xref="S5.T4.4.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S5.T4.4.1.m1.1c"><ci id="S5.T4.4.1.m1.1.1.cmml" xref="S5.T4.4.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.1.m1.1d">l</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.1.m1.1e">italic_l</annotation></semantics></math> and <math alttext="r" class="ltx_Math" display="inline" id="S5.T4.5.2.m2.1"><semantics id="S5.T4.5.2.m2.1b"><mi id="S5.T4.5.2.m2.1.1" xref="S5.T4.5.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S5.T4.5.2.m2.1c"><ci id="S5.T4.5.2.m2.1.1.cmml" xref="S5.T4.5.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.2.m2.1d">r</annotation><annotation encoding="application/x-llamapun" id="S5.T4.5.2.m2.1e">italic_r</annotation></semantics></math>, and the object as <math alttext="o" class="ltx_Math" display="inline" id="S5.T4.6.3.m3.1"><semantics id="S5.T4.6.3.m3.1b"><mi id="S5.T4.6.3.m3.1.1" xref="S5.T4.6.3.m3.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S5.T4.6.3.m3.1c"><ci id="S5.T4.6.3.m3.1.1.cmml" xref="S5.T4.6.3.m3.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.3.m3.1d">o</annotation><annotation encoding="application/x-llamapun" id="S5.T4.6.3.m3.1e">italic_o</annotation></semantics></math>.
</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T4.22" style="width:433.6pt;height:143pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.2pt,36.8pt) scale(0.658897941179853,0.658897941179853) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.22.16">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.22.16.17.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S5.T4.22.16.17.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T4.22.16.17.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt" colspan="2" id="S5.T4.22.16.17.1.3">Contact and Relative Positions</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt" colspan="2" id="S5.T4.22.16.17.1.4">Motion</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T4.22.16.17.1.5">Hand</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.22.16.17.1.6">Object</th>
</tr>
<tr class="ltx_tr" id="S5.T4.22.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S5.T4.22.16.16.17">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.8.2.2.2">CDev [<math alttext="mm" class="ltx_Math" display="inline" id="S5.T4.7.1.1.1.m1.1"><semantics id="S5.T4.7.1.1.1.m1.1a"><mrow id="S5.T4.7.1.1.1.m1.1.1" xref="S5.T4.7.1.1.1.m1.1.1.cmml"><mi id="S5.T4.7.1.1.1.m1.1.1.2" xref="S5.T4.7.1.1.1.m1.1.1.2.cmml">m</mi><mo id="S5.T4.7.1.1.1.m1.1.1.1" xref="S5.T4.7.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.T4.7.1.1.1.m1.1.1.3" xref="S5.T4.7.1.1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.7.1.1.1.m1.1b"><apply id="S5.T4.7.1.1.1.m1.1.1.cmml" xref="S5.T4.7.1.1.1.m1.1.1"><times id="S5.T4.7.1.1.1.m1.1.1.1.cmml" xref="S5.T4.7.1.1.1.m1.1.1.1"></times><ci id="S5.T4.7.1.1.1.m1.1.1.2.cmml" xref="S5.T4.7.1.1.1.m1.1.1.2">𝑚</ci><ci id="S5.T4.7.1.1.1.m1.1.1.3.cmml" xref="S5.T4.7.1.1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.1.1.1.m1.1c">mm</annotation><annotation encoding="application/x-llamapun" id="S5.T4.7.1.1.1.m1.1d">italic_m italic_m</annotation></semantics></math>] <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.8.2.2.2.m2.1"><semantics id="S5.T4.8.2.2.2.m2.1a"><mo id="S5.T4.8.2.2.2.m2.1.1" stretchy="false" xref="S5.T4.8.2.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.8.2.2.2.m2.1b"><ci id="S5.T4.8.2.2.2.m2.1.1.cmml" xref="S5.T4.8.2.2.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.2.2.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.8.2.2.2.m2.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.11.5.5.5">MRRPE<sub class="ltx_sub" id="S5.T4.11.5.5.5.1"><span class="ltx_text ltx_font_italic" id="S5.T4.11.5.5.5.1.1">rl/ro</span></sub> [<math alttext="mm" class="ltx_Math" display="inline" id="S5.T4.10.4.4.4.m2.1"><semantics id="S5.T4.10.4.4.4.m2.1a"><mrow id="S5.T4.10.4.4.4.m2.1.1" xref="S5.T4.10.4.4.4.m2.1.1.cmml"><mi id="S5.T4.10.4.4.4.m2.1.1.2" xref="S5.T4.10.4.4.4.m2.1.1.2.cmml">m</mi><mo id="S5.T4.10.4.4.4.m2.1.1.1" xref="S5.T4.10.4.4.4.m2.1.1.1.cmml">⁢</mo><mi id="S5.T4.10.4.4.4.m2.1.1.3" xref="S5.T4.10.4.4.4.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.10.4.4.4.m2.1b"><apply id="S5.T4.10.4.4.4.m2.1.1.cmml" xref="S5.T4.10.4.4.4.m2.1.1"><times id="S5.T4.10.4.4.4.m2.1.1.1.cmml" xref="S5.T4.10.4.4.4.m2.1.1.1"></times><ci id="S5.T4.10.4.4.4.m2.1.1.2.cmml" xref="S5.T4.10.4.4.4.m2.1.1.2">𝑚</ci><ci id="S5.T4.10.4.4.4.m2.1.1.3.cmml" xref="S5.T4.10.4.4.4.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.10.4.4.4.m2.1c">mm</annotation><annotation encoding="application/x-llamapun" id="S5.T4.10.4.4.4.m2.1d">italic_m italic_m</annotation></semantics></math>] <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.11.5.5.5.m3.1"><semantics id="S5.T4.11.5.5.5.m3.1a"><mo id="S5.T4.11.5.5.5.m3.1.1" stretchy="false" xref="S5.T4.11.5.5.5.m3.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.11.5.5.5.m3.1b"><ci id="S5.T4.11.5.5.5.m3.1.1.cmml" xref="S5.T4.11.5.5.5.m3.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.11.5.5.5.m3.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.11.5.5.5.m3.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.13.7.7.7">MDev [<math alttext="mm" class="ltx_Math" display="inline" id="S5.T4.12.6.6.6.m1.1"><semantics id="S5.T4.12.6.6.6.m1.1a"><mrow id="S5.T4.12.6.6.6.m1.1.1" xref="S5.T4.12.6.6.6.m1.1.1.cmml"><mi id="S5.T4.12.6.6.6.m1.1.1.2" xref="S5.T4.12.6.6.6.m1.1.1.2.cmml">m</mi><mo id="S5.T4.12.6.6.6.m1.1.1.1" xref="S5.T4.12.6.6.6.m1.1.1.1.cmml">⁢</mo><mi id="S5.T4.12.6.6.6.m1.1.1.3" xref="S5.T4.12.6.6.6.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.12.6.6.6.m1.1b"><apply id="S5.T4.12.6.6.6.m1.1.1.cmml" xref="S5.T4.12.6.6.6.m1.1.1"><times id="S5.T4.12.6.6.6.m1.1.1.1.cmml" xref="S5.T4.12.6.6.6.m1.1.1.1"></times><ci id="S5.T4.12.6.6.6.m1.1.1.2.cmml" xref="S5.T4.12.6.6.6.m1.1.1.2">𝑚</ci><ci id="S5.T4.12.6.6.6.m1.1.1.3.cmml" xref="S5.T4.12.6.6.6.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.12.6.6.6.m1.1c">mm</annotation><annotation encoding="application/x-llamapun" id="S5.T4.12.6.6.6.m1.1d">italic_m italic_m</annotation></semantics></math>] <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.13.7.7.7.m2.1"><semantics id="S5.T4.13.7.7.7.m2.1a"><mo id="S5.T4.13.7.7.7.m2.1.1" stretchy="false" xref="S5.T4.13.7.7.7.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.13.7.7.7.m2.1b"><ci id="S5.T4.13.7.7.7.m2.1.1.cmml" xref="S5.T4.13.7.7.7.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.13.7.7.7.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.13.7.7.7.m2.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.16.10.10.10">ACC<sub class="ltx_sub" id="S5.T4.16.10.10.10.1"><span class="ltx_text ltx_font_italic" id="S5.T4.16.10.10.10.1.1">h/o</span></sub> [<math alttext="m/{s^{2}}" class="ltx_Math" display="inline" id="S5.T4.15.9.9.9.m2.1"><semantics id="S5.T4.15.9.9.9.m2.1a"><mrow id="S5.T4.15.9.9.9.m2.1.1" xref="S5.T4.15.9.9.9.m2.1.1.cmml"><mi id="S5.T4.15.9.9.9.m2.1.1.2" xref="S5.T4.15.9.9.9.m2.1.1.2.cmml">m</mi><mo id="S5.T4.15.9.9.9.m2.1.1.1" xref="S5.T4.15.9.9.9.m2.1.1.1.cmml">/</mo><msup id="S5.T4.15.9.9.9.m2.1.1.3" xref="S5.T4.15.9.9.9.m2.1.1.3.cmml"><mi id="S5.T4.15.9.9.9.m2.1.1.3.2" xref="S5.T4.15.9.9.9.m2.1.1.3.2.cmml">s</mi><mn id="S5.T4.15.9.9.9.m2.1.1.3.3" xref="S5.T4.15.9.9.9.m2.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.15.9.9.9.m2.1b"><apply id="S5.T4.15.9.9.9.m2.1.1.cmml" xref="S5.T4.15.9.9.9.m2.1.1"><divide id="S5.T4.15.9.9.9.m2.1.1.1.cmml" xref="S5.T4.15.9.9.9.m2.1.1.1"></divide><ci id="S5.T4.15.9.9.9.m2.1.1.2.cmml" xref="S5.T4.15.9.9.9.m2.1.1.2">𝑚</ci><apply id="S5.T4.15.9.9.9.m2.1.1.3.cmml" xref="S5.T4.15.9.9.9.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T4.15.9.9.9.m2.1.1.3.1.cmml" xref="S5.T4.15.9.9.9.m2.1.1.3">superscript</csymbol><ci id="S5.T4.15.9.9.9.m2.1.1.3.2.cmml" xref="S5.T4.15.9.9.9.m2.1.1.3.2">𝑠</ci><cn id="S5.T4.15.9.9.9.m2.1.1.3.3.cmml" type="integer" xref="S5.T4.15.9.9.9.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.15.9.9.9.m2.1c">m/{s^{2}}</annotation><annotation encoding="application/x-llamapun" id="S5.T4.15.9.9.9.m2.1d">italic_m / italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>] <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.16.10.10.10.m3.1"><semantics id="S5.T4.16.10.10.10.m3.1a"><mo id="S5.T4.16.10.10.10.m3.1.1" stretchy="false" xref="S5.T4.16.10.10.10.m3.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.16.10.10.10.m3.1b"><ci id="S5.T4.16.10.10.10.m3.1.1.cmml" xref="S5.T4.16.10.10.10.m3.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.16.10.10.10.m3.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.16.10.10.10.m3.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.18.12.12.12">MPJPE [<math alttext="mm" class="ltx_Math" display="inline" id="S5.T4.17.11.11.11.m1.1"><semantics id="S5.T4.17.11.11.11.m1.1a"><mrow id="S5.T4.17.11.11.11.m1.1.1" xref="S5.T4.17.11.11.11.m1.1.1.cmml"><mi id="S5.T4.17.11.11.11.m1.1.1.2" xref="S5.T4.17.11.11.11.m1.1.1.2.cmml">m</mi><mo id="S5.T4.17.11.11.11.m1.1.1.1" xref="S5.T4.17.11.11.11.m1.1.1.1.cmml">⁢</mo><mi id="S5.T4.17.11.11.11.m1.1.1.3" xref="S5.T4.17.11.11.11.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.17.11.11.11.m1.1b"><apply id="S5.T4.17.11.11.11.m1.1.1.cmml" xref="S5.T4.17.11.11.11.m1.1.1"><times id="S5.T4.17.11.11.11.m1.1.1.1.cmml" xref="S5.T4.17.11.11.11.m1.1.1.1"></times><ci id="S5.T4.17.11.11.11.m1.1.1.2.cmml" xref="S5.T4.17.11.11.11.m1.1.1.2">𝑚</ci><ci id="S5.T4.17.11.11.11.m1.1.1.3.cmml" xref="S5.T4.17.11.11.11.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.17.11.11.11.m1.1c">mm</annotation><annotation encoding="application/x-llamapun" id="S5.T4.17.11.11.11.m1.1d">italic_m italic_m</annotation></semantics></math>] <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.18.12.12.12.m2.1"><semantics id="S5.T4.18.12.12.12.m2.1a"><mo id="S5.T4.18.12.12.12.m2.1.1" stretchy="false" xref="S5.T4.18.12.12.12.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.18.12.12.12.m2.1b"><ci id="S5.T4.18.12.12.12.m2.1.1.cmml" xref="S5.T4.18.12.12.12.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.18.12.12.12.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.18.12.12.12.m2.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.20.14.14.14">AAE [<sup class="ltx_sup" id="S5.T4.20.14.14.14.1">∘</sup>] <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.20.14.14.14.m2.1"><semantics id="S5.T4.20.14.14.14.m2.1a"><mo id="S5.T4.20.14.14.14.m2.1.1" stretchy="false" xref="S5.T4.20.14.14.14.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.20.14.14.14.m2.1b"><ci id="S5.T4.20.14.14.14.m2.1.1.cmml" xref="S5.T4.20.14.14.14.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.20.14.14.14.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.20.14.14.14.m2.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.22.16.16.16">Success Rate [<math alttext="\%" class="ltx_Math" display="inline" id="S5.T4.21.15.15.15.m1.1"><semantics id="S5.T4.21.15.15.15.m1.1a"><mo id="S5.T4.21.15.15.15.m1.1.1" xref="S5.T4.21.15.15.15.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T4.21.15.15.15.m1.1b"><csymbol cd="latexml" id="S5.T4.21.15.15.15.m1.1.1.cmml" xref="S5.T4.21.15.15.15.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.21.15.15.15.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T4.21.15.15.15.m1.1d">%</annotation></semantics></math>] <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.22.16.16.16.m2.1"><semantics id="S5.T4.22.16.16.16.m2.1a"><mo id="S5.T4.22.16.16.16.m2.1.1" stretchy="false" xref="S5.T4.22.16.16.16.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.22.16.16.16.m2.1b"><ci id="S5.T4.22.16.16.16.m2.1.1.cmml" xref="S5.T4.22.16.16.16.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.22.16.16.16.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.22.16.16.16.m2.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.22.16.18.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.22.16.18.1.1" rowspan="5">
<span class="ltx_inline-block ltx_parbox ltx_align_top" id="S5.T4.22.16.18.1.1.1" style="width:0.0pt;">
<span class="ltx_p" id="S5.T4.22.16.18.1.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.22.16.18.1.1.1.1.1" style="width:6.9pt;height:47.5pt;vertical-align:-20.3pt;"><span class="ltx_transformed_inner" style="width:47.5pt;transform:translate(-20.29pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.22.16.18.1.1.1.1.1.1">Allocentric</span>
</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.22.16.18.1.2">ArcticNet-SF</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.22.16.18.1.3">41.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.22.16.18.1.4">52.39/37.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.22.16.18.1.5">10.40</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.22.16.18.1.6">5.72/7.57</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.22.16.18.1.7">21.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.22.16.18.1.8">5.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.22.16.18.1.9">71.39</td>
</tr>
<tr class="ltx_tr" id="S5.T4.22.16.19.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.19.2.1"><span class="ltx_text" id="S5.T4.22.16.19.2.1.1" style="color:#000000;">DIGIT</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.19.2.2">34.92</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.19.2.3">44.19/35.43</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.19.2.4"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.19.2.4.1">8.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.19.2.5">
<span class="ltx_text ltx_font_bold" id="S5.T4.22.16.19.2.5.1">4.86</span>/6.63</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.19.2.6">17.92</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.19.2.7">5.24</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.19.2.8">76.52</td>
</tr>
<tr class="ltx_tr" id="S5.T4.22.16.20.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.20.3.1"><span class="ltx_text" id="S5.T4.22.16.20.3.1.1" style="color:#000000;">UVHand</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.20.3.2">64.15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.20.3.3">84.68/70.31</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.20.3.4">14.12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.20.3.5">7.05/12.04</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.20.3.6">40.99</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.20.3.7">12.36</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.20.3.8">31.47</td>
</tr>
<tr class="ltx_tr" id="S5.T4.22.16.21.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.21.4.1"><span class="ltx_text" id="S5.T4.22.16.21.4.1.1" style="color:#000000;">AmbiguousHands</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.21.4.2">33.25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.21.4.3">45.78/34.56</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.21.4.4">10.12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.21.4.5">6.37/6.40</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.21.4.6">18.02</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.21.4.7">4.64</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.21.4.8">81.94</td>
</tr>
<tr class="ltx_tr" id="S5.T4.22.16.22.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.22.5.1"><span class="ltx_text" id="S5.T4.22.16.22.5.1.1" style="color:#000000;">JointTransformer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.22.5.2"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.22.5.2.1">27.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.22.5.3"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.22.5.3.1">36.17/28.18</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.22.5.4">8.93</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.22.5.5">6.08/<span class="ltx_text ltx_font_bold" id="S5.T4.22.16.22.5.5.1">5.79</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.22.5.6"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.22.5.6.1">17.12</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.22.5.7"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.22.5.7.1">3.95</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.22.5.8"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.22.5.8.1">89.79</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.22.16.23.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.22.16.23.6.1" rowspan="5">
<span class="ltx_inline-block ltx_parbox ltx_align_top" id="S5.T4.22.16.23.6.1.1" style="width:0.0pt;">
<span class="ltx_p" id="S5.T4.22.16.23.6.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.22.16.23.6.1.1.1.1" style="width:8.8pt;height:46.3pt;vertical-align:-20.7pt;"><span class="ltx_transformed_inner" style="width:46.3pt;transform:translate(-18.75pt,2.92pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.22.16.23.6.1.1.1.1.1">Egocentric</span>
</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.22.16.23.6.2">ArcticNet-SF</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.22.16.23.6.3">44.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.22.16.23.6.4">28.31/36.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.22.16.23.6.5">11.80</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.22.16.23.6.6">5.03/9.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.22.16.23.6.7">19.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.22.16.23.6.8">6.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.22.16.23.6.9">53.89</td>
</tr>
<tr class="ltx_tr" id="S5.T4.22.16.24.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.24.7.1"><span class="ltx_text" id="S5.T4.22.16.24.7.1.1" style="color:#000000;">DIGIT</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.24.7.2">41.31</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.24.7.3">25.49/32.61</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.24.7.4"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.24.7.4.1">9.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.24.7.5">4.01/8.32</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.24.7.6">16.74</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.24.7.7">6.60</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.24.7.8">53.33</td>
</tr>
<tr class="ltx_tr" id="S5.T4.22.16.25.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.25.8.1"><span class="ltx_text" id="S5.T4.22.16.25.8.1.1" style="color:#000000;">UVHand</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.25.8.2">40.43</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.25.8.3">40.93/36.88</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.25.8.4">9.96</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.25.8.5">5.32/8.33</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.25.8.6">24.53</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.25.8.7">7.32</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.25.8.8">57.28</td>
</tr>
<tr class="ltx_tr" id="S5.T4.22.16.26.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.26.9.1"><span class="ltx_text" id="S5.T4.22.16.26.9.1.1" style="color:#000000;">AmbiguousHands</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.26.9.2">35.93</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.26.9.3">
<span class="ltx_text ltx_font_bold" id="S5.T4.22.16.26.9.3.1">23.07</span>/27.53</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.26.9.4">9.51</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.26.9.5"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.26.9.5.1">3.95/6.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.22.16.26.9.6"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.26.9.6.1">16.26</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.26.9.7">4.86</td>
<td class="ltx_td ltx_align_center" id="S5.T4.22.16.26.9.8">68.36</td>
</tr>
<tr class="ltx_tr" id="S5.T4.22.16.27.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.22.16.27.10.1"><span class="ltx_text" id="S5.T4.22.16.27.10.1.1" style="color:#000000;">JointTransformer</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.22.16.27.10.2"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.27.10.2.1">32.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.22.16.27.10.3">26.07/<span class="ltx_text ltx_font_bold" id="S5.T4.22.16.27.10.3.1">26.22</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.22.16.27.10.4">11.34</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.22.16.27.10.5">5.52/8.68</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.22.16.27.10.6">16.33</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.22.16.27.10.7"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.27.10.7.1">4.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.22.16.27.10.8"><span class="ltx_text ltx_font_bold" id="S5.T4.22.16.27.10.8.1">74.07</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Results</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Here, we benchmark results of valid submissions for state-of-the-art comparison in <span class="ltx_text" id="S5.SS1.p1.1.1" style="color:#000000;">AssemblyHands</span> and <span class="ltx_text" id="S5.SS1.p1.1.2" style="color:#000000;">ARCTIC</span> and other more recent baselines. In particular, for <span class="ltx_text" id="S5.SS1.p1.1.3" style="color:#000000;">AssemblyHands</span>, we report egocentric hand pose estimation results.
For <span class="ltx_text" id="S5.SS1.p1.1.4" style="color:#000000;">ARCTIC</span>, we report results for the allocentric and egocentric test sets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1" style="color:#000000;">AssemblyHands</span><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.2"> benchmark:</span>
<span class="ltx_text" id="S5.SS1.p2.1.3" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.T3" title="Table 3 ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">3</span></a></span> shows the final test scores on the AssemblyHands dataset.
The methods in the table exceed the baseline (<span class="ltx_text" id="S5.SS1.p2.1.4" style="color:#000000;">Base</span>) with a test score of 20.69 MPJPE.
Notably, the methods <span class="ltx_text" id="S5.SS1.p2.1.5" style="color:#000000;">JHands</span> and <span class="ltx_text" id="S5.SS1.p2.1.6" style="color:#000000;">PICO-AI</span> achieve a nearly 40 % reduction over the baseline.
The methods <span class="ltx_text" id="S5.SS1.p2.1.7" style="color:#000000;">FRDC</span> and <span class="ltx_text" id="S5.SS1.p2.1.8" style="color:#000000;">Phi-AI</span> improve the test score by 20.3 % and 16.5 % against the baseline, respectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.3"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.3.1" style="color:#000000;">ARCTIC</span><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.3.2"> benchmark:</span>
<span class="ltx_text" id="S5.SS1.p3.3.3" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.T4" title="Table 4 ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">4</span></a></span> presents the comparative performance of methods in the <span class="ltx_text" id="S5.SS1.p3.3.4" style="color:#000000;">ARCTIC</span> dataset, where <span class="ltx_text" id="S5.SS1.p3.3.5" style="color:#000000;">ArcticNet-SF</span> serves as the initial benchmark. The majority surpass <span class="ltx_text" id="S5.SS1.p3.3.6" style="color:#000000;">ArcticNet-SF</span> in both allocentric and egocentric views, except for <span class="ltx_text" id="S5.SS1.p3.3.7" style="color:#000000;">UVHand</span>, which underperforms due to incomplete training. In the egocentric view, <span class="ltx_text" id="S5.SS1.p3.3.8" style="color:#000000;">AmbiguousHands</span> excels in creating smooth, consistent mesh motions (refer to MDev and ACC<sub class="ltx_sub" id="S5.SS1.p3.3.9"><span class="ltx_text ltx_font_italic" id="S5.SS1.p3.3.9.1">h/o</span></sub> metrics). Notably, <span class="ltx_text" id="S5.SS1.p3.3.10" style="color:#000000;">JointTransformer</span> stands out by significantly lowering CDev errors by <math alttext="32.7\%" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mn id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml">32.7</mn><mo id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1">percent</csymbol><cn id="S5.SS1.p3.2.m2.1.1.2.cmml" type="float" xref="S5.SS1.p3.2.m2.1.1.2">32.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">32.7\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">32.7 %</annotation></semantics></math> in allocentric and <math alttext="27.2\%" class="ltx_Math" display="inline" id="S5.SS1.p3.3.m3.1"><semantics id="S5.SS1.p3.3.m3.1a"><mrow id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml"><mn id="S5.SS1.p3.3.m3.1.1.2" xref="S5.SS1.p3.3.m3.1.1.2.cmml">27.2</mn><mo id="S5.SS1.p3.3.m3.1.1.1" xref="S5.SS1.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><apply id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"><csymbol cd="latexml" id="S5.SS1.p3.3.m3.1.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1.1">percent</csymbol><cn id="S5.SS1.p3.3.m3.1.1.2.cmml" type="float" xref="S5.SS1.p3.3.m3.1.1.2">27.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">27.2\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.3.m3.1d">27.2 %</annotation></semantics></math> in egocentric settings compared to the baseline.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span class="ltx_text" id="S5.SS2.1.1" style="color:#000000;">AssemblyHands</span> analysis</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We provide analysis, regarding action-wise evaluation, distortion effect in training, and the effect of multi-view fusion. See <span class="ltx_text" id="S5.SS2.p1.1.1" style="color:#000000;">SupMat</span> for addtional results.</p>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="87" id="S5.F2.g1" src="extracted/5776199/0_figures/assembly/fig/action_wise.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F2.4.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F2.5.2" style="font-size:90%;">Qualitative results per action in AssemblyHands<span class="ltx_text ltx_font_medium" id="S5.F2.5.2.1">.
We show <span class="ltx_text" id="S5.F2.5.2.1.1" style="color:#000000;">Base</span> results with “verb (noun)” actions.
The left three figures are lower error situations while the right four ones are failure cases.
The red boxes denote the area where the action occurs.
</span></span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Action-wise evaluation:</span>
To analyze errors related to hand-object occlusions and interactions,
we show pose evaluation according to fine verb action classes in <span class="ltx_text" id="S5.SS2.p2.1.2" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.T3" title="Table 3 ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">3</span></a></span>.
We use the verb classes annotated by Assembly101 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib61" title="">61</a>]</cite>, spanning every few seconds in a video.
<span class="ltx_text" id="S5.SS2.p2.1.3" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F2" title="Figure 2 ‣ 5.2 AssemblyHands analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">2</span></a></span> shows qualitative results of representative verb classes with the top and bottom error cases.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">We observe that the performance varies among different verb actions.
The verbs “tilt down/up” and “remove screw from” exhibit lower errors among the submitted methods, because hands are less occluded and their movement is relatively stable.
The “tilt” action holds a small part of the toy and turns it around alternately, leading to less overlap between the hand and the object (lib). The “remove screw from” action takes a screw out from the toy vehicle by their hand where observed hand poses do not change drastically.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">Higher error classes, such as “inspect”, “screw”, “rotate”, and “position”, contain heavy occlusions, fast hand motion, complex two hands and object interactions.
The “inspect” action brings the toy close to the human eyes where the toy occupies a large portion of the image causing heavy object occlusions.
The “screw” action involves intricate fingertip movements to rotate the screwdriver quickly.
The “rotate” and “position” actions are performed so that the two hands and the object interact in close contact, which complicates the estimation.
We observe that the top two methods <span class="ltx_text" id="S5.SS2.p4.1.1" style="color:#000000;">JHands</span> and <span class="ltx_text" id="S5.SS2.p4.1.2" style="color:#000000;">PICO-AI</span> significantly correct the results of these higher error actions compared to the other submitted methods.</p>
</div>
<figure class="ltx_figure" id="S5.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F3.1" style="width:277.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="294" id="S5.F3.1.g1" src="extracted/5776199/0_figures/assembly/fig/crop.png" width="616"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F3.1.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F3.1.4.2" style="font-size:90%;">Effect of distortion in AssemblyHands<span class="ltx_text ltx_font_medium" id="S5.F3.1.4.2.1">.
The officially released images in the dataset have highly stretched areas near the edges (original crop). The method <span class="ltx_text" id="S5.F3.1.4.2.1.1" style="color:#000000;">JHands</span> uses a perspective crop with a virtual camera to correct this distortion.
</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F3.fig1" style="width:138.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_figure_panel" id="S5.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.4.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T5.5.2" style="font-size:90%;">Multi-view fusion in AssemblyHands.<span class="ltx_text ltx_font_medium" id="S5.T5.5.2.1">
We use the <span class="ltx_text" id="S5.T5.5.2.1.1" style="color:#000000;">Base</span> result to show performance before and after fusion.
Missing instances per view are denoted as “Miss(%)”.
</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S5.F3.fig1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.F3.fig1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.F3.fig1.1.1.1.1">View</th>
<td class="ltx_td ltx_align_left" id="S5.F3.fig1.1.1.1.2">MPJPE</td>
<td class="ltx_td ltx_align_left" id="S5.F3.fig1.1.1.1.3">Miss(%)</td>
</tr>
<tr class="ltx_tr" id="S5.F3.fig1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.F3.fig1.1.2.2.1">cam1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F3.fig1.1.2.2.2">37.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F3.fig1.1.2.2.3">70.8</td>
</tr>
<tr class="ltx_tr" id="S5.F3.fig1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.F3.fig1.1.3.3.1">cam2</th>
<td class="ltx_td ltx_align_center" id="S5.F3.fig1.1.3.3.2">25.71</td>
<td class="ltx_td ltx_align_center" id="S5.F3.fig1.1.3.3.3">88.3</td>
</tr>
<tr class="ltx_tr" id="S5.F3.fig1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.F3.fig1.1.4.4.1">cam3</th>
<td class="ltx_td ltx_align_center" id="S5.F3.fig1.1.4.4.2">22.19</td>
<td class="ltx_td ltx_align_center" id="S5.F3.fig1.1.4.4.3">0.92</td>
</tr>
<tr class="ltx_tr" id="S5.F3.fig1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.F3.fig1.1.5.5.1">cam4</th>
<td class="ltx_td ltx_align_center" id="S5.F3.fig1.1.5.5.2">22.29</td>
<td class="ltx_td ltx_align_center" id="S5.F3.fig1.1.5.5.3">0.74</td>
</tr>
<tr class="ltx_tr" id="S5.F3.fig1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.F3.fig1.1.6.6.1">
<span class="ltx_ERROR undefined" id="S5.F3.fig1.1.6.6.1.1">\cdashline</span>1-3
cam3+4</th>
<td class="ltx_td ltx_align_center" id="S5.F3.fig1.1.6.6.2">21.52</td>
<td class="ltx_td ltx_align_center" id="S5.F3.fig1.1.6.6.3">0.08</td>
</tr>
<tr class="ltx_tr" id="S5.F3.fig1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.F3.fig1.1.7.7.1">all four</th>
<td class="ltx_td ltx_align_center" id="S5.F3.fig1.1.7.7.2">20.69</td>
<td class="ltx_td ltx_align_center" id="S5.F3.fig1.1.7.7.3">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p5.1.1">Bias of hand position in an image:</span>
Hands near image edges are highly distorted due to the fish-eye cameras.
Directly using these noisy images in training will degrade performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib84" title="">84</a>]</cite>; thus, some methods create new crops with less distortion, select training instances, or adaptively fuse predictions during the inference.
Specifically, <span class="ltx_text" id="S5.SS2.p5.1.2" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F3" title="Figure 3 ‣ 5.2 AssemblyHands analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">3</span></a></span> shows that the method <span class="ltx_text" id="S5.SS2.p5.1.3" style="color:#000000;">JHands</span> reformulates the perspective during cropping and creates less-distorted (perspective) crops.</p>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1">To study this effect in the final performance, we split the evaluation instances into classes with different 2D distances between the hand center and the image center in <span class="ltx_text" id="S5.SS2.p6.1.1" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.T3" title="Table 3 ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">3</span></a></span>.
Higher distances (250- pixels) indicate closer hand crops to image edges.
The method, <span class="ltx_text" id="S5.SS2.p6.1.2" style="color:#000000;">Base</span>, without any training instance selection and distortion correction, has higher error as the crops approach image edges (20.31 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.p6.1.m1.1"><semantics id="S5.SS2.p6.1.m1.1a"><mo id="S5.SS2.p6.1.m1.1.1" stretchy="false" xref="S5.SS2.p6.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.1.m1.1b"><ci id="S5.SS2.p6.1.m1.1.1.cmml" xref="S5.SS2.p6.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p6.1.m1.1d">→</annotation></semantics></math> 24.85).
In contrast, the newly proposed methods are more robust and have a lower error, particularly in the 200-250 range.
We observe that the ranges 200-250 and 250- occupy 10% and 5% of the test images, respectively, thus the improvement in the 200-250 range helps the lowering of the overall score.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p7">
<p class="ltx_p" id="S5.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p7.1.1">Effect of multi-view fusion:</span>
The multi-view egocentric camera setup is unique to the dataset.
We show the statistics and performance of multi-view fusion in <span class="ltx_text" id="S5.SS2.p7.1.2" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F3" title="Figure 3 ‣ 5.2 AssemblyHands analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">3</span></a></span>.
Note that <span class="ltx_text" id="S5.SS2.p7.1.3" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.T3" title="Table 3 ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">3</span></a></span> shows the final results after multi-view fusion.</p>
</div>
<div class="ltx_para" id="S5.SS2.p8">
<p class="ltx_p" id="S5.SS2.p8.1">We found that samples captured from the lower cameras (cam3 and cam4; see <span class="ltx_text" id="S5.SS2.p8.1.1" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">1</span></a></span> for the layout) are numerous (fewer missing samples) and their errors are lower as they are faced toward the area occurring hand interactions.
Conversely, the samples from cam1 and cam2 are fewer and unbalanced as
their cameras often fail to capture hands due to the camera layout.
For instance, cam1 (top-left) tends to capture more hand region than cam2 as the participants are mostly right-handed and bring up the object with the right hand, which can be better observed from cam1.
Given this uneven sample distribution, the proposed adaptive view selection methods in either training or testing are essential to perform effective multi-view fusion, and outperform the <span class="ltx_text" id="S5.SS2.p8.1.2" style="color:#000000;">Base</span>’s test-time average using all views all the time (see <span class="ltx_text ltx_font_bold" id="S5.SS2.p8.1.3">Multi-view fusion</span> in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S4.SS1" title="4.1 AssemblyHands methods ‣ 4 Methods ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</p>
</div>
<div class="ltx_para" id="S5.SS2.p9">
<p class="ltx_p" id="S5.SS2.p9.1">We further study the performance gain before and after multi-view fusion using <span class="ltx_text" id="S5.SS2.p9.1.1" style="color:#000000;">Base</span>’s results.
While per-view performance achieves 22.19 and 22.29 in cam3 and cam4, respectively, their fused results with simple average reduce the error to 21.52.
Merging all four views has shown to be more effective than two-view fusion (20.69 <em class="ltx_emph ltx_font_italic" id="S5.SS2.p9.1.2">vs</em>.<span class="ltx_text" id="S5.SS2.p9.1.3"></span> 21.52), indicating a 6.5% reduction compared to the single camera setup (cam3).
This suggests predictions from the top views (cam1 and cam2) are informative in averaging even when they are prone to be erroneous.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span class="ltx_text" id="S5.SS3.1.1" style="color:#000000;">ARCTIC</span> analysis</h3>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="365" id="S5.F4.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F4.4.2" style="font-size:90%;">Performance comparison: Egocentric vs Allocentric<span class="ltx_text ltx_font_medium" id="S5.F4.4.2.1">.
(a) Comparative difficulty ratio of egocentric to allocentric views.
(b) Egocentric view performance by method across objects.
(c) Allocentric view performance by method across objects.
</span></span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">Egocentric-allocentric comparison:</span>
<span class="ltx_text" id="S5.SS3.p1.1.2" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F4" title="Figure 4 ‣ 5.3 ARCTIC analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">4</span></a></span>a compares the performance between egocentric and allocentric views.
In particular, we compute a ratio between the metric values of the egocentric and allocentric view to measure the extent of difficulty for the egocentric view compared to the allocentric view.
Since success rate is a metric whose value is positively correlated to performance, we take its reciprocal ratio.
We average the ratios across methods and actions.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.2">We observe that hand pose-related metrics such as MPJPE and ACC<sub class="ltx_sub" id="S5.SS3.p2.2.1"><span class="ltx_text ltx_font_italic" id="S5.SS3.p2.2.1.1">h</span></sub> are less than 1.0 on average (see blue color cells), meaning the the egocentric view is easier than allocentric view.
This is because most allocentric cameras in <span class="ltx_text" id="S5.SS3.p2.2.2" style="color:#000000;">ARCTIC</span> are meters away from the subject while the egocentric camera is often close-up, offering higher hand visibility.
Relative translation metrics between hand and object such as MRRPE<sub class="ltx_sub" id="S5.SS3.p2.2.3"><span class="ltx_text ltx_font_italic" id="S5.SS3.p2.2.3.1">rl</span></sub> are also easier in the egocentric view because estimating translation is more difficult from further cameras.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Object reconstruction performance faces unique challenges, as highlighted by the red cells. In the egocentric view, it is notably more difficult to reconstruct accurate object surfaces, articulation (AAE), and hand-object contact (CDev). This increased difficulty arises because objects are often positioned at the image edges and obscured by human arms. Additionally, object poses exhibit greater diversity in the egocentric view due to varying camera angles and occlusions.
While a static camera maintains consistent camera extrinsics across a sequence, an egocentric camera’s extrinsics change with each frame, resulting in higher diversity in camera-view object 6D poses. This diversity complicates object pose estimation in egocentric views.
<span class="ltx_text" id="S5.SS3.p3.1.1" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F5" title="Figure 5 ‣ 5.3 ARCTIC analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">5</span></a></span> illustrates these challenges using the best-performing method, <span class="ltx_text" id="S5.SS3.p3.1.2" style="color:#000000;">JointTransformer</span>. Despite achieving reasonable hand poses, object poses are significantly impacted by occlusions from hands and arms, and the egocentric view undergoes substantial changes throughout a sequence.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="104" id="S5.F5.g1" src="extracted/5776199/0_figures/arctic/fig/failure_cases.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F5.4.2" style="font-size:90%;">Egocentric reconstruction by top method in <span class="ltx_text" id="S5.F5.4.2.1" style="color:#000000;">ARCTIC</span><span class="ltx_text ltx_font_medium" id="S5.F5.4.2.2">.
In the egocentric view, object reconstruction struggle when the object is partially observed on the image boundaries, as well as when heavy hand/arm occlusion occurs.
</span></span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">Object-wise evaluation:</span>
<span class="ltx_text" id="S5.SS3.p4.1.2" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F4" title="Figure 4 ‣ 5.3 ARCTIC analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">4</span></a></span>b and <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F4" title="Figure 4 ‣ 5.3 ARCTIC analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">4</span></a>c break down performances on different objects in the egocentric and allocentric view test sets.
The best method in both cases is <span class="ltx_text" id="S5.SS3.p4.1.3" style="color:#000000;">JointTransformer</span>. The hardest object to reconstruct with good contact consistency (see CDev) is the microwave in both settings due to global rotation sensitivity (see <span class="ltx_text" id="S5.SS3.p4.1.4" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F5" title="Figure 5 ‣ 5.3 ARCTIC analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">5</span></a></span>), though this can be mitigated by a keypoint-based approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib24" title="">24</a>]</cite>. Estimating objects in the egocentric view is more difficult than in the allocentric view, which is indicated by higher errors for all methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p5.1.1">Action-wise evaluation:</span>
<span class="ltx_text" id="S5.SS3.p5.1.2" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F7" title="Figure 7 ‣ 5.3 ARCTIC analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">7</span></a></span> compares performance of different methods in “grab” and “use” actions. In <span class="ltx_text" id="S5.SS3.p5.1.3" style="color:#000000;">ARCTIC</span>, there are sequences to interact with the object with two types of actions by either not articulating the object, or allowing object articulation.
Interestingly, the “grab” motion is more challenging in egocentric and allocentric views.
We hypothesize that this is because there are more diverse object poses for “grab” motions since during object articulation, the participants often focus on articulation instead of object manipulation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p6">
<p class="ltx_p" id="S5.SS3.p6.2"><span class="ltx_text ltx_font_bold" id="S5.SS3.p6.2.1">Effect of model size:</span>
<span class="ltx_text" id="S5.SS3.p6.2.2" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S5.F7" title="Figure 7 ‣ 5.3 ARCTIC analysis ‣ 5 Results and analysis ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">7</span></a></span> illustrates the impact of model size on hand-object contact performance, measured by CDev, for reconstruction on the allocentric validation set. Most methods utilize ResNet50, with <span class="ltx_text" id="S5.SS3.p6.2.3" style="color:#000000;">JointTransformer</span> being the top performer. As the trainable parameters in the backbone increase, <span class="ltx_text" id="S5.SS3.p6.2.4" style="color:#000000;">JointTransformer</span> consistently reduces the CDev error. Note that the x-axis is in log-scale. <span class="ltx_text" id="S5.SS3.p6.2.5" style="color:#000000;">JointTransformer</span> achieved a CDev error of <math alttext="30.5" class="ltx_Math" display="inline" id="S5.SS3.p6.1.m1.1"><semantics id="S5.SS3.p6.1.m1.1a"><mn id="S5.SS3.p6.1.m1.1.1" xref="S5.SS3.p6.1.m1.1.1.cmml">30.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.1.m1.1b"><cn id="S5.SS3.p6.1.m1.1.1.cmml" type="float" xref="S5.SS3.p6.1.m1.1.1">30.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.1.m1.1c">30.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p6.1.m1.1d">30.5</annotation></semantics></math>mm with ViT-L and <math alttext="29.0" class="ltx_Math" display="inline" id="S5.SS3.p6.2.m2.1"><semantics id="S5.SS3.p6.2.m2.1a"><mn id="S5.SS3.p6.2.m2.1.1" xref="S5.SS3.p6.2.m2.1.1.cmml">29.0</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.2.m2.1b"><cn id="S5.SS3.p6.2.m2.1.1.cmml" type="float" xref="S5.SS3.p6.2.m2.1.1">29.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.2.m2.1c">29.0</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p6.2.m2.1d">29.0</annotation></semantics></math>mm with ViT-G, which has ten times more parameters than ViT-L. Interestingly, <span class="ltx_text" id="S5.SS3.p6.2.6" style="color:#000000;">JointTransformer</span> uses frozen weights in the large-scale ViT-L and ViT-G backbones, yet achieves the best results. This suggests a potential direction for leveraging large-scale foundational backbones for hand-object reconstruction.</p>
</div>
<figure class="ltx_figure" id="S5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.F7.1" style="width:216.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="570" id="S5.F7.1.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.1.4.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F7.1.5.2" style="font-size:90%;">Hand-object contact quality for reconstructed results per action<span class="ltx_text ltx_font_medium" id="S5.F7.1.5.2.1">.
We evaluate the contact quality of the 3D reconstruction results from all methods for each action (<em class="ltx_emph ltx_font_italic" id="S5.F7.1.5.2.1.1">i.e</em>.<span class="ltx_text" id="S5.F7.1.5.2.1.2"></span>, grab or use), using Contact Deviation (CDev) in mm as the metric, where lower values indicate better quality.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.F7.2" style="width:203.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="679" id="S5.F7.2.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.2.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F7.2.3.2" style="font-size:90%;">Contact deviation vs. model size<span class="ltx_text ltx_font_medium" id="S5.F7.2.3.2.1">. We assess the contact quality of the reconstruction results, varying by the number of parameters in each model. Contact quality is measured using Contact Deviation (CDev) in mm, with lower values indicating superior results. </span></span></figcaption>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduce the HANDS23 challenge and provide analysis based on the results of the top submitted methods and more recent baselines on the leaderboards.
We organize and compare the submissions and their implementation details based on the learning methods, architecture, pre- and post-processing techniques, and training configurations.
We thoroughly analyze various aspects, such as hand-object occlusions, action and object-wise evaluation, distortion correction, multi-view fusion, egocentric-allocentric comparison, and performance gain of large transformer models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Future directions:</span>
There are several future directions that the community can take. For example, one can explore
more efficient training using multi-view egocentric cameras, leveraging 3D foundation priors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib38" title="">38</a>]</cite> to regularize template-free hand-object reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib15" title="">15</a>]</cite>, estimating hand-object poses with more expressive representations (<em class="ltx_emph ltx_font_italic" id="S6.p2.1.2">e.g</em>.<span class="ltx_text" id="S6.p2.1.3"></span>, heatmap-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib24" title="">24</a>]</cite>),
incorporating motion and temporal modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib18" title="">18</a>]</cite>, featuring more diverse egocentric interaction scenarios,
recognizing actions through captured hand poses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#bib.bib62" title="">62</a>]</cite>,
learning robotic grasping from reconstructed hand-object pose sequences,
and so forth.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work is supported by the Ministry of Science and ICT, Korea, under the ITRC
program (IITP-2024-2020-0-01789).
supervised by the IITP.
UTokyo is supported by
JST ACT-X Grant Number JPMJAX2007, JSPS KAKENHI Grant Number JP22KJ0999,
JST Adopting Sustainable Partnerships for Innovative Research Ecosystem (ASPIRE) Grant Number JPMJAP2303.
NUS is supported by the Ministry of Education, Singapore, under its MOE Academic Research Fund Tier 2 (STEM RIE2025 MOE-T2EP20220-0015).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Abou Zeid, K.: JointTransformer: Winner of the HANDS’2023 ARCTIC Challenge @ ICCV. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/kabouzeid/JointTransformer" title="">https://github.com/kabouzeid/JointTransformer</a> (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Armagan, A., Garcia-Hernando, G., Baek, S., Hampali, S., Rad, M., Zhang, Z., Xie, S., Chen, M., Zhang, B., Xiong, F., et al.: Measuring generalisation to unseen viewpoints, articulations, shapes and objects for 3d hand pose estimation under hand-object interaction. In: ECCV. pp. 85–101. Springer (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Boukhayma, A., de Bem, R., Torr, P.H.S.: 3D hand shape and pose from images in the wild. In: Computer Vision and Pattern Recognition (CVPR). pp. 10843–10852 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Cai, Y., Ge, L., Cai, J., Yuan, J.: Weakly-supervised 3D hand pose estimation from monocular RGB images. In: European Conference on Computer Vision (ECCV). pp. 678–694 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Cao, Z., Radosavovic, I., Kanazawa, A., Malik, J.: Reconstructing hand-object interactions in the wild. In: International Conference on Computer Vision (ICCV). pp. 12417–12426 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chao, Y.W., Yang, W., Xiang, Y., Molchanov, P., Handa, A., Tremblay, J., Narang, Y.S., Van Wyk, K., Iqbal, U., Birchfield, S., Kautz, J., Fox, D.: DexYCB: A benchmark for capturing hand grasping of objects. In: Computer Vision and Pattern Recognition (CVPR). pp. 9044–9053 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chatterjee, D., Sener, F., Ma, S., Yao, A.: Opening the vocabulary of egocentric actions. Conference on Neural Information Processing Systems (NeurIPS) <span class="ltx_text ltx_font_bold" id="bib.bib7.1.1">36</span> (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chen, X., Liu, Y., Dong, Y., Zhang, X., Ma, C., Xiong, Y., Zhang, Y., Guo, X.: MobRecon: mobile-friendly hand mesh reconstruction from monocular image. In: Computer Vision and Pattern Recognition (CVPR). pp. 20512–20522 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chen, X., Wang, B., Shum, H.Y.: Hand avatar: Free-pose hand animation and rendering from monocular video. In: Computer Vision and Pattern Recognition (CVPR) (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Chen, Z., Chen, S., Schmid, C., Laptev, I.: gSDF: Geometry-driven signed distance functions for 3d hand-object reconstruction. In: Computer Vision and Pattern Recognition (CVPR). pp. 12890–12900 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Christen, S., Kocabas, M., Aksan, E., Hwangbo, J., Song, J., Hilliges, O.: D-Grasp: Physically plausible dynamic grasp synthesis for hand-object interactions. In: Computer Vision and Pattern Recognition (CVPR). pp. 20545–20554 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Corona, E., Pumarola, A., Alenyà, G., Moreno-Noguer, F., Rogez, G.: GanHand: Predicting human grasp affordances in multi-object scenes. In: Computer Vision and Pattern Recognition (CVPR). pp. 5030–5040 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Duran, E., Kocabas, M., Choutas, V., Fan, Z., Black, M.J.: HMP: Hand motion priors for pose and shape estimation from video. In: Winter Conference on Applications of Computer Vision (WACV) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Erol, A., Bebis, G., Nicolescu, M., Boyle, R.D., Twombly, X.: Vision-based hand pose estimation: A review. Computer Vision and Image Understanding (CVIU) <span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">108</span>(1-2), 52–73 (2007)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Fan, Z., Parelli, M., Kadoglou, M.E., Kocabas, M., Chen, X., Black, M.J., Hilliges, O.: HOLD: Category-agnostic 3d reconstruction of interacting hands and objects from video. In: Computer Vision and Pattern Recognition (CVPR) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Fan, Z., Spurr, A., Kocabas, M., Tang, S., Black, M.J., Hilliges, O.: Learning to disambiguate strongly interacting hands via probabilistic per-pixel part segmentation. In: International Conference on 3D Vision (3DV). pp. 1–10 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Fan, Z., Taheri, O., Tzionas, D., Kocabas, M., Kaufmann, M., Black, M.J., Hilliges, O.: ARCTIC: A dataset for dexterous bimanual hand-object manipulation. In: Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Fu, Q., Liu, X., Xu, R., Niebles, J.C., Kitani, K.M.: Deformer: Dynamic fusion transformer for robust hand pose estimation. In: International Conference on Computer Vision (ICCV). pp. 23600–23611 (October 2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Garcia-Hernando, G., Yuan, S., Baek, S., Kim, T.K.: First-person hand action benchmark with rgb-d videos and 3d hand pose annotations. In: Computer Vision and Pattern Recognition (CVPR) (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Ge, L., Ren, Z., Li, Y., Xue, Z., Wang, Y., Cai, J., Yuan, J.: 3D hand shape and pose estimation from a single rgb image. In: Computer Vision and Pattern Recognition (CVPR). pp. 10833–10842 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Grady, P., Tang, C., Twigg, C.D., Vo, M., Brahmbhatt, S., Kemp, C.C.: ContactOpt: Optimizing contact to improve grasps. In: Computer Vision and Pattern Recognition (CVPR). pp. 1471–1481 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Guo, Z., Zhou, W., Wang, M., Li, L., Li, H.: HandNeRF: Neural radiance fields for animatable interacting hands. In: Computer Vision and Pattern Recognition (CVPR). pp. 21078–21087 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Hampali, S., Rad, M., Oberweger, M., Lepetit, V.: HOnnotate: A method for 3D annotation of hand and object poses. In: Computer Vision and Pattern Recognition (CVPR). pp. 3193–3203 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Hampali, S., Sarkar, S.D., Rad, M., Lepetit, V.: Keypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3D pose estimation. In: Computer Vision and Pattern Recognition (CVPR). pp. 11090–11100 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Han, S., Wu, P.C., Zhang, Y., Liu, B., Zhang, L., Wang, Z., Si, W., Zhang, P., Cai, Y., Hodan, T., Cabezas, R., Tran, L., Akbay, M., Yu, T.H., Keskin, C., Wang, R.: Umetrack: Unified multi-view end-to-end hand tracking for VR. In: International Conference on Computer Graphics and Interactive Techniques (SIGGRAPH). pp. 50:1–50:9. ACM (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Hasson, Y., Tekin, B., Bogo, F., Laptev, I., Pollefeys, M., Schmid, C.: Leveraging photometric consistency over time for sparsely supervised hand-object reconstruction. In: Computer Vision and Pattern Recognition (CVPR). pp. 568–577 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Hasson, Y., Varol, G., Schmid, C., Laptev, I.: Towards unconstrained joint hand-object reconstruction from rgb videos. In: International Conference on 3D Vision (3DV). pp. 659–668. IEEE (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Hasson, Y., Varol, G., Tzionas, D., Kalevatykh, I., Black, M.J., Laptev, I., Schmid, C.: Learning joint reconstruction of hands and manipulated objects. In: Computer Vision and Pattern Recognition (CVPR). pp. 11807–11816 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Computer Vision and Pattern Recognition (CVPR). pp. 770–778 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.B.: Masked autoencoders are scalable vision learners. In: Computer Vision and Pattern Recognition (CVPR). pp. 15979–15988 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Huang, D., Ji, X., He, X., Sun, J., He, T., Shuai, Q., Ouyang, W., Zhou, X.: Reconstructing hand-held objects from monocular video. In: SIGGRAPH Asia 2022 Conference Papers. pp. 1–9 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Iqbal, U., Molchanov, P., Gall, T.B.J., Kautz, J.: Hand pose estimation via latent 2.5D heatmap regression. In: European Conference on Computer Vision (ECCV). pp. 118–134 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Kwon, T., Tekin, B., Stühmer, J., Bogo, F., Pollefeys, M.: H2O: Two hands manipulating objects for first person interaction recognition. In: International Conference on Computer Vision (ICCV). pp. 10138–10148 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Lee, J., Sung, M., Choi, H., Kim, T.K.: Im2hands: Learning attentive implicit representation of interacting two-hand shapes. In: Computer Vision and Pattern Recognition (CVPR). pp. 21169–21178 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Li, L., Tian, L., Zhang, X., Wang, Q., Zhang, B., Bo, L., Liu, M., Chen, C.: Renderih: A large-scale synthetic dataset for 3d interacting hand pose estimation. In: International Conference on Computer Vision (ICCV). pp. 20395–20405 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Li, M., An, L., Zhang, H., Wu, L., Chen, F., Yu, T., Liu, Y.: Interacting attention graph for single image two-hand reconstruction. In: Computer Vision and Pattern Recognition (CVPR). pp. 2761–2770 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Liu, R., Ohkawa, T., Zhang, M., Sato, Y.: Single-to-dual-view adaptation for egocentric 3d hand pose estimation. In: Computer Vision and Pattern Recognition (CVPR). pp. 677–686 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Liu, R., Wu, R., Hoorick, B.V., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Liu, S., Jiang, H., Xu, J., Liu, S., Wang, X.: Semi-supervised 3D hand-object poses estimation with interactions in time. In: Computer Vision and Pattern Recognition (CVPR). pp. 14687–14697 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: International Conference on Computer Vision (ICCV). pp. 10012–10022 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Liu, Z., Mao, H., Wu, C., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Computer Vision and Pattern Recognition (CVPR). pp. 11966–11976 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Meng, H., Jin, S., Liu, W., Qian, C., Lin, M., Ouyang, W., Luo, P.: 3D interacting hand pose estimation by hand de-occlusion and removal. In: European Conference on Computer Vision (ECCV). pp. 380–397. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Moon, G.: Bringing inputs to shared domains for 3d interacting hands recovery in the wild. In: Computer Vision and Pattern Recognition (CVPR). pp. 17028–17037 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Moon, G., Saito, S., Xu, W., Joshi, R., Buffalini, J., Bellan, H., Rosen, N., Richardson, J., Mize, M., De Bree, P., et al.: A dataset of relighted 3d interacting hands. Conference on Neural Information Processing Systems (NeurIPS) <span class="ltx_text ltx_font_bold" id="bib.bib44.1.1">36</span> (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Moon, G., Yu, S., Wen, H., Shiratori, T., Lee, K.M.: Interhand2.6m: A dataset and baseline for 3d interacting hand pose estimation from a single RGB image. In: European Conference on Computer Vision (ECCV). vol. 12365, pp. 548–564 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Mueller, F., Bernard, F., Sotnychenko, O., Mehta, D., Sridhar, S., Casas, D., Theobalt, C.: GANerated hands for real-time 3D hand tracking from monocular RGB. In: Computer Vision and Pattern Recognition (CVPR). pp. 49–59 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Ohkawa, T.: AssemblyHands Toolkit. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/assemblyhands-toolkit" title="">https://github.com/facebookresearch/assemblyhands-toolkit</a> (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Ohkawa, T., Furuta, R., Sato, Y.: Efficient annotation and learning for 3d hand pose estimation: A survey. International Journal of Computer Vision (IJCV) <span class="ltx_text ltx_font_bold" id="bib.bib48.1.1">131</span>, 3193––3206 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Ohkawa, T., He, K., Sener, F., Hodan, T., Tran, L., Keskin, C.: AssemblyHands: towards egocentric activity understanding via 3d hand pose estimation. In: Computer Vision and Pattern Recognition (CVPR). pp. 12999–13008 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Ohkawa, T., Li, Y.J., Fu, Q., Furuta, R., Kitani, K.M., Sato, Y.: Domain adaptive hand keypoint and pixel localization in the wild. In: European Conference on Computer Vision (ECCV). pp. 68––87 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Ohkawa, T., Yagi, T., Hashimoto, A., Ushiku, Y., Sato, Y.: Foreground-aware stylization and consensus pseudo-labeling for domain adaptation of first-person hand segmentation. IEEE Access <span class="ltx_text ltx_font_bold" id="bib.bib51.1.1">9</span>, 94644–94655 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Oquab, M., Darcet, T., Moutakanni, T., Vo, H.V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Howes, R., Huang, P.Y., Xu, H., Sharma, V., Li, S.W., Galuba, W., Rabbat, M., Assran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski, P.: Dinov2: Learning robust visual features without supervision (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Park, J., Oh, Y., Moon, G., Choi, H., Lee, K.M.: HandOccNet: Occlusion-robust 3d hand mesh estimation network. In: Computer Vision and Pattern Recognition (CVPR). pp. 1496–1505 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Prakash, A., Tu, R., Chang, M., Gupta, S.: 3d hand pose estimation in everyday egocentric images. In: European Conference on Computer Vision (ECCV) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Radosavovic, I., Kosaraju, R.P., Girshick, R.B., He, K., Dollár, P.: Designing network design spaces. In: Computer Vision and Pattern Recognition (CVPR). pp. 10425–10433 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Rehg, J.M., Kanade, T.: Visual tracking of high DOF articulated structures: An application to human hand tracking. In: European Conference on Computer Vision (ECCV). vol. 801, pp. 35–46 (1994)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Remelli, E., Han, S., Honari, S., Fua, P., Wang, R.: Lightweight multi-view 3d pose estimation through camera-disentangled representation. In: Computer Vision and Pattern Recognition (CVPR). pp. 6039–6048 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Romero, J., Tzionas, D., Black, M.J.: Embodied hands: Modeling and capturing hands and bodies together. Transactions on Graphics (TOG) <span class="ltx_text ltx_font_bold" id="bib.bib59.1.1">36</span>(6), 245:1–245:17 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Ryali, C., Hu, Y., Bolya, D., Wei, C., Fan, H., Huang, P., Aggarwal, V., Chowdhury, A., Poursaeed, O., Hoffman, J., Malik, J., Li, Y., Feichtenhofer, C.: Hiera: A hierarchical vision transformer without the bells-and-whistles. In: International Conference on Machine Learning (ICML). vol. 202, pp. 29441–29454 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Sener, F., Chatterjee, D., Shelepov, D., He, K., Singhania, D., Wang, R., Yao, A.: Assembly101: A large-scale multi-view video dataset for understanding procedural activities. In: Computer Vision and Pattern Recognition (CVPR). pp. 21064–21074 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Shamil, M.S., Chatterjee, D., Sener, F., Ma, S., Yao, A.: On the utility of 3d hand poses for action recognition. In: European Conference on Computer Vision (ECCV) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Simon, T., Joo, H., Matthews, I., Sheikh, Y.: Hand keypoint detection in single images using multiview bootstrapping. In: Computer Vision and Pattern Recognition (CVPR). pp. 4645–4653 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Spurr, A., Dahiya, A., Wang, X., Zhang, X., Hilliges, O.: Self-supervised 3D hand pose estimation from monocular RGB via contrastive learning. In: International Conference on Computer Vision (ICCV). pp. 11210–11219 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Spurr, A., Iqbal, U., Molchanov, P., Hilliges, O., Kautz, J.: Weakly supervised 3D hand pose estimation via biomechanical constraints. In: European Conference on Computer Vision (ECCV). vol. 12362, pp. 211–228 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Spurr, A., Song, J., Park, S., Hilliges, O.: Cross-modal deep variational hand pose estimation. In: Computer Vision and Pattern Recognition (CVPR). pp. 89–98 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution representation learning for human pose estimation. In: Computer Vision and Pattern Recognition (CVPR) (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Swamy, A., Leroy, V., Weinzaepfel, P., Baradel, F., Galaaoui, S., Brégier, R., Armando, M., Franco, J.S., Rogez, G.: SHOWMe: Benchmarking object-agnostic hand-object 3d reconstruction. In: International Conference on Computer Vision (ICCV). pp. 1935–1944 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Tekin, B., Bogo, F., Pollefeys, M.: H+O: Unified egocentric recognition of 3D hand-object poses and interactions. In: Computer Vision and Pattern Recognition (CVPR). pp. 4511–4520 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Tse, T.H.E., Kim, K.I., Leonardis, A., Chang, H.J.: Collaborative learning for hand and object reconstruction with attention-guided graph convolution. In: Computer Vision and Pattern Recognition (CVPR). pp. 1664–1674 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Tzionas, D., Gall, J.: A comparison of directional distances for hand pose estimation. In: German Conference on Pattern Recognition (GCPR). vol. 8142, pp. 131–141 (2013)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Wen, Y., Pan, H., Ohkawa, T., Yang, L., Pan, J., Sato, Y., Komura, T., Wang, W.: Generative hierarchical temporal transformer for hand action recognition and motion prediction. CoRR <span class="ltx_text ltx_font_bold" id="bib.bib72.1.1">abs/2311.17366</span> (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Yang, L., Chen, S., Yao, A.: Semihand: Semi-supervised hand pose estimation with consistency. In: International Conference on Computer Vision (ICCV). pp. 11364–11373 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Yang, L., Zhan, X., Li, K., Xu, W., Li, J., Lu, C.: CPF: Learning a contact potential field to model the hand-object interaction. In: International Conference on Computer Vision (ICCV) (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Ye, Y., Gupta, A., Tulsiani, S.: What’s in your hands? 3d reconstruction of generic objects in hands. In: Computer Vision and Pattern Recognition (CVPR) (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Ye, Y., Hebbar, P., Gupta, A., Tulsiani, S.: Diffusion-guided reconstruction of everyday hand-object interaction clips. In: International Conference on Computer Vision (ICCV) (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Yuan, S., Garcia-Hernando, G., Stenger, B., Moon, G., Chang, J.Y., Lee, K.M., Molchanov, P., Kautz, J., Honari, S., Ge, L., Yuan, J., Chen, X., Wang, G., Yang, F., Akiyama, K., Wu, Y., Wan, Q., Madadi, M., Escalera, S., Li, S., Lee, D., Oikonomidis, I., Argyros, A.A., Kim, T.: Depth-based 3D hand pose estimation: From current achievements to future goals. In: Computer Vision and Pattern Recognition (CVPR). pp. 2636–2645 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L.: Scaling vision transformers. In: Computer Vision and Pattern Recognition (CVPR). pp. 12104–12113 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Zhang, H., Christen, S., Fan, Z., Hilliges, O., Song, J.: GraspXL: Generating grasping motions for diverse objects at scale. European Conference on Computer Vision (ECCV) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Zhang, H., Christen, S., Fan, Z., Zheng, L., Hwangbo, J., Song, J., Hilliges, O.: ArtiGrasp: Physically plausible synthesis of bi-manual dexterous grasping and articulation. In: International Conference on 3D Vision (3DV) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Zhang, X., Li, Q., Mo, H., Zhang, W., Zheng, W.: End-to-end hand mesh recovery from a monocular RGB image. In: International Conference on Computer Vision (ICCV). pp. 2354–2364 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Zhou, Y., Habermann, M., Xu, W., Habibie, I., Theobalt, C., Xu, F.: Monocular real-time hand shape and motion capture using multi-modal data. In: Computer Vision and Pattern Recognition (CVPR). pp. 5345–5354 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Zhou, Z.: SimpleHand: Winner of the HANDS’2023 AssemblyHands Challenge @ ICCV. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/patienceFromZhou/simpleHand" title="">https://github.com/patienceFromZhou/simpleHand</a> (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Zhou, Z., Lv, Z., Zhou, S., Zou, M., Wu, T., Yu, M., Tang, Y., Liang, J.: 1st place solution of egocentric 3d hand pose estimation challenge 2023 technical report: A concise pipeline for egocentric hand pose reconstruction. CoRR <span class="ltx_text ltx_font_bold" id="bib.bib84.1.1">abs/2310.04769</span> (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Zhou, Z., Zhou, S., Lv, Z., Zou, M., Tang, Y., Liang, J.: A simple baseline for efficient hand mesh reconstruction. In: Computer Vision and Pattern Recognition (CVPR). pp. 1367–1376 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable DETR: deformable transformers for end-to-end object detection. In: International Conference on Learning Representations (ICLR). OpenReview.net (2021), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gZ9hCDWe6ke" title="">https://openreview.net/forum?id=gZ9hCDWe6ke</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Ziani, A., Fan, Z., Kocabas, M., Christen, S., Hilliges, O.: TempCLR: Reconstructing hands via time-coherent contrastive learning. In: International Conference on 3D Vision (3DV). pp. 627–636 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Zimmermann, C., Brox, T.: Learning to estimate 3D hand pose from single RGB images. In: International Conference on Computer Vision (ICCV). pp. 4913–4921 (2017)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">Benchmarks and Challenges in Pose Estimation
for Egocentric Hand Interactions with Objects
<br class="ltx_break"/>
— Supplementary Material —</p>
</div>
<section class="ltx_section" id="S1a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">A </span>Additional results of <span class="ltx_text" id="S1a.1.1" style="color:#000000;">AssemblyHands</span>
</h2>
<div class="ltx_para ltx_noindent" id="S1a.p1">
<p class="ltx_p" id="S1a.p1.1"><span class="ltx_text ltx_font_bold" id="S1a.p1.1.1">Qualitative results:</span>
<span class="ltx_text" id="S1a.p1.1.2" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S1.F1a" title="Figure a ‣ A Additional results of AssemblyHands ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">a</span></a></span> shows the qualitative results of submitted methods and failure patterns indicated by the red circles.
The left hand in the first row grabs the object where the left thumb finger is only visible.
While <span class="ltx_text" id="S1a.p1.1.3" style="color:#000000;">Base</span> fails to infer the plausible pose, <span class="ltx_text" id="S1a.p1.1.4" style="color:#000000;">JHands</span> enables estimation in such heavy hand-object occlusions compared to the GT.
However, the methods <span class="ltx_text" id="S1a.p1.1.5" style="color:#000000;">PICO-AI</span> and <span class="ltx_text" id="S1a.p1.1.6" style="color:#000000;">FRDC</span> incorrectly predict the location of the left thumb finger and <span class="ltx_text" id="S1a.p1.1.7" style="color:#000000;">Phi-AI</span>’s prediction of the left index and middle fingers is also erroneous.
The second row is the case where two hands and an object are closely interacting, particularly the left thumb finger presents near the right hand.
The methods <span class="ltx_text" id="S1a.p1.1.8" style="color:#000000;">Base</span>, <span class="ltx_text" id="S1a.p1.1.9" style="color:#000000;">FRDC</span>, and <span class="ltx_text" id="S1a.p1.1.10" style="color:#000000;">Phi-AI</span> fail to localize the left thumb finger.
The third and fourth rows indicate hand images presented near the image edges.
The methods <span class="ltx_text" id="S1a.p1.1.11" style="color:#000000;">Base</span>, <span class="ltx_text" id="S1a.p1.1.12" style="color:#000000;">PICO-AI</span>, and <span class="ltx_text" id="S1a.p1.1.13" style="color:#000000;">Phi-AI</span> are prone to produce implausible predictions, including noise and stretched poses due to the distortion effect discussed in Section 5.2 “<span class="ltx_text ltx_font_bold" id="S1a.p1.1.14">Bias of hand position in an image</span>.”
The method <span class="ltx_text" id="S1a.p1.1.15" style="color:#000000;">JHands</span> with distortion correction successfully addresses these edge images.</p>
</div>
<figure class="ltx_figure" id="S1.F1a"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="369" id="S1.F1a.g1" src="extracted/5776199/0_figures/assembly/fig/qual.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1a.8.1.1" style="font-size:90%;">Figure a</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1a.9.2" style="font-size:90%;">Qualitative results of submitted methods in AssemblyHands.<span class="ltx_text ltx_font_medium" id="S1.F1a.9.2.1">
The columns correspond to the results of <span class="ltx_text" id="S1.F1a.9.2.1.1" style="color:#000000;">Base</span>, ground-truth (GT), submitted methods, namely (a) <span class="ltx_text" id="S1.F1a.9.2.1.2" style="color:#000000;">JHands</span>, (b) <span class="ltx_text" id="S1.F1a.9.2.1.3" style="color:#000000;">PICO-AI</span>, (c) <span class="ltx_text" id="S1.F1a.9.2.1.4" style="color:#000000;">FRDC</span>, and (d) <span class="ltx_text" id="S1.F1a.9.2.1.5" style="color:#000000;">Phi-AI</span>.
The red circles indicate where failures occur.
</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="90" id="S1.F2.g1" src="extracted/5776199/0_figures/assembly/fig/per_view.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.5.2.1" style="font-size:90%;">Figure b</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F2.2.1" style="font-size:90%;">Additional results of multi-view fusion in AssemblyHands<span class="ltx_text ltx_font_medium" id="S1.F2.2.1.1">.
We analyze the availability of samples and performance per camera view.
The two lowest cameras (cam3, cam4) out of the four cameras allow us to capture hands most of the time (<math alttext="&gt;" class="ltx_Math" display="inline" id="S1.F2.2.1.1.m1.1"><semantics id="S1.F2.2.1.1.m1.1b"><mo id="S1.F2.2.1.1.m1.1.1" xref="S1.F2.2.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S1.F2.2.1.1.m1.1c"><gt id="S1.F2.2.1.1.m1.1.1.cmml" xref="S1.F2.2.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.2.1.1.m1.1d">&gt;</annotation><annotation encoding="application/x-llamapun" id="S1.F2.2.1.1.m1.1e">&gt;</annotation></semantics></math>93 % of samples).
In contrast, the images from cam1 and cam2 are fewer and the error varies in different sequences.
</span></span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1a.p2">
<p class="ltx_p" id="S1a.p2.1"><span class="ltx_text ltx_font_bold" id="S1a.p2.1.1">Per-view analysis:</span>
<span class="ltx_text" id="S1a.p2.1.2" style="color:#000000;">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.16428v2#S1.F2" title="Figure b ‣ A Additional results of AssemblyHands ‣ Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects"><span class="ltx_text ltx_ref_tag">b</span></a></span> shows the detailed statistics and performance of per-view predictions, related to the analysis in Section 5.2 “<span class="ltx_text ltx_font_bold" id="S1a.p2.1.3">Effect of multi-view fusion</span>.”
Considering per-sequence results, we find the sample availability (blue bars) and performance (green bars) from cam1 and cam2 vary among different users.
In contrast, the number of samples and performance of cam3 and cam4 are mostly stable.
This study further necessitates the sample selection and multi-view fusion adaptively for each sequence (user).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Aug  6 03:34:31 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
