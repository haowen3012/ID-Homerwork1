<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging</title>
<!--Generated on Mon Apr  8 12:48:51 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.01941v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S1" title="1 Introduction ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S2" title="2 Related Work ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S2.SS1" title="2.1 Lensless Imaging System ‚Ä£ 2 Related Work ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Lensless Imaging System</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S2.SS2" title="2.2 Human Pose and Shape Recovery ‚Ä£ 2 Related Work ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Human Pose and Shape Recovery</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S3" title="3 Method ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S3.SS1" title="3.1 Overview ‚Ä£ 3 Method ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S3.SS2" title="3.2 Multi-Scale Lensless Feature Decoder ‚Ä£ 3 Method ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Multi-Scale Lensless Feature Decoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S3.SS3" title="3.3 Human Parametric Model Regression ‚Ä£ 3 Method ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Human Parametric Model Regression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S3.SS4" title="3.4 Double-Head Auxiliary Supervision ‚Ä£ 3 Method ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Double-Head Auxiliary Supervision</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4" title="4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.SS1" title="4.1 Imaging System and Dataset ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Imaging System and Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.SS2" title="4.2 Implementation Details ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.SS3" title="4.3 3D Human Pose and Shape Comparison. ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>3D Human Pose and Shape Comparison.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.SS4" title="4.4 Ablation Study ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.SS5" title="4.5 Results on more Datasets and Real Scene ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Results on more Datasets and Real Scene</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S5" title="5 Conclusion and Discussion ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion and Discussion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: axessibility</li>
<li>failed: epic</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2404.01941v3 [cs.CV] 08 Apr 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haoyang Ge<math alttext="{}^{1,\dagger}" class="ltx_Math" display="inline" id="id1.1.m1.2"><semantics id="id1.1.m1.2a"><msup id="id1.1.m1.2.2" xref="id1.1.m1.2.2.cmml"><mi id="id1.1.m1.2.2a" xref="id1.1.m1.2.2.cmml"></mi><mrow id="id1.1.m1.2.2.2.4" xref="id1.1.m1.2.2.2.3.cmml"><mn id="id1.1.m1.1.1.1.1" xref="id1.1.m1.1.1.1.1.cmml">1</mn><mo id="id1.1.m1.2.2.2.4.1" rspace="0em" xref="id1.1.m1.2.2.2.3.cmml">,</mo><mo id="id1.1.m1.2.2.2.2" lspace="0em" xref="id1.1.m1.2.2.2.2.cmml">‚Ä†</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.2b"><apply id="id1.1.m1.2.2.cmml" xref="id1.1.m1.2.2"><list id="id1.1.m1.2.2.2.3.cmml" xref="id1.1.m1.2.2.2.4"><cn id="id1.1.m1.1.1.1.1.cmml" type="integer" xref="id1.1.m1.1.1.1.1">1</cn><ci id="id1.1.m1.2.2.2.2.cmml" xref="id1.1.m1.2.2.2.2">‚Ä†</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.2c">{}^{1,\dagger}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.2d">start_FLOATSUPERSCRIPT 1 , ‚Ä† end_FLOATSUPERSCRIPT</annotation></semantics></math>, Qiao Feng<math alttext="{}^{1,\dagger}" class="ltx_Math" display="inline" id="id2.2.m2.2"><semantics id="id2.2.m2.2a"><msup id="id2.2.m2.2.2" xref="id2.2.m2.2.2.cmml"><mi id="id2.2.m2.2.2a" xref="id2.2.m2.2.2.cmml"></mi><mrow id="id2.2.m2.2.2.2.4" xref="id2.2.m2.2.2.2.3.cmml"><mn id="id2.2.m2.1.1.1.1" xref="id2.2.m2.1.1.1.1.cmml">1</mn><mo id="id2.2.m2.2.2.2.4.1" rspace="0em" xref="id2.2.m2.2.2.2.3.cmml">,</mo><mo id="id2.2.m2.2.2.2.2" lspace="0em" xref="id2.2.m2.2.2.2.2.cmml">‚Ä†</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.2b"><apply id="id2.2.m2.2.2.cmml" xref="id2.2.m2.2.2"><list id="id2.2.m2.2.2.2.3.cmml" xref="id2.2.m2.2.2.2.4"><cn id="id2.2.m2.1.1.1.1.cmml" type="integer" xref="id2.2.m2.1.1.1.1">1</cn><ci id="id2.2.m2.2.2.2.2.cmml" xref="id2.2.m2.2.2.2.2">‚Ä†</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.2c">{}^{1,\dagger}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.2d">start_FLOATSUPERSCRIPT 1 , ‚Ä† end_FLOATSUPERSCRIPT</annotation></semantics></math>, Hailong Jia<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mn id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><cn id="id3.3.m3.1.1.1.cmml" type="integer" xref="id3.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Xiongzheng Li<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mi id="id4.4.m4.1.1a" xref="id4.4.m4.1.1.cmml"></mi><mn id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><cn id="id4.4.m4.1.1.1.cmml" type="integer" xref="id4.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Xiangjun Yin<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id5.5.m5.1"><semantics id="id5.5.m5.1a"><msup id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml"><mi id="id5.5.m5.1.1a" xref="id5.5.m5.1.1.cmml"></mi><mn id="id5.5.m5.1.1.1" xref="id5.5.m5.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><apply id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1"><cn id="id5.5.m5.1.1.1.cmml" type="integer" xref="id5.5.m5.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>,
<br class="ltx_break"/>You Zhou<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id6.6.m6.1"><semantics id="id6.6.m6.1a"><msup id="id6.6.m6.1.1" xref="id6.6.m6.1.1.cmml"><mi id="id6.6.m6.1.1a" xref="id6.6.m6.1.1.cmml"></mi><mn id="id6.6.m6.1.1.1" xref="id6.6.m6.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id6.6.m6.1b"><apply id="id6.6.m6.1.1.cmml" xref="id6.6.m6.1.1"><cn id="id6.6.m6.1.1.1.cmml" type="integer" xref="id6.6.m6.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m6.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id6.6.m6.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Jingyu Yang<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id7.7.m7.1"><semantics id="id7.7.m7.1a"><msup id="id7.7.m7.1.1" xref="id7.7.m7.1.1.cmml"><mi id="id7.7.m7.1.1a" xref="id7.7.m7.1.1.cmml"></mi><mn id="id7.7.m7.1.1.1" xref="id7.7.m7.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id7.7.m7.1b"><apply id="id7.7.m7.1.1.cmml" xref="id7.7.m7.1.1"><cn id="id7.7.m7.1.1.1.cmml" type="integer" xref="id7.7.m7.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.m7.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id7.7.m7.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Kun Li<math alttext="{}^{1,*}" class="ltx_Math" display="inline" id="id8.8.m8.2"><semantics id="id8.8.m8.2a"><msup id="id8.8.m8.2.2" xref="id8.8.m8.2.2.cmml"><mi id="id8.8.m8.2.2a" xref="id8.8.m8.2.2.cmml"></mi><mrow id="id8.8.m8.2.2.2.4" xref="id8.8.m8.2.2.2.3.cmml"><mn id="id8.8.m8.1.1.1.1" xref="id8.8.m8.1.1.1.1.cmml">1</mn><mo id="id8.8.m8.2.2.2.4.1" rspace="0em" xref="id8.8.m8.2.2.2.3.cmml">,</mo><mo id="id8.8.m8.2.2.2.2" lspace="0em" xref="id8.8.m8.2.2.2.2.cmml">*</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id8.8.m8.2b"><apply id="id8.8.m8.2.2.cmml" xref="id8.8.m8.2.2"><list id="id8.8.m8.2.2.2.3.cmml" xref="id8.8.m8.2.2.2.4"><cn id="id8.8.m8.1.1.1.1.cmml" type="integer" xref="id8.8.m8.1.1.1.1">1</cn><times id="id8.8.m8.2.2.2.2.cmml" xref="id8.8.m8.2.2.2.2"></times></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.8.m8.2c">{}^{1,*}</annotation><annotation encoding="application/x-llamapun" id="id8.8.m8.2d">start_FLOATSUPERSCRIPT 1 , * end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"/><math alttext="{}^{1}" class="ltx_Math" display="inline" id="id9.9.m9.1"><semantics id="id9.9.m9.1a"><msup id="id9.9.m9.1.1" xref="id9.9.m9.1.1.cmml"><mi id="id9.9.m9.1.1a" xref="id9.9.m9.1.1.cmml"></mi><mn id="id9.9.m9.1.1.1" xref="id9.9.m9.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id9.9.m9.1b"><apply id="id9.9.m9.1.1.cmml" xref="id9.9.m9.1.1"><cn id="id9.9.m9.1.1.1.cmml" type="integer" xref="id9.9.m9.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.m9.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id9.9.m9.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>Tianjin University, China ‚ÄÉ<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id10.10.m10.1"><semantics id="id10.10.m10.1a"><msup id="id10.10.m10.1.1" xref="id10.10.m10.1.1.cmml"><mi id="id10.10.m10.1.1a" xref="id10.10.m10.1.1.cmml"></mi><mn id="id10.10.m10.1.1.1" xref="id10.10.m10.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id10.10.m10.1b"><apply id="id10.10.m10.1.1.cmml" xref="id10.10.m10.1.1"><cn id="id10.10.m10.1.1.1.cmml" type="integer" xref="id10.10.m10.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.10.m10.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id10.10.m10.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>Nanjing University, China
<br class="ltx_break"/><math alttext="\left\{\right." class="ltx_Math" display="inline" id="id11.11.m11.1"><semantics id="id11.11.m11.1a"><mo id="id11.11.m11.1.1" xref="id11.11.m11.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="id11.11.m11.1b"><ci id="id11.11.m11.1.1.cmml" xref="id11.11.m11.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="id11.11.m11.1c">\left\{\right.</annotation><annotation encoding="application/x-llamapun" id="id11.11.m11.1d">{</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="id12.12.1" style="font-size:90%;">ghy0623,fengqiao,jhl,lxz,yinxiangjun,yjy,lik<math alttext="\left\}\right." class="ltx_Math" display="inline" id="id12.12.1.m1.1"><semantics id="id12.12.1.m1.1a"><mo id="id12.12.1.m1.1.1" mathvariant="normal" stretchy="true" xref="id12.12.1.m1.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="id12.12.1.m1.1b"><ci id="id12.12.1.m1.1.1.cmml" xref="id12.12.1.m1.1.1">normal-}</ci></annotation-xml><annotation encoding="application/x-tex" id="id12.12.1.m1.1c">\left\}\right.</annotation><annotation encoding="application/x-llamapun" id="id12.12.1.m1.1d">}</annotation></semantics></math>@tju.edu.cn</span> ‚ÄÉ<span class="ltx_text ltx_font_typewriter" id="id13.13.id1" style="font-size:90%;">zhouyou@nju.edu.cn</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">Human pose and shape (HPS) estimation with lensless imaging is not only beneficial to privacy protection but also can be used in covert surveillance scenarios due to the small size and simple structure of this device.
However, this task presents significant challenges due to the inherent ambiguity of the captured measurements and lacks effective methods for directly estimating human pose and shape from lensless data.
In this paper, we propose the first end-to-end framework to recover 3D human poses and shapes from lensless measurements to our knowledge.
We specifically design a multi-scale lensless feature decoder to decode the lensless measurements through the optically encoded mask for efficient feature extraction.
We also propose a double-head auxiliary supervision mechanism to improve the estimation accuracy of human limb ends.
Besides, we establish a lensless imaging system and verify the effectiveness of our method on various datasets acquired by our lensless imaging system. The code and dataset are available at <a class="ltx_ref ltx_href" href="https://cic.tju.edu.cn/faculty/likun/projects/LPSNet" title="">https://cic.tju.edu.cn/faculty/likun/projects/LPSNet</a>.</p>
</div>
<div class="ltx_para ltx_align_center" id="p2">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="315" id="p2.g1" src="extracted/5523510/image/head6.png" width="685"/>
</div>
<figure class="ltx_figure ltx_align_center" id="S0.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S0.F1.4.2" style="font-size:90%;">LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging.<span class="ltx_text ltx_font_medium" id="S0.F1.4.2.1"> We contribute a framework for estimating human poses and shapes from individual lensless measurements. The first row shows the input measurements acquired by our lensless imaging system, the second row shows the estimated human poses and shapes from lensless measurements, and the bottom row shows the 3D results shown in different views.</span></span></figcaption>
</figure><span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">footnotetext: </span><math alttext="\dagger" class="ltx_Math" display="inline" id="footnotex1.m1.1"><semantics id="footnotex1.m1.1b"><mo id="footnotex1.m1.1.1" xref="footnotex1.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="footnotex1.m1.1c"><ci id="footnotex1.m1.1.1.cmml" xref="footnotex1.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex1.m1.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="footnotex1.m1.1e">‚Ä†</annotation></semantics></math> Equal contribution.</span></span></span><span class="ltx_note ltx_role_footnotetext" id="footnotex2"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">footnotetext: </span>* Corresponding author.</span></span></span>
<section class="ltx_section ltx_indent_first" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, lensless imaging technologies¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> have advanced significantly due to their many advantages, such as privacy protection, smaller size, simple structure, and lower cost.
3D human pose and shape (HPS) estimation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> requires miniaturized and lightweight imaging system as application scenarios become more diverse.
All these advantages, especially privacy, make the lensless imaging system very suitable as an imaging device for human pose and shape estimation.
In this work, we propose LPSNet, which aims to estimate 3D human pose and shape from lensless measurements instead of RGB images, achieving cheaper and privacy-protecting 3D human pose and shape estimation.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">A thin, lightweight, and potentially cost-effective optical encoder is used in a lensless imaging system instead of traditional cameras with lenses, while others are expensive, rigid, and occupy more space.
At this stage, the application of a lensless imaging system is extensive, it is mainly used in the field of microscopic imaging, RGB image reconstruction, and so on.
More valuable information can be obtained from lensless measurements due to the special optical encoding method of lensless imaging systems.
Directly estimating human pose and shape from lensless measurements is not currently possible.
The first step is to reconstruct an RGB image from a lensless measurement and then estimate the human pose and shape from the RGB images.
However, experimental findings indicate that the reconstructed RGB images are of suboptimal quality, resulting in incomplete local features and significant deviations in the position of the human body.
Combining these factors leads to inaccurate human pose estimation when using lensless measurements to reconstruct RGB images.
This approach has computational burden and computational resources, making it very unfavorable for deployment at the endpoint.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, We aim to advance human pose and shape estimation using a lensless imaging system, which needs to overcome two main challenges. First, how to extract features from lensless measurements for human pose and shape estimation.
Secondly, during early experiments, when using features extracted from lensless measurements to estimate human pose and shape, we found poor estimation accuracy of human limbs.
</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address these challenges, we introduce LPSNet, the first end-to-end human pose and shape estimation framework with lensless imaging.
To extract features from optically encoded lensless measurements, we propose a multi-scale lensless feature decoder (MSFDecoder).
Specifically, we introduce a global perception layer to enhance the global decoding capability of MSFDecoder.
The global information that has been optically encoded to the global can be efficiently decoded to obtain a feature map that can be used in subsequent processes.
To improve pose and shape estimation, we propose a Double-Head Auxiliary Supervision(DHAS) mechanism to be implemented during training.
Auxiliary supervision can improve the estimation accuracy of human limbs and correct results with large deviations.
</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our main contributions can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span>
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1">We propose LPSNet, an end-to-end pose and shape estimation network for lensless imaging systems.
This is the first work to estimate human poses and shapes directly from lensless measurements.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span>
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1">We propose MSFDecoder, a Multi-Scale Lensless Feature Decoder that decodes and extracts features from lensless measurements, which can be receptive to global features in lensless measurements.
</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span>
<div class="ltx_para" id="S1.I1.ix3.p1">
<p class="ltx_p" id="S1.I1.ix3.p1.1">We propose a Double-Head Auxiliary Supervision mechanism for both pose and shape estimation, which can improve the estimation accuracy of human limbs.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section ltx_indent_first" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection ltx_indent_first" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Lensless Imaging System</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">A conventional photographic camera typically comprises a focusing lens, which may consist of one or more optical elements and an image sensor positioned at or near the focal length of the lens. The lens in such a camera directs the projected light from the observed scene onto the sensor, aiming to accurately map specific scene points to individual pixels on the sensor.
Conversely, in a lensless imaging system, the absence of a lens defines its configuration. Instead, an optical modulator, such as a coded amplitude mask or a diffuser, is positioned between the scene and the image sensor, often close to the sensor itself. As a result, the recorded data deviates significantly from the expected RGB image during imaging. In this process, the local information of the object transforms overlapping global information through the optically encoded mask.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Within the mask-modulated lensless systems, a fixed optical mask is introduced to create a versatile lensless system that can work for a wide range of object distances and lighting scenarios, whether passive or uncontrolled.
The mask modulates the incoming light and generates a measurement that can be decoded through computational methodologies.
Mask-modulated lensless imaging systems were used to perform 2D imaging¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>, refocusing¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, 3D imaging¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, and microscopic imaging¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Generally speaking, amplitude modulators and phase modulators¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> are the two types of optical masks used in lensless imaging systems.
Phase modulators can be further sub-categorized into phase gratings¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, diffusers¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, and phase masks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. One key characteristic of a mask-modulated lensless system is the pattern the mask produces on the sensor for a point light source in the scene. We call this pattern the Point-Spread Function(PSF), and its properties determine the imaging model of the system.
As shown in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S2.F2" title="Figure 2 ‚Ä£ 2.1 Lensless Imaging System ‚Ä£ 2 Related Work ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>, we design a simple mask-modulated lensless system for data acquisition in this experiment.
The lensless imaging system designed in our experiments chose a diffuser as the mask.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">The main advantages of lensless imaging are as follows: lensless is small in size and can be assembled in a variety of miniaturized equipment; lensless is light and easy to carry; lensless cameras are still cheap to make; lensless cameras have a wide view field, larger than traditional wide-angle cameras; It can also protect the user‚Äôs privacy to a great extent.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="S2.F2.g1" src="extracted/5523510/image/3.2.2.png" width="586"/>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">The workflow of the lensless imaging system and the final measurement obtained is the result obtained from the encoding of 3D scene information by the lensless imaging system.
The optically encoded mask transforms local information in the 3D scene into overlapping global information.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection ltx_indent_first" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Human Pose and Shape Recovery</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">There are two primary methods for human pose and shape estimation: optimization-based approaches and regression-based approaches.
Numerous methodologies have been developed to estimate the three-dimensional human pose and shape by employing iterative optimization techniques, <span class="ltx_text ltx_font_slanted" id="S2.SS2.p1.1.1">e.g.</span>, SMPLify¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> and variations of the SMPLify¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Many existing Regression-based methods follow the architecture of HMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, which uses a pre-trained backbone to extract image features followed by regression to obtain SMPL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> parameters.
Many improvements to the original method have been proposed since its introduction.
In particular, many papers have proposed alternative methods for pseudo-ground truth generation, including using temporal information¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, multiple views¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, or iterative optimization¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>. SPIN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> proposed an in-the-loop optimization that incorporated SMPLify¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> in the HMR training, which combines the optimization-based approach and the regression-based approach for human pose and shape estimation.
PyMAF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> proposes a pyramidal mesh alignment feedback loop for regression of SMPL parameters.
Our approach references the design of PyMAF in the part of the SMPL parameters regression.
PARE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> proposes a body-part-guided attention mechanism for better occlusion handling.
HKMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> performs a prediction guided by the hierarchical structure of SMPL.
HMR2.0¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> employs a large training dataset and proposes a fully transformer-based approach for 3D human pose and shape estimation from a single image.
Many related approaches make non-parametric predictions, i.e., instead of estimating the parameters of the SMPL model, they explicitly regress the vertices of the mesh. GraphCMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> uses a graph neural network for the prediction, METRO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> and FastMETRO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> use a transformer, while Mesh Graphormer ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> adopts a hybrid between the two.
</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In this paper, we propose the first work to perform end-to-end human pose and shape estimation from lensless measurements.
We designed a multi-scale lensless feature decoder to decode the lensless measurements based on a mask-encoder to obtain more efficient features.
We also propose a double auxiliary supervision mechanism to improve the estimation accuracy of human limbs.</p>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<section class="ltx_subsection ltx_indent_first" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our work focuses on human pose and shape estimation from lensless measurements. In this section, we present the technical details of our approach.
As depicted in <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S3.F3" title="Figure 3 ‚Ä£ 3.1 Overview ‚Ä£ 3 Method ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>, the core of our method involves the following three components:
1) a Multi-Scale Lensless Feature Decoder(MSFDecoder) (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S3.SS2" title="3.2 Multi-Scale Lensless Feature Decoder ‚Ä£ 3 Method ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">3.2</span></a>) that can effectively decode the information encoded by the lensless imaging system;
2) a human parametric model regressor (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S3.SS3" title="3.3 Human Parametric Model Regression ‚Ä£ 3 Method ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">3.3</span></a>) that takes the multi-scale features produced by MSFDecoder as input and predicts the SMPL parameters;
3) a Double-Head Auxiliary Supervision mechanism(DHAS) (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S3.SS4" title="3.4 Double-Head Auxiliary Supervision ‚Ä£ 3 Method ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">3.4</span></a>) that can assist the LPSNet to improve the estimation accuracy of human limbs.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="274" id="S3.F3.g1" src="extracted/5523510/image/3.1.10.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.5.2.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F3.2.1" style="font-size:90%;">Overview of the proposed framework.<span class="ltx_text ltx_font_medium" id="S3.F3.2.1.1">
A measurement <math alttext="M" class="ltx_Math" display="inline" id="S3.F3.2.1.1.m1.1"><semantics id="S3.F3.2.1.1.m1.1b"><mi id="S3.F3.2.1.1.m1.1.1" xref="S3.F3.2.1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.1.1.m1.1c"><ci id="S3.F3.2.1.1.m1.1.1.cmml" xref="S3.F3.2.1.1.m1.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.1.1.m1.1d">M</annotation><annotation encoding="application/x-llamapun" id="S3.F3.2.1.1.m1.1e">italic_M</annotation></semantics></math> is passed through a Multi-Scale Lensless Feature Decoder to get spatial characteristics at different scales.
These feature maps are fed into the regressor for human pose and shape estimation.
Also, these feature maps are fed into the Double-Head Auxiliary Supervision to improve the estimation accuracy.
</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-Scale Lensless Feature Decoder</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">As illustrated in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S2.F2" title="Figure 2 ‚Ä£ 2.1 Lensless Imaging System ‚Ä£ 2 Related Work ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>, the optical information of the subject is diffused by the mask and projected onto the sensor. The process of obtaining measurements through the lensless imaging system can be treated as encoding optical information in the scene into lensless measurements.
The goal of our lensless feature decoder <math alttext="D_{M}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ùê∑</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">D_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> is to decode these lensless measurements to multi-scale features, which are then utilized for subsequent human body prediction.
The design of the global perception layer <math alttext="G_{P}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">G</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ùê∫</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">ùëÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">G_{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_G start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT</annotation></semantics></math> is inspired by HRNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>.
The global perception layer inherits many of the advantages of HRNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> and can always maintain a high resolution, while the information interaction between different branches complements the information loss caused by the reduction in the number of channels.
These advantages are important for extracting features from lensless measurements.
Then, we design different convolutional layers for different scales of features that are used to refine the features.
</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.5">Formally, the decoder takes a lensless measurement <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_M</annotation></semantics></math> as input and decodes the information encoded by the lensless imaging system.
The global perception layer ensures the integrity of global information for each pixel by maintaining the high-resolution convolution branch and the low-resolution convolution branch in parallel. It further enhances the information extraction accuracy by repeating the multi-scale fusion across parallel convolutions.
As shown in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S3.F3" title="Figure 3 ‚Ä£ 3.1 Overview ‚Ä£ 3 Method ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>, the global perception layer takes <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_M</annotation></semantics></math> as input and generates a set of multi-scale spatial features <math alttext="\bm{\phi}^{t=0,1,2,3}_{m}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.4"><semantics id="S3.SS2.p2.3.m3.4a"><msubsup id="S3.SS2.p2.3.m3.4.5" xref="S3.SS2.p2.3.m3.4.5.cmml"><mi id="S3.SS2.p2.3.m3.4.5.2.2" mathvariant="bold-italic" xref="S3.SS2.p2.3.m3.4.5.2.2.cmml">œï</mi><mi id="S3.SS2.p2.3.m3.4.5.3" xref="S3.SS2.p2.3.m3.4.5.3.cmml">m</mi><mrow id="S3.SS2.p2.3.m3.4.4.4" xref="S3.SS2.p2.3.m3.4.4.4.cmml"><mi id="S3.SS2.p2.3.m3.4.4.4.6" xref="S3.SS2.p2.3.m3.4.4.4.6.cmml">t</mi><mo id="S3.SS2.p2.3.m3.4.4.4.5" xref="S3.SS2.p2.3.m3.4.4.4.5.cmml">=</mo><mrow id="S3.SS2.p2.3.m3.4.4.4.7.2" xref="S3.SS2.p2.3.m3.4.4.4.7.1.cmml"><mn id="S3.SS2.p2.3.m3.1.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.1.cmml">0</mn><mo id="S3.SS2.p2.3.m3.4.4.4.7.2.1" xref="S3.SS2.p2.3.m3.4.4.4.7.1.cmml">,</mo><mn id="S3.SS2.p2.3.m3.2.2.2.2" xref="S3.SS2.p2.3.m3.2.2.2.2.cmml">1</mn><mo id="S3.SS2.p2.3.m3.4.4.4.7.2.2" xref="S3.SS2.p2.3.m3.4.4.4.7.1.cmml">,</mo><mn id="S3.SS2.p2.3.m3.3.3.3.3" xref="S3.SS2.p2.3.m3.3.3.3.3.cmml">2</mn><mo id="S3.SS2.p2.3.m3.4.4.4.7.2.3" xref="S3.SS2.p2.3.m3.4.4.4.7.1.cmml">,</mo><mn id="S3.SS2.p2.3.m3.4.4.4.4" xref="S3.SS2.p2.3.m3.4.4.4.4.cmml">3</mn></mrow></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.4b"><apply id="S3.SS2.p2.3.m3.4.5.cmml" xref="S3.SS2.p2.3.m3.4.5"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.4.5.1.cmml" xref="S3.SS2.p2.3.m3.4.5">subscript</csymbol><apply id="S3.SS2.p2.3.m3.4.5.2.cmml" xref="S3.SS2.p2.3.m3.4.5"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.4.5.2.1.cmml" xref="S3.SS2.p2.3.m3.4.5">superscript</csymbol><ci id="S3.SS2.p2.3.m3.4.5.2.2.cmml" xref="S3.SS2.p2.3.m3.4.5.2.2">bold-italic-œï</ci><apply id="S3.SS2.p2.3.m3.4.4.4.cmml" xref="S3.SS2.p2.3.m3.4.4.4"><eq id="S3.SS2.p2.3.m3.4.4.4.5.cmml" xref="S3.SS2.p2.3.m3.4.4.4.5"></eq><ci id="S3.SS2.p2.3.m3.4.4.4.6.cmml" xref="S3.SS2.p2.3.m3.4.4.4.6">ùë°</ci><list id="S3.SS2.p2.3.m3.4.4.4.7.1.cmml" xref="S3.SS2.p2.3.m3.4.4.4.7.2"><cn id="S3.SS2.p2.3.m3.1.1.1.1.cmml" type="integer" xref="S3.SS2.p2.3.m3.1.1.1.1">0</cn><cn id="S3.SS2.p2.3.m3.2.2.2.2.cmml" type="integer" xref="S3.SS2.p2.3.m3.2.2.2.2">1</cn><cn id="S3.SS2.p2.3.m3.3.3.3.3.cmml" type="integer" xref="S3.SS2.p2.3.m3.3.3.3.3">2</cn><cn id="S3.SS2.p2.3.m3.4.4.4.4.cmml" type="integer" xref="S3.SS2.p2.3.m3.4.4.4.4">3</cn></list></apply></apply><ci id="S3.SS2.p2.3.m3.4.5.3.cmml" xref="S3.SS2.p2.3.m3.4.5.3">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.4c">\bm{\phi}^{t=0,1,2,3}_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.4d">bold_italic_œï start_POSTSUPERSCRIPT italic_t = 0 , 1 , 2 , 3 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> as input for the subsequent steps.
The final output <math alttext="\bm{\phi}^{t}_{M}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><msubsup id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2.2" mathvariant="bold-italic" xref="S3.SS2.p2.4.m4.1.1.2.2.cmml">œï</mi><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">M</mi><mi id="S3.SS2.p2.4.m4.1.1.2.3" xref="S3.SS2.p2.4.m4.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><apply id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2.2">bold-italic-œï</ci><ci id="S3.SS2.p2.4.m4.1.1.2.3.cmml" xref="S3.SS2.p2.4.m4.1.1.2.3">ùë°</ci></apply><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\bm{\phi}^{t}_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">bold_italic_œï start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> of the decoder is obtained by passing <math alttext="\bm{\phi}^{t}_{m}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><msubsup id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2.2" mathvariant="bold-italic" xref="S3.SS2.p2.5.m5.1.1.2.2.cmml">œï</mi><mi id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">m</mi><mi id="S3.SS2.p2.5.m5.1.1.2.3" xref="S3.SS2.p2.5.m5.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><apply id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1">superscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2.2">bold-italic-œï</ci><ci id="S3.SS2.p2.5.m5.1.1.2.3.cmml" xref="S3.SS2.p2.5.m5.1.1.2.3">ùë°</ci></apply><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\bm{\phi}^{t}_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">bold_italic_œï start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> into different convolutional layers,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\phi^{t}_{M}=D_{M}\left({M}\right)." class="ltx_Math" display="block" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><msubsup id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.cmml">œï</mi><mi id="S3.E1.m1.2.2.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.3.cmml">M</mi><mi id="S3.E1.m1.2.2.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">t</mi></msubsup><mo id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml"><msub id="S3.E1.m1.2.2.1.1.3.2" xref="S3.E1.m1.2.2.1.1.3.2.cmml"><mi id="S3.E1.m1.2.2.1.1.3.2.2" xref="S3.E1.m1.2.2.1.1.3.2.2.cmml">D</mi><mi id="S3.E1.m1.2.2.1.1.3.2.3" xref="S3.E1.m1.2.2.1.1.3.2.3.cmml">M</mi></msub><mo id="S3.E1.m1.2.2.1.1.3.1" xref="S3.E1.m1.2.2.1.1.3.1.cmml">‚Å¢</mo><mrow id="S3.E1.m1.2.2.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.3.cmml"><mo id="S3.E1.m1.2.2.1.1.3.3.2.1" xref="S3.E1.m1.2.2.1.1.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">M</mi><mo id="S3.E1.m1.2.2.1.1.3.3.2.2" xref="S3.E1.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" lspace="0em" xref="S3.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"></eq><apply id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2">superscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2">italic-œï</ci><ci id="S3.E1.m1.2.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.3">ùë°</ci></apply><ci id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3">ùëÄ</ci></apply><apply id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"><times id="S3.E1.m1.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1"></times><apply id="S3.E1.m1.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2.2">ùê∑</ci><ci id="S3.E1.m1.2.2.1.1.3.2.3.cmml" xref="S3.E1.m1.2.2.1.1.3.2.3">ùëÄ</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ùëÄ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\phi^{t}_{M}=D_{M}\left({M}\right).</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">italic_œï start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT = italic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ( italic_M ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p2.7">With different scales of decoding, we can obtain feature maps at different granularities, through which we can perceive the location of people in the scene.
The results obtained by the lensless decoder are utilized for subsequent predictions with the SMPL model, incorporating pose, shape, and camera parameters <math alttext="\Theta=\{\bm{\theta},\bm{\beta},\bm{\widetilde{\pi}\}}" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m1.3"><semantics id="S3.SS2.p2.6.m1.3a"><mrow id="S3.SS2.p2.6.m1.3.4" xref="S3.SS2.p2.6.m1.3.4.cmml"><mi id="S3.SS2.p2.6.m1.3.4.2" mathvariant="normal" xref="S3.SS2.p2.6.m1.3.4.2.cmml">Œò</mi><mo id="S3.SS2.p2.6.m1.3.4.1" xref="S3.SS2.p2.6.m1.3.4.1.cmml">=</mo><mrow id="S3.SS2.p2.6.m1.3.4.3.2" xref="S3.SS2.p2.6.m1.3.4.3.1.cmml"><mo id="S3.SS2.p2.6.m1.3.4.3.2.1" stretchy="false" xref="S3.SS2.p2.6.m1.3.4.3.1.cmml">{</mo><mi id="S3.SS2.p2.6.m1.1.1" xref="S3.SS2.p2.6.m1.1.1.cmml">ùúΩ</mi><mo id="S3.SS2.p2.6.m1.3.4.3.2.2" xref="S3.SS2.p2.6.m1.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p2.6.m1.2.2" xref="S3.SS2.p2.6.m1.2.2.cmml">ùú∑</mi><mo id="S3.SS2.p2.6.m1.3.4.3.2.3" xref="S3.SS2.p2.6.m1.3.4.3.1.cmml">,</mo><mover accent="true" id="S3.SS2.p2.6.m1.3.3" xref="S3.SS2.p2.6.m1.3.3.cmml"><mi id="S3.SS2.p2.6.m1.3.3.2" xref="S3.SS2.p2.6.m1.3.3.2.cmml">ùùÖ</mi><mo id="S3.SS2.p2.6.m1.3.3.1" mathvariant="bold" xref="S3.SS2.p2.6.m1.3.3.1.cmml">~</mo></mover><mo id="S3.SS2.p2.6.m1.3.4.3.2.4" mathvariant="bold" stretchy="false" xref="S3.SS2.p2.6.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m1.3b"><apply id="S3.SS2.p2.6.m1.3.4.cmml" xref="S3.SS2.p2.6.m1.3.4"><eq id="S3.SS2.p2.6.m1.3.4.1.cmml" xref="S3.SS2.p2.6.m1.3.4.1"></eq><ci id="S3.SS2.p2.6.m1.3.4.2.cmml" xref="S3.SS2.p2.6.m1.3.4.2">Œò</ci><set id="S3.SS2.p2.6.m1.3.4.3.1.cmml" xref="S3.SS2.p2.6.m1.3.4.3.2"><ci id="S3.SS2.p2.6.m1.1.1.cmml" xref="S3.SS2.p2.6.m1.1.1">ùúΩ</ci><ci id="S3.SS2.p2.6.m1.2.2.cmml" xref="S3.SS2.p2.6.m1.2.2">ùú∑</ci><apply id="S3.SS2.p2.6.m1.3.3.cmml" xref="S3.SS2.p2.6.m1.3.3"><ci id="S3.SS2.p2.6.m1.3.3.1.cmml" xref="S3.SS2.p2.6.m1.3.3.1">bold-~</ci><ci id="S3.SS2.p2.6.m1.3.3.2.cmml" xref="S3.SS2.p2.6.m1.3.3.2">ùùÖ</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m1.3c">\Theta=\{\bm{\theta},\bm{\beta},\bm{\widetilde{\pi}\}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m1.3d">roman_Œò = { bold_italic_Œ∏ , bold_italic_Œ≤ , overbold_~ start_ARG bold_italic_œÄ end_ARG bold_}</annotation></semantics></math>, where <math alttext="\bm{\widetilde{\pi}}" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m2.1"><semantics id="S3.SS2.p2.7.m2.1a"><mover accent="true" id="S3.SS2.p2.7.m2.1.1" xref="S3.SS2.p2.7.m2.1.1.cmml"><mi id="S3.SS2.p2.7.m2.1.1.2" xref="S3.SS2.p2.7.m2.1.1.2.cmml">ùùÖ</mi><mo id="S3.SS2.p2.7.m2.1.1.1" mathvariant="bold" xref="S3.SS2.p2.7.m2.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m2.1b"><apply id="S3.SS2.p2.7.m2.1.1.cmml" xref="S3.SS2.p2.7.m2.1.1"><ci id="S3.SS2.p2.7.m2.1.1.1.cmml" xref="S3.SS2.p2.7.m2.1.1.1">bold-~</ci><ci id="S3.SS2.p2.7.m2.1.1.2.cmml" xref="S3.SS2.p2.7.m2.1.1.2">ùùÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m2.1c">\bm{\widetilde{\pi}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m2.1d">overbold_~ start_ARG bold_italic_œÄ end_ARG</annotation></semantics></math> is pseudo-camera parameters for the subsequent projection calculation.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Human Parametric Model Regression</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.6">Our human parametric model regression is mainly inspired by the improved Human Mesh Regression described in PyMAF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>.
Different from the pyramid features obtained by deconvolution in PyMAF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, our global perception layer exploits HRNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> to maintain more high-resolution information.
We use a set of sampling points <math alttext="S_{t}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">S</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">ùëÜ</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">S_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and project them onto the feature maps to extract point-wise features.
The point-wise features corresponding to each point <math alttext="s" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_s</annotation></semantics></math> in <math alttext="S_{t}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">S</mi><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">ùëÜ</ci><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">S_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> are bilinearly sampled from <math alttext="\bm{\phi}^{t=1,2,3}_{M}" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.3"><semantics id="S3.SS3.p1.4.m4.3a"><msubsup id="S3.SS3.p1.4.m4.3.4" xref="S3.SS3.p1.4.m4.3.4.cmml"><mi id="S3.SS3.p1.4.m4.3.4.2.2" mathvariant="bold-italic" xref="S3.SS3.p1.4.m4.3.4.2.2.cmml">œï</mi><mi id="S3.SS3.p1.4.m4.3.4.3" xref="S3.SS3.p1.4.m4.3.4.3.cmml">M</mi><mrow id="S3.SS3.p1.4.m4.3.3.3" xref="S3.SS3.p1.4.m4.3.3.3.cmml"><mi id="S3.SS3.p1.4.m4.3.3.3.5" xref="S3.SS3.p1.4.m4.3.3.3.5.cmml">t</mi><mo id="S3.SS3.p1.4.m4.3.3.3.4" xref="S3.SS3.p1.4.m4.3.3.3.4.cmml">=</mo><mrow id="S3.SS3.p1.4.m4.3.3.3.6.2" xref="S3.SS3.p1.4.m4.3.3.3.6.1.cmml"><mn id="S3.SS3.p1.4.m4.1.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.1.cmml">1</mn><mo id="S3.SS3.p1.4.m4.3.3.3.6.2.1" xref="S3.SS3.p1.4.m4.3.3.3.6.1.cmml">,</mo><mn id="S3.SS3.p1.4.m4.2.2.2.2" xref="S3.SS3.p1.4.m4.2.2.2.2.cmml">2</mn><mo id="S3.SS3.p1.4.m4.3.3.3.6.2.2" xref="S3.SS3.p1.4.m4.3.3.3.6.1.cmml">,</mo><mn id="S3.SS3.p1.4.m4.3.3.3.3" xref="S3.SS3.p1.4.m4.3.3.3.3.cmml">3</mn></mrow></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.3b"><apply id="S3.SS3.p1.4.m4.3.4.cmml" xref="S3.SS3.p1.4.m4.3.4"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.3.4.1.cmml" xref="S3.SS3.p1.4.m4.3.4">subscript</csymbol><apply id="S3.SS3.p1.4.m4.3.4.2.cmml" xref="S3.SS3.p1.4.m4.3.4"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.3.4.2.1.cmml" xref="S3.SS3.p1.4.m4.3.4">superscript</csymbol><ci id="S3.SS3.p1.4.m4.3.4.2.2.cmml" xref="S3.SS3.p1.4.m4.3.4.2.2">bold-italic-œï</ci><apply id="S3.SS3.p1.4.m4.3.3.3.cmml" xref="S3.SS3.p1.4.m4.3.3.3"><eq id="S3.SS3.p1.4.m4.3.3.3.4.cmml" xref="S3.SS3.p1.4.m4.3.3.3.4"></eq><ci id="S3.SS3.p1.4.m4.3.3.3.5.cmml" xref="S3.SS3.p1.4.m4.3.3.3.5">ùë°</ci><list id="S3.SS3.p1.4.m4.3.3.3.6.1.cmml" xref="S3.SS3.p1.4.m4.3.3.3.6.2"><cn id="S3.SS3.p1.4.m4.1.1.1.1.cmml" type="integer" xref="S3.SS3.p1.4.m4.1.1.1.1">1</cn><cn id="S3.SS3.p1.4.m4.2.2.2.2.cmml" type="integer" xref="S3.SS3.p1.4.m4.2.2.2.2">2</cn><cn id="S3.SS3.p1.4.m4.3.3.3.3.cmml" type="integer" xref="S3.SS3.p1.4.m4.3.3.3.3">3</cn></list></apply></apply><ci id="S3.SS3.p1.4.m4.3.4.3.cmml" xref="S3.SS3.p1.4.m4.3.4.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.3c">\bm{\phi}^{t=1,2,3}_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.3d">bold_italic_œï start_POSTSUPERSCRIPT italic_t = 1 , 2 , 3 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math>.
(Note that <math alttext="\bm{\phi}^{t=0}_{M}" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><msubsup id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2.2" mathvariant="bold-italic" xref="S3.SS3.p1.5.m5.1.1.2.2.cmml">œï</mi><mi id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml">M</mi><mrow id="S3.SS3.p1.5.m5.1.1.2.3" xref="S3.SS3.p1.5.m5.1.1.2.3.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2.3.2" xref="S3.SS3.p1.5.m5.1.1.2.3.2.cmml">t</mi><mo id="S3.SS3.p1.5.m5.1.1.2.3.1" xref="S3.SS3.p1.5.m5.1.1.2.3.1.cmml">=</mo><mn id="S3.SS3.p1.5.m5.1.1.2.3.3" xref="S3.SS3.p1.5.m5.1.1.2.3.3.cmml">0</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">subscript</csymbol><apply id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.2.1.cmml" xref="S3.SS3.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.2.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2.2">bold-italic-œï</ci><apply id="S3.SS3.p1.5.m5.1.1.2.3.cmml" xref="S3.SS3.p1.5.m5.1.1.2.3"><eq id="S3.SS3.p1.5.m5.1.1.2.3.1.cmml" xref="S3.SS3.p1.5.m5.1.1.2.3.1"></eq><ci id="S3.SS3.p1.5.m5.1.1.2.3.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2.3.2">ùë°</ci><cn id="S3.SS3.p1.5.m5.1.1.2.3.3.cmml" type="integer" xref="S3.SS3.p1.5.m5.1.1.2.3.3">0</cn></apply></apply><ci id="S3.SS3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">\bm{\phi}^{t=0}_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">bold_italic_œï start_POSTSUPERSCRIPT italic_t = 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> is unnecessary here and is used as input for subsequent initialization.)
Then the dimensions of these features will be reduced by MLP(multi-layer perceptron) and concatenated as a feature vector <math alttext="\bm{\hat{\phi}}^{t}_{M}" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><msubsup id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><mover accent="true" id="S3.SS3.p1.6.m6.1.1.2.2" xref="S3.SS3.p1.6.m6.1.1.2.2.cmml"><mi id="S3.SS3.p1.6.m6.1.1.2.2.2" mathvariant="bold-italic" xref="S3.SS3.p1.6.m6.1.1.2.2.2.cmml">œï</mi><mo id="S3.SS3.p1.6.m6.1.1.2.2.1" mathvariant="bold" xref="S3.SS3.p1.6.m6.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml">M</mi><mi id="S3.SS3.p1.6.m6.1.1.2.3" xref="S3.SS3.p1.6.m6.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">subscript</csymbol><apply id="S3.SS3.p1.6.m6.1.1.2.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.2.1.cmml" xref="S3.SS3.p1.6.m6.1.1">superscript</csymbol><apply id="S3.SS3.p1.6.m6.1.1.2.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2.2"><ci id="S3.SS3.p1.6.m6.1.1.2.2.1.cmml" xref="S3.SS3.p1.6.m6.1.1.2.2.1">bold-^</ci><ci id="S3.SS3.p1.6.m6.1.1.2.2.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2.2.2">bold-italic-œï</ci></apply><ci id="S3.SS3.p1.6.m6.1.1.2.3.cmml" xref="S3.SS3.p1.6.m6.1.1.2.3">ùë°</ci></apply><ci id="S3.SS3.p1.6.m6.1.1.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">\bm{\hat{\phi}}^{t}_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">overbold_^ start_ARG bold_italic_œï end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> which can be formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\hat{\phi}^{t}_{M}&amp;=\mathcal{F}\left(\phi^{t}_{M},S_{t}\right)\\
&amp;=\oplus\left\{f\left(\phi_{M}^{t}(s)\right),\text{ for }s\text{ in }S_{t}%
\right\},\end{split}" class="ltx_Math" display="block" id="S3.E2.m1.37"><semantics id="S3.E2.m1.37a"><mtable columnspacing="0pt" displaystyle="true" id="S3.E2.m1.37.37.4" rowspacing="0pt"><mtr id="S3.E2.m1.37.37.4a"><mtd class="ltx_align_right" columnalign="right" id="S3.E2.m1.37.37.4b"><msubsup id="S3.E2.m1.3.3.3.3.3"><mover accent="true" id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">œï</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">^</mo></mover><mi id="S3.E2.m1.3.3.3.3.3.3.1" xref="S3.E2.m1.3.3.3.3.3.3.1.cmml">M</mi><mi id="S3.E2.m1.2.2.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.2.2.1.cmml">t</mi></msubsup></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.37.37.4c"><mrow id="S3.E2.m1.36.36.3.35.15.12"><mi id="S3.E2.m1.36.36.3.35.15.12.13" xref="S3.E2.m1.34.34.1.1.1.cmml"></mi><mo id="S3.E2.m1.4.4.4.4.1.1" xref="S3.E2.m1.4.4.4.4.1.1.cmml">=</mo><mrow id="S3.E2.m1.36.36.3.35.15.12.12"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.5.5.5.5.2.2" xref="S3.E2.m1.5.5.5.5.2.2.cmml">‚Ñ±</mi><mo id="S3.E2.m1.36.36.3.35.15.12.12.3" xref="S3.E2.m1.34.34.1.1.1.cmml">‚Å¢</mo><mrow id="S3.E2.m1.36.36.3.35.15.12.12.2.2"><mo id="S3.E2.m1.6.6.6.6.3.3" xref="S3.E2.m1.34.34.1.1.1.cmml">(</mo><msubsup id="S3.E2.m1.35.35.2.34.14.11.11.1.1.1"><mi id="S3.E2.m1.7.7.7.7.4.4" xref="S3.E2.m1.7.7.7.7.4.4.cmml">œï</mi><mi id="S3.E2.m1.9.9.9.9.6.6.1" xref="S3.E2.m1.9.9.9.9.6.6.1.cmml">M</mi><mi id="S3.E2.m1.8.8.8.8.5.5.1" xref="S3.E2.m1.8.8.8.8.5.5.1.cmml">t</mi></msubsup><mo id="S3.E2.m1.10.10.10.10.7.7" xref="S3.E2.m1.34.34.1.1.1.cmml">,</mo><msub id="S3.E2.m1.36.36.3.35.15.12.12.2.2.2"><mi id="S3.E2.m1.11.11.11.11.8.8" xref="S3.E2.m1.11.11.11.11.8.8.cmml">S</mi><mi id="S3.E2.m1.12.12.12.12.9.9.1" xref="S3.E2.m1.12.12.12.12.9.9.1.cmml">t</mi></msub><mo id="S3.E2.m1.13.13.13.13.10.10" xref="S3.E2.m1.34.34.1.1.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E2.m1.37.37.4d"><mtd id="S3.E2.m1.37.37.4e" xref="S3.E2.m1.34.34.1.1.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.37.37.4f"><mrow id="S3.E2.m1.37.37.4.36.21.21.21"><mrow id="S3.E2.m1.37.37.4.36.21.21.21.1"><mi id="S3.E2.m1.37.37.4.36.21.21.21.1.3" xref="S3.E2.m1.34.34.1.1.1.cmml"></mi><mo id="S3.E2.m1.14.14.14.1.1.1" rspace="0.1389em" xref="S3.E2.m1.14.14.14.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.37.37.4.36.21.21.21.1.2"><mo id="S3.E2.m1.37.37.4.36.21.21.21.1.2a" lspace="0.1389em" rspace="0em" xref="S3.E2.m1.34.34.1.1.1.cmml">‚äï</mo><mrow id="S3.E2.m1.37.37.4.36.21.21.21.1.2.2.2"><mo id="S3.E2.m1.16.16.16.3.3.3" xref="S3.E2.m1.34.34.1.1.1.cmml">{</mo><mrow id="S3.E2.m1.37.37.4.36.21.21.21.1.1.1.1.1"><mi id="S3.E2.m1.17.17.17.4.4.4" xref="S3.E2.m1.17.17.17.4.4.4.cmml">f</mi><mo id="S3.E2.m1.37.37.4.36.21.21.21.1.1.1.1.1.2" xref="S3.E2.m1.34.34.1.1.1.cmml">‚Å¢</mo><mrow id="S3.E2.m1.37.37.4.36.21.21.21.1.1.1.1.1.1.1"><mo id="S3.E2.m1.18.18.18.5.5.5" xref="S3.E2.m1.34.34.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.37.37.4.36.21.21.21.1.1.1.1.1.1.1.1"><msubsup id="S3.E2.m1.37.37.4.36.21.21.21.1.1.1.1.1.1.1.1.2"><mi id="S3.E2.m1.19.19.19.6.6.6" xref="S3.E2.m1.19.19.19.6.6.6.cmml">œï</mi><mi id="S3.E2.m1.20.20.20.7.7.7.1" xref="S3.E2.m1.20.20.20.7.7.7.1.cmml">M</mi><mi id="S3.E2.m1.21.21.21.8.8.8.1" xref="S3.E2.m1.21.21.21.8.8.8.1.cmml">t</mi></msubsup><mo id="S3.E2.m1.37.37.4.36.21.21.21.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.34.34.1.1.1.cmml">‚Å¢</mo><mrow id="S3.E2.m1.37.37.4.36.21.21.21.1.1.1.1.1.1.1.1.3"><mo id="S3.E2.m1.22.22.22.9.9.9" stretchy="false" xref="S3.E2.m1.34.34.1.1.1.cmml">(</mo><mi id="S3.E2.m1.23.23.23.10.10.10" xref="S3.E2.m1.23.23.23.10.10.10.cmml">s</mi><mo id="S3.E2.m1.24.24.24.11.11.11" stretchy="false" xref="S3.E2.m1.34.34.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.25.25.25.12.12.12" xref="S3.E2.m1.34.34.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.26.26.26.13.13.13" xref="S3.E2.m1.34.34.1.1.1.cmml">,</mo><mrow id="S3.E2.m1.37.37.4.36.21.21.21.1.2.2.2.2"><mtext id="S3.E2.m1.27.27.27.14.14.14" xref="S3.E2.m1.27.27.27.14.14.14a.cmml">¬†for¬†</mtext><mo id="S3.E2.m1.37.37.4.36.21.21.21.1.2.2.2.2.1" xref="S3.E2.m1.34.34.1.1.1.cmml">‚Å¢</mo><mi id="S3.E2.m1.28.28.28.15.15.15" xref="S3.E2.m1.28.28.28.15.15.15.cmml">s</mi><mo id="S3.E2.m1.37.37.4.36.21.21.21.1.2.2.2.2.1a" xref="S3.E2.m1.34.34.1.1.1.cmml">‚Å¢</mo><mtext id="S3.E2.m1.29.29.29.16.16.16" xref="S3.E2.m1.29.29.29.16.16.16a.cmml">¬†in¬†</mtext><mo id="S3.E2.m1.37.37.4.36.21.21.21.1.2.2.2.2.1b" xref="S3.E2.m1.34.34.1.1.1.cmml">‚Å¢</mo><msub id="S3.E2.m1.37.37.4.36.21.21.21.1.2.2.2.2.2"><mi id="S3.E2.m1.30.30.30.17.17.17" xref="S3.E2.m1.30.30.30.17.17.17.cmml">S</mi><mi id="S3.E2.m1.31.31.31.18.18.18.1" xref="S3.E2.m1.31.31.31.18.18.18.1.cmml">t</mi></msub></mrow><mo id="S3.E2.m1.32.32.32.19.19.19" xref="S3.E2.m1.34.34.1.1.1.cmml">}</mo></mrow></mrow></mrow><mo id="S3.E2.m1.33.33.33.20.20.20" xref="S3.E2.m1.34.34.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E2.m1.37b"><apply id="S3.E2.m1.34.34.1.1.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><and id="S3.E2.m1.34.34.1.1.1a.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"></and><apply id="S3.E2.m1.34.34.1.1.1b.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><eq id="S3.E2.m1.4.4.4.4.1.1.cmml" xref="S3.E2.m1.4.4.4.4.1.1"></eq><apply id="S3.E2.m1.34.34.1.1.1.6.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><csymbol cd="ambiguous" id="S3.E2.m1.34.34.1.1.1.6.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13">subscript</csymbol><apply id="S3.E2.m1.34.34.1.1.1.6.2.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><csymbol cd="ambiguous" id="S3.E2.m1.34.34.1.1.1.6.2.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><ci id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">^</ci><ci id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">italic-œï</ci></apply><ci id="S3.E2.m1.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.1">ùë°</ci></apply><ci id="S3.E2.m1.3.3.3.3.3.3.1.cmml" xref="S3.E2.m1.3.3.3.3.3.3.1">ùëÄ</ci></apply><apply id="S3.E2.m1.34.34.1.1.1.2.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><times id="S3.E2.m1.34.34.1.1.1.2.3.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"></times><ci id="S3.E2.m1.5.5.5.5.2.2.cmml" xref="S3.E2.m1.5.5.5.5.2.2">‚Ñ±</ci><interval closure="open" id="S3.E2.m1.34.34.1.1.1.2.2.3.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><apply id="S3.E2.m1.34.34.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><csymbol cd="ambiguous" id="S3.E2.m1.34.34.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13">subscript</csymbol><apply id="S3.E2.m1.34.34.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><csymbol cd="ambiguous" id="S3.E2.m1.34.34.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13">superscript</csymbol><ci id="S3.E2.m1.7.7.7.7.4.4.cmml" xref="S3.E2.m1.7.7.7.7.4.4">italic-œï</ci><ci id="S3.E2.m1.8.8.8.8.5.5.1.cmml" xref="S3.E2.m1.8.8.8.8.5.5.1">ùë°</ci></apply><ci id="S3.E2.m1.9.9.9.9.6.6.1.cmml" xref="S3.E2.m1.9.9.9.9.6.6.1">ùëÄ</ci></apply><apply id="S3.E2.m1.34.34.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><csymbol cd="ambiguous" id="S3.E2.m1.34.34.1.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13">subscript</csymbol><ci id="S3.E2.m1.11.11.11.11.8.8.cmml" xref="S3.E2.m1.11.11.11.11.8.8">ùëÜ</ci><ci id="S3.E2.m1.12.12.12.12.9.9.1.cmml" xref="S3.E2.m1.12.12.12.12.9.9.1">ùë°</ci></apply></interval></apply></apply><apply id="S3.E2.m1.34.34.1.1.1c.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><eq id="S3.E2.m1.14.14.14.1.1.1.cmml" xref="S3.E2.m1.14.14.14.1.1.1"></eq><share href="#S3.E2.m1.34.34.1.1.1.2.cmml" id="S3.E2.m1.34.34.1.1.1d.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"></share><apply id="S3.E2.m1.34.34.1.1.1.4.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><csymbol cd="latexml" id="S3.E2.m1.15.15.15.2.2.2.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13">direct-sum</csymbol><set id="S3.E2.m1.34.34.1.1.1.4.2.3.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><apply id="S3.E2.m1.34.34.1.1.1.3.1.1.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><times id="S3.E2.m1.34.34.1.1.1.3.1.1.1.2.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"></times><ci id="S3.E2.m1.17.17.17.4.4.4.cmml" xref="S3.E2.m1.17.17.17.4.4.4">ùëì</ci><apply id="S3.E2.m1.34.34.1.1.1.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><times id="S3.E2.m1.34.34.1.1.1.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"></times><apply id="S3.E2.m1.34.34.1.1.1.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><csymbol cd="ambiguous" id="S3.E2.m1.34.34.1.1.1.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13">superscript</csymbol><apply id="S3.E2.m1.34.34.1.1.1.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><csymbol cd="ambiguous" id="S3.E2.m1.34.34.1.1.1.3.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13">subscript</csymbol><ci id="S3.E2.m1.19.19.19.6.6.6.cmml" xref="S3.E2.m1.19.19.19.6.6.6">italic-œï</ci><ci id="S3.E2.m1.20.20.20.7.7.7.1.cmml" xref="S3.E2.m1.20.20.20.7.7.7.1">ùëÄ</ci></apply><ci id="S3.E2.m1.21.21.21.8.8.8.1.cmml" xref="S3.E2.m1.21.21.21.8.8.8.1">ùë°</ci></apply><ci id="S3.E2.m1.23.23.23.10.10.10.cmml" xref="S3.E2.m1.23.23.23.10.10.10">ùë†</ci></apply></apply><apply id="S3.E2.m1.34.34.1.1.1.4.2.2.2.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><times id="S3.E2.m1.34.34.1.1.1.4.2.2.2.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"></times><ci id="S3.E2.m1.27.27.27.14.14.14a.cmml" xref="S3.E2.m1.27.27.27.14.14.14"><mtext id="S3.E2.m1.27.27.27.14.14.14.cmml" xref="S3.E2.m1.27.27.27.14.14.14">¬†for¬†</mtext></ci><ci id="S3.E2.m1.28.28.28.15.15.15.cmml" xref="S3.E2.m1.28.28.28.15.15.15">ùë†</ci><ci id="S3.E2.m1.29.29.29.16.16.16a.cmml" xref="S3.E2.m1.29.29.29.16.16.16"><mtext id="S3.E2.m1.29.29.29.16.16.16.cmml" xref="S3.E2.m1.29.29.29.16.16.16">¬†in¬†</mtext></ci><apply id="S3.E2.m1.34.34.1.1.1.4.2.2.2.5.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13"><csymbol cd="ambiguous" id="S3.E2.m1.34.34.1.1.1.4.2.2.2.5.1.cmml" xref="S3.E2.m1.36.36.3.35.15.12.13">subscript</csymbol><ci id="S3.E2.m1.30.30.30.17.17.17.cmml" xref="S3.E2.m1.30.30.30.17.17.17">ùëÜ</ci><ci id="S3.E2.m1.31.31.31.18.18.18.1.cmml" xref="S3.E2.m1.31.31.31.18.18.18.1">ùë°</ci></apply></apply></set></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.37c">\begin{split}\hat{\phi}^{t}_{M}&amp;=\mathcal{F}\left(\phi^{t}_{M},S_{t}\right)\\
&amp;=\oplus\left\{f\left(\phi_{M}^{t}(s)\right),\text{ for }s\text{ in }S_{t}%
\right\},\end{split}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.37d">start_ROW start_CELL over^ start_ARG italic_œï end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_CELL start_CELL = caligraphic_F ( italic_œï start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL = ‚äï { italic_f ( italic_œï start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_s ) ) , for italic_s in italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p1.14">where <math alttext="\mathcal{F}(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m1.1"><semantics id="S3.SS3.p1.7.m1.1a"><mrow id="S3.SS3.p1.7.m1.1.2" xref="S3.SS3.p1.7.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.7.m1.1.2.2" xref="S3.SS3.p1.7.m1.1.2.2.cmml">‚Ñ±</mi><mo id="S3.SS3.p1.7.m1.1.2.1" xref="S3.SS3.p1.7.m1.1.2.1.cmml">‚Å¢</mo><mrow id="S3.SS3.p1.7.m1.1.2.3.2" xref="S3.SS3.p1.7.m1.1.2.cmml"><mo id="S3.SS3.p1.7.m1.1.2.3.2.1" stretchy="false" xref="S3.SS3.p1.7.m1.1.2.cmml">(</mo><mo id="S3.SS3.p1.7.m1.1.1" lspace="0em" rspace="0em" xref="S3.SS3.p1.7.m1.1.1.cmml">‚ãÖ</mo><mo id="S3.SS3.p1.7.m1.1.2.3.2.2" stretchy="false" xref="S3.SS3.p1.7.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m1.1b"><apply id="S3.SS3.p1.7.m1.1.2.cmml" xref="S3.SS3.p1.7.m1.1.2"><times id="S3.SS3.p1.7.m1.1.2.1.cmml" xref="S3.SS3.p1.7.m1.1.2.1"></times><ci id="S3.SS3.p1.7.m1.1.2.2.cmml" xref="S3.SS3.p1.7.m1.1.2.2">‚Ñ±</ci><ci id="S3.SS3.p1.7.m1.1.1.cmml" xref="S3.SS3.p1.7.m1.1.1">‚ãÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m1.1c">\mathcal{F}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m1.1d">caligraphic_F ( ‚ãÖ )</annotation></semantics></math> denotes the overall point-wise feature extraction, <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p1.8.m2.1"><semantics id="S3.SS3.p1.8.m2.1a"><mrow id="S3.SS3.p1.8.m2.1.2" xref="S3.SS3.p1.8.m2.1.2.cmml"><mi id="S3.SS3.p1.8.m2.1.2.2" xref="S3.SS3.p1.8.m2.1.2.2.cmml">f</mi><mo id="S3.SS3.p1.8.m2.1.2.1" xref="S3.SS3.p1.8.m2.1.2.1.cmml">‚Å¢</mo><mrow id="S3.SS3.p1.8.m2.1.2.3.2" xref="S3.SS3.p1.8.m2.1.2.cmml"><mo id="S3.SS3.p1.8.m2.1.2.3.2.1" stretchy="false" xref="S3.SS3.p1.8.m2.1.2.cmml">(</mo><mo id="S3.SS3.p1.8.m2.1.1" lspace="0em" rspace="0em" xref="S3.SS3.p1.8.m2.1.1.cmml">‚ãÖ</mo><mo id="S3.SS3.p1.8.m2.1.2.3.2.2" stretchy="false" xref="S3.SS3.p1.8.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m2.1b"><apply id="S3.SS3.p1.8.m2.1.2.cmml" xref="S3.SS3.p1.8.m2.1.2"><times id="S3.SS3.p1.8.m2.1.2.1.cmml" xref="S3.SS3.p1.8.m2.1.2.1"></times><ci id="S3.SS3.p1.8.m2.1.2.2.cmml" xref="S3.SS3.p1.8.m2.1.2.2">ùëì</ci><ci id="S3.SS3.p1.8.m2.1.1.cmml" xref="S3.SS3.p1.8.m2.1.1">‚ãÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m2.1c">f(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.8.m2.1d">italic_f ( ‚ãÖ )</annotation></semantics></math> is the MLP layer, and <math alttext="\oplus" class="ltx_Math" display="inline" id="S3.SS3.p1.9.m3.1"><semantics id="S3.SS3.p1.9.m3.1a"><mo id="S3.SS3.p1.9.m3.1.1" xref="S3.SS3.p1.9.m3.1.1.cmml">‚äï</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m3.1b"><csymbol cd="latexml" id="S3.SS3.p1.9.m3.1.1.cmml" xref="S3.SS3.p1.9.m3.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m3.1c">\oplus</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.9.m3.1d">‚äï</annotation></semantics></math> denotes the concatenation.
We use an iterative mechanism to complete the interaction of different scale features.
Sequentially, a parameter regressor <math alttext="\mathcal{R}_{t}" class="ltx_Math" display="inline" id="S3.SS3.p1.10.m4.1"><semantics id="S3.SS3.p1.10.m4.1a"><msub id="S3.SS3.p1.10.m4.1.1" xref="S3.SS3.p1.10.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.10.m4.1.1.2" xref="S3.SS3.p1.10.m4.1.1.2.cmml">‚Ñõ</mi><mi id="S3.SS3.p1.10.m4.1.1.3" xref="S3.SS3.p1.10.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m4.1b"><apply id="S3.SS3.p1.10.m4.1.1.cmml" xref="S3.SS3.p1.10.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.10.m4.1.1.1.cmml" xref="S3.SS3.p1.10.m4.1.1">subscript</csymbol><ci id="S3.SS3.p1.10.m4.1.1.2.cmml" xref="S3.SS3.p1.10.m4.1.1.2">‚Ñõ</ci><ci id="S3.SS3.p1.10.m4.1.1.3.cmml" xref="S3.SS3.p1.10.m4.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m4.1c">\mathcal{R}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.10.m4.1d">caligraphic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> takes features <math alttext="\hat{\phi}^{t}_{M}" class="ltx_Math" display="inline" id="S3.SS3.p1.11.m5.1"><semantics id="S3.SS3.p1.11.m5.1a"><msubsup id="S3.SS3.p1.11.m5.1.1" xref="S3.SS3.p1.11.m5.1.1.cmml"><mover accent="true" id="S3.SS3.p1.11.m5.1.1.2.2" xref="S3.SS3.p1.11.m5.1.1.2.2.cmml"><mi id="S3.SS3.p1.11.m5.1.1.2.2.2" xref="S3.SS3.p1.11.m5.1.1.2.2.2.cmml">œï</mi><mo id="S3.SS3.p1.11.m5.1.1.2.2.1" xref="S3.SS3.p1.11.m5.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.SS3.p1.11.m5.1.1.3" xref="S3.SS3.p1.11.m5.1.1.3.cmml">M</mi><mi id="S3.SS3.p1.11.m5.1.1.2.3" xref="S3.SS3.p1.11.m5.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m5.1b"><apply id="S3.SS3.p1.11.m5.1.1.cmml" xref="S3.SS3.p1.11.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.11.m5.1.1.1.cmml" xref="S3.SS3.p1.11.m5.1.1">subscript</csymbol><apply id="S3.SS3.p1.11.m5.1.1.2.cmml" xref="S3.SS3.p1.11.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.11.m5.1.1.2.1.cmml" xref="S3.SS3.p1.11.m5.1.1">superscript</csymbol><apply id="S3.SS3.p1.11.m5.1.1.2.2.cmml" xref="S3.SS3.p1.11.m5.1.1.2.2"><ci id="S3.SS3.p1.11.m5.1.1.2.2.1.cmml" xref="S3.SS3.p1.11.m5.1.1.2.2.1">^</ci><ci id="S3.SS3.p1.11.m5.1.1.2.2.2.cmml" xref="S3.SS3.p1.11.m5.1.1.2.2.2">italic-œï</ci></apply><ci id="S3.SS3.p1.11.m5.1.1.2.3.cmml" xref="S3.SS3.p1.11.m5.1.1.2.3">ùë°</ci></apply><ci id="S3.SS3.p1.11.m5.1.1.3.cmml" xref="S3.SS3.p1.11.m5.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m5.1c">\hat{\phi}^{t}_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.11.m5.1d">over^ start_ARG italic_œï end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> and the current parameter estimation <math alttext="\Theta_{t}" class="ltx_Math" display="inline" id="S3.SS3.p1.12.m6.1"><semantics id="S3.SS3.p1.12.m6.1a"><msub id="S3.SS3.p1.12.m6.1.1" xref="S3.SS3.p1.12.m6.1.1.cmml"><mi id="S3.SS3.p1.12.m6.1.1.2" mathvariant="normal" xref="S3.SS3.p1.12.m6.1.1.2.cmml">Œò</mi><mi id="S3.SS3.p1.12.m6.1.1.3" xref="S3.SS3.p1.12.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.12.m6.1b"><apply id="S3.SS3.p1.12.m6.1.1.cmml" xref="S3.SS3.p1.12.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.12.m6.1.1.1.cmml" xref="S3.SS3.p1.12.m6.1.1">subscript</csymbol><ci id="S3.SS3.p1.12.m6.1.1.2.cmml" xref="S3.SS3.p1.12.m6.1.1.2">Œò</ci><ci id="S3.SS3.p1.12.m6.1.1.3.cmml" xref="S3.SS3.p1.12.m6.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.12.m6.1c">\Theta_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.12.m6.1d">roman_Œò start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> as inputs and outputs the parameter residual. Parameters are updated as <math alttext="\Theta_{t+1}" class="ltx_Math" display="inline" id="S3.SS3.p1.13.m7.1"><semantics id="S3.SS3.p1.13.m7.1a"><msub id="S3.SS3.p1.13.m7.1.1" xref="S3.SS3.p1.13.m7.1.1.cmml"><mi id="S3.SS3.p1.13.m7.1.1.2" mathvariant="normal" xref="S3.SS3.p1.13.m7.1.1.2.cmml">Œò</mi><mrow id="S3.SS3.p1.13.m7.1.1.3" xref="S3.SS3.p1.13.m7.1.1.3.cmml"><mi id="S3.SS3.p1.13.m7.1.1.3.2" xref="S3.SS3.p1.13.m7.1.1.3.2.cmml">t</mi><mo id="S3.SS3.p1.13.m7.1.1.3.1" xref="S3.SS3.p1.13.m7.1.1.3.1.cmml">+</mo><mn id="S3.SS3.p1.13.m7.1.1.3.3" xref="S3.SS3.p1.13.m7.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.13.m7.1b"><apply id="S3.SS3.p1.13.m7.1.1.cmml" xref="S3.SS3.p1.13.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.13.m7.1.1.1.cmml" xref="S3.SS3.p1.13.m7.1.1">subscript</csymbol><ci id="S3.SS3.p1.13.m7.1.1.2.cmml" xref="S3.SS3.p1.13.m7.1.1.2">Œò</ci><apply id="S3.SS3.p1.13.m7.1.1.3.cmml" xref="S3.SS3.p1.13.m7.1.1.3"><plus id="S3.SS3.p1.13.m7.1.1.3.1.cmml" xref="S3.SS3.p1.13.m7.1.1.3.1"></plus><ci id="S3.SS3.p1.13.m7.1.1.3.2.cmml" xref="S3.SS3.p1.13.m7.1.1.3.2">ùë°</ci><cn id="S3.SS3.p1.13.m7.1.1.3.3.cmml" type="integer" xref="S3.SS3.p1.13.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.13.m7.1c">\Theta_{t+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.13.m7.1d">roman_Œò start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT</annotation></semantics></math> by adding the residual to <math alttext="\Theta_{t}" class="ltx_Math" display="inline" id="S3.SS3.p1.14.m8.1"><semantics id="S3.SS3.p1.14.m8.1a"><msub id="S3.SS3.p1.14.m8.1.1" xref="S3.SS3.p1.14.m8.1.1.cmml"><mi id="S3.SS3.p1.14.m8.1.1.2" mathvariant="normal" xref="S3.SS3.p1.14.m8.1.1.2.cmml">Œò</mi><mi id="S3.SS3.p1.14.m8.1.1.3" xref="S3.SS3.p1.14.m8.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.14.m8.1b"><apply id="S3.SS3.p1.14.m8.1.1.cmml" xref="S3.SS3.p1.14.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.14.m8.1.1.1.cmml" xref="S3.SS3.p1.14.m8.1.1">subscript</csymbol><ci id="S3.SS3.p1.14.m8.1.1.2.cmml" xref="S3.SS3.p1.14.m8.1.1.2">Œò</ci><ci id="S3.SS3.p1.14.m8.1.1.3.cmml" xref="S3.SS3.p1.14.m8.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.14.m8.1c">\Theta_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.14.m8.1d">roman_Œò start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> which can be formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\Theta_{t+1}=\Theta_{t}+\mathcal{R}_{t}\left(\Theta_{t},\hat{\phi%
}^{t}_{M}\right),\text{ for }t\,&gt;\,0.\end{split}" class="ltx_Math" display="block" id="S3.E3.m1.24"><semantics id="S3.E3.m1.24a"><mtable displaystyle="true" id="S3.E3.m1.24.24.2"><mtr id="S3.E3.m1.24.24.2a"><mtd class="ltx_align_right" columnalign="right" id="S3.E3.m1.24.24.2b"><mrow id="S3.E3.m1.24.24.2.23.23.23.23"><mrow id="S3.E3.m1.24.24.2.23.23.23.23.1"><mrow id="S3.E3.m1.24.24.2.23.23.23.23.1.1.1"><msub id="S3.E3.m1.24.24.2.23.23.23.23.1.1.1.3"><mi id="S3.E3.m1.1.1.1.1.1.1" mathvariant="normal" xref="S3.E3.m1.1.1.1.1.1.1.cmml">Œò</mi><mrow id="S3.E3.m1.2.2.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.2.1.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.2.2.1.2.cmml">t</mi><mo id="S3.E3.m1.2.2.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.2.2.1.1.cmml">+</mo><mn id="S3.E3.m1.2.2.2.2.2.2.1.3" xref="S3.E3.m1.2.2.2.2.2.2.1.3.cmml">1</mn></mrow></msub><mo id="S3.E3.m1.3.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S3.E3.m1.24.24.2.23.23.23.23.1.1.1.2"><msub id="S3.E3.m1.24.24.2.23.23.23.23.1.1.1.2.3"><mi id="S3.E3.m1.4.4.4.4.4.4" mathvariant="normal" xref="S3.E3.m1.4.4.4.4.4.4.cmml">Œò</mi><mi id="S3.E3.m1.5.5.5.5.5.5.1" xref="S3.E3.m1.5.5.5.5.5.5.1.cmml">t</mi></msub><mo id="S3.E3.m1.6.6.6.6.6.6" xref="S3.E3.m1.6.6.6.6.6.6.cmml">+</mo><mrow id="S3.E3.m1.24.24.2.23.23.23.23.1.1.1.2.2"><msub id="S3.E3.m1.24.24.2.23.23.23.23.1.1.1.2.2.4"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.7.7.7.7.7.7" xref="S3.E3.m1.7.7.7.7.7.7.cmml">‚Ñõ</mi><mi id="S3.E3.m1.8.8.8.8.8.8.1" xref="S3.E3.m1.8.8.8.8.8.8.1.cmml">t</mi></msub><mo id="S3.E3.m1.24.24.2.23.23.23.23.1.1.1.2.2.3">‚Å¢</mo><mrow id="S3.E3.m1.24.24.2.23.23.23.23.1.1.1.2.2.2.2"><mo id="S3.E3.m1.9.9.9.9.9.9">(</mo><msub id="S3.E3.m1.24.24.2.23.23.23.23.1.1.1.1.1.1.1.1"><mi id="S3.E3.m1.10.10.10.10.10.10" mathvariant="normal" xref="S3.E3.m1.10.10.10.10.10.10.cmml">Œò</mi><mi id="S3.E3.m1.11.11.11.11.11.11.1" xref="S3.E3.m1.11.11.11.11.11.11.1.cmml">t</mi></msub><mo id="S3.E3.m1.12.12.12.12.12.12">,</mo><msubsup id="S3.E3.m1.24.24.2.23.23.23.23.1.1.1.2.2.2.2.2"><mover accent="true" id="S3.E3.m1.13.13.13.13.13.13" xref="S3.E3.m1.13.13.13.13.13.13.cmml"><mi id="S3.E3.m1.13.13.13.13.13.13.2" xref="S3.E3.m1.13.13.13.13.13.13.2.cmml">œï</mi><mo id="S3.E3.m1.13.13.13.13.13.13.1" xref="S3.E3.m1.13.13.13.13.13.13.1.cmml">^</mo></mover><mi id="S3.E3.m1.15.15.15.15.15.15.1" xref="S3.E3.m1.15.15.15.15.15.15.1.cmml">M</mi><mi id="S3.E3.m1.14.14.14.14.14.14.1" xref="S3.E3.m1.14.14.14.14.14.14.1.cmml">t</mi></msubsup><mo id="S3.E3.m1.16.16.16.16.16.16">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.17.17.17.17.17.17">,</mo><mrow id="S3.E3.m1.24.24.2.23.23.23.23.1.2.2"><mrow id="S3.E3.m1.24.24.2.23.23.23.23.1.2.2.1"><mtext id="S3.E3.m1.18.18.18.18.18.18" xref="S3.E3.m1.18.18.18.18.18.18a.cmml">¬†for¬†</mtext><mo id="S3.E3.m1.24.24.2.23.23.23.23.1.2.2.1.1">‚Å¢</mo><mi id="S3.E3.m1.19.19.19.19.19.19" xref="S3.E3.m1.19.19.19.19.19.19.cmml">t</mi></mrow><mo id="S3.E3.m1.20.20.20.20.20.20" lspace="0.448em" xref="S3.E3.m1.20.20.20.20.20.20.cmml">&gt;</mo><mn id="S3.E3.m1.21.21.21.21.21.21" xref="S3.E3.m1.21.21.21.21.21.21.cmml">‚Äâ0</mn></mrow></mrow><mo id="S3.E3.m1.22.22.22.22.22.22" lspace="0em">.</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E3.m1.24b"><apply id="S3.E3.m1.23.23.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S3.E3.m1.23.23.1.1.1.3a.cmml">formulae-sequence</csymbol><apply id="S3.E3.m1.23.23.1.1.1.1.1.cmml"><eq id="S3.E3.m1.3.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3"></eq><apply id="S3.E3.m1.23.23.1.1.1.1.1.4.cmml"><csymbol cd="ambiguous" id="S3.E3.m1.23.23.1.1.1.1.1.4.1.cmml">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">Œò</ci><apply id="S3.E3.m1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1"><plus id="S3.E3.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1.1"></plus><ci id="S3.E3.m1.2.2.2.2.2.2.1.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1.2">ùë°</ci><cn id="S3.E3.m1.2.2.2.2.2.2.1.3.cmml" type="integer" xref="S3.E3.m1.2.2.2.2.2.2.1.3">1</cn></apply></apply><apply id="S3.E3.m1.23.23.1.1.1.1.1.2.cmml"><plus id="S3.E3.m1.6.6.6.6.6.6.cmml" xref="S3.E3.m1.6.6.6.6.6.6"></plus><apply id="S3.E3.m1.23.23.1.1.1.1.1.2.4.cmml"><csymbol cd="ambiguous" id="S3.E3.m1.23.23.1.1.1.1.1.2.4.1.cmml">subscript</csymbol><ci id="S3.E3.m1.4.4.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4.4.4">Œò</ci><ci id="S3.E3.m1.5.5.5.5.5.5.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5.1">ùë°</ci></apply><apply id="S3.E3.m1.23.23.1.1.1.1.1.2.2.cmml"><times id="S3.E3.m1.23.23.1.1.1.1.1.2.2.3.cmml"></times><apply id="S3.E3.m1.23.23.1.1.1.1.1.2.2.4.cmml"><csymbol cd="ambiguous" id="S3.E3.m1.23.23.1.1.1.1.1.2.2.4.1.cmml">subscript</csymbol><ci id="S3.E3.m1.7.7.7.7.7.7.cmml" xref="S3.E3.m1.7.7.7.7.7.7">‚Ñõ</ci><ci id="S3.E3.m1.8.8.8.8.8.8.1.cmml" xref="S3.E3.m1.8.8.8.8.8.8.1">ùë°</ci></apply><interval closure="open" id="S3.E3.m1.23.23.1.1.1.1.1.2.2.2.3.cmml"><apply id="S3.E3.m1.23.23.1.1.1.1.1.1.1.1.1.1.cmml"><csymbol cd="ambiguous" id="S3.E3.m1.23.23.1.1.1.1.1.1.1.1.1.1.1.cmml">subscript</csymbol><ci id="S3.E3.m1.10.10.10.10.10.10.cmml" xref="S3.E3.m1.10.10.10.10.10.10">Œò</ci><ci id="S3.E3.m1.11.11.11.11.11.11.1.cmml" xref="S3.E3.m1.11.11.11.11.11.11.1">ùë°</ci></apply><apply id="S3.E3.m1.23.23.1.1.1.1.1.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S3.E3.m1.23.23.1.1.1.1.1.2.2.2.2.2.1.cmml">subscript</csymbol><apply id="S3.E3.m1.23.23.1.1.1.1.1.2.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S3.E3.m1.23.23.1.1.1.1.1.2.2.2.2.2.2.1.cmml">superscript</csymbol><apply id="S3.E3.m1.13.13.13.13.13.13.cmml" xref="S3.E3.m1.13.13.13.13.13.13"><ci id="S3.E3.m1.13.13.13.13.13.13.1.cmml" xref="S3.E3.m1.13.13.13.13.13.13.1">^</ci><ci id="S3.E3.m1.13.13.13.13.13.13.2.cmml" xref="S3.E3.m1.13.13.13.13.13.13.2">italic-œï</ci></apply><ci id="S3.E3.m1.14.14.14.14.14.14.1.cmml" xref="S3.E3.m1.14.14.14.14.14.14.1">ùë°</ci></apply><ci id="S3.E3.m1.15.15.15.15.15.15.1.cmml" xref="S3.E3.m1.15.15.15.15.15.15.1">ùëÄ</ci></apply></interval></apply></apply></apply><apply id="S3.E3.m1.23.23.1.1.1.2.2.cmml"><gt id="S3.E3.m1.20.20.20.20.20.20.cmml" xref="S3.E3.m1.20.20.20.20.20.20"></gt><apply id="S3.E3.m1.23.23.1.1.1.2.2.2.cmml"><times id="S3.E3.m1.23.23.1.1.1.2.2.2.1.cmml"></times><ci id="S3.E3.m1.18.18.18.18.18.18a.cmml" xref="S3.E3.m1.18.18.18.18.18.18"><mtext id="S3.E3.m1.18.18.18.18.18.18.cmml" xref="S3.E3.m1.18.18.18.18.18.18">¬†for¬†</mtext></ci><ci id="S3.E3.m1.19.19.19.19.19.19.cmml" xref="S3.E3.m1.19.19.19.19.19.19">ùë°</ci></apply><cn id="S3.E3.m1.21.21.21.21.21.21.cmml" type="float" xref="S3.E3.m1.21.21.21.21.21.21">‚Äâ0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.24c">\begin{split}\Theta_{t+1}=\Theta_{t}+\mathcal{R}_{t}\left(\Theta_{t},\hat{\phi%
}^{t}_{M}\right),\text{ for }t\,&gt;\,0.\end{split}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.24d">start_ROW start_CELL roman_Œò start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = roman_Œò start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + caligraphic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( roman_Œò start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , over^ start_ARG italic_œï end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) , for italic_t &gt; 0 . end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p1.17">The initial parameter <math alttext="\Theta_{0}" class="ltx_Math" display="inline" id="S3.SS3.p1.15.m1.1"><semantics id="S3.SS3.p1.15.m1.1a"><msub id="S3.SS3.p1.15.m1.1.1" xref="S3.SS3.p1.15.m1.1.1.cmml"><mi id="S3.SS3.p1.15.m1.1.1.2" mathvariant="normal" xref="S3.SS3.p1.15.m1.1.1.2.cmml">Œò</mi><mn id="S3.SS3.p1.15.m1.1.1.3" xref="S3.SS3.p1.15.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.15.m1.1b"><apply id="S3.SS3.p1.15.m1.1.1.cmml" xref="S3.SS3.p1.15.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.15.m1.1.1.1.cmml" xref="S3.SS3.p1.15.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.15.m1.1.1.2.cmml" xref="S3.SS3.p1.15.m1.1.1.2">Œò</ci><cn id="S3.SS3.p1.15.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p1.15.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.15.m1.1c">\Theta_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.15.m1.1d">roman_Œò start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is initialized by feeding <math alttext="\bm{\phi}^{t=0}_{M}" class="ltx_Math" display="inline" id="S3.SS3.p1.16.m2.1"><semantics id="S3.SS3.p1.16.m2.1a"><msubsup id="S3.SS3.p1.16.m2.1.1" xref="S3.SS3.p1.16.m2.1.1.cmml"><mi id="S3.SS3.p1.16.m2.1.1.2.2" mathvariant="bold-italic" xref="S3.SS3.p1.16.m2.1.1.2.2.cmml">œï</mi><mi id="S3.SS3.p1.16.m2.1.1.3" xref="S3.SS3.p1.16.m2.1.1.3.cmml">M</mi><mrow id="S3.SS3.p1.16.m2.1.1.2.3" xref="S3.SS3.p1.16.m2.1.1.2.3.cmml"><mi id="S3.SS3.p1.16.m2.1.1.2.3.2" xref="S3.SS3.p1.16.m2.1.1.2.3.2.cmml">t</mi><mo id="S3.SS3.p1.16.m2.1.1.2.3.1" xref="S3.SS3.p1.16.m2.1.1.2.3.1.cmml">=</mo><mn id="S3.SS3.p1.16.m2.1.1.2.3.3" xref="S3.SS3.p1.16.m2.1.1.2.3.3.cmml">0</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.16.m2.1b"><apply id="S3.SS3.p1.16.m2.1.1.cmml" xref="S3.SS3.p1.16.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.16.m2.1.1.1.cmml" xref="S3.SS3.p1.16.m2.1.1">subscript</csymbol><apply id="S3.SS3.p1.16.m2.1.1.2.cmml" xref="S3.SS3.p1.16.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.16.m2.1.1.2.1.cmml" xref="S3.SS3.p1.16.m2.1.1">superscript</csymbol><ci id="S3.SS3.p1.16.m2.1.1.2.2.cmml" xref="S3.SS3.p1.16.m2.1.1.2.2">bold-italic-œï</ci><apply id="S3.SS3.p1.16.m2.1.1.2.3.cmml" xref="S3.SS3.p1.16.m2.1.1.2.3"><eq id="S3.SS3.p1.16.m2.1.1.2.3.1.cmml" xref="S3.SS3.p1.16.m2.1.1.2.3.1"></eq><ci id="S3.SS3.p1.16.m2.1.1.2.3.2.cmml" xref="S3.SS3.p1.16.m2.1.1.2.3.2">ùë°</ci><cn id="S3.SS3.p1.16.m2.1.1.2.3.3.cmml" type="integer" xref="S3.SS3.p1.16.m2.1.1.2.3.3">0</cn></apply></apply><ci id="S3.SS3.p1.16.m2.1.1.3.cmml" xref="S3.SS3.p1.16.m2.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.16.m2.1c">\bm{\phi}^{t=0}_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.16.m2.1d">bold_italic_œï start_POSTSUPERSCRIPT italic_t = 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> into the regressor <math alttext="\mathcal{R}_{t}" class="ltx_Math" display="inline" id="S3.SS3.p1.17.m3.1"><semantics id="S3.SS3.p1.17.m3.1a"><msub id="S3.SS3.p1.17.m3.1.1" xref="S3.SS3.p1.17.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.17.m3.1.1.2" xref="S3.SS3.p1.17.m3.1.1.2.cmml">‚Ñõ</mi><mi id="S3.SS3.p1.17.m3.1.1.3" xref="S3.SS3.p1.17.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.17.m3.1b"><apply id="S3.SS3.p1.17.m3.1.1.cmml" xref="S3.SS3.p1.17.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.17.m3.1.1.1.cmml" xref="S3.SS3.p1.17.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.17.m3.1.1.2.cmml" xref="S3.SS3.p1.17.m3.1.1.2">‚Ñõ</ci><ci id="S3.SS3.p1.17.m3.1.1.3.cmml" xref="S3.SS3.p1.17.m3.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.17.m3.1c">\mathcal{R}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.17.m3.1d">caligraphic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.8">When the predicted parameters <math alttext="\Theta" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" mathvariant="normal" xref="S3.SS3.p2.1.m1.1.1.cmml">Œò</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">Œò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">roman_Œò</annotation></semantics></math> (the subscript <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.8.1">t</span> is omitted for simplicity) are obtained through each scale feature, a mesh with vertices of <math alttext="V=\mathcal{M}(\bm{\theta},\bm{\beta})\in\mathbb{R}^{N\times 3}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.2"><semantics id="S3.SS3.p2.2.m2.2a"><mrow id="S3.SS3.p2.2.m2.2.3" xref="S3.SS3.p2.2.m2.2.3.cmml"><mi id="S3.SS3.p2.2.m2.2.3.2" xref="S3.SS3.p2.2.m2.2.3.2.cmml">V</mi><mo id="S3.SS3.p2.2.m2.2.3.3" xref="S3.SS3.p2.2.m2.2.3.3.cmml">=</mo><mrow id="S3.SS3.p2.2.m2.2.3.4" xref="S3.SS3.p2.2.m2.2.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.2.m2.2.3.4.2" xref="S3.SS3.p2.2.m2.2.3.4.2.cmml">‚Ñ≥</mi><mo id="S3.SS3.p2.2.m2.2.3.4.1" xref="S3.SS3.p2.2.m2.2.3.4.1.cmml">‚Å¢</mo><mrow id="S3.SS3.p2.2.m2.2.3.4.3.2" xref="S3.SS3.p2.2.m2.2.3.4.3.1.cmml"><mo id="S3.SS3.p2.2.m2.2.3.4.3.2.1" stretchy="false" xref="S3.SS3.p2.2.m2.2.3.4.3.1.cmml">(</mo><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">ùúΩ</mi><mo id="S3.SS3.p2.2.m2.2.3.4.3.2.2" xref="S3.SS3.p2.2.m2.2.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p2.2.m2.2.2" xref="S3.SS3.p2.2.m2.2.2.cmml">ùú∑</mi><mo id="S3.SS3.p2.2.m2.2.3.4.3.2.3" stretchy="false" xref="S3.SS3.p2.2.m2.2.3.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS3.p2.2.m2.2.3.5" xref="S3.SS3.p2.2.m2.2.3.5.cmml">‚àà</mo><msup id="S3.SS3.p2.2.m2.2.3.6" xref="S3.SS3.p2.2.m2.2.3.6.cmml"><mi id="S3.SS3.p2.2.m2.2.3.6.2" xref="S3.SS3.p2.2.m2.2.3.6.2.cmml">‚Ñù</mi><mrow id="S3.SS3.p2.2.m2.2.3.6.3" xref="S3.SS3.p2.2.m2.2.3.6.3.cmml"><mi id="S3.SS3.p2.2.m2.2.3.6.3.2" xref="S3.SS3.p2.2.m2.2.3.6.3.2.cmml">N</mi><mo id="S3.SS3.p2.2.m2.2.3.6.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.2.m2.2.3.6.3.1.cmml">√ó</mo><mn id="S3.SS3.p2.2.m2.2.3.6.3.3" xref="S3.SS3.p2.2.m2.2.3.6.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.2b"><apply id="S3.SS3.p2.2.m2.2.3.cmml" xref="S3.SS3.p2.2.m2.2.3"><and id="S3.SS3.p2.2.m2.2.3a.cmml" xref="S3.SS3.p2.2.m2.2.3"></and><apply id="S3.SS3.p2.2.m2.2.3b.cmml" xref="S3.SS3.p2.2.m2.2.3"><eq id="S3.SS3.p2.2.m2.2.3.3.cmml" xref="S3.SS3.p2.2.m2.2.3.3"></eq><ci id="S3.SS3.p2.2.m2.2.3.2.cmml" xref="S3.SS3.p2.2.m2.2.3.2">ùëâ</ci><apply id="S3.SS3.p2.2.m2.2.3.4.cmml" xref="S3.SS3.p2.2.m2.2.3.4"><times id="S3.SS3.p2.2.m2.2.3.4.1.cmml" xref="S3.SS3.p2.2.m2.2.3.4.1"></times><ci id="S3.SS3.p2.2.m2.2.3.4.2.cmml" xref="S3.SS3.p2.2.m2.2.3.4.2">‚Ñ≥</ci><interval closure="open" id="S3.SS3.p2.2.m2.2.3.4.3.1.cmml" xref="S3.SS3.p2.2.m2.2.3.4.3.2"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">ùúΩ</ci><ci id="S3.SS3.p2.2.m2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2">ùú∑</ci></interval></apply></apply><apply id="S3.SS3.p2.2.m2.2.3c.cmml" xref="S3.SS3.p2.2.m2.2.3"><in id="S3.SS3.p2.2.m2.2.3.5.cmml" xref="S3.SS3.p2.2.m2.2.3.5"></in><share href="#S3.SS3.p2.2.m2.2.3.4.cmml" id="S3.SS3.p2.2.m2.2.3d.cmml" xref="S3.SS3.p2.2.m2.2.3"></share><apply id="S3.SS3.p2.2.m2.2.3.6.cmml" xref="S3.SS3.p2.2.m2.2.3.6"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.2.3.6.1.cmml" xref="S3.SS3.p2.2.m2.2.3.6">superscript</csymbol><ci id="S3.SS3.p2.2.m2.2.3.6.2.cmml" xref="S3.SS3.p2.2.m2.2.3.6.2">‚Ñù</ci><apply id="S3.SS3.p2.2.m2.2.3.6.3.cmml" xref="S3.SS3.p2.2.m2.2.3.6.3"><times id="S3.SS3.p2.2.m2.2.3.6.3.1.cmml" xref="S3.SS3.p2.2.m2.2.3.6.3.1"></times><ci id="S3.SS3.p2.2.m2.2.3.6.3.2.cmml" xref="S3.SS3.p2.2.m2.2.3.6.3.2">ùëÅ</ci><cn id="S3.SS3.p2.2.m2.2.3.6.3.3.cmml" type="integer" xref="S3.SS3.p2.2.m2.2.3.6.3.3">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.2c">V=\mathcal{M}(\bm{\theta},\bm{\beta})\in\mathbb{R}^{N\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.2d">italic_V = caligraphic_M ( bold_italic_Œ∏ , bold_italic_Œ≤ ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math> can be generated accordingly, where <math alttext="N=6890" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">N</mi><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">6890</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><eq id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></eq><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">ùëÅ</ci><cn id="S3.SS3.p2.3.m3.1.1.3.cmml" type="integer" xref="S3.SS3.p2.3.m3.1.1.3">6890</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">N=6890</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_N = 6890</annotation></semantics></math> is the number of vertices in the SMPL model.
We downsample <math alttext="V" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">italic_V</annotation></semantics></math> to get the sampling point <math alttext="S_{t+1}" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><msub id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mi id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">S</mi><mrow id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml"><mi id="S3.SS3.p2.5.m5.1.1.3.2" xref="S3.SS3.p2.5.m5.1.1.3.2.cmml">t</mi><mo id="S3.SS3.p2.5.m5.1.1.3.1" xref="S3.SS3.p2.5.m5.1.1.3.1.cmml">+</mo><mn id="S3.SS3.p2.5.m5.1.1.3.3" xref="S3.SS3.p2.5.m5.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">ùëÜ</ci><apply id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3"><plus id="S3.SS3.p2.5.m5.1.1.3.1.cmml" xref="S3.SS3.p2.5.m5.1.1.3.1"></plus><ci id="S3.SS3.p2.5.m5.1.1.3.2.cmml" xref="S3.SS3.p2.5.m5.1.1.3.2">ùë°</ci><cn id="S3.SS3.p2.5.m5.1.1.3.3.cmml" type="integer" xref="S3.SS3.p2.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">S_{t+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">italic_S start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT</annotation></semantics></math> for the next iteration.
These mesh vertices can be mapped by pre-trained linear regressor to obtain sparse 3D keypoints <math alttext="J\in\mathbb{R}^{N_{j}\times 3}" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.1"><semantics id="S3.SS3.p2.6.m6.1a"><mrow id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">J</mi><mo id="S3.SS3.p2.6.m6.1.1.1" xref="S3.SS3.p2.6.m6.1.1.1.cmml">‚àà</mo><msup id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml"><mi id="S3.SS3.p2.6.m6.1.1.3.2" xref="S3.SS3.p2.6.m6.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS3.p2.6.m6.1.1.3.3" xref="S3.SS3.p2.6.m6.1.1.3.3.cmml"><msub id="S3.SS3.p2.6.m6.1.1.3.3.2" xref="S3.SS3.p2.6.m6.1.1.3.3.2.cmml"><mi id="S3.SS3.p2.6.m6.1.1.3.3.2.2" xref="S3.SS3.p2.6.m6.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS3.p2.6.m6.1.1.3.3.2.3" xref="S3.SS3.p2.6.m6.1.1.3.3.2.3.cmml">j</mi></msub><mo id="S3.SS3.p2.6.m6.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.6.m6.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS3.p2.6.m6.1.1.3.3.3" xref="S3.SS3.p2.6.m6.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><in id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1.1"></in><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">ùêΩ</ci><apply id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.3.1.cmml" xref="S3.SS3.p2.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.3.2.cmml" xref="S3.SS3.p2.6.m6.1.1.3.2">‚Ñù</ci><apply id="S3.SS3.p2.6.m6.1.1.3.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3"><times id="S3.SS3.p2.6.m6.1.1.3.3.1.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.1"></times><apply id="S3.SS3.p2.6.m6.1.1.3.3.2.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.3.3.2.1.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.3.3.2.2.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.2.2">ùëÅ</ci><ci id="S3.SS3.p2.6.m6.1.1.3.3.2.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.2.3">ùëó</ci></apply><cn id="S3.SS3.p2.6.m6.1.1.3.3.3.cmml" type="integer" xref="S3.SS3.p2.6.m6.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">J\in\mathbb{R}^{N_{j}\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.1d">italic_J ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math>.
With the estimated pseudo-camera parameters, we can obtain 2D keypoints <math alttext="K\in\mathbb{R}^{N_{j}\times 2}" class="ltx_Math" display="inline" id="S3.SS3.p2.7.m7.1"><semantics id="S3.SS3.p2.7.m7.1a"><mrow id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml"><mi id="S3.SS3.p2.7.m7.1.1.2" xref="S3.SS3.p2.7.m7.1.1.2.cmml">K</mi><mo id="S3.SS3.p2.7.m7.1.1.1" xref="S3.SS3.p2.7.m7.1.1.1.cmml">‚àà</mo><msup id="S3.SS3.p2.7.m7.1.1.3" xref="S3.SS3.p2.7.m7.1.1.3.cmml"><mi id="S3.SS3.p2.7.m7.1.1.3.2" xref="S3.SS3.p2.7.m7.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS3.p2.7.m7.1.1.3.3" xref="S3.SS3.p2.7.m7.1.1.3.3.cmml"><msub id="S3.SS3.p2.7.m7.1.1.3.3.2" xref="S3.SS3.p2.7.m7.1.1.3.3.2.cmml"><mi id="S3.SS3.p2.7.m7.1.1.3.3.2.2" xref="S3.SS3.p2.7.m7.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS3.p2.7.m7.1.1.3.3.2.3" xref="S3.SS3.p2.7.m7.1.1.3.3.2.3.cmml">j</mi></msub><mo id="S3.SS3.p2.7.m7.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.7.m7.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS3.p2.7.m7.1.1.3.3.3" xref="S3.SS3.p2.7.m7.1.1.3.3.3.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><apply id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1"><in id="S3.SS3.p2.7.m7.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1"></in><ci id="S3.SS3.p2.7.m7.1.1.2.cmml" xref="S3.SS3.p2.7.m7.1.1.2">ùêæ</ci><apply id="S3.SS3.p2.7.m7.1.1.3.cmml" xref="S3.SS3.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.1.1.3.1.cmml" xref="S3.SS3.p2.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.7.m7.1.1.3.2.cmml" xref="S3.SS3.p2.7.m7.1.1.3.2">‚Ñù</ci><apply id="S3.SS3.p2.7.m7.1.1.3.3.cmml" xref="S3.SS3.p2.7.m7.1.1.3.3"><times id="S3.SS3.p2.7.m7.1.1.3.3.1.cmml" xref="S3.SS3.p2.7.m7.1.1.3.3.1"></times><apply id="S3.SS3.p2.7.m7.1.1.3.3.2.cmml" xref="S3.SS3.p2.7.m7.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.1.1.3.3.2.1.cmml" xref="S3.SS3.p2.7.m7.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p2.7.m7.1.1.3.3.2.2.cmml" xref="S3.SS3.p2.7.m7.1.1.3.3.2.2">ùëÅ</ci><ci id="S3.SS3.p2.7.m7.1.1.3.3.2.3.cmml" xref="S3.SS3.p2.7.m7.1.1.3.3.2.3">ùëó</ci></apply><cn id="S3.SS3.p2.7.m7.1.1.3.3.3.cmml" type="integer" xref="S3.SS3.p2.7.m7.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">K\in\mathbb{R}^{N_{j}\times 2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.7.m7.1d">italic_K ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT √ó 2 end_POSTSUPERSCRIPT</annotation></semantics></math> by projecting <math alttext="J" class="ltx_Math" display="inline" id="S3.SS3.p2.8.m8.1"><semantics id="S3.SS3.p2.8.m8.1a"><mi id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.1b"><ci id="S3.SS3.p2.8.m8.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1">ùêΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.1c">J</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.8.m8.1d">italic_J</annotation></semantics></math> on the measurement coordinate system.
We calculate the loss between the 2D keypoints and the ground truth, in this way reducing the difference between the 2D projection and the human pose in the real scene.
Concurrently, supplementary 3D supervisions regarding 3D keypoints and model parameters are integrated when authentic 3D labels are available. The total loss function for the parameter regressor is thereby formulated as follows:</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{reg }}=\lambda_{2d}\|K-\hat{K}\|_{2}+\lambda_{3d}\|J-\hat{J%
}\|_{2}+\lambda_{\text{para }}\|\Theta-\hat{\Theta}\|_{2}," class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.5" xref="S3.E4.m1.1.1.1.1.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.1.1.5.2" xref="S3.E4.m1.1.1.1.1.5.2.cmml">‚Ñí</mi><mtext id="S3.E4.m1.1.1.1.1.5.3" xref="S3.E4.m1.1.1.1.1.5.3a.cmml">reg¬†</mtext></msub><mo id="S3.E4.m1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.4.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.3.2.cmml">Œª</mi><mrow id="S3.E4.m1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.3.3.cmml"><mn id="S3.E4.m1.1.1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.1.1.3.3.2.cmml">2</mn><mo id="S3.E4.m1.1.1.1.1.1.1.3.3.1" xref="S3.E4.m1.1.1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S3.E4.m1.1.1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.1.1.3.3.3.cmml">d</mi></mrow></msub><mo id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml">‚Å¢</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">K</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><mover accent="true" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">K</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3.cmml">2</mn></msub></mrow><mo id="S3.E4.m1.1.1.1.1.3.4" xref="S3.E4.m1.1.1.1.1.3.4.cmml">+</mo><mrow id="S3.E4.m1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.2.2.cmml"><msub id="S3.E4.m1.1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.3.2" xref="S3.E4.m1.1.1.1.1.2.2.3.2.cmml">Œª</mi><mrow id="S3.E4.m1.1.1.1.1.2.2.3.3" xref="S3.E4.m1.1.1.1.1.2.2.3.3.cmml"><mn id="S3.E4.m1.1.1.1.1.2.2.3.3.2" xref="S3.E4.m1.1.1.1.1.2.2.3.3.2.cmml">3</mn><mo id="S3.E4.m1.1.1.1.1.2.2.3.3.1" xref="S3.E4.m1.1.1.1.1.2.2.3.3.1.cmml">‚Å¢</mo><mi id="S3.E4.m1.1.1.1.1.2.2.3.3.3" xref="S3.E4.m1.1.1.1.1.2.2.3.3.3.cmml">d</mi></mrow></msub><mo id="S3.E4.m1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.2.2.2.cmml">‚Å¢</mo><msub id="S3.E4.m1.1.1.1.1.2.2.1" xref="S3.E4.m1.1.1.1.1.2.2.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.2.2.1.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.2.2.1.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.2.cmml">J</mi><mo id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.1.cmml">‚àí</mo><mover accent="true" id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3.2.cmml">J</mi><mo id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.2.2.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E4.m1.1.1.1.1.2.2.1.3" xref="S3.E4.m1.1.1.1.1.2.2.1.3.cmml">2</mn></msub></mrow><mo id="S3.E4.m1.1.1.1.1.3.4a" xref="S3.E4.m1.1.1.1.1.3.4.cmml">+</mo><mrow id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml"><msub id="S3.E4.m1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.3.2.cmml">Œª</mi><mtext id="S3.E4.m1.1.1.1.1.3.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.3a.cmml">para¬†</mtext></msub><mo id="S3.E4.m1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.2.cmml">‚Å¢</mo><msub id="S3.E4.m1.1.1.1.1.3.3.1" xref="S3.E4.m1.1.1.1.1.3.3.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.3.3.1.1.1" xref="S3.E4.m1.1.1.1.1.3.3.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.1.3.3.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.3.3.1.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.2" mathvariant="normal" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.2.cmml">Œò</mi><mo id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.1.cmml">‚àí</mo><mover accent="true" id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3.2" mathvariant="normal" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3.2.cmml">Œò</mi><mo id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S3.E4.m1.1.1.1.1.3.3.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.3.3.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E4.m1.1.1.1.1.3.3.1.3" xref="S3.E4.m1.1.1.1.1.3.3.1.3.cmml">2</mn></msub></mrow></mrow></mrow><mo id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.4"></eq><apply id="S3.E4.m1.1.1.1.1.5.cmml" xref="S3.E4.m1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.5.1.cmml" xref="S3.E4.m1.1.1.1.1.5">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.5.2.cmml" xref="S3.E4.m1.1.1.1.1.5.2">‚Ñí</ci><ci id="S3.E4.m1.1.1.1.1.5.3a.cmml" xref="S3.E4.m1.1.1.1.1.5.3"><mtext id="S3.E4.m1.1.1.1.1.5.3.cmml" mathsize="70%" xref="S3.E4.m1.1.1.1.1.5.3">reg¬†</mtext></ci></apply><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><plus id="S3.E4.m1.1.1.1.1.3.4.cmml" xref="S3.E4.m1.1.1.1.1.3.4"></plus><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2"></times><apply id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.2">ùúÜ</ci><apply id="S3.E4.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3"><times id="S3.E4.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3.1"></times><cn id="S3.E4.m1.1.1.1.1.1.1.3.3.2.cmml" type="integer" xref="S3.E4.m1.1.1.1.1.1.1.3.3.2">2</cn><ci id="S3.E4.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3.3">ùëë</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2">ùêæ</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2">ùêæ</ci></apply></apply></apply><cn id="S3.E4.m1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E4.m1.1.1.1.1.1.1.1.3">2</cn></apply></apply><apply id="S3.E4.m1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2"><times id="S3.E4.m1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2"></times><apply id="S3.E4.m1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3.2">ùúÜ</ci><apply id="S3.E4.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3.3"><times id="S3.E4.m1.1.1.1.1.2.2.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3.3.1"></times><cn id="S3.E4.m1.1.1.1.1.2.2.3.3.2.cmml" type="integer" xref="S3.E4.m1.1.1.1.1.2.2.3.3.2">3</cn><ci id="S3.E4.m1.1.1.1.1.2.2.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3.3.3">ùëë</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1">subscript</csymbol><apply id="S3.E4.m1.1.1.1.1.2.2.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.1.1.1.1.2.2.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1"><minus id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.1"></minus><ci id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.2">ùêΩ</ci><apply id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3"><ci id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3.1">^</ci><ci id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.3.2">ùêΩ</ci></apply></apply></apply><cn id="S3.E4.m1.1.1.1.1.2.2.1.3.cmml" type="integer" xref="S3.E4.m1.1.1.1.1.2.2.1.3">2</cn></apply></apply><apply id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3"><times id="S3.E4.m1.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2"></times><apply id="S3.E4.m1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3.2">ùúÜ</ci><ci id="S3.E4.m1.1.1.1.1.3.3.3.3a.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3.3"><mtext id="S3.E4.m1.1.1.1.1.3.3.3.3.cmml" mathsize="70%" xref="S3.E4.m1.1.1.1.1.3.3.3.3">para¬†</mtext></ci></apply><apply id="S3.E4.m1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.3.1.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1">subscript</csymbol><apply id="S3.E4.m1.1.1.1.1.3.3.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.1.1.1.1.3.3.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1"><minus id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.1"></minus><ci id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.2">Œò</ci><apply id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3"><ci id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3.1">^</ci><ci id="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1.1.1.1.3.2">Œò</ci></apply></apply></apply><cn id="S3.E4.m1.1.1.1.1.3.3.1.3.cmml" type="integer" xref="S3.E4.m1.1.1.1.1.3.3.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\mathcal{L}_{\text{reg }}=\lambda_{2d}\|K-\hat{K}\|_{2}+\lambda_{3d}\|J-\hat{J%
}\|_{2}+\lambda_{\text{para }}\|\Theta-\hat{\Theta}\|_{2},</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT = italic_Œª start_POSTSUBSCRIPT 2 italic_d end_POSTSUBSCRIPT ‚à• italic_K - over^ start_ARG italic_K end_ARG ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_Œª start_POSTSUBSCRIPT 3 italic_d end_POSTSUBSCRIPT ‚à• italic_J - over^ start_ARG italic_J end_ARG ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_Œª start_POSTSUBSCRIPT para end_POSTSUBSCRIPT ‚à• roman_Œò - over^ start_ARG roman_Œò end_ARG ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p3.4">where <math alttext="\|\cdot\|_{2}" class="ltx_math_unparsed" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1b"><mo id="S3.SS3.p3.1.m1.1.1" rspace="0em">‚à•</mo><mo id="S3.SS3.p3.1.m1.1.2" lspace="0em" rspace="0em">‚ãÖ</mo><msub id="S3.SS3.p3.1.m1.1.3"><mo id="S3.SS3.p3.1.m1.1.3.2" lspace="0em">‚à•</mo><mn id="S3.SS3.p3.1.m1.1.3.3">2</mn></msub></mrow><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\|\cdot\|_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">‚à• ‚ãÖ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is the squared L2 norm, <math alttext="\hat{K}" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mover accent="true" id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">K</mi><mo id="S3.SS3.p3.2.m2.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><ci id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1">^</ci><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">ùêæ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\hat{K}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">over^ start_ARG italic_K end_ARG</annotation></semantics></math>,<math alttext="\hat{J}" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.1"><semantics id="S3.SS3.p3.3.m3.1a"><mover accent="true" id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">J</mi><mo id="S3.SS3.p3.3.m3.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><ci id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1">^</ci><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">ùêΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\hat{J}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.1d">over^ start_ARG italic_J end_ARG</annotation></semantics></math>, and <math alttext="\hat{\Theta}" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m4.1"><semantics id="S3.SS3.p3.4.m4.1a"><mover accent="true" id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.2" mathvariant="normal" xref="S3.SS3.p3.4.m4.1.1.2.cmml">Œò</mi><mo id="S3.SS3.p3.4.m4.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><ci id="S3.SS3.p3.4.m4.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1">^</ci><ci id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2">Œò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">\hat{\Theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m4.1d">over^ start_ARG roman_Œò end_ARG</annotation></semantics></math> denote the ground truth 2D keypoints, 3D keypoints, and model parameters, respectively.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Double-Head Auxiliary Supervision</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The spatial feature map of the human body is relatively rough and contains a lot of noise. Therefore, there are still some deviations in the perception of human limbs and pose. To improve our estimation accuracy of human limbs, we introduce a Double-Head Auxiliary Supervision (DHAS) mechanism to obtain finer spatial features during the training stage.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.2">Specifically, we first convert all the spatial features at different scales to a single feature map <math alttext="\phi^{T}" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.1"><semantics id="S3.SS4.p2.1.m1.1a"><msup id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">œï</mi><mi id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">italic-œï</ci><ci id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">ùëá</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\phi^{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.1.m1.1d">italic_œï start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> by upsampling and then concatenating them together. The feature map <math alttext="\phi^{T}" class="ltx_Math" display="inline" id="S3.SS4.p2.2.m2.1"><semantics id="S3.SS4.p2.2.m2.1a"><msup id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">œï</mi><mi id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">italic-œï</ci><ci id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">ùëá</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\phi^{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.2.m2.1d">italic_œï start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> is used for the different auxiliary supervision heads.
On the one hand, we generate a heat map representation by a classification layer to explicitly indicate the position of the 2d keypoints.
On the other hand, we also estimate a dense map through the IUV Predict layer to get a dense correspondence between the SMPL model and the feature map.
The loss function for the Double-Head Auxiliary Supervision consists of two parts, which can be written as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{das}=\mathcal{L}_{sc}+\mathcal{L}_{den}." class="ltx_Math" display="block" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.2.2.cmml">‚Ñí</mi><mrow id="S3.E5.m1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.2.3.cmml"><mi id="S3.E5.m1.1.1.1.1.2.3.2" xref="S3.E5.m1.1.1.1.1.2.3.2.cmml">d</mi><mo id="S3.E5.m1.1.1.1.1.2.3.1" xref="S3.E5.m1.1.1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S3.E5.m1.1.1.1.1.2.3.3" xref="S3.E5.m1.1.1.1.1.2.3.3.cmml">a</mi><mo id="S3.E5.m1.1.1.1.1.2.3.1a" xref="S3.E5.m1.1.1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S3.E5.m1.1.1.1.1.2.3.4" xref="S3.E5.m1.1.1.1.1.2.3.4.cmml">s</mi></mrow></msub><mo id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.3.cmml"><msub id="S3.E5.m1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.1.1.3.2.2" xref="S3.E5.m1.1.1.1.1.3.2.2.cmml">‚Ñí</mi><mrow id="S3.E5.m1.1.1.1.1.3.2.3" xref="S3.E5.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E5.m1.1.1.1.1.3.2.3.2" xref="S3.E5.m1.1.1.1.1.3.2.3.2.cmml">s</mi><mo id="S3.E5.m1.1.1.1.1.3.2.3.1" xref="S3.E5.m1.1.1.1.1.3.2.3.1.cmml">‚Å¢</mo><mi id="S3.E5.m1.1.1.1.1.3.2.3.3" xref="S3.E5.m1.1.1.1.1.3.2.3.3.cmml">c</mi></mrow></msub><mo id="S3.E5.m1.1.1.1.1.3.1" xref="S3.E5.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S3.E5.m1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.1.1.3.3.2" xref="S3.E5.m1.1.1.1.1.3.3.2.cmml">‚Ñí</mi><mrow id="S3.E5.m1.1.1.1.1.3.3.3" xref="S3.E5.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E5.m1.1.1.1.1.3.3.3.2" xref="S3.E5.m1.1.1.1.1.3.3.3.2.cmml">d</mi><mo id="S3.E5.m1.1.1.1.1.3.3.3.1" xref="S3.E5.m1.1.1.1.1.3.3.3.1.cmml">‚Å¢</mo><mi id="S3.E5.m1.1.1.1.1.3.3.3.3" xref="S3.E5.m1.1.1.1.1.3.3.3.3.cmml">e</mi><mo id="S3.E5.m1.1.1.1.1.3.3.3.1a" xref="S3.E5.m1.1.1.1.1.3.3.3.1.cmml">‚Å¢</mo><mi id="S3.E5.m1.1.1.1.1.3.3.3.4" xref="S3.E5.m1.1.1.1.1.3.3.3.4.cmml">n</mi></mrow></msub></mrow></mrow><mo id="S3.E5.m1.1.1.1.2" lspace="0em" xref="S3.E5.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><eq id="S3.E5.m1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"></eq><apply id="S3.E5.m1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2">‚Ñí</ci><apply id="S3.E5.m1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.2.3"><times id="S3.E5.m1.1.1.1.1.2.3.1.cmml" xref="S3.E5.m1.1.1.1.1.2.3.1"></times><ci id="S3.E5.m1.1.1.1.1.2.3.2.cmml" xref="S3.E5.m1.1.1.1.1.2.3.2">ùëë</ci><ci id="S3.E5.m1.1.1.1.1.2.3.3.cmml" xref="S3.E5.m1.1.1.1.1.2.3.3">ùëé</ci><ci id="S3.E5.m1.1.1.1.1.2.3.4.cmml" xref="S3.E5.m1.1.1.1.1.2.3.4">ùë†</ci></apply></apply><apply id="S3.E5.m1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.3"><plus id="S3.E5.m1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3.1"></plus><apply id="S3.E5.m1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2.2">‚Ñí</ci><apply id="S3.E5.m1.1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.1.1.3.2.3"><times id="S3.E5.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3.2.3.1"></times><ci id="S3.E5.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2.3.2">ùë†</ci><ci id="S3.E5.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E5.m1.1.1.1.1.3.2.3.3">ùëê</ci></apply></apply><apply id="S3.E5.m1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.3.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.3.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.3.2">‚Ñí</ci><apply id="S3.E5.m1.1.1.1.1.3.3.3.cmml" xref="S3.E5.m1.1.1.1.1.3.3.3"><times id="S3.E5.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3.3.3.1"></times><ci id="S3.E5.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.3.3.2">ùëë</ci><ci id="S3.E5.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E5.m1.1.1.1.1.3.3.3.3">ùëí</ci><ci id="S3.E5.m1.1.1.1.1.3.3.3.4.cmml" xref="S3.E5.m1.1.1.1.1.3.3.3.4">ùëõ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\mathcal{L}_{das}=\mathcal{L}_{sc}+\mathcal{L}_{den}.</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_d italic_a italic_s end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT italic_s italic_c end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_d italic_e italic_n end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.3"><span class="ltx_text ltx_font_bold" id="S3.SS4.p3.3.1">Keypoints Supervision.</span>
We utilize the SimCC-based algorithm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> for predicting pose keypoints. This approach treats keypoint localization as a classification task in horizontal and vertical coordinates. During training, instead of estimating the actual coordinates, we employ two vectors, <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.3.2">x</span> and <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.3.3">y</span>, and convert the ground truth 2D keypoints into such vectors to compute the loss.
The loss function is then formulated as follows:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E6">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E6X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{sc}=\lambda_{xy}\left(\operatorname{KL-Loss}({x},%
\hat{x})+\operatorname{KL-Loss}({y},\hat{y})\right)," class="ltx_Math" display="inline" id="S3.E6X.2.1.1.m1.7"><semantics id="S3.E6X.2.1.1.m1.7a"><mrow id="S3.E6X.2.1.1.m1.7.7.1" xref="S3.E6X.2.1.1.m1.7.7.1.1.cmml"><mrow id="S3.E6X.2.1.1.m1.7.7.1.1" xref="S3.E6X.2.1.1.m1.7.7.1.1.cmml"><msub id="S3.E6X.2.1.1.m1.7.7.1.1.3" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6X.2.1.1.m1.7.7.1.1.3.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.2.cmml">‚Ñí</mi><mrow id="S3.E6X.2.1.1.m1.7.7.1.1.3.3" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.3.cmml"><mi id="S3.E6X.2.1.1.m1.7.7.1.1.3.3.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.3.2.cmml">s</mi><mo id="S3.E6X.2.1.1.m1.7.7.1.1.3.3.1" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S3.E6X.2.1.1.m1.7.7.1.1.3.3.3" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.3.3.cmml">c</mi></mrow></msub><mo id="S3.E6X.2.1.1.m1.7.7.1.1.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.2.cmml">=</mo><mrow id="S3.E6X.2.1.1.m1.7.7.1.1.1" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.cmml"><msub id="S3.E6X.2.1.1.m1.7.7.1.1.1.3" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.cmml"><mi id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.2.cmml">Œª</mi><mrow id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.cmml"><mi id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.2.cmml">x</mi><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.1" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.3" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.3.cmml">y</mi></mrow></msub><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.cmml"><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.cmml"><mrow id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.1.cmml"><mrow id="S3.E6X.2.1.1.m1.1.1" xref="S3.E6X.2.1.1.m1.1.1.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.2.cmml">KL</mi><mo id="S3.E6X.2.1.1.m1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.cmml">‚àí</mo><mi id="S3.E6X.2.1.1.m1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.3.cmml">Loss</mi></mrow><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.2a" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.1.cmml">‚Å°</mo><mrow id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.2.1" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.1.cmml"><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.2.1.1" stretchy="false" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.1.cmml">(</mo><mi id="S3.E6X.2.1.1.m1.2.2" xref="S3.E6X.2.1.1.m1.2.2.cmml">x</mi><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.2.1.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.1.cmml">,</mo><mover accent="true" id="S3.E6X.2.1.1.m1.3.3" xref="S3.E6X.2.1.1.m1.3.3.cmml"><mi id="S3.E6X.2.1.1.m1.3.3.2" xref="S3.E6X.2.1.1.m1.3.3.2.cmml">x</mi><mo id="S3.E6X.2.1.1.m1.3.3.1" xref="S3.E6X.2.1.1.m1.3.3.1.cmml">^</mo></mover><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.2.1.3" stretchy="false" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.1.cmml">)</mo></mrow></mrow><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.1.cmml"><mrow id="S3.E6X.2.1.1.m1.4.4" xref="S3.E6X.2.1.1.m1.4.4.cmml"><mi id="S3.E6X.2.1.1.m1.4.4.2" xref="S3.E6X.2.1.1.m1.4.4.2.cmml">KL</mi><mo id="S3.E6X.2.1.1.m1.4.4.1" xref="S3.E6X.2.1.1.m1.4.4.1.cmml">‚àí</mo><mi id="S3.E6X.2.1.1.m1.4.4.3" xref="S3.E6X.2.1.1.m1.4.4.3.cmml">Loss</mi></mrow><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.2a" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.1.cmml">‚Å°</mo><mrow id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.2.1" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.1.cmml"><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.2.1.1" stretchy="false" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.1.cmml">(</mo><mi id="S3.E6X.2.1.1.m1.5.5" xref="S3.E6X.2.1.1.m1.5.5.cmml">y</mi><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.2.1.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.1.cmml">,</mo><mover accent="true" id="S3.E6X.2.1.1.m1.6.6" xref="S3.E6X.2.1.1.m1.6.6.cmml"><mi id="S3.E6X.2.1.1.m1.6.6.2" xref="S3.E6X.2.1.1.m1.6.6.2.cmml">y</mi><mo id="S3.E6X.2.1.1.m1.6.6.1" xref="S3.E6X.2.1.1.m1.6.6.1.cmml">^</mo></mover><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.2.1.3" stretchy="false" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6X.2.1.1.m1.7.7.1.2" xref="S3.E6X.2.1.1.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6X.2.1.1.m1.7b"><apply id="S3.E6X.2.1.1.m1.7.7.1.1.cmml" xref="S3.E6X.2.1.1.m1.7.7.1"><eq id="S3.E6X.2.1.1.m1.7.7.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.2"></eq><apply id="S3.E6X.2.1.1.m1.7.7.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.3"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.7.7.1.1.3.1.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.3">subscript</csymbol><ci id="S3.E6X.2.1.1.m1.7.7.1.1.3.2.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.2">‚Ñí</ci><apply id="S3.E6X.2.1.1.m1.7.7.1.1.3.3.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.3"><times id="S3.E6X.2.1.1.m1.7.7.1.1.3.3.1.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.3.1"></times><ci id="S3.E6X.2.1.1.m1.7.7.1.1.3.3.2.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.3.2">ùë†</ci><ci id="S3.E6X.2.1.1.m1.7.7.1.1.3.3.3.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.3.3.3">ùëê</ci></apply></apply><apply id="S3.E6X.2.1.1.m1.7.7.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1"><times id="S3.E6X.2.1.1.m1.7.7.1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.2"></times><apply id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.1.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3">subscript</csymbol><ci id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.2.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.2">ùúÜ</ci><apply id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3"><times id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.1.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.1"></times><ci id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.2.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.2">ùë•</ci><ci id="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.3.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.3.3.3">ùë¶</ci></apply></apply><apply id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1"><plus id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.1"></plus><apply id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.1.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.2.2"><apply id="S3.E6X.2.1.1.m1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1"><minus id="S3.E6X.2.1.1.m1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1"></minus><ci id="S3.E6X.2.1.1.m1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.2">KL</ci><ci id="S3.E6X.2.1.1.m1.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.3">Loss</ci></apply><ci id="S3.E6X.2.1.1.m1.2.2.cmml" xref="S3.E6X.2.1.1.m1.2.2">ùë•</ci><apply id="S3.E6X.2.1.1.m1.3.3.cmml" xref="S3.E6X.2.1.1.m1.3.3"><ci id="S3.E6X.2.1.1.m1.3.3.1.cmml" xref="S3.E6X.2.1.1.m1.3.3.1">^</ci><ci id="S3.E6X.2.1.1.m1.3.3.2.cmml" xref="S3.E6X.2.1.1.m1.3.3.2">ùë•</ci></apply></apply><apply id="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.1.cmml" xref="S3.E6X.2.1.1.m1.7.7.1.1.1.1.1.1.3.2"><apply id="S3.E6X.2.1.1.m1.4.4.cmml" xref="S3.E6X.2.1.1.m1.4.4"><minus id="S3.E6X.2.1.1.m1.4.4.1.cmml" xref="S3.E6X.2.1.1.m1.4.4.1"></minus><ci id="S3.E6X.2.1.1.m1.4.4.2.cmml" xref="S3.E6X.2.1.1.m1.4.4.2">KL</ci><ci id="S3.E6X.2.1.1.m1.4.4.3.cmml" xref="S3.E6X.2.1.1.m1.4.4.3">Loss</ci></apply><ci id="S3.E6X.2.1.1.m1.5.5.cmml" xref="S3.E6X.2.1.1.m1.5.5">ùë¶</ci><apply id="S3.E6X.2.1.1.m1.6.6.cmml" xref="S3.E6X.2.1.1.m1.6.6"><ci id="S3.E6X.2.1.1.m1.6.6.1.cmml" xref="S3.E6X.2.1.1.m1.6.6.1">^</ci><ci id="S3.E6X.2.1.1.m1.6.6.2.cmml" xref="S3.E6X.2.1.1.m1.6.6.2">ùë¶</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6X.2.1.1.m1.7c">\displaystyle\mathcal{L}_{sc}=\lambda_{xy}\left(\operatorname{KL-Loss}({x},%
\hat{x})+\operatorname{KL-Loss}({y},\hat{y})\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E6X.2.1.1.m1.7d">caligraphic_L start_POSTSUBSCRIPT italic_s italic_c end_POSTSUBSCRIPT = italic_Œª start_POSTSUBSCRIPT italic_x italic_y end_POSTSUBSCRIPT ( start_OPFUNCTION roman_KL - roman_Loss end_OPFUNCTION ( italic_x , over^ start_ARG italic_x end_ARG ) + start_OPFUNCTION roman_KL - roman_Loss end_OPFUNCTION ( italic_y , over^ start_ARG italic_y end_ARG ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(6)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS4.p3.2">where KL-Loss is the Kullback-Leibler divergence loss, <math alttext="\hat{x}" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><mover accent="true" id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">x</mi><mo id="S3.SS4.p3.1.m1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><ci id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1">^</ci><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">ùë•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\hat{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">over^ start_ARG italic_x end_ARG</annotation></semantics></math> and <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><mover accent="true" id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">y</mi><mo id="S3.SS4.p3.2.m2.1.1.1" xref="S3.SS4.p3.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><ci id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1.1">^</ci><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">ùë¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> denote the processed ground truth 2D keypoints, respectively.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.2"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.2.1">IUV Supervision.</span>
We adopt the IUV mapping defined in DensePose¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> as the dense correspondence representation. This mapping establishes deterministic correspondences between foreground pixels in 2D images and vertices on 3D surfaces. The vertices on the template mesh can be mapped back to pixels on the foreground using a predefined bijection mapping between 3D surface space and 2D UV space.
The dense correspondence representation comprises the index of body parts <math alttext="P" class="ltx_Math" display="inline" id="S3.SS4.p4.1.m1.1"><semantics id="S3.SS4.p4.1.m1.1a"><mi id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><ci id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.1.m1.1d">italic_P</annotation></semantics></math> and UV values of mesh vertices. During training, we apply classification loss to the index of body parts <math alttext="P" class="ltx_Math" display="inline" id="S3.SS4.p4.2.m2.1"><semantics id="S3.SS4.p4.2.m2.1a"><mi id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><ci id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.2.m2.1d">italic_P</annotation></semantics></math> and regression loss to the UV channels of the dense correspondence mapping.
The loss function is formulated as follows:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E7">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E7X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{den}" class="ltx_Math" display="inline" id="S3.E7X.2.1.1.m1.1"><semantics id="S3.E7X.2.1.1.m1.1a"><msub id="S3.E7X.2.1.1.m1.1.1" xref="S3.E7X.2.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7X.2.1.1.m1.1.1.2" xref="S3.E7X.2.1.1.m1.1.1.2.cmml">‚Ñí</mi><mrow id="S3.E7X.2.1.1.m1.1.1.3" xref="S3.E7X.2.1.1.m1.1.1.3.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.3.2" xref="S3.E7X.2.1.1.m1.1.1.3.2.cmml">d</mi><mo id="S3.E7X.2.1.1.m1.1.1.3.1" xref="S3.E7X.2.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.E7X.2.1.1.m1.1.1.3.3" xref="S3.E7X.2.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.E7X.2.1.1.m1.1.1.3.1a" xref="S3.E7X.2.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.E7X.2.1.1.m1.1.1.3.4" xref="S3.E7X.2.1.1.m1.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E7X.2.1.1.m1.1b"><apply id="S3.E7X.2.1.1.m1.1.1.cmml" xref="S3.E7X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.E7X.2.1.1.m1.1.1.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.2">‚Ñí</ci><apply id="S3.E7X.2.1.1.m1.1.1.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.3"><times id="S3.E7X.2.1.1.m1.1.1.3.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.1"></times><ci id="S3.E7X.2.1.1.m1.1.1.3.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.2">ùëë</ci><ci id="S3.E7X.2.1.1.m1.1.1.3.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.3">ùëí</ci><ci id="S3.E7X.2.1.1.m1.1.1.3.4.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.4">ùëõ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7X.2.1.1.m1.1c">\displaystyle\mathcal{L}_{den}</annotation><annotation encoding="application/x-llamapun" id="S3.E7X.2.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_d italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\lambda_{pi}\text{ CrossEntropy }(P,\hat{P})" class="ltx_Math" display="inline" id="S3.E7X.3.2.2.m1.2"><semantics id="S3.E7X.3.2.2.m1.2a"><mrow id="S3.E7X.3.2.2.m1.2.3" xref="S3.E7X.3.2.2.m1.2.3.cmml"><mi id="S3.E7X.3.2.2.m1.2.3.2" xref="S3.E7X.3.2.2.m1.2.3.2.cmml"></mi><mo id="S3.E7X.3.2.2.m1.2.3.1" xref="S3.E7X.3.2.2.m1.2.3.1.cmml">=</mo><mrow id="S3.E7X.3.2.2.m1.2.3.3" xref="S3.E7X.3.2.2.m1.2.3.3.cmml"><msub id="S3.E7X.3.2.2.m1.2.3.3.2" xref="S3.E7X.3.2.2.m1.2.3.3.2.cmml"><mi id="S3.E7X.3.2.2.m1.2.3.3.2.2" xref="S3.E7X.3.2.2.m1.2.3.3.2.2.cmml">Œª</mi><mrow id="S3.E7X.3.2.2.m1.2.3.3.2.3" xref="S3.E7X.3.2.2.m1.2.3.3.2.3.cmml"><mi id="S3.E7X.3.2.2.m1.2.3.3.2.3.2" xref="S3.E7X.3.2.2.m1.2.3.3.2.3.2.cmml">p</mi><mo id="S3.E7X.3.2.2.m1.2.3.3.2.3.1" xref="S3.E7X.3.2.2.m1.2.3.3.2.3.1.cmml">‚Å¢</mo><mi id="S3.E7X.3.2.2.m1.2.3.3.2.3.3" xref="S3.E7X.3.2.2.m1.2.3.3.2.3.3.cmml">i</mi></mrow></msub><mo id="S3.E7X.3.2.2.m1.2.3.3.1" xref="S3.E7X.3.2.2.m1.2.3.3.1.cmml">‚Å¢</mo><mtext id="S3.E7X.3.2.2.m1.2.3.3.3" xref="S3.E7X.3.2.2.m1.2.3.3.3a.cmml">¬†CrossEntropy¬†</mtext><mo id="S3.E7X.3.2.2.m1.2.3.3.1a" xref="S3.E7X.3.2.2.m1.2.3.3.1.cmml">‚Å¢</mo><mrow id="S3.E7X.3.2.2.m1.2.3.3.4.2" xref="S3.E7X.3.2.2.m1.2.3.3.4.1.cmml"><mo id="S3.E7X.3.2.2.m1.2.3.3.4.2.1" stretchy="false" xref="S3.E7X.3.2.2.m1.2.3.3.4.1.cmml">(</mo><mi id="S3.E7X.3.2.2.m1.1.1" xref="S3.E7X.3.2.2.m1.1.1.cmml">P</mi><mo id="S3.E7X.3.2.2.m1.2.3.3.4.2.2" xref="S3.E7X.3.2.2.m1.2.3.3.4.1.cmml">,</mo><mover accent="true" id="S3.E7X.3.2.2.m1.2.2" xref="S3.E7X.3.2.2.m1.2.2.cmml"><mi id="S3.E7X.3.2.2.m1.2.2.2" xref="S3.E7X.3.2.2.m1.2.2.2.cmml">P</mi><mo id="S3.E7X.3.2.2.m1.2.2.1" xref="S3.E7X.3.2.2.m1.2.2.1.cmml">^</mo></mover><mo id="S3.E7X.3.2.2.m1.2.3.3.4.2.3" stretchy="false" xref="S3.E7X.3.2.2.m1.2.3.3.4.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7X.3.2.2.m1.2b"><apply id="S3.E7X.3.2.2.m1.2.3.cmml" xref="S3.E7X.3.2.2.m1.2.3"><eq id="S3.E7X.3.2.2.m1.2.3.1.cmml" xref="S3.E7X.3.2.2.m1.2.3.1"></eq><csymbol cd="latexml" id="S3.E7X.3.2.2.m1.2.3.2.cmml" xref="S3.E7X.3.2.2.m1.2.3.2">absent</csymbol><apply id="S3.E7X.3.2.2.m1.2.3.3.cmml" xref="S3.E7X.3.2.2.m1.2.3.3"><times id="S3.E7X.3.2.2.m1.2.3.3.1.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.1"></times><apply id="S3.E7X.3.2.2.m1.2.3.3.2.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.E7X.3.2.2.m1.2.3.3.2.1.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.2">subscript</csymbol><ci id="S3.E7X.3.2.2.m1.2.3.3.2.2.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.2.2">ùúÜ</ci><apply id="S3.E7X.3.2.2.m1.2.3.3.2.3.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.2.3"><times id="S3.E7X.3.2.2.m1.2.3.3.2.3.1.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.2.3.1"></times><ci id="S3.E7X.3.2.2.m1.2.3.3.2.3.2.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.2.3.2">ùëù</ci><ci id="S3.E7X.3.2.2.m1.2.3.3.2.3.3.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.2.3.3">ùëñ</ci></apply></apply><ci id="S3.E7X.3.2.2.m1.2.3.3.3a.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.3"><mtext id="S3.E7X.3.2.2.m1.2.3.3.3.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.3">¬†CrossEntropy¬†</mtext></ci><interval closure="open" id="S3.E7X.3.2.2.m1.2.3.3.4.1.cmml" xref="S3.E7X.3.2.2.m1.2.3.3.4.2"><ci id="S3.E7X.3.2.2.m1.1.1.cmml" xref="S3.E7X.3.2.2.m1.1.1">ùëÉ</ci><apply id="S3.E7X.3.2.2.m1.2.2.cmml" xref="S3.E7X.3.2.2.m1.2.2"><ci id="S3.E7X.3.2.2.m1.2.2.1.cmml" xref="S3.E7X.3.2.2.m1.2.2.1">^</ci><ci id="S3.E7X.3.2.2.m1.2.2.2.cmml" xref="S3.E7X.3.2.2.m1.2.2.2">ùëÉ</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7X.3.2.2.m1.2c">\displaystyle=\lambda_{pi}\text{ CrossEntropy }(P,\hat{P})</annotation><annotation encoding="application/x-llamapun" id="S3.E7X.3.2.2.m1.2d">= italic_Œª start_POSTSUBSCRIPT italic_p italic_i end_POSTSUBSCRIPT CrossEntropy ( italic_P , over^ start_ARG italic_P end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="3"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(7)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E7Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle+\lambda_{uv}\operatorname{SmoothL1}(U,\hat{U})" class="ltx_Math" display="inline" id="S3.E7Xa.2.1.1.m1.3"><semantics id="S3.E7Xa.2.1.1.m1.3a"><mrow id="S3.E7Xa.2.1.1.m1.3.4" xref="S3.E7Xa.2.1.1.m1.3.4.cmml"><mo id="S3.E7Xa.2.1.1.m1.3.4a" xref="S3.E7Xa.2.1.1.m1.3.4.cmml">+</mo><mrow id="S3.E7Xa.2.1.1.m1.3.4.2" xref="S3.E7Xa.2.1.1.m1.3.4.2.cmml"><msub id="S3.E7Xa.2.1.1.m1.3.4.2.2" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.cmml"><mi id="S3.E7Xa.2.1.1.m1.3.4.2.2.2" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.2.cmml">Œª</mi><mrow id="S3.E7Xa.2.1.1.m1.3.4.2.2.3" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.3.cmml"><mi id="S3.E7Xa.2.1.1.m1.3.4.2.2.3.2" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.3.2.cmml">u</mi><mo id="S3.E7Xa.2.1.1.m1.3.4.2.2.3.1" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.3.1.cmml">‚Å¢</mo><mi id="S3.E7Xa.2.1.1.m1.3.4.2.2.3.3" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.3.3.cmml">v</mi></mrow></msub><mo id="S3.E7Xa.2.1.1.m1.3.4.2.1" lspace="0.167em" xref="S3.E7Xa.2.1.1.m1.3.4.2.1.cmml">‚Å¢</mo><mrow id="S3.E7Xa.2.1.1.m1.3.4.2.3.2" xref="S3.E7Xa.2.1.1.m1.3.4.2.3.1.cmml"><mi id="S3.E7Xa.2.1.1.m1.1.1" xref="S3.E7Xa.2.1.1.m1.1.1.cmml">SmoothL1</mi><mo id="S3.E7Xa.2.1.1.m1.3.4.2.3.2a" xref="S3.E7Xa.2.1.1.m1.3.4.2.3.1.cmml">‚Å°</mo><mrow id="S3.E7Xa.2.1.1.m1.3.4.2.3.2.1" xref="S3.E7Xa.2.1.1.m1.3.4.2.3.1.cmml"><mo id="S3.E7Xa.2.1.1.m1.3.4.2.3.2.1.1" stretchy="false" xref="S3.E7Xa.2.1.1.m1.3.4.2.3.1.cmml">(</mo><mi id="S3.E7Xa.2.1.1.m1.2.2" xref="S3.E7Xa.2.1.1.m1.2.2.cmml">U</mi><mo id="S3.E7Xa.2.1.1.m1.3.4.2.3.2.1.2" xref="S3.E7Xa.2.1.1.m1.3.4.2.3.1.cmml">,</mo><mover accent="true" id="S3.E7Xa.2.1.1.m1.3.3" xref="S3.E7Xa.2.1.1.m1.3.3.cmml"><mi id="S3.E7Xa.2.1.1.m1.3.3.2" xref="S3.E7Xa.2.1.1.m1.3.3.2.cmml">U</mi><mo id="S3.E7Xa.2.1.1.m1.3.3.1" xref="S3.E7Xa.2.1.1.m1.3.3.1.cmml">^</mo></mover><mo id="S3.E7Xa.2.1.1.m1.3.4.2.3.2.1.3" stretchy="false" xref="S3.E7Xa.2.1.1.m1.3.4.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7Xa.2.1.1.m1.3b"><apply id="S3.E7Xa.2.1.1.m1.3.4.cmml" xref="S3.E7Xa.2.1.1.m1.3.4"><plus id="S3.E7Xa.2.1.1.m1.3.4.1.cmml" xref="S3.E7Xa.2.1.1.m1.3.4"></plus><apply id="S3.E7Xa.2.1.1.m1.3.4.2.cmml" xref="S3.E7Xa.2.1.1.m1.3.4.2"><times id="S3.E7Xa.2.1.1.m1.3.4.2.1.cmml" xref="S3.E7Xa.2.1.1.m1.3.4.2.1"></times><apply id="S3.E7Xa.2.1.1.m1.3.4.2.2.cmml" xref="S3.E7Xa.2.1.1.m1.3.4.2.2"><csymbol cd="ambiguous" id="S3.E7Xa.2.1.1.m1.3.4.2.2.1.cmml" xref="S3.E7Xa.2.1.1.m1.3.4.2.2">subscript</csymbol><ci id="S3.E7Xa.2.1.1.m1.3.4.2.2.2.cmml" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.2">ùúÜ</ci><apply id="S3.E7Xa.2.1.1.m1.3.4.2.2.3.cmml" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.3"><times id="S3.E7Xa.2.1.1.m1.3.4.2.2.3.1.cmml" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.3.1"></times><ci id="S3.E7Xa.2.1.1.m1.3.4.2.2.3.2.cmml" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.3.2">ùë¢</ci><ci id="S3.E7Xa.2.1.1.m1.3.4.2.2.3.3.cmml" xref="S3.E7Xa.2.1.1.m1.3.4.2.2.3.3">ùë£</ci></apply></apply><apply id="S3.E7Xa.2.1.1.m1.3.4.2.3.1.cmml" xref="S3.E7Xa.2.1.1.m1.3.4.2.3.2"><ci id="S3.E7Xa.2.1.1.m1.1.1.cmml" xref="S3.E7Xa.2.1.1.m1.1.1">SmoothL1</ci><ci id="S3.E7Xa.2.1.1.m1.2.2.cmml" xref="S3.E7Xa.2.1.1.m1.2.2">ùëà</ci><apply id="S3.E7Xa.2.1.1.m1.3.3.cmml" xref="S3.E7Xa.2.1.1.m1.3.3"><ci id="S3.E7Xa.2.1.1.m1.3.3.1.cmml" xref="S3.E7Xa.2.1.1.m1.3.3.1">^</ci><ci id="S3.E7Xa.2.1.1.m1.3.3.2.cmml" xref="S3.E7Xa.2.1.1.m1.3.3.2">ùëà</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7Xa.2.1.1.m1.3c">\displaystyle+\lambda_{uv}\operatorname{SmoothL1}(U,\hat{U})</annotation><annotation encoding="application/x-llamapun" id="S3.E7Xa.2.1.1.m1.3d">+ italic_Œª start_POSTSUBSCRIPT italic_u italic_v end_POSTSUBSCRIPT SmoothL1 ( italic_U , over^ start_ARG italic_U end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E7Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle+\lambda_{uv}\operatorname{SmoothL1}(V,\hat{V})" class="ltx_Math" display="inline" id="S3.E7Xb.2.1.1.m1.3"><semantics id="S3.E7Xb.2.1.1.m1.3a"><mrow id="S3.E7Xb.2.1.1.m1.3.4" xref="S3.E7Xb.2.1.1.m1.3.4.cmml"><mo id="S3.E7Xb.2.1.1.m1.3.4a" xref="S3.E7Xb.2.1.1.m1.3.4.cmml">+</mo><mrow id="S3.E7Xb.2.1.1.m1.3.4.2" xref="S3.E7Xb.2.1.1.m1.3.4.2.cmml"><msub id="S3.E7Xb.2.1.1.m1.3.4.2.2" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.cmml"><mi id="S3.E7Xb.2.1.1.m1.3.4.2.2.2" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.2.cmml">Œª</mi><mrow id="S3.E7Xb.2.1.1.m1.3.4.2.2.3" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.3.cmml"><mi id="S3.E7Xb.2.1.1.m1.3.4.2.2.3.2" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.3.2.cmml">u</mi><mo id="S3.E7Xb.2.1.1.m1.3.4.2.2.3.1" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.3.1.cmml">‚Å¢</mo><mi id="S3.E7Xb.2.1.1.m1.3.4.2.2.3.3" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.3.3.cmml">v</mi></mrow></msub><mo id="S3.E7Xb.2.1.1.m1.3.4.2.1" lspace="0.167em" xref="S3.E7Xb.2.1.1.m1.3.4.2.1.cmml">‚Å¢</mo><mrow id="S3.E7Xb.2.1.1.m1.3.4.2.3.2" xref="S3.E7Xb.2.1.1.m1.3.4.2.3.1.cmml"><mi id="S3.E7Xb.2.1.1.m1.1.1" xref="S3.E7Xb.2.1.1.m1.1.1.cmml">SmoothL1</mi><mo id="S3.E7Xb.2.1.1.m1.3.4.2.3.2a" xref="S3.E7Xb.2.1.1.m1.3.4.2.3.1.cmml">‚Å°</mo><mrow id="S3.E7Xb.2.1.1.m1.3.4.2.3.2.1" xref="S3.E7Xb.2.1.1.m1.3.4.2.3.1.cmml"><mo id="S3.E7Xb.2.1.1.m1.3.4.2.3.2.1.1" stretchy="false" xref="S3.E7Xb.2.1.1.m1.3.4.2.3.1.cmml">(</mo><mi id="S3.E7Xb.2.1.1.m1.2.2" xref="S3.E7Xb.2.1.1.m1.2.2.cmml">V</mi><mo id="S3.E7Xb.2.1.1.m1.3.4.2.3.2.1.2" xref="S3.E7Xb.2.1.1.m1.3.4.2.3.1.cmml">,</mo><mover accent="true" id="S3.E7Xb.2.1.1.m1.3.3" xref="S3.E7Xb.2.1.1.m1.3.3.cmml"><mi id="S3.E7Xb.2.1.1.m1.3.3.2" xref="S3.E7Xb.2.1.1.m1.3.3.2.cmml">V</mi><mo id="S3.E7Xb.2.1.1.m1.3.3.1" xref="S3.E7Xb.2.1.1.m1.3.3.1.cmml">^</mo></mover><mo id="S3.E7Xb.2.1.1.m1.3.4.2.3.2.1.3" stretchy="false" xref="S3.E7Xb.2.1.1.m1.3.4.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7Xb.2.1.1.m1.3b"><apply id="S3.E7Xb.2.1.1.m1.3.4.cmml" xref="S3.E7Xb.2.1.1.m1.3.4"><plus id="S3.E7Xb.2.1.1.m1.3.4.1.cmml" xref="S3.E7Xb.2.1.1.m1.3.4"></plus><apply id="S3.E7Xb.2.1.1.m1.3.4.2.cmml" xref="S3.E7Xb.2.1.1.m1.3.4.2"><times id="S3.E7Xb.2.1.1.m1.3.4.2.1.cmml" xref="S3.E7Xb.2.1.1.m1.3.4.2.1"></times><apply id="S3.E7Xb.2.1.1.m1.3.4.2.2.cmml" xref="S3.E7Xb.2.1.1.m1.3.4.2.2"><csymbol cd="ambiguous" id="S3.E7Xb.2.1.1.m1.3.4.2.2.1.cmml" xref="S3.E7Xb.2.1.1.m1.3.4.2.2">subscript</csymbol><ci id="S3.E7Xb.2.1.1.m1.3.4.2.2.2.cmml" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.2">ùúÜ</ci><apply id="S3.E7Xb.2.1.1.m1.3.4.2.2.3.cmml" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.3"><times id="S3.E7Xb.2.1.1.m1.3.4.2.2.3.1.cmml" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.3.1"></times><ci id="S3.E7Xb.2.1.1.m1.3.4.2.2.3.2.cmml" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.3.2">ùë¢</ci><ci id="S3.E7Xb.2.1.1.m1.3.4.2.2.3.3.cmml" xref="S3.E7Xb.2.1.1.m1.3.4.2.2.3.3">ùë£</ci></apply></apply><apply id="S3.E7Xb.2.1.1.m1.3.4.2.3.1.cmml" xref="S3.E7Xb.2.1.1.m1.3.4.2.3.2"><ci id="S3.E7Xb.2.1.1.m1.1.1.cmml" xref="S3.E7Xb.2.1.1.m1.1.1">SmoothL1</ci><ci id="S3.E7Xb.2.1.1.m1.2.2.cmml" xref="S3.E7Xb.2.1.1.m1.2.2">ùëâ</ci><apply id="S3.E7Xb.2.1.1.m1.3.3.cmml" xref="S3.E7Xb.2.1.1.m1.3.3"><ci id="S3.E7Xb.2.1.1.m1.3.3.1.cmml" xref="S3.E7Xb.2.1.1.m1.3.3.1">^</ci><ci id="S3.E7Xb.2.1.1.m1.3.3.2.cmml" xref="S3.E7Xb.2.1.1.m1.3.3.2">ùëâ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7Xb.2.1.1.m1.3c">\displaystyle+\lambda_{uv}\operatorname{SmoothL1}(V,\hat{V})</annotation><annotation encoding="application/x-llamapun" id="S3.E7Xb.2.1.1.m1.3d">+ italic_Œª start_POSTSUBSCRIPT italic_u italic_v end_POSTSUBSCRIPT SmoothL1 ( italic_V , over^ start_ARG italic_V end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS4.p4.7">where <math alttext="\hat{P}" class="ltx_Math" display="inline" id="S3.SS4.p4.3.m1.1"><semantics id="S3.SS4.p4.3.m1.1a"><mover accent="true" id="S3.SS4.p4.3.m1.1.1" xref="S3.SS4.p4.3.m1.1.1.cmml"><mi id="S3.SS4.p4.3.m1.1.1.2" xref="S3.SS4.p4.3.m1.1.1.2.cmml">P</mi><mo id="S3.SS4.p4.3.m1.1.1.1" xref="S3.SS4.p4.3.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m1.1b"><apply id="S3.SS4.p4.3.m1.1.1.cmml" xref="S3.SS4.p4.3.m1.1.1"><ci id="S3.SS4.p4.3.m1.1.1.1.cmml" xref="S3.SS4.p4.3.m1.1.1.1">^</ci><ci id="S3.SS4.p4.3.m1.1.1.2.cmml" xref="S3.SS4.p4.3.m1.1.1.2">ùëÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m1.1c">\hat{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.3.m1.1d">over^ start_ARG italic_P end_ARG</annotation></semantics></math>,<math alttext="\hat{U}" class="ltx_Math" display="inline" id="S3.SS4.p4.4.m2.1"><semantics id="S3.SS4.p4.4.m2.1a"><mover accent="true" id="S3.SS4.p4.4.m2.1.1" xref="S3.SS4.p4.4.m2.1.1.cmml"><mi id="S3.SS4.p4.4.m2.1.1.2" xref="S3.SS4.p4.4.m2.1.1.2.cmml">U</mi><mo id="S3.SS4.p4.4.m2.1.1.1" xref="S3.SS4.p4.4.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m2.1b"><apply id="S3.SS4.p4.4.m2.1.1.cmml" xref="S3.SS4.p4.4.m2.1.1"><ci id="S3.SS4.p4.4.m2.1.1.1.cmml" xref="S3.SS4.p4.4.m2.1.1.1">^</ci><ci id="S3.SS4.p4.4.m2.1.1.2.cmml" xref="S3.SS4.p4.4.m2.1.1.2">ùëà</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m2.1c">\hat{U}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.4.m2.1d">over^ start_ARG italic_U end_ARG</annotation></semantics></math>, and <math alttext="\hat{V}" class="ltx_Math" display="inline" id="S3.SS4.p4.5.m3.1"><semantics id="S3.SS4.p4.5.m3.1a"><mover accent="true" id="S3.SS4.p4.5.m3.1.1" xref="S3.SS4.p4.5.m3.1.1.cmml"><mi id="S3.SS4.p4.5.m3.1.1.2" xref="S3.SS4.p4.5.m3.1.1.2.cmml">V</mi><mo id="S3.SS4.p4.5.m3.1.1.1" xref="S3.SS4.p4.5.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.5.m3.1b"><apply id="S3.SS4.p4.5.m3.1.1.cmml" xref="S3.SS4.p4.5.m3.1.1"><ci id="S3.SS4.p4.5.m3.1.1.1.cmml" xref="S3.SS4.p4.5.m3.1.1.1">^</ci><ci id="S3.SS4.p4.5.m3.1.1.2.cmml" xref="S3.SS4.p4.5.m3.1.1.2">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.5.m3.1c">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.5.m3.1d">over^ start_ARG italic_V end_ARG</annotation></semantics></math> denote the ground truth of <math alttext="P" class="ltx_Math" display="inline" id="S3.SS4.p4.6.m4.1"><semantics id="S3.SS4.p4.6.m4.1a"><mi id="S3.SS4.p4.6.m4.1.1" xref="S3.SS4.p4.6.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.6.m4.1b"><ci id="S3.SS4.p4.6.m4.1.1.cmml" xref="S3.SS4.p4.6.m4.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.6.m4.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.6.m4.1d">italic_P</annotation></semantics></math> and <math alttext="UV" class="ltx_Math" display="inline" id="S3.SS4.p4.7.m5.1"><semantics id="S3.SS4.p4.7.m5.1a"><mrow id="S3.SS4.p4.7.m5.1.1" xref="S3.SS4.p4.7.m5.1.1.cmml"><mi id="S3.SS4.p4.7.m5.1.1.2" xref="S3.SS4.p4.7.m5.1.1.2.cmml">U</mi><mo id="S3.SS4.p4.7.m5.1.1.1" xref="S3.SS4.p4.7.m5.1.1.1.cmml">‚Å¢</mo><mi id="S3.SS4.p4.7.m5.1.1.3" xref="S3.SS4.p4.7.m5.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.7.m5.1b"><apply id="S3.SS4.p4.7.m5.1.1.cmml" xref="S3.SS4.p4.7.m5.1.1"><times id="S3.SS4.p4.7.m5.1.1.1.cmml" xref="S3.SS4.p4.7.m5.1.1.1"></times><ci id="S3.SS4.p4.7.m5.1.1.2.cmml" xref="S3.SS4.p4.7.m5.1.1.2">ùëà</ci><ci id="S3.SS4.p4.7.m5.1.1.3.cmml" xref="S3.SS4.p4.7.m5.1.1.3">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.7.m5.1c">UV</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.7.m5.1d">italic_U italic_V</annotation></semantics></math> values, respectively.</p>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="544" id="S4.F4.g1" src="extracted/5523510/image/4.1.png" width="538"/>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">
On the top left is a figure of the dual camera system we created. This system consists of an RGB camera and a lensless imaging system using a diffuser as a mask. The top right shows the process of collecting our dataset. The image at the bottom is a record of the process of collecting our dataset.</span></figcaption>
</figure>
<section class="ltx_subsection ltx_indent_first" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Imaging System and Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Lensless Imaging System.</span>
Due to the scarcity of human pose and shape datasets based on lensless imaging systems, we developed a basic lensless system to gather experimental data.
The system, illustrated in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.F4" title="Figure 4 ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>, comprises two primary components: a lensless imaging system for capturing lensless measurements and an RGB camera for capturing real images corresponding to the lensless measurements. To ensure identical light paths, we employed a beam splitter between the lensless sensor and the RGB camera. This allowed for consistent imaging conditions and facilitated accurate correlation between the lensless measurements and the corresponding real images.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">For the lensless imaging part of the system, we adopted the Mask-Modulated Lensless System, inspired by design options such as diffusercam¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> and phlatcam¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. Specifically, we selected a diffuser as our mask and incorporated PhlatCam‚Äôs concept of using larger sensors to enhance global information perception.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Datasets.</span>
The inputs to LPSNet are lensless measurements. However, datasets for classical human pose estimation are not directly available at present. Leveraging the lensless imaging system we have constructed, and the mathematical model of the system‚Äôs imaging, the sources of datasets for our experiments can be categorized as follows:
</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Real Dataset.</span>
Using a lensless imaging system to capture images displayed on a screen as measurements is currently the primary method of acquiring datasets in the lensless field.
We used this approach to collect datasets from various sources, including Human3.6M, MPII, COCO, 3DPW, and MIP-INF-3DHP datasets, which we refer to as LenslessHuman3.6M, LenslessMPII, LenslessMIP-INF-3DHP, LenslessCOCO, and Lensless3DPW, respectively.
We also reclassified the training dataset and validation dataset on LenslessHuman3.6M, referred to as train-LenslessHuman3.6M and eval-LenslessHuman3.6M, respectively.
Moreover, we capture the real scenes of different individuals, backgrounds, and light intensities using our lensless imaging system.
</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Simulated Dataset.</span>
The mathematical imaging model enables the conversion (simulation) of images captured by RGB cameras into measurements captured by lensless imaging systems.
This technique is commonly employed in lensless imaging fields for testing systems and imaging methods. Leveraging this, we converted numerous human pose datasets into simulated lensless measurements. These simulated datasets extended our training set and provided a quick validation of our method.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.7">The dimensions of the raw measurements are <math alttext="1280\times 1024\times 3" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">1280</mn><mo id="S4.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">1024</mn><mo id="S4.SS2.p1.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p1.1.m1.1.1.4" xref="S4.SS2.p1.1.m1.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn id="S4.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.2">1280</cn><cn id="S4.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.3">1024</cn><cn id="S4.SS2.p1.1.m1.1.1.4.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">1280\times 1024\times 3</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">1280 √ó 1024 √ó 3</annotation></semantics></math>.
Prior to commencing the experiments, we first crop and resize the lensless measurements.
These measurements are pre-processed to <math alttext="224\times 224\times 3" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">224</mn><mo id="S4.SS2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">224</mn><mo id="S4.SS2.p1.2.m2.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p1.2.m2.1.1.4" xref="S4.SS2.p1.2.m2.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn id="S4.SS2.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1.2">224</cn><cn id="S4.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1.3">224</cn><cn id="S4.SS2.p1.2.m2.1.1.4.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">224\times 224\times 3</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">224 √ó 224 √ó 3</annotation></semantics></math> before being fed into the network.
Following this pre-processing step, the lensless measurements inputted into the global perceptual layer are <math alttext="224\times 224" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">224</mn><mo id="S4.SS2.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.3.m3.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><times id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></times><cn id="S4.SS2.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS2.p1.3.m3.1.1.2">224</cn><cn id="S4.SS2.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS2.p1.3.m3.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">224 √ó 224</annotation></semantics></math>, generating spatial features at resolutions of <math alttext="(7\times 7,14\times 14,28\times 28," class="ltx_math_unparsed" display="inline" id="S4.SS2.p1.4.m4.1"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1b"><mo id="S4.SS2.p1.4.m4.1.1" stretchy="false">(</mo><mn id="S4.SS2.p1.4.m4.1.2">7</mn><mo id="S4.SS2.p1.4.m4.1.3" lspace="0.222em" rspace="0.222em">√ó</mo><mn id="S4.SS2.p1.4.m4.1.4">7</mn><mo id="S4.SS2.p1.4.m4.1.5">,</mo><mn id="S4.SS2.p1.4.m4.1.6">14</mn><mo id="S4.SS2.p1.4.m4.1.7" lspace="0.222em" rspace="0.222em">√ó</mo><mn id="S4.SS2.p1.4.m4.1.8">14</mn><mo id="S4.SS2.p1.4.m4.1.9">,</mo><mn id="S4.SS2.p1.4.m4.1.10">28</mn><mo id="S4.SS2.p1.4.m4.1.11" lspace="0.222em" rspace="0.222em">√ó</mo><mn id="S4.SS2.p1.4.m4.1.12">28</mn><mo id="S4.SS2.p1.4.m4.1.13">,</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">(7\times 7,14\times 14,28\times 28,</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.4.m4.1d">( 7 √ó 7 , 14 √ó 14 , 28 √ó 28 ,</annotation></semantics></math> and <math alttext="56\times 56)" class="ltx_math_unparsed" display="inline" id="S4.SS2.p1.5.m5.1"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1b"><mn id="S4.SS2.p1.5.m5.1.1">56</mn><mo id="S4.SS2.p1.5.m5.1.2" lspace="0.222em" rspace="0.222em">√ó</mo><mn id="S4.SS2.p1.5.m5.1.3">56</mn><mo id="S4.SS2.p1.5.m5.1.4" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">56\times 56)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.5.m5.1d">56 √ó 56 )</annotation></semantics></math>.
During the generation of mesh-aligned features, the SMPL mesh is down-sampled using a pre-computed downsampling matrix provided in ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>, resulting in a reduction of vertex count from 6890 to 431.
The regressors <math alttext="\mathcal{R}_{t}" class="ltx_Math" display="inline" id="S4.SS2.p1.6.m6.1"><semantics id="S4.SS2.p1.6.m6.1a"><msub id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">‚Ñõ</mi><mi id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">‚Ñõ</ci><ci id="S4.SS2.p1.6.m6.1.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">\mathcal{R}_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.6.m6.1d">caligraphic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> follow a congruent architectural framework to the regressor present in HMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, albeit with slight variations in their input dimensions.
Our network is trained using the Adam optimizer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> with a learning rate of <math alttext="5\times 10^{-5}" class="ltx_Math" display="inline" id="S4.SS2.p1.7.m7.1"><semantics id="S4.SS2.p1.7.m7.1a"><mrow id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml"><mn id="S4.SS2.p1.7.m7.1.1.2" xref="S4.SS2.p1.7.m7.1.1.2.cmml">5</mn><mo id="S4.SS2.p1.7.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.7.m7.1.1.1.cmml">√ó</mo><msup id="S4.SS2.p1.7.m7.1.1.3" xref="S4.SS2.p1.7.m7.1.1.3.cmml"><mn id="S4.SS2.p1.7.m7.1.1.3.2" xref="S4.SS2.p1.7.m7.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.p1.7.m7.1.1.3.3" xref="S4.SS2.p1.7.m7.1.1.3.3.cmml"><mo id="S4.SS2.p1.7.m7.1.1.3.3a" xref="S4.SS2.p1.7.m7.1.1.3.3.cmml">‚àí</mo><mn id="S4.SS2.p1.7.m7.1.1.3.3.2" xref="S4.SS2.p1.7.m7.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><apply id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1"><times id="S4.SS2.p1.7.m7.1.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1.1"></times><cn id="S4.SS2.p1.7.m7.1.1.2.cmml" type="integer" xref="S4.SS2.p1.7.m7.1.1.2">5</cn><apply id="S4.SS2.p1.7.m7.1.1.3.cmml" xref="S4.SS2.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.7.m7.1.1.3.1.cmml" xref="S4.SS2.p1.7.m7.1.1.3">superscript</csymbol><cn id="S4.SS2.p1.7.m7.1.1.3.2.cmml" type="integer" xref="S4.SS2.p1.7.m7.1.1.3.2">10</cn><apply id="S4.SS2.p1.7.m7.1.1.3.3.cmml" xref="S4.SS2.p1.7.m7.1.1.3.3"><minus id="S4.SS2.p1.7.m7.1.1.3.3.1.cmml" xref="S4.SS2.p1.7.m7.1.1.3.3"></minus><cn id="S4.SS2.p1.7.m7.1.1.3.3.2.cmml" type="integer" xref="S4.SS2.p1.7.m7.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">5\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.7.m7.1d">5 √ó 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> and a batch size of 80 on 4 NVIDIA RTX2080 Ti GPUs. No learning rate decay is applied during training.
</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="472" id="S4.F5.g1" src="extracted/5523510/image/4.32.2.png" width="568"/>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">At the top is the frame of our baseline. Lensless measurements first recover the image by reconstruction methods and then estimate human pose and shape by PyMAF.
The bottom image compares the recovered image with the original image, and we can see that the quality of the recovered image has dropped dramatically.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>3D Human Pose and Shape Comparison.</h3>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="285" id="S4.F6.g1" src="extracted/5523510/4.6.png" width="568"/>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.5.2.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F6.2.1" style="font-size:90%;">Qualitative results of LPSNet on challenging LenslessHuman3.6M.<span class="ltx_text ltx_font_medium" id="S4.F6.2.1.1"> The results on the left are from our LPSNet estimation, those in the center are from our baseline (PyMAF) output, and those on the right are from the fine-tuned baseline (PyMAF<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.F6.2.1.1.m1.1"><semantics id="S4.F6.2.1.1.m1.1b"><msup id="S4.F6.2.1.1.m1.1.1" xref="S4.F6.2.1.1.m1.1.1.cmml"><mi id="S4.F6.2.1.1.m1.1.1b" xref="S4.F6.2.1.1.m1.1.1.cmml"></mi><mo id="S4.F6.2.1.1.m1.1.1.1" mathvariant="normal" xref="S4.F6.2.1.1.m1.1.1.1.cmml">‚Ä†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.F6.2.1.1.m1.1c"><apply id="S4.F6.2.1.1.m1.1.1.cmml" xref="S4.F6.2.1.1.m1.1.1"><ci id="S4.F6.2.1.1.m1.1.1.1.cmml" xref="S4.F6.2.1.1.m1.1.1.1">normal-‚Ä†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.2.1.1.m1.1d">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.F6.2.1.1.m1.1e">start_FLOATSUPERSCRIPT ‚Ä† end_FLOATSUPERSCRIPT</annotation></semantics></math>) output. LPSNet can be seen to be better than the baselines.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Baseline Approach.</span>
We are the first work to perform human pose and shape estimation through lensless measurements. Given the absence of other valid methods for comparison, we designed a baseline approach that approaches this as a two-stage task. The baseline architecture is depicted in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.F5" title="Figure 5 ‚Ä£ 4.2 Implementation Details ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>.
In the first stage, we utilized the lensless image reconstruction method Rego <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">et. al.</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, chosen for its wide applicability in recovering images acquired by lensless imaging systems. In the second stage, PyMAF is used to estimate human pose and shape.
</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.3.3.4"><span class="ltx_text" id="S4.T1.3.3.4.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1">
<span class="ltx_text" id="S4.T1.1.1.1.1" style="font-size:90%;">MPJPE </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.2.2">
<span class="ltx_text" id="S4.T1.2.2.2.1" style="font-size:90%;">PA-MPJPE</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T1.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.3.3.3">
<span class="ltx_text" id="S4.T1.3.3.3.1" style="font-size:90%;">PVE </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T1.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.m1.1d">‚Üì</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.4.5.1.1"><span class="ltx_text" id="S4.T1.4.5.1.1.1" style="font-size:90%;">baseline (PyMAF)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.5.1.2"><span class="ltx_text" id="S4.T1.4.5.1.2.1" style="font-size:90%;">257.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.5.1.3"><span class="ltx_text" id="S4.T1.4.5.1.3.1" style="font-size:90%;">121.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.5.1.4"><span class="ltx_text" id="S4.T1.4.5.1.4.1" style="font-size:90%;">277.18</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.4.4.1">
<span class="ltx_text" id="S4.T1.4.4.1.1" style="font-size:90%;">baseline (PyMAF</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.T1.4.4.1.m1.1"><semantics id="S4.T1.4.4.1.m1.1a"><msup id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml"><mi id="S4.T1.4.4.1.m1.1.1a" xref="S4.T1.4.4.1.m1.1.1.cmml"></mi><mo id="S4.T1.4.4.1.m1.1.1.1" mathsize="90%" xref="S4.T1.4.4.1.m1.1.1.1.cmml">‚Ä†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><apply id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1"><ci id="S4.T1.4.4.1.m1.1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1.1">‚Ä†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.1.m1.1d">start_FLOATSUPERSCRIPT ‚Ä† end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T1.4.4.1.2" style="font-size:90%;">)</span>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.2"><span class="ltx_text" id="S4.T1.4.4.2.1" style="font-size:90%;">126.28</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.3"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.3.1" style="font-size:90%;">81.37</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.4"><span class="ltx_text" id="S4.T1.4.4.4.1" style="font-size:90%;">151.60</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.4.6.2.1"><span class="ltx_text" id="S4.T1.4.6.2.1.1" style="font-size:90%;">LPSNet</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.6.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.4.6.2.2.1" style="font-size:90%;">119.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.6.2.3"><span class="ltx_text" id="S4.T1.4.6.2.3.1" style="font-size:90%;">81.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.6.2.4"><span class="ltx_text ltx_font_bold" id="S4.T1.4.6.2.4.1" style="font-size:90%;">134.74</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison with baseline. The baseline is a combination of the two-stage approach using existing methods. LPSNet outperforms the baseline on the LenslessHuman3.6M datasets.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.3"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.3.1">Comparison with Baseline.</span>
We present the results of the quantitative comparative evaluation on the LenslessHuman3.6M dataset in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.T1" title="Table 1 ‚Ä£ 4.3 3D Human Pose and Shape Comparison. ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>.
Notably, images recovered from lensless measurements exhibit lower quality. Directly comparing the baseline (PyMAF) with our approach is challenging and unfair. Therefore, we fine-tuned PyMAF on the train-LenslessHuman3.6M dataset, denoted as PyMAF<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><msup id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1a" xref="S4.SS3.p2.1.m1.1.1.cmml"></mi><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">‚Ä†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><ci id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1">‚Ä†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">start_FLOATSUPERSCRIPT ‚Ä† end_FLOATSUPERSCRIPT</annotation></semantics></math>.
Referring to ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.T1" title="Table 1 ‚Ä£ 4.3 3D Human Pose and Shape Comparison. ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, we observe significant improvements in the indicators of baseline(PyMAF<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><msup id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1a" xref="S4.SS3.p2.2.m2.1.1.cmml"></mi><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">‚Ä†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><ci id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1">‚Ä†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">start_FLOATSUPERSCRIPT ‚Ä† end_FLOATSUPERSCRIPT</annotation></semantics></math>). Specifically, compared with the baseline(PyMAF<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><msup id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1a" xref="S4.SS3.p2.3.m3.1.1.cmml"></mi><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">‚Ä†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><ci id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1">‚Ä†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">start_FLOATSUPERSCRIPT ‚Ä† end_FLOATSUPERSCRIPT</annotation></semantics></math>), the MPJPE of LPSNet on the LenslessHuman3.6M dataset is reduced by 7.08 mm.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">As shown in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.T1" title="Table 1 ‚Ä£ 4.3 3D Human Pose and Shape Comparison. ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, our LPSNet achieved competitive results compared to the baseline approach (PyMAF<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><msup id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1a" xref="S4.SS3.p3.1.m1.1.1.cmml"></mi><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">‚Ä†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><ci id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1">‚Ä†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">start_FLOATSUPERSCRIPT ‚Ä† end_FLOATSUPERSCRIPT</annotation></semantics></math>). LPSNet demonstrates more significant improvements in the MPJPE and PVE metrics. However, we would argue that the PA-MPJPE metric may not fully reveal the performance of the mesh-image alignment, as it is calculated as MPJPE after rigid alignment.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Baseline (PyMAF) has difficulty performing accurate pose estimation on low-quality recovered images. However, the outputs of our LPSNet exhibit significant improvements compared to the two baseline methods, especially for limb estimation.
Qualitative comparison results are illustrated in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.F6" title="Figure 6 ‚Ä£ 4.3 3D Human Pose and Shape Comparison. ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.3" style="width:231.5pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S4.T2.3.3"><span class="ltx_text" id="S4.T2.3.3.3" style="font-size:90%;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.3.3.3.3">
<span class="ltx_thead">
<span class="ltx_tr" id="S4.T2.3.3.3.3.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.3.3.3.3.3.4"><span class="ltx_text" id="S4.T2.3.3.3.3.3.4.1" style="color:#000000;">Method</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.1.1"><span class="ltx_text" id="S4.T2.1.1.1.1.1.1.1" style="color:#000000;">MPJPE </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.1.1.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T2.1.1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.2.2.2.2.2"><span class="ltx_text" id="S4.T2.2.2.2.2.2.2.1" style="color:#000000;">PA-MPJPE</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.2.2.2.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T2.2.2.2.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.2.2.m1.1d">‚Üì</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.3.3.3.3.3.3"><span class="ltx_text" id="S4.T2.3.3.3.3.3.3.1" style="color:#000000;">PVE </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.3.3.3.m1.1a"><mo id="S4.T2.3.3.3.3.3.3.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T2.3.3.3.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.3.3.3.m1.1d">‚Üì</annotation></semantics></math></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S4.T2.3.3.3.3.4.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.3.3.3.4.1.1"><span class="ltx_text" id="S4.T2.3.3.3.3.4.1.1.1" style="color:#000000;">w/o MSFDcoder and DHAS</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.4.1.2"><span class="ltx_text" id="S4.T2.3.3.3.3.4.1.2.1" style="color:#000000;">142.13</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.4.1.3"><span class="ltx_text" id="S4.T2.3.3.3.3.4.1.3.1" style="color:#000000;">92.20</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.4.1.4"><span class="ltx_text" id="S4.T2.3.3.3.3.4.1.4.1" style="color:#000000;">161.07</span></span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.5.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.3.3.3.5.2.1"><span class="ltx_text" id="S4.T2.3.3.3.3.5.2.1.1" style="color:#000000;">w/o DHAS</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.5.2.2"><span class="ltx_text" id="S4.T2.3.3.3.3.5.2.2.1" style="color:#000000;">139.34</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.5.2.3"><span class="ltx_text" id="S4.T2.3.3.3.3.5.2.3.1" style="color:#000000;">92.56</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.5.2.4"><span class="ltx_text" id="S4.T2.3.3.3.3.5.2.4.1" style="color:#000000;">158.97</span></span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.6.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.3.3.3.6.3.1"><span class="ltx_text" id="S4.T2.3.3.3.3.6.3.1.1" style="color:#000000;">w/o DHAS (2D keypoint)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.6.3.2"><span class="ltx_text" id="S4.T2.3.3.3.3.6.3.2.1" style="color:#000000;">123.30</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.6.3.3"><span class="ltx_text" id="S4.T2.3.3.3.3.6.3.3.1" style="color:#000000;">83.41</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.6.3.4"><span class="ltx_text" id="S4.T2.3.3.3.3.6.3.4.1" style="color:#000000;">138.67</span></span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.7.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.3.3.3.7.4.1"><span class="ltx_text" id="S4.T2.3.3.3.3.7.4.1.1" style="color:#000000;">w/o DHAS (IUV)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.7.4.2"><span class="ltx_text" id="S4.T2.3.3.3.3.7.4.2.1" style="color:#000000;">129.70</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.7.4.3"><span class="ltx_text" id="S4.T2.3.3.3.3.7.4.3.1" style="color:#000000;">86.83</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.7.4.4"><span class="ltx_text" id="S4.T2.3.3.3.3.7.4.4.1" style="color:#000000;">146.7</span></span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.8.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.3.3.3.3.8.5.1"><span class="ltx_text" id="S4.T2.3.3.3.3.8.5.1.1" style="color:#000000;">LPSNet</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.3.3.8.5.2"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.8.5.2.1" style="color:#000000;">119.20</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.3.3.8.5.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.8.5.3.1" style="color:#000000;">81.52</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.3.3.8.5.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.8.5.4.1" style="color:#000000;">134.74</span></span></span>
</span>
</span><span class="ltx_text" id="S4.T2.3.3.3.4" style="color:#000000;"></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation study on LPSNet. DHAS stands for Double-Head Auxiliary Supervision.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="201" id="S4.F7.g1" src="extracted/5523510/image/4.4.3.png" width="568"/>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.3.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F7.4.2" style="font-size:90%;">Qualitative results of LPSNet on challenging LenslessHuman3.6M.<span class="ltx_text ltx_font_medium" id="S4.F7.4.2.1">We can see that the results of LPSNet are better, and the results are improved with the addition of the double-head assisted supervision mechanism.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In this section, we conduct ablation studies on LenslessHuman3.6M under various settings to validate the effectiveness of the key components proposed in our method.
</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">All ablation variants were trained and tested on LenslessHuman3.6M, which is derived from the Human3.6M dataset. The Human3.6M dataset includes ground-truth 3D labels and serves as a widely used benchmark in 3D human pose and shape estimation.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p3.1.1">Multi-Scale Lensless Feature Decoder.</span>
In LPSNet, the decoder primarily decodes the global information of lensless measurements encoded by the diffuser, which plays a crucial role in our end-to-end system.
Our MSFDecoder is essential for efficient feature extraction from lensless measurements. We have adapted another variant of the decoder to verify this.
Specifically, we simplified the global perception layer of the decoder to a series of convolutional layers and deconvolutional layers.
We use this simpler decoder in the experiment(w/o MSFDcoder and DHAS) instead of MSFDecoder.
As shown in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.T2" title="Table 2 ‚Ä£ 4.3 3D Human Pose and Shape Comparison. ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>, comparing the result of experiments(w/o MSFDcoder and DHAS) and experiment(w/o DHAS), we can see that MSFDecoder improves the accuracy of the human pose and shape estimation significantly.
Through the qualitative experiment in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.F7" title="Figure 7 ‚Ä£ 4.3 3D Human Pose and Shape Comparison. ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">7</span></a>, we can also see that the human pose and shape estimation has a better alignment after using the MSFDecoder.
Note that neither experiment(w/o MSFDcoder and DHAS) nor the experiment(w/o DHAS) used auxiliary supervision.
</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p4.1.1">Double-Head Auxiliary Supervision.</span>
Double-Head Auxiliary Supervision is mainly used to improve the accuracy of human limbs.
For each auxiliary supervisory head, we conducted ablation experiments respectively.
Referring to the result in ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.T2" title="Table 2 ‚Ä£ 4.3 3D Human Pose and Shape Comparison. ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>, LPSNet has significant improvements in the MPJPE and the PVE metrics.
¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.F7" title="Figure 7 ‚Ä£ 4.3 3D Human Pose and Shape Comparison. ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">7</span></a> shows additional qualitative comparison results. We can see that the alignment of the human pose and shape is better with the addition of the Double-Head Auxiliary Supervision mechanism.
</p>
</div>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_inline-para ltx_minipage ltx_flex_size_1 ltx_align_top" id="S4.F8.1" style="width:129.2pt;">
<span class="ltx_para ltx_align_center" id="S4.F8.1.p1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="422" id="S4.F8.1.p1.g1" src="extracted/5523510/image/f6.1.2.png" width="685"/>
<span class="ltx_p ltx_align_center" id="S4.F8.1.p1.1"><span class="ltx_text" id="S4.F8.1.p1.1.1" style="font-size:90%;">(a) Result on more datasets</span></span>
</span></span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_inline-para ltx_minipage ltx_flex_size_1 ltx_align_top" id="S4.F8.2" style="width:109.3pt;">
<span class="ltx_para ltx_align_center" id="S4.F8.2.p1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="485" id="S4.F8.2.p1.g1" src="extracted/5523510/image/f4.2.2.png" width="685"/>
<span class="ltx_p ltx_align_center" id="S4.F8.2.p1.1"><span class="ltx_text" id="S4.F8.2.p1.1.1" style="font-size:90%;">(b) Result on real scenes</span></span>
</span></span></div>
</div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Results on more datasets and real scenes. From left to right for each set of figures: lensless measurement, alignment of results with RGB images, and 3D results shown in different views.</figcaption>
</figure>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Results on more Datasets and Real Scene</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.F8" title="Figure 8 ‚Ä£ 4.4 Ablation Study ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">8</span></a> (a) shows the qualitative results on various datasets after using the mixed datasets for training.
Specifically, we collected the MPII, COCO, 3DPW, MIP-INF-3DHP, and LSP datasets through our lensless imaging system and mixed them for training and testing.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">In addition, we also capture the real scenes of two different people, in different backgrounds and different light intensities using our lensless imaging system. ¬†<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#S4.F8" title="Figure 8 ‚Ä£ 4.4 Ablation Study ‚Ä£ 4 Experiments ‚Ä£ LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">8</span></a> (b) illustrates the experimental results of our method on these real scenes, indicating its suitability for real-world applications.</p>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Conclusion.</span>
In this paper, we propose the first end-to-end framework to recover 3D human poses and shapes from lensless measurements to the best of our knowledge.
Specifically, we design a multi-scale lensless feature decoder, which can effectively decode the information produced by the lensless imaging system.
We also propose a double auxiliary supervision mechanism to improve the accuracy of human limb end estimation.
Experimental results show that our method can achieve end-to-end human pose and shape estimation through lensless measurements.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Limitations and Future Work.</span>
Our work is an initial stride step in this direction, however, the results are still not as robust as those achieved by traditional methods for estimating the human body from RGB images.
The current scarcity of lensless datasets hampers our ability to pre-train the MSFDecoder using a large dataset like ImageNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.01941v3#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, thereby limiting the generalizability of our approach.
We will continue this work in the future, focusing on generating more extensive lensless datasets and progressing towards practical applications.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Acknowledgments.</span>
This work was supported in part by the National Key R&amp;D Program of China (2023YFC3082100), the National Natural Science Foundation of China (62122058 and 62171317), and the Science Fund for Distinguished Young Scholars of Tianjin (No. 22JCJQJC00040).


<span class="ltx_text" id="S5.p3.1.2" style="font-size:90%;"></span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Adams et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Jesse¬†K Adams, Vivek Boominathan, Benjamin¬†W Avants, Daniel¬†G Vercosa, Fan Ye, Richard¬†G Baraniuk, Jacob¬†T Robinson, and Ashok Veeraraghavan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Single-Frame 3D Fluorescence Microscopy with Ultraminiature Lensless FlatScope.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.9.1" style="font-size:90%;">Sci Adv.</em><span class="ltx_text" id="bib.bib1.10.2" style="font-size:90%;">, 3(12):e1701548, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Antipa et¬†al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Nick Antipa, Grace Kuo, Reinhard Heckel, Ben Mildenhall, Emrah Bostan, Ren Ng, and Laura Waller.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">DiffuserCam: Lensless Single-exposure 3D Imaging.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.9.1" style="font-size:90%;">Optica</em><span class="ltx_text" id="bib.bib2.10.2" style="font-size:90%;">, 5(1):1‚Äì9, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Arnab et¬†al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Anurag Arnab, Carl Doersch, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Exploiting Temporal Context for 3D Human Pose Estimation in the ild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.10.2" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog.</em><span class="ltx_text" id="bib.bib3.11.3" style="font-size:90%;">, pages 3395‚Äì3404, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Asif et¬†al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
M¬†Salman Asif, Ali Ayremlou, Aswin Sankaranarayanan, Ashok Veeraraghavan, and Richard¬†G Baraniuk.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">Flatcam: Thin, Lensless Cameras using Coded Aperture and Computation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.9.1" style="font-size:90%;">IEEE Trans Comput Imaging.</em><span class="ltx_text" id="bib.bib4.10.2" style="font-size:90%;">, 3(3):384‚Äì397, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Bogo et¬†al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael¬†J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.10.2" style="font-size:90%;">Eur. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib5.11.3" style="font-size:90%;"> Springer International Publishing, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Boominathan et¬†al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Vivek Boominathan, Jesse¬†K Adams, M¬†Salman Asif, Benjamin¬†W Avants, Jacob¬†T Robinson, Richard¬†G Baraniuk, Aswin¬†C Sankaranarayanan, and Ashok Veeraraghavan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">Lensless Imaging: A Computational Renaissance.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.9.1" style="font-size:90%;">IEEE Signal Process Mag.</em><span class="ltx_text" id="bib.bib6.10.2" style="font-size:90%;">, 33(5):23‚Äì35, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Boominathan et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Vivek Boominathan, Jesse¬†K Adams, Jacob¬†T Robinson, and Ashok Veeraraghavan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Phlatcam: Designed Phase-Mask Based Thin Lensless Camera.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.9.1" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</em><span class="ltx_text" id="bib.bib7.10.2" style="font-size:90%;">, 42(7):1618‚Äì1629, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Cho et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Cross-attention of disentangled modalities for 3d human mesh recovery with transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">Eur. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, pages 342‚Äì359. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.4.4.1" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.6.1" style="font-size:90%;">
J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.9.2" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. Workshops (CVPR Workshops)</em><span class="ltx_text" id="bib.bib9.10.3" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Georgakis et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Georgios Georgakis, Ren Li, Srikrishna Karanam, Terrence Chen, Jana Ko≈°eck√°, and Ziyan Wu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Hierarchical Kinematic Human Mesh Recovery.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.10.2" style="font-size:90%;">Eur. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib10.11.3" style="font-size:90%;">, pages 768‚Äì784. Springer, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Goel et¬†al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Humans in 4D: Reconstructing and Tracking Humans with Transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.10.2" style="font-size:90%;">Int. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib11.11.3" style="font-size:90%;">, pages 14783‚Äì14794, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.4.4.1" style="font-size:90%;">Goodman [2005]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.6.1" style="font-size:90%;">
Joseph¬†W Goodman.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.7.1" style="font-size:90%;">Introduction to Fourier optics</em><span class="ltx_text" id="bib.bib12.8.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.9.1" style="font-size:90%;">Roberts and Company publishers, 2005.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">G√ºler et¬†al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Rƒ±za¬†Alp G√ºler, Natalia Neverova, and Iasonas Kokkinos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">Densepose: Dense Human Pose Estimation in the Wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.2" style="font-size:90%;">Int. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib13.11.3" style="font-size:90%;">, pages 7297‚Äì7306, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Ionescu et¬†al. [2013]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Human3. 6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.9.1" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</em><span class="ltx_text" id="bib.bib14.10.2" style="font-size:90%;">, 36(7):1325‚Äì1339, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Joo et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">Int. Conf. 3D. Vis.</em><span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">, pages 42‚Äì52. IEEE, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Kanazawa et¬†al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Angjoo Kanazawa, Michael¬†J. Black, David¬†W. Jacobs, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">End-to-end Recovery of Human Shape and Pose.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.10.2" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog.</em><span class="ltx_text" id="bib.bib16.11.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.4.4.1" style="font-size:90%;">Kingma and Ba [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.6.1" style="font-size:90%;">
Diederik¬†P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.8.1" style="font-size:90%;">arXiv preprint arXiv:1412.6980</em><span class="ltx_text" id="bib.bib17.9.2" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Kocabas et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Muhammed Kocabas, Chun-Hao¬†P Huang, Otmar Hilliges, and Michael¬†J Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">PARE: Part Attention Regressor for 3D Human Body Estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.10.2" style="font-size:90%;">Int. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib18.11.3" style="font-size:90%;">, pages 11127‚Äì11137, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Kolotouros et¬†al. [2019a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Nikos Kolotouros, Georgios Pavlakos, Michael¬†J Black, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">Learning to Reconstruct 3D Human Pose and Shape Via Model-fitting in the Loop.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.10.2" style="font-size:90%;">Int. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib19.11.3" style="font-size:90%;">, pages 2252‚Äì2261, 2019a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Kolotouros et¬†al. [2019b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Nikos Kolotouros, Georgios Pavlakos, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Convolutional Mesh Regression for Single-Image Human Shape Reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.10.2" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog.</em><span class="ltx_text" id="bib.bib20.11.3" style="font-size:90%;">, pages 4501‚Äì4510, 2019b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Kuo et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Grace Kuo, Nick Antipa, Ren Ng, and Laura Waller.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">DiffuserCam: Diffuser-Based Lensless Cameras.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.10.2" style="font-size:90%;">Computational Optical Sensing and Imaging</em><span class="ltx_text" id="bib.bib21.11.3" style="font-size:90%;">, pages CTu3B‚Äì2. Optica Publishing Group, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Lassner et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael¬†J. Black, and Peter¬†V. Gehler.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Unite the People: Closing the Loop Between 3D and 2D Human Representations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.10.2" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog.</em><span class="ltx_text" id="bib.bib22.11.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Leroy et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Vincent Leroy, Philippe Weinzaepfel, Romain Br√©gier, Hadrien Combaluzier, and Gr√©gory Rogez.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">SMPLy Benchmarking 3D Human Pose Estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.10.2" style="font-size:90%;">Int. Conf. 3D. Vis.</em><span class="ltx_text" id="bib.bib23.11.3" style="font-size:90%;">, pages 301‚Äì310. IEEE, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Li et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Yanjie Li, Sen Yang, Peidong Liu, Shoukui Zhang, Yunxiao Wang, Zhicheng Wang, Wankou Yang, and Shu-Tao Xia.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">SimCC: A Simple Coordinate Classification Perspective for Human Pose Estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.10.2" style="font-size:90%;">Eur. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib24.11.3" style="font-size:90%;">, pages 89‚Äì106. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Lin et¬†al. [2021a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Kevin Lin, Lijuan Wang, and Zicheng Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">End-to-end human pose and mesh reconstruction with transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib25.10.2" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog.</em><span class="ltx_text" id="bib.bib25.11.3" style="font-size:90%;">, pages 1954‚Äì1963, 2021a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Lin et¬†al. [2021b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Kevin Lin, Lijuan Wang, and Zicheng Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">Mesh Graphormer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib26.10.2" style="font-size:90%;">Int. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib26.11.3" style="font-size:90%;">, pages 12939‚Äì12948, 2021b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.4.4.1" style="font-size:90%;">Ozcan and McLeod [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.6.1" style="font-size:90%;">
Aydogan Ozcan and Euan McLeod.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">Lensless Imaging and Sensing.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.8.1" style="font-size:90%;">Annual review of biomedical engineering</em><span class="ltx_text" id="bib.bib27.9.2" style="font-size:90%;">, 18:77‚Äì102, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Pavlakos et¬†al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A.¬†A. Osman, Dimitrios Tzionas, and Michael¬†J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">Expressive Body Capture: 3D Hands, Face, and Body from a Single Image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.10.2" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog.</em><span class="ltx_text" id="bib.bib28.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Pavlakos et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">Human Mesh Recovery from Multiple Shots.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib29.10.2" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog.</em><span class="ltx_text" id="bib.bib29.11.3" style="font-size:90%;">, pages 1485‚Äì1495, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Rego et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Joshua¬†D Rego, Karthik Kulkarni, and Suren Jayasuriya.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">Robust Lensless Image Reconstruction Via PSF Estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib30.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em><span class="ltx_text" id="bib.bib30.11.3" style="font-size:90%;">, pages 403‚Äì412, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Rempe et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas¬†J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">HuMoR: 3D Human Motion Model for Robust Pose Estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib31.10.2" style="font-size:90%;">Int. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib31.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.4.4.1" style="font-size:90%;">Stork and Gill [2013]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.6.1" style="font-size:90%;">
David¬†G Stork and Patrick¬†R Gill.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">Lensless Ultra-miniature CMOS Computational Imagers and Sensors.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.8.1" style="font-size:90%;">Proc. Sensorcomm</em><span class="ltx_text" id="bib.bib32.9.2" style="font-size:90%;">, pages 186‚Äì190, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.4.4.1" style="font-size:90%;">Stork and Gill [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.6.1" style="font-size:90%;">
David¬†G Stork and Patrick¬†R Gill.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">Optical, mathematical, and computational foundations of lensless ultra-miniature diffractive imagers and sensors.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.8.1" style="font-size:90%;">International Journal on Advances in Systems and Measurements</em><span class="ltx_text" id="bib.bib33.9.2" style="font-size:90%;">, 7(3):4, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Sun et¬†al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Deep High-resolution Representation Learning for Human Pose Estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib34.10.2" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog.</em><span class="ltx_text" id="bib.bib34.11.3" style="font-size:90%;">, pages 5693‚Äì5703, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Tiwari et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Garvita Tiwari, Dimitrije Antic, Jan¬†Eric Lenssen, Nikolaos Sarafianos, Tony Tung, and Gerard Pons-Moll.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.2" style="font-size:90%;">Eur. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib35.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Zhang et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and Zhenan Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">Learning 3D Human Shape and Pose from Dense Body Parts.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.9.1" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</em><span class="ltx_text" id="bib.bib36.10.2" style="font-size:90%;">, 44(5):2610‚Äì2627, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Zhang et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang, and Zhenan Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib37.10.2" style="font-size:90%;">Int. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib37.11.3" style="font-size:90%;">, pages 11446‚Äì11456, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Zhang et¬†al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Jason¬†Y Zhang, Panna Felsen, Angjoo Kanazawa, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Predicting 3D Human Dynamics from Video.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib38.10.2" style="font-size:90%;">Int. Conf. Comput. Vis.</em><span class="ltx_text" id="bib.bib38.11.3" style="font-size:90%;">, pages 7114‚Äì7123, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Apr  8 12:48:51 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
