<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Pose Estimation from Camera Images for Underwater Inspection</title>
<!--Generated on Wed Jul 24 02:52:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
underwater,  localization,  neural networks,  novel view synthesis,  NeRF,  sensor fusion.
" lang="en" name="keywords"/>
<base href="/html/2407.16961v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S1" title="In Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2" title="In Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Pose Estimation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.SS1" title="In II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.SS1.SSS1" title="In II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>1 </span>Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.SS1.SSS2" title="In II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>2 </span>Loss Function</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.SS1.SSS3" title="In II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>3 </span>Implementation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.SS2" title="In II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Testing in Controlled Environment</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.SS3" title="In II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Results &amp; Discussions</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.SS3.SSS1" title="In II-C Results &amp; Discussions â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>1 </span>Model performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.SS3.SSS2" title="In II-C Results &amp; Discussions â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>2 </span>Generalization performance</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S3" title="In Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Augmented Training with Novel View Synthesis</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S3.SS1" title="In III Augmented Training with Novel View Synthesis â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S3.SS2" title="In III Augmented Training with Novel View Synthesis â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Results &amp; Discussion</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S4" title="In Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Localization enhancement via sensor data fusion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S4.SS1" title="In IV Localization enhancement via sensor data fusion â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S4.SS1.SSS1" title="In IV-A Methods â€£ IV Localization enhancement via sensor data fusion â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S4.SS1.SSS2" title="In IV-A Methods â€£ IV Localization enhancement via sensor data fusion â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>Measurement noises</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S4.SS2" title="In IV Localization enhancement via sensor data fusion â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Results &amp; Discussion</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S5" title="In Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Field trials at sea</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S5.SS1" title="In V Field trials at sea â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S5.SS2" title="In V Field trials at sea â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Results &amp; Discussion</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S6" title="In Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Pose Estimation from Camera Images for Underwater Inspection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">LuyuanÂ Peng,Â 
HariÂ Vishnu,Â 
MandarÂ Chitre,Â 
YuenÂ MinÂ Too,Â 
BharathÂ Kalyan,Â 
RajatÂ Mishra,Â 
andÂ SooÂ PiengÂ Tan
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">High-precision localization is pivotal in underwater reinspection missions. Traditional localization methods like inertial navigation systems, Doppler velocity loggers, and acoustic positioning face significant challenges and are not cost-effective for some applications. Visual localization is a cost-effective alternative in such cases, leveraging the cameras already equipped on inspection vehicles to estimate poses from images of the surrounding scene. Amongst these, machine learning-based pose estimation from images shows promise in underwater environments, performing efficient relocalization using models trained based on previously mapped scenes. We explore the efficacy of learning-based pose estimators in both clear and turbid water inspection missions, assessing the impact of image formats, model architectures and training data diversity. We innovate by employing novel view synthesis models to generate augmented training data, significantly enhancing pose estimation in unexplored regions. Moreover, we enhance localization accuracy by integrating pose estimator outputs with sensor data via an extended Kalman filter, demonstrating improved trajectory smoothness and accuracy.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
underwater, localization, neural networks, novel view synthesis, NeRF, sensor fusion.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">LOCALIZATION plays a crucial role in underwater reinspection missionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib1" title="">1</a>]</cite>. These are tasks carried out by underwater vehicles to examine the health and function of submerged structures like pipelines, offshore platforms, and ship hulls, required to ensure the safety and durability of infrastructure vital to industries like oil and gas, renewable energy, and maritime transportÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib3" title="">3</a>]</cite>. They stand apart from many underwater navigation tasks in their complexity and the precision required. Unlike general underwater navigation that involves moving from place to place, often prioritizing pathfinding and obstacle avoidance, reinspection missions demand detailed, close-range examination of often complicated underwater structuresÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib3" title="">3</a>]</cite>. As such, reinspection missions require the precise positioning and orientation of underwater vehicles to ensure thorough coverage, accurate data collection and the safety of the vehicles and the structures themselves.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In underwater environments, the use of global positioning systems is hindered due to the rapid dissipation of electromagnetic waves in waterÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib4" title="">4</a>]</cite>. Traditionally, underwater localization has relied on inertial navigation systems (INS), Doppler velocity loggers (DVL) and acoustic positioning systems. However, these methods face significant challenges in the context of inspection missions. Acoustic navigation is often compromised by shadowing effects and multipath interference near marine structures, which can severely distort signal paths and reduce accuracy. Consequently, achieving precise acoustic navigation requires complex and costly setupsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib5" title="">5</a>]</cite>. Furthermore, INS and DVL, despite their widespread use, suffer from an accumulation of errors over timeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib5" title="">5</a>]</cite>. This limits their ability to provide the positioning accuracy required for detailed inspection of underwater structures. Although high-grade INS and DVL may be able to provide sufficient accuracy, they, too, come with high costs.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In recent years, advancements in underwater localization have explored the use of optical sensors, such as camerasÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib6" title="">6</a>]</cite>. Some of these approaches necessitate the deployment of active markersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib7" title="">7</a>]</cite> or elaborate setups by diversÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib9" title="">9</a>]</cite>, adding complexity and expense. In contrast, visual localization methodsâ€”estimating camera poses from images of the surrounding sceneâ€”present a more cost-effective solution. Since inspection vehicles typically come equipped with cameras, visual-based localization can be implemented without the need for extra hardware. Moreover, visual-based localization methods, such as simultaneous localization and mapping (SLAM)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib5" title="">5</a>]</cite>, visual odometryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib10" title="">10</a>]</cite> and visual relocalizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib12" title="">12</a>]</cite>, have shown promise in navigating terrestrial and underwater environments.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Underwater reinspection missions typically involve the vehicle returning to the same sites for routine monitoring, assessment and/or maintenance. In this sense, these missions have another difference from normal underwater navigation tasks in that they have prior information of the scene or environment available, i.e., the environment is â€œknownâ€ to some degree after the first mission. We can use this available prior information to perform relocalization. This approach can be made effective if in the initial mapping run, we collect positioning information as accurately as possible using precise (and typically expensive and complex) positioning infrastructure such as ultra-short baseline acoustic positioning to characterize the environment. Using these data collected, visual relocalization methods can directly estimate poses from camera images in the following runs, significantly reducing the cost and complexity of reinspection. While SLAM and visual odometry are effective for general navigation, they do not utilize the additional prior information available in reinspection missions. In contrast, visual relocalization uses prior information and thus allows us to use more affordable vehicles and setups for localization in subsequent reinspection missions, significantly simplifying operations.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Visual relocalization techniques are categorized into feature-based methods such as Active SearchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib13" title="">13</a>]</cite>, and deep-learning methods like PoseNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib11" title="">11</a>]</cite>. Active search achieves image-based localization by systematically identifying and matching 2D features in query images with 3D points in a scene model. PoseNet is a deep learning model that utilizes a pretrained convolutional neural network (CNN) to estimate the 6-degree-of-freedom (6-DOF) poses of a camera directly from images. This approach simplifies the camera relocalization problem by bypassing the traditional feature extraction and matching steps, instead relying on the CNN to learn and estimate the cameraâ€™s position and orientation within a previously mapped environment directly from the image data.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">While Active Search achieves state-of-the-art results in outdoor terrestrial scenes, its effectiveness and robustness are reduced in environments with sparse features or where textures are obscured, conditions common in underwater settings due to limited visibilityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib11" title="">11</a>]</cite>. Additionally, Active Search is more computationally expensive compared to PoseNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib14" title="">14</a>]</cite>. Hence, we focus on neural-network-based methods inspired by PoseNet to estimate poses from underwater images. Previous research has demonstrated PoseNetâ€™s efficacy in conducting inspection tasks within tanks with toy structures and simulated underwater environmentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib12" title="">12</a>]</cite>. However, the performance of machine learning-based pose estimators with realistic structures and in at-sea environments has not been thoroughly investigated.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The performance of learning-based pose estimators depends heavily on the diversity of the training data. However, in underwater environments, collecting comprehensive training data is expensive. We propose to use Novel View Synthesis (NVS) models to render augmented training data. Recent advancement in NVS models, such as Neural Radiance Fields (NeRF)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib17" title="">17</a>]</cite> and 3D Gaussian Splatting (3DGS)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib18" title="">18</a>]</cite>, can synthesize photorealistic views of complex 3D scenes from a sparse set of input views by optimizing an underlying continuous volumetric scene function. When provided with a camera pose, NVS models utilize classical volumetric rendering techniques to project synthesized colors and densities into an imageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib17" title="">17</a>]</cite>. Using a trained NVS model, we can render images from any viewpoint within the boundary, allowing us to bypass the need for extensive physical data collection. We can then use these rendered images to augment our training data.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In this paper, our contributions are as follows:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We examine the performance of neural-network based pose estimators with different configurations in inspection missions in confined waters. We investigate the effects of different parameters, such as using RGB information versus grayscale, on the performance. We present the dataset collected, methods employed and results obtained in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2" title="II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection">II</a>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.2">We propose a new loss function, <math alttext="d" class="ltx_Math" display="inline" id="S1.I1.i2.p1.1.m1.1"><semantics id="S1.I1.i2.p1.1.m1.1a"><mi id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><ci id="S1.I1.i2.p1.1.m1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.p1.1.m1.1d">italic_d</annotation></semantics></math>-loss, incorporating the geometry of the inspection missions for training the pose estimators. The <math alttext="d" class="ltx_Math" display="inline" id="S1.I1.i2.p1.2.m2.1"><semantics id="S1.I1.i2.p1.2.m2.1a"><mi id="S1.I1.i2.p1.2.m2.1.1" xref="S1.I1.i2.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.2.m2.1b"><ci id="S1.I1.i2.p1.2.m2.1.1.cmml" xref="S1.I1.i2.p1.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.p1.2.m2.1d">italic_d</annotation></semantics></math>-loss provides interpretability, and improves computation efficiency and estimation performance. We present the method and results in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2" title="II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection">II</a>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We utilize underwater 3D NVS techniques to generate augmented training data. We demonstrate the performance improvement due to this in Section Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S3" title="III Augmented Training with Novel View Synthesis â€£ Pose Estimation from Camera Images for Underwater Inspection">III</a>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We enhance the localization performance by integrating our pose estimation model with data from additional sensors, such as altimeters and compasses. We use an extended Kalman filter (EKF) for tracking and fusion. In Section <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S4" title="IV Localization enhancement via sensor data fusion â€£ Pose Estimation from Camera Images for Underwater Inspection">IV</a>, we present these methods and results showing improved robustness and accuracy of this approach.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">We evaluate the performance of our proposed methods in at-sea environments. We present these results and discuss the overall performance of the entire pipeline in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S5" title="V Field trials at sea â€£ Pose Estimation from Camera Images for Underwater Inspection">V</a>.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S1.p8.2">Finally, we conclude this paper in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S6" title="VI Conclusion â€£ Pose Estimation from Camera Images for Underwater Inspection">VI</a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Pose Estimation</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Nielsen et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib15" title="">15</a>]</cite> evaluated the performance of PoseNet in a small tank, inspecting a subsea connector attached to a metal stick. In our previous work, we assessed the performance of various pretrained CNNs as pose estimators in a simulated underwater environment inspecting a subsea pipeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib16" title="">16</a>]</cite>. In this section, we evaluate the performance of visual localization using two neural-network model architectures inspired from PoseNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib11" title="">11</a>]</cite>.
The data for training and testing were collected from an artificial ocean basin at the Technology Center for Offshore and Marine, Singapore (TCOMS)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib19" title="">19</a>]</cite>. The originally presented PoseNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib11" title="">11</a>]</cite> works on RGB images. Here, we also evaluate the visual localization performance using grayscale images instead of RGB images to determine if similar accuracy can be achieved with higher efficiency, based on the intuition that underwater images typically have limited color information. Finally, we investigate the modelsâ€™ capability for (1) estimating pose on test images from the same dataset (i.e., capability to interpolate within same dataset), and (2) their capability to generalize to datasets outside that used for training, by using data from different runs for training and testing, which have different paths and conditions during acquisition.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Methods</span>
</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS1.5.1.1">II-A</span>1 </span>Architecture</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.7">The objective of PoseNet is to estimate a 6-DOF pose from a single monocular RGB image given as input to a neural network. The pose consists of the position (in 3D coordinates, <math alttext="x" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.1.m1.1"><semantics id="S2.SS1.SSS1.p1.1.m1.1a"><mi id="S2.SS1.SSS1.p1.1.m1.1.1" xref="S2.SS1.SSS1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.1.m1.1b"><ci id="S2.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.1.m1.1d">italic_x</annotation></semantics></math>-<math alttext="y" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.2.m2.1"><semantics id="S2.SS1.SSS1.p1.2.m2.1a"><mi id="S2.SS1.SSS1.p1.2.m2.1.1" xref="S2.SS1.SSS1.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.2.m2.1b"><ci id="S2.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.2.m2.1d">italic_y</annotation></semantics></math>-<math alttext="z" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.3.m3.1"><semantics id="S2.SS1.SSS1.p1.3.m3.1a"><mi id="S2.SS1.SSS1.p1.3.m3.1.1" xref="S2.SS1.SSS1.p1.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.3.m3.1b"><ci id="S2.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.3.m3.1c">z</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.3.m3.1d">italic_z</annotation></semantics></math>) and the orientation, which is represented in terms of a quaternion. Thus, the model outputs a 7-dimensional (7D) estimated pose vector <math alttext="\mathbf{y}=[\hat{\mathbf{p}},\hat{\mathbf{q}}]" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.4.m4.2"><semantics id="S2.SS1.SSS1.p1.4.m4.2a"><mrow id="S2.SS1.SSS1.p1.4.m4.2.3" xref="S2.SS1.SSS1.p1.4.m4.2.3.cmml"><mi id="S2.SS1.SSS1.p1.4.m4.2.3.2" xref="S2.SS1.SSS1.p1.4.m4.2.3.2.cmml">ğ²</mi><mo id="S2.SS1.SSS1.p1.4.m4.2.3.1" xref="S2.SS1.SSS1.p1.4.m4.2.3.1.cmml">=</mo><mrow id="S2.SS1.SSS1.p1.4.m4.2.3.3.2" xref="S2.SS1.SSS1.p1.4.m4.2.3.3.1.cmml"><mo id="S2.SS1.SSS1.p1.4.m4.2.3.3.2.1" stretchy="false" xref="S2.SS1.SSS1.p1.4.m4.2.3.3.1.cmml">[</mo><mover accent="true" id="S2.SS1.SSS1.p1.4.m4.1.1" xref="S2.SS1.SSS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.SSS1.p1.4.m4.1.1.2" xref="S2.SS1.SSS1.p1.4.m4.1.1.2.cmml">ğ©</mi><mo id="S2.SS1.SSS1.p1.4.m4.1.1.1" xref="S2.SS1.SSS1.p1.4.m4.1.1.1.cmml">^</mo></mover><mo id="S2.SS1.SSS1.p1.4.m4.2.3.3.2.2" xref="S2.SS1.SSS1.p1.4.m4.2.3.3.1.cmml">,</mo><mover accent="true" id="S2.SS1.SSS1.p1.4.m4.2.2" xref="S2.SS1.SSS1.p1.4.m4.2.2.cmml"><mi id="S2.SS1.SSS1.p1.4.m4.2.2.2" xref="S2.SS1.SSS1.p1.4.m4.2.2.2.cmml">ğª</mi><mo id="S2.SS1.SSS1.p1.4.m4.2.2.1" xref="S2.SS1.SSS1.p1.4.m4.2.2.1.cmml">^</mo></mover><mo id="S2.SS1.SSS1.p1.4.m4.2.3.3.2.3" stretchy="false" xref="S2.SS1.SSS1.p1.4.m4.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.4.m4.2b"><apply id="S2.SS1.SSS1.p1.4.m4.2.3.cmml" xref="S2.SS1.SSS1.p1.4.m4.2.3"><eq id="S2.SS1.SSS1.p1.4.m4.2.3.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.2.3.1"></eq><ci id="S2.SS1.SSS1.p1.4.m4.2.3.2.cmml" xref="S2.SS1.SSS1.p1.4.m4.2.3.2">ğ²</ci><interval closure="closed" id="S2.SS1.SSS1.p1.4.m4.2.3.3.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.2.3.3.2"><apply id="S2.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1"><ci id="S2.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.1">^</ci><ci id="S2.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.2">ğ©</ci></apply><apply id="S2.SS1.SSS1.p1.4.m4.2.2.cmml" xref="S2.SS1.SSS1.p1.4.m4.2.2"><ci id="S2.SS1.SSS1.p1.4.m4.2.2.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.2.2.1">^</ci><ci id="S2.SS1.SSS1.p1.4.m4.2.2.2.cmml" xref="S2.SS1.SSS1.p1.4.m4.2.2.2">ğª</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.4.m4.2c">\mathbf{y}=[\hat{\mathbf{p}},\hat{\mathbf{q}}]</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.4.m4.2d">bold_y = [ over^ start_ARG bold_p end_ARG , over^ start_ARG bold_q end_ARG ]</annotation></semantics></math> containing a position vector estimate <math alttext="\hat{\mathbf{p}}" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.5.m5.1"><semantics id="S2.SS1.SSS1.p1.5.m5.1a"><mover accent="true" id="S2.SS1.SSS1.p1.5.m5.1.1" xref="S2.SS1.SSS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.SSS1.p1.5.m5.1.1.2" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.cmml">ğ©</mi><mo id="S2.SS1.SSS1.p1.5.m5.1.1.1" xref="S2.SS1.SSS1.p1.5.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.5.m5.1b"><apply id="S2.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1"><ci id="S2.SS1.SSS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.1">^</ci><ci id="S2.SS1.SSS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.2">ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.5.m5.1c">\hat{\mathbf{p}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.5.m5.1d">over^ start_ARG bold_p end_ARG</annotation></semantics></math> and an orientation vector estimate <math alttext="\hat{\mathbf{q}}" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.6.m6.1"><semantics id="S2.SS1.SSS1.p1.6.m6.1a"><mover accent="true" id="S2.SS1.SSS1.p1.6.m6.1.1" xref="S2.SS1.SSS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.SSS1.p1.6.m6.1.1.2" xref="S2.SS1.SSS1.p1.6.m6.1.1.2.cmml">ğª</mi><mo id="S2.SS1.SSS1.p1.6.m6.1.1.1" xref="S2.SS1.SSS1.p1.6.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.6.m6.1b"><apply id="S2.SS1.SSS1.p1.6.m6.1.1.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1"><ci id="S2.SS1.SSS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1.1">^</ci><ci id="S2.SS1.SSS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1.2">ğª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.6.m6.1c">\hat{\mathbf{q}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.6.m6.1d">over^ start_ARG bold_q end_ARG</annotation></semantics></math>, where <math alttext="\hat{}" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.7.m7.1"><semantics id="S2.SS1.SSS1.p1.7.m7.1a"><mover accent="true" id="S2.SS1.SSS1.p1.7.m7.1.1" xref="S2.SS1.SSS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.SSS1.p1.7.m7.1.1.2" xref="S2.SS1.SSS1.p1.7.m7.1.1.2.cmml"></mi><mo id="S2.SS1.SSS1.p1.7.m7.1.1.1" xref="S2.SS1.SSS1.p1.7.m7.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.7.m7.1b"><apply id="S2.SS1.SSS1.p1.7.m7.1.1.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1"><ci id="S2.SS1.SSS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1.1">^</ci><csymbol cd="latexml" id="S2.SS1.SSS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1.2">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.7.m7.1c">\hat{}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.7.m7.1d">over^ start_ARG end_ARG</annotation></semantics></math> represents an estimate.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">The PoseNet model originally presented by Kendall et alÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib11" title="">11</a>]</cite> was a CNN, a modified version of the GoogLeNet architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib20" title="">20</a>]</cite> pretrained on the ImageNet datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib21" title="">21</a>]</cite>, with the softmax classifiers changed to affine regressors, and another fully connected (FC) layer of feature size 2048 inserted before the final regressor. However, regressing a 7D pose vector from a high dimensional output of the FC layer is not optimalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib22" title="">22</a>]</cite>. A later work Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib22" title="">22</a>]</cite> aimed to tackle this by modifying PoseNet by reshaping the FC layer of size 2048 to a 32 <math alttext="\times" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p2.1.m1.1"><semantics id="S2.SS1.SSS1.p2.1.m1.1a"><mo id="S2.SS1.SSS1.p2.1.m1.1.1" xref="S2.SS1.SSS1.p2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.1.m1.1b"><times id="S2.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p2.1.m1.1d">Ã—</annotation></semantics></math> 64 matrix and applying four long-short-term-memory networks (LSTMs) to perform structured dimensionality reduction. This algorithm, which we refer to as CNN+LSTM, showed a performance improvement compared to PoseNet in terrestrial environmentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib22" title="">22</a>]</cite>, and also in an underwater tank environmentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib12" title="">12</a>]</cite>. We implement and evaluate both model architectures â€“ the CNN (shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.F1" title="Figure 1 â€£ II-A1 Architecture â€£ II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">1</span></a>) and the CNN+LSTM (shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.F2" title="Figure 2 â€£ II-A1 Architecture â€£ II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">2</span></a>). Additionally, we assess the performance of these using a pretrained ResNet50Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib23" title="">23</a>]</cite> as the backbone.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<p class="ltx_p ltx_align_center" id="S2.F1.1"><span class="ltx_text" id="S2.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="174" id="S2.F1.1.1.g1" src="extracted/5751340/Data/figures/arch_net.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.4.2" style="font-size:90%;">Overview of the CNN-based architecture for visual localization.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F2">
<p class="ltx_p ltx_align_center" id="S2.F2.1"><span class="ltx_text" id="S2.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="177" id="S2.F2.1.1.g1" src="extracted/5751340/Data/figures/arch_lstm.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.4.2" style="font-size:90%;">Overview of the CNN+LSTM-based architecture for visual localization</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS2.5.1.1">II-A</span>2 </span>Loss Function</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.2">Kendall et alÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib11" title="">11</a>]</cite> used a composite loss function that is a weighted sum of the (1) L2 loss <math alttext="\mathcal{L}_{\mathbf{p}}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p1.1.m1.1"><semantics id="S2.SS1.SSS2.p1.1.m1.1a"><msub id="S2.SS1.SSS2.p1.1.m1.1.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p1.1.m1.1.1.2" xref="S2.SS1.SSS2.p1.1.m1.1.1.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p1.1.m1.1.1.3" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.cmml">ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.1.m1.1b"><apply id="S2.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.2">â„’</ci><ci id="S2.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3">ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.1.m1.1c">\mathcal{L}_{\mathbf{p}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT</annotation></semantics></math> between the predicted positions and the true positions, and the (2) L2 loss <math alttext="\mathcal{L}_{\mathbf{q}}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p1.2.m2.1"><semantics id="S2.SS1.SSS2.p1.2.m2.1a"><msub id="S2.SS1.SSS2.p1.2.m2.1.1" xref="S2.SS1.SSS2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p1.2.m2.1.1.2" xref="S2.SS1.SSS2.p1.2.m2.1.1.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.cmml">ğª</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.2.m2.1b"><apply id="S2.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.2">â„’</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3">ğª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.2.m2.1c">\mathcal{L}_{\mathbf{q}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p1.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT</annotation></semantics></math> between the predicted quaternions and the true quaternions:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\mathcal{L}_{\mathbf{p}}+\beta\mathcal{L}_{\mathbf{q}}," class="ltx_Math" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml">â„’</mi><mo id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml"><msub id="S2.E1.m1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.1.1.3.2.2" xref="S2.E1.m1.1.1.1.1.3.2.2.cmml">â„’</mi><mi id="S2.E1.m1.1.1.1.1.3.2.3" xref="S2.E1.m1.1.1.1.1.3.2.3.cmml">ğ©</mi></msub><mo id="S2.E1.m1.1.1.1.1.3.1" xref="S2.E1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.1.1.3.3.2" xref="S2.E1.m1.1.1.1.1.3.3.2.cmml">Î²</mi><mo id="S2.E1.m1.1.1.1.1.3.3.1" xref="S2.E1.m1.1.1.1.1.3.3.1.cmml">â¢</mo><msub id="S2.E1.m1.1.1.1.1.3.3.3" xref="S2.E1.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.1.1.3.3.3.2" xref="S2.E1.m1.1.1.1.1.3.3.3.2.cmml">â„’</mi><mi id="S2.E1.m1.1.1.1.1.3.3.3.3" xref="S2.E1.m1.1.1.1.1.3.3.3.3.cmml">ğª</mi></msub></mrow></mrow></mrow><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"></eq><ci id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2">â„’</ci><apply id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"><plus id="S2.E1.m1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3.1"></plus><apply id="S2.E1.m1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2.2">â„’</ci><ci id="S2.E1.m1.1.1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3">ğ©</ci></apply><apply id="S2.E1.m1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3"><times id="S2.E1.m1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3.3.1"></times><ci id="S2.E1.m1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.3.2">ğ›½</ci><apply id="S2.E1.m1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.3.3.2">â„’</ci><ci id="S2.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3.3.3">ğª</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\mathcal{L}=\mathcal{L}_{\mathbf{p}}+\beta\mathcal{L}_{\mathbf{q}},</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">caligraphic_L = caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT + italic_Î² caligraphic_L start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.SSS2.p1.9">where <math alttext="\mathcal{L}_{\mathbf{p}}=||\mathbf{p}-\hat{\mathbf{p}}||_{2}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p1.3.m1.1"><semantics id="S2.SS1.SSS2.p1.3.m1.1a"><mrow id="S2.SS1.SSS2.p1.3.m1.1.1" xref="S2.SS1.SSS2.p1.3.m1.1.1.cmml"><msub id="S2.SS1.SSS2.p1.3.m1.1.1.3" xref="S2.SS1.SSS2.p1.3.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p1.3.m1.1.1.3.2" xref="S2.SS1.SSS2.p1.3.m1.1.1.3.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p1.3.m1.1.1.3.3" xref="S2.SS1.SSS2.p1.3.m1.1.1.3.3.cmml">ğ©</mi></msub><mo id="S2.SS1.SSS2.p1.3.m1.1.1.2" xref="S2.SS1.SSS2.p1.3.m1.1.1.2.cmml">=</mo><msub id="S2.SS1.SSS2.p1.3.m1.1.1.1" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.cmml"><mrow id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.2.cmml"><mo id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.2" stretchy="false" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.2" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.2.cmml">ğ©</mi><mo id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.1" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mover accent="true" id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3.2" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3.2.cmml">ğ©</mi><mo id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3.1" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.3" stretchy="false" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S2.SS1.SSS2.p1.3.m1.1.1.1.3" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.3.m1.1b"><apply id="S2.SS1.SSS2.p1.3.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1"><eq id="S2.SS1.SSS2.p1.3.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.2"></eq><apply id="S2.SS1.SSS2.p1.3.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.3.m1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.3">subscript</csymbol><ci id="S2.SS1.SSS2.p1.3.m1.1.1.3.2.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.3.2">â„’</ci><ci id="S2.SS1.SSS2.p1.3.m1.1.1.3.3.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.3.3">ğ©</ci></apply><apply id="S2.SS1.SSS2.p1.3.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.3.m1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.1">subscript</csymbol><apply id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.2">norm</csymbol><apply id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1"><minus id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.1"></minus><ci id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.2">ğ©</ci><apply id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3"><ci id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3.1">^</ci><ci id="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.1.1.1.3.2">ğ©</ci></apply></apply></apply><cn id="S2.SS1.SSS2.p1.3.m1.1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS2.p1.3.m1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.3.m1.1c">\mathcal{L}_{\mathbf{p}}=||\mathbf{p}-\hat{\mathbf{p}}||_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p1.3.m1.1d">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT = | | bold_p - over^ start_ARG bold_p end_ARG | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{\mathbf{q}}=||\mathbf{q}-\hat{\mathbf{q}}/\|\hat{\mathbf{q}}\|||_%
{2}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p1.4.m2.2"><semantics id="S2.SS1.SSS2.p1.4.m2.2a"><mrow id="S2.SS1.SSS2.p1.4.m2.2.2" xref="S2.SS1.SSS2.p1.4.m2.2.2.cmml"><msub id="S2.SS1.SSS2.p1.4.m2.2.2.3" xref="S2.SS1.SSS2.p1.4.m2.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p1.4.m2.2.2.3.2" xref="S2.SS1.SSS2.p1.4.m2.2.2.3.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p1.4.m2.2.2.3.3" xref="S2.SS1.SSS2.p1.4.m2.2.2.3.3.cmml">ğª</mi></msub><mo id="S2.SS1.SSS2.p1.4.m2.2.2.2" xref="S2.SS1.SSS2.p1.4.m2.2.2.2.cmml">=</mo><msub id="S2.SS1.SSS2.p1.4.m2.2.2.1" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.cmml"><mrow id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.2.cmml"><mo id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.2" stretchy="false" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.2.1.cmml">â€–</mo><mrow id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.cmml"><mi id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.2" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.2.cmml">ğª</mi><mo id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.1" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.cmml"><mover accent="true" id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2.cmml"><mi id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2.2" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2.2.cmml">ğª</mi><mo id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2.1" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2.1.cmml">^</mo></mover><mo id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.1" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.1.cmml">/</mo><mrow id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.3.2" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.3.1.cmml"><mo id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.3.2.1" stretchy="false" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.3.1.1.cmml">â€–</mo><mover accent="true" id="S2.SS1.SSS2.p1.4.m2.1.1" xref="S2.SS1.SSS2.p1.4.m2.1.1.cmml"><mi id="S2.SS1.SSS2.p1.4.m2.1.1.2" xref="S2.SS1.SSS2.p1.4.m2.1.1.2.cmml">ğª</mi><mo id="S2.SS1.SSS2.p1.4.m2.1.1.1" xref="S2.SS1.SSS2.p1.4.m2.1.1.1.cmml">^</mo></mover><mo id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.3.2.2" stretchy="false" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.3.1.1.cmml">â€–</mo></mrow></mrow></mrow><mo id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.3" stretchy="false" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.2.1.cmml">â€–</mo></mrow><mn id="S2.SS1.SSS2.p1.4.m2.2.2.1.3" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.4.m2.2b"><apply id="S2.SS1.SSS2.p1.4.m2.2.2.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2"><eq id="S2.SS1.SSS2.p1.4.m2.2.2.2.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.2"></eq><apply id="S2.SS1.SSS2.p1.4.m2.2.2.3.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.4.m2.2.2.3.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.3">subscript</csymbol><ci id="S2.SS1.SSS2.p1.4.m2.2.2.3.2.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.3.2">â„’</ci><ci id="S2.SS1.SSS2.p1.4.m2.2.2.3.3.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.3.3">ğª</ci></apply><apply id="S2.SS1.SSS2.p1.4.m2.2.2.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.4.m2.2.2.1.2.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1">subscript</csymbol><apply id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.2.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.2.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.2">norm</csymbol><apply id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1"><minus id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.1"></minus><ci id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.2">ğª</ci><apply id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3"><divide id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.1"></divide><apply id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2"><ci id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2.1">^</ci><ci id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2.2.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.2.2">ğª</ci></apply><apply id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.3.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.3.2"><csymbol cd="latexml" id="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.3.1.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.1.1.1.3.3.2.1">norm</csymbol><apply id="S2.SS1.SSS2.p1.4.m2.1.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.1.1"><ci id="S2.SS1.SSS2.p1.4.m2.1.1.1.cmml" xref="S2.SS1.SSS2.p1.4.m2.1.1.1">^</ci><ci id="S2.SS1.SSS2.p1.4.m2.1.1.2.cmml" xref="S2.SS1.SSS2.p1.4.m2.1.1.2">ğª</ci></apply></apply></apply></apply></apply><cn id="S2.SS1.SSS2.p1.4.m2.2.2.1.3.cmml" type="integer" xref="S2.SS1.SSS2.p1.4.m2.2.2.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.4.m2.2c">\mathcal{L}_{\mathbf{q}}=||\mathbf{q}-\hat{\mathbf{q}}/\|\hat{\mathbf{q}}\|||_%
{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p1.4.m2.2d">caligraphic_L start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT = | | bold_q - over^ start_ARG bold_q end_ARG / âˆ¥ over^ start_ARG bold_q end_ARG âˆ¥ | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\mathbf{p}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p1.5.m3.1"><semantics id="S2.SS1.SSS2.p1.5.m3.1a"><mi id="S2.SS1.SSS2.p1.5.m3.1.1" xref="S2.SS1.SSS2.p1.5.m3.1.1.cmml">ğ©</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.5.m3.1b"><ci id="S2.SS1.SSS2.p1.5.m3.1.1.cmml" xref="S2.SS1.SSS2.p1.5.m3.1.1">ğ©</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.5.m3.1c">\mathbf{p}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p1.5.m3.1d">bold_p</annotation></semantics></math> and <math alttext="\mathbf{q}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p1.6.m4.1"><semantics id="S2.SS1.SSS2.p1.6.m4.1a"><mi id="S2.SS1.SSS2.p1.6.m4.1.1" xref="S2.SS1.SSS2.p1.6.m4.1.1.cmml">ğª</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.6.m4.1b"><ci id="S2.SS1.SSS2.p1.6.m4.1.1.cmml" xref="S2.SS1.SSS2.p1.6.m4.1.1">ğª</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.6.m4.1c">\mathbf{q}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p1.6.m4.1d">bold_q</annotation></semantics></math> represent the true pose. <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p1.7.m5.1"><semantics id="S2.SS1.SSS2.p1.7.m5.1a"><mi id="S2.SS1.SSS2.p1.7.m5.1.1" xref="S2.SS1.SSS2.p1.7.m5.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.7.m5.1b"><ci id="S2.SS1.SSS2.p1.7.m5.1.1.cmml" xref="S2.SS1.SSS2.p1.7.m5.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.7.m5.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p1.7.m5.1d">italic_Î²</annotation></semantics></math> is a free parameter that determines the trade-off between the desired accuracy in translation and orientation. In PoseNet and CNN+LSTM, the value of <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p1.8.m6.1"><semantics id="S2.SS1.SSS2.p1.8.m6.1a"><mi id="S2.SS1.SSS2.p1.8.m6.1.1" xref="S2.SS1.SSS2.p1.8.m6.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.8.m6.1b"><ci id="S2.SS1.SSS2.p1.8.m6.1.1.cmml" xref="S2.SS1.SSS2.p1.8.m6.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.8.m6.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p1.8.m6.1d">italic_Î²</annotation></semantics></math> is fine-tuned using a grid search to ensure the expected value of position and orientation errors are approximately equal, which the authors suggest lead to overall optimal performance. We refer to this loss function as the <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p1.9.m7.1"><semantics id="S2.SS1.SSS2.p1.9.m7.1a"><mi id="S2.SS1.SSS2.p1.9.m7.1.1" xref="S2.SS1.SSS2.p1.9.m7.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.9.m7.1b"><ci id="S2.SS1.SSS2.p1.9.m7.1.1.cmml" xref="S2.SS1.SSS2.p1.9.m7.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.9.m7.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p1.9.m7.1d">italic_Î²</annotation></semantics></math>-loss.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.2">We argue that the <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.1.m1.1"><semantics id="S2.SS1.SSS2.p2.1.m1.1a"><mi id="S2.SS1.SSS2.p2.1.m1.1.1" xref="S2.SS1.SSS2.p2.1.m1.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.1.m1.1b"><ci id="S2.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.1.m1.1d">italic_Î²</annotation></semantics></math>-loss is not the optimal approach to our problem, due to three reasons. Firstly, we argue that optimal performance is not necessarily achieved when position and orientation errors are roughly equal. Instead, the performance criteria and loss should incorporate geometry and physics relevant to the inspection task at hand. Secondly, the L2 loss between the predicted and true quaternions does not directly translate to an orientation error interpretable in degrees or radians, and thus, it does not accurately reflect the geometric distance between the predicted and true orientations. Thirdly, searching for the optimal <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.2.m2.1"><semantics id="S2.SS1.SSS2.p2.2.m2.1a"><mi id="S2.SS1.SSS2.p2.2.m2.1.1" xref="S2.SS1.SSS2.p2.2.m2.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.2.m2.1b"><ci id="S2.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.2.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.2.m2.1d">italic_Î²</annotation></semantics></math> value often involves extensive computational resources. This search can become a significant bottleneck, especially in scenarios where training needs to be done fast.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p3">
<p class="ltx_p" id="S2.SS1.SSS2.p3.2">To overcome these shortcomings, we propose a new loss function more relevant to our problem, the <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.1.m1.1"><semantics id="S2.SS1.SSS2.p3.1.m1.1a"><mi id="S2.SS1.SSS2.p3.1.m1.1.1" xref="S2.SS1.SSS2.p3.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.1.m1.1b"><ci id="S2.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p3.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.1.m1.1d">italic_d</annotation></semantics></math>-loss, to improve the training effectiveness, interpretability and efficiency. The <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.2.m2.1"><semantics id="S2.SS1.SSS2.p3.2.m2.1a"><mi id="S2.SS1.SSS2.p3.2.m2.1.1" xref="S2.SS1.SSS2.p3.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.2.m2.1b"><ci id="S2.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p3.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.2.m2.1d">italic_d</annotation></semantics></math>-loss is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\mathcal{L}_{\mathbf{p}}+d\mathcal{L}_{\theta}." class="ltx_Math" display="block" id="S2.E2.m1.1"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.cmml">â„’</mi><mo id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.3.cmml"><msub id="S2.E2.m1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.1.1.1.1.3.2.2" xref="S2.E2.m1.1.1.1.1.3.2.2.cmml">â„’</mi><mi id="S2.E2.m1.1.1.1.1.3.2.3" xref="S2.E2.m1.1.1.1.1.3.2.3.cmml">ğ©</mi></msub><mo id="S2.E2.m1.1.1.1.1.3.1" xref="S2.E2.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S2.E2.m1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.3.3.cmml"><mi id="S2.E2.m1.1.1.1.1.3.3.2" xref="S2.E2.m1.1.1.1.1.3.3.2.cmml">d</mi><mo id="S2.E2.m1.1.1.1.1.3.3.1" xref="S2.E2.m1.1.1.1.1.3.3.1.cmml">â¢</mo><msub id="S2.E2.m1.1.1.1.1.3.3.3" xref="S2.E2.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.1.1.1.1.3.3.3.2" xref="S2.E2.m1.1.1.1.1.3.3.3.2.cmml">â„’</mi><mi id="S2.E2.m1.1.1.1.1.3.3.3.3" xref="S2.E2.m1.1.1.1.1.3.3.3.3.cmml">Î¸</mi></msub></mrow></mrow></mrow><mo id="S2.E2.m1.1.1.1.2" lspace="0em" xref="S2.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><eq id="S2.E2.m1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"></eq><ci id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.2">â„’</ci><apply id="S2.E2.m1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.3"><plus id="S2.E2.m1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.1"></plus><apply id="S2.E2.m1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.2.1.cmml" xref="S2.E2.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.2.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2.2">â„’</ci><ci id="S2.E2.m1.1.1.1.1.3.2.3.cmml" xref="S2.E2.m1.1.1.1.1.3.2.3">ğ©</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3"><times id="S2.E2.m1.1.1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.3.1"></times><ci id="S2.E2.m1.1.1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.3.2">ğ‘‘</ci><apply id="S2.E2.m1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.3.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.3.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.3.3.2">â„’</ci><ci id="S2.E2.m1.1.1.1.1.3.3.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3.3.3">ğœƒ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\mathcal{L}=\mathcal{L}_{\mathbf{p}}+d\mathcal{L}_{\theta}.</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.1d">caligraphic_L = caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT + italic_d caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.SSS2.p3.14">Note that we have replaced the quaternion loss in (<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.E1" title="In II-A2 Loss Function â€£ II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">1</span></a>) with a loss based on the Eulerian angular difference, <math alttext="\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.3.m1.1"><semantics id="S2.SS1.SSS2.p3.3.m1.1a"><msub id="S2.SS1.SSS2.p3.3.m1.1.1" xref="S2.SS1.SSS2.p3.3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p3.3.m1.1.1.2" xref="S2.SS1.SSS2.p3.3.m1.1.1.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p3.3.m1.1.1.3" xref="S2.SS1.SSS2.p3.3.m1.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.3.m1.1b"><apply id="S2.SS1.SSS2.p3.3.m1.1.1.cmml" xref="S2.SS1.SSS2.p3.3.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p3.3.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p3.3.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p3.3.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p3.3.m1.1.1.2">â„’</ci><ci id="S2.SS1.SSS2.p3.3.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p3.3.m1.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.3.m1.1c">\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.3.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math>, which is calculated as follows. We first determine the rotation between the estimated and ground truth quaternions through quaternion multiplication, <math alttext="\mathbf{\Delta q}=\mathbf{q}\left(\hat{\mathbf{q}}/\|\hat{\mathbf{q}}\|\right)%
^{*}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.4.m2.2"><semantics id="S2.SS1.SSS2.p3.4.m2.2a"><mrow id="S2.SS1.SSS2.p3.4.m2.2.2" xref="S2.SS1.SSS2.p3.4.m2.2.2.cmml"><mrow id="S2.SS1.SSS2.p3.4.m2.2.2.3" xref="S2.SS1.SSS2.p3.4.m2.2.2.3.cmml"><mi id="S2.SS1.SSS2.p3.4.m2.2.2.3.2" xref="S2.SS1.SSS2.p3.4.m2.2.2.3.2.cmml">ğš«</mi><mo id="S2.SS1.SSS2.p3.4.m2.2.2.3.1" xref="S2.SS1.SSS2.p3.4.m2.2.2.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p3.4.m2.2.2.3.3" xref="S2.SS1.SSS2.p3.4.m2.2.2.3.3.cmml">ğª</mi></mrow><mo id="S2.SS1.SSS2.p3.4.m2.2.2.2" xref="S2.SS1.SSS2.p3.4.m2.2.2.2.cmml">=</mo><mrow id="S2.SS1.SSS2.p3.4.m2.2.2.1" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.cmml"><mi id="S2.SS1.SSS2.p3.4.m2.2.2.1.3" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.3.cmml">ğª</mi><mo id="S2.SS1.SSS2.p3.4.m2.2.2.1.2" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.2.cmml">â¢</mo><msup id="S2.SS1.SSS2.p3.4.m2.2.2.1.1" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.cmml"><mrow id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.cmml"><mo id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.2" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.cmml"><mover accent="true" id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2.cmml"><mi id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2.2" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2.2.cmml">ğª</mi><mo id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2.1" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.1" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.1.cmml">/</mo><mrow id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.3.2" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.3.1.cmml"><mo id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.3.2.1" stretchy="false" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.3.1.1.cmml">â€–</mo><mover accent="true" id="S2.SS1.SSS2.p3.4.m2.1.1" xref="S2.SS1.SSS2.p3.4.m2.1.1.cmml"><mi id="S2.SS1.SSS2.p3.4.m2.1.1.2" xref="S2.SS1.SSS2.p3.4.m2.1.1.2.cmml">ğª</mi><mo id="S2.SS1.SSS2.p3.4.m2.1.1.1" xref="S2.SS1.SSS2.p3.4.m2.1.1.1.cmml">^</mo></mover><mo id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.3.2.2" stretchy="false" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.3.1.1.cmml">â€–</mo></mrow></mrow><mo id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.3" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.3" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.3.cmml">âˆ—</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.4.m2.2b"><apply id="S2.SS1.SSS2.p3.4.m2.2.2.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2"><eq id="S2.SS1.SSS2.p3.4.m2.2.2.2.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.2"></eq><apply id="S2.SS1.SSS2.p3.4.m2.2.2.3.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.3"><times id="S2.SS1.SSS2.p3.4.m2.2.2.3.1.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.3.1"></times><ci id="S2.SS1.SSS2.p3.4.m2.2.2.3.2.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.3.2">ğš«</ci><ci id="S2.SS1.SSS2.p3.4.m2.2.2.3.3.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.3.3">ğª</ci></apply><apply id="S2.SS1.SSS2.p3.4.m2.2.2.1.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1"><times id="S2.SS1.SSS2.p3.4.m2.2.2.1.2.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.2"></times><ci id="S2.SS1.SSS2.p3.4.m2.2.2.1.3.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.3">ğª</ci><apply id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.2.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1">superscript</csymbol><apply id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1"><divide id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.1"></divide><apply id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2"><ci id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2.1">^</ci><ci id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2.2.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.2.2">ğª</ci></apply><apply id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.3.2"><csymbol cd="latexml" id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.3.1.1.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.1.1.1.3.2.1">norm</csymbol><apply id="S2.SS1.SSS2.p3.4.m2.1.1.cmml" xref="S2.SS1.SSS2.p3.4.m2.1.1"><ci id="S2.SS1.SSS2.p3.4.m2.1.1.1.cmml" xref="S2.SS1.SSS2.p3.4.m2.1.1.1">^</ci><ci id="S2.SS1.SSS2.p3.4.m2.1.1.2.cmml" xref="S2.SS1.SSS2.p3.4.m2.1.1.2">ğª</ci></apply></apply></apply><times id="S2.SS1.SSS2.p3.4.m2.2.2.1.1.3.cmml" xref="S2.SS1.SSS2.p3.4.m2.2.2.1.1.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.4.m2.2c">\mathbf{\Delta q}=\mathbf{q}\left(\hat{\mathbf{q}}/\|\hat{\mathbf{q}}\|\right)%
^{*}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.4.m2.2d">bold_Î” bold_q = bold_q ( over^ start_ARG bold_q end_ARG / âˆ¥ over^ start_ARG bold_q end_ARG âˆ¥ ) start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math>, where <sup class="ltx_sup" id="S2.SS1.SSS2.p3.14.1"><span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p3.14.1.1">âˆ—</span></sup> denotes the conjugate of the quaternion. <math alttext="\mathbf{\Delta q}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.6.m4.1"><semantics id="S2.SS1.SSS2.p3.6.m4.1a"><mrow id="S2.SS1.SSS2.p3.6.m4.1.1" xref="S2.SS1.SSS2.p3.6.m4.1.1.cmml"><mi id="S2.SS1.SSS2.p3.6.m4.1.1.2" xref="S2.SS1.SSS2.p3.6.m4.1.1.2.cmml">ğš«</mi><mo id="S2.SS1.SSS2.p3.6.m4.1.1.1" xref="S2.SS1.SSS2.p3.6.m4.1.1.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p3.6.m4.1.1.3" xref="S2.SS1.SSS2.p3.6.m4.1.1.3.cmml">ğª</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.6.m4.1b"><apply id="S2.SS1.SSS2.p3.6.m4.1.1.cmml" xref="S2.SS1.SSS2.p3.6.m4.1.1"><times id="S2.SS1.SSS2.p3.6.m4.1.1.1.cmml" xref="S2.SS1.SSS2.p3.6.m4.1.1.1"></times><ci id="S2.SS1.SSS2.p3.6.m4.1.1.2.cmml" xref="S2.SS1.SSS2.p3.6.m4.1.1.2">ğš«</ci><ci id="S2.SS1.SSS2.p3.6.m4.1.1.3.cmml" xref="S2.SS1.SSS2.p3.6.m4.1.1.3">ğª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.6.m4.1c">\mathbf{\Delta q}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.6.m4.1d">bold_Î” bold_q</annotation></semantics></math> is a unit quaternion which can be expressed as <math alttext="(r,\vec{v})" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.7.m5.2"><semantics id="S2.SS1.SSS2.p3.7.m5.2a"><mrow id="S2.SS1.SSS2.p3.7.m5.2.3.2" xref="S2.SS1.SSS2.p3.7.m5.2.3.1.cmml"><mo id="S2.SS1.SSS2.p3.7.m5.2.3.2.1" stretchy="false" xref="S2.SS1.SSS2.p3.7.m5.2.3.1.cmml">(</mo><mi id="S2.SS1.SSS2.p3.7.m5.1.1" xref="S2.SS1.SSS2.p3.7.m5.1.1.cmml">r</mi><mo id="S2.SS1.SSS2.p3.7.m5.2.3.2.2" xref="S2.SS1.SSS2.p3.7.m5.2.3.1.cmml">,</mo><mover accent="true" id="S2.SS1.SSS2.p3.7.m5.2.2" xref="S2.SS1.SSS2.p3.7.m5.2.2.cmml"><mi id="S2.SS1.SSS2.p3.7.m5.2.2.2" xref="S2.SS1.SSS2.p3.7.m5.2.2.2.cmml">v</mi><mo id="S2.SS1.SSS2.p3.7.m5.2.2.1" stretchy="false" xref="S2.SS1.SSS2.p3.7.m5.2.2.1.cmml">â†’</mo></mover><mo id="S2.SS1.SSS2.p3.7.m5.2.3.2.3" stretchy="false" xref="S2.SS1.SSS2.p3.7.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.7.m5.2b"><interval closure="open" id="S2.SS1.SSS2.p3.7.m5.2.3.1.cmml" xref="S2.SS1.SSS2.p3.7.m5.2.3.2"><ci id="S2.SS1.SSS2.p3.7.m5.1.1.cmml" xref="S2.SS1.SSS2.p3.7.m5.1.1">ğ‘Ÿ</ci><apply id="S2.SS1.SSS2.p3.7.m5.2.2.cmml" xref="S2.SS1.SSS2.p3.7.m5.2.2"><ci id="S2.SS1.SSS2.p3.7.m5.2.2.1.cmml" xref="S2.SS1.SSS2.p3.7.m5.2.2.1">â†’</ci><ci id="S2.SS1.SSS2.p3.7.m5.2.2.2.cmml" xref="S2.SS1.SSS2.p3.7.m5.2.2.2">ğ‘£</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.7.m5.2c">(r,\vec{v})</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.7.m5.2d">( italic_r , overâ†’ start_ARG italic_v end_ARG )</annotation></semantics></math> where <math alttext="r" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.8.m6.1"><semantics id="S2.SS1.SSS2.p3.8.m6.1a"><mi id="S2.SS1.SSS2.p3.8.m6.1.1" xref="S2.SS1.SSS2.p3.8.m6.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.8.m6.1b"><ci id="S2.SS1.SSS2.p3.8.m6.1.1.cmml" xref="S2.SS1.SSS2.p3.8.m6.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.8.m6.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.8.m6.1d">italic_r</annotation></semantics></math> is the scalar part of the quaternion, and <math alttext="\vec{v}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.9.m7.1"><semantics id="S2.SS1.SSS2.p3.9.m7.1a"><mover accent="true" id="S2.SS1.SSS2.p3.9.m7.1.1" xref="S2.SS1.SSS2.p3.9.m7.1.1.cmml"><mi id="S2.SS1.SSS2.p3.9.m7.1.1.2" xref="S2.SS1.SSS2.p3.9.m7.1.1.2.cmml">v</mi><mo id="S2.SS1.SSS2.p3.9.m7.1.1.1" stretchy="false" xref="S2.SS1.SSS2.p3.9.m7.1.1.1.cmml">â†’</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.9.m7.1b"><apply id="S2.SS1.SSS2.p3.9.m7.1.1.cmml" xref="S2.SS1.SSS2.p3.9.m7.1.1"><ci id="S2.SS1.SSS2.p3.9.m7.1.1.1.cmml" xref="S2.SS1.SSS2.p3.9.m7.1.1.1">â†’</ci><ci id="S2.SS1.SSS2.p3.9.m7.1.1.2.cmml" xref="S2.SS1.SSS2.p3.9.m7.1.1.2">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.9.m7.1c">\vec{v}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.9.m7.1d">overâ†’ start_ARG italic_v end_ARG</annotation></semantics></math> is the vector part. <math alttext="r" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.10.m8.1"><semantics id="S2.SS1.SSS2.p3.10.m8.1a"><mi id="S2.SS1.SSS2.p3.10.m8.1.1" xref="S2.SS1.SSS2.p3.10.m8.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.10.m8.1b"><ci id="S2.SS1.SSS2.p3.10.m8.1.1.cmml" xref="S2.SS1.SSS2.p3.10.m8.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.10.m8.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.10.m8.1d">italic_r</annotation></semantics></math> is related to a spatial rotation around a fixed point of <math alttext="\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.11.m9.1"><semantics id="S2.SS1.SSS2.p3.11.m9.1a"><msub id="S2.SS1.SSS2.p3.11.m9.1.1" xref="S2.SS1.SSS2.p3.11.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p3.11.m9.1.1.2" xref="S2.SS1.SSS2.p3.11.m9.1.1.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p3.11.m9.1.1.3" xref="S2.SS1.SSS2.p3.11.m9.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.11.m9.1b"><apply id="S2.SS1.SSS2.p3.11.m9.1.1.cmml" xref="S2.SS1.SSS2.p3.11.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p3.11.m9.1.1.1.cmml" xref="S2.SS1.SSS2.p3.11.m9.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p3.11.m9.1.1.2.cmml" xref="S2.SS1.SSS2.p3.11.m9.1.1.2">â„’</ci><ci id="S2.SS1.SSS2.p3.11.m9.1.1.3.cmml" xref="S2.SS1.SSS2.p3.11.m9.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.11.m9.1c">\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.11.m9.1d">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> radians about a unit axis by <math alttext="r=\cos(\mathcal{L}_{\theta}/2)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.12.m10.2"><semantics id="S2.SS1.SSS2.p3.12.m10.2a"><mrow id="S2.SS1.SSS2.p3.12.m10.2.2" xref="S2.SS1.SSS2.p3.12.m10.2.2.cmml"><mi id="S2.SS1.SSS2.p3.12.m10.2.2.3" xref="S2.SS1.SSS2.p3.12.m10.2.2.3.cmml">r</mi><mo id="S2.SS1.SSS2.p3.12.m10.2.2.2" xref="S2.SS1.SSS2.p3.12.m10.2.2.2.cmml">=</mo><mrow id="S2.SS1.SSS2.p3.12.m10.2.2.1.1" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.2.cmml"><mi id="S2.SS1.SSS2.p3.12.m10.1.1" xref="S2.SS1.SSS2.p3.12.m10.1.1.cmml">cos</mi><mo id="S2.SS1.SSS2.p3.12.m10.2.2.1.1a" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.2.cmml">â¡</mo><mrow id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.2.cmml"><mo id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.2" stretchy="false" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.2.cmml">(</mo><mrow id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.cmml"><msub id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.2" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.3" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.3.cmml">Î¸</mi></msub><mo id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.1" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.1.cmml">/</mo><mn id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.3" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.3.cmml">2</mn></mrow><mo id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.3" stretchy="false" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.12.m10.2b"><apply id="S2.SS1.SSS2.p3.12.m10.2.2.cmml" xref="S2.SS1.SSS2.p3.12.m10.2.2"><eq id="S2.SS1.SSS2.p3.12.m10.2.2.2.cmml" xref="S2.SS1.SSS2.p3.12.m10.2.2.2"></eq><ci id="S2.SS1.SSS2.p3.12.m10.2.2.3.cmml" xref="S2.SS1.SSS2.p3.12.m10.2.2.3">ğ‘Ÿ</ci><apply id="S2.SS1.SSS2.p3.12.m10.2.2.1.2.cmml" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1"><cos id="S2.SS1.SSS2.p3.12.m10.1.1.cmml" xref="S2.SS1.SSS2.p3.12.m10.1.1"></cos><apply id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.cmml" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1"><divide id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.1"></divide><apply id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.2.cmml" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.2">â„’</ci><ci id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.3.cmml" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.2.3">ğœƒ</ci></apply><cn id="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS2.p3.12.m10.2.2.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.12.m10.2c">r=\cos(\mathcal{L}_{\theta}/2)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.12.m10.2d">italic_r = roman_cos ( caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT / 2 )</annotation></semantics></math>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib24" title="">24</a>]</cite>, thus <math alttext="\mathcal{L}_{\theta}=2\arccos(r)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.13.m11.2"><semantics id="S2.SS1.SSS2.p3.13.m11.2a"><mrow id="S2.SS1.SSS2.p3.13.m11.2.3" xref="S2.SS1.SSS2.p3.13.m11.2.3.cmml"><msub id="S2.SS1.SSS2.p3.13.m11.2.3.2" xref="S2.SS1.SSS2.p3.13.m11.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p3.13.m11.2.3.2.2" xref="S2.SS1.SSS2.p3.13.m11.2.3.2.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p3.13.m11.2.3.2.3" xref="S2.SS1.SSS2.p3.13.m11.2.3.2.3.cmml">Î¸</mi></msub><mo id="S2.SS1.SSS2.p3.13.m11.2.3.1" xref="S2.SS1.SSS2.p3.13.m11.2.3.1.cmml">=</mo><mrow id="S2.SS1.SSS2.p3.13.m11.2.3.3" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.cmml"><mn id="S2.SS1.SSS2.p3.13.m11.2.3.3.2" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.2.cmml">2</mn><mo id="S2.SS1.SSS2.p3.13.m11.2.3.3.1" lspace="0.167em" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.1.cmml">â¢</mo><mrow id="S2.SS1.SSS2.p3.13.m11.2.3.3.3.2" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.3.1.cmml"><mi id="S2.SS1.SSS2.p3.13.m11.1.1" xref="S2.SS1.SSS2.p3.13.m11.1.1.cmml">arccos</mi><mo id="S2.SS1.SSS2.p3.13.m11.2.3.3.3.2a" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.3.1.cmml">â¡</mo><mrow id="S2.SS1.SSS2.p3.13.m11.2.3.3.3.2.1" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.3.1.cmml"><mo id="S2.SS1.SSS2.p3.13.m11.2.3.3.3.2.1.1" stretchy="false" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.3.1.cmml">(</mo><mi id="S2.SS1.SSS2.p3.13.m11.2.2" xref="S2.SS1.SSS2.p3.13.m11.2.2.cmml">r</mi><mo id="S2.SS1.SSS2.p3.13.m11.2.3.3.3.2.1.2" stretchy="false" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.13.m11.2b"><apply id="S2.SS1.SSS2.p3.13.m11.2.3.cmml" xref="S2.SS1.SSS2.p3.13.m11.2.3"><eq id="S2.SS1.SSS2.p3.13.m11.2.3.1.cmml" xref="S2.SS1.SSS2.p3.13.m11.2.3.1"></eq><apply id="S2.SS1.SSS2.p3.13.m11.2.3.2.cmml" xref="S2.SS1.SSS2.p3.13.m11.2.3.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p3.13.m11.2.3.2.1.cmml" xref="S2.SS1.SSS2.p3.13.m11.2.3.2">subscript</csymbol><ci id="S2.SS1.SSS2.p3.13.m11.2.3.2.2.cmml" xref="S2.SS1.SSS2.p3.13.m11.2.3.2.2">â„’</ci><ci id="S2.SS1.SSS2.p3.13.m11.2.3.2.3.cmml" xref="S2.SS1.SSS2.p3.13.m11.2.3.2.3">ğœƒ</ci></apply><apply id="S2.SS1.SSS2.p3.13.m11.2.3.3.cmml" xref="S2.SS1.SSS2.p3.13.m11.2.3.3"><times id="S2.SS1.SSS2.p3.13.m11.2.3.3.1.cmml" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.1"></times><cn id="S2.SS1.SSS2.p3.13.m11.2.3.3.2.cmml" type="integer" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.2">2</cn><apply id="S2.SS1.SSS2.p3.13.m11.2.3.3.3.1.cmml" xref="S2.SS1.SSS2.p3.13.m11.2.3.3.3.2"><arccos id="S2.SS1.SSS2.p3.13.m11.1.1.cmml" xref="S2.SS1.SSS2.p3.13.m11.1.1"></arccos><ci id="S2.SS1.SSS2.p3.13.m11.2.2.cmml" xref="S2.SS1.SSS2.p3.13.m11.2.2">ğ‘Ÿ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.13.m11.2c">\mathcal{L}_{\theta}=2\arccos(r)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.13.m11.2d">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT = 2 roman_arccos ( italic_r )</annotation></semantics></math>. We approximate <math alttext="\mathcal{L}_{\theta}\approx\frac{\pi}{2}(1-r)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.14.m12.1"><semantics id="S2.SS1.SSS2.p3.14.m12.1a"><mrow id="S2.SS1.SSS2.p3.14.m12.1.1" xref="S2.SS1.SSS2.p3.14.m12.1.1.cmml"><msub id="S2.SS1.SSS2.p3.14.m12.1.1.3" xref="S2.SS1.SSS2.p3.14.m12.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p3.14.m12.1.1.3.2" xref="S2.SS1.SSS2.p3.14.m12.1.1.3.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p3.14.m12.1.1.3.3" xref="S2.SS1.SSS2.p3.14.m12.1.1.3.3.cmml">Î¸</mi></msub><mo id="S2.SS1.SSS2.p3.14.m12.1.1.2" xref="S2.SS1.SSS2.p3.14.m12.1.1.2.cmml">â‰ˆ</mo><mrow id="S2.SS1.SSS2.p3.14.m12.1.1.1" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.cmml"><mfrac id="S2.SS1.SSS2.p3.14.m12.1.1.1.3" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.3.cmml"><mi id="S2.SS1.SSS2.p3.14.m12.1.1.1.3.2" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.3.2.cmml">Ï€</mi><mn id="S2.SS1.SSS2.p3.14.m12.1.1.1.3.3" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.3.3.cmml">2</mn></mfrac><mo id="S2.SS1.SSS2.p3.14.m12.1.1.1.2" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.2.cmml">â¢</mo><mrow id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.cmml"><mo id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.2" stretchy="false" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.cmml"><mn id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.2" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.2.cmml">1</mn><mo id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.1" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.3" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.3.cmml">r</mi></mrow><mo id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.3" stretchy="false" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.14.m12.1b"><apply id="S2.SS1.SSS2.p3.14.m12.1.1.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1"><approx id="S2.SS1.SSS2.p3.14.m12.1.1.2.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.2"></approx><apply id="S2.SS1.SSS2.p3.14.m12.1.1.3.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p3.14.m12.1.1.3.1.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.3">subscript</csymbol><ci id="S2.SS1.SSS2.p3.14.m12.1.1.3.2.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.3.2">â„’</ci><ci id="S2.SS1.SSS2.p3.14.m12.1.1.3.3.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.3.3">ğœƒ</ci></apply><apply id="S2.SS1.SSS2.p3.14.m12.1.1.1.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.1"><times id="S2.SS1.SSS2.p3.14.m12.1.1.1.2.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.2"></times><apply id="S2.SS1.SSS2.p3.14.m12.1.1.1.3.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.3"><divide id="S2.SS1.SSS2.p3.14.m12.1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.3"></divide><ci id="S2.SS1.SSS2.p3.14.m12.1.1.1.3.2.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.3.2">ğœ‹</ci><cn id="S2.SS1.SSS2.p3.14.m12.1.1.1.3.3.cmml" type="integer" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.3.3">2</cn></apply><apply id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1"><minus id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.1"></minus><cn id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.2.cmml" type="integer" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.2">1</cn><ci id="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.3.cmml" xref="S2.SS1.SSS2.p3.14.m12.1.1.1.1.1.1.3">ğ‘Ÿ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.14.m12.1c">\mathcal{L}_{\theta}\approx\frac{\pi}{2}(1-r)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.14.m12.1d">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT â‰ˆ divide start_ARG italic_Ï€ end_ARG start_ARG 2 end_ARG ( 1 - italic_r )</annotation></semantics></math>, using a Taylor series approximation. The Eulerian angular difference loss provides a more intuitive and direct measure of orientation error.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p4">
<p class="ltx_p" id="S2.SS1.SSS2.p4.2">Additionally, we replace the hyperparameter weight factor <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p4.1.m1.1"><semantics id="S2.SS1.SSS2.p4.1.m1.1a"><mi id="S2.SS1.SSS2.p4.1.m1.1.1" xref="S2.SS1.SSS2.p4.1.m1.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p4.1.m1.1b"><ci id="S2.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p4.1.m1.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p4.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p4.1.m1.1d">italic_Î²</annotation></semantics></math> in (<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.E1" title="In II-A2 Loss Function â€£ II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">1</span></a>) which required tuning, with the average distance <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p4.2.m2.1"><semantics id="S2.SS1.SSS2.p4.2.m2.1a"><mi id="S2.SS1.SSS2.p4.2.m2.1.1" xref="S2.SS1.SSS2.p4.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p4.2.m2.1b"><ci id="S2.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p4.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p4.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p4.2.m2.1d">italic_d</annotation></semantics></math> between the camera and the object of interest. The intuition here is that this factor translates the rotational error to an equivalent â€œaverageâ€ translational error (attributed to the orientation difference). Thus, the overall loss can be interpreted as the â€œtotal positional errorâ€ in meters, including contributions from translational and orientation error components.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p5">
<p class="ltx_p" id="S2.SS1.SSS2.p5.8">The translation between rotational error and the â€œaverageâ€ translational error is described as follows. As illustrated in the example in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.F3" title="Figure 3 â€£ II-A2 Loss Function â€£ II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">3</span></a>, if the camera has a pitch orientation error <math alttext="\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p5.1.m1.1"><semantics id="S2.SS1.SSS2.p5.1.m1.1a"><msub id="S2.SS1.SSS2.p5.1.m1.1.1" xref="S2.SS1.SSS2.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p5.1.m1.1.1.2" xref="S2.SS1.SSS2.p5.1.m1.1.1.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p5.1.m1.1.1.3" xref="S2.SS1.SSS2.p5.1.m1.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p5.1.m1.1b"><apply id="S2.SS1.SSS2.p5.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p5.1.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p5.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p5.1.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p5.1.m1.1.1.2">â„’</ci><ci id="S2.SS1.SSS2.p5.1.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p5.1.m1.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p5.1.m1.1c">\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p5.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> of <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p5.2.m2.1"><semantics id="S2.SS1.SSS2.p5.2.m2.1a"><mi id="S2.SS1.SSS2.p5.2.m2.1.1" xref="S2.SS1.SSS2.p5.2.m2.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p5.2.m2.1b"><ci id="S2.SS1.SSS2.p5.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p5.2.m2.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p5.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p5.2.m2.1d">italic_Î¸</annotation></semantics></math>, the point it observes on the structure remains roughly the same as if the camera had an equivalent translational error <math alttext="\mathcal{L}_{\mathbf{p}}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p5.3.m3.1"><semantics id="S2.SS1.SSS2.p5.3.m3.1a"><msub id="S2.SS1.SSS2.p5.3.m3.1.1" xref="S2.SS1.SSS2.p5.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p5.3.m3.1.1.2" xref="S2.SS1.SSS2.p5.3.m3.1.1.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p5.3.m3.1.1.3" xref="S2.SS1.SSS2.p5.3.m3.1.1.3.cmml">ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p5.3.m3.1b"><apply id="S2.SS1.SSS2.p5.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p5.3.m3.1.1.1.cmml" xref="S2.SS1.SSS2.p5.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p5.3.m3.1.1.2.cmml" xref="S2.SS1.SSS2.p5.3.m3.1.1.2">â„’</ci><ci id="S2.SS1.SSS2.p5.3.m3.1.1.3.cmml" xref="S2.SS1.SSS2.p5.3.m3.1.1.3">ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p5.3.m3.1c">\mathcal{L}_{\mathbf{p}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p5.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT</annotation></semantics></math> of <math alttext="h" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p5.4.m4.1"><semantics id="S2.SS1.SSS2.p5.4.m4.1a"><mi id="S2.SS1.SSS2.p5.4.m4.1.1" xref="S2.SS1.SSS2.p5.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p5.4.m4.1b"><ci id="S2.SS1.SSS2.p5.4.m4.1.1.cmml" xref="S2.SS1.SSS2.p5.4.m4.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p5.4.m4.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p5.4.m4.1d">italic_h</annotation></semantics></math> (i.e., moves up by <math alttext="h" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p5.5.m5.1"><semantics id="S2.SS1.SSS2.p5.5.m5.1a"><mi id="S2.SS1.SSS2.p5.5.m5.1.1" xref="S2.SS1.SSS2.p5.5.m5.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p5.5.m5.1b"><ci id="S2.SS1.SSS2.p5.5.m5.1.1.cmml" xref="S2.SS1.SSS2.p5.5.m5.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p5.5.m5.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p5.5.m5.1d">italic_h</annotation></semantics></math>) for small values of <math alttext="h" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p5.6.m6.1"><semantics id="S2.SS1.SSS2.p5.6.m6.1a"><mi id="S2.SS1.SSS2.p5.6.m6.1.1" xref="S2.SS1.SSS2.p5.6.m6.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p5.6.m6.1b"><ci id="S2.SS1.SSS2.p5.6.m6.1.1.cmml" xref="S2.SS1.SSS2.p5.6.m6.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p5.6.m6.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p5.6.m6.1d">italic_h</annotation></semantics></math> and <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p5.7.m7.1"><semantics id="S2.SS1.SSS2.p5.7.m7.1a"><mi id="S2.SS1.SSS2.p5.7.m7.1.1" xref="S2.SS1.SSS2.p5.7.m7.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p5.7.m7.1b"><ci id="S2.SS1.SSS2.p5.7.m7.1.1.cmml" xref="S2.SS1.SSS2.p5.7.m7.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p5.7.m7.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p5.7.m7.1d">italic_Î¸</annotation></semantics></math>. Based on the geometry, equivalent translational error can be expressed in terms of orientation error <math alttext="\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p5.8.m8.1"><semantics id="S2.SS1.SSS2.p5.8.m8.1a"><msub id="S2.SS1.SSS2.p5.8.m8.1.1" xref="S2.SS1.SSS2.p5.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p5.8.m8.1.1.2" xref="S2.SS1.SSS2.p5.8.m8.1.1.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p5.8.m8.1.1.3" xref="S2.SS1.SSS2.p5.8.m8.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p5.8.m8.1b"><apply id="S2.SS1.SSS2.p5.8.m8.1.1.cmml" xref="S2.SS1.SSS2.p5.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p5.8.m8.1.1.1.cmml" xref="S2.SS1.SSS2.p5.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p5.8.m8.1.1.2.cmml" xref="S2.SS1.SSS2.p5.8.m8.1.1.2">â„’</ci><ci id="S2.SS1.SSS2.p5.8.m8.1.1.3.cmml" xref="S2.SS1.SSS2.p5.8.m8.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p5.8.m8.1c">\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p5.8.m8.1d">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> and the average horizontal range between the camera and the structure as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\mathbf{p}}=d{\tan(\mathcal{L}_{\theta})}." class="ltx_Math" display="block" id="S2.E3.m1.2"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.2.1" xref="S2.E3.m1.2.2.1.1.cmml"><mrow id="S2.E3.m1.2.2.1.1" xref="S2.E3.m1.2.2.1.1.cmml"><msub id="S2.E3.m1.2.2.1.1.3" xref="S2.E3.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.2.2.1.1.3.2" xref="S2.E3.m1.2.2.1.1.3.2.cmml">â„’</mi><mi id="S2.E3.m1.2.2.1.1.3.3" xref="S2.E3.m1.2.2.1.1.3.3.cmml">ğ©</mi></msub><mo id="S2.E3.m1.2.2.1.1.2" xref="S2.E3.m1.2.2.1.1.2.cmml">=</mo><mrow id="S2.E3.m1.2.2.1.1.1" xref="S2.E3.m1.2.2.1.1.1.cmml"><mi id="S2.E3.m1.2.2.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.3.cmml">d</mi><mo id="S2.E3.m1.2.2.1.1.1.2" lspace="0.167em" xref="S2.E3.m1.2.2.1.1.1.2.cmml">â¢</mo><mrow id="S2.E3.m1.2.2.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.2.cmml"><mi id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">tan</mi><mo id="S2.E3.m1.2.2.1.1.1.1.1a" xref="S2.E3.m1.2.2.1.1.1.1.2.cmml">â¡</mo><mrow id="S2.E3.m1.2.2.1.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.2.cmml"><mo id="S2.E3.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.2.2.1.1.1.1.2.cmml">(</mo><msub id="S2.E3.m1.2.2.1.1.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml">â„’</mi><mi id="S2.E3.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.3.cmml">Î¸</mi></msub><mo id="S2.E3.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E3.m1.2.2.1.2" lspace="0em" xref="S2.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.2.1.1.cmml" xref="S2.E3.m1.2.2.1"><eq id="S2.E3.m1.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.2"></eq><apply id="S2.E3.m1.2.2.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.3.1.cmml" xref="S2.E3.m1.2.2.1.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.3.2.cmml" xref="S2.E3.m1.2.2.1.1.3.2">â„’</ci><ci id="S2.E3.m1.2.2.1.1.3.3.cmml" xref="S2.E3.m1.2.2.1.1.3.3">ğ©</ci></apply><apply id="S2.E3.m1.2.2.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1"><times id="S2.E3.m1.2.2.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.2"></times><ci id="S2.E3.m1.2.2.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.3">ğ‘‘</ci><apply id="S2.E3.m1.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1"><tan id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"></tan><apply id="S2.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.2">â„’</ci><ci id="S2.E3.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.3">ğœƒ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">\mathcal{L}_{\mathbf{p}}=d{\tan(\mathcal{L}_{\theta})}.</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.2d">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT = italic_d roman_tan ( caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.SSS2.p5.9">Assuming the case when the rotational error is small, we approximate <math alttext="\tan(\mathcal{L}_{\theta})\approx\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p5.9.m1.2"><semantics id="S2.SS1.SSS2.p5.9.m1.2a"><mrow id="S2.SS1.SSS2.p5.9.m1.2.2" xref="S2.SS1.SSS2.p5.9.m1.2.2.cmml"><mrow id="S2.SS1.SSS2.p5.9.m1.2.2.1.1" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.2.cmml"><mi id="S2.SS1.SSS2.p5.9.m1.1.1" xref="S2.SS1.SSS2.p5.9.m1.1.1.cmml">tan</mi><mo id="S2.SS1.SSS2.p5.9.m1.2.2.1.1a" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.2.cmml">â¡</mo><mrow id="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.2.cmml"><mo id="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.2" stretchy="false" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.2.cmml">(</mo><msub id="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.2" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.3" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.3.cmml">Î¸</mi></msub><mo id="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.3" stretchy="false" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.2.cmml">)</mo></mrow></mrow><mo id="S2.SS1.SSS2.p5.9.m1.2.2.2" xref="S2.SS1.SSS2.p5.9.m1.2.2.2.cmml">â‰ˆ</mo><msub id="S2.SS1.SSS2.p5.9.m1.2.2.3" xref="S2.SS1.SSS2.p5.9.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p5.9.m1.2.2.3.2" xref="S2.SS1.SSS2.p5.9.m1.2.2.3.2.cmml">â„’</mi><mi id="S2.SS1.SSS2.p5.9.m1.2.2.3.3" xref="S2.SS1.SSS2.p5.9.m1.2.2.3.3.cmml">Î¸</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p5.9.m1.2b"><apply id="S2.SS1.SSS2.p5.9.m1.2.2.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2"><approx id="S2.SS1.SSS2.p5.9.m1.2.2.2.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2.2"></approx><apply id="S2.SS1.SSS2.p5.9.m1.2.2.1.2.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.1"><tan id="S2.SS1.SSS2.p5.9.m1.1.1.cmml" xref="S2.SS1.SSS2.p5.9.m1.1.1"></tan><apply id="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.2">â„’</ci><ci id="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.3.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2.1.1.1.1.3">ğœƒ</ci></apply></apply><apply id="S2.SS1.SSS2.p5.9.m1.2.2.3.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p5.9.m1.2.2.3.1.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2.3">subscript</csymbol><ci id="S2.SS1.SSS2.p5.9.m1.2.2.3.2.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2.3.2">â„’</ci><ci id="S2.SS1.SSS2.p5.9.m1.2.2.3.3.cmml" xref="S2.SS1.SSS2.p5.9.m1.2.2.3.3">ğœƒ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p5.9.m1.2c">\tan(\mathcal{L}_{\theta})\approx\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p5.9.m1.2d">roman_tan ( caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ) â‰ˆ caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math>. Thus, we obtain:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\mathbf{p}}\approx d\mathcal{L}_{\theta}." class="ltx_Math" display="block" id="S2.E4.m1.1"><semantics id="S2.E4.m1.1a"><mrow id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><msub id="S2.E4.m1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.1.1.1.1.2.2" xref="S2.E4.m1.1.1.1.1.2.2.cmml">â„’</mi><mi id="S2.E4.m1.1.1.1.1.2.3" xref="S2.E4.m1.1.1.1.1.2.3.cmml">ğ©</mi></msub><mo id="S2.E4.m1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.cmml">â‰ˆ</mo><mrow id="S2.E4.m1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.3.2.cmml">d</mi><mo id="S2.E4.m1.1.1.1.1.3.1" xref="S2.E4.m1.1.1.1.1.3.1.cmml">â¢</mo><msub id="S2.E4.m1.1.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.1.1.1.1.3.3.2" xref="S2.E4.m1.1.1.1.1.3.3.2.cmml">â„’</mi><mi id="S2.E4.m1.1.1.1.1.3.3.3" xref="S2.E4.m1.1.1.1.1.3.3.3.cmml">Î¸</mi></msub></mrow></mrow><mo id="S2.E4.m1.1.1.1.2" lspace="0em" xref="S2.E4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"><approx id="S2.E4.m1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1"></approx><apply id="S2.E4.m1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.1.2">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.1.1.2.2">â„’</ci><ci id="S2.E4.m1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.1.1.2.3">ğ©</ci></apply><apply id="S2.E4.m1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.3"><times id="S2.E4.m1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.3.1"></times><ci id="S2.E4.m1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.3.2">ğ‘‘</ci><apply id="S2.E4.m1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m1.1.1.1.1.3.3.2">â„’</ci><ci id="S2.E4.m1.1.1.1.1.3.3.3.cmml" xref="S2.E4.m1.1.1.1.1.3.3.3">ğœƒ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">\mathcal{L}_{\mathbf{p}}\approx d\mathcal{L}_{\theta}.</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.1d">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT â‰ˆ italic_d caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="S2.F3">
<p class="ltx_p ltx_align_center" id="S2.F3.1"><span class="ltx_text" id="S2.F3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="187" id="S2.F3.1.1.g1" src="extracted/5751340/Data/figures/d_loss.png" width="299"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.4.2" style="font-size:90%;">Schematic showing the interpretation of the orientation error in terms of equivalent translational error.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS2.p6">
<p class="ltx_p" id="S2.SS1.SSS2.p6.1">This modified loss function (Eq.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.E2" title="In II-A2 Loss Function â€£ II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">2</span></a>) leverages the inherent geometric relationship between positional and rotational errors in inspection missions, reducing computational complexity in finding the optimal <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p6.1.m1.1"><semantics id="S2.SS1.SSS2.p6.1.m1.1a"><mi id="S2.SS1.SSS2.p6.1.m1.1.1" xref="S2.SS1.SSS2.p6.1.m1.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p6.1.m1.1b"><ci id="S2.SS1.SSS2.p6.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p6.1.m1.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p6.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p6.1.m1.1d">italic_Î²</annotation></semantics></math> as well as provides a more intuitive and interpretable overall loss function which has a physical meaning and represents the pose error in terms of meters.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS3.5.1.1">II-A</span>3 </span>Implementation</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">To evaluate the effectiveness of deeper backbones, additional LSTM layers, the proposed <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p1.1.m1.1"><semantics id="S2.SS1.SSS3.p1.1.m1.1a"><mi id="S2.SS1.SSS3.p1.1.m1.1.1" xref="S2.SS1.SSS3.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.1.m1.1b"><ci id="S2.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p1.1.m1.1d">italic_d</annotation></semantics></math>-loss, and the color information in images, we tested multiple configurations of the two visual localization network architectures. The details of these configurations are summarized in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.T1" title="TABLE I â€£ II-A3 Implementation â€£ II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.8.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S2.T1.9.2" style="font-size:90%;">Description of configurations</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.6.7.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.6.7.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.6.7.1.1.1">ID</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.6.7.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.6.7.1.2.1">Architecture</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.6.7.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.6.7.1.3.1">Backbone</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.6.7.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.6.7.1.4.1">Loss</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.6.7.1.5"><span class="ltx_text ltx_font_bold" id="S2.T1.6.7.1.5.1">Color</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2">C1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3">CNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.4">GoogLeNet</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1">
<math alttext="\beta" class="ltx_Math" display="inline" id="S2.T1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.m1.1a"><mi id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.m1.1d">italic_Î²</annotation></semantics></math>-loss</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.5">RGB</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2">
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.2">C2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.3">CNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.4">GoogLeNet</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.1">
<math alttext="d" class="ltx_Math" display="inline" id="S2.T1.2.2.1.m1.1"><semantics id="S2.T1.2.2.1.m1.1a"><mi id="S2.T1.2.2.1.m1.1.1" xref="S2.T1.2.2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.m1.1b"><ci id="S2.T1.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.1.m1.1d">italic_d</annotation></semantics></math>-loss</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5">Grayscale</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.3">
<td class="ltx_td ltx_align_center" id="S2.T1.3.3.2">C3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.3.3">CNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.3.4">GoogLeNet</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.3.1">
<math alttext="d" class="ltx_Math" display="inline" id="S2.T1.3.3.1.m1.1"><semantics id="S2.T1.3.3.1.m1.1a"><mi id="S2.T1.3.3.1.m1.1.1" xref="S2.T1.3.3.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.1.m1.1b"><ci id="S2.T1.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.1.m1.1d">italic_d</annotation></semantics></math>-loss</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.3.5">RGB</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.4">
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.2">C4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.3">CNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.4">ResNet50</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.1">
<math alttext="d" class="ltx_Math" display="inline" id="S2.T1.4.4.1.m1.1"><semantics id="S2.T1.4.4.1.m1.1a"><mi id="S2.T1.4.4.1.m1.1.1" xref="S2.T1.4.4.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.1.m1.1b"><ci id="S2.T1.4.4.1.m1.1.1.cmml" xref="S2.T1.4.4.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.1.m1.1d">italic_d</annotation></semantics></math>-loss</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.5">RGB</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.5">
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.2">C5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.3">CNN+LSTM</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.4">GoogLeNet</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.1">
<math alttext="d" class="ltx_Math" display="inline" id="S2.T1.5.5.1.m1.1"><semantics id="S2.T1.5.5.1.m1.1a"><mi id="S2.T1.5.5.1.m1.1.1" xref="S2.T1.5.5.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.m1.1b"><ci id="S2.T1.5.5.1.m1.1.1.cmml" xref="S2.T1.5.5.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.5.1.m1.1d">italic_d</annotation></semantics></math>-loss</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.5">RGB</td>
</tr>
<tr class="ltx_tr" id="S2.T1.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.6.6.2">C6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.6.6.3">CNN+LSTM</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.6.6.4">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.6.6.1">
<math alttext="d" class="ltx_Math" display="inline" id="S2.T1.6.6.1.m1.1"><semantics id="S2.T1.6.6.1.m1.1a"><mi id="S2.T1.6.6.1.m1.1.1" xref="S2.T1.6.6.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.1.m1.1b"><ci id="S2.T1.6.6.1.m1.1.1.cmml" xref="S2.T1.6.6.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.T1.6.6.1.m1.1d">italic_d</annotation></semantics></math>-loss</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.6.6.5">RGB</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S2.SS1.SSS3.p2">
<p class="ltx_p" id="S2.SS1.SSS3.p2.1">To adapt the pretrained GoogLeNet architecture to operate on grayscale images, we modified the networkâ€™s first convolutional layer by adjusting it to accept single-channel (grayscale) inputs instead of the original three-channel (RGB) inputs. This adjustment was achieved by reducing the number of input channels from three to one. The single-channel input layer was initialized by summing the weights across the RGB channels in the original network to utilize as much prior information as possible. Following this, the layer was fine-tuned via training.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p3">
<p class="ltx_p" id="S2.SS1.SSS3.p3.3">During both training and testing for all configurations, we rescaled input images directly into a 224<math alttext="\times" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p3.1.m1.1"><semantics id="S2.SS1.SSS3.p3.1.m1.1a"><mo id="S2.SS1.SSS3.p3.1.m1.1.1" xref="S2.SS1.SSS3.p3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p3.1.m1.1b"><times id="S2.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p3.1.m1.1d">Ã—</annotation></semantics></math>224 pixels input, deviating from PoseNetâ€™s approach of resizing the images to 256<math alttext="\times" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p3.2.m2.1"><semantics id="S2.SS1.SSS3.p3.2.m2.1a"><mo id="S2.SS1.SSS3.p3.2.m2.1.1" xref="S2.SS1.SSS3.p3.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p3.2.m2.1b"><times id="S2.SS1.SSS3.p3.2.m2.1.1.cmml" xref="S2.SS1.SSS3.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p3.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p3.2.m2.1d">Ã—</annotation></semantics></math>256 before cropping into 224<math alttext="\times" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p3.3.m3.1"><semantics id="S2.SS1.SSS3.p3.3.m3.1a"><mo id="S2.SS1.SSS3.p3.3.m3.1.1" xref="S2.SS1.SSS3.p3.3.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p3.3.m3.1b"><times id="S2.SS1.SSS3.p3.3.m3.1.1.cmml" xref="S2.SS1.SSS3.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p3.3.m3.1d">Ã—</annotation></semantics></math>224. This adjustment was made to minimize the loss of image information, a concern particularly acute in underwater images where available information is inherently more limited compared to terrestrial settings. To speed up training, we normalized the images against the ImageNet datasetâ€™s mean and standard deviation. Additionally, poses are normalized to lie within the range [-1, 1].</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p4">
<p class="ltx_p" id="S2.SS1.SSS3.p4.1">We used the PyTorch deep learning framework to implement and train the models. The experiments were conducted using an RTX 6000 Ada GPU. For training, we used the stochastic gradient descent optimizer for configurations C1, C2, and C3. For the remaining configurations, we used the Adam optimizer. A batch size of 32 was used. Hyperparameters, including the learning rate, weight decay, and <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p4.1.m1.1"><semantics id="S2.SS1.SSS3.p4.1.m1.1a"><mi id="S2.SS1.SSS3.p4.1.m1.1.1" xref="S2.SS1.SSS3.p4.1.m1.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p4.1.m1.1b"><ci id="S2.SS1.SSS3.p4.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p4.1.m1.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p4.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p4.1.m1.1d">italic_Î²</annotation></semantics></math> for C1, were tuned using grid search strategy over a predefined set of values. The best set of hyperparameters was selected based on validation performance. Training continued until early stopping was triggered.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Testing in Controlled Environment</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.4">The artificial ocean basin at TCOMS is an indoor pool measuring 60Â m <math alttext="\times" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mo id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><times id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">Ã—</annotation></semantics></math> 48Â m <math alttext="\times" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mo id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><times id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">Ã—</annotation></semantics></math> 12Â m. As illustrated in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.F4" title="Figure 4 â€£ II-B Testing in Controlled Environment â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">4</span></a>, a structure was placed in the basin, which consisted of six piles interconnected by metallic pipes, with each pile comprising three metallic oil barrels. The overall dimensions of the structure were approximately 3.9Â m <math alttext="\times" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><mo id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><times id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">Ã—</annotation></semantics></math> 4.6Â m <math alttext="\times" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><mo id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><times id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">Ã—</annotation></semantics></math> 3.0Â m. The whole structure was yellow in color.
We used a customized remotely operated vehicle (ROV) which is equipped with a monocular camera for collecting RGB image data, a compass for collecting orientation information, and an altimeter for collecting altitude information. We placed a high frequency acoustic modem with four receivers near the operating region (as shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.F5" title="Figure 5 â€£ II-B Testing in Controlled Environment â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">5</span></a>) to estimate the position of the ROV using ultra-short baseline (USBL) positioning. We operated the ROV to inspect the piles in a lawnmower path.</p>
</div>
<figure class="ltx_figure" id="S2.F4">
<p class="ltx_p ltx_align_center" id="S2.F4.1"><span class="ltx_text" id="S2.F4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="204" id="S2.F4.1.1.g1" src="extracted/5751340/Data/figures/6pile_struc.png" width="299"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S2.F4.4.2" style="font-size:90%;">Schematic of the structure surveyed in the TCOMS facility.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F5">
<p class="ltx_p ltx_align_center" id="S2.F5.1"><span class="ltx_text" id="S2.F5.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="225" id="S2.F5.1.1.g1" src="extracted/5751340/Data/figures/hydra_and_usbl.jpg" width="299"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S2.F5.4.2" style="font-size:90%;">The USBL setup at TCOMS to estimate the location of the ROV (shown).</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">We executed three trials within the environment at different depths to gather data while the ROV surveyed the structure, with each trial featuring a roughly similar trajectory. We refer to these trials as D1, D2 and D3, corresponding to respective average depth levels -1.5Â m, -3Â m and -4Â m. The details of the datasets are described in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.T2" title="TABLE II â€£ II-B Testing in Controlled Environment â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.2.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S2.T2.3.2" style="font-size:90%;">Description of datasets</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.4.1.1.1.1">ID</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.4.1.1.2.1">Dataset Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.4.1.1.3.1">Dataset Size</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.4.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.4.2.1.1">D1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.4.2.1.2">Clear Water-Deep</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.4.2.1.3">2165</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.3.2">
<td class="ltx_td ltx_align_center" id="S2.T2.4.3.2.1">D2</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.3.2.2">Clear Water-Shallow</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.3.2.3">2956</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.4.3">
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.3.1">D3</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.3.2">Clear Water-Mid</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.3.3">933</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.5.4">
<td class="ltx_td ltx_align_center" id="S2.T2.4.5.4.1">D4</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.5.4.2">Clear Water-NVS</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.5.4.3">4193</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.6.5">
<td class="ltx_td ltx_align_center" id="S2.T2.4.6.5.1">D5</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.6.5.2">Sea Water-1</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.6.5.3">2360</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.7.6">
<td class="ltx_td ltx_align_center" id="S2.T2.4.7.6.1">D6</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.7.6.2">Sea Water-2</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.7.6.3">735</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.8.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.4.8.7.1">D7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.4.8.7.2">Sea Water-NVS</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.4.8.7.3">18918</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The sensor data from the vehicle was captured using ROS (Robot Operating System), and we sampled the dataset at a frequency of 5Â Hz from each recorded data file. We synchronized the sampled data with the USBL position information based on timestamp and interpolate the position data when necessary. For our ground truth, we utilized the x and y coordinates from the USBL position estimates, the z coordinate from the altimeter, and the orientation data from the compass.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">We use D1 as the primary dataset to evaluate the modelsâ€™ capability of interpolation. We randomly select 60% points from the data for training, 20% for validation and 20% for testing. We further assess the modelsâ€™ ability to generalize to new depths by employing D1 as the training dataset and D3 for validation and testing. Additionally, we investigate the impact of incorporating data from diverse depths on the modelsâ€™ generalization performance by using D1 and D2 together as the training data and D3 as the validation and test data.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Results &amp; Discussions</span>
</h3>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS1.5.1.1">II-C</span>1 </span>Model performance</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">We present the performance of different configurations in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.T3" title="TABLE III â€£ II-C1 Model performance â€£ II-C Results &amp; Discussions â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">III</span></a>. The benchmark for our evaluation is the performance of C1.</p>
</div>
<figure class="ltx_table" id="S2.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T3.13.5.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S2.T3.8.4" style="font-size:90%;">Performance of all configurations trained and tested on dataset D1. <math alttext="\mathcal{L}_{\mathbf{p}}" class="ltx_Math" display="inline" id="S2.T3.5.1.m1.1"><semantics id="S2.T3.5.1.m1.1b"><msub id="S2.T3.5.1.m1.1.1" xref="S2.T3.5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T3.5.1.m1.1.1.2" xref="S2.T3.5.1.m1.1.1.2.cmml">â„’</mi><mi id="S2.T3.5.1.m1.1.1.3" xref="S2.T3.5.1.m1.1.1.3.cmml">ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T3.5.1.m1.1c"><apply id="S2.T3.5.1.m1.1.1.cmml" xref="S2.T3.5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T3.5.1.m1.1.1.1.cmml" xref="S2.T3.5.1.m1.1.1">subscript</csymbol><ci id="S2.T3.5.1.m1.1.1.2.cmml" xref="S2.T3.5.1.m1.1.1.2">â„’</ci><ci id="S2.T3.5.1.m1.1.1.3.cmml" xref="S2.T3.5.1.m1.1.1.3">ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.5.1.m1.1d">\mathcal{L}_{\mathbf{p}}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.5.1.m1.1e">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S2.T3.6.2.m2.1"><semantics id="S2.T3.6.2.m2.1b"><msub id="S2.T3.6.2.m2.1.1" xref="S2.T3.6.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T3.6.2.m2.1.1.2" xref="S2.T3.6.2.m2.1.1.2.cmml">â„’</mi><mi id="S2.T3.6.2.m2.1.1.3" xref="S2.T3.6.2.m2.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T3.6.2.m2.1c"><apply id="S2.T3.6.2.m2.1.1.cmml" xref="S2.T3.6.2.m2.1.1"><csymbol cd="ambiguous" id="S2.T3.6.2.m2.1.1.1.cmml" xref="S2.T3.6.2.m2.1.1">subscript</csymbol><ci id="S2.T3.6.2.m2.1.1.2.cmml" xref="S2.T3.6.2.m2.1.1.2">â„’</ci><ci id="S2.T3.6.2.m2.1.1.3.cmml" xref="S2.T3.6.2.m2.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.6.2.m2.1d">\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.6.2.m2.1e">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> tabulated are the median of estimates across the test data. <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S2.T3.7.3.m3.1"><semantics id="S2.T3.7.3.m3.1b"><mi class="ltx_font_mathcaligraphic" id="S2.T3.7.3.m3.1.1" xref="S2.T3.7.3.m3.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S2.T3.7.3.m3.1c"><ci id="S2.T3.7.3.m3.1.1.cmml" xref="S2.T3.7.3.m3.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.7.3.m3.1d">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.7.3.m3.1e">caligraphic_L</annotation></semantics></math> was calculated using Eq.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.E2" title="In II-A2 Loss Function â€£ II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">2</span></a> with <math alttext="d=3" class="ltx_Math" display="inline" id="S2.T3.8.4.m4.1"><semantics id="S2.T3.8.4.m4.1b"><mrow id="S2.T3.8.4.m4.1.1" xref="S2.T3.8.4.m4.1.1.cmml"><mi id="S2.T3.8.4.m4.1.1.2" xref="S2.T3.8.4.m4.1.1.2.cmml">d</mi><mo id="S2.T3.8.4.m4.1.1.1" xref="S2.T3.8.4.m4.1.1.1.cmml">=</mo><mn id="S2.T3.8.4.m4.1.1.3" xref="S2.T3.8.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.8.4.m4.1c"><apply id="S2.T3.8.4.m4.1.1.cmml" xref="S2.T3.8.4.m4.1.1"><eq id="S2.T3.8.4.m4.1.1.1.cmml" xref="S2.T3.8.4.m4.1.1.1"></eq><ci id="S2.T3.8.4.m4.1.1.2.cmml" xref="S2.T3.8.4.m4.1.1.2">ğ‘‘</ci><cn id="S2.T3.8.4.m4.1.1.3.cmml" type="integer" xref="S2.T3.8.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.8.4.m4.1d">d=3</annotation><annotation encoding="application/x-llamapun" id="S2.T3.8.4.m4.1e">italic_d = 3</annotation></semantics></math>Â m. The best performance for each metric is highlighted in bold.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T3.11">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T3.11.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.11.3.4"><span class="ltx_text ltx_font_bold" id="S2.T3.11.3.4.1">ID</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.9.1.1">
<math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S2.T3.9.1.1.m1.1"><semantics id="S2.T3.9.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.T3.9.1.1.m1.1.1" xref="S2.T3.9.1.1.m1.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S2.T3.9.1.1.m1.1b"><ci id="S2.T3.9.1.1.m1.1.1.cmml" xref="S2.T3.9.1.1.m1.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.9.1.1.m1.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.9.1.1.m1.1d">caligraphic_L</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S2.T3.9.1.1.1"> (m)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.10.2.2">
<math alttext="\mathcal{L}_{\mathbf{p}}" class="ltx_Math" display="inline" id="S2.T3.10.2.2.m1.1"><semantics id="S2.T3.10.2.2.m1.1a"><msub id="S2.T3.10.2.2.m1.1.1" xref="S2.T3.10.2.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T3.10.2.2.m1.1.1.2" xref="S2.T3.10.2.2.m1.1.1.2.cmml">â„’</mi><mi id="S2.T3.10.2.2.m1.1.1.3" xref="S2.T3.10.2.2.m1.1.1.3.cmml">ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T3.10.2.2.m1.1b"><apply id="S2.T3.10.2.2.m1.1.1.cmml" xref="S2.T3.10.2.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T3.10.2.2.m1.1.1.1.cmml" xref="S2.T3.10.2.2.m1.1.1">subscript</csymbol><ci id="S2.T3.10.2.2.m1.1.1.2.cmml" xref="S2.T3.10.2.2.m1.1.1.2">â„’</ci><ci id="S2.T3.10.2.2.m1.1.1.3.cmml" xref="S2.T3.10.2.2.m1.1.1.3">ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.10.2.2.m1.1c">\mathcal{L}_{\mathbf{p}}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.10.2.2.m1.1d">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S2.T3.10.2.2.1"> (m)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.11.3.3">
<math alttext="\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S2.T3.11.3.3.m1.1"><semantics id="S2.T3.11.3.3.m1.1a"><msub id="S2.T3.11.3.3.m1.1.1" xref="S2.T3.11.3.3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T3.11.3.3.m1.1.1.2" xref="S2.T3.11.3.3.m1.1.1.2.cmml">â„’</mi><mi id="S2.T3.11.3.3.m1.1.1.3" xref="S2.T3.11.3.3.m1.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T3.11.3.3.m1.1b"><apply id="S2.T3.11.3.3.m1.1.1.cmml" xref="S2.T3.11.3.3.m1.1.1"><csymbol cd="ambiguous" id="S2.T3.11.3.3.m1.1.1.1.cmml" xref="S2.T3.11.3.3.m1.1.1">subscript</csymbol><ci id="S2.T3.11.3.3.m1.1.1.2.cmml" xref="S2.T3.11.3.3.m1.1.1.2">â„’</ci><ci id="S2.T3.11.3.3.m1.1.1.3.cmml" xref="S2.T3.11.3.3.m1.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.11.3.3.m1.1c">\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.11.3.3.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S2.T3.11.3.3.1"> (Â°)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.11.3.5"><span class="ltx_text ltx_font_bold" id="S2.T3.11.3.5.1">Inference</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T3.11.4.1">
<td class="ltx_td" id="S2.T3.11.4.1.1"></td>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T3.11.4.1.2"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T3.11.4.1.3"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T3.11.4.1.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.11.4.1.5"><span class="ltx_text ltx_font_bold" id="S2.T3.11.4.1.5.1">time (ms)</span></th>
</tr>
<tr class="ltx_tr" id="S2.T3.11.5.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.11.5.2.1">C1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.11.5.2.2">2.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.11.5.2.3">2.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.11.5.2.4"><span class="ltx_text ltx_font_bold" id="S2.T3.11.5.2.4.1">0.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.11.5.2.5">2.20</td>
</tr>
<tr class="ltx_tr" id="S2.T3.11.6.3">
<td class="ltx_td ltx_align_center" id="S2.T3.11.6.3.1">C2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.6.3.2">0.61</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.6.3.3">0.53</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.6.3.4">1.50</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.6.3.5">1.65</td>
</tr>
<tr class="ltx_tr" id="S2.T3.11.7.4">
<td class="ltx_td ltx_align_center" id="S2.T3.11.7.4.1">C3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.7.4.2">0.41</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.7.4.3">0.36</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.7.4.4">0.99</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.7.4.5">1.62</td>
</tr>
<tr class="ltx_tr" id="S2.T3.11.8.5">
<td class="ltx_td ltx_align_center" id="S2.T3.11.8.5.1">C4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.8.5.2">0.34</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.8.5.3">0.29</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.8.5.4">0.88</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.8.5.5">1.16</td>
</tr>
<tr class="ltx_tr" id="S2.T3.11.9.6">
<td class="ltx_td ltx_align_center" id="S2.T3.11.9.6.1">C5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.9.6.2">0.30</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.9.6.3">0.22</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.9.6.4">1.51</td>
<td class="ltx_td ltx_align_center" id="S2.T3.11.9.6.5">0.78</td>
</tr>
<tr class="ltx_tr" id="S2.T3.11.10.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.11.10.7.1">C6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.11.10.7.2"><span class="ltx_text ltx_font_bold" id="S2.T3.11.10.7.2.1">0.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.11.10.7.3"><span class="ltx_text ltx_font_bold" id="S2.T3.11.10.7.3.1">0.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.11.10.7.4">1.34</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.11.10.7.5"><span class="ltx_text ltx_font_bold" id="S2.T3.11.10.7.5.1">0.77</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.1">We observe the following:</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p3">
<p class="ltx_p" id="S2.SS3.SSS1.p3.2">1. Comparing the performance of C3 against C1, our results demonstrate that training with our proposed <math alttext="d" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p3.1.m1.1"><semantics id="S2.SS3.SSS1.p3.1.m1.1a"><mi id="S2.SS3.SSS1.p3.1.m1.1.1" xref="S2.SS3.SSS1.p3.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p3.1.m1.1b"><ci id="S2.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p3.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p3.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p3.1.m1.1d">italic_d</annotation></semantics></math>-loss significantly enhances model performance, especially in terms of the overall performance metric <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p3.2.m2.1"><semantics id="S2.SS3.SSS1.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.SSS1.p3.2.m2.1.1" xref="S2.SS3.SSS1.p3.2.m2.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p3.2.m2.1b"><ci id="S2.SS3.SSS1.p3.2.m2.1.1.cmml" xref="S2.SS3.SSS1.p3.2.m2.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p3.2.m2.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p3.2.m2.1d">caligraphic_L</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p4">
<p class="ltx_p" id="S2.SS3.SSS1.p4.1">2. Comparing the performance of C2 against C3, it can be observed that using grayscale images shows significantly worse performance and too little an improvement in inference time, contrary to our initial expectation. The worse performance of grayscale images can be attributed to the fact that since D1 was collected in a non-turbid fresh water environment, the color information in the underwater images is not as limited as one might anticipate in an image taken in a sea environment. As shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.F6" title="Figure 6 â€£ II-C2 Generalization performance â€£ II-C Results &amp; Discussions â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">6</span></a>, the underwater RGB images in D1 retain valuable color information that may provide distinguishing features in these environments. Thus, the grayscale images have much less information than RGB images and thus lead to poorer performance. The lack of improvement in inference time is due to the fact that we only reduce the number of channels in the first CNN layer of the pretrained model, resulting in a minimal reduction in computational load. To achieve more substantial computational savings, the entire model architecture would need to be better streamlined for grayscale images, not just the initial layer.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p5">
<p class="ltx_p" id="S2.SS3.SSS1.p5.1">3. Comparing the performance of C6 to C5 and C4 to C3 shows that using ResNet50, a deeper network, as the backbone, improves performance for both CNN and CNN+LSTM.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p6">
<p class="ltx_p" id="S2.SS3.SSS1.p6.1">4. Comparing the performance of C6 to C4 and C5 to C3 shows that the CNN+LSTM architecture consistently outperforms the CNN architecture.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p7">
<p class="ltx_p" id="S2.SS3.SSS1.p7.1">Among all the configurations, C6, which uses the CNN+LSTM architecture with the ResNet50 backbone and is trained using the proposed <math alttext="d" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p7.1.m1.1"><semantics id="S2.SS3.SSS1.p7.1.m1.1a"><mi id="S2.SS3.SSS1.p7.1.m1.1.1" xref="S2.SS3.SSS1.p7.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p7.1.m1.1b"><ci id="S2.SS3.SSS1.p7.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p7.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p7.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p7.1.m1.1d">italic_d</annotation></semantics></math>-loss, performs the best, achieving 0.12Â m of positional accuracy and 1.34Â°of orientation accuracy with an inference time of 0.77Â ms.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS2.5.1.1">II-C</span>2 </span>Generalization performance</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">We test the performance of generalization using the model with the best configuration, C6. We first trained the model on D1 and tested on D3. A significant performance degradation is observed, as shown in the first row of TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.T4" title="TABLE IV â€£ II-C2 Generalization performance â€£ II-C Results &amp; Discussions â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">IV</span></a>. This is on expected lines because the test data is sampled from a different distribution than the training data with possibly different paths and conditions, and deep-learning models often fail to extrapolate beyond the bounds of the training data.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p2">
<p class="ltx_p" id="S2.SS3.SSS2.p2.1">To address this issue, we evaluate the use of a larger and more diverse training dataset, by expanding the training data to include both D1 and D2. This augmentation introduces a wider distribution of data, notably enhancing the diversity in depth information. This leads to a 49% improvement in model performance in overall loss, as shown in the second row in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.T4" title="TABLE IV â€£ II-C2 Generalization performance â€£ II-C Results &amp; Discussions â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p3">
<p class="ltx_p" id="S2.SS3.SSS2.p3.1">These findings underscore the importance of comprehensive baseline mapping to collect sufficiently diverse training data. This is essential for training models that are robust enough to perform accurate localization during reinspection tasks.</p>
</div>
<figure class="ltx_table" id="S2.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T4.13.5.1" style="font-size:90%;">TABLE IV</span>: </span><span class="ltx_text" id="S2.T4.8.4" style="font-size:90%;">Performance of configuration C6 on dataset D3. <math alttext="\mathcal{L}_{\mathbf{p}}" class="ltx_Math" display="inline" id="S2.T4.5.1.m1.1"><semantics id="S2.T4.5.1.m1.1b"><msub id="S2.T4.5.1.m1.1.1" xref="S2.T4.5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T4.5.1.m1.1.1.2" xref="S2.T4.5.1.m1.1.1.2.cmml">â„’</mi><mi id="S2.T4.5.1.m1.1.1.3" xref="S2.T4.5.1.m1.1.1.3.cmml">ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T4.5.1.m1.1c"><apply id="S2.T4.5.1.m1.1.1.cmml" xref="S2.T4.5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T4.5.1.m1.1.1.1.cmml" xref="S2.T4.5.1.m1.1.1">subscript</csymbol><ci id="S2.T4.5.1.m1.1.1.2.cmml" xref="S2.T4.5.1.m1.1.1.2">â„’</ci><ci id="S2.T4.5.1.m1.1.1.3.cmml" xref="S2.T4.5.1.m1.1.1.3">ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.5.1.m1.1d">\mathcal{L}_{\mathbf{p}}</annotation><annotation encoding="application/x-llamapun" id="S2.T4.5.1.m1.1e">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S2.T4.6.2.m2.1"><semantics id="S2.T4.6.2.m2.1b"><msub id="S2.T4.6.2.m2.1.1" xref="S2.T4.6.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T4.6.2.m2.1.1.2" xref="S2.T4.6.2.m2.1.1.2.cmml">â„’</mi><mi id="S2.T4.6.2.m2.1.1.3" xref="S2.T4.6.2.m2.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T4.6.2.m2.1c"><apply id="S2.T4.6.2.m2.1.1.cmml" xref="S2.T4.6.2.m2.1.1"><csymbol cd="ambiguous" id="S2.T4.6.2.m2.1.1.1.cmml" xref="S2.T4.6.2.m2.1.1">subscript</csymbol><ci id="S2.T4.6.2.m2.1.1.2.cmml" xref="S2.T4.6.2.m2.1.1.2">â„’</ci><ci id="S2.T4.6.2.m2.1.1.3.cmml" xref="S2.T4.6.2.m2.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.6.2.m2.1d">\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.T4.6.2.m2.1e">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> are median values across the test data. <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S2.T4.7.3.m3.1"><semantics id="S2.T4.7.3.m3.1b"><mi class="ltx_font_mathcaligraphic" id="S2.T4.7.3.m3.1.1" xref="S2.T4.7.3.m3.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S2.T4.7.3.m3.1c"><ci id="S2.T4.7.3.m3.1.1.cmml" xref="S2.T4.7.3.m3.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.7.3.m3.1d">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S2.T4.7.3.m3.1e">caligraphic_L</annotation></semantics></math> was calculated using Eq.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.E2" title="In II-A2 Loss Function â€£ II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">2</span></a> with the average distance <math alttext="d=3" class="ltx_Math" display="inline" id="S2.T4.8.4.m4.1"><semantics id="S2.T4.8.4.m4.1b"><mrow id="S2.T4.8.4.m4.1.1" xref="S2.T4.8.4.m4.1.1.cmml"><mi id="S2.T4.8.4.m4.1.1.2" xref="S2.T4.8.4.m4.1.1.2.cmml">d</mi><mo id="S2.T4.8.4.m4.1.1.1" xref="S2.T4.8.4.m4.1.1.1.cmml">=</mo><mn id="S2.T4.8.4.m4.1.1.3" xref="S2.T4.8.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T4.8.4.m4.1c"><apply id="S2.T4.8.4.m4.1.1.cmml" xref="S2.T4.8.4.m4.1.1"><eq id="S2.T4.8.4.m4.1.1.1.cmml" xref="S2.T4.8.4.m4.1.1.1"></eq><ci id="S2.T4.8.4.m4.1.1.2.cmml" xref="S2.T4.8.4.m4.1.1.2">ğ‘‘</ci><cn id="S2.T4.8.4.m4.1.1.3.cmml" type="integer" xref="S2.T4.8.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.8.4.m4.1d">d=3</annotation><annotation encoding="application/x-llamapun" id="S2.T4.8.4.m4.1e">italic_d = 3</annotation></semantics></math>Â m. The best performance for each metric is highlighted in bold.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T4.11" style="width:433.6pt;height:199pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(59.9pt,-27.5pt) scale(1.38160839315192,1.38160839315192) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T4.11.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T4.11.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T4.11.3.4.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T4.11.3.4.1.1.1">Training Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T4.11.3.4.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T4.11.3.4.1.2.1">EKF</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T4.11.3.4.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T4.11.3.4.1.3.1">Color Jittering</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S2.T4.11.3.4.1.4"><span class="ltx_text ltx_font_bold" id="S2.T4.11.3.4.1.4.1">Performance Metrics</span></td>
</tr>
<tr class="ltx_tr" id="S2.T4.11.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.9.1.1.1">
<math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S2.T4.9.1.1.1.m1.1"><semantics id="S2.T4.9.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.T4.9.1.1.1.m1.1.1" xref="S2.T4.9.1.1.1.m1.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S2.T4.9.1.1.1.m1.1b"><ci id="S2.T4.9.1.1.1.m1.1.1.cmml" xref="S2.T4.9.1.1.1.m1.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.9.1.1.1.m1.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S2.T4.9.1.1.1.m1.1d">caligraphic_L</annotation></semantics></math> (m)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.10.2.2.2">
<math alttext="\mathcal{L}_{\mathbf{p}}" class="ltx_Math" display="inline" id="S2.T4.10.2.2.2.m1.1"><semantics id="S2.T4.10.2.2.2.m1.1a"><msub id="S2.T4.10.2.2.2.m1.1.1" xref="S2.T4.10.2.2.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T4.10.2.2.2.m1.1.1.2" xref="S2.T4.10.2.2.2.m1.1.1.2.cmml">â„’</mi><mi id="S2.T4.10.2.2.2.m1.1.1.3" xref="S2.T4.10.2.2.2.m1.1.1.3.cmml">ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T4.10.2.2.2.m1.1b"><apply id="S2.T4.10.2.2.2.m1.1.1.cmml" xref="S2.T4.10.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T4.10.2.2.2.m1.1.1.1.cmml" xref="S2.T4.10.2.2.2.m1.1.1">subscript</csymbol><ci id="S2.T4.10.2.2.2.m1.1.1.2.cmml" xref="S2.T4.10.2.2.2.m1.1.1.2">â„’</ci><ci id="S2.T4.10.2.2.2.m1.1.1.3.cmml" xref="S2.T4.10.2.2.2.m1.1.1.3">ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.10.2.2.2.m1.1c">\mathcal{L}_{\mathbf{p}}</annotation><annotation encoding="application/x-llamapun" id="S2.T4.10.2.2.2.m1.1d">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT</annotation></semantics></math> (m)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.11.3.3.3">
<math alttext="\mathcal{L_{\theta}}" class="ltx_Math" display="inline" id="S2.T4.11.3.3.3.m1.1"><semantics id="S2.T4.11.3.3.3.m1.1a"><msub id="S2.T4.11.3.3.3.m1.1.1" xref="S2.T4.11.3.3.3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T4.11.3.3.3.m1.1.1.2" xref="S2.T4.11.3.3.3.m1.1.1.2.cmml">â„’</mi><mi id="S2.T4.11.3.3.3.m1.1.1.3" xref="S2.T4.11.3.3.3.m1.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T4.11.3.3.3.m1.1b"><apply id="S2.T4.11.3.3.3.m1.1.1.cmml" xref="S2.T4.11.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S2.T4.11.3.3.3.m1.1.1.1.cmml" xref="S2.T4.11.3.3.3.m1.1.1">subscript</csymbol><ci id="S2.T4.11.3.3.3.m1.1.1.2.cmml" xref="S2.T4.11.3.3.3.m1.1.1.2">â„’</ci><ci id="S2.T4.11.3.3.3.m1.1.1.3.cmml" xref="S2.T4.11.3.3.3.m1.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.11.3.3.3.m1.1c">\mathcal{L_{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S2.T4.11.3.3.3.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> (Â°)</td>
</tr>
<tr class="ltx_tr" id="S2.T4.11.3.5.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.11.3.5.2.1">D1</td>
<td class="ltx_td ltx_border_t" id="S2.T4.11.3.5.2.2"></td>
<td class="ltx_td ltx_border_t" id="S2.T4.11.3.5.2.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.11.3.5.2.4">1.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.11.3.5.2.5">1.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.11.3.5.2.6">2.09</td>
</tr>
<tr class="ltx_tr" id="S2.T4.11.3.6.3">
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.6.3.1">D1+D2</td>
<td class="ltx_td" id="S2.T4.11.3.6.3.2"></td>
<td class="ltx_td" id="S2.T4.11.3.6.3.3"></td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.6.3.4">0.75</td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.6.3.5">0.58</td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.6.3.6">3.20</td>
</tr>
<tr class="ltx_tr" id="S2.T4.11.3.7.4">
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.7.4.1">D1+D2</td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.7.4.2">âœ“</td>
<td class="ltx_td" id="S2.T4.11.3.7.4.3"></td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.7.4.4">0.47</td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.7.4.5">0.47</td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.7.4.6">0.00</td>
</tr>
<tr class="ltx_tr" id="S2.T4.11.3.8.5">
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.8.5.1">D1+D2+D4</td>
<td class="ltx_td" id="S2.T4.11.3.8.5.2"></td>
<td class="ltx_td" id="S2.T4.11.3.8.5.3"></td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.8.5.4">0.52</td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.8.5.5">0.40</td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.8.5.6">2.28</td>
</tr>
<tr class="ltx_tr" id="S2.T4.11.3.9.6">
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.9.6.1">D1+D2+D4</td>
<td class="ltx_td" id="S2.T4.11.3.9.6.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.9.6.3">âœ“</td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.9.6.4">0.20</td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.9.6.5">0.15</td>
<td class="ltx_td ltx_align_center" id="S2.T4.11.3.9.6.6">0.93</td>
</tr>
<tr class="ltx_tr" id="S2.T4.11.3.10.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.11.3.10.7.1">D1+D2+D4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.11.3.10.7.2">âœ“</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.11.3.10.7.3">âœ“</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.11.3.10.7.4"><span class="ltx_text ltx_font_bold" id="S2.T4.11.3.10.7.4.1">0.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.11.3.10.7.5"><span class="ltx_text ltx_font_bold" id="S2.T4.11.3.10.7.5.1">0.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.11.3.10.7.6"><span class="ltx_text ltx_font_bold" id="S2.T4.11.3.10.7.6.1">0.00</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S2.F6.6"><span class="ltx_text ltx_font_bold ltx_inline-block" id="S2.F6.6.1" style="width:104.1pt;">Camera image</span> <span class="ltx_text ltx_font_bold ltx_inline-block" id="S2.F6.6.2" style="width:312.2pt;">Rendered images</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S2.F6.4"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="S2.F6.4.4" style="border-color: #000000;">
</span>
<span class="ltx_inline-logical-block ltx_minipage ltx_align_top ltx_framed ltx_framed_rectangle" id="S2.F6.4.3.3" style="width:303.5pt;">
<span class="ltx_figure" id="S2.F6.2.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S2.F6.2.1.1.1.g1" src="extracted/5751340/Data/figures/tcoms_rendered_image1.png" width="598"/>
</span>
<span class="ltx_figure" id="S2.F6.3.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S2.F6.3.2.2.2.g1" src="extracted/5751340/Data/figures/tcoms_rendered_image2.png" width="598"/>
</span>
<span class="ltx_figure" id="S2.F6.4.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S2.F6.4.3.3.3.g1" src="extracted/5751340/Data/figures/tcoms_rendered_image3.png" width="598"/>
</span></span>
</p>
</div>
<div class="ltx_flex_break"></div>
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S2.F6.1.g1" src="extracted/5751340/Data/figures/tcoms_cam_image.jpg" width="598"/>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F6.7.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S2.F6.8.2" style="font-size:90%;">Camera images and NVS rendered images in the controlled experiment in TCOMS. The rendered images produce photorealistic views of the structure but exhibit discrepancies in brightness. Some of the rendered views have artifacts in the background as shown in the right-most image.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F7">
<p class="ltx_p ltx_align_center" id="S2.F7.1"><span class="ltx_text" id="S2.F7.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="278" id="S2.F7.1.1.g1" src="extracted/5751340/Data/figures/tcoms_3dtraj.png" width="299"/></span></p>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F7.3.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S2.F7.4.2" style="font-size:90%;">3D trajectory comparison of ground truth versus model estimations with and without NVS augmentation in controlled environment. The model trained with NVS augmentation closely follows the ground truth trajectory, indicating improved positional accuracy over the model trained without NVS augmentation.</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Augmented Training with Novel View Synthesis</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The previous section demonstrated the importance of diverse training data with good coverage of the surveyed location. Although it may sometimes be possible to collect such data by extensively covering areas during the baseline mapping run, the practical constraints of cost and labor often limit this approach or render it infeasible. We explore alternative approaches to improve model performance in such data-limited scenarios. We propose to use NVS techniques to create models of the 3D scene, and then use these to generate more images from new aspects to augment the training data. In this section, we present the methods of augmenting training data using NVS models and the results of this approach.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Methods</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We first select 540 images from D1 and D2 to train an NVS model for the TCOMS scene. For this, we employ <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">COLMAP</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib26" title="">26</a>]</cite>, an open-source Structure-from-Motion computation software, to compute the camera pose associated with each image within an arbitrary reference coordinate.
We use <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">nerfstudio</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib27" title="">27</a>]</cite> for training the NVS model and rendering images, using the <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">nerfacto</span> approach. To enhance the modelâ€™s robustness to dynamic elements in the scene such as lighting changes, we employ the robust loss function proposed by Sabour et alÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib28" title="">28</a>]</cite>. The details of training the model are presented in our previous workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib29" title="">29</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To render images for new poses that were not sampled during the ROV run, we first synthesize these new camera poses based on the existing poses in the training dataset. This is done by varying the depth and distance to the structure in the existing poses, keeping the orientation the same to ensure the camera is pointing towards the structure in the newly generated poses. In total, we generate 4193 images, and we refer to this dataset as D4. We then use D1, D2, and D4 for training, and D3 for validation and testing to test the improvement provided by using the NVS-based augmentation.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Additionally, it is noted that the images in D4 exhibited different brightness levels and background noise as compared to the original data, introduced during the NVS model reconstruction. To address the potential degradation due to this, we further augment the data by jittering the color of each image during training, thus making the pose estimator robust to minute color and lighting changes. For evaluation, we use the same GPU, framework, and hyperparameter tuning methods as described in the previous section.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Results &amp; Discussion</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our results show that utilizing augmented training data generated by a NVS model leads to a significant enhancement in localization accuracy. Comparing row 2 and row 4 in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.T4" title="TABLE IV â€£ II-C2 Generalization performance â€£ II-C Results &amp; Discussions â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">IV</span></a>, we find that by augmenting the training data with D4, the overall localization error can be reduced by 30%.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Color jittering augmentation is also highly effective in further improving the model performance, further reducing the error by an additional 61.5%. We compare the performance of the augmented training with color jittering with the performance without augmented training in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.F7" title="Figure 7 â€£ II-C2 Generalization performance â€£ II-C Results &amp; Discussions â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">7</span></a> and Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S3.F8" title="Figure 8 â€£ III-B Results &amp; Discussion â€£ III Augmented Training with Novel View Synthesis â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">8</span></a>. These plots show that the proposed augmented training with NVS significantly improves the pose estimatorâ€™s accuracy and reliability in terms of both position and orientation.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Nonetheless, we observed the presence of outliers. Upon examining the data, we found that these outliers were caused by transient objects, such as the tether shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S3.F9" title="Figure 9 â€£ III-B Results &amp; Discussion â€£ III Augmented Training with Novel View Synthesis â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">9</span></a>(b), which were not present in the training data.</p>
</div>
<figure class="ltx_figure" id="S3.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="299" id="S3.F8.sf1.g1" src="extracted/5751340/Data/figures/tcoms_angular_error_cdf.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="299" id="S3.F8.sf2.g1" src="extracted/5751340/Data/figures/tcoms_position_error_cdf.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S3.F8.3.2" style="font-size:90%;">Cumulative distribution function (CDF) of (a) orientation and (b) position errors for models trained with and without NVS augmentation in controlled environment. The plots show that augmented training with NVS yields significantly lower errors for both orientation and position compared to training without augmentation.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S3.F9.sf1.g1" src="extracted/5751340/Data/figures/tcoms_good_pose2.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F9.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S3.F9.sf2.g1" src="extracted/5751340/Data/figures/tcoms_bad_pose1.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F9.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S3.F9.3.2" style="font-size:90%;">Test images from Clear Water-Mid with amongst the best and worst pose estimation accuracy. Panel (a) is the image with one of the best pose estimation accuracy and panel (b) is the image with one of the worst pose estimation accuracy.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Localization enhancement via sensor data fusion</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">While the trained pose estimators yield small median orientation and position errors, their estimates exhibit some volatility. Our model currently treats each sample independently, ignoring temporal context, and utilizes only the camera inputs during deployment. However, additional information, such as temporal information and other sensor inputs from the ROV, is available. To enhance localization accuracy and achieve a more stable trajectory estimation, we propose sensor fusion using an EKF. This section details the integration of the pose estimator with additional sensor data and presents the results of the sensor fusion.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Methods</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Given the sequential nature of data in reinspection missions and the availability of additional sensors, incorporating temporal information and other sensor data presents a viable strategy for improving the modelâ€™s estimation stability and accuracy. Currently, the visual localization model without sensor fusion occasionally results in estimation of poses that are physically implausible or outliers, in context of the dynamics from previous poses. By integrating knowledge of the ROVâ€™s physics model and leveraging previous pose estimates, we can enhance pose accuracy and stability.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Furthermore, during reinspection missions, ROVs are commonly equipped with altimeters and compasses, which have a reasonable accuracy. As such, we could use these reliable depth and orientation measurements during reinspection to further improve the overall localization accuracy.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.5.1.1">IV-A</span>1 </span>Model</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">We assume that the vehicle moves with a constant translational velocity and constant angular velocity since the vehicle normally moves slowly during inspection missions.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">We consider the pose estimator outputs, compass data and altimeter data as measurements. The compass yields orientation measurements in the form of a quaternion. The altimeter provides z-coordinate measurements. The pose estimator outputs comprise (1) position in x, y and z coordinates, and (2) orientation in the form of a quaternion. We use an EKF with this model to integrate measurements from the model and these sensor measurements.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.5.1.1">IV-A</span>2 </span>Measurement noises</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">The measurement noise is the other hyperparameter that needs to be carefully selected in the EKF. Nominal values for the measurement noise standard deviations for the compass and altimeter are available from the specifications provided by their manufacturers, and can be set accordingly. However, the noise associated with the pose estimator presents a more complex challenge. Setting a static value for the pose estimatorâ€™s measurement noiseâ€”such as the standard deviation of localization error derived from validation performanceâ€”is inadequate. This is due to the inconsistent nature of the networkâ€™s estimations, which can sometimes exhibit substantial errors. To more accurately represent the dynamic noise in pose estimator, we employ <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.1.1">dropout</span> techniques at test time for Monte Carlo sampling from the modelâ€™s posterior distribution.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.1.1">Dropout</span> is a technique commonly used as a regularizer in training neural networks to prevent overfitting. Recent works have shown that using dropout during inference can be used to approximate Bayesian inference over the distribution of the networkâ€™s weights at test time, without requiring any additional model parametersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib30" title="">30</a>]</cite>. We sample 100 Monte Carlo dropout realizations for the inference of each image input sample, and use the standard deviation of these dropout samples as the measurement noise for the inference sample.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Results &amp; Discussion</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We tune the process noise standard deviation as a hyperparameter to get the best performance. As shown in Table.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.T4" title="TABLE IV â€£ II-C2 Generalization performance â€£ II-C Results &amp; Discussions â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">IV</span></a>, sensor fusion with the EKF yields more accurate estimated poses. We also find that the predicted trajectory using the EKF is smoother compared to that without using the EKF, as illustrated in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S4.F10" title="Figure 10 â€£ IV-B Results &amp; Discussion â€£ IV Localization enhancement via sensor data fusion â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">10</span></a>.
However, the inference time using the EKF is about ten times as long.</p>
</div>
<figure class="ltx_figure" id="S4.F10">
<p class="ltx_p ltx_align_center" id="S4.F10.1"><span class="ltx_text" id="S4.F10.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="277" id="S4.F10.1.1.g1" src="extracted/5751340/Data/figures/tcoms_3dtraj_nvs_ekf.png" width="299"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.3.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S4.F10.4.2" style="font-size:90%;">3D trajectory comparison of ground truth versus model estimates with and without EKF. The EKF output closely follows the ground truth trajectory, indicating improved positional accuracy over the pose estimator output.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Field trials at sea</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To further validate our proposed methods, we conducted field trials in a bay near St. Johnâ€™s Island, Singapore (SJI). In this section, we present the methods, results and challenges encountered in using our proposed methods from the previous section in a real-world setting.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Methods</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We used the ROV to collect data in an at-sea environment, inspecting a submerged pillar. The pillar selected was approximately 5Â m tall and 0.5Â m in diameter. Although the pillar was a simple black metallic structure, the barnacles and algae growing on its surface provided visual features that could be used for pose estimation. We drove the ROV following a vertical lawnmower path around the pillar, while recording the video from the camera. Due to the high turbidity in the water, we operated the ROV in close proximity to the structure with the average distance being 1Â m.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We collected two datasets, named as D5 and D6, on two different days. Samples of images collected in these datasets are shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S5.F11" title="Figure 11 â€£ V-A Methods â€£ V Field trials at sea â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F11.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S5.F11.sf1.g1" src="extracted/5751340/Data/figures/sji_train_image2.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F11.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F11.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S5.F11.sf2.g1" src="extracted/5751340/Data/figures/sji_train_image1.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F11.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F11.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S5.F11.sf3.g1" src="extracted/5751340/Data/figures/sji_test_image1.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F11.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F11.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S5.F11.sf4.g1" src="extracted/5751340/Data/figures/sji_test_image2.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F11.sf4.2.1.1" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S5.F11.3.2" style="font-size:90%;">Sample images from Sea Water-1 and Sea Water-2 dataset. Panels (a) and (b) are from Sea Water-1 dataset, and (c) and (d) are from Sea Water-2 dataset. The images from Sea Water-2 dataset show higher turbidity and thus fewer features than images from Sea Water-1.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S5.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F12.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S5.F12.sf1.g1" src="extracted/5751340/Data/figures/sji_good_pose1.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F12.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F12.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S5.F12.sf2.g1" src="extracted/5751340/Data/figures/sji_bad_pose1.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F12.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S5.F12.3.2" style="font-size:90%;">Test images from Sea Water-2 with amongst the best and worst pose estimation accuracy. Panel (a) is the image with one of the best pose estimation accuracy and (b) is the image with one of the worst pose estimation accuracy.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">We use D5 to train an NVS model following the method described in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2" title="II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection">II</a>. New camera poses are generated using the same approach. The NVS model is then utilized to create an augmented training dataset, named D7. Samples of images generated at new poses using the NVS model are shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S5.F13" title="Figure 13 â€£ V-A Methods â€£ V Field trials at sea â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">We train the best visual localization architecture configuration, C6, both with augmented training data (datasets D5+D7) and without any augmentation (only D5). Dataset D6 is used for validation and testing. The training methods are similar to those described in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2" title="II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection">II</a>.</p>
</div>
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F13.6"><span class="ltx_text ltx_font_bold ltx_inline-block" id="S5.F13.6.1" style="width:104.1pt;">Camera Image</span> <span class="ltx_text ltx_font_bold ltx_inline-block" id="S5.F13.6.2" style="width:312.2pt;">Rendered Images</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F13.4"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="S5.F13.4.4" style="border-color: #000000;">
</span>
<span class="ltx_inline-logical-block ltx_minipage ltx_align_top ltx_framed ltx_framed_rectangle" id="S5.F13.4.3.3" style="width:303.5pt;">
<span class="ltx_figure" id="S5.F13.2.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S5.F13.2.1.1.1.g1" src="extracted/5751340/Data/figures/sji_rendered_image1.jpg" width="598"/>
</span>
<span class="ltx_figure" id="S5.F13.3.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S5.F13.3.2.2.2.g1" src="extracted/5751340/Data/figures/sji_rendered_image3.png" width="598"/>
</span>
<span class="ltx_figure" id="S5.F13.4.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S5.F13.4.3.3.3.g1" src="extracted/5751340/Data/figures/sji_rendered_image4.png" width="598"/>
</span></span>
</p>
</div>
<div class="ltx_flex_break"></div>
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S5.F13.1.g1" src="extracted/5751340/Data/figures/sji_cam_image4.jpg" width="598"/>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F13.7.1.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text" id="S5.F13.8.2" style="font-size:90%;">Camera image and NVS rendered images in the bay near SJI. The rendered images produce photorealistic views of the structure but exhibit some artifacts and noise depending on the camera pose.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Results &amp; Discussion</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">As shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S5.F14" title="Figure 14 â€£ V-B Results &amp; Discussion â€£ V Field trials at sea â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">14</span></a>, augmented training with NVS yields significant improvement in both position and orientation accuracy compared to training without NVS augmentation. With configuration C6 and augmented training, we are able to achieve a position accuracy of 0.17Â m and orientation accuracy of 5.09Â°. We present the performance of C6 on D6 in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S5.T5" title="TABLE V â€£ V-B Results &amp; Discussion â€£ V Field trials at sea â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">V</span></a>. While the median accuracy is comparable to the performance in the controlled environment, we note that the standard deviation in the errors are much larger at sea.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Clearly, the real-world setting at sea presents several challenges that are not present in controlled environments. The biggest challenge is the turbidity of the water, which significantly affects the quality of the images. Moreover, lighting is inconsistent at different camera poses and on different days, causing high variablity in the image quality. This introduced three new challenges. First, the noisy images make it challenging to compute camera poses in <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.1">COLMAP</span>, resulting in a sparse number of registered images. Consequently, the EKF model could not be used for performance improvement since it would not be feasible to assume constant velocity and angular velocity in the vehicle model. Second, the turbidity and inconsistent lighting in the training data introduced artifacts in the NVS model. Thus, the rendered images are more noisy compared to images in clear waters, as shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S5.F13" title="Figure 13 â€£ V-A Methods â€£ V Field trials at sea â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">13</span></a>. Third, the high variability in image quality can lead to more estimation outliers and large errors during inference. All of these contribute to a decrease in the modelâ€™s performance. Nevertheless, we note that the NVS model is still able to produce photorealistic views of the structure.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.13.5.1" style="font-size:90%;">TABLE V</span>: </span><span class="ltx_text" id="S5.T5.8.4" style="font-size:90%;">Performance of configuration C6 on dataset D6. <math alttext="\mathcal{L}_{\mathbf{p}}" class="ltx_Math" display="inline" id="S5.T5.5.1.m1.1"><semantics id="S5.T5.5.1.m1.1b"><msub id="S5.T5.5.1.m1.1.1" xref="S5.T5.5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.5.1.m1.1.1.2" xref="S5.T5.5.1.m1.1.1.2.cmml">â„’</mi><mi id="S5.T5.5.1.m1.1.1.3" xref="S5.T5.5.1.m1.1.1.3.cmml">ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T5.5.1.m1.1c"><apply id="S5.T5.5.1.m1.1.1.cmml" xref="S5.T5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.5.1.m1.1.1.1.cmml" xref="S5.T5.5.1.m1.1.1">subscript</csymbol><ci id="S5.T5.5.1.m1.1.1.2.cmml" xref="S5.T5.5.1.m1.1.1.2">â„’</ci><ci id="S5.T5.5.1.m1.1.1.3.cmml" xref="S5.T5.5.1.m1.1.1.3">ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.1.m1.1d">\mathcal{L}_{\mathbf{p}}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.5.1.m1.1e">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S5.T5.6.2.m2.1"><semantics id="S5.T5.6.2.m2.1b"><msub id="S5.T5.6.2.m2.1.1" xref="S5.T5.6.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.6.2.m2.1.1.2" xref="S5.T5.6.2.m2.1.1.2.cmml">â„’</mi><mi id="S5.T5.6.2.m2.1.1.3" xref="S5.T5.6.2.m2.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T5.6.2.m2.1c"><apply id="S5.T5.6.2.m2.1.1.cmml" xref="S5.T5.6.2.m2.1.1"><csymbol cd="ambiguous" id="S5.T5.6.2.m2.1.1.1.cmml" xref="S5.T5.6.2.m2.1.1">subscript</csymbol><ci id="S5.T5.6.2.m2.1.1.2.cmml" xref="S5.T5.6.2.m2.1.1.2">â„’</ci><ci id="S5.T5.6.2.m2.1.1.3.cmml" xref="S5.T5.6.2.m2.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.2.m2.1d">\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.6.2.m2.1e">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> are median values across the test data. <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S5.T5.7.3.m3.1"><semantics id="S5.T5.7.3.m3.1b"><mi class="ltx_font_mathcaligraphic" id="S5.T5.7.3.m3.1.1" xref="S5.T5.7.3.m3.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S5.T5.7.3.m3.1c"><ci id="S5.T5.7.3.m3.1.1.cmml" xref="S5.T5.7.3.m3.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.3.m3.1d">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.7.3.m3.1e">caligraphic_L</annotation></semantics></math> was calculated using Eq.Â <a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#S2.E2" title="In II-A2 Loss Function â€£ II-A Methods â€£ II Pose Estimation â€£ Pose Estimation from Camera Images for Underwater Inspection"><span class="ltx_text ltx_ref_tag">2</span></a> with the average distance <math alttext="d=1" class="ltx_Math" display="inline" id="S5.T5.8.4.m4.1"><semantics id="S5.T5.8.4.m4.1b"><mrow id="S5.T5.8.4.m4.1.1" xref="S5.T5.8.4.m4.1.1.cmml"><mi id="S5.T5.8.4.m4.1.1.2" xref="S5.T5.8.4.m4.1.1.2.cmml">d</mi><mo id="S5.T5.8.4.m4.1.1.1" xref="S5.T5.8.4.m4.1.1.1.cmml">=</mo><mn id="S5.T5.8.4.m4.1.1.3" xref="S5.T5.8.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.8.4.m4.1c"><apply id="S5.T5.8.4.m4.1.1.cmml" xref="S5.T5.8.4.m4.1.1"><eq id="S5.T5.8.4.m4.1.1.1.cmml" xref="S5.T5.8.4.m4.1.1.1"></eq><ci id="S5.T5.8.4.m4.1.1.2.cmml" xref="S5.T5.8.4.m4.1.1.2">ğ‘‘</ci><cn id="S5.T5.8.4.m4.1.1.3.cmml" type="integer" xref="S5.T5.8.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.4.m4.1d">d=1</annotation><annotation encoding="application/x-llamapun" id="S5.T5.8.4.m4.1e">italic_d = 1</annotation></semantics></math>Â m.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.11" style="width:433.6pt;height:111.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(76.4pt,-19.6pt) scale(1.54455848932886,1.54455848932886) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.11.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.11.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.11.3.4.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T5.11.3.4.1.1.1">Training Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.11.3.4.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T5.11.3.4.1.2.1">Color Jittering</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T5.11.3.4.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.11.3.4.1.3.1">Performance Metrics</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.11.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.9.1.1.1">
<math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S5.T5.9.1.1.1.m1.1"><semantics id="S5.T5.9.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T5.9.1.1.1.m1.1.1" xref="S5.T5.9.1.1.1.m1.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S5.T5.9.1.1.1.m1.1b"><ci id="S5.T5.9.1.1.1.m1.1.1.cmml" xref="S5.T5.9.1.1.1.m1.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.1.1.1.m1.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.9.1.1.1.m1.1d">caligraphic_L</annotation></semantics></math> (m)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.10.2.2.2">
<math alttext="\mathcal{L}_{\mathbf{p}}" class="ltx_Math" display="inline" id="S5.T5.10.2.2.2.m1.1"><semantics id="S5.T5.10.2.2.2.m1.1a"><msub id="S5.T5.10.2.2.2.m1.1.1" xref="S5.T5.10.2.2.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.10.2.2.2.m1.1.1.2" xref="S5.T5.10.2.2.2.m1.1.1.2.cmml">â„’</mi><mi id="S5.T5.10.2.2.2.m1.1.1.3" xref="S5.T5.10.2.2.2.m1.1.1.3.cmml">ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T5.10.2.2.2.m1.1b"><apply id="S5.T5.10.2.2.2.m1.1.1.cmml" xref="S5.T5.10.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.10.2.2.2.m1.1.1.1.cmml" xref="S5.T5.10.2.2.2.m1.1.1">subscript</csymbol><ci id="S5.T5.10.2.2.2.m1.1.1.2.cmml" xref="S5.T5.10.2.2.2.m1.1.1.2">â„’</ci><ci id="S5.T5.10.2.2.2.m1.1.1.3.cmml" xref="S5.T5.10.2.2.2.m1.1.1.3">ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.10.2.2.2.m1.1c">\mathcal{L}_{\mathbf{p}}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.10.2.2.2.m1.1d">caligraphic_L start_POSTSUBSCRIPT bold_p end_POSTSUBSCRIPT</annotation></semantics></math> (m)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.11.3.3.3">
<math alttext="\mathcal{L_{\theta}}" class="ltx_Math" display="inline" id="S5.T5.11.3.3.3.m1.1"><semantics id="S5.T5.11.3.3.3.m1.1a"><msub id="S5.T5.11.3.3.3.m1.1.1" xref="S5.T5.11.3.3.3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.11.3.3.3.m1.1.1.2" xref="S5.T5.11.3.3.3.m1.1.1.2.cmml">â„’</mi><mi id="S5.T5.11.3.3.3.m1.1.1.3" xref="S5.T5.11.3.3.3.m1.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T5.11.3.3.3.m1.1b"><apply id="S5.T5.11.3.3.3.m1.1.1.cmml" xref="S5.T5.11.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.11.3.3.3.m1.1.1.1.cmml" xref="S5.T5.11.3.3.3.m1.1.1">subscript</csymbol><ci id="S5.T5.11.3.3.3.m1.1.1.2.cmml" xref="S5.T5.11.3.3.3.m1.1.1.2">â„’</ci><ci id="S5.T5.11.3.3.3.m1.1.1.3.cmml" xref="S5.T5.11.3.3.3.m1.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.11.3.3.3.m1.1c">\mathcal{L_{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.11.3.3.3.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> (Â°)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.11.3.5.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.11.3.5.2.1">D5</td>
<td class="ltx_td ltx_border_t" id="S5.T5.11.3.5.2.2"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.11.3.5.2.3">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.11.3.5.2.4">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.11.3.5.2.5">12.15</td>
</tr>
<tr class="ltx_tr" id="S5.T5.11.3.6.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.11.3.6.3.1">D5+D7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.11.3.6.3.2">âœ“</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.11.3.6.3.3"><span class="ltx_text ltx_font_bold" id="S5.T5.11.3.6.3.3.1">0.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.11.3.6.3.4"><span class="ltx_text ltx_font_bold" id="S5.T5.11.3.6.3.4.1">0.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.11.3.6.3.5"><span class="ltx_text ltx_font_bold" id="S5.T5.11.3.6.3.5.1">5.09</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S5.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F14.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="299" id="S5.F14.sf1.g1" src="extracted/5751340/Data/figures/sji_angular_error_cdf.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F14.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F14.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="299" id="S5.F14.sf2.g1" src="extracted/5751340/Data/figures/sji_position_error_cdf.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F14.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F14.2.1.1" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text" id="S5.F14.3.2" style="font-size:90%;">CDF of (a) orientation and (b) position errors for models trained with and without NVS augmentation in at-sea environment. The plots show that augmented training with NVS yields significantly lower errors for both orientation and position compared to training without augmentation.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.2">In this paper, we addressed the challenge of localization in underwater inspection missions with a neural-network based pose estimator. We proposed a new loss function to train the pose estimator, and demonstrated that training with <math alttext="d" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mi id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><ci id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">italic_d</annotation></semantics></math>-loss significantly improved the modelâ€™s performance in pose estimation tasks. This improvement is attributed to the incorporation of domain-specific physics, as the <math alttext="d" class="ltx_Math" display="inline" id="S6.p1.2.m2.1"><semantics id="S6.p1.2.m2.1a"><mi id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><ci id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S6.p1.2.m2.1d">italic_d</annotation></semantics></math>-loss accounts for the relevant geometric considerations in the inspection mission. Furthermore, this loss function also lends more interpretability to the loss. Employing the ResNet50 backbone with a CNN+LSTM architecture allows us to efficiently use the available visual information to estimate the pose, and yielded improvements in the localization performance as compared to benchmark architectures.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In terms of the generalization, using more diverse data with a wider distribution significantly enhances the localization performance on test data that lies outside the training distribution. We additionally investigated the use of NVS techniques to augment training data and showed that this significantly improves the estimatorâ€™s performance with previously unsurveyed poses. Thus, this provides a cost-effective and information-efficient method to improve the generalization performance without having to undertake expensive field trials to collect additional data. Further integrating the pose estimator with an EKF allows us to fuse sensor data with the visual-based estimates, and we demonstrated that this further improved the performance and stability. We validated our proposed methods in both controlled environments in a clear water tank and real-world settings at sea.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Overall, our results show that our proposed methods significantly improve the visual localization performance in both controlled underwater environments and real-world settings and achieve good localization accuracy to within desired limits, providing a cost-effective alternative or complement to existing localization solutions. Real-world challenges such as turbidity and noise limit the performance achievable, but the proposed method still performs reasonably, especially when data augmentation using color-based augmentation is used to robustify the technique against color distortion.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">Potential improvements to this technique may include utilizing temporal information (i.e., more than one image at a time) to improve the accuracy of pose estimates, fusing more data such as control input information and sonar data, exploring better sensor fusion techniques such as particle filters and using per-pixel loss together with NVS rendered images to fine-tune the model.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">The algorithm developed in this work is also utilized as part of a model-based image compression technique for low bandwidth scenarios. The details of this approach and the preliminary results are presented in our previous workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.16961v1#bib.bib31" title="">31</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This research project is supported by A*STAR under its RIE2020 Advanced Manufacturing and Engineering (AME) Industry Alignment Fund - Pre-Positioning (IAF-PP) Grant No. A20H8a0241.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
E.Â Vargas, R.Â Scona, J.Â S. Willners, T.Â Luczynski, Y.Â Cao, S.Â Wang, and Y.Â R. Petillot, â€œRobust underwater visual SLAM fusing acoustic sensing,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2021 IEEE International Conference on Robotics and Automation (ICRA)</em>.Â Â Â IEEE, pp. 2140â€“2146. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/9561537/" title="">https://ieeexplore.ieee.org/document/9561537/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
B.Â Bingham, B.Â Foley, H.Â Singh, R.Â Camilli, K.Â Delaporta, R.Â Eustice, A.Â Mallios, D.Â Mindell, C.Â Roman, and D.Â Sakellariou, â€œRobotic tools for deep water archaeology: Surveying an ancient shipwreck with an autonomous underwater vehicle,â€ vol.Â 27, no.Â 6, pp. 702â€“717. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://onlinelibrary.wiley.com/doi/10.1002/rob.20350" title="">https://onlinelibrary.wiley.com/doi/10.1002/rob.20350</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M.Â Carreras, J.Â D. Hernandez, E.Â Vidal, N.Â Palomeras, and P.Â Ridao, â€œOnline motion planning for underwater inspection,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2016 IEEE/OES Autonomous Underwater Vehicles (AUV)</em>.Â Â Â IEEE, pp. 336â€“341. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ieeexplore.ieee.org/document/7778693/" title="">http://ieeexplore.ieee.org/document/7778693/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H.-P. Tan, R.Â Diamant, W.Â K.Â G. Seah, and M.Â Waldmeyer, â€œA survey of techniques and challenges in underwater localization,â€ vol.Â 38, no.Â 14, pp. 1663â€“1676. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0029801811001624" title="">https://www.sciencedirect.com/science/article/pii/S0029801811001624</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S.Â Zhang, S.Â Zhao, D.Â An, J.Â Liu, H.Â Wang, Y.Â Feng, D.Â Li, and R.Â Zhao, â€œVisual SLAM for underwater vehicles: A survey,â€ vol.Â 46, p. 100510. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1574013722000442" title="">https://www.sciencedirect.com/science/article/pii/S1574013722000442</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S.Â Kuutti, S.Â Fallah, K.Â Katsaros, M.Â Dianati, F.Â Mccullough, and A.Â Mouzakitis, â€œA survey of the state-of-the-art localization techniques and their potentials for autonomous vehicle applications,â€ vol.Â 5, no.Â 2, pp. 829â€“846. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/8306879/" title="">https://ieeexplore.ieee.org/document/8306879/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A.Â D. Buchan, E.Â Solowjow, D.-A. Duecker, and E.Â Kreuzer, â€œLow-cost monocular localization with active markers for micro autonomous underwater vehicles,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>.Â Â Â IEEE, pp. 4181â€“4188. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ieeexplore.ieee.org/document/8206279/" title="">http://ieeexplore.ieee.org/document/8206279/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.Â GomezÂ Chavez, C.Â Mueller, T.Â Doernbach, D.Â Chiarella, and A.Â Birk, â€œRobust gesture-based communication for underwater human-robot interaction in the context of search and rescue diver missions.â€

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D.Â Chiarella, M.Â Bibuli, G.Â Bruzzone, M.Â Caccia, A.Â Ranieri, E.Â Zereik, L.Â Marconi, and P.Â Cutugno, â€œGesture-based language for diver-robot underwater interaction,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">OCEANS 2015 - Genova</em>, pp. 1â€“9. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/7271710" title="">https://ieeexplore.ieee.org/document/7271710</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B.Â Teixeira, H.Â Silva, A.Â Matos, and E.Â Silva, â€œDeep learning for underwater visual odometry estimation,â€ vol.Â 8, pp. 44â€‰687â€“44â€‰701. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/9024043/" title="">https://ieeexplore.ieee.org/document/9024043/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A.Â Kendall, M.Â Grimes, and R.Â Cipolla, â€œPoseNet: A convolutional network for real-time 6-DOF camera relocalization,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2015 IEEE International Conference on Computer Vision (ICCV)</em>, pp. 2938â€“2946, ISSN: 2380-7504.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
L.Â Peng, H.Â Vishnu, M.Â Chitre, Y.Â M. Too, B.Â Kalyan, and R.Â Mishra, â€œImproved image-based pose regressor models for underwater environments.â€ [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2403.08360" title="">http://arxiv.org/abs/2403.08360</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
T.Â Sattler, B.Â Leibe, and L.Â Kobbelt, â€œEfficient &amp; effective prioritized matching for large-scale image-based localization,â€ vol.Â 39, no.Â 9, pp. 1744â€“1756. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ieeexplore.ieee.org/document/7572201/" title="">http://ieeexplore.ieee.org/document/7572201/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y.Â Shavit and R.Â Ferens, â€œIntroduction to camera pose estimation with deep learning.â€

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M.Â C. Nielsen, M.Â H. Leonhardsen, and I.Â Schjolberg, â€œEvaluation of PoseNet for 6-DOF underwater pose estimation,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">OCEANS 2019 MTS/IEEE SEATTLE</em>.Â Â Â IEEE, pp. 1â€“6. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/8962814/" title="">https://ieeexplore.ieee.org/document/8962814/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
L.Â Peng and M.Â Chitre, â€œRegressing poses from monocular images in an underwater environment,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">OCEANS 2022 - Chennai</em>, pp. 1â€“4. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/abstract/document/9775281" title="">https://ieeexplore.ieee.org/abstract/document/9775281</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
B.Â Mildenhall, P.Â P. Srinivasan, M.Â Tancik, J.Â T. Barron, R.Â Ramamoorthi, and R.Â Ng, â€œNeRF: Representing scenes as neural radiance fields for view synthesis.â€ [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2003.08934" title="">http://arxiv.org/abs/2003.08934</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
B.Â Kerbl, G.Â Kopanas, T.Â Leimkuehler, and G.Â Drettakis, â€œ3d gaussian splatting for real-time radiance field rendering,â€ vol.Â 42, no.Â 4, pp. 1â€“14. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3592433" title="">https://dl.acm.org/doi/10.1145/3592433</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
â€œTCOMS Research &amp; Development.â€ [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tcoms.sg/research-development/" title="">https://www.tcoms.sg/research-development/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C.Â Szegedy, W.Â Liu, Y.Â Jia, P.Â Sermanet, S.Â Reed, D.Â Anguelov, D.Â Erhan, V.Â Vanhoucke, and A.Â Rabinovich, â€œGoing deeper with convolutions.â€ [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1409.4842" title="">http://arxiv.org/abs/1409.4842</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J.Â Deng, W.Â Dong, R.Â Socher, L.-J. Li, K.Â Li, and L.Â Fei-Fei, â€œImageNet: A Large-Scale Hierarchical Image Database,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">CVPR09</em>, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
F.Â Walch, C.Â Hazirbas, L.Â Leal-Taixe, T.Â Sattler, S.Â Hilsenbeck, and D.Â Cremers, â€œImage-based localization using LSTMs for structured feature correlation,â€ p.Â 11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun, â€œDeep residual learning for image recognition.â€ [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1512.03385" title="">http://arxiv.org/abs/1512.03385</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
E.Â Bernardes and S.Â Viollet, â€œQuaternion to Euler angles conversion: A direct, general and computationally efficient method,â€ vol.Â 17, no.Â 11, p. e0276302. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9648712/" title="">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9648712/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J.Â L. SchÃ¶nberger, E.Â Zheng, J.-M. Frahm, and M.Â Pollefeys, â€œPixelwise view selection for unstructured multi-view stereo,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Computer Vision â€“ ECCV 2016</em>, B.Â Leibe, J.Â Matas, N.Â Sebe, and M.Â Welling, Eds.Â Â Â Springer International Publishing, pp. 501â€“518.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J.Â L. Schonberger and J.-M. Frahm, â€œStructure-from-motion revisited,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.Â Â Â IEEE, pp. 4104â€“4113. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ieeexplore.ieee.org/document/7780814/" title="">http://ieeexplore.ieee.org/document/7780814/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M.Â Tancik, E.Â Weber, E.Â Ng, R.Â Li, B.Â Yi, J.Â Kerr, T.Â Wang, A.Â Kristoffersen, J.Â Austin, K.Â Salahi, A.Â Ahuja, D.Â McAllister, and A.Â Kanazawa, â€œNerfstudio: A modular framework for neural radiance field development,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">ACM SIGGRAPH 2023 Conference Proceedings</em>, ser. SIGGRAPH â€™23, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
S.Â Sabour, S.Â Vora, D.Â Duckworth, I.Â Krasin, D.Â J. Fleet, and A.Â Tagliasacchi, â€œRobustNeRF: Ignoring distractors with robust losses.â€ [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2302.00833" title="">http://arxiv.org/abs/2302.00833</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Y. M. Too, H. Vishnu, M. Chitre, B. Kalyan, L. Peng, and R. Mishra, â€œFeasibility study on novel view synthesis of underwater structures using neural radiance fields,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">OCEANS 2024 MTS/IEEE Singapore</em>.Â Â Â IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A.Â Kendall and R.Â Cipolla, â€œModelling uncertainty in deep learning for camera relocalization.â€ [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1509.05909" title="">http://arxiv.org/abs/1509.05909</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
R. Mishra, M. Chitre, B. Kalyan, Y. M. Too, H. Vishnu, and L. Peng, â€œAn architecture for virtual tethering of ROVs,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">OCEANS 2024 MTS/IEEE Singapore</em>.Â Â Â IEEE.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul 24 02:52:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
