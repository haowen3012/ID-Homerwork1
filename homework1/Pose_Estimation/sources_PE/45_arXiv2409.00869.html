<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Detection, Recognition and Pose Estimation of Tabletop Objects</title>
<!--Generated on Sun Sep  1 23:27:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.00869v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S1" title="In Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">INTRODUCTION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S2" title="In Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">LITERATURE REVIEW</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S2.SS1" title="In II LITERATURE REVIEW ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Using CNNs for Descriptor Learning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S2.SS2" title="In II LITERATURE REVIEW ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Using CNNs to Match Point Clouds to 3D Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S2.SS3" title="In II LITERATURE REVIEW ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Using CNNs with Posed Datasets</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S2.SS4" title="In II LITERATURE REVIEW ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Descriptor Regression using a Convolutional Autoencoder</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S2.SS5" title="In II LITERATURE REVIEW ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-E</span> </span><span class="ltx_text ltx_font_italic">Using Spatial Transformer Networks</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S2.SS6" title="In II LITERATURE REVIEW ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-F</span> </span><span class="ltx_text ltx_font_italic">Estimating pose with distance of object from camera</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S2.SS7" title="In II LITERATURE REVIEW ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-G</span> </span><span class="ltx_text ltx_font_italic">Deliberative Approach</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S3" title="In Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">PROBLEM DESCRIPTION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S4" title="In Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">DATASET</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S5" title="In Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">DATA PREPROCESSING</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S5.SS1" title="In V DATA PREPROCESSING ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Data Parsing</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S6" title="In Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">APPROACH</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S7" title="In Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">EXPERIMENTS AND RESULTS</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S7.SS1" title="In VII EXPERIMENTS AND RESULTS ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-A</span> </span><span class="ltx_text ltx_font_italic">Object Recognition</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S7.SS2" title="In VII EXPERIMENTS AND RESULTS ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-B</span> </span><span class="ltx_text ltx_font_italic">Angle Estimation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S8" title="In Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">CONCLUSION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S9" title="In Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IX </span><span class="ltx_text ltx_font_smallcaps">FUTURE WORK</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Detection, Recognition and Pose Estimation of Tabletop Objects
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sanjuksha Nirgude, Kevin DuCharme, &amp; Namrita Madhusoodanan
</span><span class="ltx_author_notes"></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The problem of cleaning a messy table using Deep Neural Networks is a very interesting problem in both social and industrial robotics. This project focuses on the social application of this technology. A neural network model that is capable of detecting and recognizing common tabletop objects, such as a mug, mouse, or stapler is developed. The model also predicts the angle at which these objects are placed on a table,with respect to some reference. Assuming each object has a fixed intended position and orientation on the tabletop, the orientation of a particular object predicted by the deep learning model can be used to compute the transformation matrix to move the object from its initial position to the intended position. This can be fed to a pick and place robot to carry out the transfer.This paper talks about the deep learning approaches used in this project for object detection and orientation estimation.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">INTRODUCTION</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Social robotics, or robots working with humans in social environments, is an exciting field. Robots can be used to do daily chores for humans. Here, we take up one of these daily tasks and use deep learning algorithms to make robots perform a particular task. Most of us would love a robot that can clean up our house, set the dinner table, clear up the mess in a room, or just rearrange a messy table. There can be similar applications of this in industrial environments where robots can be used for the management of the tools and equipment. The idea in this project is to apply deep learning to robotics in such a way that it can organize a messy table in the future, perhaps even customize the table for a particular user.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Our project focuses on detecting and recognizing objects on a tabletop, and estimating the orientation of the object using deep learning techniques. Items in the tabletop have a defined home location such as employed by 5S workplace. With the knowledge of the orientation of the object, the transformation matrix required to transform the object to its home location can be computed. The output can then be utilized to instruct a pick and place robot, serial manipulator, humanoid robot, etc. to carry out the transfer of the item.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">LITERATURE REVIEW</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Object detection and recognition along with pose estimation has been a topic of research since many years. Computer vision techniques using hand crafted feature descriptors such as SIFT, SURF etc. in conjunction with RANSAC have been used to understand images, irrespective of the scale, translation or rotation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib2" title="">2</a>]</cite>. This is extremely useful in many applications where objects need to be recognized in dynamic environments. It also helps in pose estimation, which is essential in the robotics industry to perform grasping and placing operations.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Deep learning is an emerging area of machine learning, that uses many layers to represent multiple layers of abstraction. Neural networks has become a good tool to replace hard coded feature descriptors. Convolutional Neural Networks have proven to be very useful in understanding images and extracting relevant features from them.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Deep learning based pose estimation is an ongoing research problem. It has used various network architectures in the past, such as 2D and 3D Convolutional Neural Networks(CNNs) and autoencoders. Here we describe various techniques used by researchers in the field.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Using CNNs for Descriptor Learning</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Wohlhart et all <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib3" title="">3</a>]</cite> proposed a method to learn feature descriptors using CNNs. In this method, images of an object are taken in different viewpoints, and multiple objects were present. The CNNs were trained to learn the descriptors in a manner such that the Euclidean Distance between descriptors of different objects remains large, and the Euclidean Distance between descriptors from the same object were representative of the similarity between their poses. Finally, when a new test image is given as an input to the system, a k nearest neighbor algorithm was used to find the descriptor that best describes the object, thus giving its category(class) and pose.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Using CNNs to Match Point Clouds to 3D Models</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In the Amazon Picking Challenge 2016 the MIT-Princeton Team, A. Zeng et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib4" title="">4</a>]</cite>, deployed a system that took an image from 15-18 view points with a depth camera and fed that into a fully convolutional network for 2D object segmentation. This resulted in a 3D point cloud that was then aligned with a pre-scanned 3D model using iterative closest point to obtain a 6D pose.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Using CNNs with Posed Datasets</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">J. Yu et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib5" title="">5</a>]</cite> put forth a method of using Max-Pooling CNNs to recognize and estimate the pose of an object. The dataset was comprised of varying images grouped based upon object type and pose. Each object has 12 subclasses broken into 30 degree rotation segments to train as different classifiers. Pose is then determined by which of the subclass classifier is identified. D. Liang et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib6" title="">6</a>]</cite> expand on this work by separating the network into two Deep Belief Networks, adding a second camera to the second DBN and then joining the last layer together for the classifier. While both approaches demonstrated high accuracy in recognition and pose estimation, it would be difficult to apply this approach in large scale due to the nature of the dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.4.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.5.2">Descriptor Regression using a Convolutional Autoencoder</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Inspired by Wohlhart et all <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib3" title="">3</a>]</cite>, Kehl et all <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib7" title="">7</a>]</cite> proposed a method that could perform 3D object detection and pose estimation under clutter and occlusion. The method uses neural networks along with a local voting based approach. A convolutional autoencoder is trained using random patches from RGB-D images with the aim of creating a model that performs descriptor regression. Finally, when a test image is given as an input, patches are sampled out and passed to the model. The algorithm returns a number of candidate votes which are cast only if their matching score surpasses a certain threshold.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS5.4.1.1">II-E</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS5.5.2">Using Spatial Transformer Networks</span>
</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">Another interesting method uses Spatial Transformer Networks(STNs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib8" title="">8</a>]</cite>. STNs add spatial transformation capabilities to CNNs. STNs learn to store the knowledge of how to transform each training sample in the weights of its layers, in order to ease the classification process. Transforms include cropping, rotations, scaling and non rigid deformations. STNs are superior to pooling layers in CNNs.The Spatial Transformer module consists of three components:a localization network, a grid generator and a sampler.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS6.4.1.1">II-F</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS6.5.2">Estimating pose with distance of object from camera</span>
</h3>
<div class="ltx_para" id="S2.SS6.p1">
<p class="ltx_p" id="S2.SS6.p1.1">The technique of object detection, recognition and its spatial location estimation was used by a group to help blind people see the world using audio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib9" title="">9</a>]</cite>.Their project consists of modules where video is captured using a portable camera, it is then streamed to the server where real-time object detection and recognition is done using YOLO(You Only Look Once).Then the 3D location of the object is estimated using the size and location of the bounding box obtained during detection. For position estimation they defined the default heights of the object and user and hard coded it for 20 classes in the classifier then from the height of the bounding box and the height of the object they estimated the depth. This particular algorithm only calculates the distance of object from the camera and not the orientation of the object. Also, this particular concept was not applied to tabletop objects. Hence, range of positions are large compared to tabletop objects.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS7.4.1.1">II-G</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS7.5.2">Deliberative Approach</span>
</h3>
<div class="ltx_para" id="S2.SS7.p1">
<p class="ltx_p" id="S2.SS7.p1.1">The deliberative approaches work by using multi-object pose estimation as a combinatorial search over the space of possible rendered scenes of objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib10" title="">10</a>]</cite>. Hence it can predict and account for occlusions.They successfully demonstrated object recognition and uncertainty-aware localization in challenging scenes with non-modeled clutter.They extended the deliberative approach of PERCH(Perception via search ) such that it can be applied to cluttered environments without prior models of the objects. It also produces uncertainty estimates for object poses. This approach can be applied to the real-time problem of messy table as it consists of more than one objects in a cluttered environment.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">PROBLEM DESCRIPTION</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The goal of this project is to detect and recognize an object on a tabletop and estimate the orientation of the object with respect to a particular reference frame. Every object in the dataset has a fixed ideal spot on the tabletop, where it should be placed in a certain fixed orientation. The project focuses on building a deep learning model that can detect, recognize, and estimate the orientation of a test input image of an object. Once this information is obtained, the transformation matrix required to place the object in its home location from its current location can be calculated.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">This is quite an interesting problem because the output of such a model can be fed as an input to a pick and place robot. This technology can be applied in numerous fields such as in automated supermarkets, industrial robotics, and personal robots.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The idea can be extended to build personal robots that can organize places in a manner that is customized for a specific user.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">DATASET</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This project uses the ‘Tabletop’ dataset, developed by Min Sun et all <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#bib.bib1" title="">1</a>]</cite>. This dataset consists of three categories of commonly found office desk objects each having 10 object instances: mugs, computer mice, and staplers. The image of each object is captured by a camera in 16 different poses. The images are taken in 8 different angles, at two different heights, H1 and H2. In this project, the orientation of the object refers to the 8 different angles,denoted by A1, A2, A3, A4, A5, A6, A7, and A8.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The dataset comprised of 80 gray-scaled images of size 640x480 pixels per angle for all three objects (mugs, computer mice, and staplers). The dataset also consists of masks for some of these images.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">DATA PREPROCESSING</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">As all the images in the dataset are gray-scaled, the object of focus on the image cannot be differentiated very well from the background. Additionally, the set of images for the objects under consideration sometimes includes the other objects in the background. For example, images of staplers may include a mouse or mug in the background. To eliminate this problem, the dataset included masks to strip out the background for each image. The information format within the masks was 0 to 255 based, meaning anything to be ignored was 0 and locations to retain was 255. We divided the mask by 255 to transform the image matrix into a logical bit mapping that we could bitwise multiply with the dataset images. This resulted in images that focused only on the object in consideration with the rest of the image being black. However, only a few images had masks, resulting in a very small processed dataset. To counter this problem, the data was augmented. The images multiplied by their masks were shifted horizontally and vertically to produce new images. As the end goal was to predict the orientation of an object, the images were not rotated for augmentation. The resulting dataset comprised of 64,160 images in total, with 8,020 images per angle per object. Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S5.F1" title="Figure 1 ‣ V DATA PREPROCESSING ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_tag">1</span></a> shows a sample of images found in the original dataset, along with their masks, and the images obtained by adding the original images with their masks.</p>
</div>
<figure class="ltx_figure" id="S5.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="413" id="S5.F1.g1" src="extracted/5826931/Image_masking1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Masking of the Images</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Data Parsing</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The images in the dataset were grouped according to the object represented in the image, resulting in three folders: ’mug’,’mouse’, and ’stapler’. Orientation of the objects are defined in the image’s file name. A python script was developed to parse the images. Using the OpenCV Library functions, the images were imported and converted to numpy arrays with the orientation also being extracted from the file names. The three numpy arrays representing the images and labels for object type and orientation angle were then pickled for further use.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">The dataset was split into two categories. Images that were captured from the ’H1’ height viewpoint was used to train the model, while the images that were captured from the ’H2’ height viewpoint was used for testing the model.This made sure the testing and training data were mutually exclusive. These heights are included in the filename of each image.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">APPROACH</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">According to the problem description, we have to first detect and recognize the object represented by a test image. A simple convolutional neural network is used for this purpose. Once this information is obtained, another deep learning model is used to predict the orientation of the object. This is so because a stapler and a mouse at the same angle, say ’A8’, appear very different from each other. Therefore, the features that define the orientation of a mouse are very different from those that define the same orientation for a stapler. Hence, three different deep learning models that use convolutional neural networks are trained on datasets of images of mice, staplers, and mug. The models formed are then used to predict the orientation of the object.
As mentioned before, the images captured at height ’H1’ are used to train the model and the images captures at height ’H2’ are used to test the model. This way we can make sure that the model is tested on images it has not seen during training. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S6.F2" title="Figure 2 ‣ VI APPROACH ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_tag">2</span></a> depicts the difference between the images of a mug at the angle specified by ’A1’ captured at two different heights ’H1’ and ’H2’. It can be observed that there is a considerable difference between the two images, thus making this an interesting and challenging problem.</p>
</div>
<figure class="ltx_figure" id="S6.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="438" id="S6.F2.g1" src="extracted/5826931/height.jpg" width="778"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Image of mug from two different heights.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">EXPERIMENTS AND RESULTS</span>
</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS1.4.1.1">VII-A</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS1.5.2">Object Recognition</span>
</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">Object recognition proved to be a very simple problem. A model comprising of two convolutional layers with 3 x 3 filters and having 64 and 32 activation maps.
This is then followed by a fully connected layer of 300 units, with an output layer behind that. This configuration was able to achieve an accuracy of 98.0 percent. The images were not masked during training or testing, as it was not needed. The model was able to recognize the object of focus without any data preprocessing. For faster training and testing, the sizes of the images were halved, keeping the aspect ratio the same.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS2.4.1.1">VII-B</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS2.5.2">Angle Estimation</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">To estimate the object orientation angle, the dataset of masked images was employed. Though a lot of experimentation, we finalized on a deep learning model architecture comprising of 5 convolutional layers. Each layer was followed by a max pooling layer stacked with a fully connected layer and an output layer. The first convolutional layer had an output dimensionality of 16, with 5 x 5 filters. The parameters are then halved using a max pooling layer. The next 4 convolutional layers all have 3 x 3 filters with output dimensionality of 32, 64, 70, and 80. All the convolutional layers are activated by the ’ReLU’ function, and are followed by a max pooling layer which halves the parameters. After flattening, a fully connected layer of size 300 units was added. To avoid over-fitting, a dropout of 0.3 was added in between the fully connected layer and the final output layer. The output layer was activated by the ’softmax’ function. The optimizer used was ’RMSProp’. The models were trained for around 5 epochs and the weights for the models with the highest validation accuracies were saved.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">The models were all trained using the images captured at height ’H1’ and tested on images captured at height ’H2’. The model architecture was identical for all three objects, but consisted of different weights as they were trained separately. During training the validation accuracies achieved were quite high, around 99 percent, for all the three objects. However, when the best performing models were tested using the images captured at height ’H2’, the results were different. The model trained for recognizing the orientation of a stapler achieved the best score, around 80 percent accuracy. The model trained for recognizing the orientation of a mug performed second best with an accuracy of 77 percent. The model trained to recognize the orientation of a mouse performed the worst with an accuracy of 55 percent. These results can be attributed to the geometrical features of the three objects. The stapler, being a long and thin object, has the least rotational symmetry. This makes it easier for a neural network model to predict the stapler’s orientation. On the other hand, the model does not do a good job while predicting the orientation of the computer mouse due to its much higher degree of rotational symmetry. The filters of each conventional layer were visualized and they are depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S9.F3" title="Figure 3 ‣ IX FUTURE WORK ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_tag">3</span></a>. Here, the image of a mug is given as input. We can see how certain layers extract features that define the orientation of the mug.</p>
</div>
<figure class="ltx_table" id="S7.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Accuracies</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S7.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_rr ltx_border_t" id="S7.T1.1.1.1.1">Object</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S7.T1.1.1.1.2">Stapler</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S7.T1.1.1.1.3">Mug</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S7.T1.1.1.1.4">Mouse</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_tt" id="S7.T1.1.2.1.1">Training on images captured at H1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S7.T1.1.2.1.2">98.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S7.T1.1.2.1.3">99.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S7.T1.1.2.1.4">98.08</td>
</tr>
<tr class="ltx_tr" id="S7.T1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" id="S7.T1.1.3.2.1">Testing on images captured at H2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T1.1.3.2.2">80</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T1.1.3.2.3">77</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T1.1.3.2.4">55</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00869v1#S7.T1" title="TABLE I ‣ VII-B Angle Estimation ‣ VII EXPERIMENTS AND RESULTS ‣ Detection, Recognition and Pose Estimation of Tabletop Objects"><span class="ltx_text ltx_ref_tag">I</span></a> shows the validation accuracies of the models while predicting the orientation of the object during training and the final accuracy while testing the models on the test dataset, which comprises of images captured at height ’H2’.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">CONCLUSION</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">This paper focused on making a deep neural network model to recognize the class and orientation of an object. The objects the network was trained to recognize was mugs, mice, and staplers. Recognizing the object proved to be a very simple task, as the three objects have very different visual features. Object recognition was possible using a simple convolutional neural network.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Determining the orientation of the object proved to be a much more difficult task, which required a lot of data preprocessing. The best performing model for each object category was tested on a dataset that comprised of images taken at a different height. The model for detecting the orientation of staplers performed the best, due to the fact that staplers have a very low degree of rotational symmetry, thus making different orientations more distinct and unique. On the other hand, the model for detecting the orientation of a computer mouse did not perform that well, due to its high degree of rotational symmetry, which is a direct consequence of its oval shape.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span class="ltx_text ltx_font_smallcaps" id="S9.1.1">FUTURE WORK</span>
</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">The work demonstrated in this paper can be used to recognize an object from a list of objects and further recognize its orientation with respect to some fixed frame of reference. This information can be extremely useful. For example, the data can be fed to a pick and place robot. The orientation data will determine how the robot must grasp the object. Such robots can be useful in warehouses, automated labs, smart homes, grocery shops, etc. The robot will be required to compute the transformation matrix that must be applied to the position and orientation of the object to move the object to its intended destination location.</p>
</div>
<figure class="ltx_figure" id="S9.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="438" id="S9.F3.g1" src="extracted/5826931/visualization.jpg" width="778"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualization of the CNN layers with the input image is of mug </figcaption>
</figure>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> Sun, Min, et al. ”Depth-encoded hough voting for joint object detection and shape recovery.” Computer Vision–ECCV 2010 (2010): 658-671.
APA 
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> R. Szeliski and SpringerLink ebooks - Computer Science, Computer Vision: Algorithms and Applications. 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> Wohlhart, Paul, and Vincent Lepetit. ”Learning descriptors for object recognition and 3d pose estimation.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> A. Zeng et al, ”Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge,” 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> J. Yu et al, ”A vision-based robotic grasping system using deep learning for 3D object recognition and pose estimation,” in 2013, . DOI: 10.1109/ROBIO.2013.6739623

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> D. Liang et al, ”A 3D object recognition and pose estimation system using deep learning method,” in 2014, . DOI: 10.1109/ICIST.2014.6920502.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> W. Kehl et al, ”Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation,” 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> M. Jaderberg et al, ”Spatial Transformer Networks,” 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> Rui (Forest) Jiang, Qian Lin,Shuhui Qu,”Let Blind People See: Real-Time Visual Recognition with Results Converted to 3D Audio.”

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> Narayanan, Venkatraman and Likhachev, Maxim,”Deliberative Object Pose Estimation in Clutter”,Robotics and Automation (ICRA), 2017 IEEE International Conference.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Sep  1 23:27:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
