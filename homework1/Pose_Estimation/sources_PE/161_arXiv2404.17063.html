<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users</title>
<!--Generated on Thu Apr 25 22:07:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Accessibility,  Data Synthesis,  Wheelchair Users,  Pose Estimation" lang="en" name="keywords"/>
<base href="/html/2404.17063v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S1" title="In WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S2" title="In WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S2.SS1" title="In 2. Related Work ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S2.SS2" title="In 2. Related Work ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Sensing for People with Limited Mobility</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S2.SS3" title="In 2. Related Work ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Synthetic Data for Computer Vision</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S2.SS4" title="In 2. Related Work ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Evaluating the Quality of Synthetic Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3" title="In WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_italic">WheelPose</span> Dataset Synthesis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.SS1" title="In 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Generating Postures</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.SS1.SSS1" title="In 3.1. Generating Postures ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Human Skeletal Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.SS1.SSS2" title="In 3.1. Generating Postures ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Motion Sequence Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.SS1.SSS3" title="In 3.1. Generating Postures ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Pose Modification and Conversion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.SS2" title="In 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Generating Wheelchair User Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.SS3" title="In 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Generating the Simulation Environment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.SS3.SSS1" title="In 3.3. Generating the Simulation Environment ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Camera Configuration and Keypoint Annotations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.SS4" title="In 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Assembling <span class="ltx_text ltx_font_italic">WheelPose</span> Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4" title="In WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation of <span class="ltx_text ltx_font_italic">WheelPose</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS1" title="In 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Human Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS1.SSS1" title="In 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Procedure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS1.SSS2" title="In 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Data Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS1.SSS3" title="In 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Results and Findings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS2" title="In 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Statistical Analyses</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS2.SSS1" title="In 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>High-Level Dataset Features</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS2.SSS2" title="In 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Bounding Boxes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS2.SSS3" title="In 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Keypoints</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS2.SSS4" title="In 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span>Camera</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS3" title="In 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Model Performance Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS3.SSS1" title="In 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Testing Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS3.SSS2" title="In 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Training Strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS3.SSS3" title="In 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Ablation Testing Strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS3.SSS4" title="In 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.4 </span><span class="ltx_text ltx_font_italic" style="color:#000000;">WheelPose<span class="ltx_text ltx_font_upright">-Opt Model Performance Deep Dive</span></span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS3.SSS5" title="In 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.5 </span>Key Prediction Changes</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S5" title="In WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S6" title="In WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1" title="In WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.SS1" title="In Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Animation Clip Fix</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.SS2" title="In Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Posture to <span class="ltx_text ltx_font_italic">AnimationClip</span> Conversion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.SS3" title="In Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span><span class="ltx_text ltx_font_italic">WheelPose</span> Randomizers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.SS4" title="In Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Testing Dataset Action Classes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.SS5" title="In Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Impacts of Keypoint Location Definitions</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">William Huang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-7651-2190" title="ORCID identifier">0000-0001-7651-2190</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of California, Los Angeles</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">California</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:william.huang@ucla.edu">william.huang@ucla.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sam Ghahremani
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0007-5844-4834" title="ORCID identifier">0009-0007-5844-4834</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">University of California, Los Angeles</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">California</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:samg2024@berkeley.edu">samg2024@berkeley.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Siyou Pei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-3802-8298" title="ORCID identifier">0000-0003-3802-8298</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">University of California, Los Angeles</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">California</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sypei@g.ucla.edu">sypei@g.ucla.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-2472-6968" title="ORCID identifier">0000-0003-2472-6968</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">University of California, Los Angeles</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">California</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yangzhang@ucla.edu">yangzhang@ucla.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id13.id1">Existing pose estimation models perform poorly on wheelchair users due to a lack of representation in training data. We present a data synthesis pipeline to address this disparity in data collection and subsequently improve pose estimation performance for wheelchair users. Our configurable pipeline generates synthetic data of wheelchair users using motion capture data and motion generation outputs simulated in the Unity game engine. We validated our pipeline by conducting a human evaluation, investigating perceived realism, diversity, and an AI performance evaluation on a set of synthetic datasets from our pipeline that synthesized different backgrounds, models, and postures. We found our generated datasets were perceived as realistic by human evaluators, had more diversity than existing image datasets, and had improved person detection and pose estimation performance when fine-tuned on existing pose estimation models. Through this work, we hope to create a foothold for future efforts in tackling the inclusiveness of AI in a data-centric and human-centric manner with the data synthesis techniques demonstrated in this work. Finally, for future works to extend upon, we open source all code in this research and provide a fully configurable Unity Environment used to generate our datasets. In the case of any models we are unable to share due to redistribution and licensing policies, we provide detailed instructions on how to source and replace said models. All materials can be found at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hilab-open-source/wheelpose" title="">https://github.com/hilab-open-source/wheelpose</a>.</p>
</div>
<div class="ltx_keywords">Accessibility, Data Synthesis, Wheelchair Users, Pose Estimation
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the CHI Conference on Human Factors in Computing Systems; May 11–16, 2024; Honolulu, HI, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI ’24), May 11–16, 2024, Honolulu, HI, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3613904.3642555</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0330-0/24/05</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Accessibility systems and tools</span></span></span>
<figure class="ltx_figure ltx_teaserfigure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="320" id="S0.F1.g1" src="extracted/5503605/figures/data_pipeline.png" width="568"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Overview of the full <span class="ltx_text ltx_font_italic" id="S0.F1.3.1">WheelPose</span> data generation pipeline. Developers can choose different motion sources. Motion sequences are modified according to the specification stated below before being evaluated by human evaluators. Developers can regenerate, filter, and clean motion sequences from human evaluations before all motions are converted into Unity readable <span class="ltx_text ltx_font_italic" id="S0.F1.4.2">AnimationClips</span>. Converted motion sequences, selected background images, and parameters are used in simulation to generate synthetic images and their related annotations for use in model boosting.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S0.F1.5">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S0.F1.6">Eight components: Initial Motion Generation, Pose Modification, Human Evaluations, Animation Conversion, Background Generation, Unity Simulation Environment, Pipeline Output, and Model Boosting. Initial motion generation consists of a choice between generative models, motion datasets, or self captured data. This data is in the form of 3D joint motion sequences and fed into Pose Modification which consists of a low pass filter, lower body modification, and joint position to rotation conversion. This feeds into Human Evaluations which returns user feedback back to Initial Motion Generation and passes data into Animation Conversion which consists of FBX conversion and <span class="ltx_text ltx_font_italic" id="S0.F1.6.1">AnimationClip</span> conversion. <span class="ltx_text ltx_font_italic" id="S0.F1.6.2">AnimationClips</span> are fed into the Unity Simulation Environment along with the user choice of real world background images or simulated background images in Background Generation. Unity Simulation environment outputs Pipeline Outputs consisting of synthetic images and annotations which inputted into model boosting.</p>
</div>
</div>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The inclusiveness of AI depends on the quality and diversity of data used to train AI models. We focus on pose estimation models, which have found widespread use in health care, environmental safety, entertainment, context-aware smart environments, and more. These models are a major concern in the push for AI fairness due to the disparity in their accuracy of predicted postures between able-bodied users and users with disabilities <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib39" title="">2020</a>; Trewin, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib96" title="">2018</a>; Whittaker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib107" title="">2019</a>)</cite>. <span class="ltx_text" id="S1.p1.1.1" style="color:#000000;">Focusing on human movement, Olugbade et al. <cite class="ltx_cite ltx_citemacro_citep">(Olugbade et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib74" title="">2022</a>)</cite> surveyed 704 open datasets and found a major lack of diversity. The authors found no datasets that included people with disabilities performing sports, engaging in artistic expressions, or simply performing everyday tasks. We suspect that the lack of diversity has contributed to biases and poor performance on users with disabilities in many popular AI models trained on common human movement datasets like Detectron2 ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib109" title="">2019</a>)</cite>.</span> This is especially apparent in users who use mobility-assistive technologies (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S1.F2" title="In 1. Introduction ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>). The lack of disability representation in training data can be directly attributed to poor accessibility in the data collection process for people with disabilities <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib75" title="">2021</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib39" title="">2020</a>)</cite>. People with disabilities often must overcome more challenges in data collection during their commute and communications in the recruitment and participation process. One such example and the focus of our work are wheelchair users, who may not be able to navigate through motion capture rigs easily. Additionally, certain poses that able-bodied users could easily perform might be difficult or even dangerous for people with motor impairments. To improve disability representation in training data in the push for inclusive AI, we must make data collection equitable across people with <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">all</span> levels of capabilities.</p>
</div>
<figure class="ltx_figure" id="S1.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S1.F2.1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S1.F2.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S1.F2.1.g1" src="extracted/5503605/figures/bad_imagenet_predictions/s_1.jpg" width="108"/><span class="ltx_ERROR undefined" id="S1.F2.1.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S1.F2.1.2">A side-view of a wheelchair user pushing. Both her arms are on the wheels.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S1.F2.2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S1.F2.2.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S1.F2.2.g1" src="extracted/5503605/figures/bad_imagenet_predictions/s_3.jpg" width="108"/><span class="ltx_ERROR undefined" id="S1.F2.2.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S1.F2.2.2">A side-view of a wheelchair user opening a door. The wheelchair is directly in front of the door and both her hands are placed on the handle.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S1.F2.3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S1.F2.3.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S1.F2.3.g1" src="extracted/5503605/figures/bad_imagenet_predictions/s_4.jpg" width="108"/><span class="ltx_ERROR undefined" id="S1.F2.3.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S1.F2.3.2">A side-view of a wheelchair user putting on a graduation hat. Both their hands are holding a hang which is over her head.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S1.F2.4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S1.F2.4.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S1.F2.4.g1" src="extracted/5503605/figures/bad_imagenet_predictions/s_5.jpg" width="108"/><span class="ltx_ERROR undefined" id="S1.F2.4.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S1.F2.4.2">A 45-degree side-front ground-up view of a wheelchair user doing a wheelie. Both hands are on the wheels of the wheelchair.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S1.F2.5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S1.F2.5.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S1.F2.5.g1" src="extracted/5503605/figures/bad_imagenet_predictions/s_6.jpg" width="108"/><span class="ltx_ERROR undefined" id="S1.F2.5.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S1.F2.5.2">A wheelchair dancer facing the camera with their wheelchair parallel to the plane of the camera. They are in front of four crowd members in a white dress.</p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Examples of poor keypoint prediction performance from Detectron2 ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib109" title="">2019</a>)</cite>.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S1.F2" title="In 1. Introduction ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> Poor prediction of the legs and torso which are slightly occluded by the wheelchair.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S1.F2" title="In 1. Introduction ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> Both shins are predicted to be on the hand due to the occlusion of the wheelchair.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S1.F2" title="In 1. Introduction ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> Legs are predicted to be on the upper body.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S1.F2" title="In 1. Introduction ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> The right shin is predicted to be on the wheels of the wheelchair.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S1.F2" title="In 1. Introduction ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> The wheelchair dancer is completely undetected by ImageNet.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Prior work has combated this issue by proposing guidelines in the design of studies <cite class="ltx_cite ltx_citemacro_citep">(Bennett and Rosner, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib15" title="">2019</a>)</cite> and online data collection systems that could be more accessible to people with disabilities <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib75" title="">2021</a>)</cite>. We propose an alternative solution to the data collection problem: synthetic data. Like how synthesized materials could help preserve scarce natural resources – synthetic rubbers were developed in part because of concerns about the availability of natural rubber – synthetic data could be valuable in supplementing insufficient data collection from people with disabilities. While traditional motion capture procedures lie on a spectrum of difficulty dictated by the motion of the user and exacerbated by the innate difficulties of data collection with users with assistive technologies, synthetic data offers a more accessible alternative where different actions, settings, and assistive technologies, ranging from cooking at home to performing back-flips in the forest, are equal in the difficulty of implementation and feasibility.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text" id="S1.p3.1.1" style="color:#000000;">In this research, we present a novel data synthesis pipeline which leverages motion generation models to simulate highly customizable image data of wheelchair users. Our approach includes steps for user-defined parameters, data screening, and developer feedback. This pipeline yields image data which can be used to improve the performance of AI models for wheelchair users. We evaluate this in the case of pose estimation by fine-tuning common pose estimation models, trained on common human movement datasets, with our synthetic data. Fine-tuned models are then tested on a new dataset of wheelchair users to analyze the degree of improvement from adding synthetic data to training datasets.</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Finally, as we are cautious about the problematic simulation of disabilities (e.g., blindfolded participants to simulate people without vision or with low vision), synthesized wheelchair user postures are carefully reviewed in human evaluations to avoid inadvertently exacerbating existing equitability problems our approach attempts <cite class="ltx_cite ltx_citemacro_citep">(Bennett and Rosner, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib15" title="">2019</a>)</cite>. Our goal is not to exclude wheelchair users from AI training, but rather present a data collection solution that enables them to shepherd the synthesis of data. In doing so, this research leverages data as an intuitive way for wheelchair users to impact AI training, through which we hope to produce more fair and inclusive AI models.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Through our work, we aim to answer the following research questions and related sub-questions in the context of wheelchair users and pose estimation problems:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">RQ1:</span> How to <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.I1.i1.p1.1.2">effectively generate synthetic data</span>?</p>
<ul class="ltx_itemize" id="S1.I1.i1.I1">
<li class="ltx_item" id="S1.I1.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.I1.i1.p1.1.1">RQ1.1:</span> How can we model wheelchair users?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.I1.i2.p1.1.1">RQ1.2:</span> What are the controlling parameters in synthetic data generation?</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">RQ2:</span> What are the <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.I1.i2.p1.1.2">efficacies of synthetic data</span>?</p>
<ul class="ltx_itemize" id="S1.I1.i2.I1">
<li class="ltx_item" id="S1.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.I1.i1.p1.1.1">RQ2.1:</span> How do individual parameters of synthetic data generation affect pose estimation performance?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.I1.i2.p1.1.1">RQ2.2:</span> What are the benefits and drawbacks of using synthetic data?</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">To summarize, our contribution is three-fold:</p>
</div>
<div class="ltx_para" id="S1.p8">
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">adoption of data synthesis to improve inclusion of AI.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">a custom data-synthesis pipeline for pose tracking with improved performance for wheelchair users.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">investigations of the efficacy of the overall approach.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Pose Estimation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Pose estimation plays a key role in fields like the animation and video industry <cite class="ltx_cite ltx_citemacro_citep">(SUTIL, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib90" title="">2015</a>; Kin, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib2" title="">2019</a>)</cite>. Developments in deep learning have enabled users to circumvent the need for cumbersome marker suits in traditional motion capture and directly generate human postures from camera outputs <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib110" title="">2022</a>; Luvizon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib67" title="">2023</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib115" title="">2023</a>; Dee, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib4" title="">2023</a>)</cite>. Of particular note is 2D posture recognition, where the recent releases of easily accessible pipelines like MediaPipe <cite class="ltx_cite ltx_citemacro_citep">(Lugaresi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib66" title="">2019</a>)</cite>, OpenPose <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib21" title="">2019</a>)</cite>, and BlazePose <cite class="ltx_cite ltx_citemacro_citep">(Bazarevsky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib13" title="">2020</a>)</cite> have enabled widespread access to posture recognition in a wide variety of applications including biomechanics <cite class="ltx_cite ltx_citemacro_citep">(Mundt et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib73" title="">2022</a>)</cite>, autonomous driving <cite class="ltx_cite ltx_citemacro_citep">(Way, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib3" title="">2022</a>)</cite>, sign language interpretation <cite class="ltx_cite ltx_citemacro_citep">(Moryossef et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib72" title="">2020</a>)</cite>, and more.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Sensing for People with Limited Mobility</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Currently, over 8.5% of the population of the world is age 65 or over. This number is projected to grow to nearly 17% of the world’s population by 2050 <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib45" title="">2016a</a>)</cite>. Given the direct correlation between age and and mobility limitations and disabilities, this trend implies a growing need for mobility-related technologies <cite class="ltx_cite ltx_citemacro_citep">(Ferrucci et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib33" title="">2016</a>; Freiberger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib34" title="">2020</a>; Gardener et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib37" title="">2006</a>)</cite>. Our research focuses on the community of wheelchair users, where technologies like SpokeSense <cite class="ltx_cite ltx_citemacro_citep">(Carrington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib23" title="">2020</a>)</cite> have established themselves as a part of a broader focus in research related to developing smart wheelchairs <cite class="ltx_cite ltx_citemacro_citep">(Leaman and La, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib56" title="">2017</a>)</cite> which integrate different sensors, including camera, lidar, and EEG, to make wheelchairs more comfortable and safe. Other works focus on developing more accessible control systems <cite class="ltx_cite ltx_citemacro_citep">(Upadhyaya et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib99" title="">2022</a>)</cite> or routing systems <cite class="ltx_cite ltx_citemacro_citep">(Kirkham and Tannert, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib54" title="">2021</a>)</cite> for users with mobility impairments. Posture estimation techniques for wheelchair users can reveal a user’s sitting habits, analyze their mood, and predict health risks such as pressure ulcers or lower back pain <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib68" title="">2017</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Synthetic Data for Computer Vision</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Computer vision models have traditionally been trained using large-scale human-labeled datasets such as PASCAL VOC <cite class="ltx_cite ltx_citemacro_citep">(Everingham et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib31" title="">2010</a>)</cite>, Microsoft COCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib65" title="">2015</a>)</cite>, and ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib25" title="">2009</a>)</cite>. While effective, these datasets are costly to produce, requiring large amounts of publicly available images, manpower, and time to create. Furthermore, these datasets are often static and offer little in the form of customizability to allow researchers and engineers to use data specific to their task. One solution to these problems is the use of data simulators. SYNTHIA <cite class="ltx_cite ltx_citemacro_citep">(Ros et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib83" title="">2016</a>)</cite>, Synscapes <cite class="ltx_cite ltx_citemacro_citep">(Wrenninge and Unger, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib108" title="">2018</a>)</cite>, Hypersim <cite class="ltx_cite ltx_citemacro_citep">(Roberts et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib82" title="">2021</a>)</cite>, and OpenRooms <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib62" title="">2021b</a>)</cite> provide synthetic datasets for computer vision tasks related to object detection in different settings. Other robotics simulators including AI-2THOR <cite class="ltx_cite ltx_citemacro_citep">(Kolve et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib55" title="">2022</a>)</cite>, NVIDIA Isaac Sim <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib63" title="">2018</a>)</cite>, Mujoco <cite class="ltx_cite ltx_citemacro_citep">(Todorov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib94" title="">2012</a>)</cite>, and iGibson <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib87" title="">2021</a>)</cite> offer a rich set of tools for embodied AI tasks. Other systems, like BlenderProc <cite class="ltx_cite ltx_citemacro_citep">(Denninger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib26" title="">2019</a>)</cite>, BlendTorch <cite class="ltx_cite ltx_citemacro_citep">(Heindl et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib46" title="">2020</a>)</cite>, NVISII <cite class="ltx_cite ltx_citemacro_citep">(Morrical et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib71" title="">2021</a>)</cite>, and Unity Perception <cite class="ltx_cite ltx_citemacro_citep">(Bartolomé Villar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib12" title="">2022</a>)</cite> prefer to instead enable the developer to generate their own data through highly configurable simulators. These tools and datasets have already demonstrated considerable success in deep learning-related training tasks <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib44" title="">2023</a>; Anderson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib7" title="">2022</a>; Tian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib92" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Synthetic humans provide further challenges due to the complexities of human bodies and the variations and limitations of a human’s appearance and posture. Various approaches have been taken, using different types of simulators to generate labeled datasets. Examples of different approaches include deriving data from hand-crafted scenes <cite class="ltx_cite ltx_citemacro_citep">(Bak et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib10" title="">2018</a>)</cite>, custom 3D scenes with SMPL models <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib112" title="">2023</a>; Pumarola et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib79" title="">2019</a>; Varol et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib101" title="">2017a</a>; Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib77" title="">2021</a>; Bazavan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib14" title="">2022</a>; Purkrábek and Matas, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib80" title="">2023</a>)</cite>, existing games like Grand Theft Auto V <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib20" title="">2020</a>; Fabbri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib32" title="">2018</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib49" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib50" title="">2021</a>; Cai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib19" title="">2022</a>)</cite>, and game engines <cite class="ltx_cite ltx_citemacro_citep">(Ebadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib29" title="">2022b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib28" title="">a</a>)</cite>. We were inspired by this line of work and extended upon the existing PeopleSansPeople (PSP) data generator with the Unity Perception package <cite class="ltx_cite ltx_citemacro_citep">(Bartolomé Villar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib12" title="">2022</a>)</cite> using domain randomization principles which help AI models trained in simulated environments to effectively transfer to real-world tasks <cite class="ltx_cite ltx_citemacro_citep">(Tobin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib93" title="">2017</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Evaluating the Quality of Synthetic Data</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Despite the advantages of data synthesis, an implicit assumption of using synthetic data is that it should be sufficiently high-quality to achieve performance similar to real data. To evaluate the quality of synthetic data, researchers have explored a wide range of metrics.
Emam et al. <cite class="ltx_cite ltx_citemacro_citep">(El Emam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib30" title="">2020</a>)</cite> outlined three types of approaches to assess synthetic data utility in their book - workload-aware evaluations, generic assessments, and subjective assessments of data utility. Among them, workload-aware evaluations check if synthetic data replicates the performance of real data, widely used in data synthesis research <cite class="ltx_cite ltx_citemacro_citep">(Varol et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib102" title="">2017b</a>; Pishchulin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib78" title="">2012</a>; Gaidon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib36" title="">2016</a>)</cite>.
Generic assessments measure the utility indicators of real and synthetic data when the indicators are quantifiable and clear <cite class="ltx_cite ltx_citemacro_citep">(Kaloskampis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib53" title="">2019</a>; Howe et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib47" title="">2017</a>)</cite> (e.g., the distance between their statistical indicators such as mean, average, and distribution).
Furthermore, subjective assessments involve real users to evaluate data realism. Some researchers investigate distinguishability, assuming highly-realistic synthetic data leans to be perceived as real <cite class="ltx_cite ltx_citemacro_citep">(Snoke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib88" title="">2018</a>)</cite>, similar to deploying a discriminator in algorithms <cite class="ltx_cite ltx_citemacro_citep">(Tzikas, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib97" title="">2022</a>)</cite>. Other researchers design Likert-Scale questionnaires for realism, which are broadly adopted in clinical training simulation <cite class="ltx_cite ltx_citemacro_citep">(Bartolomé Villar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib12" title="">2022</a>)</cite>. Another criterion is to collect user preferences between several synthetic samples to form a high-quality dataset <cite class="ltx_cite ltx_citemacro_citep">(Weng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib106" title="">2023</a>; Tevet et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib91" title="">2022</a>)</cite>. With a consistent and valid examination of synthetic data, researchers can obtain feedback to improve generation methods and understand how reliable synthetic data is. In our work, we not only conduct workload-aware evaluation but also involve subjective assessments by asking real users to evaluate the data realism. The user-in-the-loop process provides filter handles for more realistic datasets under various contexts and inspires key findings on how data realism affects performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span><span class="ltx_text ltx_font_italic" id="S3.1.1">WheelPose</span> Dataset Synthesis</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We address <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">(RQ1.1)</span> with our system, <span class="ltx_text ltx_font_italic" id="S3.p1.1.2">WheelPose</span>, a data synthesis framework where motion data is converted to wheelchair user animations and rigged on human models in a Unity simulation environment to generate synthetic images and annotations. We present a simple simulation environment, with human models randomly placed in front of a background as the most primitive example of synthetic data still capable of generating positive results in pose estimation (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS3" title="4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>). A visualization of the overall pipeline is found in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S0.F1" title="In WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Our pipeline generates a set of datasets, each including 70,000 captured frames (<math alttext="1280\times 720" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mn id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">1280</mn><mo id="S3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">720</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><times id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></times><cn id="S3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.p1.1.m1.1.1.2">1280</cn><cn id="S3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.p1.1.m1.1.1.3">720</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">1280\times 720</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">1280 × 720</annotation></semantics></math>). Each frame is fully annotated using the COCO 17-keypoint 2D skeletal model, shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.F5.sf2" title="In Figure 5 ‣ 3.1.1. Human Skeletal Modeling ‣ 3.1. Generating Postures ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5(b)</span></a>. Beyond the fact that our dataset is the first fully annotated dataset of wheelchair users, the size of our dataset is comparable with existing datasets like 3DPW with 51,000 captured frames <cite class="ltx_cite ltx_citemacro_citep">(Marcard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib70" title="">2018</a>)</cite> or SMPLy with 24,428 captured frames <cite class="ltx_cite ltx_citemacro_citep">(Leroy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib58" title="">2020</a>)</cite>. Example synthesized data is shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.F3" title="In 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="229" id="S3.F3.g1" src="extracted/5503605/figures/example_synthetic_data.jpg" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="229" id="S3.F3.g2" src="extracted/5503605/figures/example_synthetic_data_1.jpg" width="598"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Example output from the full <span class="ltx_text ltx_font_italic" id="S3.F3.2.1">WheelPose</span> data generation pipeline. The top row includes the generated RGB images. The bottom row includes the generated RGB images with the keypoint and bounding box annotations superimposed on top. Keypoints outlined in black are labeled as ”occluded” while keypoints outlined in white are labeled as ”visible”. Each image is generated with randomized backgrounds, lighting conditions, humans, postures, and occluders.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><span class="ltx_ERROR ltx_figure_panel undefined" id="S3.F3.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.F3.4">Six examples of <span class="ltx_text ltx_font_italic" id="S3.F3.4.1">WheelPose</span> output. Each example features a set of different wheelchair users in front of a unique interior setting. Geometric objects with differing textures are placed around the scene. Below each image is the same image with the annotation of keypoints and bounding boxes superimposed on top.</p>
</div>
</div>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Generating Postures</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our data synthesis pipeline begins with pose generation, where motion data is converted into animations to be used in data synthesis. We choose to use two motion data sources, HumanML3D <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib40" title="">2022a</a>)</cite> and Text2Motion <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib41" title="">2022b</a>)</cite> in our case study. Other motion sources can be easily adapted and used within our pipeline. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.F4" title="In 3.1. Generating Postures ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> demonstrates 14 motion sequences and their resulting postures from our pose generation, documented next. We separate posture generation from the rest of our pipeline to allow developers to iterate upon generated postures through human evaluation, regenerating and filtering data as needed.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="299" id="S3.F4.g1" src="extracted/5503605/figures/motionsequence.jpg" width="598"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Example pose frames in their motion sequences resulting from our pose frame generation. Each column is an individual animation with pose frames selected in chronological order from back to front.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S3.F4.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.F4.2">Models in wheelchairs performing different motions. Fourteen columns of five humans each are in a scene. Each column features different upper body movements with a large variety of ranges of motion across.</p>
</div>
</div>
</figure>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Human Skeletal Modeling</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">In order to enable a wide variety of different postures, we base the implementation of <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.1">WheelPose</span> on a 23-keypoint skeletal model, which is easily converted from commonly available human posture datasets including COCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib65" title="">2015</a>)</cite> and MPII <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib8" title="">2014</a>)</cite> and the output of common pose estimation algorithms including BlazePose <cite class="ltx_cite ltx_citemacro_citep">(Bazarevsky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib13" title="">2020</a>)</cite>, MediaPipe <cite class="ltx_cite ltx_citemacro_citep">(Lugaresi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib66" title="">2019</a>)</cite>, and OpenPose <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib21" title="">2019</a>)</cite>. Detailed information on these keypoints is shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.F5.sf1" title="In Figure 5 ‣ 3.1.1. Human Skeletal Modeling ‣ 3.1. Generating Postures ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5(a)</span></a>. <span class="ltx_text" id="S3.SS1.SSS1.p1.1.2" style="color:#000000;">We note that our current pipeline assumes users have all four limbs and acknowledge that data synthesis for wheelchair users with amputation requires efforts beyond simple ad-hoc removals of key points in our current model. Nonetheless, we interviewed two participants with limb loss, leading to insights for future work which we will discuss later in the paper (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS1" title="4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>).</span></p>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F5.sf1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S3.F5.sf1.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="269" id="S3.F5.sf1.g1" src="extracted/5503605/figures/WheelPoseKeypoints.png" width="269"/><span class="ltx_ERROR undefined" id="S3.F5.sf1.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.F5.sf1.3">Visualization of the <span class="ltx_text ltx_font_italic" id="S3.F5.sf1.3.1">WheelPose</span> keypoint designation. A human model sits on a wheelchair in a T-pose facing forward. Twenty-three keypoints extend across their body, going through the wrists, elbows, and shoulders to create the arms, the top of the head, the top of the neck, the neck base, the upper chest, the upper spine, the lower spine, and the middle hip to create a spine, and hips, knees, ankles, and toes to create legs.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf1.5.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text ltx_font_italic" id="S3.F5.sf1.6.2" style="font-size:80%;">WheelPose<span class="ltx_text ltx_font_upright" id="S3.F5.sf1.6.2.1"> Animation Keypoints</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F5.sf2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S3.F5.sf2.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="269" id="S3.F5.sf2.g1" src="extracted/5503605/figures/COCOKeypoints.png" width="269"/><span class="ltx_ERROR undefined" id="S3.F5.sf2.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.F5.sf2.3">Visualization of the COCO keypoint designation. A human model sits on a wheelchair in a T-pose facing forward. Seventeen keypoints extend across their body, going through the wrists, elbows, and shoulders to create arms, ears, eyes, and nose to create a face, and hips, knees, and ankles to create legs.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf2.4.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S3.F5.sf2.5.2" style="font-size:80%;">COCO Keypoints</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Keypoint mappings used in WheelPose. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.F5.sf1" title="In Figure 5 ‣ 3.1.1. Human Skeletal Modeling ‣ 3.1. Generating Postures ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5(a)</span></a> WheelPose 23-keypoint animation format. Used as the input format of motion sequences before pose modification. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.F5.sf2" title="In Figure 5 ‣ 3.1.1. Human Skeletal Modeling ‣ 3.1. Generating Postures ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5(b)</span></a> COCO 17-keypoint annotation scheme. Used as the final output format of WheelPose annotations.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Motion Sequence Generation</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">We use HumanML3D <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib40" title="">2022a</a>)</cite>, a 3D human motion dataset collected from real-world motion capture in the form of 3D joint positions as an example of motion sequence generation from existing motion capture datasets. Motions with high translational movement, high lower body movement, and broken animations (e.g., jittering, limb snapping, unrealistic rotations) were filtered out from HumanML3D. Individual motions were then randomly sampled and evaluated for their uniqueness and range of motion compared to previously collected data until 100 unique motions were collected.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">Additionally, we leverage Text2Motion <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib41" title="">2022b</a>)</cite>, a motion generation model that uses textual descriptions to generate motions, as a fully generative alternative source of human motion sequences. Text2Motion is only one example of human motion generation through textual descriptions <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib114" title="">2022</a>; Azadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib9" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib113" title="">2023</a>)</cite> and can be easily substituted in our data generation pipeline. Evaluators from the research team assigned textual descriptions to each of the 100 selected HumanML3D motions before inputting the descriptions into Text2Motion and generating 3 potential motions for each description. The most realistic motion was selected and evaluated on whether the motion would be possible for the evaluator to perform and the absence of any noise or artifacts from the generation process that may lead to unrealistic limb movements or positions. Our Text2Motion generation process results in a new dataset of 100 human motions that directly mirror the actions of the sampled HumanML3D motions and provide a direct comparison between synthesized and real data. Our goal in enabling the use of generative motion models like Text2Motion is to investigate the feasibility of using generative deep learning models to further simplify the data collection process for synthetic data generation and therefore improve the efficiency and overall accessibility, especially in the context of users with disabilities and assistive technologies.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1">In total, this process yielded 200 motion sequences from both HumanML3D and Text2Motion (i.e., each yielded 100 sequences). On average, HumanML3D motions had 60.26 (SD=42.05) unique frames per animation and Text2Motion has 146.32 (SD=50.84) frames per animation. Since Text2Motion has no set animation length parameter, we choose to take the full animation output for each motion, leading to the discrepancy in average motion lengths, to directly compare data sourced from motion capture and deep learning models. Text2Motion outputs tend to extend and slow down the described action, leading to a longer but not necessarily more diverse animation compared to HumanML3D.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Pose Modification and Conversion</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">Both HumanML3D and Text2Motion represent motions through the 3D position of joints. All motions are converted into the corresponding 23-keypoint skeletal model used by <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS3.p1.1.1">WheelPose</span> through a direct mapping of corresponding joints or the positional average between the surrounding joints. As is common in biomechanical analysis <cite class="ltx_cite ltx_citemacro_citep">(Schreven et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib86" title="">2015</a>; Bartlett, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib11" title="">2014</a>)</cite>, a 5Hz low pass filter is then applied to the data to handle high-frequency noise generated from the motion capture or data generation process. We convert all motions from a 3D position to a joint rotation representation. In the process of this conversion, internal and external rotations – rotations around the axis parallel to the bone not able to be described in joint position notation – of the arms are affixed to the rotation of the parent bone.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p2">
<p class="ltx_p" id="S3.SS1.SSS3.p2.1">We modified the resulting pose sequences from the two generation methods by affixing the model’s legs onto the wheelchair model with an additional rotational noise applied independently on the flexion/extension, abduction/adduction, and internal/external rotation on each of the lower body bones (i.e. three joints in total) to simulate regular lower body movements when in a wheelchair. The following steps document the procedure for generating rotational noise on one joint. This procedure was motivated by the need for smooth interpolated noise and inspired by the Poisson process.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p3">
<p class="ltx_p" id="S3.SS1.SSS3.p3.3">Given an array of joint angles <math alttext="F_{orig}" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p3.1.m1.1"><semantics id="S3.SS1.SSS3.p3.1.m1.1a"><msub id="S3.SS1.SSS3.p3.1.m1.1.1" xref="S3.SS1.SSS3.p3.1.m1.1.1.cmml"><mi id="S3.SS1.SSS3.p3.1.m1.1.1.2" xref="S3.SS1.SSS3.p3.1.m1.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS3.p3.1.m1.1.1.3" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS3.p3.1.m1.1.1.3.2" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.2.cmml">o</mi><mo id="S3.SS1.SSS3.p3.1.m1.1.1.3.1" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.p3.1.m1.1.1.3.3" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.3.cmml">r</mi><mo id="S3.SS1.SSS3.p3.1.m1.1.1.3.1a" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.p3.1.m1.1.1.3.4" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.4.cmml">i</mi><mo id="S3.SS1.SSS3.p3.1.m1.1.1.3.1b" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.p3.1.m1.1.1.3.5" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.5.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p3.1.m1.1b"><apply id="S3.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.2">𝐹</ci><apply id="S3.SS1.SSS3.p3.1.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3"><times id="S3.SS1.SSS3.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS3.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.2">𝑜</ci><ci id="S3.SS1.SSS3.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.3">𝑟</ci><ci id="S3.SS1.SSS3.p3.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.4">𝑖</ci><ci id="S3.SS1.SSS3.p3.1.m1.1.1.3.5.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.5">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p3.1.m1.1c">F_{orig}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p3.1.m1.1d">italic_F start_POSTSUBSCRIPT italic_o italic_r italic_i italic_g end_POSTSUBSCRIPT</annotation></semantics></math> expressed in degrees with length <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p3.2.m2.1"><semantics id="S3.SS1.SSS3.p3.2.m2.1a"><mi id="S3.SS1.SSS3.p3.2.m2.1.1" xref="S3.SS1.SSS3.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p3.2.m2.1b"><ci id="S3.SS1.SSS3.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p3.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p3.2.m2.1d">italic_n</annotation></semantics></math> total frames in the animation, the noise is generated by first sampling a set of frames indices <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p3.3.m3.1"><semantics id="S3.SS1.SSS3.p3.3.m3.1a"><mi id="S3.SS1.SSS3.p3.3.m3.1.1" xref="S3.SS1.SSS3.p3.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p3.3.m3.1b"><ci id="S3.SS1.SSS3.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p3.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p3.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p3.3.m3.1d">italic_S</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\textstyle S=\left\{x_{0},x_{1},...,x_{n}\ \middle|\begin{array}[]{l}x_{0}=0,%
\\
x_{i}=x_{i-1}+k_{i},\\
k_{i}\sim\mathcal{N}(\frac{n}{4},(\frac{n}{32})^{2}),\\
x_{i}&lt;n,\\
i=1,2,..,n\end{array}\right\}" class="ltx_math_unparsed" display="block" id="S3.E1.m1.9"><semantics id="S3.E1.m1.9a"><mrow id="S3.E1.m1.9b"><mi id="S3.E1.m1.9.10">S</mi><mo id="S3.E1.m1.9.11">=</mo><mrow id="S3.E1.m1.9.12"><mo id="S3.E1.m1.9.12.1">{</mo><msub id="S3.E1.m1.9.12.2"><mi id="S3.E1.m1.9.12.2.2">x</mi><mn id="S3.E1.m1.9.12.2.3">0</mn></msub><mo id="S3.E1.m1.9.12.3">,</mo><msub id="S3.E1.m1.9.12.4"><mi id="S3.E1.m1.9.12.4.2">x</mi><mn id="S3.E1.m1.9.12.4.3">1</mn></msub><mo id="S3.E1.m1.9.12.5">,</mo><mi id="S3.E1.m1.9.9" mathvariant="normal">…</mi><mo id="S3.E1.m1.9.12.6">,</mo><msub id="S3.E1.m1.9.12.7"><mi id="S3.E1.m1.9.12.7.2">x</mi><mi id="S3.E1.m1.9.12.7.3">n</mi></msub><mo id="S3.E1.m1.9.12.8" lspace="0em" rspace="0.167em" stretchy="true">|</mo><mtable displaystyle="true" id="S3.E1.m1.8.8" rowspacing="0pt"><mtr id="S3.E1.m1.8.8a"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.8.8b"><mrow id="S3.E1.m1.1.1.1.1.1.1"><mrow id="S3.E1.m1.1.1.1.1.1.1.1"><msub id="S3.E1.m1.1.1.1.1.1.1.1.2"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2">x</mi><mn id="S3.E1.m1.1.1.1.1.1.1.1.2.3">0</mn></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1">=</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.3">0</mn></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.2">,</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.8.8c"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.8.8d"><mrow id="S3.E1.m1.2.2.2.1.1.1"><mrow id="S3.E1.m1.2.2.2.1.1.1.1"><msub id="S3.E1.m1.2.2.2.1.1.1.1.2"><mi id="S3.E1.m1.2.2.2.1.1.1.1.2.2">x</mi><mi id="S3.E1.m1.2.2.2.1.1.1.1.2.3">i</mi></msub><mo id="S3.E1.m1.2.2.2.1.1.1.1.1">=</mo><mrow id="S3.E1.m1.2.2.2.1.1.1.1.3"><msub id="S3.E1.m1.2.2.2.1.1.1.1.3.2"><mi id="S3.E1.m1.2.2.2.1.1.1.1.3.2.2">x</mi><mrow id="S3.E1.m1.2.2.2.1.1.1.1.3.2.3"><mi id="S3.E1.m1.2.2.2.1.1.1.1.3.2.3.2">i</mi><mo id="S3.E1.m1.2.2.2.1.1.1.1.3.2.3.1">−</mo><mn id="S3.E1.m1.2.2.2.1.1.1.1.3.2.3.3">1</mn></mrow></msub><mo id="S3.E1.m1.2.2.2.1.1.1.1.3.1">+</mo><msub id="S3.E1.m1.2.2.2.1.1.1.1.3.3"><mi id="S3.E1.m1.2.2.2.1.1.1.1.3.3.2">k</mi><mi id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3">i</mi></msub></mrow></mrow><mo id="S3.E1.m1.2.2.2.1.1.1.2">,</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.8.8e"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.8.8f"><mrow id="S3.E1.m1.5.5.5.3.3.3"><mrow id="S3.E1.m1.5.5.5.3.3.3.1"><msub id="S3.E1.m1.5.5.5.3.3.3.1.3"><mi id="S3.E1.m1.5.5.5.3.3.3.1.3.2">k</mi><mi id="S3.E1.m1.5.5.5.3.3.3.1.3.3">i</mi></msub><mo id="S3.E1.m1.5.5.5.3.3.3.1.2">∼</mo><mrow id="S3.E1.m1.5.5.5.3.3.3.1.1"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.5.5.5.3.3.3.1.1.3">𝒩</mi><mo id="S3.E1.m1.5.5.5.3.3.3.1.1.2">⁢</mo><mrow id="S3.E1.m1.5.5.5.3.3.3.1.1.1.1"><mo id="S3.E1.m1.5.5.5.3.3.3.1.1.1.1.2" stretchy="false">(</mo><mstyle displaystyle="false" id="S3.E1.m1.4.4.4.2.2.2"><mfrac id="S3.E1.m1.4.4.4.2.2.2a"><mi id="S3.E1.m1.4.4.4.2.2.2.2">n</mi><mn id="S3.E1.m1.4.4.4.2.2.2.3">4</mn></mfrac></mstyle><mo id="S3.E1.m1.5.5.5.3.3.3.1.1.1.1.3">,</mo><msup id="S3.E1.m1.5.5.5.3.3.3.1.1.1.1.1"><mrow id="S3.E1.m1.5.5.5.3.3.3.1.1.1.1.1.2.2"><mo id="S3.E1.m1.5.5.5.3.3.3.1.1.1.1.1.2.2.1" stretchy="false">(</mo><mstyle displaystyle="false" id="S3.E1.m1.3.3.3.1.1.1"><mfrac id="S3.E1.m1.3.3.3.1.1.1a"><mi id="S3.E1.m1.3.3.3.1.1.1.2">n</mi><mn id="S3.E1.m1.3.3.3.1.1.1.3">32</mn></mfrac></mstyle><mo id="S3.E1.m1.5.5.5.3.3.3.1.1.1.1.1.2.2.2" stretchy="false">)</mo></mrow><mn id="S3.E1.m1.5.5.5.3.3.3.1.1.1.1.1.3">2</mn></msup><mo id="S3.E1.m1.5.5.5.3.3.3.1.1.1.1.4" stretchy="false">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.5.3.3.3.2">,</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.8.8g"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.8.8h"><mrow id="S3.E1.m1.6.6.6.1.1.1"><mrow id="S3.E1.m1.6.6.6.1.1.1.1"><msub id="S3.E1.m1.6.6.6.1.1.1.1.2"><mi id="S3.E1.m1.6.6.6.1.1.1.1.2.2">x</mi><mi id="S3.E1.m1.6.6.6.1.1.1.1.2.3">i</mi></msub><mo id="S3.E1.m1.6.6.6.1.1.1.1.1">&lt;</mo><mi id="S3.E1.m1.6.6.6.1.1.1.1.3">n</mi></mrow><mo id="S3.E1.m1.6.6.6.1.1.1.2">,</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.8.8i"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.8.8j"><mrow id="S3.E1.m1.8.8.8.2.2"><mi id="S3.E1.m1.8.8.8.2.2.3">i</mi><mo id="S3.E1.m1.8.8.8.2.2.4">=</mo><mn id="S3.E1.m1.7.7.7.1.1.1">1</mn><mo id="S3.E1.m1.8.8.8.2.2.5">,</mo><mn id="S3.E1.m1.8.8.8.2.2.2">2</mn><mo id="S3.E1.m1.8.8.8.2.2.6">,</mo><mo id="S3.E1.m1.8.8.8.2.2.7" lspace="0em" rspace="0.0835em">.</mo><mo id="S3.E1.m1.8.8.8.2.2.8" lspace="0.0835em" rspace="0.167em">.</mo><mo id="S3.E1.m1.8.8.8.2.2.9">,</mo><mi id="S3.E1.m1.8.8.8.2.2.10">n</mi></mrow></mtd></mtr></mtable><mo id="S3.E1.m1.9.12.9">}</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.9c">\textstyle S=\left\{x_{0},x_{1},...,x_{n}\ \middle|\begin{array}[]{l}x_{0}=0,%
\\
x_{i}=x_{i-1}+k_{i},\\
k_{i}\sim\mathcal{N}(\frac{n}{4},(\frac{n}{32})^{2}),\\
x_{i}&lt;n,\\
i=1,2,..,n\end{array}\right\}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.9d">italic_S = { italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | start_ARRAY start_ROW start_CELL italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0 , end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT + italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , end_CELL end_ROW start_ROW start_CELL italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ caligraphic_N ( divide start_ARG italic_n end_ARG start_ARG 4 end_ARG , ( divide start_ARG italic_n end_ARG start_ARG 32 end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT &lt; italic_n , end_CELL end_ROW start_ROW start_CELL italic_i = 1 , 2 , . . , italic_n end_CELL end_ROW end_ARRAY }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p5">
<p class="ltx_p" id="S3.SS1.SSS3.p5.5">A new array of joint angle noise values <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p5.1.m1.1"><semantics id="S3.SS1.SSS3.p5.1.m1.1a"><mi id="S3.SS1.SSS3.p5.1.m1.1.1" xref="S3.SS1.SSS3.p5.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.1.m1.1b"><ci id="S3.SS1.SSS3.p5.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p5.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p5.1.m1.1d">italic_N</annotation></semantics></math> is then constructed. Let <math alttext="f(i)" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p5.2.m2.1"><semantics id="S3.SS1.SSS3.p5.2.m2.1a"><mrow id="S3.SS1.SSS3.p5.2.m2.1.2" xref="S3.SS1.SSS3.p5.2.m2.1.2.cmml"><mi id="S3.SS1.SSS3.p5.2.m2.1.2.2" xref="S3.SS1.SSS3.p5.2.m2.1.2.2.cmml">f</mi><mo id="S3.SS1.SSS3.p5.2.m2.1.2.1" xref="S3.SS1.SSS3.p5.2.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS1.SSS3.p5.2.m2.1.2.3.2" xref="S3.SS1.SSS3.p5.2.m2.1.2.cmml"><mo id="S3.SS1.SSS3.p5.2.m2.1.2.3.2.1" stretchy="false" xref="S3.SS1.SSS3.p5.2.m2.1.2.cmml">(</mo><mi id="S3.SS1.SSS3.p5.2.m2.1.1" xref="S3.SS1.SSS3.p5.2.m2.1.1.cmml">i</mi><mo id="S3.SS1.SSS3.p5.2.m2.1.2.3.2.2" stretchy="false" xref="S3.SS1.SSS3.p5.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.2.m2.1b"><apply id="S3.SS1.SSS3.p5.2.m2.1.2.cmml" xref="S3.SS1.SSS3.p5.2.m2.1.2"><times id="S3.SS1.SSS3.p5.2.m2.1.2.1.cmml" xref="S3.SS1.SSS3.p5.2.m2.1.2.1"></times><ci id="S3.SS1.SSS3.p5.2.m2.1.2.2.cmml" xref="S3.SS1.SSS3.p5.2.m2.1.2.2">𝑓</ci><ci id="S3.SS1.SSS3.p5.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p5.2.m2.1.1">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.2.m2.1c">f(i)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p5.2.m2.1d">italic_f ( italic_i )</annotation></semantics></math> be a function that generates the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p5.3.m3.1"><semantics id="S3.SS1.SSS3.p5.3.m3.1a"><mi id="S3.SS1.SSS3.p5.3.m3.1.1" xref="S3.SS1.SSS3.p5.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.3.m3.1b"><ci id="S3.SS1.SSS3.p5.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p5.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p5.3.m3.1d">italic_i</annotation></semantics></math>-th joint angle of the animation. Given <math alttext="U\sim\mathcal{U}(-10,10)" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p5.4.m4.2"><semantics id="S3.SS1.SSS3.p5.4.m4.2a"><mrow id="S3.SS1.SSS3.p5.4.m4.2.2" xref="S3.SS1.SSS3.p5.4.m4.2.2.cmml"><mi id="S3.SS1.SSS3.p5.4.m4.2.2.3" xref="S3.SS1.SSS3.p5.4.m4.2.2.3.cmml">U</mi><mo id="S3.SS1.SSS3.p5.4.m4.2.2.2" xref="S3.SS1.SSS3.p5.4.m4.2.2.2.cmml">∼</mo><mrow id="S3.SS1.SSS3.p5.4.m4.2.2.1" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS3.p5.4.m4.2.2.1.3" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.3.cmml">𝒰</mi><mo id="S3.SS1.SSS3.p5.4.m4.2.2.1.2" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.2.cmml">⁢</mo><mrow id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.2.cmml"><mo id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.2" stretchy="false" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.2.cmml">(</mo><mrow id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1.cmml"><mo id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1a" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1.cmml">−</mo><mn id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1.2" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1.2.cmml">10</mn></mrow><mo id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.3" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.2.cmml">,</mo><mn id="S3.SS1.SSS3.p5.4.m4.1.1" xref="S3.SS1.SSS3.p5.4.m4.1.1.cmml">10</mn><mo id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.4" stretchy="false" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.4.m4.2b"><apply id="S3.SS1.SSS3.p5.4.m4.2.2.cmml" xref="S3.SS1.SSS3.p5.4.m4.2.2"><csymbol cd="latexml" id="S3.SS1.SSS3.p5.4.m4.2.2.2.cmml" xref="S3.SS1.SSS3.p5.4.m4.2.2.2">similar-to</csymbol><ci id="S3.SS1.SSS3.p5.4.m4.2.2.3.cmml" xref="S3.SS1.SSS3.p5.4.m4.2.2.3">𝑈</ci><apply id="S3.SS1.SSS3.p5.4.m4.2.2.1.cmml" xref="S3.SS1.SSS3.p5.4.m4.2.2.1"><times id="S3.SS1.SSS3.p5.4.m4.2.2.1.2.cmml" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.2"></times><ci id="S3.SS1.SSS3.p5.4.m4.2.2.1.3.cmml" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.3">𝒰</ci><interval closure="open" id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.2.cmml" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1"><apply id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1"><minus id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1.1.cmml" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1"></minus><cn id="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1.2.cmml" type="integer" xref="S3.SS1.SSS3.p5.4.m4.2.2.1.1.1.1.2">10</cn></apply><cn id="S3.SS1.SSS3.p5.4.m4.1.1.cmml" type="integer" xref="S3.SS1.SSS3.p5.4.m4.1.1">10</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.4.m4.2c">U\sim\mathcal{U}(-10,10)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p5.4.m4.2d">italic_U ∼ caligraphic_U ( - 10 , 10 )</annotation></semantics></math>, representing a random angular noise added to frames in <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p5.5.m5.1"><semantics id="S3.SS1.SSS3.p5.5.m5.1a"><mi id="S3.SS1.SSS3.p5.5.m5.1.1" xref="S3.SS1.SSS3.p5.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.5.m5.1b"><ci id="S3.SS1.SSS3.p5.5.m5.1.1.cmml" xref="S3.SS1.SSS3.p5.5.m5.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.5.m5.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p5.5.m5.1d">italic_S</annotation></semantics></math>,</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p6">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f(i)=\begin{cases}U&amp;i\in S\\
\text{NaN}&amp;else\end{cases}" class="ltx_Math" display="block" id="S3.E2.m1.5"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.6" xref="S3.E2.m1.5.6.cmml"><mrow id="S3.E2.m1.5.6.2" xref="S3.E2.m1.5.6.2.cmml"><mi id="S3.E2.m1.5.6.2.2" xref="S3.E2.m1.5.6.2.2.cmml">f</mi><mo id="S3.E2.m1.5.6.2.1" xref="S3.E2.m1.5.6.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.5.6.2.3.2" xref="S3.E2.m1.5.6.2.cmml"><mo id="S3.E2.m1.5.6.2.3.2.1" stretchy="false" xref="S3.E2.m1.5.6.2.cmml">(</mo><mi id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">i</mi><mo id="S3.E2.m1.5.6.2.3.2.2" stretchy="false" xref="S3.E2.m1.5.6.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.5.6.1" xref="S3.E2.m1.5.6.1.cmml">=</mo><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.5.6.3.1.cmml"><mo id="S3.E2.m1.4.4.5" xref="S3.E2.m1.5.6.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S3.E2.m1.4.4.4" rowspacing="0pt" xref="S3.E2.m1.5.6.3.1.cmml"><mtr id="S3.E2.m1.4.4.4a" xref="S3.E2.m1.5.6.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4b" xref="S3.E2.m1.5.6.3.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml">U</mi></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4c" xref="S3.E2.m1.5.6.3.1.cmml"><mrow id="S3.E2.m1.2.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.2.1.cmml"><mi id="S3.E2.m1.2.2.2.2.2.1.2" xref="S3.E2.m1.2.2.2.2.2.1.2.cmml">i</mi><mo id="S3.E2.m1.2.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.2.1.1.cmml">∈</mo><mi id="S3.E2.m1.2.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.2.1.3.cmml">S</mi></mrow></mtd></mtr><mtr id="S3.E2.m1.4.4.4d" xref="S3.E2.m1.5.6.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4e" xref="S3.E2.m1.5.6.3.1.cmml"><mtext id="S3.E2.m1.3.3.3.3.1.1" xref="S3.E2.m1.3.3.3.3.1.1a.cmml">NaN</mtext></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4f" xref="S3.E2.m1.5.6.3.1.cmml"><mrow id="S3.E2.m1.4.4.4.4.2.1" xref="S3.E2.m1.4.4.4.4.2.1.cmml"><mi id="S3.E2.m1.4.4.4.4.2.1.2" xref="S3.E2.m1.4.4.4.4.2.1.2.cmml">e</mi><mo id="S3.E2.m1.4.4.4.4.2.1.1" xref="S3.E2.m1.4.4.4.4.2.1.1.cmml">⁢</mo><mi id="S3.E2.m1.4.4.4.4.2.1.3" xref="S3.E2.m1.4.4.4.4.2.1.3.cmml">l</mi><mo id="S3.E2.m1.4.4.4.4.2.1.1a" xref="S3.E2.m1.4.4.4.4.2.1.1.cmml">⁢</mo><mi id="S3.E2.m1.4.4.4.4.2.1.4" xref="S3.E2.m1.4.4.4.4.2.1.4.cmml">s</mi><mo id="S3.E2.m1.4.4.4.4.2.1.1b" xref="S3.E2.m1.4.4.4.4.2.1.1.cmml">⁢</mo><mi id="S3.E2.m1.4.4.4.4.2.1.5" xref="S3.E2.m1.4.4.4.4.2.1.5.cmml">e</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.6.cmml" xref="S3.E2.m1.5.6"><eq id="S3.E2.m1.5.6.1.cmml" xref="S3.E2.m1.5.6.1"></eq><apply id="S3.E2.m1.5.6.2.cmml" xref="S3.E2.m1.5.6.2"><times id="S3.E2.m1.5.6.2.1.cmml" xref="S3.E2.m1.5.6.2.1"></times><ci id="S3.E2.m1.5.6.2.2.cmml" xref="S3.E2.m1.5.6.2.2">𝑓</ci><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">𝑖</ci></apply><apply id="S3.E2.m1.5.6.3.1.cmml" xref="S3.E2.m1.4.4"><csymbol cd="latexml" id="S3.E2.m1.5.6.3.1.1.cmml" xref="S3.E2.m1.4.4.5">cases</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1">𝑈</ci><apply id="S3.E2.m1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1"><in id="S3.E2.m1.2.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.1"></in><ci id="S3.E2.m1.2.2.2.2.2.1.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.2">𝑖</ci><ci id="S3.E2.m1.2.2.2.2.2.1.3.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3">𝑆</ci></apply><ci id="S3.E2.m1.3.3.3.3.1.1a.cmml" xref="S3.E2.m1.3.3.3.3.1.1"><mtext id="S3.E2.m1.3.3.3.3.1.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1">NaN</mtext></ci><apply id="S3.E2.m1.4.4.4.4.2.1.cmml" xref="S3.E2.m1.4.4.4.4.2.1"><times id="S3.E2.m1.4.4.4.4.2.1.1.cmml" xref="S3.E2.m1.4.4.4.4.2.1.1"></times><ci id="S3.E2.m1.4.4.4.4.2.1.2.cmml" xref="S3.E2.m1.4.4.4.4.2.1.2">𝑒</ci><ci id="S3.E2.m1.4.4.4.4.2.1.3.cmml" xref="S3.E2.m1.4.4.4.4.2.1.3">𝑙</ci><ci id="S3.E2.m1.4.4.4.4.2.1.4.cmml" xref="S3.E2.m1.4.4.4.4.2.1.4">𝑠</ci><ci id="S3.E2.m1.4.4.4.4.2.1.5.cmml" xref="S3.E2.m1.4.4.4.4.2.1.5">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">f(i)=\begin{cases}U&amp;i\in S\\
\text{NaN}&amp;else\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.5d">italic_f ( italic_i ) = { start_ROW start_CELL italic_U end_CELL start_CELL italic_i ∈ italic_S end_CELL end_ROW start_ROW start_CELL NaN end_CELL start_CELL italic_e italic_l italic_s italic_e end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p7">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="N=[f(i)\mid 0\leq i&lt;n]" class="ltx_Math" display="block" id="S3.E3.m1.2"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mi id="S3.E3.m1.2.2.3" xref="S3.E3.m1.2.2.3.cmml">N</mi><mo id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.2.cmml"><mo id="S3.E3.m1.2.2.1.1.2" stretchy="false" xref="S3.E3.m1.2.2.1.2.1.cmml">[</mo><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.2.cmml"><mrow id="S3.E3.m1.2.2.1.1.1.2.2" xref="S3.E3.m1.2.2.1.1.1.2.2.cmml"><mi id="S3.E3.m1.2.2.1.1.1.2.2.2" xref="S3.E3.m1.2.2.1.1.1.2.2.2.cmml">f</mi><mo id="S3.E3.m1.2.2.1.1.1.2.2.1" xref="S3.E3.m1.2.2.1.1.1.2.2.1.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.1.1.1.2.2.3.2" xref="S3.E3.m1.2.2.1.1.1.2.2.cmml"><mo id="S3.E3.m1.2.2.1.1.1.2.2.3.2.1" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.2.2.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">i</mi><mo id="S3.E3.m1.2.2.1.1.1.2.2.3.2.2" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.1.2.1" xref="S3.E3.m1.2.2.1.1.1.2.1.cmml">∣</mo><mn id="S3.E3.m1.2.2.1.1.1.2.3" xref="S3.E3.m1.2.2.1.1.1.2.3.cmml">0</mn></mrow><mo id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml">≤</mo><mi id="S3.E3.m1.2.2.1.1.1.4" xref="S3.E3.m1.2.2.1.1.1.4.cmml">i</mi><mo id="S3.E3.m1.2.2.1.1.1.5" xref="S3.E3.m1.2.2.1.1.1.5.cmml">&lt;</mo><mi id="S3.E3.m1.2.2.1.1.1.6" xref="S3.E3.m1.2.2.1.1.1.6.cmml">n</mi></mrow><mo id="S3.E3.m1.2.2.1.1.3" stretchy="false" xref="S3.E3.m1.2.2.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><eq id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"></eq><ci id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.3">𝑁</ci><apply id="S3.E3.m1.2.2.1.2.cmml" xref="S3.E3.m1.2.2.1.1"><csymbol cd="latexml" id="S3.E3.m1.2.2.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2">delimited-[]</csymbol><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><and id="S3.E3.m1.2.2.1.1.1a.cmml" xref="S3.E3.m1.2.2.1.1.1"></and><apply id="S3.E3.m1.2.2.1.1.1b.cmml" xref="S3.E3.m1.2.2.1.1.1"><leq id="S3.E3.m1.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3"></leq><apply id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2"><csymbol cd="latexml" id="S3.E3.m1.2.2.1.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.2.1">conditional</csymbol><apply id="S3.E3.m1.2.2.1.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2.2"><times id="S3.E3.m1.2.2.1.1.1.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.2.2.1"></times><ci id="S3.E3.m1.2.2.1.1.1.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2.2.2">𝑓</ci><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑖</ci></apply><cn id="S3.E3.m1.2.2.1.1.1.2.3.cmml" type="integer" xref="S3.E3.m1.2.2.1.1.1.2.3">0</cn></apply><ci id="S3.E3.m1.2.2.1.1.1.4.cmml" xref="S3.E3.m1.2.2.1.1.1.4">𝑖</ci></apply><apply id="S3.E3.m1.2.2.1.1.1c.cmml" xref="S3.E3.m1.2.2.1.1.1"><lt id="S3.E3.m1.2.2.1.1.1.5.cmml" xref="S3.E3.m1.2.2.1.1.1.5"></lt><share href="https://arxiv.org/html/2404.17063v1#S3.E3.m1.2.2.1.1.1.4.cmml" id="S3.E3.m1.2.2.1.1.1d.cmml" xref="S3.E3.m1.2.2.1.1.1"></share><ci id="S3.E3.m1.2.2.1.1.1.6.cmml" xref="S3.E3.m1.2.2.1.1.1.6">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">N=[f(i)\mid 0\leq i&lt;n]</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.2d">italic_N = [ italic_f ( italic_i ) ∣ 0 ≤ italic_i &lt; italic_n ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p8">
<p class="ltx_p" id="S3.SS1.SSS3.p8.4">Linear interpolation is then applied to fill in all NaN values in <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p8.1.m1.1"><semantics id="S3.SS1.SSS3.p8.1.m1.1a"><mi id="S3.SS1.SSS3.p8.1.m1.1.1" xref="S3.SS1.SSS3.p8.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p8.1.m1.1b"><ci id="S3.SS1.SSS3.p8.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p8.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p8.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p8.1.m1.1d">italic_N</annotation></semantics></math>. The new array of joint angles <math alttext="F_{new}" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p8.2.m2.1"><semantics id="S3.SS1.SSS3.p8.2.m2.1a"><msub id="S3.SS1.SSS3.p8.2.m2.1.1" xref="S3.SS1.SSS3.p8.2.m2.1.1.cmml"><mi id="S3.SS1.SSS3.p8.2.m2.1.1.2" xref="S3.SS1.SSS3.p8.2.m2.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS3.p8.2.m2.1.1.3" xref="S3.SS1.SSS3.p8.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS3.p8.2.m2.1.1.3.2" xref="S3.SS1.SSS3.p8.2.m2.1.1.3.2.cmml">n</mi><mo id="S3.SS1.SSS3.p8.2.m2.1.1.3.1" xref="S3.SS1.SSS3.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.p8.2.m2.1.1.3.3" xref="S3.SS1.SSS3.p8.2.m2.1.1.3.3.cmml">e</mi><mo id="S3.SS1.SSS3.p8.2.m2.1.1.3.1a" xref="S3.SS1.SSS3.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.p8.2.m2.1.1.3.4" xref="S3.SS1.SSS3.p8.2.m2.1.1.3.4.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p8.2.m2.1b"><apply id="S3.SS1.SSS3.p8.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p8.2.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p8.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p8.2.m2.1.1.2.cmml" xref="S3.SS1.SSS3.p8.2.m2.1.1.2">𝐹</ci><apply id="S3.SS1.SSS3.p8.2.m2.1.1.3.cmml" xref="S3.SS1.SSS3.p8.2.m2.1.1.3"><times id="S3.SS1.SSS3.p8.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS3.p8.2.m2.1.1.3.1"></times><ci id="S3.SS1.SSS3.p8.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS3.p8.2.m2.1.1.3.2">𝑛</ci><ci id="S3.SS1.SSS3.p8.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS3.p8.2.m2.1.1.3.3">𝑒</ci><ci id="S3.SS1.SSS3.p8.2.m2.1.1.3.4.cmml" xref="S3.SS1.SSS3.p8.2.m2.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p8.2.m2.1c">F_{new}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p8.2.m2.1d">italic_F start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> is thus the element-wise sum of <math alttext="F_{orig}" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p8.3.m3.1"><semantics id="S3.SS1.SSS3.p8.3.m3.1a"><msub id="S3.SS1.SSS3.p8.3.m3.1.1" xref="S3.SS1.SSS3.p8.3.m3.1.1.cmml"><mi id="S3.SS1.SSS3.p8.3.m3.1.1.2" xref="S3.SS1.SSS3.p8.3.m3.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS3.p8.3.m3.1.1.3" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS3.p8.3.m3.1.1.3.2" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.2.cmml">o</mi><mo id="S3.SS1.SSS3.p8.3.m3.1.1.3.1" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.p8.3.m3.1.1.3.3" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.3.cmml">r</mi><mo id="S3.SS1.SSS3.p8.3.m3.1.1.3.1a" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.p8.3.m3.1.1.3.4" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.4.cmml">i</mi><mo id="S3.SS1.SSS3.p8.3.m3.1.1.3.1b" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.p8.3.m3.1.1.3.5" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.5.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p8.3.m3.1b"><apply id="S3.SS1.SSS3.p8.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p8.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p8.3.m3.1.1.1.cmml" xref="S3.SS1.SSS3.p8.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p8.3.m3.1.1.2.cmml" xref="S3.SS1.SSS3.p8.3.m3.1.1.2">𝐹</ci><apply id="S3.SS1.SSS3.p8.3.m3.1.1.3.cmml" xref="S3.SS1.SSS3.p8.3.m3.1.1.3"><times id="S3.SS1.SSS3.p8.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.1"></times><ci id="S3.SS1.SSS3.p8.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.2">𝑜</ci><ci id="S3.SS1.SSS3.p8.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.3">𝑟</ci><ci id="S3.SS1.SSS3.p8.3.m3.1.1.3.4.cmml" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.4">𝑖</ci><ci id="S3.SS1.SSS3.p8.3.m3.1.1.3.5.cmml" xref="S3.SS1.SSS3.p8.3.m3.1.1.3.5">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p8.3.m3.1c">F_{orig}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p8.3.m3.1d">italic_F start_POSTSUBSCRIPT italic_o italic_r italic_i italic_g end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p8.4.m4.1"><semantics id="S3.SS1.SSS3.p8.4.m4.1a"><mi id="S3.SS1.SSS3.p8.4.m4.1.1" xref="S3.SS1.SSS3.p8.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p8.4.m4.1b"><ci id="S3.SS1.SSS3.p8.4.m4.1.1.cmml" xref="S3.SS1.SSS3.p8.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p8.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p8.4.m4.1d">italic_N</annotation></semantics></math>. Our algorithm is motivated by the need for a simple and efficient noise algorithm that does not jitter as the user iterates through frames of the animation.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p9">
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F_{new}=[f_{orig}[i]+N[i]\mid 0\leq i&lt;n]" class="ltx_Math" display="block" id="S3.E4.m1.3"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml"><msub id="S3.E4.m1.3.3.3" xref="S3.E4.m1.3.3.3.cmml"><mi id="S3.E4.m1.3.3.3.2" xref="S3.E4.m1.3.3.3.2.cmml">F</mi><mrow id="S3.E4.m1.3.3.3.3" xref="S3.E4.m1.3.3.3.3.cmml"><mi id="S3.E4.m1.3.3.3.3.2" xref="S3.E4.m1.3.3.3.3.2.cmml">n</mi><mo id="S3.E4.m1.3.3.3.3.1" xref="S3.E4.m1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.3.3.3.3.3" xref="S3.E4.m1.3.3.3.3.3.cmml">e</mi><mo id="S3.E4.m1.3.3.3.3.1a" xref="S3.E4.m1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.3.3.3.3.4" xref="S3.E4.m1.3.3.3.3.4.cmml">w</mi></mrow></msub><mo id="S3.E4.m1.3.3.2" xref="S3.E4.m1.3.3.2.cmml">=</mo><mrow id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.2.cmml"><mo id="S3.E4.m1.3.3.1.1.2" stretchy="false" xref="S3.E4.m1.3.3.1.2.1.cmml">[</mo><mrow id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.2.cmml"><mrow id="S3.E4.m1.3.3.1.1.1.2.2" xref="S3.E4.m1.3.3.1.1.1.2.2.cmml"><msub id="S3.E4.m1.3.3.1.1.1.2.2.2" xref="S3.E4.m1.3.3.1.1.1.2.2.2.cmml"><mi id="S3.E4.m1.3.3.1.1.1.2.2.2.2" xref="S3.E4.m1.3.3.1.1.1.2.2.2.2.cmml">f</mi><mrow id="S3.E4.m1.3.3.1.1.1.2.2.2.3" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.2.2.2.3.2" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.2.cmml">o</mi><mo id="S3.E4.m1.3.3.1.1.1.2.2.2.3.1" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.3.3.1.1.1.2.2.2.3.3" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.3.cmml">r</mi><mo id="S3.E4.m1.3.3.1.1.1.2.2.2.3.1a" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.3.3.1.1.1.2.2.2.3.4" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.4.cmml">i</mi><mo id="S3.E4.m1.3.3.1.1.1.2.2.2.3.1b" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.3.3.1.1.1.2.2.2.3.5" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.5.cmml">g</mi></mrow></msub><mo id="S3.E4.m1.3.3.1.1.1.2.2.1" xref="S3.E4.m1.3.3.1.1.1.2.2.1.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.1.1.1.2.2.3.2" xref="S3.E4.m1.3.3.1.1.1.2.2.3.1.cmml"><mo id="S3.E4.m1.3.3.1.1.1.2.2.3.2.1" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.2.2.3.1.1.cmml">[</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">i</mi><mo id="S3.E4.m1.3.3.1.1.1.2.2.3.2.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.2.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.1.2.1" xref="S3.E4.m1.3.3.1.1.1.2.1.cmml">+</mo><mrow id="S3.E4.m1.3.3.1.1.1.2.3" xref="S3.E4.m1.3.3.1.1.1.2.3.cmml"><mrow id="S3.E4.m1.3.3.1.1.1.2.3.2" xref="S3.E4.m1.3.3.1.1.1.2.3.2.cmml"><mi id="S3.E4.m1.3.3.1.1.1.2.3.2.2" xref="S3.E4.m1.3.3.1.1.1.2.3.2.2.cmml">N</mi><mo id="S3.E4.m1.3.3.1.1.1.2.3.2.1" xref="S3.E4.m1.3.3.1.1.1.2.3.2.1.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.1.1.1.2.3.2.3.2" xref="S3.E4.m1.3.3.1.1.1.2.3.2.3.1.cmml"><mo id="S3.E4.m1.3.3.1.1.1.2.3.2.3.2.1" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.2.3.2.3.1.1.cmml">[</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">i</mi><mo id="S3.E4.m1.3.3.1.1.1.2.3.2.3.2.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.2.3.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.1.2.3.1" xref="S3.E4.m1.3.3.1.1.1.2.3.1.cmml">∣</mo><mn id="S3.E4.m1.3.3.1.1.1.2.3.3" xref="S3.E4.m1.3.3.1.1.1.2.3.3.cmml">0</mn></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.3.cmml">≤</mo><mi id="S3.E4.m1.3.3.1.1.1.4" xref="S3.E4.m1.3.3.1.1.1.4.cmml">i</mi><mo id="S3.E4.m1.3.3.1.1.1.5" xref="S3.E4.m1.3.3.1.1.1.5.cmml">&lt;</mo><mi id="S3.E4.m1.3.3.1.1.1.6" xref="S3.E4.m1.3.3.1.1.1.6.cmml">n</mi></mrow><mo id="S3.E4.m1.3.3.1.1.3" stretchy="false" xref="S3.E4.m1.3.3.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.3b"><apply id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3"><eq id="S3.E4.m1.3.3.2.cmml" xref="S3.E4.m1.3.3.2"></eq><apply id="S3.E4.m1.3.3.3.cmml" xref="S3.E4.m1.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.1.cmml" xref="S3.E4.m1.3.3.3">subscript</csymbol><ci id="S3.E4.m1.3.3.3.2.cmml" xref="S3.E4.m1.3.3.3.2">𝐹</ci><apply id="S3.E4.m1.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3"><times id="S3.E4.m1.3.3.3.3.1.cmml" xref="S3.E4.m1.3.3.3.3.1"></times><ci id="S3.E4.m1.3.3.3.3.2.cmml" xref="S3.E4.m1.3.3.3.3.2">𝑛</ci><ci id="S3.E4.m1.3.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.3">𝑒</ci><ci id="S3.E4.m1.3.3.3.3.4.cmml" xref="S3.E4.m1.3.3.3.3.4">𝑤</ci></apply></apply><apply id="S3.E4.m1.3.3.1.2.cmml" xref="S3.E4.m1.3.3.1.1"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.2">delimited-[]</csymbol><apply id="S3.E4.m1.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"><and id="S3.E4.m1.3.3.1.1.1a.cmml" xref="S3.E4.m1.3.3.1.1.1"></and><apply id="S3.E4.m1.3.3.1.1.1b.cmml" xref="S3.E4.m1.3.3.1.1.1"><leq id="S3.E4.m1.3.3.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.3"></leq><apply id="S3.E4.m1.3.3.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2"><plus id="S3.E4.m1.3.3.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.1"></plus><apply id="S3.E4.m1.3.3.1.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2"><times id="S3.E4.m1.3.3.1.1.1.2.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.1"></times><apply id="S3.E4.m1.3.3.1.1.1.2.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.2.2.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.2.2.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.2.2">𝑓</ci><apply id="S3.E4.m1.3.3.1.1.1.2.2.2.3.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3"><times id="S3.E4.m1.3.3.1.1.1.2.2.2.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.1"></times><ci id="S3.E4.m1.3.3.1.1.1.2.2.2.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.2">𝑜</ci><ci id="S3.E4.m1.3.3.1.1.1.2.2.2.3.3.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.3">𝑟</ci><ci id="S3.E4.m1.3.3.1.1.1.2.2.2.3.4.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.4">𝑖</ci><ci id="S3.E4.m1.3.3.1.1.1.2.2.2.3.5.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.2.3.5">𝑔</ci></apply></apply><apply id="S3.E4.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.3.2"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.1.2.2.3.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.3.2.1">delimited-[]</csymbol><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝑖</ci></apply></apply><apply id="S3.E4.m1.3.3.1.1.1.2.3.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.1.2.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3.1">conditional</csymbol><apply id="S3.E4.m1.3.3.1.1.1.2.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3.2"><times id="S3.E4.m1.3.3.1.1.1.2.3.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3.2.1"></times><ci id="S3.E4.m1.3.3.1.1.1.2.3.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3.2.2">𝑁</ci><apply id="S3.E4.m1.3.3.1.1.1.2.3.2.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3.2.3.2"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.1.2.3.2.3.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3.2.3.2.1">delimited-[]</csymbol><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝑖</ci></apply></apply><cn id="S3.E4.m1.3.3.1.1.1.2.3.3.cmml" type="integer" xref="S3.E4.m1.3.3.1.1.1.2.3.3">0</cn></apply></apply><ci id="S3.E4.m1.3.3.1.1.1.4.cmml" xref="S3.E4.m1.3.3.1.1.1.4">𝑖</ci></apply><apply id="S3.E4.m1.3.3.1.1.1c.cmml" xref="S3.E4.m1.3.3.1.1.1"><lt id="S3.E4.m1.3.3.1.1.1.5.cmml" xref="S3.E4.m1.3.3.1.1.1.5"></lt><share href="https://arxiv.org/html/2404.17063v1#S3.E4.m1.3.3.1.1.1.4.cmml" id="S3.E4.m1.3.3.1.1.1d.cmml" xref="S3.E4.m1.3.3.1.1.1"></share><ci id="S3.E4.m1.3.3.1.1.1.6.cmml" xref="S3.E4.m1.3.3.1.1.1.6">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.3c">F_{new}=[f_{orig}[i]+N[i]\mid 0\leq i&lt;n]</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.3d">italic_F start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT = [ italic_f start_POSTSUBSCRIPT italic_o italic_r italic_i italic_g end_POSTSUBSCRIPT [ italic_i ] + italic_N [ italic_i ] ∣ 0 ≤ italic_i &lt; italic_n ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p10">
<p class="ltx_p" id="S3.SS1.SSS3.p10.1">All motion sequences are applied to a Blender<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>A free and open source 3D modeling software. More information is found at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.blender.org/" title="">https://www.blender.org/</a></span></span></span> model before being imported into Unity and converted to Unity Perception-readable <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS3.p10.1.1">AnimationClips</span> files. Example outputs from our posture generation step described above can be found in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.F4" title="In 3.1. Generating Postures ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Generating Wheelchair User Models</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In order to capture synthetic images, we must rig the postures resulting from previous sections onto Unity human models.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For human models, <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">WheelPose</span> enables both the default human models provided by PeopleSansPeople <cite class="ltx_cite ltx_citemacro_citep">(Ebadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib29" title="">2022b</a>)</cite> and randomized humans leveraging the Unity SyntheticHumans <cite class="ltx_cite ltx_citemacro_citep">(Syn, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib5" title="">2023</a>)</cite>, a Unity Perception package using domain randomization to generate unique human models from a sampling of different clothes, body types, sexes, and more. This utilization allows <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">WheelPose</span> to generate unique human models that better capture the wide variety of appearances of real people. We use 8,750 unique human instances using the default SyntheticHumans configuration limited to people over the age of 10 to better reflect the general population of people in wheelchairs <cite class="ltx_cite ltx_citemacro_citep">(Vignier et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib104" title="">2008</a>)</cite>. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.F6" title="In 3.2. Generating Wheelchair User Models ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows 19 examples of these human models.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">We also enable the human models to spawn with different objects (e.g., wheelchairs, crutches, canes, walkers, etc.) in user-defined positions when placed into the environment. For the scope of this project, we focused on wheelchair users and used a realistic wheelchair model, sourced from the Unity Asset Store, scaled by the size of the human model. Finally, the posture of each human model is randomly sampled from the <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.1">AnimationClips</span> generated in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.SS1" title="3.1. Generating Postures ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="197" id="S3.F6.g1" src="extracted/5503605/figures/humanmodel.jpg" width="598"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Example human models used in the synthesis of wheelchair user images.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S3.F6.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.F6.2">Nineteen unique human models in wheelchairs. Each model has a different combination of clothing, age, ethnicity, sex, hairdo, and body size.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Generating the Simulation Environment</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We address <span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">(RQ1.2)</span> by developing a highly configurable simulation environment. We use the PeopleSansPeople (PSP) <cite class="ltx_cite ltx_citemacro_citep">(Ebadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib29" title="">2022b</a>)</cite> base template and its related extension, PSP-HDRI <cite class="ltx_cite ltx_citemacro_citep">(Ebadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib28" title="">2022a</a>)</cite>, as our baseline data generator built in the Unity<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>More information found at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unity.com/" title="">https://unity.com/</a></span></span></span> game engine through the High Definition Render Pipeline (HDRP). PSP is a parametric human image generator that contains a fully developed simulation environment including rigged human models, parameterized lighting and camera systems, occluders, synthetic RGB image outputs, and ground truth annotations. PSP is built on the idea of <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.2">domain randomization</em> <cite class="ltx_cite ltx_citemacro_citep">(Tobin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib93" title="">2017</a>)</cite> where different aspects of a simulation environment are independently randomized to diversify the generated synthetic data, exposing models to a wider array of different environments during training and improving testing accuracy <cite class="ltx_cite ltx_citemacro_citep">(Tremblay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib95" title="">2018</a>; Valtchev and Wu, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib100" title="">2021</a>)</cite>. All domain randomization is implemented through the ”randomizer” paradigm designed in the Unity Perception package <cite class="ltx_cite ltx_citemacro_citep">(Borkman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib16" title="">2021</a>)</cite>, a Unity toolkit to generate synthetic data for computer vision training. Within each scene, individual randomizers are assigned to a specific parameter (i.e. lighting, occluder positions, human poses, etc.) and independently sample parameter values from a uniform distribution. PSP was then updated to Unity 2021.3 and Unity Perception 1.0.0<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>First official release of Unity Perception</span></span></span> which enabled more annotations, more extendable randomizers, and flexibility with other Unity packages. We then added a new background image parameter and its related randomizer for the sampling of user-defined images to be used as a backdrop in each scene. We use this parameter to enable three different sets of background images: PSP default textures, 100 background images randomly sampled from the BG-20k background image dataset <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib60" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib59" title="">2021a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib61" title="">2021c</a>)</cite>, and 100 generative images from Unity SynthHomes <cite class="ltx_cite ltx_citemacro_citep">(Unity Technologies, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib98" title="">2022</a>)</cite>, a dataset generator for photorealistic home interiors. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.T1" title="In A.3. WheelPose Randomizers ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> outlines the statistical distributions of the environment parameters used.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Camera Configuration and Keypoint Annotations</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">A main camera in the Unity scene is used as the primary capture source of all images and annotations. The Unity Perception <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p1.1.1">Perception Camera</em> is used to simulate realistic camera features including focal length and field of view (FOV) and to capture all annotations. The position, rotation, focal length, and FOV of the main camera are set through a series of randomizers with default parameters found in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.T1" title="In A.3. WheelPose Randomizers ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>. The main camera captures RGB, depth, surface normal, and instance segmentation images in <math alttext="1280\times 720" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.1.m1.1"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><mrow id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml"><mn id="S3.SS3.SSS1.p1.1.m1.1.1.2" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml">1280</mn><mo id="S3.SS3.SSS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.SSS1.p1.1.m1.1.1.3" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml">720</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><apply id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1"><times id="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.1"></times><cn id="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.SSS1.p1.1.m1.1.1.2">1280</cn><cn id="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.1.m1.1.1.3">720</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">1280\times 720</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.1.m1.1d">1280 × 720</annotation></semantics></math> for each frame of captured data. Default Unity Perception annotation labelers are placed on the main camera to capture 2D/3D bounding boxes for each human model, object counts, rendered object metadata, semantic segmentation, 2D/3D keypoint locations in COCO format, and percent of human model occluded. Out-of-view and fully occluded human instances are automatically ignored in annotation capture, recording only data on human instances within direct view of the main camera.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Assembling <span class="ltx_text ltx_font_italic" id="S3.SS4.1.1">WheelPose</span> Datasets</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Overall, our pipeline yields 70,000 images for each generated dataset. All data was generated in a Unity 2021.3 project configured with parameters preset to the values listed in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.T1" title="In A.3. WheelPose Randomizers ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>. We ran our data synthesis pipeline on a PC with a 4.2GHz 6-Core/12-Thread AMD Ryzen R5 3600, NVIDIA GTX 1070 8GB VRAM, and 32 GB 3600MHz DDR4 memory for an average generation time of <math alttext="\approx" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mo id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><approx id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\approx</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">≈</annotation></semantics></math> 1 hour and 45 minutes for 10,000 images – which translates to 12 hours and 15 minutes for each dataset. This time includes all steps of the generation process including motion generation, parameter randomization, data capture, label creation, and writing to disk. Examples of generated synthetic images are shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S3.F7" title="In 3.4. Assembling WheelPose Datasets ‣ 3. WheelPose Dataset Synthesis ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>. We open-source our data synthesis pipeline including pose modification and the full configurable Unity 2021.3 project for data generation in <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hilab-open-source/wheelpose" title="">https://github.com/hilab-open-source/wheelpose</a>.</p>
</div>
<figure class="ltx_figure" id="S3.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="111" id="S3.F7.g1" src="extracted/5503605/figures/example_scene.jpg" width="586"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Examples of a scene being generated. Notice the random placement of wheelchair users, different occluder objects, different lighting conditions, and various SynthHome backgrounds. The red, green, and blue arrows represent the camera coordinates which we used to insert randomization in forms of Cartesian translations and Euler rotations.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S3.F7.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.F7.2">Three examples of scene generation in <span class="ltx_text ltx_font_italic" id="S3.F7.2.1">WheelPose</span>. Each example features a third-person example of wheelchair users in front of a backdrop with occluders randomly suspended in the air. Arrows indicating the camera are all placed in different locations and rotations for each image.</p>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Evaluation of <span class="ltx_text ltx_font_italic" id="S4.1.1">WheelPose</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To answer <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">(RQ2)</span> on the benefits and drawbacks of synthetic data, we evaluate the <span class="ltx_text ltx_font_italic" id="S4.p1.1.2">WheelPose</span> pipeline and generated data through three specific methods: 1) human evaluation on realism, 2) statistical analysis of innate dataset characteristics, and 3) evaluation of our generated datasets’ effects on AI model performance. We document the results of these methods in the following sections.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Human Evaluation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We involved real wheelchair users in the loop to evaluate the realism of our synthetic data. In our study, “realism” manifests as <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p1.1.1">ease</span> and <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p1.1.2">frequency</span>.
<span class="ltx_text" id="S4.SS1.p1.1.3" style="color:#000000;">We sent out online surveys and strictly verified users’ eligibility and authenticity manually to prevent scammers. We recruited 13 daily wheelchair users (5 F, 8 M), with ages ranging from 26 to 56 (M=32, SD=9.6), as shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T1" title="In 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</span> <span class="ltx_text" id="S4.SS1.p1.1.4" style="color:#000000;">A key limitation of this research is the limited diversity among the wheelchair user participants. All participants were individuals with spinal cord injuries (SCI), a specific condition that has distinct movement patterns. This represents only a small subsection of the broader wheelchair user population. Despite our efforts to diversify the participant pool by including participants with different levels of SCI, we acknowledge that our participant population is not sufficiently representative to draw statistical insights for wheelchair users with different conditions or bodies than those in this study (e.g., muscular dystrophy, amputations, dwarfism, spinal deformities).</span></p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Demographics of participants (P1-P13) in human evaluation.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S4.T1.1" style="width:433.6pt;height:226.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-118.3pt,61.8pt) scale(0.646992524041332,0.646992524041332) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1.1">ID</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.2.1">Age</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.3.1">Gender</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.4.1">Occupation</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.5.1">SCI Level</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1.6.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.6.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.6.1.1.1">Exercise Routines</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1.7.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.7.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.7.1.1.1">Full Mobility of Arms, Shoulders, and Hands</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.2.1.1">P1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.2.1.2">56</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.2.1.3">M</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.2.1.4">Professor</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.2.1.5">T-12/L-1</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.1.2.1.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.2.1.6.1">
<span class="ltx_p" id="S4.T1.1.1.2.1.6.1.1" style="width:170.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.1.2.1.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.2.1.7.1">
<span class="ltx_p" id="S4.T1.1.1.2.1.7.1.1" style="width:113.8pt;">Yes</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.3.2.1">P2</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.3.2.2">29</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.3.2.3">F</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.3.2.4">Home-maker</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.3.2.5">T-3</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.3.2.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.3.2.6.1">
<span class="ltx_p" id="S4.T1.1.1.3.2.6.1.1" style="width:170.7pt;">Weight lifting, Strolling, and Stretches</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.3.2.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.3.2.7.1">
<span class="ltx_p" id="S4.T1.1.1.3.2.7.1.1" style="width:113.8pt;">Yes</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.4.3.1">P3</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.4.3.2">40</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.4.3.3">M</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.4.3.4">Entrepreneur</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.4.3.5">T-12</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.4.3.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.4.3.6.1">
<span class="ltx_p" id="S4.T1.1.1.4.3.6.1.1" style="width:170.7pt;">Swimming, Cycling, and Gym</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.4.3.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.4.3.7.1">
<span class="ltx_p" id="S4.T1.1.1.4.3.7.1.1" style="width:113.8pt;">Yes</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.4">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.5.4.1">P4</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.5.4.2">40</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.5.4.3">F</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.5.4.4">Self-employed</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.5.4.5">C-5</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.5.4.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.5.4.6.1">
<span class="ltx_p" id="S4.T1.1.1.5.4.6.1.1" style="width:170.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.5.4.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.5.4.7.1">
<span class="ltx_p" id="S4.T1.1.1.5.4.7.1.1" style="width:113.8pt;">No (DASH Score 79.2/100 <cite class="ltx_cite ltx_citemacro_citep">(Hudak et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib51" title="">1996</a>)</cite>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.5">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.6.5.1">P5</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.6.5.2">36</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.6.5.3">M</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.6.5.4">Customer Relations</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.6.5.5">C-5</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.6.5.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.6.5.6.1">
<span class="ltx_p" id="S4.T1.1.1.6.5.6.1.1" style="width:170.7pt;">Stretching and Strength training</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.6.5.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.6.5.7.1">
<span class="ltx_p" id="S4.T1.1.1.6.5.7.1.1" style="width:113.8pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.6">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.7.6.1">P6</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.7.6.2">26</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.7.6.3">M</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.7.6.4">Software Developer</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.7.6.5">Lumbar spinal stenosis</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.7.6.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.7.6.6.1">
<span class="ltx_p" id="S4.T1.1.1.7.6.6.1.1" style="width:170.7pt;">Wheelchair walking</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.7.6.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.7.6.7.1">
<span class="ltx_p" id="S4.T1.1.1.7.6.7.1.1" style="width:113.8pt;">Yes</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8.7">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.8.7.1">P7</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.8.7.2">51</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.8.7.3">M</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.8.7.4">Proctor/Graphic Designer</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.8.7.5">T-12; L1</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.8.7.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.8.7.6.1">
<span class="ltx_p" id="S4.T1.1.1.8.7.6.1.1" style="width:170.7pt;">Gym workouts periodically, Wheelchair Basketball, Wheelchair Tennis, and Pushing long distances</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.8.7.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.8.7.7.1">
<span class="ltx_p" id="S4.T1.1.1.8.7.7.1.1" style="width:113.8pt;">Yes</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.9.8">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.9.8.1">P8</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.9.8.2">26</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.9.8.3">M</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.9.8.4">Librarian</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.9.8.5">C7</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.9.8.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.9.8.6.1">
<span class="ltx_p" id="S4.T1.1.1.9.8.6.1.1" style="width:170.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.9.8.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.9.8.7.1">
<span class="ltx_p" id="S4.T1.1.1.9.8.7.1.1" style="width:113.8pt;">No (DASH Score 40.0/100 <cite class="ltx_cite ltx_citemacro_citep">(Hudak et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib51" title="">1996</a>)</cite>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.10.9">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.10.9.1">P9</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.10.9.2">27</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.10.9.3">F</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.10.9.4">Remote Computer Programmer</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.10.9.5">L5</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.10.9.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.10.9.6.1">
<span class="ltx_p" id="S4.T1.1.1.10.9.6.1.1" style="width:170.7pt;">Arm training using a band</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.10.9.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.10.9.7.1">
<span class="ltx_p" id="S4.T1.1.1.10.9.7.1.1" style="width:113.8pt;">Yes</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.11.10">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.11.10.1">P10</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.11.10.2">27</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.11.10.3">F</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.11.10.4">Teacher</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.11.10.5">Lumbar SCI</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.11.10.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.11.10.6.1">
<span class="ltx_p" id="S4.T1.1.1.11.10.6.1.1" style="width:170.7pt;">Aquatic therapy</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.11.10.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.11.10.7.1">
<span class="ltx_p" id="S4.T1.1.1.11.10.7.1.1" style="width:113.8pt;">Yes</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.12.11">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.12.11.1">P11</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.12.11.2">32</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.12.11.3">F</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.12.11.4">Receptionist</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.12.11.5">Thoracic SCI</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.12.11.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.12.11.6.1">
<span class="ltx_p" id="S4.T1.1.1.12.11.6.1.1" style="width:170.7pt;">Aerobic exercise</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.12.11.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.12.11.7.1">
<span class="ltx_p" id="S4.T1.1.1.12.11.7.1.1" style="width:113.8pt;">Yes</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.13.12">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.13.12.1">P12</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.13.12.2">34</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.13.12.3">M</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.13.12.4">Marketing Manager</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.13.12.5">Sacral SCI</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.13.12.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.13.12.6.1">
<span class="ltx_p" id="S4.T1.1.1.13.12.6.1.1" style="width:170.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T1.1.1.13.12.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.13.12.7.1">
<span class="ltx_p" id="S4.T1.1.1.13.12.7.1.1" style="width:113.8pt;">Yes</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.14.13">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.1.14.13.1">P13</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.1.14.13.2">29</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.1.14.13.3">M</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.1.14.13.4">Freelancer</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.1.14.13.5">Lumbar SCI</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T1.1.1.14.13.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.14.13.6.1">
<span class="ltx_p" id="S4.T1.1.1.14.13.6.1.1" style="width:170.7pt;">Water exercise</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T1.1.1.14.13.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.14.13.7.1">
<span class="ltx_p" id="S4.T1.1.1.14.13.7.1.1" style="width:113.8pt;">Yes</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S4.T1.2">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.T1.3">This table shows demographic information about participants in the human evaluation. The header includes ID, Age, Gender, Occupation, SCI Level, Exercise Routines, and Full Upper Limb Mobility.</p>
</div>
</div>
</figure>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Procedure</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Participation was conducted entirely online, allowing users to contribute at their convenience. Users first answered a questionnaire consisting of demographic information and mobility capability before then evaluating two groups of synthetic motions - HumanML3D motions and Text2Motion motions, using a browser-based user interface (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F8" title="In 4.1.1. Procedure ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>). The motions were presented as animation GIFs of a human skeleton performing a certain movement. We chose to use skeletons rather than a more photorealistic model as an embodiment technique facilitating users to think of these skeletons as tracked motion of their bodies. Related work <cite class="ltx_cite ltx_citemacro_citep">(Fribourg et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib35" title="">2020</a>)</cite> has shown that a dummy avatar provides stronger senses of embodiment comparable to non-personalized realistic avatars. Our uses of the skeletal model aim to shift the focus towards motion, steering attention away from superficial cosmetic details. Users observed one animation clip at a time, simultaneously from four perspectives (45-degree oblique top view, top view, side view, and front view). To navigate through motion clips, users click the previous/next buttons or press the arrow keys. Buttons were designed for participants to select scores for two Likert-Scale questions.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">For each motion, users answered three questions:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Q1:</span> “How difficult is it for you to do this motion?” - by rating from 1 (Cannot perform the sequence at all) to 7 (Without any difficulty).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Q2:</span> “How often do you do this motion?” - by rating from 1 (Never) to 7 (Everyday).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Q3:</span> “Have you seen or do you know of other wheelchair users who perform this motion?” (Yes/No).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1">We used all 100 motions in both HumanML3D and Text2Motion converted motion sets. After finishing the last animation GIF, users proceeded to the other motion group. The question set is consistent for both groups. The order of groups was random. Three users evaluated HumanML3D first, while the others saw Text2Motion first.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p5">
<p class="ltx_p" id="S4.SS1.SSS1.p5.1">After all animations were scored, we followed up with participants via email to better understand the following:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p6">
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">What are the criteria used in your evaluation of a motion’s difficulty?</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">What determines the ”frequency” of performing a motion?</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">What motion do most wheelchair users often perform and what motion do you frequently perform, that did not show up in our dataset?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p7">
<p class="ltx_p" id="S4.SS1.SSS1.p7.1">Participants were paid $20 per hour as compensation for their time. As the study did not enforce a time limit and was purely online, users could take a brief break whenever they wanted as long as the questionnaire remained open.
<span class="ltx_text" id="S4.SS1.SSS1.p7.1.1" style="color:#000000;">Excluding breaks, the study took 1.78 hours on average.</span> The study was evaluated and approved by the Institutional Review Board (IRB) at UCLA.</p>
</div>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="252" id="S4.F8.g1" src="x1.png" width="581"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Screenshot of the human evaluation interface. Note that the four subplots on the left are supposed to show an animation of a human skeleton performing a motion sequence in loops from different perspectives (45-degree oblique top view, top view, side view, and front view). Participants were asked to observe an animation and give Likert-Scale scores and a binary response before moving on to evaluation of the next motion sequence.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S4.F8.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F8.2">Example of human evaluation interface. Four different views of a skeletal model performing one of the selected motions are on the left. On the right, a set of questions on motion difficulty, frequency, and whether they have seen the motion before with multiple choice answers are displayed.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Data Analysis</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">User responses were Likert-Scale scores (1-7 for Q1 and Q2) and binary responses (yes/no for Q3). We first visualized the distribution of data across users (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F9" title="In 4.1.2. Data Analysis ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>). Afterward, we separately ranked the motions based on average ease and frequency scores. We also calculated the correlation between difficulty and frequency.
<span class="ltx_text" id="S4.SS1.SSS2.p1.1.1" style="color:#000000;">From follow-up emails, we collected their comments and performed a thematic analysis of their perception of metrics, and the validity of our dataset. The initial codes were the summary of their rating reasons, which were later merged and discussed.</span>
In the end, we ranked Text2Motion motions by the total frequency and difficulty score and identified the bottom 10% for later use in the model performance evaluation.</p>
</div>
<figure class="ltx_figure" id="S4.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="329" id="S4.F9.g1" src="x2.png" width="747"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>From left to right, Figure (a)(b)(c)(d) are 100% stacked bar charts showing the motion distribution in HumanML3D or Text2Motion, with (a) and (b) depicting the perceived ease and frequency of HumanML3D, while (c) and (d) depicting Text2Motion. In (a) and (c), a score of 1 denotes <span class="ltx_text ltx_font_italic" id="S4.F9.5.1">Cannot perform the sequence at all</span> while a score of 7 denotes <span class="ltx_text ltx_font_italic" id="S4.F9.6.2">Can perform without difficulty</span>. In (b) and (d), 1 means <span class="ltx_text ltx_font_italic" id="S4.F9.7.3">Never</span>, and 7 means <span class="ltx_text ltx_font_italic" id="S4.F9.8.4">Everyday</span>. The two scatter plots, Figure (e) and (f), demonstrate the strong correlation between ease and frequency in both motion groups. Figures (g) and (h) depict stacked bar graphs showing whether participants have seen or known of a wheelchair user who has performed this motion.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S4.F9.9">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F9.10">Four stacked bar graphs with percent of animations from 0% to 100% on the Y axis and the participants on the X axis. Each bar graph has responses 1 - 7 stacked to add up to 100%. Two more scatterplots with frequency on the Y axis from 1 to 7 and ease on the X axis from 1 to 7. Both scatter plots have strong linear correlations. Two final stacked bar graphs with Yes or No frequency on the Y axis and the participant on the X axis. A majority of participants answered yes they have seen most motion.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Results and Findings</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">User perception of the generated dataset (i.e., perceived realism) is a strong indicator of the efficacy of our data generation pipeline in that efficient data generation should only yield data that wheelchair users perceive as being realistic. Investigating deeper causes for the perceived realism of our generated data also inevitably led to insights about wheelchair users. We summarize the findings of our user evaluation in the rest of this section.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p2.1.1">Participants found our synthetic motion sequences realistic.</span> <span class="ltx_text" id="S4.SS1.SSS3.p2.1.2" style="color:#000000;">HumanML3D motions received an average ease score of 4.742 (SD=0.943) and an average frequency score of 4.079 (SD=1.085) across the dataset. Text2Motion motions performed comparably, receiving an average ease score of 4.977 (SD=0.915) and an average frequency score of 4.507 (SD=0.889). </span>
Regarding Q3, for each motion we presented, our participants had seen or knew of other wheelchair users who performed that motion. Specifically, as <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F9" title="In 4.1.2. Data Analysis ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>(g)(h) shows, most participants have seen most motions performed by other wheelchair users.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p3">
<p class="ltx_p" id="S4.SS1.SSS3.p3.1"><span class="ltx_text" id="S4.SS1.SSS3.p3.1.1" style="color:#000000;">Regarding the representativeness of datasets, most participants said they had seen all of the common motions (e.g., “rolling forward”, “driving”, “writing”, “drinking”, “cooking”) they knew about in our datasets. We suspect two factors that account for the outcomes observed in these participants: 1) lack of contextual cues made it challenging for them to recall specific motions, and 2) our embodiment technique facilitated their use of imagination that bridged the gaps between the motions we demonstrated and those they executed in daily tasks. Nevertheless, some other participants commented on popular but missing motions, including “wearing some lower body clothes” (P10), “clapping hands” (P11), “typing on a keyboard” (P13). The rest of the users were unsure about their recollection of the datasets and used general phrasings e.g., “seen almost all”. Our datasets were not comprehensive enough to cover all common motions. We believe user feedback is the key to improving data completeness and representativeness.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p4">
<p class="ltx_p" id="S4.SS1.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p4.1.1">Ease (Q1) and frequency (Q2) of performing a motion vary across participants.</span> From <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F9" title="In 4.1.2. Data Analysis ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>(a) to <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F9" title="In 4.1.2. Data Analysis ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>(d), we observe diversity of perception on ease and frequency across individuals. The horizontal axis is participant ID, the vertical axis is the percentage of motions in the whole group. The sequential color palette depicts the ease of doing a motion from 1 to 7, with 7 being very easy. For example, P4 and P5, who had a higher injury position, rated more motions into the difficult pool. The takeaway is that the ease and frequency of performing a motion are highly personal. A realistic motion dataset should include motions across the entire spectrum while eliminating motions that no wheelchair users will ever perceive as being easy/frequent to perform. However, the result of this user evaluation would change as the size of the participant group grows, which lowers the likelihood of unrealistic motions. Nonetheless, we argue that a realistic dataset should include motions that most wheelchair users would perceive as easy and frequent, to ensure the data quality and avoid introducing new biases, while addressing existing issues in the inclusiveness of data collection. Our human evaluation serves as a reference for conducting user assessments of data quality in motion synthesis.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p5">
<p class="ltx_p" id="S4.SS1.SSS3.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p5.1.1">There was a strong correlation between ease and frequency.</span> The last two scatter plots (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F9" title="In 4.1.2. Data Analysis ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>(e)(f)) showcase a strong positive correlation between ease and frequency. In other words, less difficulty was associated with a higher frequency of usage.
<span class="ltx_text" id="S4.SS1.SSS3.p5.1.2" style="color:#000000;">The Pearson correlation coefficients <cite class="ltx_cite ltx_citemacro_citep">(Schober et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib85" title="">2018</a>)</cite> are respectively 0.956 and 0.959 for Text2Motion and HumanML3D. </span>
This result was expected for motions that are difficult to perform for wheelchair users being less frequently performed because they are often circumvented by alternative motions, which was confirmed by comments from participants later on.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p6">
<p class="ltx_p" id="S4.SS1.SSS3.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p6.1.1">Factors on ease: range of motion, pain, balance, and tiredness.</span>
<span class="ltx_text" id="S4.SS1.SSS3.p6.1.2" style="color:#000000;">Participants evaluated how easy a motion was with several factors. 7 out of 13 people highlighted their range of motion using keywords “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p6.1.2.1">range of motion</span>”, “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p6.1.2.2">being paralyzed</span>”, “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p6.1.2.3">based on my abilities</span>”, “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p6.1.2.4">considering the angles</span>.” Six participants mentioned that pain affected their decisions. </span>
Regarding tiredness, P2 explained “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p6.1.3">Since I have no core muscles, being paralyzed from the nipple line down, a lot I can do, but can’t do for long</span>”. Besides, P2, P4, and P5 emphasized balance. For example, P2 commented “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p6.1.4">My left side is a little higher on my injury, so I struggle a lot with that side, or even staying upright when both hands are in use.</span>” Their perception of ease was reflected in Likert-scale scores in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F9" title="In 4.1.2. Data Analysis ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a> (a)(c).</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p7">
<p class="ltx_p" id="S4.SS1.SSS3.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p7.1.1">Factors on frequency: utility and ease.</span>
How frequently one would make a motion depended on both utility and ease. This explained why the correlation coefficient was strong but not definite (e.g., R=0.99).
An easy motion did not necessarily lead to frequent usage. Participants determined frequency mainly based on their routine. P1 commented, “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p7.1.2">Cooking, cleaning, driving, shopping… based on where/when/how I might use the motion, I decided how often I might actually use the motion</span>.”
<span class="ltx_text" id="S4.SS1.SSS3.p7.1.3" style="color:#000000;">Along with P1, six more users recalled their daily routine when rating for the frequency. P3 and P8 also referred to exercises/therapies to define the frequency of each motion. </span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p8">
<p class="ltx_p" id="S4.SS1.SSS3.p8.1">On the other hand, when difficulty and utility had conflicts, participants were experts in circumventing them with alternative motions if possible. P4 explained, “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p8.1.1">I’m limited in my arm functions with putting hands or arms over my head and behind my back… so I use straps on shoulders and high back on my wheelchair</span>.” P5 also commented on how he used elbows and hands to support himself without abdominal muscles when needed, “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p8.1.2">As a tetraplegia, I don’t really have abs. So, I’m kinda ’crawling’ to get back up. Propping myself up with elbows on knees and such.</span>”</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p9">
<p class="ltx_p" id="S4.SS1.SSS3.p9.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p9.1.1">Sense of embodiment.</span>
Results show that our uses of the skeletal model successfully facilitated the sense of embodiment – participants could think of the human model as their own body, and imagine themselves performing those motions when evaluating the ease and frequency. P1 commented, “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p9.1.2">I mimicked the motions and tried to decide when/how I would use the motion</span>”. Similarly, another comment said, “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p9.1.3">I picture myself doing certain tasks throughout the day, or motions I use for exercise</span>.”
On this note of sense of embodiment facilitation, we also chose not to attach text descriptions to animation clips, but encouraged users’ imagination by allowing them to interpret the visuals on their own.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p10">
<p class="ltx_p" id="S4.SS1.SSS3.p10.1"><span class="ltx_text" id="S4.SS1.SSS3.p10.1.1" style="color:#000000;">We expanded our study and recruited two new amputee participants (U1 and U2, not from P1-P13). They (1F, 1M) both had an artificial leg and used wheelchairs every day. We asked open-ended questions about their expectation of pose estimation, including but not limited to an amputated skeleton, the integration of artificial limbs, and the desired features/interfaces for pose estimation. To increase the sense of embodiment, U1 and U2 both proposed using an amputated model that accurately represented their body. For example, U1 commented, “To be accurate I think the skeleton should be amputated to maintain accurate pose estimations.” Similarly, U2 said the model “should be able to be customized to meet different users with different amputations.”
However, expectations changed when artificial limbs came in. U2 talked about our research’s reflection of his artificial limb, and said “The skeleton should adapt to work together with the artificial part that is added.” Meanwhile, U1 insisted the integration of artificial limbs depended on users, saying “I think should allow users to access settings and have a choice of their own.”
As for the feature/interface, both U1 and U2 mentioned a settings panel to annotate where a person was amputated, and U1 further suggested an option to turn on/off the integration of artificial limbs.</span></p>
</div>
<figure class="ltx_figure" id="S4.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F10.sf1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F10.sf1.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="199" id="S4.F10.sf1.g1" src="extracted/5503605/figures/evaluation/n_bounding_boxes.png" width="275"/><span class="ltx_ERROR undefined" id="S4.F10.sf1.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F10.sf1.3">Histogram showing the density of images from 0 to 0.4 on the Y axis against the number of bounding boxes from 0 to 15 on the X axis. Two lines are shown. <span class="ltx_text ltx_font_italic" id="S4.F10.sf1.3.1">WheelPose</span>-Gen was more evenly distributed from 0-12 bounding boxes at around 0.13 density while COCO peaked at 1 bounding box at 0.38 density and quickly descended to 0.04 at 10 bounding boxes before slightly jumping to 0.10 at 15 bounding boxes.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.sf1.4.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S4.F10.sf1.5.2" style="font-size:80%;">Number of Bounding Boxes per Image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F10.sf2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F10.sf2.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="200" id="S4.F10.sf2.g1" src="extracted/5503605/figures/evaluation/bounding_box_rel_area.png" width="275"/><span class="ltx_ERROR undefined" id="S4.F10.sf2.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F10.sf2.3">Histogram showing the density of images from 0 to 35 on the Y axis and the area of a bounding box relative to the image from 0 to 1 on the X axis. Two lines are shown. Both <span class="ltx_text ltx_font_italic" id="S4.F10.sf2.3.1">WheelPose</span>-Gen and COCO follow a similar exponential decay-shaped distribution, starting at a density of 30 and 38 respectively, and dropping to nearly 0 when the area relative to the image is greater than 0.2.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.sf2.4.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S4.F10.sf2.5.2" style="font-size:80%;">Relative Size of Bounding Box</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F10.sf3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F10.sf3.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="258" id="S4.F10.sf3.g1" src="extracted/5503605/figures/evaluation/bbox_heatmap.png" width="586"/><span class="ltx_ERROR undefined" id="S4.F10.sf3.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F10.sf3.3">Two heatmaps showing the distribution of bounding boxes. <span class="ltx_text ltx_font_italic" id="S4.F10.sf3.3.1">WheelPose</span> has a more oblong distribution stretching upwards into the top edge of the heatmap. COCO has a very circular distribution centered right in the middle of the heatmap with little to no distribution at the edges of the heatmap.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.sf3.4.1.1" style="font-size:80%;">(c)</span> </span><span class="ltx_text" id="S4.F10.sf3.5.2" style="font-size:80%;">Bounding Box Spatial Location Heatmap</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Bounding Box Statistics. All COCO statistics are computed for images that contain at least one person instance in COCO.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F10.sf1" title="In Figure 10 ‣ 4.1.3. Results and Findings ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10(a)</span></a> Number of bounding boxes per image. Images with no human bounding boxes are not counted as COCO is not a purely human dataset.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F10.sf2" title="In Figure 10 ‣ 4.1.3. Results and Findings ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10(b)</span></a> Relative area of each bounding box compared to the image area. Relative area is computed through <math alttext="\sqrt{\text{bounding box area}/\text{image area}}" class="ltx_Math" display="inline" id="S4.F10.3.m1.1"><semantics id="S4.F10.3.m1.1b"><msqrt id="S4.F10.3.m1.1.1" xref="S4.F10.3.m1.1.1.cmml"><mrow id="S4.F10.3.m1.1.1.2" xref="S4.F10.3.m1.1.1.2.cmml"><mtext id="S4.F10.3.m1.1.1.2.2" xref="S4.F10.3.m1.1.1.2.2a.cmml">bounding box area</mtext><mo id="S4.F10.3.m1.1.1.2.1" xref="S4.F10.3.m1.1.1.2.1.cmml">/</mo><mtext id="S4.F10.3.m1.1.1.2.3" xref="S4.F10.3.m1.1.1.2.3a.cmml">image area</mtext></mrow></msqrt><annotation-xml encoding="MathML-Content" id="S4.F10.3.m1.1c"><apply id="S4.F10.3.m1.1.1.cmml" xref="S4.F10.3.m1.1.1"><root id="S4.F10.3.m1.1.1a.cmml" xref="S4.F10.3.m1.1.1"></root><apply id="S4.F10.3.m1.1.1.2.cmml" xref="S4.F10.3.m1.1.1.2"><divide id="S4.F10.3.m1.1.1.2.1.cmml" xref="S4.F10.3.m1.1.1.2.1"></divide><ci id="S4.F10.3.m1.1.1.2.2a.cmml" xref="S4.F10.3.m1.1.1.2.2"><mtext id="S4.F10.3.m1.1.1.2.2.cmml" xref="S4.F10.3.m1.1.1.2.2">bounding box area</mtext></ci><ci id="S4.F10.3.m1.1.1.2.3a.cmml" xref="S4.F10.3.m1.1.1.2.3"><mtext id="S4.F10.3.m1.1.1.2.3.cmml" xref="S4.F10.3.m1.1.1.2.3">image area</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F10.3.m1.1d">\sqrt{\text{bounding box area}/\text{image area}}</annotation><annotation encoding="application/x-llamapun" id="S4.F10.3.m1.1e">square-root start_ARG bounding box area / image area end_ARG</annotation></semantics></math>.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F10.sf3" title="In Figure 10 ‣ 4.1.3. Results and Findings ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10(c)</span></a> Heatmap of bounding box location scaled by image size.
The color of each pixel maps to the likelihood of that corresponding coordinate in an image being bounded by bounding boxes. The peak location of the heatmap is marked with a green <math alttext="+" class="ltx_Math" display="inline" id="S4.F10.4.m2.1"><semantics id="S4.F10.4.m2.1b"><mo id="S4.F10.4.m2.1.1" xref="S4.F10.4.m2.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.F10.4.m2.1c"><plus id="S4.F10.4.m2.1.1.cmml" xref="S4.F10.4.m2.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.F10.4.m2.1d">+</annotation><annotation encoding="application/x-llamapun" id="S4.F10.4.m2.1e">+</annotation></semantics></math>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Statistical Analyses</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We performed a set of statistical analyses to examine how the diversity and size of our dataset compare to the full COCO 2017 persons dataset (training and validation) <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib65" title="">2015</a>)</cite>, selected as our benchmark for its ubiquity in other 2D human pose estimation related research <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib22" title="">2017</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib89" title="">2019</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib111" title="">2023</a>)</cite>. Greater diversity and size in training datasets has been shown to improve machine learning performance <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib38" title="">2019</a>)</cite> and is a common solution to prevent overfitting. We consider the following categories in our analyses: high-level dataset features, bounding box location, size, and number in generated images, keypoint number and occlusion per image and instance, diversity of human poses, and camera placement. For all subsequent dataset statistics, we used the <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">WheelPose</span>-Gen dataset, which was generated with no real-world data using SyntheticHumans models, Text2Motion animations, and SynthHome background images. The other datasets from <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">WheelPose</span> shared similar statistics and were skipped in this validation to avoid redundancy.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>High-Level Dataset Features</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">In total, our dataset has 70,000 images with bounding boxes and keypoint annotations. We chose to generate 70,000 images to mimic the size of the COCO dataset. For reference, a recent effort in synthetic data has proven that a PSP dataset of as little as 49,000 images was found to have meaningful pose estimation improvements in Detectron2 fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Ebadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib29" title="">2022b</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F11.sf1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F11.sf1.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="128" id="S4.F11.sf1.g1" src="extracted/5503605/figures/evaluation/not_visible_kp_probability.png" width="180"/><span class="ltx_ERROR undefined" id="S4.F11.sf1.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F11.sf1.3">Bar graph showing the probability of keypoints being not visible on the Y axis and the 17 different COCO keypoints on the X axis. Two bars for each keypoint. COCO had a much probability for all keypoints, hovering around 0.5 while <span class="ltx_text ltx_font_italic" id="S4.F11.sf1.3.1">WheelPose</span>-Gen stayed around 0.2.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.sf1.4.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S4.F11.sf1.5.2" style="font-size:80%;">Not Visible</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F11.sf2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F11.sf2.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="128" id="S4.F11.sf2.g1" src="extracted/5503605/figures/evaluation/occluded_kp_probability.png" width="180"/><span class="ltx_ERROR undefined" id="S4.F11.sf2.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F11.sf2.3">Bar graph showing the probability of keypoints being occluded on the Y axis and the 17 different COCO keypoints on the X axis. Two bars for each keypoint. <span class="ltx_text ltx_font_italic" id="S4.F11.sf2.3.1">WheelPose</span>-Gen had a much higher probability for all keypoints, hovering around 0.3 while COCO stayed around 0.05.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.sf2.4.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S4.F11.sf2.5.2" style="font-size:80%;">Occluded</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F11.sf3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F11.sf3.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="128" id="S4.F11.sf3.g1" src="extracted/5503605/figures/evaluation/visible_kp_probability.png" width="180"/><span class="ltx_ERROR undefined" id="S4.F11.sf3.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F11.sf3.3">Bar graph showing the probability of keypoints being visible on the Y axis and the 17 different COCO keypoints on the X axis. Two bars for each keypoint. <span class="ltx_text ltx_font_italic" id="S4.F11.sf3.3.1">WheelPose</span>-Gen had a higher probability for most keypoints except the hips, hovering around 0.5 while COCO stayed around 0.4. <span class="ltx_text ltx_font_italic" id="S4.F11.sf3.3.2">WheelPose</span>-Gen hips had a probability of around .35.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.sf3.4.1.1" style="font-size:80%;">(c)</span> </span><span class="ltx_text" id="S4.F11.sf3.5.2" style="font-size:80%;">Visible</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Probability of Occlusion Labelings per Keypoint. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F11.sf1" title="In Figure 11 ‣ 4.2.1. High-Level Dataset Features ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11(a)</span></a> Probability of a keypoint being labeled as ”not visible.”
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F11.sf2" title="In Figure 11 ‣ 4.2.1. High-Level Dataset Features ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11(b)</span></a> Probability of a keypoint being labeled as ”occluded.”
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F11.sf3" title="In Figure 11 ‣ 4.2.1. High-Level Dataset Features ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11(c)</span></a> Probability of a keypoint being labeled as ”visible.” Labeling definitions are taken directly from the COCO occlusion and keypoint labeling standard. <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib65" title="">2015</a>)</cite>.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="199" id="S4.F12.g1" src="extracted/5503605/figures/evaluation/kp_loc_heatmap.png" width="538"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>Heatmap of Five Keypoints Location. Top row: <span class="ltx_text ltx_font_italic" id="S4.F12.4.1">WheelPose</span>-Gen. Bottom row: COCO. All keypoints locations in the heatmap are computed by <math alttext="(\frac{\text{x}-\text{bounding box top corner x}}{\text{bounding box width}},%
\frac{\text{y}-\text{bounding box top corner y}}{\text{bounding box height}})" class="ltx_Math" display="inline" id="S4.F12.2.m1.2"><semantics id="S4.F12.2.m1.2b"><mrow id="S4.F12.2.m1.2.3.2" xref="S4.F12.2.m1.2.3.1.cmml"><mo id="S4.F12.2.m1.2.3.2.1" stretchy="false" xref="S4.F12.2.m1.2.3.1.cmml">(</mo><mfrac id="S4.F12.2.m1.1.1" xref="S4.F12.2.m1.1.1.cmml"><mrow id="S4.F12.2.m1.1.1.2" xref="S4.F12.2.m1.1.1.2.cmml"><mtext id="S4.F12.2.m1.1.1.2.2" xref="S4.F12.2.m1.1.1.2.2a.cmml">x</mtext><mo id="S4.F12.2.m1.1.1.2.1" xref="S4.F12.2.m1.1.1.2.1.cmml">−</mo><mtext id="S4.F12.2.m1.1.1.2.3" xref="S4.F12.2.m1.1.1.2.3a.cmml">bounding box top corner x</mtext></mrow><mtext id="S4.F12.2.m1.1.1.3" xref="S4.F12.2.m1.1.1.3a.cmml">bounding box width</mtext></mfrac><mo id="S4.F12.2.m1.2.3.2.2" xref="S4.F12.2.m1.2.3.1.cmml">,</mo><mfrac id="S4.F12.2.m1.2.2" xref="S4.F12.2.m1.2.2.cmml"><mrow id="S4.F12.2.m1.2.2.2" xref="S4.F12.2.m1.2.2.2.cmml"><mtext id="S4.F12.2.m1.2.2.2.2" xref="S4.F12.2.m1.2.2.2.2a.cmml">y</mtext><mo id="S4.F12.2.m1.2.2.2.1" xref="S4.F12.2.m1.2.2.2.1.cmml">−</mo><mtext id="S4.F12.2.m1.2.2.2.3" xref="S4.F12.2.m1.2.2.2.3a.cmml">bounding box top corner y</mtext></mrow><mtext id="S4.F12.2.m1.2.2.3" xref="S4.F12.2.m1.2.2.3a.cmml">bounding box height</mtext></mfrac><mo id="S4.F12.2.m1.2.3.2.3" stretchy="false" xref="S4.F12.2.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F12.2.m1.2c"><interval closure="open" id="S4.F12.2.m1.2.3.1.cmml" xref="S4.F12.2.m1.2.3.2"><apply id="S4.F12.2.m1.1.1.cmml" xref="S4.F12.2.m1.1.1"><divide id="S4.F12.2.m1.1.1.1.cmml" xref="S4.F12.2.m1.1.1"></divide><apply id="S4.F12.2.m1.1.1.2.cmml" xref="S4.F12.2.m1.1.1.2"><minus id="S4.F12.2.m1.1.1.2.1.cmml" xref="S4.F12.2.m1.1.1.2.1"></minus><ci id="S4.F12.2.m1.1.1.2.2a.cmml" xref="S4.F12.2.m1.1.1.2.2"><mtext id="S4.F12.2.m1.1.1.2.2.cmml" mathsize="70%" xref="S4.F12.2.m1.1.1.2.2">x</mtext></ci><ci id="S4.F12.2.m1.1.1.2.3a.cmml" xref="S4.F12.2.m1.1.1.2.3"><mtext id="S4.F12.2.m1.1.1.2.3.cmml" mathsize="70%" xref="S4.F12.2.m1.1.1.2.3">bounding box top corner x</mtext></ci></apply><ci id="S4.F12.2.m1.1.1.3a.cmml" xref="S4.F12.2.m1.1.1.3"><mtext id="S4.F12.2.m1.1.1.3.cmml" mathsize="70%" xref="S4.F12.2.m1.1.1.3">bounding box width</mtext></ci></apply><apply id="S4.F12.2.m1.2.2.cmml" xref="S4.F12.2.m1.2.2"><divide id="S4.F12.2.m1.2.2.1.cmml" xref="S4.F12.2.m1.2.2"></divide><apply id="S4.F12.2.m1.2.2.2.cmml" xref="S4.F12.2.m1.2.2.2"><minus id="S4.F12.2.m1.2.2.2.1.cmml" xref="S4.F12.2.m1.2.2.2.1"></minus><ci id="S4.F12.2.m1.2.2.2.2a.cmml" xref="S4.F12.2.m1.2.2.2.2"><mtext id="S4.F12.2.m1.2.2.2.2.cmml" mathsize="70%" xref="S4.F12.2.m1.2.2.2.2">y</mtext></ci><ci id="S4.F12.2.m1.2.2.2.3a.cmml" xref="S4.F12.2.m1.2.2.2.3"><mtext id="S4.F12.2.m1.2.2.2.3.cmml" mathsize="70%" xref="S4.F12.2.m1.2.2.2.3">bounding box top corner y</mtext></ci></apply><ci id="S4.F12.2.m1.2.2.3a.cmml" xref="S4.F12.2.m1.2.2.3"><mtext id="S4.F12.2.m1.2.2.3.cmml" mathsize="70%" xref="S4.F12.2.m1.2.2.3">bounding box height</mtext></ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.F12.2.m1.2d">(\frac{\text{x}-\text{bounding box top corner x}}{\text{bounding box width}},%
\frac{\text{y}-\text{bounding box top corner y}}{\text{bounding box height}})</annotation><annotation encoding="application/x-llamapun" id="S4.F12.2.m1.2e">( divide start_ARG x - bounding box top corner x end_ARG start_ARG bounding box width end_ARG , divide start_ARG y - bounding box top corner y end_ARG start_ARG bounding box height end_ARG )</annotation></semantics></math>. The heatmap is normalized according to the size of the dataset.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F12.5">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F12.6">Ten heatmaps depicting the spatial distribution of 5 keypoints with respect to the bounding box for <span class="ltx_text ltx_font_italic" id="S4.F12.6.1">WheelPose</span>-Gen and COCO. The distributions were as follows. Nose: <span class="ltx_text ltx_font_italic" id="S4.F12.6.2">WheelPose</span>-Gen and COCO both had a majority of the distribution fall at the top of the heatmap. Wrists: <span class="ltx_text ltx_font_italic" id="S4.F12.6.3">WheelPose</span>-Gen had a distribution across the entire top half of the heatmap while COCO centered around the left/right edge of the corresponding-sided wrist. COCO was also asymmetrical where the distribution peaked on the edge of the corresponding sided wrist. Hips: <span class="ltx_text ltx_font_italic" id="S4.F12.6.4">WheelPose</span>-Gen had hips centered in the middle of the heatmap with little deviation while COCO had hips centered around their corresponding side in the middle of the heatmap and deviated all the way to the bottom of the heatmap in a near T shape.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">Our dataset contains 296,508 human instances, of which 271,803 instances have annotated keypoint labeling. Note that not all human instances have annotated keypoints because some human instances had no joints within the camera’s view despite still being visible. In comparison, COCO has 66,808 images with 273,469 human instances, of which 156,165 have annotated keypoints. This difference in human instances and annotated keypoints are both due to out-of-view keypoints like in our dataset, and a lack of keypoint labels due to human labeling errors.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Bounding Boxes</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.2">We analyze the bounding box annotations by generating a set of statistics comparing <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p1.2.1">WheelPose</span>-Gen against COCO (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F10" title="In 4.1.3. Results and Findings ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10</span></a>) to evaluate the diversity of the frequency, placement, and size of human instances. We find that <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p1.2.2">WheelPose</span>-Gen contains a more even distribution of the number of bounding boxes, or human instances, per image compared to COCO, indicating a greater diversity of images featuring wheelchair users in differently sized groups (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F10.sf1" title="In Figure 10 ‣ 4.1.3. Results and Findings ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10(a)</span></a>). The COCO dataset does however have a higher concentration of images with large amounts of human instances within them, most likely due to the number of images depicting crowds of people. We also calculated the area of the bounding box relative to the overall image size to analyze the size of individual human instances (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F10.sf2" title="In Figure 10 ‣ 4.1.3. Results and Findings ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10(b)</span></a>). Here, we observe that our bounding boxes have slightly more evenly distributed sizes in relation to the image size when compared to COCO. Note that our dataset consists of images of a uniform size (<math alttext="1280\times 720" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.1.m1.1"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mrow id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml">1280</mn><mo id="S4.SS2.SSS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS2.p1.1.m1.1.1.3" xref="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml">720</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"><times id="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.1"></times><cn id="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.SSS2.p1.1.m1.1.1.2">1280</cn><cn id="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.1.m1.1.1.3">720</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">1280\times 720</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.1.m1.1d">1280 × 720</annotation></semantics></math>), and COCO contains a wide variety of different image sizes up to a maximum size of <math alttext="640\times 640" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.2.m2.1"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mrow id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS2.p1.2.m2.1.1.2" xref="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml">640</mn><mo id="S4.SS2.SSS2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS2.p1.2.m2.1.1.3" xref="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><apply id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1"><times id="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.1"></times><cn id="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.SSS2.p1.2.m2.1.1.2">640</cn><cn id="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.2.m2.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">640\times 640</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.2.m2.1d">640 × 640</annotation></semantics></math>. Thus, images with the same relative size in relation to the image dimensions are still a higher definition in <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p1.2.3">WheelPose</span>-Gen comparatively.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">We then analyze the spatial distribution of bounding boxes by plotting a heatmap of bounding box locations for both <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p2.1.1">WheelPose</span>-Gen and COCO (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F10.sf3" title="In Figure 10 ‣ 4.1.3. Results and Findings ‣ 4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10(c)</span></a>). We note that the location of bounding boxes is a direct product of the human generation, occluder, and camera randomizers and acts as a quantification of their effects. For both datasets, we overlay the bounding boxes with their location scaled by the overall size of the image. For the COCO dataset, we observe a majority of bounding boxes are centered in the middle of the image. We also observe that <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p2.1.2">WheelPose</span>-Gen has a wider bounding box distribution with more spreading into the top edge of the image compared to COCO, indicating our randomization parameters create a more even spread of human instances across the image with more examples of humans at the edge of the camera. The center of the distribution of bounding boxes in <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p2.1.3">WheelPose</span>-Gen is also slightly higher in the image than that of COCO.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Keypoints</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">We first measure the probability of a keypoint to be one of the three predefined COCO occlusion levels (not visible, occluded, visible) in <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p1.1.1">WheelPose</span>-Gen and COCO as further quantification of the effects of the randomizers listed previously. In the context of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p1.1.2">WheelPose</span>, not visible is when a keypoint is not in the image and has no prediction, occluded is when a keypoint is in the image but not visible (e.g., behind an object), and visible is when a keypoint is seen in an image. Here we see that <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p1.1.3">WheelPose</span>-Gen displays a significantly smaller probability of having nonvisible keypoints and a more uniform distribution of keypoints compared to COCO (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F11.sf1" title="In Figure 11 ‣ 4.2.1. High-Level Dataset Features ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11(a)</span></a>). We also notice that <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p1.1.4">WheelPose</span>-Gen has a far higher probability for a keypoint to be labeled as occluded compared to COCO, especially in the hips (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F11.sf2" title="In Figure 11 ‣ 4.2.1. High-Level Dataset Features ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11(b)</span></a>). This can be explained by the different methods both datasets used in keypoint annotation. While <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p1.1.5">WheelPose</span>-Gen uses a self-occlusion labeler defined in PSP which computes the distance between each keypoint and the closest visible part of the object within a threshold to determine occlusion labeling <cite class="ltx_cite ltx_citemacro_citep">(Ebadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib29" title="">2022b</a>)</cite>, COCO is a fully human-labeled dataset. Within human annotators, there are variations between how an annotator might define and classify as occluded and not visible. <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p1.1.6">WheelPose</span> does not suffer from the same issue as the labeler has information on the full 3D scene and can precisely identify every keypoint location over a human annotator who only has access to a single 2D view with no additional context. This phenomenon can be further seen in the probability of keypoint visibility, where <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p1.1.7">WheelPose</span>-Gen displayed a higher probability for all keypoints except the two keypoints on hips (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F11.sf3" title="In Figure 11 ‣ 4.2.1. High-Level Dataset Features ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11(c)</span></a>) where many hip keypoints are labeled as occluded due to the self-occlusion of the wheelchair.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">We evaluate the diversity of our poses by creating a heatmap of keypoint annotations locations scaled by the corresponding bounding box in <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p2.1.1">WheelPose</span>-Gen and COCO (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F12" title="In 4.2.1. High-Level Dataset Features ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">12</span></a>). <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p2.1.2">WheelPose</span>-Gen displays a wider distribution of potential keypoint locations in upper body keypoints compared to COCO. Lower body keypoints display a smaller distribution due to the limitations of postures in a wheelchair. We see for asymmetrical keypoints (left/right wrist), <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p2.1.3">WheelPose</span>-Gen is far more evenly distributed across the X axis compared to COCO, a clear indication of a more even distribution of front, side, and back-facing human instances. We also notice a smaller Y-axis distribution in <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p2.1.4">WheelPose</span>-Gen due to the presence of the wheelchair limiting potential movements up and down.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4. </span>Camera</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">Finally, we quantify the variations in our camera placement and rotation. Recent studies have shown the critical impact of diverse camera angles on model performance in 3D human recovery problems <cite class="ltx_cite ltx_citemacro_citep">(Cai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib19" title="">2022</a>; Madan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib69" title="">2021</a>; Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib76" title="">2023</a>)</cite>. We visualize our diversity of camera angles and distances and observe a wide distribution of potential elevations, azimuths, and distances (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F13" title="In 4.2.4. Camera ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13</span></a>). We then sample a set of camera locations relative to individual human instances and visualize their angle around the instance to observe full coverage of camera angles all around instances (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F13.sf4" title="In Figure 13 ‣ 4.2.4. Camera ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13(d)</span></a>). All visualizations indicate a loosely followed Gaussian distribution with a wide variety of different camera angles. We do not compare these statistics with COCO since it does not include camera configuration information. Our camera configuration parameters can be found in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.T1" title="In A.3. WheelPose Randomizers ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F13.sf1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F13.sf1.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="206" id="S4.F13.sf1.g1" src="extracted/5503605/figures/evaluation/cam_elevation.png" width="269"/><span class="ltx_ERROR undefined" id="S4.F13.sf1.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F13.sf1.3">Histogram of camera elevation with density from 0 to 0.5 on the Y axis and angles in radians from -1.57 to 1.57 on the X axis. The distribution roughly follows a normal distribution but peaks twice to 0.5 density at -0.5 and 0.5 radians before dipping to 0.4 at 0 radians.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F13.sf1.4.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S4.F13.sf1.5.2" style="font-size:80%;">Camera Elevation</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F13.sf2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F13.sf2.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="206" id="S4.F13.sf2.g1" src="extracted/5503605/figures/evaluation/cam_azimuth.png" width="269"/><span class="ltx_ERROR undefined" id="S4.F13.sf2.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F13.sf2.3">Histogram of camera azimuth with density from 0 to 0.5 on the Y axis and angles in radians from -1.57 to 1.57 on the X axis. The distribution roughly follows a normal distribution but peaks twice to 0.5 density at -0.5 and 0.5 radians before dipping to 0.45 at 0 radians.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F13.sf2.4.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S4.F13.sf2.5.2" style="font-size:80%;">Camera Azimuth</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F13.sf3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F13.sf3.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="203" id="S4.F13.sf3.g1" src="extracted/5503605/figures/evaluation/cam_distance.png" width="269"/><span class="ltx_ERROR undefined" id="S4.F13.sf3.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F13.sf3.3">Histogram of camera distance with density from 0 to 0.12 on the Y axis and distance in meters from 7.5 to 27.5 on the X axis. The distribution roughly follows a normal distribution peaking at 20 meters with a density of about 0.11.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F13.sf3.4.1.1" style="font-size:80%;">(c)</span> </span><span class="ltx_text" id="S4.F13.sf3.5.2" style="font-size:80%;">Camera Distance</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F13.sf4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F13.sf4.2">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="227" id="S4.F13.sf4.g1" src="extracted/5503605/figures/evaluation/cam_angles.png" width="228"/><span class="ltx_ERROR undefined" id="S4.F13.sf4.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F13.sf4.3">Visualization of potential camera angles. A human model in a wheelchair in T-pose is surrounded by a set of points on the surface of a sphere representing a potential camera angle. The human model has camera angles surrounding all sides of it, creating an evenly distributed sphere of camera angles.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F13.sf4.4.1.1" style="font-size:80%;">(d)</span> </span><span class="ltx_text" id="S4.F13.sf4.5.2" style="font-size:80%;">Camera Potential Angles</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>Distribution of Potential Camera Angles and Distances.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F13.sf1" title="In Figure 13 ‣ 4.2.4. Camera ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13(a)</span></a> Distribution of elevation angle (up-down, positive indicating a camera above the nose and looking down).
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F13.sf2" title="In Figure 13 ‣ 4.2.4. Camera ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13(b)</span></a> Distribution of azimuth angle (left-right, positive indicating a camera is to the right looking left) distribution.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F13.sf3" title="In Figure 13 ‣ 4.2.4. Camera ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13(c)</span></a> Distribution of camera distance to a human instance.
<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F13.sf4" title="In Figure 13 ‣ 4.2.4. Camera ‣ 4.2. Statistical Analyses ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13(d)</span></a> Visualization of potential camera angles. Computed by sampling camera location relative to human instance every 100 human instances and visualizing the corresponding unit vector.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Model Performance Evaluation</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>Testing Dataset</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">There currently does not exist an image dataset focused primarily on wheelchair users. For the testing of our system, we create a new dataset of 2,464 images of wheelchair users collected from 84 <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.1.1">YouTube</span> videos in a similar data collection process as other computer vision works <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib8" title="">2014</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib105" title="">2019</a>)</cite>. A set of 16 action classes consisting of common daily tasks wheelchair and able-bodied users both perform and unique wheelchair sports were selected (e.g., talking, basketball, rugby, dancing, etc.). Action classes are listed in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.T2" title="In A.4. Testing Dataset Action Classes ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>. Videos are collected through keyword searches revolving around each of the action classes. Annotators iterate through 500 equally spaced frames (minimum one-second intervals) from each video and identify frames with poses and settings sufficiently different from the previously collected images. Annotators ensure that there is a wheelchair user within view for every collected frame. Crowd worker involvement is then utilized to annotate bounding boxes and keypoint locations on collected images. Researchers manually validated results from this process for accuracy. Examples of this dataset can be found in our open-source repository at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hilab-open-source/wheelpose" title="">https://github.com/hilab-open-source/wheelpose</a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Ablation trials with configurations of each dataset and results in terms of the mean Average Precision (mAP) in the bounding box (BB) and keypoint (KP) detection tasks.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Dataset</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Human Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">Background</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1">Animation</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.1">BB mAP</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.6.1">KP mAP</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.2.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.2.2.1.1" style="color:#000000;">WheelPose<span class="ltx_text ltx_font_upright" id="S4.T2.1.2.2.1.1.1">-base</span></span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.2.2"><span class="ltx_text" id="S4.T2.1.2.2.2.1" style="color:#000000;">PSP default</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.2.3"><span class="ltx_text" id="S4.T2.1.2.2.3.1" style="color:#000000;">PSP default</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.2.4"><span class="ltx_text" id="S4.T2.1.2.2.4.1" style="color:#000000;">HumanML3D</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.2.5"><span class="ltx_text" id="S4.T2.1.2.2.5.1" style="color:#000000;">68.68</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.1.2.2.6"><span class="ltx_text" id="S4.T2.1.2.2.6.1" style="color:#000000;">65.18</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.3.3.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.3.3.1.1" style="color:#000000;">WheelPose<span class="ltx_text ltx_font_upright" id="S4.T2.1.3.3.1.1.1">-SH</span></span></th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.3.2"><span class="ltx_text" id="S4.T2.1.3.3.2.1" style="color:#000000;">SyntheticHumans</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.3.3"><span class="ltx_text" id="S4.T2.1.3.3.3.1" style="color:#000000;">PSP default</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.3.4"><span class="ltx_text" id="S4.T2.1.3.3.4.1" style="color:#000000;">HumanML3D</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.3.5"><span class="ltx_text" id="S4.T2.1.3.3.5.1" style="color:#000000;">68.53</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.3.3.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.3.6.1" style="color:#000000;">68.04</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.4.1">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.4.4.1.1">WheelPose</span>-t2m</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.4.2">PSP default</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.4.3">PSP default</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.4.4">Text2Motion</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.4.5">68.95</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.4.4.6">64.83</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.5.1">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.5.5.1.1">WheelPose</span>-t2mR10</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.5.2">PSP default</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.5.3">PSP default</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.5.4">Text2Motion random 10% removed</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.5.5">68.97</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.5.5.6">65.19</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.6.1">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.6.6.1.1">WheelPose</span>-t2mHE10</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.6.2">PSP default</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.6.3">PSP default</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.6.4">Text2Motion HE 10% removed</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.6.5"><span class="ltx_text" id="S4.T2.1.6.6.5.1" style="color:#000000;">69.36</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.6.6.6"><span class="ltx_text" id="S4.T2.1.6.6.6.1" style="color:#000000;">65.53</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.7.7.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.7.7.1.1" style="color:#000000;">WheelPose<span class="ltx_text ltx_font_upright" id="S4.T2.1.7.7.1.1.1">-SB</span></span></th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.7.7.2"><span class="ltx_text" id="S4.T2.1.7.7.2.1" style="color:#000000;">PSP default</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.7.7.3"><span class="ltx_text" id="S4.T2.1.7.7.3.1" style="color:#000000;">SynthHomes</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.7.7.4"><span class="ltx_text" id="S4.T2.1.7.7.4.1" style="color:#000000;">HumanML3D</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.7.7.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.7.5.1" style="color:#000000;">69.75</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.7.7.6"><span class="ltx_text" id="S4.T2.1.7.7.6.1" style="color:#000000;">66.60</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.8.8.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.8.8.1.1" style="color:#000000;">WheelPose<span class="ltx_text ltx_font_upright" id="S4.T2.1.8.8.1.1.1">-RB</span></span></th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.8.2"><span class="ltx_text" id="S4.T2.1.8.8.2.1" style="color:#000000;">PSP default</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.8.3"><span class="ltx_text" id="S4.T2.1.8.8.3.1" style="color:#000000;">BG-20K</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.8.4"><span class="ltx_text" id="S4.T2.1.8.8.4.1" style="color:#000000;">HumanML3D</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.8.5"><span class="ltx_text" id="S4.T2.1.8.8.5.1" style="color:#000000;">64.44</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.8.8.6"><span class="ltx_text" id="S4.T2.1.8.8.6.1" style="color:#000000;">63.89</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.9.9.1">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.9.9.1.1">WheelPose</span>-Gen</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.9.9.2">SyntheticHumans</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.9.9.3">SynthHomes</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.9.9.4">Text2Motion</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.9.9.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.9.9.5.1">69.71</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.1.9.9.6">67.53</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.10.10.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.10.10.1.1" style="color:#000000;">WheelPose<span class="ltx_text ltx_font_upright" id="S4.T2.1.10.10.1.1.1">-Opt</span></span></th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.10.10.2"><span class="ltx_text" id="S4.T2.1.10.10.2.1" style="color:#000000;">SyntheticHumans</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.10.10.3"><span class="ltx_text" id="S4.T2.1.10.10.3.1" style="color:#000000;">SynthHomes</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.10.10.4"><span class="ltx_text" id="S4.T2.1.10.10.4.1" style="color:#000000;">Text2Motion HE 10% removed</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.10.10.5"><span class="ltx_text" id="S4.T2.1.10.10.5.1" style="color:#000000;">69.58</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.10.10.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.10.10.6.1" style="color:#000000;">67.96</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.11.11.1">ImageNet (baseline)</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.11.11.2">N.A.</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.11.11.3">N.A.</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.11.11.4">N.A.</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.11.11.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.11.11.5.1">35.19</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.1.11.11.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.11.11.6.1">63.11</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.12.12.1">PSP (baseline)</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.12.12.2">PSP Default</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.12.12.3">PSP Default</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.12.12.4">PSP Default Able-Bodied</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.12.12.5">31.03</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T2.1.12.12.6">53.26</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span>Training Strategy</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">Similar to training outlined in PSP-HDRI <cite class="ltx_cite ltx_citemacro_citep">(Ebadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib28" title="">2022a</a>)</cite>, all of our models in this evaluation utilized ResNet-50 <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib43" title="">2016b</a>)</cite> plus Feature Pyramid Network (FPN) <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib64" title="">2017</a>)</cite> backbones. Additionally, these models were fine-tuned using the starting weights and framework of the Detectron2 ImageNet Keypoint R-CNN R50-FPN variant <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib42" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">We create each model in the same way: fine-tuning with the relevant dataset on the backbone described previously. We opted for a simple training approach, setting the initial learning rate to 0.000025 for 20 epochs, and then lowering the learning rate by a factor of 10 for an additional 10 epochs. For the first 1000 iterations we also conduct a linear warm-up of the learning rate to its starting value, slowly increasing the learning rate to its starting value. The momentum was set to 0.9 and the weight decay was set to 0.0001. All training runs were completed using a 4.2GHz 16-core/32-thread AMD Ryzen Threadripper PRO 3955WX CPU, 2 NVIDIA RTX A5500 24GB VRAM, and 256 GB 3200MHz DDR4 memory with a mini-batch size of 13 images per GPU, where each image was normalized using the mean pixel value and standard deviation of the ImageNet base model. For each model, we checkpointed the model weights during every epoch and selected the epoch with the best-performing keypoint AP to report in evaluation.
This evaluation scheme was consistent across baseline datasets and our synthetic datasets.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p3">
<p class="ltx_p" id="S4.SS3.SSS2.p3.1">We note that <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p3.1.1">WheelPose</span> fine-tuned models and individual human evaluators may not agree with each other on where a keypoint is due to the innate difference between a modeled person and a real person. Changes in the way <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p3.1.2">WheelPose</span> defines keypoints can alter where a keypoint is predicted and the metrics computed in the coming sections as seen in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.SS5" title="A.5. Impacts of Keypoint Location Definitions ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.5</span></a>. We attempted to minimize these differences as much as possible through realistic keypoint definitions in Unity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3. </span>Ablation Testing Strategy</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">We address <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.p1.1.1">(RQ2.1)</span> through ablation testing<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Ablation testing involves the removal of certain components to understand the contribution of the component to the overall performance of an AI system</span></span></span> on a set of selected domain randomization parameters, including animations, backgrounds, and human models, to better understand the performance impacts of select data generation parameters.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p2">
<p class="ltx_p" id="S4.SS3.SSS3.p2.4"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.p2.4.1">Configuration.</span>
Regarding the <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p2.4.2">human model</span>, we analyze the effects on the model performance of using: 1) PSP default human models (PSP Default), and 2) SyntheticHuman human models (SyntheticHumans).
As to the <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p2.4.3">background</span>, we compare between 1) PSP default texture backgrounds (PSP Default), 2) SynthHomes background images (SynthHomes), and 3) BG-20K real background images (BG-20K).
Finally, regarding the <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p2.4.4">motion sequence</span>, we compare between 1) HumanML3D animations (HumanML3D), 2) Text2Motion animations (Text2Motion), 3) Text2Motion with <math alttext="10\%" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.1.m1.1"><semantics id="S4.SS3.SSS3.p2.1.m1.1a"><mrow id="S4.SS3.SSS3.p2.1.m1.1.1" xref="S4.SS3.SSS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.SSS3.p2.1.m1.1.1.2" xref="S4.SS3.SSS3.p2.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS3.SSS3.p2.1.m1.1.1.1" xref="S4.SS3.SSS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.1.m1.1b"><apply id="S4.SS3.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS3.p2.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.SSS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.SSS3.p2.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.1.m1.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.1.m1.1d">10 %</annotation></semantics></math> of animations randomly removed (Text2Motion random <math alttext="10\%" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.2.m2.1"><semantics id="S4.SS3.SSS3.p2.2.m2.1a"><mrow id="S4.SS3.SSS3.p2.2.m2.1.1" xref="S4.SS3.SSS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.SSS3.p2.2.m2.1.1.2" xref="S4.SS3.SSS3.p2.2.m2.1.1.2.cmml">10</mn><mo id="S4.SS3.SSS3.p2.2.m2.1.1.1" xref="S4.SS3.SSS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.2.m2.1b"><apply id="S4.SS3.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.SSS3.p2.2.m2.1.1.1">percent</csymbol><cn id="S4.SS3.SSS3.p2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS3.SSS3.p2.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.2.m2.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.2.m2.1d">10 %</annotation></semantics></math> removed), and 4) Text2Motion animations with the bottom <math alttext="10\%" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.3.m3.1"><semantics id="S4.SS3.SSS3.p2.3.m3.1a"><mrow id="S4.SS3.SSS3.p2.3.m3.1.1" xref="S4.SS3.SSS3.p2.3.m3.1.1.cmml"><mn id="S4.SS3.SSS3.p2.3.m3.1.1.2" xref="S4.SS3.SSS3.p2.3.m3.1.1.2.cmml">10</mn><mo id="S4.SS3.SSS3.p2.3.m3.1.1.1" xref="S4.SS3.SSS3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.3.m3.1b"><apply id="S4.SS3.SSS3.p2.3.m3.1.1.cmml" xref="S4.SS3.SSS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.SSS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.SSS3.p2.3.m3.1.1.1">percent</csymbol><cn id="S4.SS3.SSS3.p2.3.m3.1.1.2.cmml" type="integer" xref="S4.SS3.SSS3.p2.3.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.3.m3.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.3.m3.1d">10 %</annotation></semantics></math> of animations in total ease and frequency from human evaluations (HE) (Text2Motion HE <math alttext="10\%" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.4.m4.1"><semantics id="S4.SS3.SSS3.p2.4.m4.1a"><mrow id="S4.SS3.SSS3.p2.4.m4.1.1" xref="S4.SS3.SSS3.p2.4.m4.1.1.cmml"><mn id="S4.SS3.SSS3.p2.4.m4.1.1.2" xref="S4.SS3.SSS3.p2.4.m4.1.1.2.cmml">10</mn><mo id="S4.SS3.SSS3.p2.4.m4.1.1.1" xref="S4.SS3.SSS3.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.4.m4.1b"><apply id="S4.SS3.SSS3.p2.4.m4.1.1.cmml" xref="S4.SS3.SSS3.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS3.SSS3.p2.4.m4.1.1.1.cmml" xref="S4.SS3.SSS3.p2.4.m4.1.1.1">percent</csymbol><cn id="S4.SS3.SSS3.p2.4.m4.1.1.2.cmml" type="integer" xref="S4.SS3.SSS3.p2.4.m4.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.4.m4.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.4.m4.1d">10 %</annotation></semantics></math> removed). <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T2" title="In 4.3.1. Testing Dataset ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows this list of datasets and their configurations.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p3">
<p class="ltx_p" id="S4.SS3.SSS3.p3.2">We use combinations of these parameters to assemble a set of datasets of 70,000 images each. Each dataset consisted of <math alttext="\approx 65,000" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p3.1.m1.2"><semantics id="S4.SS3.SSS3.p3.1.m1.2a"><mrow id="S4.SS3.SSS3.p3.1.m1.2.3" xref="S4.SS3.SSS3.p3.1.m1.2.3.cmml"><mi id="S4.SS3.SSS3.p3.1.m1.2.3.2" xref="S4.SS3.SSS3.p3.1.m1.2.3.2.cmml"></mi><mo id="S4.SS3.SSS3.p3.1.m1.2.3.1" xref="S4.SS3.SSS3.p3.1.m1.2.3.1.cmml">≈</mo><mrow id="S4.SS3.SSS3.p3.1.m1.2.3.3.2" xref="S4.SS3.SSS3.p3.1.m1.2.3.3.1.cmml"><mn id="S4.SS3.SSS3.p3.1.m1.1.1" xref="S4.SS3.SSS3.p3.1.m1.1.1.cmml">65</mn><mo id="S4.SS3.SSS3.p3.1.m1.2.3.3.2.1" xref="S4.SS3.SSS3.p3.1.m1.2.3.3.1.cmml">,</mo><mn id="S4.SS3.SSS3.p3.1.m1.2.2" xref="S4.SS3.SSS3.p3.1.m1.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p3.1.m1.2b"><apply id="S4.SS3.SSS3.p3.1.m1.2.3.cmml" xref="S4.SS3.SSS3.p3.1.m1.2.3"><approx id="S4.SS3.SSS3.p3.1.m1.2.3.1.cmml" xref="S4.SS3.SSS3.p3.1.m1.2.3.1"></approx><csymbol cd="latexml" id="S4.SS3.SSS3.p3.1.m1.2.3.2.cmml" xref="S4.SS3.SSS3.p3.1.m1.2.3.2">absent</csymbol><list id="S4.SS3.SSS3.p3.1.m1.2.3.3.1.cmml" xref="S4.SS3.SSS3.p3.1.m1.2.3.3.2"><cn id="S4.SS3.SSS3.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS3.SSS3.p3.1.m1.1.1">65</cn><cn id="S4.SS3.SSS3.p3.1.m1.2.2.cmml" type="integer" xref="S4.SS3.SSS3.p3.1.m1.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p3.1.m1.2c">\approx 65,000</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p3.1.m1.2d">≈ 65 , 000</annotation></semantics></math> images with at least one human instance and was used to fine-tune the base ImageNet model using the strategy listed in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS3.SSS2" title="4.3.2. Training Strategy ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3.2</span></a>. We also include the original PSP dataset with <math alttext="\approx 65,000" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p3.2.m2.2"><semantics id="S4.SS3.SSS3.p3.2.m2.2a"><mrow id="S4.SS3.SSS3.p3.2.m2.2.3" xref="S4.SS3.SSS3.p3.2.m2.2.3.cmml"><mi id="S4.SS3.SSS3.p3.2.m2.2.3.2" xref="S4.SS3.SSS3.p3.2.m2.2.3.2.cmml"></mi><mo id="S4.SS3.SSS3.p3.2.m2.2.3.1" xref="S4.SS3.SSS3.p3.2.m2.2.3.1.cmml">≈</mo><mrow id="S4.SS3.SSS3.p3.2.m2.2.3.3.2" xref="S4.SS3.SSS3.p3.2.m2.2.3.3.1.cmml"><mn id="S4.SS3.SSS3.p3.2.m2.1.1" xref="S4.SS3.SSS3.p3.2.m2.1.1.cmml">65</mn><mo id="S4.SS3.SSS3.p3.2.m2.2.3.3.2.1" xref="S4.SS3.SSS3.p3.2.m2.2.3.3.1.cmml">,</mo><mn id="S4.SS3.SSS3.p3.2.m2.2.2" xref="S4.SS3.SSS3.p3.2.m2.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p3.2.m2.2b"><apply id="S4.SS3.SSS3.p3.2.m2.2.3.cmml" xref="S4.SS3.SSS3.p3.2.m2.2.3"><approx id="S4.SS3.SSS3.p3.2.m2.2.3.1.cmml" xref="S4.SS3.SSS3.p3.2.m2.2.3.1"></approx><csymbol cd="latexml" id="S4.SS3.SSS3.p3.2.m2.2.3.2.cmml" xref="S4.SS3.SSS3.p3.2.m2.2.3.2">absent</csymbol><list id="S4.SS3.SSS3.p3.2.m2.2.3.3.1.cmml" xref="S4.SS3.SSS3.p3.2.m2.2.3.3.2"><cn id="S4.SS3.SSS3.p3.2.m2.1.1.cmml" type="integer" xref="S4.SS3.SSS3.p3.2.m2.1.1">65</cn><cn id="S4.SS3.SSS3.p3.2.m2.2.2.cmml" type="integer" xref="S4.SS3.SSS3.p3.2.m2.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p3.2.m2.2c">\approx 65,000</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p3.2.m2.2d">≈ 65 , 000</annotation></semantics></math> images with at least one human instance generated from the provided Unity environment to test the efficacy of fine-tuning with synthetic able-bodied user data <cite class="ltx_cite ltx_citemacro_citep">(Ebadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib29" title="">2022b</a>)</cite>. For these tests, all models and our baseline tests, ImageNet and PSP, were tested on our real wheelchair data testing set using the industry standard metric for detection and estimation accuracy, COCO mean Average Precision <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib65" title="">2015</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p4">
<p class="ltx_p" id="S4.SS3.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.p4.1.1">Results.</span>
Results in terms of bounding box (BB) and keypoint (KP) mean APs (mAP) across ablation trials are shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T2" title="In 4.3.1. Testing Dataset ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p5">
<p class="ltx_p" id="S4.SS3.SSS3.p5.2">Regarding person detection (BB) performance, all datasets demonstrated significant performance boosts of varying degrees when compared to ImageNet and PSP. Notably, our best-performing ablation test led to a <math alttext="98.21\%" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p5.1.m1.1"><semantics id="S4.SS3.SSS3.p5.1.m1.1a"><mrow id="S4.SS3.SSS3.p5.1.m1.1.1" xref="S4.SS3.SSS3.p5.1.m1.1.1.cmml"><mn id="S4.SS3.SSS3.p5.1.m1.1.1.2" xref="S4.SS3.SSS3.p5.1.m1.1.1.2.cmml">98.21</mn><mo id="S4.SS3.SSS3.p5.1.m1.1.1.1" xref="S4.SS3.SSS3.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p5.1.m1.1b"><apply id="S4.SS3.SSS3.p5.1.m1.1.1.cmml" xref="S4.SS3.SSS3.p5.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS3.p5.1.m1.1.1.1.cmml" xref="S4.SS3.SSS3.p5.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.SSS3.p5.1.m1.1.1.2.cmml" type="float" xref="S4.SS3.SSS3.p5.1.m1.1.1.2">98.21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p5.1.m1.1c">98.21\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p5.1.m1.1d">98.21 %</annotation></semantics></math> improvement in BB mAP (<span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p5.2.1">WheelPose</span>-SB) and a <math alttext="7.81\%" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p5.2.m2.1"><semantics id="S4.SS3.SSS3.p5.2.m2.1a"><mrow id="S4.SS3.SSS3.p5.2.m2.1.1" xref="S4.SS3.SSS3.p5.2.m2.1.1.cmml"><mn id="S4.SS3.SSS3.p5.2.m2.1.1.2" xref="S4.SS3.SSS3.p5.2.m2.1.1.2.cmml">7.81</mn><mo id="S4.SS3.SSS3.p5.2.m2.1.1.1" xref="S4.SS3.SSS3.p5.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p5.2.m2.1b"><apply id="S4.SS3.SSS3.p5.2.m2.1.1.cmml" xref="S4.SS3.SSS3.p5.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS3.p5.2.m2.1.1.1.cmml" xref="S4.SS3.SSS3.p5.2.m2.1.1.1">percent</csymbol><cn id="S4.SS3.SSS3.p5.2.m2.1.1.2.cmml" type="float" xref="S4.SS3.SSS3.p5.2.m2.1.1.2">7.81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p5.2.m2.1c">7.81\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p5.2.m2.1d">7.81 %</annotation></semantics></math> improvement in keypoint mAP (<span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p5.2.2">WheelPose</span>-SH) over the best baseline dataset (ImageNet). This indicates the efficacy of our synthetic data and the large headroom for improvement in detecting poses of wheelchair users with industry-standard deep learning models.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Bounding box AP performance comparison between base models and <span class="ltx_text ltx_font_italic" id="S4.T3.10.1">WheelPose</span>-Opt. We list the mean of our seeded testing <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.2.m1.1"><semantics id="S4.T3.2.m1.1b"><mo id="S4.T3.2.m1.1.1" xref="S4.T3.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><csymbol cd="latexml" id="S4.T3.2.m1.1.1.cmml" xref="S4.T3.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.m1.1e">±</annotation></semantics></math> the maximum absolute deviation from the mean.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.8.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.8.7.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.8.7.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.8.7.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.8.7.1.2.1">BB mAP</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.8.7.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.8.7.1.3.1">BB AP<sup class="ltx_sup" id="S4.T3.8.7.1.3.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.8.7.1.3.1.1.1">IoU=.50</span></sup></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.8.7.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.8.7.1.4.1">BB AP<sup class="ltx_sup" id="S4.T3.8.7.1.4.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.8.7.1.4.1.1.1">IoU=.75</span></sup></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.8.7.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.8.7.1.5.1">BB AP<sup class="ltx_sup" id="S4.T3.8.7.1.5.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.8.7.1.5.1.1.1">small</span></sup></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.8.7.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.8.7.1.6.1">BB AP<sup class="ltx_sup" id="S4.T3.8.7.1.6.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.8.7.1.6.1.1.1">medium</span></sup></span></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.8.7.1.7"><span class="ltx_text ltx_font_bold" id="S4.T3.8.7.1.7.1">BB AP<sup class="ltx_sup" id="S4.T3.8.7.1.7.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.8.7.1.7.1.1.1">large</span></sup></span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.8.6.7"><span class="ltx_text ltx_font_italic" id="S4.T3.8.6.7.1" style="color:#000000;">WheelPose<span class="ltx_text ltx_font_upright" id="S4.T3.8.6.7.1.1">-Opt</span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.1"><math alttext="69.46\pm 0.22" class="ltx_Math" display="inline" id="S4.T3.3.1.1.m1.1"><semantics id="S4.T3.3.1.1.m1.1a"><mrow id="S4.T3.3.1.1.m1.1.1" xref="S4.T3.3.1.1.m1.1.1.cmml"><mn id="S4.T3.3.1.1.m1.1.1.2" mathcolor="#000000" xref="S4.T3.3.1.1.m1.1.1.2.cmml">69.46</mn><mo id="S4.T3.3.1.1.m1.1.1.1" mathcolor="#000000" xref="S4.T3.3.1.1.m1.1.1.1.cmml">±</mo><mn id="S4.T3.3.1.1.m1.1.1.3" mathcolor="#000000" xref="S4.T3.3.1.1.m1.1.1.3.cmml">0.22</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.1.1.m1.1b"><apply id="S4.T3.3.1.1.m1.1.1.cmml" xref="S4.T3.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T3.3.1.1.m1.1.1.1.cmml" xref="S4.T3.3.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.3.1.1.m1.1.1.2.cmml" type="float" xref="S4.T3.3.1.1.m1.1.1.2">69.46</cn><cn id="S4.T3.3.1.1.m1.1.1.3.cmml" type="float" xref="S4.T3.3.1.1.m1.1.1.3">0.22</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.1.1.m1.1c">69.46\pm 0.22</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.1.1.m1.1d">69.46 ± 0.22</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.4.2.2"><math alttext="90.77\pm 0.20" class="ltx_Math" display="inline" id="S4.T3.4.2.2.m1.1"><semantics id="S4.T3.4.2.2.m1.1a"><mrow id="S4.T3.4.2.2.m1.1.1" xref="S4.T3.4.2.2.m1.1.1.cmml"><mn id="S4.T3.4.2.2.m1.1.1.2" mathcolor="#000000" xref="S4.T3.4.2.2.m1.1.1.2.cmml">90.77</mn><mo id="S4.T3.4.2.2.m1.1.1.1" mathcolor="#000000" xref="S4.T3.4.2.2.m1.1.1.1.cmml">±</mo><mn id="S4.T3.4.2.2.m1.1.1.3" mathcolor="#000000" xref="S4.T3.4.2.2.m1.1.1.3.cmml">0.20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.2.2.m1.1b"><apply id="S4.T3.4.2.2.m1.1.1.cmml" xref="S4.T3.4.2.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.4.2.2.m1.1.1.1.cmml" xref="S4.T3.4.2.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.4.2.2.m1.1.1.2.cmml" type="float" xref="S4.T3.4.2.2.m1.1.1.2">90.77</cn><cn id="S4.T3.4.2.2.m1.1.1.3.cmml" type="float" xref="S4.T3.4.2.2.m1.1.1.3">0.20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.2.2.m1.1c">90.77\pm 0.20</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.2.2.m1.1d">90.77 ± 0.20</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.5.3.3"><math alttext="82.75\pm 0.61" class="ltx_Math" display="inline" id="S4.T3.5.3.3.m1.1"><semantics id="S4.T3.5.3.3.m1.1a"><mrow id="S4.T3.5.3.3.m1.1.1" xref="S4.T3.5.3.3.m1.1.1.cmml"><mn id="S4.T3.5.3.3.m1.1.1.2" mathcolor="#000000" xref="S4.T3.5.3.3.m1.1.1.2.cmml">82.75</mn><mo id="S4.T3.5.3.3.m1.1.1.1" mathcolor="#000000" xref="S4.T3.5.3.3.m1.1.1.1.cmml">±</mo><mn id="S4.T3.5.3.3.m1.1.1.3" mathcolor="#000000" xref="S4.T3.5.3.3.m1.1.1.3.cmml">0.61</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.3.3.m1.1b"><apply id="S4.T3.5.3.3.m1.1.1.cmml" xref="S4.T3.5.3.3.m1.1.1"><csymbol cd="latexml" id="S4.T3.5.3.3.m1.1.1.1.cmml" xref="S4.T3.5.3.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.5.3.3.m1.1.1.2.cmml" type="float" xref="S4.T3.5.3.3.m1.1.1.2">82.75</cn><cn id="S4.T3.5.3.3.m1.1.1.3.cmml" type="float" xref="S4.T3.5.3.3.m1.1.1.3">0.61</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.3.3.m1.1c">82.75\pm 0.61</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.3.3.m1.1d">82.75 ± 0.61</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.4.4"><math alttext="3.24\pm 0.76" class="ltx_Math" display="inline" id="S4.T3.6.4.4.m1.1"><semantics id="S4.T3.6.4.4.m1.1a"><mrow id="S4.T3.6.4.4.m1.1.1" xref="S4.T3.6.4.4.m1.1.1.cmml"><mn id="S4.T3.6.4.4.m1.1.1.2" mathcolor="#000000" xref="S4.T3.6.4.4.m1.1.1.2.cmml">3.24</mn><mo id="S4.T3.6.4.4.m1.1.1.1" mathcolor="#000000" xref="S4.T3.6.4.4.m1.1.1.1.cmml">±</mo><mn id="S4.T3.6.4.4.m1.1.1.3" mathcolor="#000000" xref="S4.T3.6.4.4.m1.1.1.3.cmml">0.76</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.6.4.4.m1.1b"><apply id="S4.T3.6.4.4.m1.1.1.cmml" xref="S4.T3.6.4.4.m1.1.1"><csymbol cd="latexml" id="S4.T3.6.4.4.m1.1.1.1.cmml" xref="S4.T3.6.4.4.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.6.4.4.m1.1.1.2.cmml" type="float" xref="S4.T3.6.4.4.m1.1.1.2">3.24</cn><cn id="S4.T3.6.4.4.m1.1.1.3.cmml" type="float" xref="S4.T3.6.4.4.m1.1.1.3">0.76</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.4.4.m1.1c">3.24\pm 0.76</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.4.4.m1.1d">3.24 ± 0.76</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.7.5.5"><math alttext="59.82\pm 0.85" class="ltx_Math" display="inline" id="S4.T3.7.5.5.m1.1"><semantics id="S4.T3.7.5.5.m1.1a"><mrow id="S4.T3.7.5.5.m1.1.1" xref="S4.T3.7.5.5.m1.1.1.cmml"><mn id="S4.T3.7.5.5.m1.1.1.2" mathcolor="#000000" xref="S4.T3.7.5.5.m1.1.1.2.cmml">59.82</mn><mo id="S4.T3.7.5.5.m1.1.1.1" mathcolor="#000000" xref="S4.T3.7.5.5.m1.1.1.1.cmml">±</mo><mn id="S4.T3.7.5.5.m1.1.1.3" mathcolor="#000000" xref="S4.T3.7.5.5.m1.1.1.3.cmml">0.85</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.7.5.5.m1.1b"><apply id="S4.T3.7.5.5.m1.1.1.cmml" xref="S4.T3.7.5.5.m1.1.1"><csymbol cd="latexml" id="S4.T3.7.5.5.m1.1.1.1.cmml" xref="S4.T3.7.5.5.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.7.5.5.m1.1.1.2.cmml" type="float" xref="S4.T3.7.5.5.m1.1.1.2">59.82</cn><cn id="S4.T3.7.5.5.m1.1.1.3.cmml" type="float" xref="S4.T3.7.5.5.m1.1.1.3">0.85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.5.5.m1.1c">59.82\pm 0.85</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.5.5.m1.1d">59.82 ± 0.85</annotation></semantics></math></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.8.6.6"><math alttext="69.91\pm 0.21" class="ltx_Math" display="inline" id="S4.T3.8.6.6.m1.1"><semantics id="S4.T3.8.6.6.m1.1a"><mrow id="S4.T3.8.6.6.m1.1.1" xref="S4.T3.8.6.6.m1.1.1.cmml"><mn id="S4.T3.8.6.6.m1.1.1.2" mathcolor="#000000" xref="S4.T3.8.6.6.m1.1.1.2.cmml">69.91</mn><mo id="S4.T3.8.6.6.m1.1.1.1" mathcolor="#000000" xref="S4.T3.8.6.6.m1.1.1.1.cmml">±</mo><mn id="S4.T3.8.6.6.m1.1.1.3" mathcolor="#000000" xref="S4.T3.8.6.6.m1.1.1.3.cmml">0.21</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.8.6.6.m1.1b"><apply id="S4.T3.8.6.6.m1.1.1.cmml" xref="S4.T3.8.6.6.m1.1.1"><csymbol cd="latexml" id="S4.T3.8.6.6.m1.1.1.1.cmml" xref="S4.T3.8.6.6.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.8.6.6.m1.1.1.2.cmml" type="float" xref="S4.T3.8.6.6.m1.1.1.2">69.91</cn><cn id="S4.T3.8.6.6.m1.1.1.3.cmml" type="float" xref="S4.T3.8.6.6.m1.1.1.3">0.21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.6.6.m1.1c">69.91\pm 0.21</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.6.6.m1.1d">69.91 ± 0.21</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.8.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.8.8.1.1">ImageNet (baseline)</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.8.8.1.2">35.19</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.8.8.1.3">71.49</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.8.8.1.4">28.50</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.8.8.1.5">0.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.8.8.1.6">3.15</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.8.8.1.7">36.12</td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.9.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.8.9.2.1">PSP (baseline)</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.8.9.2.2">31.03</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.8.9.2.3">63.64</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.8.9.2.4">25.08</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.8.9.2.5">0.00</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.8.9.2.6">6.27</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T3.8.9.2.7">32.35</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Keypoint AP performance comparison between baseline and <span class="ltx_text ltx_font_italic" id="S4.T4.10.1">WheelPose</span>-Opt. We list the mean of our seeded testing <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.2.m1.1"><semantics id="S4.T4.2.m1.1b"><mo id="S4.T4.2.m1.1.1" xref="S4.T4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.m1.1c"><csymbol cd="latexml" id="S4.T4.2.m1.1.1.cmml" xref="S4.T4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.m1.1d">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.m1.1e">±</annotation></semantics></math> the maximum absolute deviation from the mean. We do not include AP<sup class="ltx_sup" id="S4.T4.11.2"><span class="ltx_text ltx_font_italic" id="S4.T4.11.2.1">small</span></sup> since the human is too small to accurately assign keypoints.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.7.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.7.6.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.7.6.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.7.6.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.7.6.1.2.1">KP mAP</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.7.6.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.7.6.1.3.1">KP AP<sup class="ltx_sup" id="S4.T4.7.6.1.3.1.1"><span class="ltx_text ltx_font_italic" id="S4.T4.7.6.1.3.1.1.1">OKS=.50</span></sup></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.7.6.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.7.6.1.4.1">KP AP<sup class="ltx_sup" id="S4.T4.7.6.1.4.1.1"><span class="ltx_text ltx_font_italic" id="S4.T4.7.6.1.4.1.1.1">OKS=.75</span></sup></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.7.6.1.5"><span class="ltx_text ltx_font_bold" id="S4.T4.7.6.1.5.1">KP AP<sup class="ltx_sup" id="S4.T4.7.6.1.5.1.1"><span class="ltx_text ltx_font_italic" id="S4.T4.7.6.1.5.1.1.1">medium</span></sup></span></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.7.6.1.6"><span class="ltx_text ltx_font_bold" id="S4.T4.7.6.1.6.1">KP AP<sup class="ltx_sup" id="S4.T4.7.6.1.6.1.1"><span class="ltx_text ltx_font_italic" id="S4.T4.7.6.1.6.1.1.1">large</span></sup></span></th>
</tr>
<tr class="ltx_tr" id="S4.T4.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T4.7.5.6"><span class="ltx_text ltx_font_italic" id="S4.T4.7.5.6.1" style="color:#000000;">WheelPose<span class="ltx_text ltx_font_upright" id="S4.T4.7.5.6.1.1">-Opt</span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.1.1"><math alttext="67.93\pm 0.02" class="ltx_Math" display="inline" id="S4.T4.3.1.1.m1.1"><semantics id="S4.T4.3.1.1.m1.1a"><mrow id="S4.T4.3.1.1.m1.1.1" xref="S4.T4.3.1.1.m1.1.1.cmml"><mn id="S4.T4.3.1.1.m1.1.1.2" mathcolor="#000000" xref="S4.T4.3.1.1.m1.1.1.2.cmml">67.93</mn><mo id="S4.T4.3.1.1.m1.1.1.1" mathcolor="#000000" xref="S4.T4.3.1.1.m1.1.1.1.cmml">±</mo><mn id="S4.T4.3.1.1.m1.1.1.3" mathcolor="#000000" xref="S4.T4.3.1.1.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.3.1.1.m1.1b"><apply id="S4.T4.3.1.1.m1.1.1.cmml" xref="S4.T4.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T4.3.1.1.m1.1.1.1.cmml" xref="S4.T4.3.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T4.3.1.1.m1.1.1.2.cmml" type="float" xref="S4.T4.3.1.1.m1.1.1.2">67.93</cn><cn id="S4.T4.3.1.1.m1.1.1.3.cmml" type="float" xref="S4.T4.3.1.1.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.1.1.m1.1c">67.93\pm 0.02</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.1.1.m1.1d">67.93 ± 0.02</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.4.2.2"><math alttext="87.61\pm 0.19" class="ltx_Math" display="inline" id="S4.T4.4.2.2.m1.1"><semantics id="S4.T4.4.2.2.m1.1a"><mrow id="S4.T4.4.2.2.m1.1.1" xref="S4.T4.4.2.2.m1.1.1.cmml"><mn id="S4.T4.4.2.2.m1.1.1.2" mathcolor="#000000" xref="S4.T4.4.2.2.m1.1.1.2.cmml">87.61</mn><mo id="S4.T4.4.2.2.m1.1.1.1" mathcolor="#000000" xref="S4.T4.4.2.2.m1.1.1.1.cmml">±</mo><mn id="S4.T4.4.2.2.m1.1.1.3" mathcolor="#000000" xref="S4.T4.4.2.2.m1.1.1.3.cmml">0.19</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.4.2.2.m1.1b"><apply id="S4.T4.4.2.2.m1.1.1.cmml" xref="S4.T4.4.2.2.m1.1.1"><csymbol cd="latexml" id="S4.T4.4.2.2.m1.1.1.1.cmml" xref="S4.T4.4.2.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T4.4.2.2.m1.1.1.2.cmml" type="float" xref="S4.T4.4.2.2.m1.1.1.2">87.61</cn><cn id="S4.T4.4.2.2.m1.1.1.3.cmml" type="float" xref="S4.T4.4.2.2.m1.1.1.3">0.19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.2.2.m1.1c">87.61\pm 0.19</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.2.2.m1.1d">87.61 ± 0.19</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.5.3.3"><math alttext="74.48\pm 0.25" class="ltx_Math" display="inline" id="S4.T4.5.3.3.m1.1"><semantics id="S4.T4.5.3.3.m1.1a"><mrow id="S4.T4.5.3.3.m1.1.1" xref="S4.T4.5.3.3.m1.1.1.cmml"><mn id="S4.T4.5.3.3.m1.1.1.2" mathcolor="#000000" xref="S4.T4.5.3.3.m1.1.1.2.cmml">74.48</mn><mo id="S4.T4.5.3.3.m1.1.1.1" mathcolor="#000000" xref="S4.T4.5.3.3.m1.1.1.1.cmml">±</mo><mn id="S4.T4.5.3.3.m1.1.1.3" mathcolor="#000000" xref="S4.T4.5.3.3.m1.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.5.3.3.m1.1b"><apply id="S4.T4.5.3.3.m1.1.1.cmml" xref="S4.T4.5.3.3.m1.1.1"><csymbol cd="latexml" id="S4.T4.5.3.3.m1.1.1.1.cmml" xref="S4.T4.5.3.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T4.5.3.3.m1.1.1.2.cmml" type="float" xref="S4.T4.5.3.3.m1.1.1.2">74.48</cn><cn id="S4.T4.5.3.3.m1.1.1.3.cmml" type="float" xref="S4.T4.5.3.3.m1.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.3.3.m1.1c">74.48\pm 0.25</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.3.3.m1.1d">74.48 ± 0.25</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.6.4.4"><math alttext="35.48\pm 0.40" class="ltx_Math" display="inline" id="S4.T4.6.4.4.m1.1"><semantics id="S4.T4.6.4.4.m1.1a"><mrow id="S4.T4.6.4.4.m1.1.1" xref="S4.T4.6.4.4.m1.1.1.cmml"><mn id="S4.T4.6.4.4.m1.1.1.2" mathcolor="#000000" xref="S4.T4.6.4.4.m1.1.1.2.cmml">35.48</mn><mo id="S4.T4.6.4.4.m1.1.1.1" mathcolor="#000000" xref="S4.T4.6.4.4.m1.1.1.1.cmml">±</mo><mn id="S4.T4.6.4.4.m1.1.1.3" mathcolor="#000000" xref="S4.T4.6.4.4.m1.1.1.3.cmml">0.40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.6.4.4.m1.1b"><apply id="S4.T4.6.4.4.m1.1.1.cmml" xref="S4.T4.6.4.4.m1.1.1"><csymbol cd="latexml" id="S4.T4.6.4.4.m1.1.1.1.cmml" xref="S4.T4.6.4.4.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T4.6.4.4.m1.1.1.2.cmml" type="float" xref="S4.T4.6.4.4.m1.1.1.2">35.48</cn><cn id="S4.T4.6.4.4.m1.1.1.3.cmml" type="float" xref="S4.T4.6.4.4.m1.1.1.3">0.40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.4.4.m1.1c">35.48\pm 0.40</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.4.4.m1.1d">35.48 ± 0.40</annotation></semantics></math></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.7.5.5"><math alttext="68.99\pm 0.06" class="ltx_Math" display="inline" id="S4.T4.7.5.5.m1.1"><semantics id="S4.T4.7.5.5.m1.1a"><mrow id="S4.T4.7.5.5.m1.1.1" xref="S4.T4.7.5.5.m1.1.1.cmml"><mn id="S4.T4.7.5.5.m1.1.1.2" mathcolor="#000000" xref="S4.T4.7.5.5.m1.1.1.2.cmml">68.99</mn><mo id="S4.T4.7.5.5.m1.1.1.1" mathcolor="#000000" xref="S4.T4.7.5.5.m1.1.1.1.cmml">±</mo><mn id="S4.T4.7.5.5.m1.1.1.3" mathcolor="#000000" xref="S4.T4.7.5.5.m1.1.1.3.cmml">0.06</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.7.5.5.m1.1b"><apply id="S4.T4.7.5.5.m1.1.1.cmml" xref="S4.T4.7.5.5.m1.1.1"><csymbol cd="latexml" id="S4.T4.7.5.5.m1.1.1.1.cmml" xref="S4.T4.7.5.5.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T4.7.5.5.m1.1.1.2.cmml" type="float" xref="S4.T4.7.5.5.m1.1.1.2">68.99</cn><cn id="S4.T4.7.5.5.m1.1.1.3.cmml" type="float" xref="S4.T4.7.5.5.m1.1.1.3">0.06</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.5.5.m1.1c">68.99\pm 0.06</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.5.5.m1.1d">68.99 ± 0.06</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.7.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.7.7.1.1">ImageNet (baseline)</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.7.7.1.2">63.11</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.7.7.1.3">77.43</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.7.7.1.4">67.20</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.7.7.1.5">6.22</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T4.7.7.1.6">64.96</td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.8.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.7.8.2.1">PSP (baseline)</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.7.8.2.2">53.26</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.7.8.2.3">68.07</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.7.8.2.4">57.36</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.7.8.2.5">10.15</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T4.7.8.2.6">56.12</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.SSS3.p6">
<p class="ltx_p" id="S4.SS3.SSS3.p6.1">Poor performance from BG-20K may be explained by the detail of background images. PSP default and SynthHomes data tend to feature a set of simple or smooth textures while real-world images are often more detailed and consist of more texture. These results may signal a preference for other background characteristics over pure realism.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p7">
<p class="ltx_p" id="S4.SS3.SSS3.p7.1">When examining keypoint performance, the most significant improvement was the inclusion of Unity SyntheticHumans models. This makes intuitive sense, as the diverse and more representative human models more closely match up with the humans found in the real world rather than the generalized Unity humanoid models. The variations between models also help combat overfitting issues by introducing many different definitions of what is a ”human” to the model.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p8">
<p class="ltx_p" id="S4.SS3.SSS3.p8.1">Finally, we examine the performance tradeoffs in our different animation sets. We found motion generation outputs with random filtering (<span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p8.1.1">WheelPose</span>-t2mR10) performed comparably to motion capture data (<span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p8.1.2">WheelPose</span>), indicating similar motion quality between the two data sources. We also found that randomly filtered Text2Motion animations (<span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p8.1.3">WheelPose</span>-t2mR10) performed better than the full motion set (<span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p8.1.4">WheelPose</span>-t2m). This may imply that the number of animations and poses is not directly correlated with model performance. It is important to note these results may change depending on what animations have been filtered. This idea is further shown in the HE model (<span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p8.1.5">WheelPose</span>-t2mHE10), which showed improvement in both BB and KP mAP over the randomly removed <math alttext="10\%" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p8.1.m1.1"><semantics id="S4.SS3.SSS3.p8.1.m1.1a"><mrow id="S4.SS3.SSS3.p8.1.m1.1.1" xref="S4.SS3.SSS3.p8.1.m1.1.1.cmml"><mn id="S4.SS3.SSS3.p8.1.m1.1.1.2" xref="S4.SS3.SSS3.p8.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS3.SSS3.p8.1.m1.1.1.1" xref="S4.SS3.SSS3.p8.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p8.1.m1.1b"><apply id="S4.SS3.SSS3.p8.1.m1.1.1.cmml" xref="S4.SS3.SSS3.p8.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS3.p8.1.m1.1.1.1.cmml" xref="S4.SS3.SSS3.p8.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.SSS3.p8.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.SSS3.p8.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p8.1.m1.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p8.1.m1.1d">10 %</annotation></semantics></math> model. We see that the removal of specific animations that are not perceived as ”realistic” can improve the model performance of generated data.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p9">
<p class="ltx_p" id="S4.SS3.SSS3.p9.1"><span class="ltx_text" id="S4.SS3.SSS3.p9.1.1" style="color:#000000;">From our ablation testing results, we assembled two new datasets: <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p9.1.1.1">WheelPose</span>-Gen, a dataset created from fully generative parameters using SyntheticHuman models, SynthHomes backgrounds, and all <math alttext="100" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p9.1.1.m1.1"><semantics id="S4.SS3.SSS3.p9.1.1.m1.1a"><mn id="S4.SS3.SSS3.p9.1.1.m1.1.1" mathcolor="#000000" xref="S4.SS3.SSS3.p9.1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p9.1.1.m1.1b"><cn id="S4.SS3.SSS3.p9.1.1.m1.1.1.cmml" type="integer" xref="S4.SS3.SSS3.p9.1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p9.1.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p9.1.1.m1.1d">100</annotation></semantics></math> Text2Motion animations, and <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p9.1.1.2">WheelPose</span>-Opt, a dataset created from the best performing parameters from ablation testing which include SyntheticHuman models, SynthHomes backgrounds, and Text2Motion HE 10% removed animations. Both datasets performed comparably to the best performing ablation test in BB and KP mAP (<span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p9.1.1.3">WheelPose</span>-SH) with <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p9.1.1.4">WheelPose</span>-Opt. We note that <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p9.1.1.5">WheelPose</span>-Opt outperforms <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p9.1.1.6">WheelPose</span>-Gen in KP mAP which follows our findings in the initial ablation test. Our findings indicate that different combinations of domain randomization parameters can produce better AI models than the best perform parameters individually.</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4. </span><span class="ltx_text ltx_font_italic" id="S4.SS3.SSS4.1.1" style="color:#000000;">WheelPose<span class="ltx_text ltx_font_upright" id="S4.SS3.SSS4.1.1.1">-Opt Model Performance Deep Dive</span></span>
</h4>
<div class="ltx_para" id="S4.SS3.SSS4.p1">
<p class="ltx_p" id="S4.SS3.SSS4.p1.1">We further evaluate <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p1.1.1">(RQ2.2)</span> by conducting an in-depth quantitative and qualitative analysis of the changes in performance in Detectron2 when fine-tuned with <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS4.p1.1.2">WheelPose</span>-Opt, the best performing dataset from ablation testing using synthetic data and simple human evaluations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS4.p2">
<p class="ltx_p" id="S4.SS3.SSS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p2.1.1">Configuration.</span>
We trained and evaluated the results for <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS4.p2.1.2">WheelPose</span>-Opt with the same strategy described in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS3.SSS2" title="4.3.2. Training Strategy ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3.2</span></a> three separate times using different model seeds (42, 4242, 424242). We then computed the mean and maximum absolute deviation of a set of evaluation metrics, including BB AP, KP AP, and individual keypoint metrics, across the three trials. We compute the same metrics on ImageNet and PSP for use as our baseline.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS4.p3">
<p class="ltx_p" id="S4.SS3.SSS4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p3.1.1">Results.</span>
We first quantify our overall bounding box and keypoint performance with AP and its related breakdowns to build an overarching view of our dataset’s performance across different scenarios. AP at different IoU and OKS<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>IoU and OKS perform the same fundamental purpose for bounding boxes and keypoints respectively</span></span></span> thresholds measure the prediction accuracy at varying degrees of recall (Higher indicates a stricter ground truth definition). Additionally, the overall AP score is split into small, medium, and large based on the detection segment area to quantify the performance at different camera distances and human instance sizes. More information is found in the COCO documentation <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib65" title="">2015</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS4.p4">
<p class="ltx_p" id="S4.SS3.SSS4.p4.1"><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T3" title="In 4.3.3. Ablation Testing Strategy ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> lists the BB AP performance for <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS4.p4.1.1">WheelPose</span>-Opt and the baselines. Our dataset displays over a 98% improvement over both baseline scores in all subcategories of AP. Furthermore, we notice a major drop in performance in the baseline models as the IoU threshold becomes stricter. In contrast, our <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS4.p4.1.2">WheelPose</span> models maintain a high level of accuracy across a much wider range of IoU thresholds. These results indicate <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS4.p4.1.3">WheelPose</span>-Opt can not only identify a wheelchair user but is capable of drawing an accurate bounding box around the entire human instance compared to the baseline models which are only capable of low-fidelity wheelchair user detections.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS4.p5">
<p class="ltx_p" id="S4.SS3.SSS4.p5.1">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T4" title="In 4.3.3. Ablation Testing Strategy ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>, pose estimation improved by up to 7.64% in KP mAP in <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS4.p5.1.1">WheelPose</span>-Opt when compared to ImageNet and up to 27.54% when compared to PSP. Furthermore, we see a similar or greater magnitude of improvement across all AP metrics, indicating the use of <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS4.p5.1.2">WheelPose</span>-Opt improves pose estimation in all scenarios. We note drastic improvements in AP on medium-sized human instances, where ImageNet had noticeably poor performance (6.22). Our system, thus, not only improves but even enables existing models to estimate the postures of wheelchair chairs at further distances.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Keypoint performance breakdown of our primary datasets. We list the mean of our seeded testing <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.2.m1.1"><semantics id="S4.T5.2.m1.1b"><mo id="S4.T5.2.m1.1.1" xref="S4.T5.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T5.2.m1.1c"><csymbol cd="latexml" id="S4.T5.2.m1.1.1.cmml" xref="S4.T5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.m1.1d">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.m1.1e">±</annotation></semantics></math> the maximum absolute deviation from the mean. Within the parentheses on the right is the percent difference from the base model, ImageNet. Percentage of Detected Joints (PDJ) @ 5 describes the percentage of correctly predicted joints within a 5% bounding box diagonal radius <cite class="ltx_cite ltx_citemacro_citep">(Sapp and Taskar, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib84" title="">2013</a>)</cite>. Per Joint Position Error (PJPE) describes the mean Euclidean distance for each keypoint from the ground truth <cite class="ltx_cite ltx_citemacro_citep">(Zwölfer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib116" title="">2023</a>)</cite>. Object Keypoint Similarity Score (OKS) as described by COCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib65" title="">2015</a>)</cite> and used in Chen <span class="ltx_text ltx_font_italic" id="S4.T5.40.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen and Zwicker, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib24" title="">2021</a>)</cite> is the mean precision per keypoint evaluated at both standard loose and strict similarity thresholds of 0.5 and 0.75 respectively. Superiority directions are noted as + and - next to each metric. </figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T5.38">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.38.37.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T5.38.37.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.38.37.1.1.1">Keypoint</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T5.38.37.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.38.37.1.2.1">PDJ@5 (+)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T5.38.37.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.38.37.1.3.1">PJPE (-)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T5.38.37.1.4"><span class="ltx_text ltx_font_bold" id="S4.T5.38.37.1.4.1">OKS50 (+)</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T5.38.37.1.5"><span class="ltx_text ltx_font_bold" id="S4.T5.38.37.1.5.1">OKS75 (+)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.6.4.5">nose</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.3.1.1"><math alttext="0.91\pm 0.02(-2.15\%)" class="ltx_Math" display="inline" id="S4.T5.3.1.1.m1.1"><semantics id="S4.T5.3.1.1.m1.1a"><mrow id="S4.T5.3.1.1.m1.1.1" xref="S4.T5.3.1.1.m1.1.1.cmml"><mn id="S4.T5.3.1.1.m1.1.1.3" xref="S4.T5.3.1.1.m1.1.1.3.cmml">0.91</mn><mo id="S4.T5.3.1.1.m1.1.1.2" xref="S4.T5.3.1.1.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.3.1.1.m1.1.1.1" xref="S4.T5.3.1.1.m1.1.1.1.cmml"><mn id="S4.T5.3.1.1.m1.1.1.1.3" xref="S4.T5.3.1.1.m1.1.1.1.3.cmml">0.02</mn><mo id="S4.T5.3.1.1.m1.1.1.1.2" xref="S4.T5.3.1.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.3.1.1.m1.1.1.1.1.1" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.3.1.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.3.1.1.m1.1.1.1.1.1.1" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.3.1.1.m1.1.1.1.1.1.1a" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.3.1.1.m1.1.1.1.1.1.1.2" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.3.1.1.m1.1.1.1.1.1.1.2.2" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.2.2.cmml">2.15</mn><mo id="S4.T5.3.1.1.m1.1.1.1.1.1.1.2.1" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.3.1.1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.3.1.1.m1.1b"><apply id="S4.T5.3.1.1.m1.1.1.cmml" xref="S4.T5.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.3.1.1.m1.1.1.2.cmml" xref="S4.T5.3.1.1.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.3.1.1.m1.1.1.3.cmml" type="float" xref="S4.T5.3.1.1.m1.1.1.3">0.91</cn><apply id="S4.T5.3.1.1.m1.1.1.1.cmml" xref="S4.T5.3.1.1.m1.1.1.1"><times id="S4.T5.3.1.1.m1.1.1.1.2.cmml" xref="S4.T5.3.1.1.m1.1.1.1.2"></times><cn id="S4.T5.3.1.1.m1.1.1.1.3.cmml" type="float" xref="S4.T5.3.1.1.m1.1.1.1.3">0.02</cn><apply id="S4.T5.3.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.T5.3.1.1.m1.1.1.1.1.1"><minus id="S4.T5.3.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.3.1.1.m1.1.1.1.1.1"></minus><apply id="S4.T5.3.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.3.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.3.1.1.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.3.1.1.m1.1.1.1.1.1.1.2.2">2.15</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.1.1.m1.1c">0.91\pm 0.02(-2.15\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.3.1.1.m1.1d">0.91 ± 0.02 ( - 2.15 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.4.2.2"><math alttext="8.73\pm 4.06(-77.38\%)" class="ltx_Math" display="inline" id="S4.T5.4.2.2.m1.1"><semantics id="S4.T5.4.2.2.m1.1a"><mrow id="S4.T5.4.2.2.m1.1.1" xref="S4.T5.4.2.2.m1.1.1.cmml"><mn id="S4.T5.4.2.2.m1.1.1.3" xref="S4.T5.4.2.2.m1.1.1.3.cmml">8.73</mn><mo id="S4.T5.4.2.2.m1.1.1.2" xref="S4.T5.4.2.2.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.4.2.2.m1.1.1.1" xref="S4.T5.4.2.2.m1.1.1.1.cmml"><mn id="S4.T5.4.2.2.m1.1.1.1.3" xref="S4.T5.4.2.2.m1.1.1.1.3.cmml">4.06</mn><mo id="S4.T5.4.2.2.m1.1.1.1.2" xref="S4.T5.4.2.2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.4.2.2.m1.1.1.1.1.1" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.4.2.2.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.4.2.2.m1.1.1.1.1.1.1" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.4.2.2.m1.1.1.1.1.1.1a" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.4.2.2.m1.1.1.1.1.1.1.2" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.4.2.2.m1.1.1.1.1.1.1.2.2" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.2.2.cmml">77.38</mn><mo id="S4.T5.4.2.2.m1.1.1.1.1.1.1.2.1" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.4.2.2.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.4.2.2.m1.1b"><apply id="S4.T5.4.2.2.m1.1.1.cmml" xref="S4.T5.4.2.2.m1.1.1"><csymbol cd="latexml" id="S4.T5.4.2.2.m1.1.1.2.cmml" xref="S4.T5.4.2.2.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.4.2.2.m1.1.1.3.cmml" type="float" xref="S4.T5.4.2.2.m1.1.1.3">8.73</cn><apply id="S4.T5.4.2.2.m1.1.1.1.cmml" xref="S4.T5.4.2.2.m1.1.1.1"><times id="S4.T5.4.2.2.m1.1.1.1.2.cmml" xref="S4.T5.4.2.2.m1.1.1.1.2"></times><cn id="S4.T5.4.2.2.m1.1.1.1.3.cmml" type="float" xref="S4.T5.4.2.2.m1.1.1.1.3">4.06</cn><apply id="S4.T5.4.2.2.m1.1.1.1.1.1.1.cmml" xref="S4.T5.4.2.2.m1.1.1.1.1.1"><minus id="S4.T5.4.2.2.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.4.2.2.m1.1.1.1.1.1"></minus><apply id="S4.T5.4.2.2.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.4.2.2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.4.2.2.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.4.2.2.m1.1.1.1.1.1.1.2.2">77.38</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.2.2.m1.1c">8.73\pm 4.06(-77.38\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.4.2.2.m1.1d">8.73 ± 4.06 ( - 77.38 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.5.3.3"><math alttext="0.90\pm 0.02(-3.23\%)" class="ltx_Math" display="inline" id="S4.T5.5.3.3.m1.1"><semantics id="S4.T5.5.3.3.m1.1a"><mrow id="S4.T5.5.3.3.m1.1.1" xref="S4.T5.5.3.3.m1.1.1.cmml"><mn id="S4.T5.5.3.3.m1.1.1.3" xref="S4.T5.5.3.3.m1.1.1.3.cmml">0.90</mn><mo id="S4.T5.5.3.3.m1.1.1.2" xref="S4.T5.5.3.3.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.5.3.3.m1.1.1.1" xref="S4.T5.5.3.3.m1.1.1.1.cmml"><mn id="S4.T5.5.3.3.m1.1.1.1.3" xref="S4.T5.5.3.3.m1.1.1.1.3.cmml">0.02</mn><mo id="S4.T5.5.3.3.m1.1.1.1.2" xref="S4.T5.5.3.3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.5.3.3.m1.1.1.1.1.1" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.5.3.3.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.5.3.3.m1.1.1.1.1.1.1" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.5.3.3.m1.1.1.1.1.1.1a" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.5.3.3.m1.1.1.1.1.1.1.2" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.5.3.3.m1.1.1.1.1.1.1.2.2" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.2.2.cmml">3.23</mn><mo id="S4.T5.5.3.3.m1.1.1.1.1.1.1.2.1" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.5.3.3.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.5.3.3.m1.1b"><apply id="S4.T5.5.3.3.m1.1.1.cmml" xref="S4.T5.5.3.3.m1.1.1"><csymbol cd="latexml" id="S4.T5.5.3.3.m1.1.1.2.cmml" xref="S4.T5.5.3.3.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.5.3.3.m1.1.1.3.cmml" type="float" xref="S4.T5.5.3.3.m1.1.1.3">0.90</cn><apply id="S4.T5.5.3.3.m1.1.1.1.cmml" xref="S4.T5.5.3.3.m1.1.1.1"><times id="S4.T5.5.3.3.m1.1.1.1.2.cmml" xref="S4.T5.5.3.3.m1.1.1.1.2"></times><cn id="S4.T5.5.3.3.m1.1.1.1.3.cmml" type="float" xref="S4.T5.5.3.3.m1.1.1.1.3">0.02</cn><apply id="S4.T5.5.3.3.m1.1.1.1.1.1.1.cmml" xref="S4.T5.5.3.3.m1.1.1.1.1.1"><minus id="S4.T5.5.3.3.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.5.3.3.m1.1.1.1.1.1"></minus><apply id="S4.T5.5.3.3.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.5.3.3.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.5.3.3.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.5.3.3.m1.1.1.1.1.1.1.2.2">3.23</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.3.3.m1.1c">0.90\pm 0.02(-3.23\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.5.3.3.m1.1d">0.90 ± 0.02 ( - 3.23 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T5.6.4.4"><math alttext="0.89\pm 0.03(-1.83\%)" class="ltx_Math" display="inline" id="S4.T5.6.4.4.m1.1"><semantics id="S4.T5.6.4.4.m1.1a"><mrow id="S4.T5.6.4.4.m1.1.1" xref="S4.T5.6.4.4.m1.1.1.cmml"><mn id="S4.T5.6.4.4.m1.1.1.3" xref="S4.T5.6.4.4.m1.1.1.3.cmml">0.89</mn><mo id="S4.T5.6.4.4.m1.1.1.2" xref="S4.T5.6.4.4.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.6.4.4.m1.1.1.1" xref="S4.T5.6.4.4.m1.1.1.1.cmml"><mn id="S4.T5.6.4.4.m1.1.1.1.3" xref="S4.T5.6.4.4.m1.1.1.1.3.cmml">0.03</mn><mo id="S4.T5.6.4.4.m1.1.1.1.2" xref="S4.T5.6.4.4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.6.4.4.m1.1.1.1.1.1" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.6.4.4.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.6.4.4.m1.1.1.1.1.1.1" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.6.4.4.m1.1.1.1.1.1.1a" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.6.4.4.m1.1.1.1.1.1.1.2" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.6.4.4.m1.1.1.1.1.1.1.2.2" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.2.2.cmml">1.83</mn><mo id="S4.T5.6.4.4.m1.1.1.1.1.1.1.2.1" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.6.4.4.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.6.4.4.m1.1b"><apply id="S4.T5.6.4.4.m1.1.1.cmml" xref="S4.T5.6.4.4.m1.1.1"><csymbol cd="latexml" id="S4.T5.6.4.4.m1.1.1.2.cmml" xref="S4.T5.6.4.4.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.6.4.4.m1.1.1.3.cmml" type="float" xref="S4.T5.6.4.4.m1.1.1.3">0.89</cn><apply id="S4.T5.6.4.4.m1.1.1.1.cmml" xref="S4.T5.6.4.4.m1.1.1.1"><times id="S4.T5.6.4.4.m1.1.1.1.2.cmml" xref="S4.T5.6.4.4.m1.1.1.1.2"></times><cn id="S4.T5.6.4.4.m1.1.1.1.3.cmml" type="float" xref="S4.T5.6.4.4.m1.1.1.1.3">0.03</cn><apply id="S4.T5.6.4.4.m1.1.1.1.1.1.1.cmml" xref="S4.T5.6.4.4.m1.1.1.1.1.1"><minus id="S4.T5.6.4.4.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.6.4.4.m1.1.1.1.1.1"></minus><apply id="S4.T5.6.4.4.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.6.4.4.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.6.4.4.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.6.4.4.m1.1.1.1.1.1.1.2.2">1.83</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.4.4.m1.1c">0.89\pm 0.03(-1.83\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.6.4.4.m1.1d">0.89 ± 0.03 ( - 1.83 % )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T5.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.10.8.5">eyes</th>
<td class="ltx_td ltx_align_left" id="S4.T5.7.5.1"><math alttext="0.99\pm 0.00(+5.70\%)" class="ltx_Math" display="inline" id="S4.T5.7.5.1.m1.1"><semantics id="S4.T5.7.5.1.m1.1a"><mrow id="S4.T5.7.5.1.m1.1.1" xref="S4.T5.7.5.1.m1.1.1.cmml"><mn id="S4.T5.7.5.1.m1.1.1.3" xref="S4.T5.7.5.1.m1.1.1.3.cmml">0.99</mn><mo id="S4.T5.7.5.1.m1.1.1.2" xref="S4.T5.7.5.1.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.7.5.1.m1.1.1.1" xref="S4.T5.7.5.1.m1.1.1.1.cmml"><mn id="S4.T5.7.5.1.m1.1.1.1.3" xref="S4.T5.7.5.1.m1.1.1.1.3.cmml">0.00</mn><mo id="S4.T5.7.5.1.m1.1.1.1.2" xref="S4.T5.7.5.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.7.5.1.m1.1.1.1.1.1" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.7.5.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.7.5.1.m1.1.1.1.1.1.1" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.7.5.1.m1.1.1.1.1.1.1a" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.7.5.1.m1.1.1.1.1.1.1.2" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.7.5.1.m1.1.1.1.1.1.1.2.2" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.2.2.cmml">5.70</mn><mo id="S4.T5.7.5.1.m1.1.1.1.1.1.1.2.1" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.7.5.1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.7.5.1.m1.1b"><apply id="S4.T5.7.5.1.m1.1.1.cmml" xref="S4.T5.7.5.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.7.5.1.m1.1.1.2.cmml" xref="S4.T5.7.5.1.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.7.5.1.m1.1.1.3.cmml" type="float" xref="S4.T5.7.5.1.m1.1.1.3">0.99</cn><apply id="S4.T5.7.5.1.m1.1.1.1.cmml" xref="S4.T5.7.5.1.m1.1.1.1"><times id="S4.T5.7.5.1.m1.1.1.1.2.cmml" xref="S4.T5.7.5.1.m1.1.1.1.2"></times><cn id="S4.T5.7.5.1.m1.1.1.1.3.cmml" type="float" xref="S4.T5.7.5.1.m1.1.1.1.3">0.00</cn><apply id="S4.T5.7.5.1.m1.1.1.1.1.1.1.cmml" xref="S4.T5.7.5.1.m1.1.1.1.1.1"><plus id="S4.T5.7.5.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.7.5.1.m1.1.1.1.1.1"></plus><apply id="S4.T5.7.5.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.7.5.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.7.5.1.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.7.5.1.m1.1.1.1.1.1.1.2.2">5.70</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.5.1.m1.1c">0.99\pm 0.00(+5.70\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.7.5.1.m1.1d">0.99 ± 0.00 ( + 5.70 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.8.6.2"><math alttext="6.30\pm 0.64(-77.6\%)" class="ltx_Math" display="inline" id="S4.T5.8.6.2.m1.1"><semantics id="S4.T5.8.6.2.m1.1a"><mrow id="S4.T5.8.6.2.m1.1.1" xref="S4.T5.8.6.2.m1.1.1.cmml"><mn id="S4.T5.8.6.2.m1.1.1.3" xref="S4.T5.8.6.2.m1.1.1.3.cmml">6.30</mn><mo id="S4.T5.8.6.2.m1.1.1.2" xref="S4.T5.8.6.2.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.8.6.2.m1.1.1.1" xref="S4.T5.8.6.2.m1.1.1.1.cmml"><mn id="S4.T5.8.6.2.m1.1.1.1.3" xref="S4.T5.8.6.2.m1.1.1.1.3.cmml">0.64</mn><mo id="S4.T5.8.6.2.m1.1.1.1.2" xref="S4.T5.8.6.2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.8.6.2.m1.1.1.1.1.1" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.8.6.2.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.8.6.2.m1.1.1.1.1.1.1" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.8.6.2.m1.1.1.1.1.1.1a" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.8.6.2.m1.1.1.1.1.1.1.2" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.8.6.2.m1.1.1.1.1.1.1.2.2" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.2.2.cmml">77.6</mn><mo id="S4.T5.8.6.2.m1.1.1.1.1.1.1.2.1" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.8.6.2.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.8.6.2.m1.1b"><apply id="S4.T5.8.6.2.m1.1.1.cmml" xref="S4.T5.8.6.2.m1.1.1"><csymbol cd="latexml" id="S4.T5.8.6.2.m1.1.1.2.cmml" xref="S4.T5.8.6.2.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.8.6.2.m1.1.1.3.cmml" type="float" xref="S4.T5.8.6.2.m1.1.1.3">6.30</cn><apply id="S4.T5.8.6.2.m1.1.1.1.cmml" xref="S4.T5.8.6.2.m1.1.1.1"><times id="S4.T5.8.6.2.m1.1.1.1.2.cmml" xref="S4.T5.8.6.2.m1.1.1.1.2"></times><cn id="S4.T5.8.6.2.m1.1.1.1.3.cmml" type="float" xref="S4.T5.8.6.2.m1.1.1.1.3">0.64</cn><apply id="S4.T5.8.6.2.m1.1.1.1.1.1.1.cmml" xref="S4.T5.8.6.2.m1.1.1.1.1.1"><minus id="S4.T5.8.6.2.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.8.6.2.m1.1.1.1.1.1"></minus><apply id="S4.T5.8.6.2.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.8.6.2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.8.6.2.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.8.6.2.m1.1.1.1.1.1.1.2.2">77.6</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.6.2.m1.1c">6.30\pm 0.64(-77.6\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.8.6.2.m1.1d">6.30 ± 0.64 ( - 77.6 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.9.7.3"><math alttext="0.96\pm 0.01(+3.76\%)" class="ltx_Math" display="inline" id="S4.T5.9.7.3.m1.1"><semantics id="S4.T5.9.7.3.m1.1a"><mrow id="S4.T5.9.7.3.m1.1.1" xref="S4.T5.9.7.3.m1.1.1.cmml"><mn id="S4.T5.9.7.3.m1.1.1.3" xref="S4.T5.9.7.3.m1.1.1.3.cmml">0.96</mn><mo id="S4.T5.9.7.3.m1.1.1.2" xref="S4.T5.9.7.3.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.9.7.3.m1.1.1.1" xref="S4.T5.9.7.3.m1.1.1.1.cmml"><mn id="S4.T5.9.7.3.m1.1.1.1.3" xref="S4.T5.9.7.3.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.9.7.3.m1.1.1.1.2" xref="S4.T5.9.7.3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.9.7.3.m1.1.1.1.1.1" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.9.7.3.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.9.7.3.m1.1.1.1.1.1.1" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.9.7.3.m1.1.1.1.1.1.1a" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.9.7.3.m1.1.1.1.1.1.1.2" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.9.7.3.m1.1.1.1.1.1.1.2.2" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.2.2.cmml">3.76</mn><mo id="S4.T5.9.7.3.m1.1.1.1.1.1.1.2.1" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.9.7.3.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.9.7.3.m1.1b"><apply id="S4.T5.9.7.3.m1.1.1.cmml" xref="S4.T5.9.7.3.m1.1.1"><csymbol cd="latexml" id="S4.T5.9.7.3.m1.1.1.2.cmml" xref="S4.T5.9.7.3.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.9.7.3.m1.1.1.3.cmml" type="float" xref="S4.T5.9.7.3.m1.1.1.3">0.96</cn><apply id="S4.T5.9.7.3.m1.1.1.1.cmml" xref="S4.T5.9.7.3.m1.1.1.1"><times id="S4.T5.9.7.3.m1.1.1.1.2.cmml" xref="S4.T5.9.7.3.m1.1.1.1.2"></times><cn id="S4.T5.9.7.3.m1.1.1.1.3.cmml" type="float" xref="S4.T5.9.7.3.m1.1.1.1.3">0.01</cn><apply id="S4.T5.9.7.3.m1.1.1.1.1.1.1.cmml" xref="S4.T5.9.7.3.m1.1.1.1.1.1"><plus id="S4.T5.9.7.3.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.9.7.3.m1.1.1.1.1.1"></plus><apply id="S4.T5.9.7.3.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.9.7.3.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.9.7.3.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.9.7.3.m1.1.1.1.1.1.1.2.2">3.76</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.7.3.m1.1c">0.96\pm 0.01(+3.76\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.9.7.3.m1.1d">0.96 ± 0.01 ( + 3.76 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.10.8.4"><math alttext="0.95\pm 0.01(+4.19\%)" class="ltx_Math" display="inline" id="S4.T5.10.8.4.m1.1"><semantics id="S4.T5.10.8.4.m1.1a"><mrow id="S4.T5.10.8.4.m1.1.1" xref="S4.T5.10.8.4.m1.1.1.cmml"><mn id="S4.T5.10.8.4.m1.1.1.3" xref="S4.T5.10.8.4.m1.1.1.3.cmml">0.95</mn><mo id="S4.T5.10.8.4.m1.1.1.2" xref="S4.T5.10.8.4.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.10.8.4.m1.1.1.1" xref="S4.T5.10.8.4.m1.1.1.1.cmml"><mn id="S4.T5.10.8.4.m1.1.1.1.3" xref="S4.T5.10.8.4.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.10.8.4.m1.1.1.1.2" xref="S4.T5.10.8.4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.10.8.4.m1.1.1.1.1.1" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.10.8.4.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.10.8.4.m1.1.1.1.1.1.1" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.10.8.4.m1.1.1.1.1.1.1a" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.10.8.4.m1.1.1.1.1.1.1.2" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.10.8.4.m1.1.1.1.1.1.1.2.2" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.2.2.cmml">4.19</mn><mo id="S4.T5.10.8.4.m1.1.1.1.1.1.1.2.1" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.10.8.4.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.10.8.4.m1.1b"><apply id="S4.T5.10.8.4.m1.1.1.cmml" xref="S4.T5.10.8.4.m1.1.1"><csymbol cd="latexml" id="S4.T5.10.8.4.m1.1.1.2.cmml" xref="S4.T5.10.8.4.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.10.8.4.m1.1.1.3.cmml" type="float" xref="S4.T5.10.8.4.m1.1.1.3">0.95</cn><apply id="S4.T5.10.8.4.m1.1.1.1.cmml" xref="S4.T5.10.8.4.m1.1.1.1"><times id="S4.T5.10.8.4.m1.1.1.1.2.cmml" xref="S4.T5.10.8.4.m1.1.1.1.2"></times><cn id="S4.T5.10.8.4.m1.1.1.1.3.cmml" type="float" xref="S4.T5.10.8.4.m1.1.1.1.3">0.01</cn><apply id="S4.T5.10.8.4.m1.1.1.1.1.1.1.cmml" xref="S4.T5.10.8.4.m1.1.1.1.1.1"><plus id="S4.T5.10.8.4.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.10.8.4.m1.1.1.1.1.1"></plus><apply id="S4.T5.10.8.4.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.10.8.4.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.10.8.4.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.10.8.4.m1.1.1.1.1.1.1.2.2">4.19</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.10.8.4.m1.1c">0.95\pm 0.01(+4.19\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.10.8.4.m1.1d">0.95 ± 0.01 ( + 4.19 % )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T5.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.14.12.5">ears</th>
<td class="ltx_td ltx_align_left" id="S4.T5.11.9.1"><math alttext="0.92\pm 0.01(+5.56\%)" class="ltx_Math" display="inline" id="S4.T5.11.9.1.m1.1"><semantics id="S4.T5.11.9.1.m1.1a"><mrow id="S4.T5.11.9.1.m1.1.1" xref="S4.T5.11.9.1.m1.1.1.cmml"><mn id="S4.T5.11.9.1.m1.1.1.3" xref="S4.T5.11.9.1.m1.1.1.3.cmml">0.92</mn><mo id="S4.T5.11.9.1.m1.1.1.2" xref="S4.T5.11.9.1.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.11.9.1.m1.1.1.1" xref="S4.T5.11.9.1.m1.1.1.1.cmml"><mn id="S4.T5.11.9.1.m1.1.1.1.3" xref="S4.T5.11.9.1.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.11.9.1.m1.1.1.1.2" xref="S4.T5.11.9.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.11.9.1.m1.1.1.1.1.1" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.11.9.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.11.9.1.m1.1.1.1.1.1.1" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.11.9.1.m1.1.1.1.1.1.1a" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.11.9.1.m1.1.1.1.1.1.1.2" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.11.9.1.m1.1.1.1.1.1.1.2.2" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.2.2.cmml">5.56</mn><mo id="S4.T5.11.9.1.m1.1.1.1.1.1.1.2.1" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.11.9.1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.11.9.1.m1.1b"><apply id="S4.T5.11.9.1.m1.1.1.cmml" xref="S4.T5.11.9.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.11.9.1.m1.1.1.2.cmml" xref="S4.T5.11.9.1.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.11.9.1.m1.1.1.3.cmml" type="float" xref="S4.T5.11.9.1.m1.1.1.3">0.92</cn><apply id="S4.T5.11.9.1.m1.1.1.1.cmml" xref="S4.T5.11.9.1.m1.1.1.1"><times id="S4.T5.11.9.1.m1.1.1.1.2.cmml" xref="S4.T5.11.9.1.m1.1.1.1.2"></times><cn id="S4.T5.11.9.1.m1.1.1.1.3.cmml" type="float" xref="S4.T5.11.9.1.m1.1.1.1.3">0.01</cn><apply id="S4.T5.11.9.1.m1.1.1.1.1.1.1.cmml" xref="S4.T5.11.9.1.m1.1.1.1.1.1"><plus id="S4.T5.11.9.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.11.9.1.m1.1.1.1.1.1"></plus><apply id="S4.T5.11.9.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.11.9.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.11.9.1.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.11.9.1.m1.1.1.1.1.1.1.2.2">5.56</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.11.9.1.m1.1c">0.92\pm 0.01(+5.56\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.11.9.1.m1.1d">0.92 ± 0.01 ( + 5.56 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.12.10.2"><math alttext="10.90\pm 0.03(-61.60\%)" class="ltx_Math" display="inline" id="S4.T5.12.10.2.m1.1"><semantics id="S4.T5.12.10.2.m1.1a"><mrow id="S4.T5.12.10.2.m1.1.1" xref="S4.T5.12.10.2.m1.1.1.cmml"><mn id="S4.T5.12.10.2.m1.1.1.3" xref="S4.T5.12.10.2.m1.1.1.3.cmml">10.90</mn><mo id="S4.T5.12.10.2.m1.1.1.2" xref="S4.T5.12.10.2.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.12.10.2.m1.1.1.1" xref="S4.T5.12.10.2.m1.1.1.1.cmml"><mn id="S4.T5.12.10.2.m1.1.1.1.3" xref="S4.T5.12.10.2.m1.1.1.1.3.cmml">0.03</mn><mo id="S4.T5.12.10.2.m1.1.1.1.2" xref="S4.T5.12.10.2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.12.10.2.m1.1.1.1.1.1" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.12.10.2.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.12.10.2.m1.1.1.1.1.1.1" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.12.10.2.m1.1.1.1.1.1.1a" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.12.10.2.m1.1.1.1.1.1.1.2" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.12.10.2.m1.1.1.1.1.1.1.2.2" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.2.2.cmml">61.60</mn><mo id="S4.T5.12.10.2.m1.1.1.1.1.1.1.2.1" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.12.10.2.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.12.10.2.m1.1b"><apply id="S4.T5.12.10.2.m1.1.1.cmml" xref="S4.T5.12.10.2.m1.1.1"><csymbol cd="latexml" id="S4.T5.12.10.2.m1.1.1.2.cmml" xref="S4.T5.12.10.2.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.12.10.2.m1.1.1.3.cmml" type="float" xref="S4.T5.12.10.2.m1.1.1.3">10.90</cn><apply id="S4.T5.12.10.2.m1.1.1.1.cmml" xref="S4.T5.12.10.2.m1.1.1.1"><times id="S4.T5.12.10.2.m1.1.1.1.2.cmml" xref="S4.T5.12.10.2.m1.1.1.1.2"></times><cn id="S4.T5.12.10.2.m1.1.1.1.3.cmml" type="float" xref="S4.T5.12.10.2.m1.1.1.1.3">0.03</cn><apply id="S4.T5.12.10.2.m1.1.1.1.1.1.1.cmml" xref="S4.T5.12.10.2.m1.1.1.1.1.1"><minus id="S4.T5.12.10.2.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.12.10.2.m1.1.1.1.1.1"></minus><apply id="S4.T5.12.10.2.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.12.10.2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.12.10.2.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.12.10.2.m1.1.1.1.1.1.1.2.2">61.60</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.12.10.2.m1.1c">10.90\pm 0.03(-61.60\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.12.10.2.m1.1d">10.90 ± 0.03 ( - 61.60 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.13.11.3"><math alttext="0.93\pm 0.00(+5.90\%)" class="ltx_Math" display="inline" id="S4.T5.13.11.3.m1.1"><semantics id="S4.T5.13.11.3.m1.1a"><mrow id="S4.T5.13.11.3.m1.1.1" xref="S4.T5.13.11.3.m1.1.1.cmml"><mn id="S4.T5.13.11.3.m1.1.1.3" xref="S4.T5.13.11.3.m1.1.1.3.cmml">0.93</mn><mo id="S4.T5.13.11.3.m1.1.1.2" xref="S4.T5.13.11.3.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.13.11.3.m1.1.1.1" xref="S4.T5.13.11.3.m1.1.1.1.cmml"><mn id="S4.T5.13.11.3.m1.1.1.1.3" xref="S4.T5.13.11.3.m1.1.1.1.3.cmml">0.00</mn><mo id="S4.T5.13.11.3.m1.1.1.1.2" xref="S4.T5.13.11.3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.13.11.3.m1.1.1.1.1.1" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.13.11.3.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.13.11.3.m1.1.1.1.1.1.1" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.13.11.3.m1.1.1.1.1.1.1a" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.13.11.3.m1.1.1.1.1.1.1.2" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.13.11.3.m1.1.1.1.1.1.1.2.2" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.2.2.cmml">5.90</mn><mo id="S4.T5.13.11.3.m1.1.1.1.1.1.1.2.1" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.13.11.3.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.13.11.3.m1.1b"><apply id="S4.T5.13.11.3.m1.1.1.cmml" xref="S4.T5.13.11.3.m1.1.1"><csymbol cd="latexml" id="S4.T5.13.11.3.m1.1.1.2.cmml" xref="S4.T5.13.11.3.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.13.11.3.m1.1.1.3.cmml" type="float" xref="S4.T5.13.11.3.m1.1.1.3">0.93</cn><apply id="S4.T5.13.11.3.m1.1.1.1.cmml" xref="S4.T5.13.11.3.m1.1.1.1"><times id="S4.T5.13.11.3.m1.1.1.1.2.cmml" xref="S4.T5.13.11.3.m1.1.1.1.2"></times><cn id="S4.T5.13.11.3.m1.1.1.1.3.cmml" type="float" xref="S4.T5.13.11.3.m1.1.1.1.3">0.00</cn><apply id="S4.T5.13.11.3.m1.1.1.1.1.1.1.cmml" xref="S4.T5.13.11.3.m1.1.1.1.1.1"><plus id="S4.T5.13.11.3.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.13.11.3.m1.1.1.1.1.1"></plus><apply id="S4.T5.13.11.3.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.13.11.3.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.13.11.3.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.13.11.3.m1.1.1.1.1.1.1.2.2">5.90</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.13.11.3.m1.1c">0.93\pm 0.00(+5.90\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.13.11.3.m1.1d">0.93 ± 0.00 ( + 5.90 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.14.12.4"><math alttext="0.83\pm 0.01(+0.81\%)" class="ltx_Math" display="inline" id="S4.T5.14.12.4.m1.1"><semantics id="S4.T5.14.12.4.m1.1a"><mrow id="S4.T5.14.12.4.m1.1.1" xref="S4.T5.14.12.4.m1.1.1.cmml"><mn id="S4.T5.14.12.4.m1.1.1.3" xref="S4.T5.14.12.4.m1.1.1.3.cmml">0.83</mn><mo id="S4.T5.14.12.4.m1.1.1.2" xref="S4.T5.14.12.4.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.14.12.4.m1.1.1.1" xref="S4.T5.14.12.4.m1.1.1.1.cmml"><mn id="S4.T5.14.12.4.m1.1.1.1.3" xref="S4.T5.14.12.4.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.14.12.4.m1.1.1.1.2" xref="S4.T5.14.12.4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.14.12.4.m1.1.1.1.1.1" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.14.12.4.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.14.12.4.m1.1.1.1.1.1.1" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.14.12.4.m1.1.1.1.1.1.1a" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.14.12.4.m1.1.1.1.1.1.1.2" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.14.12.4.m1.1.1.1.1.1.1.2.2" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.2.2.cmml">0.81</mn><mo id="S4.T5.14.12.4.m1.1.1.1.1.1.1.2.1" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.14.12.4.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.14.12.4.m1.1b"><apply id="S4.T5.14.12.4.m1.1.1.cmml" xref="S4.T5.14.12.4.m1.1.1"><csymbol cd="latexml" id="S4.T5.14.12.4.m1.1.1.2.cmml" xref="S4.T5.14.12.4.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.14.12.4.m1.1.1.3.cmml" type="float" xref="S4.T5.14.12.4.m1.1.1.3">0.83</cn><apply id="S4.T5.14.12.4.m1.1.1.1.cmml" xref="S4.T5.14.12.4.m1.1.1.1"><times id="S4.T5.14.12.4.m1.1.1.1.2.cmml" xref="S4.T5.14.12.4.m1.1.1.1.2"></times><cn id="S4.T5.14.12.4.m1.1.1.1.3.cmml" type="float" xref="S4.T5.14.12.4.m1.1.1.1.3">0.01</cn><apply id="S4.T5.14.12.4.m1.1.1.1.1.1.1.cmml" xref="S4.T5.14.12.4.m1.1.1.1.1.1"><plus id="S4.T5.14.12.4.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.14.12.4.m1.1.1.1.1.1"></plus><apply id="S4.T5.14.12.4.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.14.12.4.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.14.12.4.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.14.12.4.m1.1.1.1.1.1.1.2.2">0.81</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.14.12.4.m1.1c">0.83\pm 0.01(+0.81\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.14.12.4.m1.1d">0.83 ± 0.01 ( + 0.81 % )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T5.18.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.18.16.5">shoulders</th>
<td class="ltx_td ltx_align_left" id="S4.T5.15.13.1"><math alttext="0.92\pm 0.02(+6.74\%)" class="ltx_Math" display="inline" id="S4.T5.15.13.1.m1.1"><semantics id="S4.T5.15.13.1.m1.1a"><mrow id="S4.T5.15.13.1.m1.1.1" xref="S4.T5.15.13.1.m1.1.1.cmml"><mn id="S4.T5.15.13.1.m1.1.1.3" xref="S4.T5.15.13.1.m1.1.1.3.cmml">0.92</mn><mo id="S4.T5.15.13.1.m1.1.1.2" xref="S4.T5.15.13.1.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.15.13.1.m1.1.1.1" xref="S4.T5.15.13.1.m1.1.1.1.cmml"><mn id="S4.T5.15.13.1.m1.1.1.1.3" xref="S4.T5.15.13.1.m1.1.1.1.3.cmml">0.02</mn><mo id="S4.T5.15.13.1.m1.1.1.1.2" xref="S4.T5.15.13.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.15.13.1.m1.1.1.1.1.1" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.15.13.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.15.13.1.m1.1.1.1.1.1.1" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.15.13.1.m1.1.1.1.1.1.1a" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.15.13.1.m1.1.1.1.1.1.1.2" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.15.13.1.m1.1.1.1.1.1.1.2.2" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.2.2.cmml">6.74</mn><mo id="S4.T5.15.13.1.m1.1.1.1.1.1.1.2.1" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.15.13.1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.15.13.1.m1.1b"><apply id="S4.T5.15.13.1.m1.1.1.cmml" xref="S4.T5.15.13.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.15.13.1.m1.1.1.2.cmml" xref="S4.T5.15.13.1.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.15.13.1.m1.1.1.3.cmml" type="float" xref="S4.T5.15.13.1.m1.1.1.3">0.92</cn><apply id="S4.T5.15.13.1.m1.1.1.1.cmml" xref="S4.T5.15.13.1.m1.1.1.1"><times id="S4.T5.15.13.1.m1.1.1.1.2.cmml" xref="S4.T5.15.13.1.m1.1.1.1.2"></times><cn id="S4.T5.15.13.1.m1.1.1.1.3.cmml" type="float" xref="S4.T5.15.13.1.m1.1.1.1.3">0.02</cn><apply id="S4.T5.15.13.1.m1.1.1.1.1.1.1.cmml" xref="S4.T5.15.13.1.m1.1.1.1.1.1"><plus id="S4.T5.15.13.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.15.13.1.m1.1.1.1.1.1"></plus><apply id="S4.T5.15.13.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.15.13.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.15.13.1.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.15.13.1.m1.1.1.1.1.1.1.2.2">6.74</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.15.13.1.m1.1c">0.92\pm 0.02(+6.74\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.15.13.1.m1.1d">0.92 ± 0.02 ( + 6.74 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.16.14.2"><math alttext="10.98\pm 0.26(-63.89\%)" class="ltx_Math" display="inline" id="S4.T5.16.14.2.m1.1"><semantics id="S4.T5.16.14.2.m1.1a"><mrow id="S4.T5.16.14.2.m1.1.1" xref="S4.T5.16.14.2.m1.1.1.cmml"><mn id="S4.T5.16.14.2.m1.1.1.3" xref="S4.T5.16.14.2.m1.1.1.3.cmml">10.98</mn><mo id="S4.T5.16.14.2.m1.1.1.2" xref="S4.T5.16.14.2.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.16.14.2.m1.1.1.1" xref="S4.T5.16.14.2.m1.1.1.1.cmml"><mn id="S4.T5.16.14.2.m1.1.1.1.3" xref="S4.T5.16.14.2.m1.1.1.1.3.cmml">0.26</mn><mo id="S4.T5.16.14.2.m1.1.1.1.2" xref="S4.T5.16.14.2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.16.14.2.m1.1.1.1.1.1" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.16.14.2.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.16.14.2.m1.1.1.1.1.1.1" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.16.14.2.m1.1.1.1.1.1.1a" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.16.14.2.m1.1.1.1.1.1.1.2" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.16.14.2.m1.1.1.1.1.1.1.2.2" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.2.2.cmml">63.89</mn><mo id="S4.T5.16.14.2.m1.1.1.1.1.1.1.2.1" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.16.14.2.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.16.14.2.m1.1b"><apply id="S4.T5.16.14.2.m1.1.1.cmml" xref="S4.T5.16.14.2.m1.1.1"><csymbol cd="latexml" id="S4.T5.16.14.2.m1.1.1.2.cmml" xref="S4.T5.16.14.2.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.16.14.2.m1.1.1.3.cmml" type="float" xref="S4.T5.16.14.2.m1.1.1.3">10.98</cn><apply id="S4.T5.16.14.2.m1.1.1.1.cmml" xref="S4.T5.16.14.2.m1.1.1.1"><times id="S4.T5.16.14.2.m1.1.1.1.2.cmml" xref="S4.T5.16.14.2.m1.1.1.1.2"></times><cn id="S4.T5.16.14.2.m1.1.1.1.3.cmml" type="float" xref="S4.T5.16.14.2.m1.1.1.1.3">0.26</cn><apply id="S4.T5.16.14.2.m1.1.1.1.1.1.1.cmml" xref="S4.T5.16.14.2.m1.1.1.1.1.1"><minus id="S4.T5.16.14.2.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.16.14.2.m1.1.1.1.1.1"></minus><apply id="S4.T5.16.14.2.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.16.14.2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.16.14.2.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.16.14.2.m1.1.1.1.1.1.1.2.2">63.89</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.16.14.2.m1.1c">10.98\pm 0.26(-63.89\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.16.14.2.m1.1d">10.98 ± 0.26 ( - 63.89 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.17.15.3"><math alttext="1.00\pm 0.01(+4.74\%)" class="ltx_Math" display="inline" id="S4.T5.17.15.3.m1.1"><semantics id="S4.T5.17.15.3.m1.1a"><mrow id="S4.T5.17.15.3.m1.1.1" xref="S4.T5.17.15.3.m1.1.1.cmml"><mn id="S4.T5.17.15.3.m1.1.1.3" xref="S4.T5.17.15.3.m1.1.1.3.cmml">1.00</mn><mo id="S4.T5.17.15.3.m1.1.1.2" xref="S4.T5.17.15.3.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.17.15.3.m1.1.1.1" xref="S4.T5.17.15.3.m1.1.1.1.cmml"><mn id="S4.T5.17.15.3.m1.1.1.1.3" xref="S4.T5.17.15.3.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.17.15.3.m1.1.1.1.2" xref="S4.T5.17.15.3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.17.15.3.m1.1.1.1.1.1" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.17.15.3.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.17.15.3.m1.1.1.1.1.1.1" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.17.15.3.m1.1.1.1.1.1.1a" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.17.15.3.m1.1.1.1.1.1.1.2" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.17.15.3.m1.1.1.1.1.1.1.2.2" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.2.2.cmml">4.74</mn><mo id="S4.T5.17.15.3.m1.1.1.1.1.1.1.2.1" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.17.15.3.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.17.15.3.m1.1b"><apply id="S4.T5.17.15.3.m1.1.1.cmml" xref="S4.T5.17.15.3.m1.1.1"><csymbol cd="latexml" id="S4.T5.17.15.3.m1.1.1.2.cmml" xref="S4.T5.17.15.3.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.17.15.3.m1.1.1.3.cmml" type="float" xref="S4.T5.17.15.3.m1.1.1.3">1.00</cn><apply id="S4.T5.17.15.3.m1.1.1.1.cmml" xref="S4.T5.17.15.3.m1.1.1.1"><times id="S4.T5.17.15.3.m1.1.1.1.2.cmml" xref="S4.T5.17.15.3.m1.1.1.1.2"></times><cn id="S4.T5.17.15.3.m1.1.1.1.3.cmml" type="float" xref="S4.T5.17.15.3.m1.1.1.1.3">0.01</cn><apply id="S4.T5.17.15.3.m1.1.1.1.1.1.1.cmml" xref="S4.T5.17.15.3.m1.1.1.1.1.1"><plus id="S4.T5.17.15.3.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.17.15.3.m1.1.1.1.1.1"></plus><apply id="S4.T5.17.15.3.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.17.15.3.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.17.15.3.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.17.15.3.m1.1.1.1.1.1.1.2.2">4.74</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.17.15.3.m1.1c">1.00\pm 0.01(+4.74\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.17.15.3.m1.1d">1.00 ± 0.01 ( + 4.74 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.18.16.4"><math alttext="0.97\pm 0.01(+6.38\%)" class="ltx_Math" display="inline" id="S4.T5.18.16.4.m1.1"><semantics id="S4.T5.18.16.4.m1.1a"><mrow id="S4.T5.18.16.4.m1.1.1" xref="S4.T5.18.16.4.m1.1.1.cmml"><mn id="S4.T5.18.16.4.m1.1.1.3" xref="S4.T5.18.16.4.m1.1.1.3.cmml">0.97</mn><mo id="S4.T5.18.16.4.m1.1.1.2" xref="S4.T5.18.16.4.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.18.16.4.m1.1.1.1" xref="S4.T5.18.16.4.m1.1.1.1.cmml"><mn id="S4.T5.18.16.4.m1.1.1.1.3" xref="S4.T5.18.16.4.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.18.16.4.m1.1.1.1.2" xref="S4.T5.18.16.4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.18.16.4.m1.1.1.1.1.1" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.18.16.4.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.18.16.4.m1.1.1.1.1.1.1" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.18.16.4.m1.1.1.1.1.1.1a" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.18.16.4.m1.1.1.1.1.1.1.2" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.18.16.4.m1.1.1.1.1.1.1.2.2" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.2.2.cmml">6.38</mn><mo id="S4.T5.18.16.4.m1.1.1.1.1.1.1.2.1" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.18.16.4.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.18.16.4.m1.1b"><apply id="S4.T5.18.16.4.m1.1.1.cmml" xref="S4.T5.18.16.4.m1.1.1"><csymbol cd="latexml" id="S4.T5.18.16.4.m1.1.1.2.cmml" xref="S4.T5.18.16.4.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.18.16.4.m1.1.1.3.cmml" type="float" xref="S4.T5.18.16.4.m1.1.1.3">0.97</cn><apply id="S4.T5.18.16.4.m1.1.1.1.cmml" xref="S4.T5.18.16.4.m1.1.1.1"><times id="S4.T5.18.16.4.m1.1.1.1.2.cmml" xref="S4.T5.18.16.4.m1.1.1.1.2"></times><cn id="S4.T5.18.16.4.m1.1.1.1.3.cmml" type="float" xref="S4.T5.18.16.4.m1.1.1.1.3">0.01</cn><apply id="S4.T5.18.16.4.m1.1.1.1.1.1.1.cmml" xref="S4.T5.18.16.4.m1.1.1.1.1.1"><plus id="S4.T5.18.16.4.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.18.16.4.m1.1.1.1.1.1"></plus><apply id="S4.T5.18.16.4.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.18.16.4.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.18.16.4.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.18.16.4.m1.1.1.1.1.1.1.2.2">6.38</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.18.16.4.m1.1c">0.97\pm 0.01(+6.38\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.18.16.4.m1.1d">0.97 ± 0.01 ( + 6.38 % )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T5.22.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.22.20.5">elbows</th>
<td class="ltx_td ltx_align_left" id="S4.T5.19.17.1"><math alttext="0.86\pm 0.01(+2.78\%)" class="ltx_Math" display="inline" id="S4.T5.19.17.1.m1.1"><semantics id="S4.T5.19.17.1.m1.1a"><mrow id="S4.T5.19.17.1.m1.1.1" xref="S4.T5.19.17.1.m1.1.1.cmml"><mn id="S4.T5.19.17.1.m1.1.1.3" xref="S4.T5.19.17.1.m1.1.1.3.cmml">0.86</mn><mo id="S4.T5.19.17.1.m1.1.1.2" xref="S4.T5.19.17.1.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.19.17.1.m1.1.1.1" xref="S4.T5.19.17.1.m1.1.1.1.cmml"><mn id="S4.T5.19.17.1.m1.1.1.1.3" xref="S4.T5.19.17.1.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.19.17.1.m1.1.1.1.2" xref="S4.T5.19.17.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.19.17.1.m1.1.1.1.1.1" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.19.17.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.19.17.1.m1.1.1.1.1.1.1" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.19.17.1.m1.1.1.1.1.1.1a" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.19.17.1.m1.1.1.1.1.1.1.2" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.19.17.1.m1.1.1.1.1.1.1.2.2" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.2.2.cmml">2.78</mn><mo id="S4.T5.19.17.1.m1.1.1.1.1.1.1.2.1" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.19.17.1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.19.17.1.m1.1b"><apply id="S4.T5.19.17.1.m1.1.1.cmml" xref="S4.T5.19.17.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.19.17.1.m1.1.1.2.cmml" xref="S4.T5.19.17.1.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.19.17.1.m1.1.1.3.cmml" type="float" xref="S4.T5.19.17.1.m1.1.1.3">0.86</cn><apply id="S4.T5.19.17.1.m1.1.1.1.cmml" xref="S4.T5.19.17.1.m1.1.1.1"><times id="S4.T5.19.17.1.m1.1.1.1.2.cmml" xref="S4.T5.19.17.1.m1.1.1.1.2"></times><cn id="S4.T5.19.17.1.m1.1.1.1.3.cmml" type="float" xref="S4.T5.19.17.1.m1.1.1.1.3">0.01</cn><apply id="S4.T5.19.17.1.m1.1.1.1.1.1.1.cmml" xref="S4.T5.19.17.1.m1.1.1.1.1.1"><plus id="S4.T5.19.17.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.19.17.1.m1.1.1.1.1.1"></plus><apply id="S4.T5.19.17.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.19.17.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.19.17.1.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.19.17.1.m1.1.1.1.1.1.1.2.2">2.78</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.19.17.1.m1.1c">0.86\pm 0.01(+2.78\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.19.17.1.m1.1d">0.86 ± 0.01 ( + 2.78 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.20.18.2"><math alttext="14.54\pm 0.23(-69.38\%)" class="ltx_Math" display="inline" id="S4.T5.20.18.2.m1.1"><semantics id="S4.T5.20.18.2.m1.1a"><mrow id="S4.T5.20.18.2.m1.1.1" xref="S4.T5.20.18.2.m1.1.1.cmml"><mn id="S4.T5.20.18.2.m1.1.1.3" xref="S4.T5.20.18.2.m1.1.1.3.cmml">14.54</mn><mo id="S4.T5.20.18.2.m1.1.1.2" xref="S4.T5.20.18.2.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.20.18.2.m1.1.1.1" xref="S4.T5.20.18.2.m1.1.1.1.cmml"><mn id="S4.T5.20.18.2.m1.1.1.1.3" xref="S4.T5.20.18.2.m1.1.1.1.3.cmml">0.23</mn><mo id="S4.T5.20.18.2.m1.1.1.1.2" xref="S4.T5.20.18.2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.20.18.2.m1.1.1.1.1.1" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.20.18.2.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.20.18.2.m1.1.1.1.1.1.1" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.20.18.2.m1.1.1.1.1.1.1a" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.20.18.2.m1.1.1.1.1.1.1.2" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.20.18.2.m1.1.1.1.1.1.1.2.2" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.2.2.cmml">69.38</mn><mo id="S4.T5.20.18.2.m1.1.1.1.1.1.1.2.1" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.20.18.2.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.20.18.2.m1.1b"><apply id="S4.T5.20.18.2.m1.1.1.cmml" xref="S4.T5.20.18.2.m1.1.1"><csymbol cd="latexml" id="S4.T5.20.18.2.m1.1.1.2.cmml" xref="S4.T5.20.18.2.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.20.18.2.m1.1.1.3.cmml" type="float" xref="S4.T5.20.18.2.m1.1.1.3">14.54</cn><apply id="S4.T5.20.18.2.m1.1.1.1.cmml" xref="S4.T5.20.18.2.m1.1.1.1"><times id="S4.T5.20.18.2.m1.1.1.1.2.cmml" xref="S4.T5.20.18.2.m1.1.1.1.2"></times><cn id="S4.T5.20.18.2.m1.1.1.1.3.cmml" type="float" xref="S4.T5.20.18.2.m1.1.1.1.3">0.23</cn><apply id="S4.T5.20.18.2.m1.1.1.1.1.1.1.cmml" xref="S4.T5.20.18.2.m1.1.1.1.1.1"><minus id="S4.T5.20.18.2.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.20.18.2.m1.1.1.1.1.1"></minus><apply id="S4.T5.20.18.2.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.20.18.2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.20.18.2.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.20.18.2.m1.1.1.1.1.1.1.2.2">69.38</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.20.18.2.m1.1c">14.54\pm 0.23(-69.38\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.20.18.2.m1.1d">14.54 ± 0.23 ( - 69.38 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.21.19.3"><math alttext="0.97\pm 0.00(+5.80\%)" class="ltx_Math" display="inline" id="S4.T5.21.19.3.m1.1"><semantics id="S4.T5.21.19.3.m1.1a"><mrow id="S4.T5.21.19.3.m1.1.1" xref="S4.T5.21.19.3.m1.1.1.cmml"><mn id="S4.T5.21.19.3.m1.1.1.3" xref="S4.T5.21.19.3.m1.1.1.3.cmml">0.97</mn><mo id="S4.T5.21.19.3.m1.1.1.2" xref="S4.T5.21.19.3.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.21.19.3.m1.1.1.1" xref="S4.T5.21.19.3.m1.1.1.1.cmml"><mn id="S4.T5.21.19.3.m1.1.1.1.3" xref="S4.T5.21.19.3.m1.1.1.1.3.cmml">0.00</mn><mo id="S4.T5.21.19.3.m1.1.1.1.2" xref="S4.T5.21.19.3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.21.19.3.m1.1.1.1.1.1" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.21.19.3.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.21.19.3.m1.1.1.1.1.1.1" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.21.19.3.m1.1.1.1.1.1.1a" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.21.19.3.m1.1.1.1.1.1.1.2" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.21.19.3.m1.1.1.1.1.1.1.2.2" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.2.2.cmml">5.80</mn><mo id="S4.T5.21.19.3.m1.1.1.1.1.1.1.2.1" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.21.19.3.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.21.19.3.m1.1b"><apply id="S4.T5.21.19.3.m1.1.1.cmml" xref="S4.T5.21.19.3.m1.1.1"><csymbol cd="latexml" id="S4.T5.21.19.3.m1.1.1.2.cmml" xref="S4.T5.21.19.3.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.21.19.3.m1.1.1.3.cmml" type="float" xref="S4.T5.21.19.3.m1.1.1.3">0.97</cn><apply id="S4.T5.21.19.3.m1.1.1.1.cmml" xref="S4.T5.21.19.3.m1.1.1.1"><times id="S4.T5.21.19.3.m1.1.1.1.2.cmml" xref="S4.T5.21.19.3.m1.1.1.1.2"></times><cn id="S4.T5.21.19.3.m1.1.1.1.3.cmml" type="float" xref="S4.T5.21.19.3.m1.1.1.1.3">0.00</cn><apply id="S4.T5.21.19.3.m1.1.1.1.1.1.1.cmml" xref="S4.T5.21.19.3.m1.1.1.1.1.1"><plus id="S4.T5.21.19.3.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.21.19.3.m1.1.1.1.1.1"></plus><apply id="S4.T5.21.19.3.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.21.19.3.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.21.19.3.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.21.19.3.m1.1.1.1.1.1.1.2.2">5.80</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.21.19.3.m1.1c">0.97\pm 0.00(+5.80\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.21.19.3.m1.1d">0.97 ± 0.00 ( + 5.80 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.22.20.4"><math alttext="0.92\pm 0.00(+6.55\%)" class="ltx_Math" display="inline" id="S4.T5.22.20.4.m1.1"><semantics id="S4.T5.22.20.4.m1.1a"><mrow id="S4.T5.22.20.4.m1.1.1" xref="S4.T5.22.20.4.m1.1.1.cmml"><mn id="S4.T5.22.20.4.m1.1.1.3" xref="S4.T5.22.20.4.m1.1.1.3.cmml">0.92</mn><mo id="S4.T5.22.20.4.m1.1.1.2" xref="S4.T5.22.20.4.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.22.20.4.m1.1.1.1" xref="S4.T5.22.20.4.m1.1.1.1.cmml"><mn id="S4.T5.22.20.4.m1.1.1.1.3" xref="S4.T5.22.20.4.m1.1.1.1.3.cmml">0.00</mn><mo id="S4.T5.22.20.4.m1.1.1.1.2" xref="S4.T5.22.20.4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.22.20.4.m1.1.1.1.1.1" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.22.20.4.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.22.20.4.m1.1.1.1.1.1.1" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.22.20.4.m1.1.1.1.1.1.1a" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.22.20.4.m1.1.1.1.1.1.1.2" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.22.20.4.m1.1.1.1.1.1.1.2.2" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.2.2.cmml">6.55</mn><mo id="S4.T5.22.20.4.m1.1.1.1.1.1.1.2.1" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.22.20.4.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.22.20.4.m1.1b"><apply id="S4.T5.22.20.4.m1.1.1.cmml" xref="S4.T5.22.20.4.m1.1.1"><csymbol cd="latexml" id="S4.T5.22.20.4.m1.1.1.2.cmml" xref="S4.T5.22.20.4.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.22.20.4.m1.1.1.3.cmml" type="float" xref="S4.T5.22.20.4.m1.1.1.3">0.92</cn><apply id="S4.T5.22.20.4.m1.1.1.1.cmml" xref="S4.T5.22.20.4.m1.1.1.1"><times id="S4.T5.22.20.4.m1.1.1.1.2.cmml" xref="S4.T5.22.20.4.m1.1.1.1.2"></times><cn id="S4.T5.22.20.4.m1.1.1.1.3.cmml" type="float" xref="S4.T5.22.20.4.m1.1.1.1.3">0.00</cn><apply id="S4.T5.22.20.4.m1.1.1.1.1.1.1.cmml" xref="S4.T5.22.20.4.m1.1.1.1.1.1"><plus id="S4.T5.22.20.4.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.22.20.4.m1.1.1.1.1.1"></plus><apply id="S4.T5.22.20.4.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.22.20.4.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.22.20.4.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.22.20.4.m1.1.1.1.1.1.1.2.2">6.55</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.22.20.4.m1.1c">0.92\pm 0.00(+6.55\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.22.20.4.m1.1d">0.92 ± 0.00 ( + 6.55 % )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T5.26.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.26.24.5">wrists</th>
<td class="ltx_td ltx_align_left" id="S4.T5.23.21.1"><math alttext="0.85\pm 0.01(-0.19\%)" class="ltx_Math" display="inline" id="S4.T5.23.21.1.m1.1"><semantics id="S4.T5.23.21.1.m1.1a"><mrow id="S4.T5.23.21.1.m1.1.1" xref="S4.T5.23.21.1.m1.1.1.cmml"><mn id="S4.T5.23.21.1.m1.1.1.3" xref="S4.T5.23.21.1.m1.1.1.3.cmml">0.85</mn><mo id="S4.T5.23.21.1.m1.1.1.2" xref="S4.T5.23.21.1.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.23.21.1.m1.1.1.1" xref="S4.T5.23.21.1.m1.1.1.1.cmml"><mn id="S4.T5.23.21.1.m1.1.1.1.3" xref="S4.T5.23.21.1.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.23.21.1.m1.1.1.1.2" xref="S4.T5.23.21.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.23.21.1.m1.1.1.1.1.1" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.23.21.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.23.21.1.m1.1.1.1.1.1.1" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.23.21.1.m1.1.1.1.1.1.1a" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.23.21.1.m1.1.1.1.1.1.1.2" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.23.21.1.m1.1.1.1.1.1.1.2.2" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.2.2.cmml">0.19</mn><mo id="S4.T5.23.21.1.m1.1.1.1.1.1.1.2.1" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.23.21.1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.23.21.1.m1.1b"><apply id="S4.T5.23.21.1.m1.1.1.cmml" xref="S4.T5.23.21.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.23.21.1.m1.1.1.2.cmml" xref="S4.T5.23.21.1.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.23.21.1.m1.1.1.3.cmml" type="float" xref="S4.T5.23.21.1.m1.1.1.3">0.85</cn><apply id="S4.T5.23.21.1.m1.1.1.1.cmml" xref="S4.T5.23.21.1.m1.1.1.1"><times id="S4.T5.23.21.1.m1.1.1.1.2.cmml" xref="S4.T5.23.21.1.m1.1.1.1.2"></times><cn id="S4.T5.23.21.1.m1.1.1.1.3.cmml" type="float" xref="S4.T5.23.21.1.m1.1.1.1.3">0.01</cn><apply id="S4.T5.23.21.1.m1.1.1.1.1.1.1.cmml" xref="S4.T5.23.21.1.m1.1.1.1.1.1"><minus id="S4.T5.23.21.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.23.21.1.m1.1.1.1.1.1"></minus><apply id="S4.T5.23.21.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.23.21.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.23.21.1.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.23.21.1.m1.1.1.1.1.1.1.2.2">0.19</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.23.21.1.m1.1c">0.85\pm 0.01(-0.19\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.23.21.1.m1.1d">0.85 ± 0.01 ( - 0.19 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.24.22.2"><math alttext="16.22\pm 1.65(-76.22\%)" class="ltx_Math" display="inline" id="S4.T5.24.22.2.m1.1"><semantics id="S4.T5.24.22.2.m1.1a"><mrow id="S4.T5.24.22.2.m1.1.1" xref="S4.T5.24.22.2.m1.1.1.cmml"><mn id="S4.T5.24.22.2.m1.1.1.3" xref="S4.T5.24.22.2.m1.1.1.3.cmml">16.22</mn><mo id="S4.T5.24.22.2.m1.1.1.2" xref="S4.T5.24.22.2.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.24.22.2.m1.1.1.1" xref="S4.T5.24.22.2.m1.1.1.1.cmml"><mn id="S4.T5.24.22.2.m1.1.1.1.3" xref="S4.T5.24.22.2.m1.1.1.1.3.cmml">1.65</mn><mo id="S4.T5.24.22.2.m1.1.1.1.2" xref="S4.T5.24.22.2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.24.22.2.m1.1.1.1.1.1" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.24.22.2.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.24.22.2.m1.1.1.1.1.1.1" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.24.22.2.m1.1.1.1.1.1.1a" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.24.22.2.m1.1.1.1.1.1.1.2" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.24.22.2.m1.1.1.1.1.1.1.2.2" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.2.2.cmml">76.22</mn><mo id="S4.T5.24.22.2.m1.1.1.1.1.1.1.2.1" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.24.22.2.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.24.22.2.m1.1b"><apply id="S4.T5.24.22.2.m1.1.1.cmml" xref="S4.T5.24.22.2.m1.1.1"><csymbol cd="latexml" id="S4.T5.24.22.2.m1.1.1.2.cmml" xref="S4.T5.24.22.2.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.24.22.2.m1.1.1.3.cmml" type="float" xref="S4.T5.24.22.2.m1.1.1.3">16.22</cn><apply id="S4.T5.24.22.2.m1.1.1.1.cmml" xref="S4.T5.24.22.2.m1.1.1.1"><times id="S4.T5.24.22.2.m1.1.1.1.2.cmml" xref="S4.T5.24.22.2.m1.1.1.1.2"></times><cn id="S4.T5.24.22.2.m1.1.1.1.3.cmml" type="float" xref="S4.T5.24.22.2.m1.1.1.1.3">1.65</cn><apply id="S4.T5.24.22.2.m1.1.1.1.1.1.1.cmml" xref="S4.T5.24.22.2.m1.1.1.1.1.1"><minus id="S4.T5.24.22.2.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.24.22.2.m1.1.1.1.1.1"></minus><apply id="S4.T5.24.22.2.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.24.22.2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.24.22.2.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.24.22.2.m1.1.1.1.1.1.1.2.2">76.22</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.24.22.2.m1.1c">16.22\pm 1.65(-76.22\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.24.22.2.m1.1d">16.22 ± 1.65 ( - 76.22 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.25.23.3"><math alttext="0.92\pm 0.02(+6.13\%)" class="ltx_Math" display="inline" id="S4.T5.25.23.3.m1.1"><semantics id="S4.T5.25.23.3.m1.1a"><mrow id="S4.T5.25.23.3.m1.1.1" xref="S4.T5.25.23.3.m1.1.1.cmml"><mn id="S4.T5.25.23.3.m1.1.1.3" xref="S4.T5.25.23.3.m1.1.1.3.cmml">0.92</mn><mo id="S4.T5.25.23.3.m1.1.1.2" xref="S4.T5.25.23.3.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.25.23.3.m1.1.1.1" xref="S4.T5.25.23.3.m1.1.1.1.cmml"><mn id="S4.T5.25.23.3.m1.1.1.1.3" xref="S4.T5.25.23.3.m1.1.1.1.3.cmml">0.02</mn><mo id="S4.T5.25.23.3.m1.1.1.1.2" xref="S4.T5.25.23.3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.25.23.3.m1.1.1.1.1.1" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.25.23.3.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.25.23.3.m1.1.1.1.1.1.1" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.25.23.3.m1.1.1.1.1.1.1a" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.25.23.3.m1.1.1.1.1.1.1.2" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.25.23.3.m1.1.1.1.1.1.1.2.2" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.2.2.cmml">6.13</mn><mo id="S4.T5.25.23.3.m1.1.1.1.1.1.1.2.1" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.25.23.3.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.25.23.3.m1.1b"><apply id="S4.T5.25.23.3.m1.1.1.cmml" xref="S4.T5.25.23.3.m1.1.1"><csymbol cd="latexml" id="S4.T5.25.23.3.m1.1.1.2.cmml" xref="S4.T5.25.23.3.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.25.23.3.m1.1.1.3.cmml" type="float" xref="S4.T5.25.23.3.m1.1.1.3">0.92</cn><apply id="S4.T5.25.23.3.m1.1.1.1.cmml" xref="S4.T5.25.23.3.m1.1.1.1"><times id="S4.T5.25.23.3.m1.1.1.1.2.cmml" xref="S4.T5.25.23.3.m1.1.1.1.2"></times><cn id="S4.T5.25.23.3.m1.1.1.1.3.cmml" type="float" xref="S4.T5.25.23.3.m1.1.1.1.3">0.02</cn><apply id="S4.T5.25.23.3.m1.1.1.1.1.1.1.cmml" xref="S4.T5.25.23.3.m1.1.1.1.1.1"><plus id="S4.T5.25.23.3.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.25.23.3.m1.1.1.1.1.1"></plus><apply id="S4.T5.25.23.3.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.25.23.3.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.25.23.3.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.25.23.3.m1.1.1.1.1.1.1.2.2">6.13</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.25.23.3.m1.1c">0.92\pm 0.02(+6.13\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.25.23.3.m1.1d">0.92 ± 0.02 ( + 6.13 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.26.24.4"><math alttext="0.88\pm 0.02(+1.73\%)" class="ltx_Math" display="inline" id="S4.T5.26.24.4.m1.1"><semantics id="S4.T5.26.24.4.m1.1a"><mrow id="S4.T5.26.24.4.m1.1.1" xref="S4.T5.26.24.4.m1.1.1.cmml"><mn id="S4.T5.26.24.4.m1.1.1.3" xref="S4.T5.26.24.4.m1.1.1.3.cmml">0.88</mn><mo id="S4.T5.26.24.4.m1.1.1.2" xref="S4.T5.26.24.4.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.26.24.4.m1.1.1.1" xref="S4.T5.26.24.4.m1.1.1.1.cmml"><mn id="S4.T5.26.24.4.m1.1.1.1.3" xref="S4.T5.26.24.4.m1.1.1.1.3.cmml">0.02</mn><mo id="S4.T5.26.24.4.m1.1.1.1.2" xref="S4.T5.26.24.4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.26.24.4.m1.1.1.1.1.1" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.26.24.4.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.26.24.4.m1.1.1.1.1.1.1" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.26.24.4.m1.1.1.1.1.1.1a" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.26.24.4.m1.1.1.1.1.1.1.2" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.26.24.4.m1.1.1.1.1.1.1.2.2" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.2.2.cmml">1.73</mn><mo id="S4.T5.26.24.4.m1.1.1.1.1.1.1.2.1" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.26.24.4.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.26.24.4.m1.1b"><apply id="S4.T5.26.24.4.m1.1.1.cmml" xref="S4.T5.26.24.4.m1.1.1"><csymbol cd="latexml" id="S4.T5.26.24.4.m1.1.1.2.cmml" xref="S4.T5.26.24.4.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.26.24.4.m1.1.1.3.cmml" type="float" xref="S4.T5.26.24.4.m1.1.1.3">0.88</cn><apply id="S4.T5.26.24.4.m1.1.1.1.cmml" xref="S4.T5.26.24.4.m1.1.1.1"><times id="S4.T5.26.24.4.m1.1.1.1.2.cmml" xref="S4.T5.26.24.4.m1.1.1.1.2"></times><cn id="S4.T5.26.24.4.m1.1.1.1.3.cmml" type="float" xref="S4.T5.26.24.4.m1.1.1.1.3">0.02</cn><apply id="S4.T5.26.24.4.m1.1.1.1.1.1.1.cmml" xref="S4.T5.26.24.4.m1.1.1.1.1.1"><plus id="S4.T5.26.24.4.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.26.24.4.m1.1.1.1.1.1"></plus><apply id="S4.T5.26.24.4.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.26.24.4.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.26.24.4.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.26.24.4.m1.1.1.1.1.1.1.2.2">1.73</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.26.24.4.m1.1c">0.88\pm 0.02(+1.73\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.26.24.4.m1.1d">0.88 ± 0.02 ( + 1.73 % )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T5.30.28">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.30.28.5">hips</th>
<td class="ltx_td ltx_align_left" id="S4.T5.27.25.1"><math alttext="0.65\pm 0.01(-18.96\%)" class="ltx_Math" display="inline" id="S4.T5.27.25.1.m1.1"><semantics id="S4.T5.27.25.1.m1.1a"><mrow id="S4.T5.27.25.1.m1.1.1" xref="S4.T5.27.25.1.m1.1.1.cmml"><mn id="S4.T5.27.25.1.m1.1.1.3" xref="S4.T5.27.25.1.m1.1.1.3.cmml">0.65</mn><mo id="S4.T5.27.25.1.m1.1.1.2" xref="S4.T5.27.25.1.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.27.25.1.m1.1.1.1" xref="S4.T5.27.25.1.m1.1.1.1.cmml"><mn id="S4.T5.27.25.1.m1.1.1.1.3" xref="S4.T5.27.25.1.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.27.25.1.m1.1.1.1.2" xref="S4.T5.27.25.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.27.25.1.m1.1.1.1.1.1" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.27.25.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.27.25.1.m1.1.1.1.1.1.1" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.27.25.1.m1.1.1.1.1.1.1a" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.27.25.1.m1.1.1.1.1.1.1.2" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.27.25.1.m1.1.1.1.1.1.1.2.2" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.2.2.cmml">18.96</mn><mo id="S4.T5.27.25.1.m1.1.1.1.1.1.1.2.1" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.27.25.1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.27.25.1.m1.1b"><apply id="S4.T5.27.25.1.m1.1.1.cmml" xref="S4.T5.27.25.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.27.25.1.m1.1.1.2.cmml" xref="S4.T5.27.25.1.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.27.25.1.m1.1.1.3.cmml" type="float" xref="S4.T5.27.25.1.m1.1.1.3">0.65</cn><apply id="S4.T5.27.25.1.m1.1.1.1.cmml" xref="S4.T5.27.25.1.m1.1.1.1"><times id="S4.T5.27.25.1.m1.1.1.1.2.cmml" xref="S4.T5.27.25.1.m1.1.1.1.2"></times><cn id="S4.T5.27.25.1.m1.1.1.1.3.cmml" type="float" xref="S4.T5.27.25.1.m1.1.1.1.3">0.01</cn><apply id="S4.T5.27.25.1.m1.1.1.1.1.1.1.cmml" xref="S4.T5.27.25.1.m1.1.1.1.1.1"><minus id="S4.T5.27.25.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.27.25.1.m1.1.1.1.1.1"></minus><apply id="S4.T5.27.25.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.27.25.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.27.25.1.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.27.25.1.m1.1.1.1.1.1.1.2.2">18.96</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.27.25.1.m1.1c">0.65\pm 0.01(-18.96\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.27.25.1.m1.1d">0.65 ± 0.01 ( - 18.96 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.28.26.2"><math alttext="22.96\pm 0.61(-51.24\%)" class="ltx_Math" display="inline" id="S4.T5.28.26.2.m1.1"><semantics id="S4.T5.28.26.2.m1.1a"><mrow id="S4.T5.28.26.2.m1.1.1" xref="S4.T5.28.26.2.m1.1.1.cmml"><mn id="S4.T5.28.26.2.m1.1.1.3" xref="S4.T5.28.26.2.m1.1.1.3.cmml">22.96</mn><mo id="S4.T5.28.26.2.m1.1.1.2" xref="S4.T5.28.26.2.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.28.26.2.m1.1.1.1" xref="S4.T5.28.26.2.m1.1.1.1.cmml"><mn id="S4.T5.28.26.2.m1.1.1.1.3" xref="S4.T5.28.26.2.m1.1.1.1.3.cmml">0.61</mn><mo id="S4.T5.28.26.2.m1.1.1.1.2" xref="S4.T5.28.26.2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.28.26.2.m1.1.1.1.1.1" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.28.26.2.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.28.26.2.m1.1.1.1.1.1.1" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.28.26.2.m1.1.1.1.1.1.1a" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.28.26.2.m1.1.1.1.1.1.1.2" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.28.26.2.m1.1.1.1.1.1.1.2.2" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.2.2.cmml">51.24</mn><mo id="S4.T5.28.26.2.m1.1.1.1.1.1.1.2.1" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.28.26.2.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.28.26.2.m1.1b"><apply id="S4.T5.28.26.2.m1.1.1.cmml" xref="S4.T5.28.26.2.m1.1.1"><csymbol cd="latexml" id="S4.T5.28.26.2.m1.1.1.2.cmml" xref="S4.T5.28.26.2.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.28.26.2.m1.1.1.3.cmml" type="float" xref="S4.T5.28.26.2.m1.1.1.3">22.96</cn><apply id="S4.T5.28.26.2.m1.1.1.1.cmml" xref="S4.T5.28.26.2.m1.1.1.1"><times id="S4.T5.28.26.2.m1.1.1.1.2.cmml" xref="S4.T5.28.26.2.m1.1.1.1.2"></times><cn id="S4.T5.28.26.2.m1.1.1.1.3.cmml" type="float" xref="S4.T5.28.26.2.m1.1.1.1.3">0.61</cn><apply id="S4.T5.28.26.2.m1.1.1.1.1.1.1.cmml" xref="S4.T5.28.26.2.m1.1.1.1.1.1"><minus id="S4.T5.28.26.2.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.28.26.2.m1.1.1.1.1.1"></minus><apply id="S4.T5.28.26.2.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.28.26.2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.28.26.2.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.28.26.2.m1.1.1.1.1.1.1.2.2">51.24</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.28.26.2.m1.1c">22.96\pm 0.61(-51.24\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.28.26.2.m1.1d">22.96 ± 0.61 ( - 51.24 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.29.27.3"><math alttext="0.97\pm 0.00(-2.02\%)" class="ltx_Math" display="inline" id="S4.T5.29.27.3.m1.1"><semantics id="S4.T5.29.27.3.m1.1a"><mrow id="S4.T5.29.27.3.m1.1.1" xref="S4.T5.29.27.3.m1.1.1.cmml"><mn id="S4.T5.29.27.3.m1.1.1.3" xref="S4.T5.29.27.3.m1.1.1.3.cmml">0.97</mn><mo id="S4.T5.29.27.3.m1.1.1.2" xref="S4.T5.29.27.3.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.29.27.3.m1.1.1.1" xref="S4.T5.29.27.3.m1.1.1.1.cmml"><mn id="S4.T5.29.27.3.m1.1.1.1.3" xref="S4.T5.29.27.3.m1.1.1.1.3.cmml">0.00</mn><mo id="S4.T5.29.27.3.m1.1.1.1.2" xref="S4.T5.29.27.3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.29.27.3.m1.1.1.1.1.1" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.29.27.3.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.29.27.3.m1.1.1.1.1.1.1" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.29.27.3.m1.1.1.1.1.1.1a" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.29.27.3.m1.1.1.1.1.1.1.2" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.29.27.3.m1.1.1.1.1.1.1.2.2" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.2.2.cmml">2.02</mn><mo id="S4.T5.29.27.3.m1.1.1.1.1.1.1.2.1" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.29.27.3.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.29.27.3.m1.1b"><apply id="S4.T5.29.27.3.m1.1.1.cmml" xref="S4.T5.29.27.3.m1.1.1"><csymbol cd="latexml" id="S4.T5.29.27.3.m1.1.1.2.cmml" xref="S4.T5.29.27.3.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.29.27.3.m1.1.1.3.cmml" type="float" xref="S4.T5.29.27.3.m1.1.1.3">0.97</cn><apply id="S4.T5.29.27.3.m1.1.1.1.cmml" xref="S4.T5.29.27.3.m1.1.1.1"><times id="S4.T5.29.27.3.m1.1.1.1.2.cmml" xref="S4.T5.29.27.3.m1.1.1.1.2"></times><cn id="S4.T5.29.27.3.m1.1.1.1.3.cmml" type="float" xref="S4.T5.29.27.3.m1.1.1.1.3">0.00</cn><apply id="S4.T5.29.27.3.m1.1.1.1.1.1.1.cmml" xref="S4.T5.29.27.3.m1.1.1.1.1.1"><minus id="S4.T5.29.27.3.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.29.27.3.m1.1.1.1.1.1"></minus><apply id="S4.T5.29.27.3.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.29.27.3.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.29.27.3.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.29.27.3.m1.1.1.1.1.1.1.2.2">2.02</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.29.27.3.m1.1c">0.97\pm 0.00(-2.02\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.29.27.3.m1.1d">0.97 ± 0.00 ( - 2.02 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.30.28.4"><math alttext="0.89\pm 0.01(-6.63\%)" class="ltx_Math" display="inline" id="S4.T5.30.28.4.m1.1"><semantics id="S4.T5.30.28.4.m1.1a"><mrow id="S4.T5.30.28.4.m1.1.1" xref="S4.T5.30.28.4.m1.1.1.cmml"><mn id="S4.T5.30.28.4.m1.1.1.3" xref="S4.T5.30.28.4.m1.1.1.3.cmml">0.89</mn><mo id="S4.T5.30.28.4.m1.1.1.2" xref="S4.T5.30.28.4.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.30.28.4.m1.1.1.1" xref="S4.T5.30.28.4.m1.1.1.1.cmml"><mn id="S4.T5.30.28.4.m1.1.1.1.3" xref="S4.T5.30.28.4.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.30.28.4.m1.1.1.1.2" xref="S4.T5.30.28.4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.30.28.4.m1.1.1.1.1.1" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.30.28.4.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.30.28.4.m1.1.1.1.1.1.1" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.30.28.4.m1.1.1.1.1.1.1a" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.30.28.4.m1.1.1.1.1.1.1.2" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.30.28.4.m1.1.1.1.1.1.1.2.2" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.2.2.cmml">6.63</mn><mo id="S4.T5.30.28.4.m1.1.1.1.1.1.1.2.1" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.30.28.4.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.30.28.4.m1.1b"><apply id="S4.T5.30.28.4.m1.1.1.cmml" xref="S4.T5.30.28.4.m1.1.1"><csymbol cd="latexml" id="S4.T5.30.28.4.m1.1.1.2.cmml" xref="S4.T5.30.28.4.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.30.28.4.m1.1.1.3.cmml" type="float" xref="S4.T5.30.28.4.m1.1.1.3">0.89</cn><apply id="S4.T5.30.28.4.m1.1.1.1.cmml" xref="S4.T5.30.28.4.m1.1.1.1"><times id="S4.T5.30.28.4.m1.1.1.1.2.cmml" xref="S4.T5.30.28.4.m1.1.1.1.2"></times><cn id="S4.T5.30.28.4.m1.1.1.1.3.cmml" type="float" xref="S4.T5.30.28.4.m1.1.1.1.3">0.01</cn><apply id="S4.T5.30.28.4.m1.1.1.1.1.1.1.cmml" xref="S4.T5.30.28.4.m1.1.1.1.1.1"><minus id="S4.T5.30.28.4.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.30.28.4.m1.1.1.1.1.1"></minus><apply id="S4.T5.30.28.4.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.30.28.4.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.30.28.4.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.30.28.4.m1.1.1.1.1.1.1.2.2">6.63</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.30.28.4.m1.1c">0.89\pm 0.01(-6.63\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.30.28.4.m1.1d">0.89 ± 0.01 ( - 6.63 % )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T5.34.32">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.34.32.5">knees</th>
<td class="ltx_td ltx_align_left" id="S4.T5.31.29.1"><math alttext="0.89\pm 0.02(+6.63\%)" class="ltx_Math" display="inline" id="S4.T5.31.29.1.m1.1"><semantics id="S4.T5.31.29.1.m1.1a"><mrow id="S4.T5.31.29.1.m1.1.1" xref="S4.T5.31.29.1.m1.1.1.cmml"><mn id="S4.T5.31.29.1.m1.1.1.3" xref="S4.T5.31.29.1.m1.1.1.3.cmml">0.89</mn><mo id="S4.T5.31.29.1.m1.1.1.2" xref="S4.T5.31.29.1.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.31.29.1.m1.1.1.1" xref="S4.T5.31.29.1.m1.1.1.1.cmml"><mn id="S4.T5.31.29.1.m1.1.1.1.3" xref="S4.T5.31.29.1.m1.1.1.1.3.cmml">0.02</mn><mo id="S4.T5.31.29.1.m1.1.1.1.2" xref="S4.T5.31.29.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.31.29.1.m1.1.1.1.1.1" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.31.29.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.31.29.1.m1.1.1.1.1.1.1" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.31.29.1.m1.1.1.1.1.1.1a" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.31.29.1.m1.1.1.1.1.1.1.2" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.31.29.1.m1.1.1.1.1.1.1.2.2" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.2.2.cmml">6.63</mn><mo id="S4.T5.31.29.1.m1.1.1.1.1.1.1.2.1" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.31.29.1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.31.29.1.m1.1b"><apply id="S4.T5.31.29.1.m1.1.1.cmml" xref="S4.T5.31.29.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.31.29.1.m1.1.1.2.cmml" xref="S4.T5.31.29.1.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.31.29.1.m1.1.1.3.cmml" type="float" xref="S4.T5.31.29.1.m1.1.1.3">0.89</cn><apply id="S4.T5.31.29.1.m1.1.1.1.cmml" xref="S4.T5.31.29.1.m1.1.1.1"><times id="S4.T5.31.29.1.m1.1.1.1.2.cmml" xref="S4.T5.31.29.1.m1.1.1.1.2"></times><cn id="S4.T5.31.29.1.m1.1.1.1.3.cmml" type="float" xref="S4.T5.31.29.1.m1.1.1.1.3">0.02</cn><apply id="S4.T5.31.29.1.m1.1.1.1.1.1.1.cmml" xref="S4.T5.31.29.1.m1.1.1.1.1.1"><plus id="S4.T5.31.29.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.31.29.1.m1.1.1.1.1.1"></plus><apply id="S4.T5.31.29.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.31.29.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.31.29.1.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.31.29.1.m1.1.1.1.1.1.1.2.2">6.63</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.31.29.1.m1.1c">0.89\pm 0.02(+6.63\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.31.29.1.m1.1d">0.89 ± 0.02 ( + 6.63 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.32.30.2"><math alttext="14.99\pm 0.97(-78.85\%)" class="ltx_Math" display="inline" id="S4.T5.32.30.2.m1.1"><semantics id="S4.T5.32.30.2.m1.1a"><mrow id="S4.T5.32.30.2.m1.1.1" xref="S4.T5.32.30.2.m1.1.1.cmml"><mn id="S4.T5.32.30.2.m1.1.1.3" xref="S4.T5.32.30.2.m1.1.1.3.cmml">14.99</mn><mo id="S4.T5.32.30.2.m1.1.1.2" xref="S4.T5.32.30.2.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.32.30.2.m1.1.1.1" xref="S4.T5.32.30.2.m1.1.1.1.cmml"><mn id="S4.T5.32.30.2.m1.1.1.1.3" xref="S4.T5.32.30.2.m1.1.1.1.3.cmml">0.97</mn><mo id="S4.T5.32.30.2.m1.1.1.1.2" xref="S4.T5.32.30.2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.32.30.2.m1.1.1.1.1.1" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.32.30.2.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.32.30.2.m1.1.1.1.1.1.1" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.32.30.2.m1.1.1.1.1.1.1a" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.32.30.2.m1.1.1.1.1.1.1.2" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.32.30.2.m1.1.1.1.1.1.1.2.2" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.2.2.cmml">78.85</mn><mo id="S4.T5.32.30.2.m1.1.1.1.1.1.1.2.1" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.32.30.2.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.32.30.2.m1.1b"><apply id="S4.T5.32.30.2.m1.1.1.cmml" xref="S4.T5.32.30.2.m1.1.1"><csymbol cd="latexml" id="S4.T5.32.30.2.m1.1.1.2.cmml" xref="S4.T5.32.30.2.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.32.30.2.m1.1.1.3.cmml" type="float" xref="S4.T5.32.30.2.m1.1.1.3">14.99</cn><apply id="S4.T5.32.30.2.m1.1.1.1.cmml" xref="S4.T5.32.30.2.m1.1.1.1"><times id="S4.T5.32.30.2.m1.1.1.1.2.cmml" xref="S4.T5.32.30.2.m1.1.1.1.2"></times><cn id="S4.T5.32.30.2.m1.1.1.1.3.cmml" type="float" xref="S4.T5.32.30.2.m1.1.1.1.3">0.97</cn><apply id="S4.T5.32.30.2.m1.1.1.1.1.1.1.cmml" xref="S4.T5.32.30.2.m1.1.1.1.1.1"><minus id="S4.T5.32.30.2.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.32.30.2.m1.1.1.1.1.1"></minus><apply id="S4.T5.32.30.2.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.32.30.2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.32.30.2.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.32.30.2.m1.1.1.1.1.1.1.2.2">78.85</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.32.30.2.m1.1c">14.99\pm 0.97(-78.85\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.32.30.2.m1.1d">14.99 ± 0.97 ( - 78.85 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T5.33.31.3"><math alttext="0.97\pm 0.01(+5.25\%)" class="ltx_Math" display="inline" id="S4.T5.33.31.3.m1.1"><semantics id="S4.T5.33.31.3.m1.1a"><mrow id="S4.T5.33.31.3.m1.1.1" xref="S4.T5.33.31.3.m1.1.1.cmml"><mn id="S4.T5.33.31.3.m1.1.1.3" xref="S4.T5.33.31.3.m1.1.1.3.cmml">0.97</mn><mo id="S4.T5.33.31.3.m1.1.1.2" xref="S4.T5.33.31.3.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.33.31.3.m1.1.1.1" xref="S4.T5.33.31.3.m1.1.1.1.cmml"><mn id="S4.T5.33.31.3.m1.1.1.1.3" xref="S4.T5.33.31.3.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.33.31.3.m1.1.1.1.2" xref="S4.T5.33.31.3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.33.31.3.m1.1.1.1.1.1" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.33.31.3.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.33.31.3.m1.1.1.1.1.1.1" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.33.31.3.m1.1.1.1.1.1.1a" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.33.31.3.m1.1.1.1.1.1.1.2" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.33.31.3.m1.1.1.1.1.1.1.2.2" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.2.2.cmml">5.25</mn><mo id="S4.T5.33.31.3.m1.1.1.1.1.1.1.2.1" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.33.31.3.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.33.31.3.m1.1b"><apply id="S4.T5.33.31.3.m1.1.1.cmml" xref="S4.T5.33.31.3.m1.1.1"><csymbol cd="latexml" id="S4.T5.33.31.3.m1.1.1.2.cmml" xref="S4.T5.33.31.3.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.33.31.3.m1.1.1.3.cmml" type="float" xref="S4.T5.33.31.3.m1.1.1.3">0.97</cn><apply id="S4.T5.33.31.3.m1.1.1.1.cmml" xref="S4.T5.33.31.3.m1.1.1.1"><times id="S4.T5.33.31.3.m1.1.1.1.2.cmml" xref="S4.T5.33.31.3.m1.1.1.1.2"></times><cn id="S4.T5.33.31.3.m1.1.1.1.3.cmml" type="float" xref="S4.T5.33.31.3.m1.1.1.1.3">0.01</cn><apply id="S4.T5.33.31.3.m1.1.1.1.1.1.1.cmml" xref="S4.T5.33.31.3.m1.1.1.1.1.1"><plus id="S4.T5.33.31.3.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.33.31.3.m1.1.1.1.1.1"></plus><apply id="S4.T5.33.31.3.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.33.31.3.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.33.31.3.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.33.31.3.m1.1.1.1.1.1.1.2.2">5.25</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.33.31.3.m1.1c">0.97\pm 0.01(+5.25\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.33.31.3.m1.1d">0.97 ± 0.01 ( + 5.25 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.34.32.4"><math alttext="0.94\pm 0.01(+7.01\%)" class="ltx_Math" display="inline" id="S4.T5.34.32.4.m1.1"><semantics id="S4.T5.34.32.4.m1.1a"><mrow id="S4.T5.34.32.4.m1.1.1" xref="S4.T5.34.32.4.m1.1.1.cmml"><mn id="S4.T5.34.32.4.m1.1.1.3" xref="S4.T5.34.32.4.m1.1.1.3.cmml">0.94</mn><mo id="S4.T5.34.32.4.m1.1.1.2" xref="S4.T5.34.32.4.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.34.32.4.m1.1.1.1" xref="S4.T5.34.32.4.m1.1.1.1.cmml"><mn id="S4.T5.34.32.4.m1.1.1.1.3" xref="S4.T5.34.32.4.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.34.32.4.m1.1.1.1.2" xref="S4.T5.34.32.4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.34.32.4.m1.1.1.1.1.1" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.34.32.4.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.34.32.4.m1.1.1.1.1.1.1" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.34.32.4.m1.1.1.1.1.1.1a" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.34.32.4.m1.1.1.1.1.1.1.2" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.34.32.4.m1.1.1.1.1.1.1.2.2" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.2.2.cmml">7.01</mn><mo id="S4.T5.34.32.4.m1.1.1.1.1.1.1.2.1" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.34.32.4.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.34.32.4.m1.1b"><apply id="S4.T5.34.32.4.m1.1.1.cmml" xref="S4.T5.34.32.4.m1.1.1"><csymbol cd="latexml" id="S4.T5.34.32.4.m1.1.1.2.cmml" xref="S4.T5.34.32.4.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.34.32.4.m1.1.1.3.cmml" type="float" xref="S4.T5.34.32.4.m1.1.1.3">0.94</cn><apply id="S4.T5.34.32.4.m1.1.1.1.cmml" xref="S4.T5.34.32.4.m1.1.1.1"><times id="S4.T5.34.32.4.m1.1.1.1.2.cmml" xref="S4.T5.34.32.4.m1.1.1.1.2"></times><cn id="S4.T5.34.32.4.m1.1.1.1.3.cmml" type="float" xref="S4.T5.34.32.4.m1.1.1.1.3">0.01</cn><apply id="S4.T5.34.32.4.m1.1.1.1.1.1.1.cmml" xref="S4.T5.34.32.4.m1.1.1.1.1.1"><plus id="S4.T5.34.32.4.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.34.32.4.m1.1.1.1.1.1"></plus><apply id="S4.T5.34.32.4.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.34.32.4.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.34.32.4.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.34.32.4.m1.1.1.1.1.1.1.2.2">7.01</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.34.32.4.m1.1c">0.94\pm 0.01(+7.01\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.34.32.4.m1.1d">0.94 ± 0.01 ( + 7.01 % )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T5.38.36">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T5.38.36.5">ankles</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.35.33.1"><math alttext="0.78\pm 0.01(+76.14\%)" class="ltx_Math" display="inline" id="S4.T5.35.33.1.m1.1"><semantics id="S4.T5.35.33.1.m1.1a"><mrow id="S4.T5.35.33.1.m1.1.1" xref="S4.T5.35.33.1.m1.1.1.cmml"><mn id="S4.T5.35.33.1.m1.1.1.3" xref="S4.T5.35.33.1.m1.1.1.3.cmml">0.78</mn><mo id="S4.T5.35.33.1.m1.1.1.2" xref="S4.T5.35.33.1.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.35.33.1.m1.1.1.1" xref="S4.T5.35.33.1.m1.1.1.1.cmml"><mn id="S4.T5.35.33.1.m1.1.1.1.3" xref="S4.T5.35.33.1.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.35.33.1.m1.1.1.1.2" xref="S4.T5.35.33.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.35.33.1.m1.1.1.1.1.1" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.35.33.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.35.33.1.m1.1.1.1.1.1.1" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.35.33.1.m1.1.1.1.1.1.1a" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.35.33.1.m1.1.1.1.1.1.1.2" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.35.33.1.m1.1.1.1.1.1.1.2.2" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.2.2.cmml">76.14</mn><mo id="S4.T5.35.33.1.m1.1.1.1.1.1.1.2.1" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.35.33.1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.35.33.1.m1.1b"><apply id="S4.T5.35.33.1.m1.1.1.cmml" xref="S4.T5.35.33.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.35.33.1.m1.1.1.2.cmml" xref="S4.T5.35.33.1.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.35.33.1.m1.1.1.3.cmml" type="float" xref="S4.T5.35.33.1.m1.1.1.3">0.78</cn><apply id="S4.T5.35.33.1.m1.1.1.1.cmml" xref="S4.T5.35.33.1.m1.1.1.1"><times id="S4.T5.35.33.1.m1.1.1.1.2.cmml" xref="S4.T5.35.33.1.m1.1.1.1.2"></times><cn id="S4.T5.35.33.1.m1.1.1.1.3.cmml" type="float" xref="S4.T5.35.33.1.m1.1.1.1.3">0.01</cn><apply id="S4.T5.35.33.1.m1.1.1.1.1.1.1.cmml" xref="S4.T5.35.33.1.m1.1.1.1.1.1"><plus id="S4.T5.35.33.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.35.33.1.m1.1.1.1.1.1"></plus><apply id="S4.T5.35.33.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.35.33.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.35.33.1.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.35.33.1.m1.1.1.1.1.1.1.2.2">76.14</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.35.33.1.m1.1c">0.78\pm 0.01(+76.14\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.35.33.1.m1.1d">0.78 ± 0.01 ( + 76.14 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.36.34.2"><math alttext="18.16\pm 0.95(-69.04\%)" class="ltx_Math" display="inline" id="S4.T5.36.34.2.m1.1"><semantics id="S4.T5.36.34.2.m1.1a"><mrow id="S4.T5.36.34.2.m1.1.1" xref="S4.T5.36.34.2.m1.1.1.cmml"><mn id="S4.T5.36.34.2.m1.1.1.3" xref="S4.T5.36.34.2.m1.1.1.3.cmml">18.16</mn><mo id="S4.T5.36.34.2.m1.1.1.2" xref="S4.T5.36.34.2.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.36.34.2.m1.1.1.1" xref="S4.T5.36.34.2.m1.1.1.1.cmml"><mn id="S4.T5.36.34.2.m1.1.1.1.3" xref="S4.T5.36.34.2.m1.1.1.1.3.cmml">0.95</mn><mo id="S4.T5.36.34.2.m1.1.1.1.2" xref="S4.T5.36.34.2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.36.34.2.m1.1.1.1.1.1" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.36.34.2.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.36.34.2.m1.1.1.1.1.1.1" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.36.34.2.m1.1.1.1.1.1.1a" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.T5.36.34.2.m1.1.1.1.1.1.1.2" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.36.34.2.m1.1.1.1.1.1.1.2.2" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.2.2.cmml">69.04</mn><mo id="S4.T5.36.34.2.m1.1.1.1.1.1.1.2.1" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.36.34.2.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.36.34.2.m1.1b"><apply id="S4.T5.36.34.2.m1.1.1.cmml" xref="S4.T5.36.34.2.m1.1.1"><csymbol cd="latexml" id="S4.T5.36.34.2.m1.1.1.2.cmml" xref="S4.T5.36.34.2.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.36.34.2.m1.1.1.3.cmml" type="float" xref="S4.T5.36.34.2.m1.1.1.3">18.16</cn><apply id="S4.T5.36.34.2.m1.1.1.1.cmml" xref="S4.T5.36.34.2.m1.1.1.1"><times id="S4.T5.36.34.2.m1.1.1.1.2.cmml" xref="S4.T5.36.34.2.m1.1.1.1.2"></times><cn id="S4.T5.36.34.2.m1.1.1.1.3.cmml" type="float" xref="S4.T5.36.34.2.m1.1.1.1.3">0.95</cn><apply id="S4.T5.36.34.2.m1.1.1.1.1.1.1.cmml" xref="S4.T5.36.34.2.m1.1.1.1.1.1"><minus id="S4.T5.36.34.2.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.36.34.2.m1.1.1.1.1.1"></minus><apply id="S4.T5.36.34.2.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.36.34.2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.36.34.2.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.36.34.2.m1.1.1.1.1.1.1.2.2">69.04</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.36.34.2.m1.1c">18.16\pm 0.95(-69.04\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.36.34.2.m1.1d">18.16 ± 0.95 ( - 69.04 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.37.35.3"><math alttext="0.94\pm 0.00(+87.67\%)" class="ltx_Math" display="inline" id="S4.T5.37.35.3.m1.1"><semantics id="S4.T5.37.35.3.m1.1a"><mrow id="S4.T5.37.35.3.m1.1.1" xref="S4.T5.37.35.3.m1.1.1.cmml"><mn id="S4.T5.37.35.3.m1.1.1.3" xref="S4.T5.37.35.3.m1.1.1.3.cmml">0.94</mn><mo id="S4.T5.37.35.3.m1.1.1.2" xref="S4.T5.37.35.3.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.37.35.3.m1.1.1.1" xref="S4.T5.37.35.3.m1.1.1.1.cmml"><mn id="S4.T5.37.35.3.m1.1.1.1.3" xref="S4.T5.37.35.3.m1.1.1.1.3.cmml">0.00</mn><mo id="S4.T5.37.35.3.m1.1.1.1.2" xref="S4.T5.37.35.3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.37.35.3.m1.1.1.1.1.1" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.37.35.3.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.37.35.3.m1.1.1.1.1.1.1" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.37.35.3.m1.1.1.1.1.1.1a" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.37.35.3.m1.1.1.1.1.1.1.2" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.37.35.3.m1.1.1.1.1.1.1.2.2" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.2.2.cmml">87.67</mn><mo id="S4.T5.37.35.3.m1.1.1.1.1.1.1.2.1" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.37.35.3.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.37.35.3.m1.1b"><apply id="S4.T5.37.35.3.m1.1.1.cmml" xref="S4.T5.37.35.3.m1.1.1"><csymbol cd="latexml" id="S4.T5.37.35.3.m1.1.1.2.cmml" xref="S4.T5.37.35.3.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.37.35.3.m1.1.1.3.cmml" type="float" xref="S4.T5.37.35.3.m1.1.1.3">0.94</cn><apply id="S4.T5.37.35.3.m1.1.1.1.cmml" xref="S4.T5.37.35.3.m1.1.1.1"><times id="S4.T5.37.35.3.m1.1.1.1.2.cmml" xref="S4.T5.37.35.3.m1.1.1.1.2"></times><cn id="S4.T5.37.35.3.m1.1.1.1.3.cmml" type="float" xref="S4.T5.37.35.3.m1.1.1.1.3">0.00</cn><apply id="S4.T5.37.35.3.m1.1.1.1.1.1.1.cmml" xref="S4.T5.37.35.3.m1.1.1.1.1.1"><plus id="S4.T5.37.35.3.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.37.35.3.m1.1.1.1.1.1"></plus><apply id="S4.T5.37.35.3.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.37.35.3.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.37.35.3.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.37.35.3.m1.1.1.1.1.1.1.2.2">87.67</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.37.35.3.m1.1c">0.94\pm 0.00(+87.67\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.37.35.3.m1.1d">0.94 ± 0.00 ( + 87.67 % )</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T5.38.36.4"><math alttext="0.90\pm 0.01(+91.49\%)" class="ltx_Math" display="inline" id="S4.T5.38.36.4.m1.1"><semantics id="S4.T5.38.36.4.m1.1a"><mrow id="S4.T5.38.36.4.m1.1.1" xref="S4.T5.38.36.4.m1.1.1.cmml"><mn id="S4.T5.38.36.4.m1.1.1.3" xref="S4.T5.38.36.4.m1.1.1.3.cmml">0.90</mn><mo id="S4.T5.38.36.4.m1.1.1.2" xref="S4.T5.38.36.4.m1.1.1.2.cmml">±</mo><mrow id="S4.T5.38.36.4.m1.1.1.1" xref="S4.T5.38.36.4.m1.1.1.1.cmml"><mn id="S4.T5.38.36.4.m1.1.1.1.3" xref="S4.T5.38.36.4.m1.1.1.1.3.cmml">0.01</mn><mo id="S4.T5.38.36.4.m1.1.1.1.2" xref="S4.T5.38.36.4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.T5.38.36.4.m1.1.1.1.1.1" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.38.36.4.m1.1.1.1.1.1.2" stretchy="false" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.38.36.4.m1.1.1.1.1.1.1" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.cmml"><mo id="S4.T5.38.36.4.m1.1.1.1.1.1.1a" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.T5.38.36.4.m1.1.1.1.1.1.1.2" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.T5.38.36.4.m1.1.1.1.1.1.1.2.2" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.2.2.cmml">91.49</mn><mo id="S4.T5.38.36.4.m1.1.1.1.1.1.1.2.1" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S4.T5.38.36.4.m1.1.1.1.1.1.3" stretchy="false" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.38.36.4.m1.1b"><apply id="S4.T5.38.36.4.m1.1.1.cmml" xref="S4.T5.38.36.4.m1.1.1"><csymbol cd="latexml" id="S4.T5.38.36.4.m1.1.1.2.cmml" xref="S4.T5.38.36.4.m1.1.1.2">plus-or-minus</csymbol><cn id="S4.T5.38.36.4.m1.1.1.3.cmml" type="float" xref="S4.T5.38.36.4.m1.1.1.3">0.90</cn><apply id="S4.T5.38.36.4.m1.1.1.1.cmml" xref="S4.T5.38.36.4.m1.1.1.1"><times id="S4.T5.38.36.4.m1.1.1.1.2.cmml" xref="S4.T5.38.36.4.m1.1.1.1.2"></times><cn id="S4.T5.38.36.4.m1.1.1.1.3.cmml" type="float" xref="S4.T5.38.36.4.m1.1.1.1.3">0.01</cn><apply id="S4.T5.38.36.4.m1.1.1.1.1.1.1.cmml" xref="S4.T5.38.36.4.m1.1.1.1.1.1"><plus id="S4.T5.38.36.4.m1.1.1.1.1.1.1.1.cmml" xref="S4.T5.38.36.4.m1.1.1.1.1.1"></plus><apply id="S4.T5.38.36.4.m1.1.1.1.1.1.1.2.cmml" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.T5.38.36.4.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn id="S4.T5.38.36.4.m1.1.1.1.1.1.1.2.2.cmml" type="float" xref="S4.T5.38.36.4.m1.1.1.1.1.1.1.2.2">91.49</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.38.36.4.m1.1c">0.90\pm 0.01(+91.49\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T5.38.36.4.m1.1d">0.90 ± 0.01 ( + 91.49 % )</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.SSS4.p6">
<p class="ltx_p" id="S4.SS3.SSS4.p6.1">Finally, we compute a set of per-keypoint metrics shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T5" title="In 4.3.4. WheelPose-Opt Model Performance Deep Dive ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> to analyze the differences in specific keypoint predictions between <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS4.p6.1.1">WheelPose</span>-Opt and the baseline. We only showcase percent change with respect to ImageNet as the PSP fine-tuned model performs drastically worse in nearly all BB and KP metrics. Upon examining the percentage of detected joint (PDJ) <cite class="ltx_cite ltx_citemacro_citep">(Sapp and Taskar, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib84" title="">2013</a>)</cite> at a 5% threshold, a measure of a model’s ability to identify a joint, we notice a 76.14% improvement in ankle detection, attributed to more information on foot placement in wheelchairs, and a 18.96% decrease in detected hips, attributed to the wheelchair obstructing most of the lower torso and hip area. We then compute the per joint position error (PJPE), a simple accuracy metric measuring the Euclidean distance error in detected joints, and found that as long as a joint is detected, <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS4.p6.1.2">WheelPose</span>-Opt predicts keypoints over 51.24% (hips) more accurately. Finally, we compute the meaned per-keypoint precision values at OKS thresholds of 0.5 and 0.75 as another measure of individual keypoint prediction accuracy. We see slight improvements across most joints. Similar to PDJ, we can see similar trends in the ankles and hips, improving a significant amount or slightly worse respectively.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5. </span>Key Prediction Changes</h4>
<div class="ltx_para" id="S4.SS3.SSS5.p1">
<p class="ltx_p" id="S4.SS3.SSS5.p1.1">We conduct a visual analysis of the changes in predicted keypoints between ImageNet and <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS5.p1.1.1">WheelPose</span>-Opt to analyze the information transfer between our synthetic data and the real world across different scenarios to identify specific situations where we perform better or worse. We ignore the PSP dataset as it demonstrated noticeably worse performance compared to ImageNet in both bounding box and pose estimation (<a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T3" title="In 4.3.3. Ablation Testing Strategy ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Tables</span> <span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T4" title="Table 4 ‣ 4.3.3. Ablation Testing Strategy ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">4</span></a>). Examples of different trends were plotted with the predictions from both <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS5.p1.1.2">WheelPose</span>-Opt, in green, and ImageNet, in red, overlaid on top.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS5.p2">
<p class="ltx_p" id="S4.SS3.SSS5.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS5.p2.1.1">Improvements in wheelchair user detection.</span>
As shown in the BB AP improvements between ImageNet and <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS5.p2.1.2">WheelPose</span>-Opt in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T3" title="In 4.3.3. Ablation Testing Strategy ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we notice major improvements in wheelchair user detection. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a> shows some examples of these improvements in a variety of different environments. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a> shows two examples of proper wheelchair user detection through <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS5.p2.1.3">WheelPose</span>-Opt. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">14</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a> shows two examples of wheelchair users being detected in low visibility settings due to both extremely bright and dark lighting conditions. Of particular note is <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a>, where even the background poster of a wheelchair image has been detected, which human annotators had missed. We also notice that even if ImageNet had detected a wheelchair user, its bounding box prediction still was not accurate. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a> shows some examples of this, where ImageNet tends to cut off portions of the wheelchair user’s full body while <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS5.p2.1.4">WheelPose</span>-Opt captures a more accurate and representative bounding box.</p>
</div>
<figure class="ltx_figure" id="S4.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F14.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S4.F14.1.g1" src="extracted/5503605/figures/detection_improvements/s_1_overlayed.png" width="108"/><span class="ltx_ERROR undefined" id="S4.F14.1.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F14.1.2">A wheelchair dancer gets ready for her performance in front of a crowd at a 45-degree angle from the camera.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F14.2.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S4.F14.2.g1" src="extracted/5503605/figures/detection_improvements/s_2_overlayed.png" width="108"/><span class="ltx_ERROR undefined" id="S4.F14.2.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F14.2.2">A wheelchair leans back while holding hands with an able bodied dancer.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F14.3.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S4.F14.3.g1" src="extracted/5503605/figures/detection_improvements/s_3_overlayed.png" width="108"/><span class="ltx_ERROR undefined" id="S4.F14.3.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F14.3.2">A side view of a wheelchair user in a dark lighting condition against a projector.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F14.4.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S4.F14.4.g1" src="extracted/5503605/figures/detection_improvements/s_5_overlayed.png" width="108"/><span class="ltx_ERROR undefined" id="S4.F14.4.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F14.4.2">A side view of a wheelchair dancer with their back hunched down. In the background, there are four judges against a promotional poster featuring a wheelchair user.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F14.5.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S4.F14.5.g1" src="extracted/5503605/figures/detection_improvements/s_12_overlayed.png" width="108"/><span class="ltx_ERROR undefined" id="S4.F14.5.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F14.5.2">A top down view of a wheelchair tennis player in a court. The wheelchair user is holding a tennis racket. Another person is standing on the court.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F14.6.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S4.F14.6.g1" src="extracted/5503605/figures/detection_improvements/s_6_overlayed.png" width="108"/><span class="ltx_ERROR undefined" id="S4.F14.6.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F14.6.2">A back view of a wheelchair user going towards a car. Both hands are on their wheels as they push towards the vehicle.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F14.7.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S4.F14.7.g1" src="extracted/5503605/figures/detection_improvements/s_7_overlayed.png" width="108"/><span class="ltx_ERROR undefined" id="S4.F14.7.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F14.7.2">A back view of a wheelchair user dancing with an able-bodied user. The able bodied user is sitting on the ground as the wheelchair user looks at them.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F14.8.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S4.F14.8.g1" src="extracted/5503605/figures/detection_improvements/s_11_overlayed.png" width="108"/><span class="ltx_ERROR undefined" id="S4.F14.8.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F14.8.2">A close-up 45-degree front view of a wheelchair user at a table. The camera cuts off part of their head as they use their phone. The camera captures their legs under the table.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F14.9.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S4.F14.9.g1" src="extracted/5503605/figures/detection_improvements/s_9_overlayed.png" width="108"/><span class="ltx_ERROR undefined" id="S4.F14.9.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F14.9.2">A back-view of a wheelchair user entering a slope at a skate park. The user is pushing fast with both hands in front of them. They are wearing a helmet and elbow pads.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F14.10.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S4.F14.10.g1" src="extracted/5503605/figures/detection_improvements/s_10_overlayed.png" width="108"/><span class="ltx_ERROR undefined" id="S4.F14.10.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F14.10.2">A front view of a wheelchair user holding a basketball in their lap. They are looking directly at the camera.</p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14. </span>Examples of wheelchair user detection improvements with <span class="ltx_text ltx_font_italic" id="S4.F14.15.1">WheelPose</span>-Opt over ImageNet. Red represents ImageNet predictions while green represents <span class="ltx_text ltx_font_italic" id="S4.F14.16.2">WheelPose</span>-Opt predictions. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a> all show wheelchair users in different scenarios who were completely undetected by ImageNet but detected with <span class="ltx_text ltx_font_italic" id="S4.F14.17.3">WheelPose</span>-Opt fine-tuning. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F14" title="Figure 14 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">14</span></a> all show wheelchair users in different scenarios who were detected by ImageNet, but had poor bounding box predictions which were improved in <span class="ltx_text ltx_font_italic" id="S4.F14.18.4">WheelPose</span>-Opt fine-tuning.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS5.p3">
<p class="ltx_p" id="S4.SS3.SSS5.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS5.p3.1.1">Similar performance in front-facing scenarios.</span>
In front-facing scenarios, ImageNet and other pose estimation models often perform very well on wheelchair users. This is because, at this angle, the user can simply be interpreted to be sitting, with all limbs in full view of the camera. Thus in practice, the front-facing wheelchair user is very similar to a front-facing able-bodied user that is sitting. We find that fine-tuning with <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS5.p3.1.2">WheelPose</span>-Opt performs comparably with the base ImageNet models in front-facing scenarios. Thus, our system maintains crucial information learned from the initial training of ImageNet that has proven to work well on wheelchair users already. Examples of this phenomenon are shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F15" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">15</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F15">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F15.1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F15.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F15.1.g1" src="extracted/5503605/figures/wheelpose_improvements/s_4_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F15.1.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F15.1.2">A wheelchair user in a motorized wheelchair facing forward in the middle of a tiled room. The user’s arms are moving in front of them.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F15.2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F15.2.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F15.2.g1" src="extracted/5503605/figures/wheelpose_improvements/s_8_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F15.2.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F15.2.2">A wheelchair user pushing forward towards the camera in the middle of a bright room with mirrors. The overall brightness of the image is high.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F15.3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F15.3.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F15.3.g1" src="extracted/5503605/figures/wheelpose_improvements/s_9_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F15.3.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F15.3.2">A wheelchair user pushing forward towards the camera in a parking lot. Both hands are placed on the front of the wheel.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F15.4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F15.4.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F15.4.g1" src="extracted/5503605/figures/wheelpose_improvements/s_13_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F15.4.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F15.4.2">A wheelchair user is stationary in front of a camera on a field and in front of the road. Both hands are placed on the wheels. The user is not wearing shoes.</p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15. </span>Examples of similar performance between <span class="ltx_text ltx_font_italic" id="S4.F15.8.1">WheelPose</span>-Opt and ImageNet in front-facing wheelchair users. Red represents ImageNet predictions while green represents <span class="ltx_text ltx_font_italic" id="S4.F15.9.2">WheelPose</span>-Opt predictions. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F15" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F15" title="Figure 15 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F15" title="Figure 15 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">15</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F15" title="Figure 15 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">15</span></a> all depict examples of front-facing wheelchair users in a variety of different settings. The <span class="ltx_text ltx_font_italic" id="S4.F15.10.3">WheelPose</span>-Opt fine-tuned model and ImageNet both performed similarly, generating relatively matching keypoint predictions.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS5.p4">
<p class="ltx_p" id="S4.SS3.SSS5.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS5.p4.1.1">Improvements in keypoint estimation in wheelchair self-occlusion scenarios.</span>
While existing pose estimation models may work well when the wheelchair user is facing directly forward, they often break down when the user is turned away and the wheelchair begins obstructing the view of the full human body. We find that the additional synthetic data from <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS5.p4.1.2">WheelPose</span>-Opt helps Detectron2 discern between what is a part of the wheelchair and what is a part of the user’s body for a more accurate prediction. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F16" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">16</span></a> illustrates a few examples of such poor performance in ImageNet and improved predictions enabled through <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS5.p4.1.3">WheelPose</span>-Opt. In situations where the legs are fully occluded by the wheelchair like in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F16" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">16</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F16" title="Figure 16 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">16</span></a>, our system generates more reasonable predictions compared to those of ImageNet, which placed the legs onto the wheels of the wheelchair or even the elbow. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F16" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">16</span></a> shows an example of how <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS5.p4.1.4">WheelPose</span>-Opt can improve the detection of self occluded keypoints like the user’s left knee and ankle. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F16" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">16</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F16" title="Figure 16 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">16</span></a> shows examples of where ImageNet has mistakenly classified parts of the wheelchair as a keypoint.</p>
</div>
<figure class="ltx_figure" id="S4.F16">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F16.1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F16.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F16.1.g1" src="extracted/5503605/figures/wheelpose_improvements/s_1_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F16.1.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F16.1.2">A wheelchair user near the corner of the room with their back to the camera. Both hands are placed on the wheels of the wheelchair.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F16.2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F16.2.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F16.2.g1" src="extracted/5503605/figures/wheelpose_improvements/s_6_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F16.2.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F16.2.2">A side-view of a wheelchair user traveling down an outdoor path. Both hands are on the wheels doing a slight wheelie.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F16.3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F16.3.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F16.3.g1" src="extracted/5503605/figures/wheelpose_improvements/s_7_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F16.3.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F16.3.2">A wheelchair athlete at the skate park. The athlete is at a 45-degree angle to the camera in the middle of the air. They are wearing protective gear including a helmet and gloves.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F16.4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F16.4.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F16.4.g1" src="extracted/5503605/figures/wheelpose_improvements/s_14_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F16.4.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F16.4.2">A wheelchair user on the street traveling at a 45-degree angle away from the camera towards a parked car. Both hands are placed on the wheels as they move towards the car.</p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16. </span>Examples of improvements in keypoint estimation in <span class="ltx_text ltx_font_italic" id="S4.F16.8.1">WheelPose</span>-Opt over ImageNet in scenarios where the wheelchair occludes part of the user’s body. Red represents ImageNet predictions while green represents <span class="ltx_text ltx_font_italic" id="S4.F16.9.2">WheelPose</span>-Opt predictions. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F16" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F16" title="Figure 16 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F16" title="Figure 16 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">16</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F16" title="Figure 16 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">16</span></a> all display improvements from <span class="ltx_text ltx_font_italic" id="S4.F16.10.3">WheelPose</span>-Opt on keypoint predictions, specifically in the lower body.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS5.p5">
<p class="ltx_p" id="S4.SS3.SSS5.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS5.p5.1.1">Overfitting on wheelchairs.</span>
Upon examining the predictions made by ImageNet and ImageNet fine-tuned with <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS5.p5.1.2">WheelPose</span>-Opt, we notice that both systems perform poorly on users with no lower limbs. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F17" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">17</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F17" title="Figure 17 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">17</span></a> While ImageNet tends to classify the wheelchair as the missing legs, we notice that our system instead ”fills in the blanks” and predicts legs in reference to the wheelchair where they might usually be for a wheelchair user. We further notice that our system tends to have more false positives in detecting what can be defined as a wheelchair. As seen in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F17" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">17</span></a>, objects that resemble a wheelchair, like a grocery cart, may affect the keypoint estimation of a wheelchair user. In other cases like the one shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F17" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">17</span></a>, we see that our system can even detect empty wheelchairs and fill them in with humans when there are not any. While this shows great promise in the information transfer between digitally modeled mobility assistive devices and real-world data, we find these tendencies can obfuscate the real postures of wheelchair users.</p>
</div>
<figure class="ltx_figure" id="S4.F17">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F17.1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F17.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F17.1.g1" src="extracted/5503605/figures/wheelpose_poor_performance/s_1_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F17.1.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F17.1.2">A young child without lower limbs at a 45-degree angle in a wheelchair playing pickleball. The child is partially occluded by the pickleball net.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F17.2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F17.2.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F17.2.g1" src="extracted/5503605/figures/wheelpose_poor_performance/s_4_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F17.2.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F17.2.2">A wheelchair basketball player without lower limbs is dribbling while facing the camera. The players right hand is on the wheel and the left hand is dribbling the basketball.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F17.3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F17.3.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F17.3.g1" src="extracted/5503605/figures/wheelpose_poor_performance/s_6_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F17.3.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F17.3.2">A person is at a 45-degree angle in a motorized grocery cart behind other grocery carts. They are explaining something with both hands sticking out.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F17.4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F17.4.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F17.4.g1" src="extracted/5503605/figures/wheelpose_poor_performance/s_9_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="S4.F17.4.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F17.4.2">A game of wheelchair tennis. The camera is at a 45-degree angle in an overhead view. A wheelchair user is raising their racket up in front of an empty wheelchair and a person crouches on the ground. There is a crowd of spectators beyond the field of the game.</p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17. </span>Examples of overfitting on wheelchairs from <span class="ltx_text ltx_font_italic" id="S4.F17.7.1">WheelPose</span>-Opt. Red represents ImageNet predictions while green represents <span class="ltx_text ltx_font_italic" id="S4.F17.8.2">WheelPose</span>-Opt predictions. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F17" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">17</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F17" title="Figure 17 ‣ 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">17</span></a> Examples of poor predictions on wheelchair users without lower limbs. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F17" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">17</span></a> An example of overfitting onto any wheelchair resembling object, like a grocery cart. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.F17" title="In 4.3.5. Key Prediction Changes ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">17</span></a> An example of overfitting where even an empty wheelchair is detected to have a user.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion and Future Work</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1" style="color:#000000;">Generalizable knowledge and transferability to HCI research.</span><span class="ltx_text" id="S5.p1.1.2" style="color:#000000;"> Data generation approaches have been widely adopted in projects in HCI for problems involving thermal imaging </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S5.p1.1.3.1" style="color:#000000;">(</span>Hu et al<span class="ltx_text">.</span><span class="ltx_text" id="S5.p1.1.4.2.1.1" style="color:#000000;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib48" title="">2020</a><span class="ltx_text" id="S5.p1.1.5.3" style="color:#000000;">)</span></cite><span class="ltx_text" id="S5.p1.1.6" style="color:#000000;">, IMU </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S5.p1.1.7.1" style="color:#000000;">(</span>Jain et al<span class="ltx_text">.</span><span class="ltx_text" id="S5.p1.1.8.2.1.1" style="color:#000000;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib52" title="">2023</a>; Rey et al<span class="ltx_text">.</span><span class="ltx_text" id="S5.p1.1.8.2.1.1" style="color:#000000;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib81" title="">2019</a><span class="ltx_text" id="S5.p1.1.9.3" style="color:#000000;">)</span></cite><span class="ltx_text" id="S5.p1.1.10" style="color:#000000;">, stroke gesture </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S5.p1.1.11.1" style="color:#000000;">(</span>Leiva et al<span class="ltx_text">.</span><span class="ltx_text" id="S5.p1.1.12.2.1.1" style="color:#000000;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib57" title="">2017</a><span class="ltx_text" id="S5.p1.1.13.3" style="color:#000000;">)</span></cite><span class="ltx_text" id="S5.p1.1.14" style="color:#000000;">, and RF data </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S5.p1.1.15.1" style="color:#000000;">(</span>Ahuja et al<span class="ltx_text">.</span><span class="ltx_text" id="S5.p1.1.16.2.1.1" style="color:#000000;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib6" title="">2021</a>; Cai et al<span class="ltx_text">.</span><span class="ltx_text" id="S5.p1.1.16.2.1.1" style="color:#000000;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib18" title="">2020</a><span class="ltx_text" id="S5.p1.1.17.3" style="color:#000000;">)</span></cite><span class="ltx_text" id="S5.p1.1.18" style="color:#000000;">. Given the popularity of data generation in HCI, we believe our techniques could be easily transferable to related and future works in accessibility, motion generation, and pose estimation. Furthermore, the modularization in our pipeline could improve transferability by facilitating segmented changes – a flexible way for data synthesis to experiment with different components. Finally, methods shown in our statistical analysis and model performance evaluation could be highly reusable in future work that adopts our technique. That being said, our pipeline provides the baseline framework for futures efforts in research that require different humanoid models, motion synthesis techniques for upper body and lower body, environmental factors, and VR toolchains.</span><span class="ltx_text" id="S5.p1.1.19"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1" style="color:#000000;">Configurability to cater needs of developers and end users.</span><span class="ltx_text" id="S5.p2.1.2" style="color:#000000;"> We believe the results shown in </span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T2" style="color:#000000;" title="In 4.3.1. Testing Dataset ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S5.p2.1.3" style="color:#000000;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.SS1" style="color:#000000;" title="A.1. Animation Clip Fix ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.1</span></a><span class="ltx_text" id="S5.p2.1.4" style="color:#000000;">, and </span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.SS5" style="color:#000000;" title="A.5. Impacts of Keypoint Location Definitions ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.5</span></a><span class="ltx_text" id="S5.p2.1.5" style="color:#000000;"> demonstrate a clear need for developers to have an interactive tool to modify training data, where slight changes in the data modeling can have major effects on AI performance. Currently, there are few tools to do so, with many developers choosing to leverage static motion datasets which may not perfectly fit their needs. This has been one of the dominant reasons for the rise of inequitable AI models. For this reason, </span><span class="ltx_text ltx_font_italic" id="S5.p2.1.6" style="color:#000000;">WheelPose</span><span class="ltx_text" id="S5.p2.1.7" style="color:#000000;"> is a pipeline specifically designed to enable a high degree of configurability. This allows for the creation of personalized synthetic datasets that cater to developers’ needs, thereby increasing the likelihood of catering to the needs of end users. This would lead to more effective and inclusive AI models, which can be tuned to the individual needs of the user instead of a one-size-fits-all solution for better performance in real-world applications.</span><span class="ltx_text" id="S5.p2.1.8"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1" style="color:#000000;">Improvements on the realism of generated data.</span><span class="ltx_text" id="S5.p3.1.2" style="color:#000000;"> We hope the use of Unity enables future research into this idea and enables developers to build a synthetic data generator that extends beyond our simple simulation environment using 3D modeled scenes and rooms instead of flat background images. Recent achievements in Neural Radiance Fields (NeRF) could be leveraged to synthesize photorealistic background images that adapt in response to changes in the virtual camera’s perspective. Future work in realistic 3D environment modeling will enable research in surface semantics for more realistic configurations – having wheelchair users positioned at ground surfaces. Moreover, physics could be incorporated to simulate the locomotion of wheelchair users which would enable the modeling of more realistic motions and collisions with other models compared to our simple animations.</span><span class="ltx_text" id="S5.p3.1.3"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1" style="color:#000000;">Efficacy of human evaluation.</span><span class="ltx_text" id="S5.p4.1.2" style="color:#000000;"> We found that motions from motion generation models and motion capture were largely perceived similarly by participants in the human evaluation (</span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.SS1" style="color:#000000;" title="4.1. Human Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a><span class="ltx_text" id="S5.p4.1.3" style="color:#000000;">). We further found that these two types of motions after being filtered for human-perceived realism performed comparably in model performance evaluation (</span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T2" style="color:#000000;" title="In 4.3.1. Testing Dataset ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S5.p4.1.4" style="color:#000000;">).
However, whether due to current generative AI performance or the lack of training data for disability-related movement, our human evaluations indicate that generative AI models are still not able to represent users with disabilities accurately without external human feedback.
This is shown in the improved performance with the addition of human evaluations (</span><a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#S4.T2" style="color:#000000;" title="In 4.3.1. Testing Dataset ‣ 4.3. Model Performance Evaluation ‣ 4. Evaluation of WheelPose ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S5.p4.1.5" style="color:#000000;">).
Due to this, we believe that human evaluators are still vital to ensure that generated data is representative. We recognize that our method for assessing motions, although not exhaustive or entirely free of bias, contributes as an initial stride towards creating datasets that are more inclusive.
Future research should extend upon this work to integrate a more involved human-centered system which will, instead, allow evaluators to meaningfully guide the generation of data, including motions, simulation scenarios, and user modeling, through iterative feedback (e.g., guided prompting) to create a more representative dataset. We also found little literature on motion synthesis for people with disabilities and recognized this vacuum as both a challenge and an opportunity for generative AI.</span><span class="ltx_text" id="S5.p4.1.6"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.1"><span class="ltx_text ltx_font_bold" id="S5.p5.1.1" style="color:#000000;">Comprehensive model performance evaluation.</span><span class="ltx_text" id="S5.p5.1.2"> Our model testing did not involve a serious grid search for data generation hyperparameters, model seeds, initializations, or model configurations. We also used the same training strategy for all tested datasets. We held all these values constant to focus on the quality and impact of the synthetic dataset on model performance. Even with this naïve approach to training, <span class="ltx_text ltx_font_italic" id="S5.p5.1.2.1">WheelPose</span> still yielded promising results in the improvement of pose estimation models through synthetic data on wheelchair users. We found noticeable improvements in both person detection and pose estimation problems. This indicates that the specific synthetic modeling of users in wheelchairs in <span class="ltx_text ltx_font_italic" id="S5.p5.1.2.2">WheelPose</span> can make existing computer vision models more equitable by improving performance on wheelchairs. We believe our findings pave the way for future works in synthetic data on humans with other mobility-assistive technologies to improve pose estimation equitability.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p6">
<p class="ltx_p" id="S5.p6.1"><span class="ltx_text ltx_font_bold" id="S5.p6.1.1" style="color:#000000;">Diverse participant groups.</span><span class="ltx_text" id="S5.p6.1.2" style="color:#000000;"> A key limitation in this work is that we only analyze the digitalized representation of users with all four limbs and feet fixed on the foot rest, which does not express the full range of wheelchair users with different bodies such as those with amputations, dwarfism, spinal deformities, and other conditions. Thus the findings may not be reflective of the wheelchair population at large. We hope our data generation pipeline provides a framework and will enable future developers to leverage 3D modeling and Unity tools to create a more diverse body of wheelchair user models. For instance, developers can easily add new bones to existing models to more realistically represent spinal deformities. We hope that these tools will also enable future works analyzing different disabilities and mobility assistive devices which were not addressed in our current research.</span><span class="ltx_text" id="S5.p6.1.3"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p7">
<p class="ltx_p" id="S5.p7.1"><span class="ltx_text ltx_font_bold" id="S5.p7.1.1" style="color:#000000;">Improve inclusiveness of AI for more recognition modalities.</span><span class="ltx_text" id="S5.p7.1.2"> Furthermore, <span class="ltx_text ltx_font_italic" id="S5.p7.1.2.1">WheelPose</span> enables more work beyond 2D bounding boxes and pose estimation. Annotations on depth, surface normals, object segmentation, occlusion, 3D bounding boxes, and 3D keypoints are fully implemented in our current data synthesis pipeline but still unexplored. These annotations can be even more difficult and costly to collect compared to RGB images and 2D pose annotations, often requiring the use of specialized equipment and data collection processes. Synthetic data has no such problem, where any desired annotation and labeling are all equally accessible to collect. Thus, we believe that <span class="ltx_text ltx_font_italic" id="S5.p7.1.2.2">WheelPose</span> can be adapted to potentially address problems in wheelchair pedestrian detection with object segmentation and occlusion annotations <cite class="ltx_cite ltx_citemacro_citep">(Dollár et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib27" title="">2011</a>)</cite>, 3D pose estimation using 3D bounding box and pose annotations, and robotics detection of people and mobility aids through depth data <cite class="ltx_cite ltx_citemacro_citep">(Vasquez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib103" title="">2017</a>)</cite> among other accessibility-related problems cheaply and efficiently.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p8">
<p class="ltx_p" id="S5.p8.1"><span class="ltx_text ltx_font_bold" id="S5.p8.1.1" style="color:#000000;">Pitfalls of exclusion in data generation vs. data collection.</span><span class="ltx_text" id="S5.p8.1.2" style="color:#000000;"> </span><span class="ltx_text ltx_font_italic" id="S5.p8.1.3" style="color:#000000;">WheelPose</span><span class="ltx_text" id="S5.p8.1.4" style="color:#000000;"> is a pipeline for both AI developers and wheelchair users to circumvent existing inaccessible data collection methods and meaningfully improve the training process of AI models by generating data for wheelchair users.
However, we are cautious that by circumventing existing inaccessible data collection methods with our tool, we could run the potential risk of furthering exclusion, which echoes long-standing debates within the accessibility community. Our paper is based on the assumption that making AI equitable requires pursuing multiple approaches together – effective approaches to improving representations of training data from people with disabilities leveraging both data collection and data generation.
We advocate that new tools for data generation and the existing data collection methods are not mutually exclusive. Their synergy could lead to a more practical approach to resolving accessibility challenges than what could be offered by either of the two approaches alone. We believe that the following characteristics are vital in future works to avoid pitfalls of exclusion: 1) representative and diverse participant groups, for which we have conducted studies around spinal injuries of various levels and recommend future work to consider participants from wider backgrounds; 2) realistic generated data, for which we invented several data generation techniques optimizing data realism; and 3) effective tools for human evaluation, for which we adopted embodiment in our human evaluation interface allowing participants to seamlessly transfer the presented motion sequences to their own bodies for a more intuitive evaluation.</span><span class="ltx_text" id="S5.p8.1.5"></span></p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduce <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">WheelPose</span>, an extension of the highly-parameterized synthetic data generator PeopleSansPeople, for wheelchair users with the possibility of use in other mobility-assistive technologies to improve the performance of common pose estimation algorithms in the traditionally underrepresented group of wheelchair users. <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">WheelPose</span> includes a full end-to-end pipeline to convert existing motion capture and motion generation model outputs into wheelchair user animations for use in a complete Unity Simulation scene <span class="ltx_text ltx_font_bold" id="S6.p1.1.3">(RQ1.1)</span> containing a range of 3D human models from Unity SyntheticHumans in wheelchairs, backgrounds, occluders, and unique lighting conditions. We provide full control over all related parameters, including keypoint labeling schema, for computer vision tasks <span class="ltx_text ltx_font_bold" id="S6.p1.1.4">(RQ1.2)</span>. We tested our pipeline using two different motion sequence sources: motion capture data from HumanML3D and motion generation outputs from Text2Motion. These motions underwent a set of human evaluations. We then analyzed the impacts of different domain randomization parameters and motions on model performance, finding an ”optimal” combination of parameters and comparable performance between our motion sources <span class="ltx_text ltx_font_bold" id="S6.p1.1.5">(RQ2.1)</span>. Finally, we tested the model performance of the dataset generated through <span class="ltx_text ltx_font_italic" id="S6.p1.1.6">WheelPose</span> with no real-world data using the optimal parameters found previously on a dataset of real wheelchair users to find noticeable improvements in model performance when compared against existing pose estimation models <span class="ltx_text ltx_font_bold" id="S6.p1.1.7">(RQ2.2)</span>. We expect <span class="ltx_text ltx_font_italic" id="S6.p1.1.8">WheelPose</span> to enable a new range of research in using synthetic data to model users with disabilities in improving the equity of AI.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kin (2019)</span>
<span class="ltx_bibblock">
2019.

</span>
<span class="ltx_bibblock">The Kinesthetic Index: Video Games and the
Body of Motion Capture – InVisible Culture.

</span>
<span class="ltx_bibblock">https://ivc.lib.rochester.edu/the-kinesthetic-index-video-games-and-the-body-of-motion-capture/.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Way (2022)</span>
<span class="ltx_bibblock">
2022.

</span>
<span class="ltx_bibblock">Waypoint - The Official Waymo Blog:
Utilizing Key Point and Pose Estimation for the Task of Autonomous
Driving.

</span>
<span class="ltx_bibblock">https://waymo.com/blog/2022/02/utilizing-key-point-and-pose-estimation.html.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dee (2023)</span>
<span class="ltx_bibblock">
2023.

</span>
<span class="ltx_bibblock">DeepMotion - AI Motion Capture &amp; Body
Tracking.

</span>
<span class="ltx_bibblock">https://www.deepmotion.com/.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Syn (2023)</span>
<span class="ltx_bibblock">
2023.

</span>
<span class="ltx_bibblock">SyntheticHumans Package (Unity Computer
Vision).

</span>
<span class="ltx_bibblock">Unity Technologies.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahuja et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Karan Ahuja, Yue Jiang,
Mayank Goel, and Chris Harrison.
2021.

</span>
<span class="ltx_bibblock">Vid2Doppler: Synthesizing Doppler Radar Data from
Videos for Training Privacy-Preserving Activity Recognition. In
<em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 2021 CHI Conference on Human
Factors in Computing Systems</em> (¡conf-loc¿, ¡city¿Yokohama¡/city¿,
¡country¿Japan¡/country¿, ¡/conf-loc¿) <em class="ltx_emph ltx_font_italic" id="bib.bib6.4.2">(CHI ’21)</em>.
Association for Computing Machinery,
New York, NY, USA, Article 292,
10 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3411764.3445138" title="">https://doi.org/10.1145/3411764.3445138</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason W. Anderson, Marcin
Ziolkowski, Ken Kennedy, and Amy W.
Apon. 2022.

</span>
<span class="ltx_bibblock">Synthetic Image Data for Deep Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2212.06232 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Mykhaylo Andriluka, Leonid
Pishchulin, Peter Gehler, and Bernt
Schiele. 2014.

</span>
<span class="ltx_bibblock">2D Human Pose Estimation: New Benchmark and State
of the Art Analysis. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azadi et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Samaneh Azadi, Akbar
Shah, Thomas Hayes, Devi Parikh, and
Sonal Gupta. 2023.

</span>
<span class="ltx_bibblock">Make-An-Animation: Large-Scale Text-conditional 3D
Human Motion Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.09662 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bak et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Slawomir Bak, Peter Carr,
and Jean-Francois Lalonde.
2018.

</span>
<span class="ltx_bibblock">Domain Adaptation through Synthesis for Unsupervised
Person Re-identification.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1804.10094 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bartlett (2014)</span>
<span class="ltx_bibblock">
Roger Bartlett.
2014.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Introduction to Sports Biomechanics:
Analysing Human Movement Patterns</em>.

</span>
<span class="ltx_bibblock">Routledge.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bartolomé Villar et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Begoña Bartolomé Villar,
Irene Real Benlloch, Ana De la Hoz Calvo,
and Gleyvis Coro-Montanet.
2022.

</span>
<span class="ltx_bibblock">Perception of Realism and Acquisition of Clinical
Skills in Simulated Pediatric Dentistry Scenarios.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">International Journal of Environmental
Research and Public Health</em> 19, 18
(2022), 11387.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bazarevsky et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Valentin Bazarevsky, Ivan
Grishchenko, Karthik Raveendran, Tyler
Zhu, Fan Zhang, and Matthias
Grundmann. 2020.

</span>
<span class="ltx_bibblock">BlazePose: On-device Real-time Body Pose tracking.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2006.10204 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bazavan et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Eduard Gabriel Bazavan,
Andrei Zanfir, Mihai Zanfir,
William T. Freeman, Rahul Sukthankar,
and Cristian Sminchisescu.
2022.

</span>
<span class="ltx_bibblock">HSPACE: Synthetic Parametric Humans Animated in
Complex Environments.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2112.12867 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bennett and Rosner (2019)</span>
<span class="ltx_bibblock">
Cynthia L Bennett and
Daniela K Rosner. 2019.

</span>
<span class="ltx_bibblock">The promise of empathy: Design, disability, and
knowing the” other”. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2019
CHI conference on human factors in computing systems</em>.
1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borkman et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Steve Borkman, Adam
Crespi, Saurav Dhakad, Sujoy Ganguly,
Jonathan Hogins, You-Cyuan Jhang,
Mohsen Kamalzadeh, Bowen Li,
Steven Leal, Pete Parisi,
Cesar Romero, Wesley Smith,
Alex Thaman, Samuel Warren, and
Nupur Yadav. 2021.

</span>
<span class="ltx_bibblock">Unity Perception: Generate Synthetic Data for
Computer Vision.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2107.04259 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bridson (2007)</span>
<span class="ltx_bibblock">
Robert Bridson.
2007.

</span>
<span class="ltx_bibblock">Fast Poisson Disk Sampling in Arbitrary
Dimensions. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ACM SIGGRAPH 2007 Sketches</em> (San
Diego, California) <em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2">(SIGGRAPH ’07)</em>.
Association for Computing Machinery,
New York, NY, USA, 22–es.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1278780.1278807" title="">https://doi.org/10.1145/1278780.1278807</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Hong Cai, Belal Korany,
Chitra R. Karanam, and Yasamin
Mostofi. 2020.

</span>
<span class="ltx_bibblock">Teaching RF to Sense without RF Training
Measurements.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proc. ACM Interact. Mob. Wearable Ubiquitous
Technol.</em> 4, 4, Article
120 (dec 2020),
22 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3432224" title="">https://doi.org/10.1145/3432224</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhongang Cai, Mingyuan
Zhang, Jiawei Ren, Chen Wei,
Daxuan Ren, Zhengyu Lin,
Haiyu Zhao, Lei Yang,
Chen Change Loy, and Ziwei Liu.
2022.

</span>
<span class="ltx_bibblock">Playing for 3D Human Recovery.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2110.07588 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhe Cao, Hang Gao,
Karttikeya Mangalam, Qi-Zhi Cai,
Minh Vo, and Jitendra Malik.
2020.

</span>
<span class="ltx_bibblock">Long-term Human Motion Prediction with Scene
Context.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2007.03672 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Zhe Cao, Gines Hidalgo,
Tomas Simon, Shih-En Wei, and
Yaser Sheikh. 2019.

</span>
<span class="ltx_bibblock">OpenPose: Realtime Multi-Person 2D Pose Estimation
using Part Affinity Fields.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1812.08008 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Zhe Cao, Tomas Simon,
Shih-En Wei, and Yaser Sheikh.
2017.

</span>
<span class="ltx_bibblock">Realtime Multi-Person 2D Pose Estimation using Part
Affinity Fields.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1611.08050 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carrington et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Carrington, Gierad
Laput, and Jeffrey P. Bigham.
2020.

</span>
<span class="ltx_bibblock">SpokeSense: Developing a Real-Time Sensing Platform
for Wheelchair Sports.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">SIGACCESS Access. Comput.</em>
124, Article 2 (mar
2020), 1 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3386308.3386310" title="">https://doi.org/10.1145/3386308.3386310</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Zwicker (2021)</span>
<span class="ltx_bibblock">
Shuhong Chen and
Matthias Zwicker. 2021.

</span>
<span class="ltx_bibblock">Transfer Learning for Pose Estimation of Illustrated
Characters.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2108.01819 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong,
Richard Socher, Li-Jia Li,
Kai Li, and Li Fei-Fei.
2009.

</span>
<span class="ltx_bibblock">ImageNet: A large-scale hierarchical image
database. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">2009 IEEE Conference on Computer
Vision and Pattern Recognition</em>. 248–255.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2009.5206848" title="">https://doi.org/10.1109/CVPR.2009.5206848</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denninger et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Maximilian Denninger,
Martin Sundermeyer, Dominik Winkelbauer,
Youssef Zidan, Dmitry Olefir,
Mohamad Elbadrawy, Ahsan Lodhi, and
Harinandan Katam. 2019.

</span>
<span class="ltx_bibblock">BlenderProc.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1911.01911 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dollár et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Piotr Dollár, Christian
Wojek, Bernt Schiele, and Pietro
Perona. 2011.

</span>
<span class="ltx_bibblock">Pedestrian Detection: An Evaluation of the State of
the Art.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">IEEE transactions on pattern analysis and
machine intelligence</em> 34 (07
2011), 743–61.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TPAMI.2011.155" title="">https://doi.org/10.1109/TPAMI.2011.155</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ebadi et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Salehe Erfanian Ebadi,
Saurav Dhakad, Sanjay Vishwakarma,
Chunpu Wang, You-Cyuan Jhang,
Maciek Chociej, Adam Crespi,
Alex Thaman, and Sujoy Ganguly.
2022a.

</span>
<span class="ltx_bibblock">PSP-HDRI$+$: A Synthetic Dataset Generator
for Pre-Training of Human-Centric Computer Vision Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2207.05025 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ebadi et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Salehe Erfanian Ebadi,
You-Cyuan Jhang, Alex Zook,
Saurav Dhakad, Adam Crespi,
Pete Parisi, Steven Borkman,
Jonathan Hogins, and Sujoy Ganguly.
2022b.

</span>
<span class="ltx_bibblock">PeopleSansPeople: A Synthetic Data Generator
for Human-Centric Computer Vision.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2112.09290 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">El Emam et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Khaled El Emam, Lucy
Mosquera, and Richard Hoptroff.
2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Practical synthetic data generation:
balancing privacy and the broad availability of data</em>.

</span>
<span class="ltx_bibblock">O’Reilly Media.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everingham et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Mark Everingham, Luc
Gool, Christopher K. Williams, John
Winn, and Andrew Zisserman.
2010.

</span>
<span class="ltx_bibblock">The Pascal Visual Object Classes (VOC) Challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Int. J. Comput. Vision</em>
88, 2 (jun
2010), 303–338.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11263-009-0275-4" title="">https://doi.org/10.1007/s11263-009-0275-4</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Matteo Fabbri, Fabio
Lanzi, Simone Calderara, Andrea Palazzi,
Roberto Vezzani, and Rita Cucchiara.
2018.

</span>
<span class="ltx_bibblock">Learning to Detect and Track Visible and Occluded
Body Joints in a Virtual World.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1803.08319 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferrucci et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Luigi Ferrucci, Rachel
Cooper, Michelle Shardell, Eleanor M.
Simonsick, Jennifer A. Schrack, and
Diana Kuh. 2016.

</span>
<span class="ltx_bibblock">Age-Related Change in Mobility:
Perspectives From Life Course Epidemiology and Geroscience.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">The Journals of Gerontology Series A:
Biological Sciences and Medical Sciences</em> 71,
9 (Sept. 2016),
1184–1194.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1093/gerona/glw043" title="">https://doi.org/10.1093/gerona/glw043</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freiberger et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ellen Freiberger,
Cornel Christian Sieber, and Robert
Kob. 2020.

</span>
<span class="ltx_bibblock">Mobility in Older Community-Dwelling Persons:
A Narrative Review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Frontiers in Physiology</em>
11 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fribourg et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Rebecca Fribourg, Ferran
Argelaguet, Anatole Lécuyer, and
Ludovic Hoyet. 2020.

</span>
<span class="ltx_bibblock">Avatar and sense of embodiment: Studying the
relative preference between appearance, control and point of view.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">IEEE transactions on visualization and
computer graphics</em> 26, 5
(2020), 2062–2072.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gaidon et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Adrien Gaidon, Qiao Wang,
Yohann Cabon, and Eleonora Vig.
2016.

</span>
<span class="ltx_bibblock">Virtual worlds as proxy for multi-object tracking
analysis. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the IEEE conference on
computer vision and pattern recognition</em>. 4340–4349.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gardener et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Elizabeth A Gardener,
Felicia A Huppert, Jack M Guralnik, and
David Melzer. 2006.

</span>
<span class="ltx_bibblock">Middle-Aged and Mobility-Limited:
Prevalence of Disability and Symptom Attributions in a National
Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Journal of General Internal Medicine</em>
21, 10 (Oct.
2006), 1091–1096.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1111/j.1525-1497.2006.00564.x" title="">https://doi.org/10.1111/j.1525-1497.2006.00564.x</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Zhiqiang Gong, Ping
Zhong, and Weidong Hu. 2019.

</span>
<span class="ltx_bibblock">Diversity in Machine Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">IEEE Access</em> 7
(2019), 64323–64350.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/access.2019.2917620" title="">https://doi.org/10.1109/access.2019.2917620</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Anhong Guo, Ece Kamar,
Jennifer Wortman Vaughan, Hanna Wallach,
and Meredith Ringel Morris.
2020.

</span>
<span class="ltx_bibblock">Toward fairness in AI for people with disabilities
SBG@ a research roadmap.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">ACM SIGACCESS Accessibility and Computing</em>
125 (2020), 1–1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Chuan Guo, Shihao Zou,
Xinxin Zuo, Sen Wang,
Wei Ji, Xingyu Li, and
Li Cheng. 2022a.

</span>
<span class="ltx_bibblock">Generating Diverse and Natural 3D Human Motions
From Text. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
5152–5161.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Chuan Guo, Shihao Zou,
Xinxin Zuo, Sen Wang,
Wei Ji, Xingyu Li, and
Li Cheng. 2022b.

</span>
<span class="ltx_bibblock">Generating Diverse and Natural 3D Human
Motions From Text.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Kaiming He, Georgia
Gkioxari, Piotr Dollár, and Ross
Girshick. 2018.

</span>
<span class="ltx_bibblock">Mask R-CNN.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1703.06870 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2016b)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu
Zhang, Shaoqing Ren, and Jian Sun.
2016b.

</span>
<span class="ltx_bibblock">Deep Residual Learning for Image Recognition. In
<em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>. 770–778.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2016.90" title="">https://doi.org/10.1109/CVPR.2016.90</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ruifei He, Shuyang Sun,
Xin Yu, Chuhui Xue,
Wenqing Zhang, Philip Torr,
Song Bai, and Xiaojuan Qi.
2023.

</span>
<span class="ltx_bibblock">Is synthetic data from generative models ready for
image recognition?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2210.07574 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2016a)</span>
<span class="ltx_bibblock">
Wan He, Daniel Goodkind,
and Paul Kowal. 2016a.

</span>
<span class="ltx_bibblock">International Population Reports.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heindl et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Christoph Heindl, Lukas
Brunner, Sebastian Zambal, and Josef
Scharinger. 2020.

</span>
<span class="ltx_bibblock">BlendTorch: A Real-Time, Adaptive Domain
Randomization Library.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2010.11696 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howe et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Bill Howe, Julia
Stoyanovich, Haoyue Ping, Bernease
Herman, and Matt Gee. 2017.

</span>
<span class="ltx_bibblock">Synthetic data for social good.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">arXiv preprint arXiv:1710.08874</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Fang Hu, Peng He,
Songlin Xu, Yin Li, and
Cheng Zhang. 2020.

</span>
<span class="ltx_bibblock">FingerTrak: Continuous 3D hand pose tracking by
deep learning hand silhouettes captured by miniature thermal cameras on
wrist.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies</em> 4,
2 (2020), 1–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yuan-Ting Hu, Hong-Shuo
Chen, Kexin Hui, Jia-Bin Huang, and
Alexander G. Schwing. 2019.

</span>
<span class="ltx_bibblock">SAIL-VOS: Semantic Amodal Instance Level Video
Object Segmentation – A Synthetic Dataset and Baselines. In
<em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</em>. 3100–3110.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2019.00322" title="">https://doi.org/10.1109/CVPR.2019.00322</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yuan-Ting Hu, Jiahong
Wang, Raymond A. Yeh, and Alexander G.
Schwing. 2021.

</span>
<span class="ltx_bibblock">SAIL-VOS 3D: A Synthetic Dataset and Baselines for
Object Detection and 3D Mesh Reconstruction from Video Data.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2105.08612 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudak et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (1996)</span>
<span class="ltx_bibblock">
Pamela L Hudak, Peter C
Amadio, Claire Bombardier, Dorcas
Beaton, Donald Cole, Aileen Davis,
Gillian Hawker, Jeffrey N Katz,
Matti Makela, Robert G Marx,
et al<span class="ltx_text" id="bib.bib51.3.1">.</span> 1996.

</span>
<span class="ltx_bibblock">Development of an upper extremity outcome measure:
the DASH (disabilities of the arm, shoulder, and head).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.4.1">American journal of industrial medicine</em>
29, 6 (1996),
602–608.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yash Jain, Hyeokhyen
Kwon, and Thomas Ploetz.
2023.

</span>
<span class="ltx_bibblock">On the Effectiveness of Virtual IMU Data for Eating
Detection with Wrist Sensors. In <em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Adjunct
Proceedings of the 2022 ACM International Joint Conference on Pervasive and
Ubiquitous Computing and the 2022 ACM International Symposium on Wearable
Computers</em> (Cambridge, United Kingdom) <em class="ltx_emph ltx_font_italic" id="bib.bib52.4.2">(UbiComp/ISWC
’22 Adjunct)</em>. Association for Computing Machinery,
New York, NY, USA, 50–52.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544793.3560337" title="">https://doi.org/10.1145/3544793.3560337</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaloskampis et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ioannis Kaloskampis, David
Pugh, Chaitanya Joshi, and Louisa
Nolan. 2019.

</span>
<span class="ltx_bibblock">Data science for the public good.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirkham and Tannert (2021)</span>
<span class="ltx_bibblock">
Reuben Kirkham and
Benjamin Tannert. 2021.

</span>
<span class="ltx_bibblock">Using Computer Simulations to Investigate the
Potential Performance of ’A to B’ Routing Systems for People with Mobility
Impairments.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2107.01570 [cs.HC]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolve et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Eric Kolve, Roozbeh
Mottaghi, Winson Han, Eli VanderBilt,
Luca Weihs, Alvaro Herrasti,
Matt Deitke, Kiana Ehsani,
Daniel Gordon, Yuke Zhu,
Aniruddha Kembhavi, Abhinav Gupta, and
Ali Farhadi. 2022.

</span>
<span class="ltx_bibblock">AI2-THOR: An Interactive 3D Environment for Visual
AI.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1712.05474 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leaman and La (2017)</span>
<span class="ltx_bibblock">
Jesse Leaman and Hung M.
La. 2017.

</span>
<span class="ltx_bibblock">A Comprehensive Review of Smart Wheelchairs: Past,
Present and Future.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1704.04697 [cs.RO]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leiva et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Luis A. Leiva, Daniel
Martín-Albo, and Radu-Daniel Vatavu.
2017.

</span>
<span class="ltx_bibblock">Synthesizing Stroke Gestures Across User
Populations: A Case for Users with Visual Impairments. In
<em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">Proceedings of the 2017 CHI Conference on Human
Factors in Computing Systems</em> (Denver, Colorado, USA)
<em class="ltx_emph ltx_font_italic" id="bib.bib57.4.2">(CHI ’17)</em>. Association for
Computing Machinery, New York, NY, USA,
4182–4193.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3025453.3025906" title="">https://doi.org/10.1145/3025453.3025906</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leroy et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Vincent Leroy, Philippe
Weinzaepfel, Romain Brégier, Hadrien
Combaluzier, and Grégory Rogez.
2020.

</span>
<span class="ltx_bibblock">SMPLy Benchmarking 3D Human Pose Estimation in the
Wild.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2012.02743 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Jizhizi Li, Sihan Ma,
Jing Zhang, and Dacheng Tao.
2021a.

</span>
<span class="ltx_bibblock">Privacy-Preserving Portrait Matting.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2104.14222 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jizhizi Li, Jing Zhang,
Stephen J Maybank, and Dacheng Tao.
2022.

</span>
<span class="ltx_bibblock">Bridging composite and real: towards end-to-end
deep image matting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">International Journal of Computer Vision</em>
130, 2 (2022),
246–266.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2021c)</span>
<span class="ltx_bibblock">
Jizhizi Li, Jing Zhang,
and Dacheng Tao. 2021c.

</span>
<span class="ltx_bibblock">Deep Automatic Natural Image Matting.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2107.07235 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Zhengqin Li, Ting-Wei Yu,
Shen Sang, Sarah Wang,
Meng Song, Yuhan Liu,
Yu-Ying Yeh, Rui Zhu,
Nitesh Gundavarapu, Jia Shi,
Sai Bi, Zexiang Xu,
Hong-Xing Yu, Kalyan Sunkavalli,
Miloš Hašan, Ravi Ramamoorthi, and
Manmohan Chandraker. 2021b.

</span>
<span class="ltx_bibblock">OpenRooms: An End-to-End Open Framework for
Photorealistic Indoor Scene Datasets.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2007.12868 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacky Liang, Viktor
Makoviychuk, Ankur Handa, Nuttapong
Chentanez, Miles Macklin, and Dieter
Fox. 2018.

</span>
<span class="ltx_bibblock">GPU-Accelerated Robotic Simulation for Distributed
Reinforcement Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1810.05762 [cs.RO]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Piotr
Dollár, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie.
2017.

</span>
<span class="ltx_bibblock">Feature Pyramid Networks for Object Detection.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1612.03144 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael
Maire, Serge Belongie, Lubomir Bourdev,
Ross Girshick, James Hays,
Pietro Perona, Deva Ramanan,
C. Lawrence Zitnick, and Piotr
Dollár. 2015.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common Objects in Context.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1405.0312 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lugaresi et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Camillo Lugaresi, Jiuqiang
Tang, Hadon Nash, Chris McClanahan,
Esha Uboweja, Michael Hays,
Fan Zhang, Chuo-Ling Chang,
Ming Guang Yong, Juhyun Lee,
Wan-Teh Chang, Wei Hua,
Manfred Georg, and Matthias Grundmann.
2019.

</span>
<span class="ltx_bibblock">MediaPipe: A Framework for Building Perception
Pipelines.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1906.08172 [cs.DC]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luvizon et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Diogo Luvizon, Marc
Habermann, Vladislav Golyanik, Adam
Kortylewski, and Christian Theobalt.
2023.

</span>
<span class="ltx_bibblock">Scene-Aware 3D Multi-Human Motion Capture from a
Single Camera.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2301.05175 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Congcong Ma, Wenfeng Li,
Raffaele Gravina, and Giancarlo
Fortino. 2017.

</span>
<span class="ltx_bibblock">Posture Detection Based on Smart Cushion
for Wheelchair Users.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Sensors (Basel, Switzerland)</em>
17, 4 (March
2017), 719.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3390/s17040719" title="">https://doi.org/10.3390/s17040719</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madan et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Spandan Madan, Timothy
Henry, Jamell Dozier, Helen Ho,
Nishchal Bhandari, Tomotake Sasaki,
Frédo Durand, Hanspeter Pfister, and
Xavier Boix. 2021.

</span>
<span class="ltx_bibblock">When and how CNNs generalize to out-of-distribution
category-viewpoint combinations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2007.08032 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcard et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Timo Marcard, Roberto
Henschel, Michael Black, Bodo Rosenhahn,
and Gerard Pons-Moll. 2018.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">Recovering Accurate 3D Human Pose in the
Wild Using IMUs and a Moving Camera: 15th European Conference, Munich,
Germany, September 8-14, 2018, Proceedings, Part X</em>.

</span>
<span class="ltx_bibblock">614–631.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-030-01249-6_37" title="">https://doi.org/10.1007/978-3-030-01249-6_37</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morrical et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nathan Morrical, Jonathan
Tremblay, Yunzhi Lin, Stephen Tyree,
Stan Birchfield, Valerio Pascucci, and
Ingo Wald. 2021.

</span>
<span class="ltx_bibblock">NViSII: A Scriptable Tool for Photorealistic Image
Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2105.13962 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moryossef et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Amit Moryossef, Ioannis
Tsochantaridis, Roee Aharoni, Sarah
Ebling, and Srini Narayanan.
2020.

</span>
<span class="ltx_bibblock">Real-Time Sign Language Detection using Human Pose
Estimation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2008.04637 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mundt et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Marion Mundt, Zachery
Born, Molly Goldacre, and Jacqueline
Alderson. 2022.

</span>
<span class="ltx_bibblock">Estimating Ground Reaction Forces from
Two-Dimensional Pose Data: A Biomechanics-Based Comparison of
AlphaPose, BlazePose, and OpenPose.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">Sensors (Basel, Switzerland)</em>
23, 1 (Dec.
2022), 78.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3390/s23010078" title="">https://doi.org/10.3390/s23010078</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olugbade et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Temitayo Olugbade, Marta
Bieńkiewicz, Giulia Barbareschi,
Vincenzo D’Amato, Luca Oneto,
Antonio Camurri, Catherine Holloway,
Mårten Björkman, Peter Keller,
Martin Clayton, Amanda Williams,
Nicolas Gold, Cristina Becchio,
Benoît Bardy, and Nadia
Bianchi-Berthouze. 2022.

</span>
<span class="ltx_bibblock">Human Movement Datasets: An Interdisciplinary
Scoping Review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">Comput. Surveys</em> 55
(05 2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3534970" title="">https://doi.org/10.1145/3534970</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Joon Sung Park, Danielle
Bragg, Ece Kamar, and Meredith Ringel
Morris. 2021.

</span>
<span class="ltx_bibblock">Designing an online infrastructure for collecting
AI data from people with disabilities. In
<em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">Proceedings of the 2021 ACM Conference on Fairness,
Accountability, and Transparency</em>. 52–63.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sola Park, Seungjin Yang,
and Hyuk-Jae Lee. 2023.

</span>
<span class="ltx_bibblock">MVDet: multi-view multi-class object detection
without ground plane assumption.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">Pattern Analysis and Applications</em>
26 (06 2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s10044-023-01168-6" title="">https://doi.org/10.1007/s10044-023-01168-6</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Priyanka Patel,
Chun-Hao P. Huang, Joachim Tesch,
David T. Hoffmann, Shashank Tripathi,
and Michael J. Black. 2021.

</span>
<span class="ltx_bibblock">AGORA: Avatars in Geography Optimized for Regression
Analysis.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2104.14643 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pishchulin et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Leonid Pishchulin, Arjun
Jain, Mykhaylo Andriluka, Thorsten
Thormählen, and Bernt Schiele.
2012.

</span>
<span class="ltx_bibblock">Articulated people detection and pose estimation:
Reshaping the future. In <em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">2012 IEEE Conference on
Computer Vision and Pattern Recognition</em>. IEEE, 3178–3185.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pumarola et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Albert Pumarola, Jordi
Sanchez, Gary Choi, Alberto Sanfeliu,
and Francesc Moreno-Noguer.
2019.

</span>
<span class="ltx_bibblock">3DPeople: Modeling the Geometry of Dressed
Humans. In <em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">International Conference in Computer
Vision (ICCV)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Purkrábek and Matas (2023)</span>
<span class="ltx_bibblock">
Miroslav Purkrábek and
Jiří Matas. 2023.

</span>
<span class="ltx_bibblock">Improving 2D Human Pose Estimation across Unseen
Camera Views with Synthetic Data.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2307.06737 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rey et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Vitor Fortes Rey, Peter
Hevesi, Onorina Kovalenko, and Paul
Lukowicz. 2019.

</span>
<span class="ltx_bibblock">Let There Be IMU Data: Generating Training Data for
Wearable, Motion Sensor Based Activity Recognition from Monocular RGB
Videos. In <em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">Adjunct Proceedings of the 2019 ACM
International Joint Conference on Pervasive and Ubiquitous Computing and
Proceedings of the 2019 ACM International Symposium on Wearable Computers</em>
(London, United Kingdom) <em class="ltx_emph ltx_font_italic" id="bib.bib81.4.2">(UbiComp/ISWC ’19 Adjunct)</em>.
Association for Computing Machinery,
New York, NY, USA, 699–708.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3341162.3345590" title="">https://doi.org/10.1145/3341162.3345590</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mike Roberts, Jason
Ramapuram, Anurag Ranjan, Atulit Kumar,
Miguel Angel Bautista, Nathan Paczan,
Russ Webb, and Joshua M. Susskind.
2021.

</span>
<span class="ltx_bibblock">Hypersim: A Photorealistic Synthetic Dataset for
Holistic Indoor Scene Understanding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2011.02523 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ros et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
German Ros, Laura
Sellart, Joanna Materzynska, David
Vazquez, and Antonio M. Lopez.
2016.

</span>
<span class="ltx_bibblock">The SYNTHIA Dataset: A Large Collection of
Synthetic Images for Semantic Segmentation of Urban Scenes. In
<em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>. 3234–3243.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2016.352" title="">https://doi.org/10.1109/CVPR.2016.352</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sapp and Taskar (2013)</span>
<span class="ltx_bibblock">
Ben Sapp and Ben
Taskar. 2013.

</span>
<span class="ltx_bibblock">MODEC: Multimodal Decomposable Models for Human
Pose Estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">Proceedings / CVPR, IEEE Computer Society
Conference on Computer Vision and Pattern Recognition. IEEE Computer Society
Conference on Computer Vision and Pattern Recognition</em>,
3674–3681.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2013.471" title="">https://doi.org/10.1109/CVPR.2013.471</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schober et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Patrick Schober, Christa
Boer, and Lothar A Schwarte.
2018.

</span>
<span class="ltx_bibblock">Correlation coefficients: appropriate use and
interpretation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">Anesthesia &amp; analgesia</em>
126, 5 (2018),
1763–1768.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schreven et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Sander Schreven, Peter J.
Beek, and Jeroen B. J. Smeets.
2015.

</span>
<span class="ltx_bibblock">Optimising Filtering Parameters for a 3D Motion
Analysis System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">Journal of Electromyography and Kinesiology</em>
25, 5 (Oct.
2015), 808–814.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.jelekin.2015.06.004" title="">https://doi.org/10.1016/j.jelekin.2015.06.004</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Bokui Shen, Fei Xia,
Chengshu Li, Roberto Martín-Martín,
Linxi Fan, Guanzhi Wang,
Claudia Pérez-D’Arpino, Shyamal Buch,
Sanjana Srivastava, Lyne P. Tchapmi,
Micael E. Tchapmi, Kent Vainio,
Josiah Wong, Li Fei-Fei, and
Silvio Savarese. 2021.

</span>
<span class="ltx_bibblock">iGibson 1.0: a Simulation Environment for Interactive
Tasks in Large Realistic Scenes.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2012.02924 [cs.AI]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snoke et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Joshua Snoke, Gillian M
Raab, Beata Nowok, Chris Dibben, and
Aleksandra Slavkovic. 2018.

</span>
<span class="ltx_bibblock">General and specific utility measures for synthetic
data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.3.1">Journal of the Royal Statistical Society
Series A: Statistics in Society</em> 181, 3
(2018), 663–688.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ke Sun, Bin Xiao,
Dong Liu, and Jingdong Wang.
2019.

</span>
<span class="ltx_bibblock">Deep High-Resolution Representation Learning for
Human Pose Estimation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1902.09212 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SUTIL (2015)</span>
<span class="ltx_bibblock">
NICOLÁS SALAZAR SUTIL.
2015.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Motion and Representation: The Language of
Human Movement</em>.

</span>
<span class="ltx_bibblock">The MIT Press.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.jstor.org/stable/j.ctt17kk8zx" title="">http://www.jstor.org/stable/j.ctt17kk8zx</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tevet et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Guy Tevet, Sigal Raab,
Brian Gordon, Yonatan Shafir,
Daniel Cohen-Or, and Amit H Bermano.
2022.

</span>
<span class="ltx_bibblock">Human motion diffusion model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">arXiv preprint arXiv:2209.14916</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yonglong Tian, Lijie Fan,
Phillip Isola, Huiwen Chang, and
Dilip Krishnan. 2023.

</span>
<span class="ltx_bibblock">StableRep: Synthetic Images from Text-to-Image Models
Make Strong Visual Representation Learners.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2306.00984 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tobin et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Josh Tobin, Rachel Fong,
Alex Ray, Jonas Schneider,
Wojciech Zaremba, and Pieter Abbeel.
2017.

</span>
<span class="ltx_bibblock">Domain Randomization for Transferring Deep Neural
Networks from Simulation to the Real World.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1703.06907 [cs.RO]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Todorov et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Emanuel Todorov, Tom
Erez, and Yuval Tassa. 2012.

</span>
<span class="ltx_bibblock">MuJoCo: A physics engine for model-based control.
In <em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems</em>. 5026–5033.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/IROS.2012.6386109" title="">https://doi.org/10.1109/IROS.2012.6386109</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tremblay et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jonathan Tremblay, Aayush
Prakash, David Acuna, Mark Brophy,
Varun Jampani, Cem Anil,
Thang To, Eric Cameracci,
Shaad Boochoon, and Stan Birchfield.
2018.

</span>
<span class="ltx_bibblock">Training Deep Networks with Synthetic Data: Bridging
the Reality Gap by Domain Randomization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1804.06516 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trewin (2018)</span>
<span class="ltx_bibblock">
Shari Trewin.
2018.

</span>
<span class="ltx_bibblock">AI fairness for people with disabilities: Point of
view.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">arXiv preprint arXiv:1811.10670</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tzikas (2022)</span>
<span class="ltx_bibblock">
Rigas Tzikas.
2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">How realistic is my synthetic data? A
qualitative approach.</em>
</span>
<span class="ltx_bibblock">Master’s thesis.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Unity Technologies (2022)</span>
<span class="ltx_bibblock">
Unity Technologies.
2022.

</span>
<span class="ltx_bibblock">Unity SynthHomes: A Synthetic Home Interior Dataset
Generator.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Unity-Technologies/SynthHomes" title="">https://github.com/Unity-Technologies/SynthHomes</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Upadhyaya et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Skanda Upadhyaya, Shravan
Bhat, Siddhanth P. Rao, V Ashwin, and
Krishnan Chemmangat. 2022.

</span>
<span class="ltx_bibblock">A cost effective eye movement tracker based wheel
chair control algorithm for people with paraplegia.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2207.10511 [cs.HC]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valtchev and Wu (2021)</span>
<span class="ltx_bibblock">
Svetozar Zarko Valtchev and
Jianhong Wu. 2021.

</span>
<span class="ltx_bibblock">Domain Randomization for Neural Network
Classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">Journal of Big Data</em> 8,
1 (July 2021),
94.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1186/s40537-021-00455-5" title="">https://doi.org/10.1186/s40537-021-00455-5</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varol et al<span class="ltx_text" id="bib.bib101.2.2.1">.</span> (2017a)</span>
<span class="ltx_bibblock">
Gül Varol, Javier
Romero, Xavier Martin, Naureen Mahmood,
Michael J. Black, Ivan Laptev, and
Cordelia Schmid. 2017a.

</span>
<span class="ltx_bibblock">Learning from Synthetic Humans. In
<em class="ltx_emph ltx_font_italic" id="bib.bib101.3.1">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varol et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2017b)</span>
<span class="ltx_bibblock">
Gul Varol, Javier Romero,
Xavier Martin, Naureen Mahmood,
Michael J Black, Ivan Laptev, and
Cordelia Schmid. 2017b.

</span>
<span class="ltx_bibblock">Learning from synthetic humans. In
<em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>. 109–117.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vasquez et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Andres Vasquez, Marina
Kollmitz, Andreas Eitel, and Wolfram
Burgard. 2017.

</span>
<span class="ltx_bibblock">Deep Detection of People and their Mobility Aids for
a Hospital Robot.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1708.00674 [cs.RO]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vignier et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Nicolas Vignier,
Jean-François Ravaud, Myriam Winance,
François-Xavier Lepoutre, and Isabelle
Ville. 2008.

</span>
<span class="ltx_bibblock">Demographics of wheelchair users in France: Results
of National community-based handicaps-incapacités-dépendance surveys.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.3.1">Journal of rehabilitation medicine : official
journal of the UEMS European Board of Physical and Rehabilitation Medicine</em>
40 (04 2008),
231–9.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.2340/16501977-0159" title="">https://doi.org/10.2340/16501977-0159</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yilin Wang, Sasi Inguva,
and Balu Adsumilli. 2019.

</span>
<span class="ltx_bibblock">YouTube UGC Dataset for Video Compression
Research. In <em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">2019 IEEE 21st International Workshop
on Multimedia Signal Processing (MMSP)</em>. 1–5.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/MMSP.2019.8901772" title="">https://doi.org/10.1109/MMSP.2019.8901772</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weng et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhenzhen Weng, Laura
Bravo-Sánchez, and Serena Yeung.
2023.

</span>
<span class="ltx_bibblock">Diffusion-HPC: Generating Synthetic Images with
Realistic Humans.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.3.1">arXiv preprint arXiv:2303.09541</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Whittaker et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Meredith Whittaker, Meryl
Alper, Cynthia L Bennett, Sara Hendren,
Liz Kaziunas, Mara Mills,
Meredith Ringel Morris, Joy Rankin,
Emily Rogers, Marcel Salas,
et al<span class="ltx_text" id="bib.bib107.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Disability, bias, and AI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.4.1">AI Now Institute</em> 8
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wrenninge and Unger (2018)</span>
<span class="ltx_bibblock">
Magnus Wrenninge and
Jonas Unger. 2018.

</span>
<span class="ltx_bibblock">Synscapes: A Photorealistic Synthetic Dataset for
Street Scene Parsing.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1810.08705 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yuxin Wu, Alexander
Kirillov, Francisco Massa, Wan-Yen Lo,
and Ross Girshick. 2019.

</span>
<span class="ltx_bibblock">Detectron2.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/detectron2" title="">https://github.com/facebookresearch/detectron2</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kevin Xie, Tingwu Wang,
Umar Iqbal, Yunrong Guo,
Sanja Fidler, and Florian Shkurti.
2022.

</span>
<span class="ltx_bibblock">Physics-based Human Motion Estimation and Synthesis
from Videos.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2109.09913 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yufei Xu, Jing Zhang,
Qiming Zhang, and Dacheng Tao.
2023.

</span>
<span class="ltx_bibblock">ViTPose++: Vision Transformer Foundation Model for
Generic Body Pose Estimation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2212.04246 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib112.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhitao Yang, Zhongang
Cai, Haiyi Mei, Shuai Liu,
Zhaoxi Chen, Weiye Xiao,
Yukun Wei, Zhongfei Qing,
Chen Wei, Bo Dai, Wayne
Wu, Chen Qian, Dahua Lin,
Ziwei Liu, and Lei Yang.
2023.

</span>
<span class="ltx_bibblock">SynBody: Synthetic Dataset with Layered Human Models
for 3D Human Perception and Modeling.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.17368 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib113.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jianrong Zhang, Yangsong
Zhang, Xiaodong Cun, Shaoli Huang,
Yong Zhang, Hongwei Zhao,
Hongtao Lu, and Xi Shen.
2023.

</span>
<span class="ltx_bibblock">T2M-GPT: Generating Human Motion from Textual
Descriptions with Discrete Representations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2301.06052 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Mingyuan Zhang, Zhongang
Cai, Liang Pan, Fangzhou Hong,
Xinying Guo, Lei Yang, and
Ziwei Liu. 2022.

</span>
<span class="ltx_bibblock">MotionDiffuse: Text-Driven Human Motion Generation
with Diffusion Model.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2208.15001 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib115.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ce Zheng, Wenhan Wu,
Chen Chen, Taojiannan Yang,
Sijie Zhu, Ju Shen,
Nasser Kehtarnavaz, and Mubarak Shah.
2023.

</span>
<span class="ltx_bibblock">Deep Learning-Based Human Pose Estimation: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2012.13392 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zwölfer et al<span class="ltx_text" id="bib.bib116.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Michael Zwölfer,
Dieter Heinrich, Kurt Schindelwig,
Bastian Wandt, Helge Rhodin,
Jörg Spörri, and Werner
Nachbauer. 2023.

</span>
<span class="ltx_bibblock">Deep Learning-Based 2D Keypoint Detection in
Alpine Ski Racing – A Performance Analysis of
State-of-the-Art Algorithms Applied to Regular Skiing and Injury Situations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.3.1">JSAMS Plus</em> 2
(2023), 100034.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.jsampl.2023.100034" title="">https://doi.org/10.1016/j.jsampl.2023.100034</a>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Animation Clip Fix</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">To ensure the human model’s do not overlap with the wheelchair footplates, a <math alttext="35" class="ltx_Math" display="inline" id="A1.SS1.p1.1.m1.1"><semantics id="A1.SS1.p1.1.m1.1a"><mn id="A1.SS1.p1.1.m1.1.1" xref="A1.SS1.p1.1.m1.1.1.cmml">35</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.1.m1.1b"><cn id="A1.SS1.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS1.p1.1.m1.1.1">35</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.1.m1.1c">35</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.1.m1.1d">35</annotation></semantics></math> degree rotation of the hips up toward the sky is applied. This hip rotation decreases the amount of clipping between the legs and the wheelchair. Clipping is a common occurrence in 3D modeling where when objects are within each other, only the object closer to the camera will be rendered and obscure the overlapped object. Examples of this extra clipping are shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.F1" title="In A.1. Animation Clip Fix ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="231" id="A1.F1.g1" src="extracted/5503605/figures/clipping_examples.jpg" width="598"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Appendix figure 1. </span>Examples of generated data with and without increased lower body clipping. Notice the overlap between the ankles and the footplate in the motion sequences with clipping creates models where it looks like the human model has fused into the wheelchair.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="A1.F1.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F1.2">Four examples of animations with clipping and four examples of animations without clipping. Examples of clipping feature the human model with feet that extend into the footplate of the wheelchair and below. Examples without clipping place the feet on top of the footplate.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>Posture to <span class="ltx_text ltx_font_italic" id="A1.SS2.1.1">AnimationClip</span> Conversion</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">We convert the postures resulting from previous sections into human images using human models in Unity which take <span class="ltx_text ltx_font_italic" id="A1.SS2.p1.1.1">AnimationClips</span> as input for pose configurations. To convert pose frames into <span class="ltx_text ltx_font_italic" id="A1.SS2.p1.1.2">AnimationClips</span>, all motion sequences from each set, represented by a series of joint rotations, are individually imported into Blender. Each frame’s joint rotations are applied to the corresponding joint in a Unity Perception human model Blender template and exported as an FBX file. Upon importing an FBX file into Unity, Unity will automatically convert all baked animations into Unity-readable <span class="ltx_text ltx_font_italic" id="A1.SS2.p1.1.3">AnimationClip</span> files which can be used in Unity Perception. All <span class="ltx_text ltx_font_italic" id="A1.SS2.p1.1.4">AnimationClips</span> are then set to read as Unity Humanoid animations for use in data synthesis.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3. </span><span class="ltx_text ltx_font_italic" id="A1.SS3.1.1">WheelPose</span> Randomizers</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">Unity Perception enables the use of the ”randomizer” paradigm to enable users to configure the <span class="ltx_text ltx_font_italic" id="A1.SS3.p1.1.1">domain randomization</span> of individual parameters
<cite class="ltx_cite ltx_citemacro_citep">(Borkman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib16" title="">2021</a>)</cite>. <span class="ltx_text ltx_font_italic" id="A1.SS3.p1.1.2">WheelPose</span> uses multiple Unity Perception default randomizers, PSP custom randomizers <cite class="ltx_cite ltx_citemacro_citep">(Ebadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib29" title="">2022b</a>)</cite>, and a collection of custom <span class="ltx_text ltx_font_italic" id="A1.SS3.p1.1.3">WheelPose</span> randomizers. It is important to note that many of PSP’s custom randomizers have made it into the release version of Unity Perception (1.0.0). We have chosen to maintain the original randomizers used for a direct comparison between data synthesized between PSP and <span class="ltx_text ltx_font_italic" id="A1.SS3.p1.1.4">WheelPose</span>. Like in PSP, our randomizers are regarded as further data augmentation techniques which limits the need for data augmentations during training itself. All randomizers sampled values from a uniform distribution. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.T1" title="In A.3. WheelPose Randomizers ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> outlines the statistical distributions for our randomizer parameters. A brief description of each randomizer used in <span class="ltx_text ltx_font_italic" id="A1.SS3.p1.1.5">WheelPose</span> is described below.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p2">
<p class="ltx_p" id="A1.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p2.1.1">BackgroundObjectPlacementRandomizer.</span> Randomly spawns background and occluder objects within a user-defined 3D volume. Separation distance can be set to dictate the proximity of objects from each other. Poisson-Disk sampling <cite class="ltx_cite ltx_citemacro_citep">(Bridson, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#bib.bib17" title="">2007</a>)</cite> is used to randomly place objects sourced from a set of primitive 3D game objects (cubes, cylinders, spheres, etc.) from Unity Perception in a given area.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p3">
<p class="ltx_p" id="A1.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p3.1.1">BackgroundOccluderScaleRandomizer.</span> Randomizes the scale of the background and occluder objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p4">
<p class="ltx_p" id="A1.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p4.1.1">RotationRandomizer.</span> Randomizes the 3D rotation of background and occluder objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p5">
<p class="ltx_p" id="A1.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p5.1.1">ForegroundObjectPlacementRandomizer.</span> Similar to <span class="ltx_text ltx_font_italic" id="A1.SS3.p5.1.2">BackgroundObjectPlacementRandomizer</span>. Randomly spawns foreground objects selected from the default set of PSP models affixed in wheelchair models.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p6">
<p class="ltx_p" id="A1.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p6.1.1">ForegroundScaleRandomizer.</span> Similar to <span class="ltx_text ltx_font_italic" id="A1.SS3.p6.1.2">BackgroundOccluderScaleRandomizer.</span> Randomizes the scale of foreground objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p7">
<p class="ltx_p" id="A1.SS3.p7.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p7.1.1">TextureRandomizer.</span> Randomizes the texture of predefined objects provided as a JPEG or PNG. We used the set of example textures from Unity Perception which are applied to the background and occluder objects as well as to the background wall when no specific background is set.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p8">
<p class="ltx_p" id="A1.SS3.p8.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p8.1.1">HueOffsetRandomizer.</span> Randomizes the hue offset applied to textures on the object. Applied to background and occluder objects as well as to the background wall when no specific background is set.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p9">
<p class="ltx_p" id="A1.SS3.p9.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p9.1.1">SpriteRandomizer.</span> Randomizes the background wall. Used as an alternative to the <span class="ltx_text ltx_font_italic" id="A1.SS3.p9.1.2">TextureRandomizer</span> when images should not be stretched to fill a canvas.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p10">
<p class="ltx_p" id="A1.SS3.p10.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p10.1.1">HumanGenerationRandomizer.</span> Randomizes the age, sex, ethnicity, height, weight, and clothing of spawned human assets. Humans are spawned in batches called pools which are periodically regenerated through the simulation process. All humans are spawned within a predefined base which contains the wheelchair model used. All textures and models used are sourced directly from SyntheticHumans.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p11">
<p class="ltx_p" id="A1.SS3.p11.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p11.1.1">NonTagAnimationRandomizer.</span> Randomizes the pose applied to a character. The pose is a randomly selected frame from a randomly selected <span class="ltx_text ltx_font_italic" id="A1.SS3.p11.1.2">AnimationClip</span> taken from a universal pool of <span class="ltx_text ltx_font_italic" id="A1.SS3.p11.1.3">AnimationClips</span>. Provides a custom alternative to the Unity Perception AnimationRandomizer for randomizing animations taken from a single pool.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p12">
<p class="ltx_p" id="A1.SS3.p12.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p12.1.1">TransformPlacementRandomizer.</span> Randomizes the position, rotation, and size of generated SyntheticHumans. Rotations around the <span class="ltx_text ltx_font_italic" id="A1.SS3.p12.1.2">X,Z</span>-axis are limited to better represent real world data where users are rarely seen in such orientations.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p13">
<p class="ltx_p" id="A1.SS3.p13.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p13.1.1">SunAngleRandomizer.</span> Randomizes a directional light’s intensity, elevation, and orientation to mimic the lighting effects of the Sun.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p14">
<p class="ltx_p" id="A1.SS3.p14.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p14.1.1">LightRandomizer.</span> Randomizes a light’s intensity and color (RGBA). Also enables the randomization of a light’s on/off state.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p15">
<p class="ltx_p" id="A1.SS3.p15.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p15.1.1">LightPositionRotationRandomizer.</span> Randomizes a light’s global position and rotation in the scene.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p16">
<p class="ltx_p" id="A1.SS3.p16.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p16.1.1">CameraRandomizer.</span> Randomizes the extrinsic parameters of a camera including its global position and rotation. Enables the randomization of intrinsic camera parameters including field of view and focal length to better mimic a physical camera. Adds camera bloom and lens blur around objects that are out of focus to capture more diverse perspectives of the scene.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p17">
<p class="ltx_p" id="A1.SS3.p17.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p17.1.1">PostProcessVolumeRandomizer.</span> Randomizes select post processing effects including vignette, exposure, white balance, depth of field, and color adjustments.</p>
</div>
<figure class="ltx_table" id="A1.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Appendix table 1. </span>Domain randomization parameters of <span class="ltx_text ltx_font_italic" id="A1.T1.3.1">WheelPose</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T1.1" style="width:433.6pt;height:725.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.2pt,15.4pt) scale(0.959336563854018,0.959336563854018) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T1.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.2.1.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_bold" id="A1.T1.1.1.2.1.1.1" style="font-size:50%;">Category</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.2.1.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_bold" id="A1.T1.1.1.2.1.2.1" style="font-size:50%;">Randomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.2.1.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_bold" id="A1.T1.1.1.2.1.3.1" style="font-size:50%;">Parameter</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.2.1.4" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_bold" id="A1.T1.1.1.2.1.4.1" style="font-size:50%;">Distribution</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.3.2.1" rowspan="7" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.3.2.1.1" style="font-size:50%;">Background/Occluder Objects</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.3.2.2" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.3.2.2.1" style="font-size:50%;">BackgroundObjectPlacementRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.3.2.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.3.2.3.1" style="font-size:50%;">object placement</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.3.2.4" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.3.2.4.1" style="font-size:50%;">Cartesian[Uniform(-7.5, 7.5), Uniform(-7.5, 7.5), Uniform(-7.5, 7.5)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.4.3.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.4.3.1.1" style="font-size:50%;">separation distance</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.4.3.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.4.3.2.1" style="font-size:50%;">Cartesian[Constant(2.5), Constant(2.5), Constant(2.5)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.5.4.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.5.4.1.1" style="font-size:50%;">BackgroundOccluderScaleRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.5.4.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.5.4.2.1" style="font-size:50%;">object scale range</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.5.4.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.5.4.3.1" style="font-size:50%;">Cartesian[Uniform(1, 12), Uniform(1, 12), Uniform(1, 12)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.6.5.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.6.5.1.1" style="font-size:50%;">RotationRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.6.5.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.6.5.2.1" style="font-size:50%;">object rotation</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.6.5.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.6.5.3.1" style="font-size:50%;">Euler[Uniform(0, 360), Uniform(0, 360), Uniform(0, 360)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.7.6.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.7.6.1.1" style="font-size:50%;">TextureRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.7.6.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.7.6.2.1" style="font-size:50%;">textures</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.7.6.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.7.6.3.1" style="font-size:50%;">A set of of texture assets</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.8.7.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.8.7.1.1" style="font-size:50%;">SpriteRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.8.7.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.8.7.2.1" style="font-size:50%;">sprites</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.8.7.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.8.7.3.1" style="font-size:50%;">A set of sprite assets</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.9.8.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.9.8.1.1" style="font-size:50%;">HueOffsetRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.9.8.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.9.8.2.1" style="font-size:50%;">hue offset</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.9.8.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.9.8.3.1" style="font-size:50%;">Uniform(-180, 180)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.10.9.1" rowspan="16" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.10.9.1.1" style="font-size:50%;">Human Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.10.9.2" rowspan="8" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.10.9.2.1" style="font-size:50%;">HumanGenerationRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.10.9.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.10.9.3.1" style="font-size:50%;">humans per iteration</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.10.9.4" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.10.9.4.1" style="font-size:50%;">Uniform(5, 12)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.11.10">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.11.10.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.11.10.1.1" style="font-size:50%;">human pool size</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.11.10.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.11.10.2.1" style="font-size:50%;">Constant(50)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.12.11.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.12.11.1.1" style="font-size:50%;">pool refresh interval</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.12.11.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.12.11.2.1" style="font-size:50%;">Constant(400)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.13.12.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.13.12.1.1" style="font-size:50%;">age</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.13.12.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.13.12.2.1" style="font-size:50%;">Uniform(10, 100)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.14.13">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.14.13.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.14.13.1.1" style="font-size:50%;">height</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.14.13.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.14.13.2.1" style="font-size:50%;">Uniform(0.1, 1)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.15.14">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.15.14.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.15.14.1.1" style="font-size:50%;">weight</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.15.14.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.15.14.2.1" style="font-size:50%;">Uniform(0, 1)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.16.15">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.16.15.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.16.15.1.1" style="font-size:50%;">sex</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.16.15.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.16.15.2.1" style="font-size:50%;">male, female</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.17.16">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.17.16.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.17.16.1.1" style="font-size:50%;">ethnicity</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.17.16.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.17.16.2.1" style="font-size:50%;">Caucasian, Asian, Latin American, African, Middle Eastern</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.18.17">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.18.17.1" rowspan="3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.18.17.1.1" style="font-size:50%;">TransformPlacementRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.18.17.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.18.17.2.1" style="font-size:50%;">synthetic human placement</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.18.17.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.18.17.3.1" style="font-size:50%;">Cartesian[Uniform(-7.5, 7.5), Uniform(-7.5, 7.5), Uniform(-4, 1)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.19.18">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.19.18.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.19.18.1.1" style="font-size:50%;">synthetic human rotation</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.19.18.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.19.18.2.1" style="font-size:50%;">Euler[Uniform(0, 20), Uniform(0, 360), Uniform(0, 20)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.20.19">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.20.19.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.20.19.1.1" style="font-size:50%;">synthetic human size range</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.20.19.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.20.19.2.1" style="font-size:50%;">Cartesian[Uniform(0.5, 3), Uniform(0.5, 3), Uniform(0.5, 3)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.21.20">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.21.20.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.21.20.1.1" style="font-size:50%;">ForegroundObjectPlacementRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.21.20.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.21.20.2.1" style="font-size:50%;">predefined model placement</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.21.20.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.21.20.3.1" style="font-size:50%;">Cartesian[Uniform(-7.5, 7.5), Uniform(-7.5, 7.5), Uniform(-9, 6)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.22.21">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.22.21.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.22.21.1.1" style="font-size:50%;">predefined model separation distance</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.22.21.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.22.21.2.1" style="font-size:50%;">Cartesian[Constant(3), Constant(3), Constant(3)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.23.22">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.23.22.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.23.22.1.1" style="font-size:50%;">ForegroundScaleRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.23.22.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.23.22.2.1" style="font-size:50%;">predefined model scale range</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.23.22.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.23.22.3.1" style="font-size:50%;">Cartesian[Uniform(0.5, 3), Uniform(0.5, 3), Uniform(0.5, 3)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.24.23">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.24.23.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.24.23.1.1" style="font-size:50%;">ForegroundRotationRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.24.23.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.24.23.2.1" style="font-size:50%;">predefined model rotation</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.24.23.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.24.23.3.1" style="font-size:50%;">Euler[Uniform(0, 20), Uniform(0, 360), Uniform(0, 20)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.25.24">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.25.24.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.25.24.1.1" style="font-size:50%;">NonTagAnimationRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.25.24.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.25.24.2.1" style="font-size:50%;">animations</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.25.24.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.25.24.3.1" style="font-size:50%;">A set of AnimationClips of arbitrary length</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.26.25">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.26.25.1" rowspan="8" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.26.25.1.1" style="font-size:50%;">Lights</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.26.25.2" rowspan="3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.26.25.2.1" style="font-size:50%;">SunAngleRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.26.25.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.26.25.3.1" style="font-size:50%;">hour</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.26.25.4" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.26.25.4.1" style="font-size:50%;">Uniform(0, 24)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.27.26">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.27.26.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.27.26.1.1" style="font-size:50%;">day of the year</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.27.26.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.27.26.2.1" style="font-size:50%;">Uniform(0, 365)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.28.27">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.28.27.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.28.27.1.1" style="font-size:50%;">latitude</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.28.27.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.28.27.2.1" style="font-size:50%;">Uniform(-90, 90)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.29.28">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.29.28.1" rowspan="3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.29.28.1.1" style="font-size:50%;">LightRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.29.28.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.29.28.2.1" style="font-size:50%;">intensity</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.29.28.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.29.28.3.1" style="font-size:50%;">Uniform(5000, 50000)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.30.29">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.30.29.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.30.29.1.1" style="font-size:50%;">color</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.30.29.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.30.29.2.1" style="font-size:50%;">RGBA[Uniform(0, 1), Uniform(0, 1), Uniform(0, 1), Constant( 1)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.1.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.1.2.1" style="font-size:50%;">enabled</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.1.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><math alttext="P(enabled)=0.8,P(disabled)=0.2" class="ltx_Math" display="inline" id="A1.T1.1.1.1.1.m1.2"><semantics id="A1.T1.1.1.1.1.m1.2a"><mrow id="A1.T1.1.1.1.1.m1.2.2.2" xref="A1.T1.1.1.1.1.m1.2.2.3.cmml"><mrow id="A1.T1.1.1.1.1.m1.1.1.1.1" xref="A1.T1.1.1.1.1.m1.1.1.1.1.cmml"><mrow id="A1.T1.1.1.1.1.m1.1.1.1.1.1" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.cmml"><mi id="A1.T1.1.1.1.1.m1.1.1.1.1.1.3" mathsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.3.cmml">P</mi><mo id="A1.T1.1.1.1.1.m1.1.1.1.1.1.2" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.2" maxsize="50%" minsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.2" mathsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.2.cmml">e</mi><mo id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.3" mathsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.3.cmml">n</mi><mo id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1a" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.4" mathsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.4.cmml">a</mi><mo id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1b" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.5" mathsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.5.cmml">b</mi><mo id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1c" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.6" mathsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.6.cmml">l</mi><mo id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1d" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.7" mathsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.7.cmml">e</mi><mo id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1e" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.8" mathsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.8.cmml">d</mi></mrow><mo id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.3" maxsize="50%" minsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A1.T1.1.1.1.1.m1.1.1.1.1.2" mathsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.2.cmml">=</mo><mn id="A1.T1.1.1.1.1.m1.1.1.1.1.3" mathsize="50%" xref="A1.T1.1.1.1.1.m1.1.1.1.1.3.cmml">0.8</mn></mrow><mo id="A1.T1.1.1.1.1.m1.2.2.2.3" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.3a.cmml">,</mo><mrow id="A1.T1.1.1.1.1.m1.2.2.2.2" xref="A1.T1.1.1.1.1.m1.2.2.2.2.cmml"><mrow id="A1.T1.1.1.1.1.m1.2.2.2.2.1" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.cmml"><mi id="A1.T1.1.1.1.1.m1.2.2.2.2.1.3" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.3.cmml">P</mi><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.1.2" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.2.cmml">⁢</mo><mrow id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.cmml"><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.2" maxsize="50%" minsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.cmml">(</mo><mrow id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.cmml"><mi id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.2" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.2.cmml">d</mi><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.3" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.3.cmml">i</mi><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1a" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.4" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.4.cmml">s</mi><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1b" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.5" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.5.cmml">a</mi><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1c" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.6" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.6.cmml">b</mi><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1d" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.7" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.7.cmml">l</mi><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1e" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.8" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.8.cmml">e</mi><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1f" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1.cmml">⁢</mo><mi id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.9" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.9.cmml">d</mi></mrow><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.3" maxsize="50%" minsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A1.T1.1.1.1.1.m1.2.2.2.2.2" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.2.cmml">=</mo><mn id="A1.T1.1.1.1.1.m1.2.2.2.2.3" mathsize="50%" xref="A1.T1.1.1.1.1.m1.2.2.2.2.3.cmml">0.2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.T1.1.1.1.1.m1.2b"><apply id="A1.T1.1.1.1.1.m1.2.2.3.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2"><csymbol cd="ambiguous" id="A1.T1.1.1.1.1.m1.2.2.3a.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="A1.T1.1.1.1.1.m1.1.1.1.1.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1"><eq id="A1.T1.1.1.1.1.m1.1.1.1.1.2.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.2"></eq><apply id="A1.T1.1.1.1.1.m1.1.1.1.1.1.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1"><times id="A1.T1.1.1.1.1.m1.1.1.1.1.1.2.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.2"></times><ci id="A1.T1.1.1.1.1.m1.1.1.1.1.1.3.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.3">𝑃</ci><apply id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1"><times id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.1"></times><ci id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.2">𝑒</ci><ci id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.3">𝑛</ci><ci id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.4">𝑎</ci><ci id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.5.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.5">𝑏</ci><ci id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.6.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.6">𝑙</ci><ci id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.7.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.7">𝑒</ci><ci id="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.8.cmml" xref="A1.T1.1.1.1.1.m1.1.1.1.1.1.1.1.1.8">𝑑</ci></apply></apply><cn id="A1.T1.1.1.1.1.m1.1.1.1.1.3.cmml" type="float" xref="A1.T1.1.1.1.1.m1.1.1.1.1.3">0.8</cn></apply><apply id="A1.T1.1.1.1.1.m1.2.2.2.2.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2"><eq id="A1.T1.1.1.1.1.m1.2.2.2.2.2.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.2"></eq><apply id="A1.T1.1.1.1.1.m1.2.2.2.2.1.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1"><times id="A1.T1.1.1.1.1.m1.2.2.2.2.1.2.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.2"></times><ci id="A1.T1.1.1.1.1.m1.2.2.2.2.1.3.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.3">𝑃</ci><apply id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1"><times id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.1"></times><ci id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.2.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.2">𝑑</ci><ci id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.3.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.3">𝑖</ci><ci id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.4.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.4">𝑠</ci><ci id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.5.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.5">𝑎</ci><ci id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.6.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.6">𝑏</ci><ci id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.7.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.7">𝑙</ci><ci id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.8.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.8">𝑒</ci><ci id="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.9.cmml" xref="A1.T1.1.1.1.1.m1.2.2.2.2.1.1.1.1.9">𝑑</ci></apply></apply><cn id="A1.T1.1.1.1.1.m1.2.2.2.2.3.cmml" type="float" xref="A1.T1.1.1.1.1.m1.2.2.2.2.3">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T1.1.1.1.1.m1.2c">P(enabled)=0.8,P(disabled)=0.2</annotation><annotation encoding="application/x-llamapun" id="A1.T1.1.1.1.1.m1.2d">italic_P ( italic_e italic_n italic_a italic_b italic_l italic_e italic_d ) = 0.8 , italic_P ( italic_d italic_i italic_s italic_a italic_b italic_l italic_e italic_d ) = 0.2</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.31.30">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.31.30.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.31.30.1.1" style="font-size:50%;">LightPositionRotationRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.31.30.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.31.30.2.1" style="font-size:50%;">position offset from initial position</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.31.30.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.31.30.3.1" style="font-size:50%;">Cartesian[Uniform(-3.65, 3.65), Uniform(-3. 65, 3.65), Uniform(-3.65, 3.65)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.32.31">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.32.31.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.32.31.1.1" style="font-size:50%;">rotation offset from initial rotation</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.32.31.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.32.31.2.1" style="font-size:50%;">Euler[Uniform(-50, 50), Uniform(-50, 50), Uniform(-50, 50)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.33.32">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.33.32.1" rowspan="4" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.33.32.1.1" style="font-size:50%;">Camera</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.33.32.2" rowspan="4" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.33.32.2.1" style="font-size:50%;">CameraRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.33.32.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.33.32.3.1" style="font-size:50%;">field of view</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.33.32.4" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.33.32.4.1" style="font-size:50%;">Uniform(5, 50)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.34.33">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.34.33.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.34.33.1.1" style="font-size:50%;">focal length</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.34.33.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.34.33.2.1" style="font-size:50%;">Uniform(1, 23)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.35.34">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.35.34.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.35.34.1.1" style="font-size:50%;">position offset from initial position</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.35.34.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.35.34.2.1" style="font-size:50%;">Cartesian[Uniform(-5, 5), Uniform(-5, 5), Uniform(-5, 5)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.36.35">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.36.35.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.36.35.1.1" style="font-size:50%;">rotation offset from initial rotation</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.36.35.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.36.35.2.1" style="font-size:50%;">Euler(Uniform(-5, 5), Uniform(-5, 5), Uniform(-5, 5)]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.37.36">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="A1.T1.1.1.37.36.1" rowspan="6" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.37.36.1.1" style="font-size:50%;">Post Processing</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="A1.T1.1.1.37.36.2" rowspan="6" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.37.36.2.1" style="font-size:50%;">PostProcessVolumeRandomizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.37.36.3" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.37.36.3.1" style="font-size:50%;">vignette intensity</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.37.36.4" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.37.36.4.1" style="font-size:50%;">Uniform(5, 50)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.38.37">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.38.37.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.38.37.1.1" style="font-size:50%;">fixed exposure</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.38.37.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.38.37.2.1" style="font-size:50%;">Uniform(5, 10)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.39.38">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.39.38.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.39.38.1.1" style="font-size:50%;">white balance temperature</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.39.38.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.39.38.2.1" style="font-size:50%;">Uniform(-20, 20)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.40.39">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.40.39.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.40.39.1.1" style="font-size:50%;">depth of field focus distance</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.40.39.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.40.39.2.1" style="font-size:50%;">Uniform(.1, 4)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.41.40">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T1.1.1.41.40.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.41.40.1.1" style="font-size:50%;">color adjustments: contrast</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.41.40.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.41.40.2.1" style="font-size:50%;">Uniform(-30, 30)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.42.41">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="A1.T1.1.1.42.41.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text" id="A1.T1.1.1.42.41.1.1" style="font-size:50%;">color adjustments: saturation</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="A1.T1.1.1.42.41.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.42.41.2.1" style="font-size:50%;">Uniform(-30, 30)</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4. </span>Testing Dataset Action Classes</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.2">A set of real-world wheelchair data was collected from <span class="ltx_text ltx_font_italic" id="A1.SS4.p1.2.1">YouTube</span>. A predefined set of <math alttext="16" class="ltx_Math" display="inline" id="A1.SS4.p1.1.m1.1"><semantics id="A1.SS4.p1.1.m1.1a"><mn id="A1.SS4.p1.1.m1.1.1" xref="A1.SS4.p1.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.1.m1.1b"><cn id="A1.SS4.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS4.p1.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.1.m1.1c">16</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p1.1.m1.1d">16</annotation></semantics></math> action classes was defined before being used as keyword searches to identify relevant videos. Action classes were selected based on a mix of common actions and unique wheelchair movements. A total of <math alttext="2,464" class="ltx_Math" display="inline" id="A1.SS4.p1.2.m2.2"><semantics id="A1.SS4.p1.2.m2.2a"><mrow id="A1.SS4.p1.2.m2.2.3.2" xref="A1.SS4.p1.2.m2.2.3.1.cmml"><mn id="A1.SS4.p1.2.m2.1.1" xref="A1.SS4.p1.2.m2.1.1.cmml">2</mn><mo id="A1.SS4.p1.2.m2.2.3.2.1" xref="A1.SS4.p1.2.m2.2.3.1.cmml">,</mo><mn id="A1.SS4.p1.2.m2.2.2" xref="A1.SS4.p1.2.m2.2.2.cmml">464</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.2.m2.2b"><list id="A1.SS4.p1.2.m2.2.3.1.cmml" xref="A1.SS4.p1.2.m2.2.3.2"><cn id="A1.SS4.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS4.p1.2.m2.1.1">2</cn><cn id="A1.SS4.p1.2.m2.2.2.cmml" type="integer" xref="A1.SS4.p1.2.m2.2.2">464</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.2.m2.2c">2,464</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p1.2.m2.2d">2 , 464</annotation></semantics></math> images were collected. More information on action classes is found in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.T2" title="In A.4. Testing Dataset Action Classes ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="A1.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Appendix table 2. </span>Distribution of activity classes in the testing dataset.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.1.1">Activity Class</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.2.1">Percentage of Dataset</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T2.1.2.1.1">talking</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T2.1.2.1.2">21.659%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.3.2">
<td class="ltx_td ltx_align_left" id="A1.T2.1.3.2.1">wheelchair skills</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.3.2.2">14.692%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.4.3">
<td class="ltx_td ltx_align_left" id="A1.T2.1.4.3.1">daily routine</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.4.3.2">13.460%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.5.4">
<td class="ltx_td ltx_align_left" id="A1.T2.1.5.4.1">dance</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.5.4.2">10.664%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.6.5">
<td class="ltx_td ltx_align_left" id="A1.T2.1.6.5.1">basketball</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.6.5.2">8.863%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.7.6">
<td class="ltx_td ltx_align_left" id="A1.T2.1.7.6.1">tennis</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.7.6.2">5.829%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.8.7">
<td class="ltx_td ltx_align_left" id="A1.T2.1.8.7.1">extreme sports</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.8.7.2">5.640%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.9.8">
<td class="ltx_td ltx_align_left" id="A1.T2.1.9.8.1">general sports</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.9.8.2">4.645%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.10.9">
<td class="ltx_td ltx_align_left" id="A1.T2.1.10.9.1">household chores</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.10.9.2">3.318%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.11.10">
<td class="ltx_td ltx_align_left" id="A1.T2.1.11.10.1">shopping</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.11.10.2">2.180%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.12.11">
<td class="ltx_td ltx_align_left" id="A1.T2.1.12.11.1">cooking</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.12.11.2">2.085%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.13.12">
<td class="ltx_td ltx_align_left" id="A1.T2.1.13.12.1">travel</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.13.12.2">1.801%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.14.13">
<td class="ltx_td ltx_align_left" id="A1.T2.1.14.13.1">photoshoot</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.14.13.2">1.611%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.15.14">
<td class="ltx_td ltx_align_left" id="A1.T2.1.15.14.1">rugby</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.15.14.2">1.422%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.16.15">
<td class="ltx_td ltx_align_left" id="A1.T2.1.16.15.1">pickleball</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.16.15.2">1.185%</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.17.16">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T2.1.17.16.1">stretches</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="A1.T2.1.17.16.2">0.948%</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5. </span>Impacts of Keypoint Location Definitions</h3>
<div class="ltx_para" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.3">Unity Perception enables users to redefine different keypoint definitions for image annotations. Unity Perception provides a default COCO <math alttext="17" class="ltx_Math" display="inline" id="A1.SS5.p1.1.m1.1"><semantics id="A1.SS5.p1.1.m1.1a"><mn id="A1.SS5.p1.1.m1.1.1" xref="A1.SS5.p1.1.m1.1.1.cmml">17</mn><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.1.m1.1b"><cn id="A1.SS5.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS5.p1.1.m1.1.1">17</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.1.m1.1c">17</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.1.m1.1d">17</annotation></semantics></math>-keypoint annotation schema which places each keypoint directly on the joint between two bones. However, in human-annotated datasets, many evaluators place the hip much higher than the actual joint between the hips and the femur. We test Unity’s default keypoint schema with lower hips against our own custom COCO <math alttext="17" class="ltx_Math" display="inline" id="A1.SS5.p1.2.m2.1"><semantics id="A1.SS5.p1.2.m2.1a"><mn id="A1.SS5.p1.2.m2.1.1" xref="A1.SS5.p1.2.m2.1.1.cmml">17</mn><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.2.m2.1b"><cn id="A1.SS5.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS5.p1.2.m2.1.1">17</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.2.m2.1c">17</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.2.m2.1d">17</annotation></semantics></math>-keypoint schema which raises the hip keypoint up the torso by <math alttext="\approx 9" class="ltx_Math" display="inline" id="A1.SS5.p1.3.m3.1"><semantics id="A1.SS5.p1.3.m3.1a"><mrow id="A1.SS5.p1.3.m3.1.1" xref="A1.SS5.p1.3.m3.1.1.cmml"><mi id="A1.SS5.p1.3.m3.1.1.2" xref="A1.SS5.p1.3.m3.1.1.2.cmml"></mi><mo id="A1.SS5.p1.3.m3.1.1.1" xref="A1.SS5.p1.3.m3.1.1.1.cmml">≈</mo><mn id="A1.SS5.p1.3.m3.1.1.3" xref="A1.SS5.p1.3.m3.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.3.m3.1b"><apply id="A1.SS5.p1.3.m3.1.1.cmml" xref="A1.SS5.p1.3.m3.1.1"><approx id="A1.SS5.p1.3.m3.1.1.1.cmml" xref="A1.SS5.p1.3.m3.1.1.1"></approx><csymbol cd="latexml" id="A1.SS5.p1.3.m3.1.1.2.cmml" xref="A1.SS5.p1.3.m3.1.1.2">absent</csymbol><cn id="A1.SS5.p1.3.m3.1.1.3.cmml" type="integer" xref="A1.SS5.p1.3.m3.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.3.m3.1c">\approx 9</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.3.m3.1d">≈ 9</annotation></semantics></math>cm. All data is generated in the exact same way with the only difference of how the hip keypoints are defined. Examples of how the annotation schema affects predictions are displayed in <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.F2" title="In A.5. Impacts of Keypoint Location Definitions ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>. We see that changes in the definition of keypoints in synthetic data can drastically change the position of the changed keypoint in predictions. We believe this concept can be used to adapt and tune existing pose estimation models with different keypoint definitions through the use of only synthetic data.</p>
</div>
<figure class="ltx_figure" id="A1.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="A1.F2.1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="A1.F2.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="A1.F2.1.g1" src="extracted/5503605/figures/hip_annotation_examples/s_2_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="A1.F2.1.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F2.1.2">A person in a wheelchair doing a wheelie on grass facing the camera at a 45 degree angle. Both hands are on the wheels and both feet are in the footplate.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="A1.F2.2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="A1.F2.2.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="A1.F2.2.g1" src="extracted/5503605/figures/hip_annotation_examples/s_3_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="A1.F2.2.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F2.2.2">A person in a wheelchair holding a basketball facing the camera. the right hand is holding the basketball near the chest while the left hand is on the wheel.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="A1.F2.3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="A1.F2.3.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="A1.F2.3.g1" src="extracted/5503605/figures/hip_annotation_examples/s_4_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="A1.F2.3.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F2.3.2">A person in a motorized wheelchair facing the camera in a bathroom. Both elbows rest on the armrest while the arms rest near their lap.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="A1.F2.4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="A1.F2.4.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="A1.F2.4.g1" src="extracted/5503605/figures/hip_annotation_examples/s_5_overlayed.png" width="138"/><span class="ltx_ERROR undefined" id="A1.F2.4.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F2.4.2">A side view of a person in a wheelchair rolling forward. Both arms are near the person’s lap.</p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Appendix figure 2. </span>Examples of the different prediction outputs between a lower hip definition and a higher hip definition in synthetic data. Green represents the lower hip definition and red represents the higher hip definition. <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.F2" title="In A.5. Impacts of Keypoint Location Definitions ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.F2" title="Figure 2 ‣ A.5. Impacts of Keypoint Location Definitions ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.F2" title="Figure 2 ‣ A.5. Impacts of Keypoint Location Definitions ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.17063v1#A1.F2" title="Figure 2 ‣ A.5. Impacts of Keypoint Location Definitions ‣ Appendix A Appendix ‣ WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users"><span class="ltx_text ltx_ref_tag">2</span></a> all show examples of where other keypoint predictions are relatively similar with the exception of the hips where the lower hip annotations are placed lower on the body compared to the higher hip annotation. Each image depicts the wheelchair user in a different angle, setting, and action.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Apr 25 22:07:46 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
