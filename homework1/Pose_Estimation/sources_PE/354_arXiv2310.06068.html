<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Augmenting Vision-based Human Pose Estimation with Rotation Matrix</title>
<!--Generated on Mon Oct  9 18:15:13 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="https://browse.arxiv.org/latexml/ar5iv_0.7.4.min.css" type="text/css">
<link rel="stylesheet" href="https://browse.arxiv.org/latexml/styles.css" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://browse.arxiv.org/latexml/addons.js"></script>
<script src="https://browse.arxiv.org/latexml/feedbackOverlay.js"></script>
</head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S1" title="1 INTRODUCTION ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>INTRODUCTION</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S2" title="2 Literature Review ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Literature Review</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS1" title="2.1 Vision-based Human Pose Estimation ‣ 2 Literature Review ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Vision-based Human Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS2" title="2.2 Human Pose Data Augmentation ‣ 2 Literature Review ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Human Pose Data Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS3" title="2.3 Vision-based workout analysis ‣ 2 Literature Review ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Vision-based workout analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S3" title="3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Materials and Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS1" title="3.1 Overview ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS2" title="3.2 Exercise Types and Data Collection ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Exercise Types and Data Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS3" title="3.3 Pose Estimator and Create a dataset ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Pose Estimator and Create a dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS4" title="3.4 Data Augmentation Using the Rotation Matrix ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Data Augmentation Using the Rotation Matrix</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS5" title="3.5 Data preprocessing and Feature Reduction ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Data preprocessing and Feature Reduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS6" title="3.6 Evaluation Measures ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Evaluation Measures</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S4" title="4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>EXPERIMENTAL RESULTS</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS1" title="4.1 Comparison Between the Models ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Comparison Between the Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS2" title="4.2 Key points impacts ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Key points impacts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS3" title="4.3 Comparison with State-of-the-Art Models ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Comparison with State-of-the-Art Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S5" title="5 CONCLUSIONS AND FUTURE WORK ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>CONCLUSIONS AND FUTURE WORK</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Augmenting Vision-based Human Pose Estimation with Rotation Matrix
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Milad Vazan 
<br class="ltx_break">Department of Computer and Data Science, Faculty of Mathematical Science
<br class="ltx_break">Shahid Beheshti University
<br class="ltx_break">Tehran, Iran
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">m_vazan@sbu.ac.ir</span> 
<br class="ltx_break">&amp;Fatemeh Sadat Masoumi 
<br class="ltx_break">Department of Computer Science, Faculty of Mathematics, Statistics, and Computer Science 
<br class="ltx_break">University of Allameh Tabataba’i
<br class="ltx_break">Tehran, Iran
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">fatemeh_masoumi@atu.ac.ir</span> 
<br class="ltx_break">&amp;Ruizhi Ou
<br class="ltx_break">Department of Computer Science at College of Art and Sciences 
<br class="ltx_break">Boston University
<br class="ltx_break">USA
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">ruizhiou@bu.edu
<br class="ltx_break"></span>&amp;Reza Rawassizadeh 
<br class="ltx_break">Department of Computer Science at Metropolitan College 
<br class="ltx_break">Boston University
<br class="ltx_break">USA
<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_typewriter">rezar@bu.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Fitness applications are commonly used to monitor activities within the gym, but they often fail to automatically track indoor activities inside the gym. This study proposes a model that utilizes pose estimation combined with a novel data augmentation method, i.e., rotation matrix. We aim to enhance the classification accuracy of activity recognition based on pose estimation data. Through our experiments, we experiment with different classification algorithms along with image augmentation approaches. Our findings demonstrate that the SVM with SGD optimization, using data augmentation with the Rotation Matrix, yields the most accurate results, achieving a 96% accuracy rate in classifying five physical activities. Conversely, without implementing the data augmentation techniques, the baseline accuracy remains at a modest 64%.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><em id="p1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.1.2" class="ltx_text ltx_font_bold">eywords</span> Physical Activity Classification  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.1.m1.1d">⋅</annotation></semantics></math>
Data Augmentation</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>INTRODUCTION</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">The Covid-19 pandemic has brought sudden changes to individuals and society’s way of life. Lockdown measures have deeply impacted the fitness activities that were regularly practiced in gyms, outdoor spaces (<cite class="ltx_cite ltx_citemacro_cite">Kaur et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>), under expert supervision. Thus, with the suspension and closure of fitness, people were compelled to exercise at home. However, the absence of a trainer and supervision of physical activity hindered this approach. Consequently, conventional training methods for maintaining health and fitness have been replaced by technology (<cite class="ltx_cite ltx_citemacro_cite">Butola et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>); Wang et al. (<a href="#bib.bib74" title="" class="ltx_ref">2021a</a>)</cite>). Individuals attending the gym can use fitness trackers (either on their wrist or inside their smartphone) or indoor sensors as their training aid (<cite class="ltx_cite ltx_citemacro_cite">Khan et al. (<a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite>). Nonetheless, if exercises are performed incorrectly, they can be ineffective and even dangerous (<cite class="ltx_cite ltx_citemacro_cite">Chen and Yang (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>).</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Exercise mistakes often arise when users fail to maintain proper form or pose during their workouts. These mistakes can significantly increase the risk of injuries <cite class="ltx_cite ltx_citemacro_cite">Chen and Yang (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Eivindsen and Kristensen (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>. To prevent such injuries, receiving guidance and coaching on the correct technique is crucial. Automatically tracking physical activities inside the gym, is an inexpensive technique to facilitate identifying mistakes. Besides, it can reduce the user’s cognitive burden while exercising and automatizing the process of fitness data logging. This process can be implemented by an external camera in the user’s proximity <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib74" title="" class="ltx_ref">2021a</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">The use of technology to track and identify human activities is referred to as Human Activity Recognition (HAR). This data is acquired from various devices, such as cameras or wearable sensors <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib75" title="" class="ltx_ref">2021b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Hussain et al. (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>. In practice, HAR approaches can be broadly classified into two categories (<cite class="ltx_cite ltx_citemacro_cite">Hussain et al. (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>); Tee et al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>); Ganesh et al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>); Dang et al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>): <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">vision-based</span> and <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">sensor-based</span> methods. In the vision-based method, a camera, LiDAR, or similar sensor is placed in the user prxoimity to record the activity from the video stream (<cite class="ltx_cite ltx_citemacro_cite">Li and Wang (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>), eliminating the need for users to wear sensors <cite class="ltx_cite ltx_citemacro_cite">Beddiar et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">This research aims to focus on classifying physical activities collected by camera. However, the significant challenge associated with this task is the high requirement for a large and accurately labeled dataset for training. The process of manually collecting and labeling large amounts of data is resource-intensive and time-consuming. To address this challenge, we proposed a novel approach that utilizes only one sample from each activity.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">. Since the camera captures data from a single source and users are positioned toward the camera, the absence of multiple samples creates an ambiguous view for the algorithm. This issue is highlighted when another layer of machine intelligence, such as pose estimation, is added on top of the raw video data. As a result, the HAR algorithm may struggle to detect the correct activity due to limited insight from different angles. To mitigate this issue, we introduce different camera perspectives on physical activities, which enhance the algorithm’s efficiency. In particular, to enhance the pose information obtained from the video, we propose a data augmentation method based on the Rotation Matrix (<cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib66" title="" class="ltx_ref">2020</a>)</cite>). This approach helps to alleviate the ambiguity caused by differing camera perspectives.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">In this study, we experimented with different pose estimation algorithms and identified BlazePose (<cite class="ltx_cite ltx_citemacro_cite">Bazarevsky et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>) as the preferred model to accurately derive the crucial human body key points from every video. Explaining the details of our experiments for pose estimation algorithm selection is not in the scope of this paper. After we apply our augmentation, we experiment with different machine learning models and report their accuracy.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">The contribution of this research can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We tackle the issue of time and labor-intensive data collecting and labeling by adopting a single sample per class approach.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a Rotation matrix as a data augmentation method to mitigate the challenges associated with variable camera perspectives.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">By comparing several classification algorithms, We identify SVM with SGD optimization (SVM-SGD) as the most efficient machine learning model for classifying five activities, achieving an impressive accuracy rate of 96%.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Review</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Since our approach uses pose estimation data collected by video from a third person’s view, and data augmentation in computer vision, we describe related work in each section separately. In particular, we review <span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Vision-based Human Pose Estimation</span>, <span id="S2.p1.1.2" class="ltx_text ltx_font_bold">Vision-based workout analysis</span>, and <span id="S2.p1.1.3" class="ltx_text ltx_font_bold">data augmentation approaches</span> used on pose data.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Vision-based Human Pose Estimation</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Human Pose Estimation (HPE) is one of the applications of computer vision. It refers to a technique for obtaining the spatial coordinates of human joints on a person’s body in images or videos (<cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib82" title="" class="ltx_ref">2021</a>); Amini et al. (<a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>). HPE can assist in understanding and retrieving information about human activity (<cite class="ltx_cite ltx_citemacro_cite">Mishra et al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>). Human behavior understanding, violence detection, athletics activity analysis, and generating performance feedback are a few of the many areas in which it has immense applications (<cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib45" title="" class="ltx_ref">2021</a>); Eivindsen and Kristensen (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>). Based on the dimensionality of the output, there are two types of human pose estimation: <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">2D pose estimation</span> (<cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>); Eichner et al. (<a href="#bib.bib12" title="" class="ltx_ref">2012</a>); Raaj et al. (<a href="#bib.bib57" title="" class="ltx_ref">2019</a>); Varadarajan et al. (<a href="#bib.bib69" title="" class="ltx_ref">2018</a>); Li et al. (<a href="#bib.bib35" title="" class="ltx_ref">2019a</a>)</cite>) and <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_bold">3D pose estimation</span> (<cite class="ltx_cite ltx_citemacro_cite">Haque et al. (<a href="#bib.bib18" title="" class="ltx_ref">2016</a>); Liu et al. (<a href="#bib.bib43" title="" class="ltx_ref">2019</a>); Tu et al. (<a href="#bib.bib68" title="" class="ltx_ref">2020</a>)</cite>). 2D pose estimation refers to predicting the key points appearing in the image. In 3D pose estimation, key points are arranged in three dimensions as an output (<cite class="ltx_cite ltx_citemacro_cite">Dawange et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>). A major challenge of 3D pose estimation is the loss of depth information caused by occlusion and blur in the usual intensity images (<cite class="ltx_cite ltx_citemacro_cite">Vosoughi and Amer (<a href="#bib.bib71" title="" class="ltx_ref">2018</a>)</cite>).</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">From another perspective, Human pose estimation can be classified further according to the number of individuals in an image into <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">single-person human pose estimation</span> and <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_bold">multi-person human pose estimation</span>. A single-person human pose estimate describes the task of computing the human skeletal key points from an image or video frame (<cite class="ltx_cite ltx_citemacro_cite">Groos et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>). On the other hand, a multi-person pose estimate includes estimating the key points of multiple individuals, in which the number of individuals is not known beforehand (<cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite>). To estimate the poses of more than one individual, it is necessary to distinguish the poses of each person (<cite class="ltx_cite ltx_citemacro_cite">Ou and Wu (<a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite>). However, recovering the absolute pose of a 3D object in camera-centered coordinates is challenging. The inherent depth ambiguity and occlusions present in cluttered scenes make it challenging for multiple instances to estimate their precise locations <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib72" title="" class="ltx_ref">2020</a>)</cite>.
</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">The pose estimation process can be divided into two major approaches: <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">bottom-up</span> and <span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_bold">top-down</span> approaches (<cite class="ltx_cite ltx_citemacro_cite">Jung et al. (<a href="#bib.bib26" title="" class="ltx_ref">2007</a>); Li et al. (<a href="#bib.bib34" title="" class="ltx_ref">2021a</a>); Azmi et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>); Li et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021b</a>); Handa et al. (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>); Wang and Liang (<a href="#bib.bib73" title="" class="ltx_ref">2022</a>); Xu et al. (<a href="#bib.bib76" title="" class="ltx_ref">2021</a>)</cite>). By fitting human body models to input images, bottom-up methods estimate all parts of the human body (<cite class="ltx_cite ltx_citemacro_cite">Jung et al. (<a href="#bib.bib26" title="" class="ltx_ref">2007</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib76" title="" class="ltx_ref">2021</a>)</cite>). In other words, these methods detect body parts first and then assemble them into an object that looks like a human (<cite class="ltx_cite ltx_citemacro_cite">Jiang (<a href="#bib.bib24" title="" class="ltx_ref">2012</a>)</cite>). These methods generally operate in two steps (<cite class="ltx_cite ltx_citemacro_cite">Kocabas et al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>); Zhao et al. (<a href="#bib.bib84" title="" class="ltx_ref">2020</a>)</cite>): first, body joints are detected without knowing how many people are present or where they are located. Next, a person’s postures are formed by grouping detected joints. By using the target detection algorithm to locate the human target contour, the top-down method is used to find the human key points in the body contour using the human bone key points detection algorithm (<cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021b</a>)</cite>). These methods also operate in two steps (<cite class="ltx_cite ltx_citemacro_cite">Hu and Lv (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>); Rodrigues et al. (<a href="#bib.bib62" title="" class="ltx_ref">2019</a>); Zhao et al. (<a href="#bib.bib84" title="" class="ltx_ref">2020</a>)</cite>): first, detecting the person, and in the next step, estimating their pose based on the cropped area.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p">As a result of the bottom-up approach, all of the key points are detected and grouped before human joints are assigned key points. In contrast, the top-bottom method operates in the opposite manner. It detects the human first and then predicts the person’s key points (<cite class="ltx_cite ltx_citemacro_cite">Parekh and Patel (<a href="#bib.bib54" title="" class="ltx_ref">2021</a>)</cite>). Multi-person pose estimation generally exhibits better accuracy with top-down methods, while bottom-up methods are better at controlling inference time (<cite class="ltx_cite ltx_citemacro_cite">Wang and Liang (<a href="#bib.bib73" title="" class="ltx_ref">2022</a>)</cite>). In summary, the bottom-up pose estimation approaches make it difficult to merge estimations of human body parts. In contrast, the top-down approach has the disadvantage that the calculation cost increases as the number of people increases (<cite class="ltx_cite ltx_citemacro_cite">Handa et al. (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Human Pose Data Augmentation</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">It is a common practice for computer vision applications to augment image datasets with synthetic data to gain better accuracy in a machine learning task (<cite class="ltx_cite ltx_citemacro_cite">Shorten and Khoshgoftaar (<a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite>). Human pose estimation is another area for data augmentation, but the lack of diversity in existing human posture datasets restricts the generalization and, thus, augmentation methods.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">PoseAug <cite class="ltx_cite ltx_citemacro_cite">Gong et al. (<a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite> is an augmentation framework, which learns to augment available poses towards a greater variation in geometry aspect by using a differentiable procedure. In this approach, the estimation errors are taken as feedback, and more robust poses are generated. This improves the generalization, leading to an increase in estimation performance. This method has been applied to Human3.6M (H36m) (<cite class="ltx_cite ltx_citemacro_cite">Ionescu et al. (<a href="#bib.bib22" title="" class="ltx_ref">2013a</a>)</cite>), MPI-INF-3DHP (3DHP) (<cite class="ltx_cite ltx_citemacro_cite">Mehta et al. (<a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite>), 3DPW (<cite class="ltx_cite ltx_citemacro_cite">Von Marcard et al. (<a href="#bib.bib70" title="" class="ltx_ref">2018</a>)</cite>), MPII (<cite class="ltx_cite ltx_citemacro_cite">Andriluka et al. (<a href="#bib.bib2" title="" class="ltx_ref">2014</a>)</cite>), and LSP <cite class="ltx_cite ltx_citemacro_cite">Johnson and Everingham (<a href="#bib.bib25" title="" class="ltx_ref">2010</a>)</cite> datasets. It improves the model performance with different amounts of training data on H36M and 3DHP, this types of improvement is useful where there is limited training data.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Yuan and Du (<a href="#bib.bib81" title="" class="ltx_ref">2021</a>)</cite> designed a data augmentation approach that conducts pose network estimation and data augmentation simultaneously by applying a reward/penalty on a 3D HPE network to optimize it. Pre-trained information is used to create cross-distribution of variation. This approach gained higher performance in contrary to some state-of-the-art methods with both weakly (<cite class="ltx_cite ltx_citemacro_cite">Rhodin et al. (<a href="#bib.bib60" title="" class="ltx_ref">2018b</a>); Kocabas et al. (<a href="#bib.bib32" title="" class="ltx_ref">2019b</a>); Pavllo et al. (<a href="#bib.bib56" title="" class="ltx_ref">2019b</a>); Li et al. (<a href="#bib.bib41" title="" class="ltx_ref">2019c</a>, <a href="#bib.bib37" title="" class="ltx_ref">2020b</a>)</cite>) and fully supervised (<cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib78" title="" class="ltx_ref">2018</a>); Zhao et al. (<a href="#bib.bib83" title="" class="ltx_ref">2019</a>); Sharma et al. (<a href="#bib.bib64" title="" class="ltx_ref">2019</a>); Moon et al. (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite>). Authors built an autoencoder that learns the given pose and generates new poses based on the same style. This generated pose is input to a generative adversarial network (GAN). Their method can generate new labeled data from small labeled data, and it was applied to the Human3.6M dataset (<cite class="ltx_cite ltx_citemacro_cite">Ionescu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2013b</a>)</cite>). This 3D pose estimation method increases the neural networks’ accuracy to 56% on the PCK0.2 dataset, and with regards to 2D pose estimation, it increases the accuracy to 69% on the PCK0.2 dataset.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Rogez and Schmid (<a href="#bib.bib63" title="" class="ltx_ref">2016</a>)</cite> presented a synthesis method using Motion Capture (MoCap) data to augment a real-time 2D pose dataset. Their approach generates synthetic images by mixing annotated images, providing equivalent 3D pose annotations. Initially, the 3D poses are clustered into <em id="S2.SS2.p4.1.1" class="ltx_emph ltx_font_italic">K</em> classes, and then a CNN classifier with K classes is trained to predict pose class distributions given an individual’s bounding box. The evaluation was conducted using the protocol introduced in (<cite class="ltx_cite ltx_citemacro_cite">Yasin et al. (<a href="#bib.bib79" title="" class="ltx_ref">2016</a>); Kostrikov and Gall (<a href="#bib.bib33" title="" class="ltx_ref">2014</a>)</cite>), considering six subjects (S1, S5, S6, S7, S8, and S9). The protocol was tested under three conditions: (i) using synthetic data from H3.6M dataset, (ii) using real images of 190,000 poses, and (iii) using both synthetic and real images combined. Ultimately, the classifier achieved a performance of 97.2 (mm) error when trained on either synthetic or real datasets, and an 88.1 (mm) error when trained on both datasets together.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para ltx_noindent">
<p id="S2.SS2.p5.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib36" title="" class="ltx_ref">2020a</a>)</cite> suggested an evolutionary framework improving the generalization of 2D-to-3D networks for human pose estimation. As Human 3.6M (H36M) dataset (<cite class="ltx_cite ltx_citemacro_cite">Ionescu et al. (<a href="#bib.bib22" title="" class="ltx_ref">2013a</a>)</cite>) was used, and the model’s performance was evaluated by two protocols in Mean Per Joint Position Error (MPJPE) in millimeters. In the first protocol (P1), MPJPE is directly computed. In the second protocol (P2), the ground-truth 3D poses are aligned with the predictions using a rigid transformation before calculating the MPJPE. Another variant of the evaluation protocol, referred to as p1*, incorporates ground truth 2D key points as inputs and eliminates the impact of the initial stage model. Furthermore, MPI-INF-3DHP (<cite class="ltx_cite ltx_citemacro_cite">Mehta et al. (<a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite>) was applied as a benchmark to evaluate the proposed model’s generalization in unseen environments. MPJPE method in comparison with weakly-supervised (<cite class="ltx_cite ltx_citemacro_cite">Rhodin et al. (<a href="#bib.bib59" title="" class="ltx_ref">2018a</a>); Kocabas et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019a</a>); Pavllo et al. (<a href="#bib.bib55" title="" class="ltx_ref">2019a</a>); Li et al. (<a href="#bib.bib41" title="" class="ltx_ref">2019c</a>)</cite>) and fully-supervised approaches<cite class="ltx_cite ltx_citemacro_cite">Martinez et al. (<a href="#bib.bib46" title="" class="ltx_ref">2017</a>); Yang et al. (<a href="#bib.bib78" title="" class="ltx_ref">2018</a>); Zhao et al. (<a href="#bib.bib83" title="" class="ltx_ref">2019</a>); Rhodin et al. (<a href="#bib.bib60" title="" class="ltx_ref">2018b</a>); Sharma et al. (<a href="#bib.bib64" title="" class="ltx_ref">2019</a>); Moon et al. (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite>,for all actions in H36M (<cite class="ltx_cite ltx_citemacro_cite">Ionescu et al. (<a href="#bib.bib22" title="" class="ltx_ref">2013a</a>)</cite>) gained the lowest error for P1, with 60.8 (mm) and 50.09(mm), and for P1*, with 50.05 (mm) and 34.5 (mm), respectively. In terms of p2, the weakly-supervised method gained a 46.02 (mm) error, while the fully-supervised method gained 38.0 (mm), which was more than the achieved error by Yang et al. method (<cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib78" title="" class="ltx_ref">2018</a>)</cite>), with 37.7 (mm).</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Vision-based workout analysis</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">Artificial Intelligence (AI) and Wearable technologies show a significant impact on sports tracking and coaching (<cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib74" title="" class="ltx_ref">2021a</a>)</cite>). Computer vision and Natural Language Processing (<cite class="ltx_cite ltx_citemacro_cite">Rawassizadeh and Rong (<a href="#bib.bib58" title="" class="ltx_ref">2023</a>)</cite>) play a particularly important role among these technologies. AI-enabled applications can effectively monitor and enhance physical exercises, providing valuable assistance to the end users. In the following, we list some examples of how machine learning benefits physical activity tracking.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Rishan et al. (<a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite> developed a real-time deep learning system capable of recognizing yoga poses performed by a user and providing visual guidance through a smartphone camera. Their system incorporates two modules: a pose estimator that employs OpenPose (<cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite>) to identify 25 key points on the user’s body, and a pose detector that uses a deep neural network model with a time-distributed CNN layer and a long short-term memory (LSTM) layer. The outputs of these layers are passed through a SoftMax function and transmitted to a dense layer. The chosen OpenPose model achieved an accuracy of 99.87% and 99.91% for the training and test datasets, respectively.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Yadav et al. (<a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> have proposed a hybrid deep learning model for real-time yoga pose recognition using a standard RGB camera. This model includes a time-distributed CNN layer that extracts features from key points in each frame detected by OpenPose, followed by an LSTM layer for temporal prediction. The use of LSTM improves the model’s robustness and reduces error by taking the pattern of the last frame into account. The model outputs "No Asena" when the SoftMax value for the majority of the predictions in a series is less than the threshold value. After analyzing 45 frames, the model detects yoga poses with a frame-wise accuracy of 99.04% and an overall accuracy of 99.38%. In real-time testing with 12 individuals, the model achieved an accuracy of 98.92%.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.1" class="ltx_p">In a study<cite class="ltx_cite ltx_citemacro_cite">Militaru et al. (<a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite>, a physical form correction framework that utilizes CNN to classify images of individuals performing fitness exercises as either correct, e.g., hips too low, or hips too high. Then, their framework provides live feedback to the user to correct their posture. A labeled dataset of 2400 images of two static fitness exercises was used to train their model. The CNN model achieved an impressive 99% accuracy in real-time.
</p>
</div>
<div id="S2.SS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS3.p5.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Nagarkoti et al. (<a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite> developed a real-time indoor error detection system by analyzing the angle between limb pairs and providing corrective actions to the user. They utilized a CNN model on the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib42" title="" class="ltx_ref">2014</a>)</cite> to extract the coordinates of body parts. To compare the user’s performance with the trainer’s action using key-frame mapping, videos of the two individuals were recorded and synchronized using Dynamic Time Wrapping (DTW) <cite class="ltx_cite ltx_citemacro_cite">Müller (<a href="#bib.bib51" title="" class="ltx_ref">2007</a>)</cite>. However, there is no information on the accuracy of this method is provided.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para ltx_noindent">
<p id="S2.SS3.p6.1" class="ltx_p">In another study<cite class="ltx_cite ltx_citemacro_cite">Khurana et al. (<a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite>, researchers presented a camera-based system that utilizes an end-to-end pipeline, to simultaneously detect, recognize, and track multiple individuals performing exercises. Their system can accurately segment exercises with an 84.6% accuracy rate, recognize five different exercise types with 93.6% accuracy, and count repetitions within an average error margin of +-1.7. However, this system may face two potential challenges: (1) Two different exercises may be clustered together in a single segment, and (2) One exercise may be clustered across two different segments.</p>
</div>
<div id="S2.SS3.p7" class="ltx_para ltx_noindent">
<p id="S2.SS3.p7.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib80" title="" class="ltx_ref">2021</a>)</cite> proposed a system that performs three tasks: person’s pose estimation, exercise recognition, and repetition counting. In their study, the heatmaps matrices were found to be effective in identifying the movement and distribution of the body joints. The ResNet34-based network (<cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib19" title="" class="ltx_ref">2015</a>)</cite>) was used for exercise recognition and achieved 95.69% accuracy. This approach is designed for single-person detection and can estimate seven human poses.</p>
</div>
<div id="S2.SS3.p8" class="ltx_para">
<p id="S2.SS3.p8.1" class="ltx_p">Table <a href="#S2.T1" title="Table 1 ‣ 2.3 Vision-based workout analysis ‣ 2 Literature Review ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the previous works. Generally, the earlier studies aimed to tackle the following issues:</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">A significant portion of previous research on motion detection has concentrated on yoga movements (<cite class="ltx_cite ltx_citemacro_cite">Rishan et al. (<a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Yadav et al. (<a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite>), which are relatively simpler to detect than more intricate fitness movements.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i2.p1.1" class="ltx_p">The number of their training samples has been large and, therefore, hard to generalize for different types of physical activities.</p>
</div>
</li>
</ol>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>summarizes the previous works</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:112pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-152.6pt,39.4pt) scale(0.586963248030252,0.586963248030252) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Reference</span></th>
<th id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Data type</span></th>
<th id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S2.T1.1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.1.4.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.4.1.1.1" class="ltx_td ltx_align_left"><span id="S2.T1.1.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Type of</span></td>
</tr>
<tr id="S2.T1.1.1.1.1.4.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.1.4.1.2.1" class="ltx_td ltx_align_left"><span id="S2.T1.1.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">Activity</span></td>
</tr>
</table>
</th>
<th id="S2.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S2.T1.1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.1.5.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.5.1.1.1" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Number of</span></td>
</tr>
<tr id="S2.T1.1.1.1.1.5.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.1.5.1.2.1" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">activities</span></td>
</tr>
</table>
</th>
<th id="S2.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S2.T1.1.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.1.6.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.6.1.1.1" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Number of</span></td>
</tr>
<tr id="S2.T1.1.1.1.1.6.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.1.6.1.2.1" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">samples</span></td>
</tr>
</table>
</th>
<th id="S2.T1.1.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Keypoints Detection</span></th>
<th id="S2.T1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.8.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Rishan et al. (<a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite></td>
<td id="S2.T1.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">CNN-LSTM</td>
<td id="S2.T1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">video</td>
<td id="S2.T1.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Yoga</td>
<td id="S2.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6</td>
<td id="S2.T1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88 videos</td>
<td id="S2.T1.1.1.2.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">OpenPose, MaskRCNN</td>
<td id="S2.T1.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.91%</td>
</tr>
<tr id="S2.T1.1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Yadav et al. (<a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite></td>
<td id="S2.T1.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">CNN-LSTM</td>
<td id="S2.T1.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">video</td>
<td id="S2.T1.1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Yoga</td>
<td id="S2.T1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6</td>
<td id="S2.T1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88 videos</td>
<td id="S2.T1.1.1.3.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">OpenPose</td>
<td id="S2.T1.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.38%</td>
</tr>
<tr id="S2.T1.1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Militaru et al. (<a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite></td>
<td id="S2.T1.1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">CNN</td>
<td id="S2.T1.1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">image</td>
<td id="S2.T1.1.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Fitness</td>
<td id="S2.T1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S2.T1.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2400 images</td>
<td id="S2.T1.1.1.4.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99%</td>
</tr>
<tr id="S2.T1.1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Nagarkoti et al. (<a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite></td>
<td id="S2.T1.1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">DTW</td>
<td id="S2.T1.1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">video</td>
<td id="S2.T1.1.1.5.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Fitness</td>
<td id="S2.T1.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S2.T1.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.1.5.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.1.5.4.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.5.4.7.1.1" class="ltx_tr">
<td id="S2.T1.1.1.5.4.7.1.1.1" class="ltx_td ltx_align_left">Realtime Multi-Person</td>
</tr>
<tr id="S2.T1.1.1.5.4.7.1.2" class="ltx_tr">
<td id="S2.T1.1.1.5.4.7.1.2.1" class="ltx_td ltx_align_left">2D Pose Estimation using</td>
</tr>
<tr id="S2.T1.1.1.5.4.7.1.3" class="ltx_tr">
<td id="S2.T1.1.1.5.4.7.1.3.1" class="ltx_td ltx_align_left">Part Affinity Fields \<cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib66" title="" class="ltx_ref">2020</a>)</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Khurana et al. (<a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite></td>
<td id="S2.T1.1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">multilayer perceptron</td>
<td id="S2.T1.1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">video</td>
<td id="S2.T1.1.1.6.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Fitness</td>
<td id="S2.T1.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="S2.T1.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">412 videos</td>
<td id="S2.T1.1.1.6.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">93.6%</td>
</tr>
<tr id="S2.T1.1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib80" title="" class="ltx_ref">2021</a>)</cite></td>
<td id="S2.T1.1.1.7.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">ResNet34</td>
<td id="S2.T1.1.1.7.6.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">video</td>
<td id="S2.T1.1.1.7.6.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Fitness</td>
<td id="S2.T1.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">7</td>
<td id="S2.T1.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">29187 videos</td>
<td id="S2.T1.1.1.7.6.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">MSPN <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib38" title="" class="ltx_ref">2019b</a>)</cite>
</td>
<td id="S2.T1.1.1.7.6.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">95.69%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S2.SS3.p9" class="ltx_para ltx_noindent">
<p id="S2.SS3.p9.1" class="ltx_p">Our research proposes a model by using SVM with a Stochastic Gradient Descent (SGD) optimizer. It can estimate five fitness activities. For keypoint detection, Blasepose <cite class="ltx_cite ltx_citemacro_cite">Bazarevsky et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> is used, and the model achieves an accuracy of 96% with 30 videos as the sample. Our cost-effective approach minimizes labeling expenses by using a single sample per class and augmenting it with rotation matrix data. This resolves challenges with camera orientations and placement ambiguity. Consequently, the implementation of this methodology led to a substantial enhancement in the classifier’s performance, raising its result output from 64% to 96%.</p>
</div>
<div id="S2.SS3.p10" class="ltx_para ltx_noindent">
<p id="S2.SS3.p10.1" class="ltx_p">In this study, we focus on complex fitness movements using only <em id="S2.SS3.p10.1.1" class="ltx_emph ltx_font_italic">one</em> training sample for each class and employ data augmentation techniques to enhance the accuracy.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Materials and Methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our method aims to employ computer Vision and classify physical activities. Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Overview ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts the method’s workflow. After receiving each video and extracting the body’s key points through pose estimation, our data augmentation is performed on each video using a rotation matrix. Afterward, the data undergoes a feature reduction process before model training. This step aims to decrease the data’s complexity to a more manageable size, enhancing efficiency in the training process while preserving accuracy. A detailed introduction to each component will be presented in the subsequent sections.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="830" height="1207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Workflow diagram of the proposed method</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Exercise Types and Data Collection</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We considered five types of exercise: angled leg presses, Chin-ups, dumbbell lunges, hack squats, and squats. Our rationale for selecting these experts is to have a sample exercise for each body part. Then, to train the dataset, we collected one sample of each class from Youtube, Instagram or TikTok and checked with a reference video that is evaluated by the human. This ground truth video is also collected by human. For the test dataset, we manually collected and labeled videos for each class in the number that can be seen in Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Exercise Types and Data Collection ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Details of the test dataset</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Class</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">angled leg presses</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">chin-ups</th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">dumbbell lunges</th>
<th id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">hack squats</th>
<th id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">squats</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<th id="S3.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.1.1.1" class="ltx_text ltx_font_bold">number of samples</span></th>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">5</td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6</td>
<td id="S3.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">4</td>
<td id="S3.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">4</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Pose Estimator and Create a dataset</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">We used BlazePose (<cite class="ltx_cite ltx_citemacro_cite">Bazarevsky et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>), a pre-trained model for extracting the key points of the human body. BlazePose is a lightweight convolutional neural network architecture for mobile devices and can be sped up to super-real time on a mobile Graphic Processing Unit (GPU). The network identifies 33 key points of body parts of an individual. This body pose estimation neural network uses heatmaps and regression to coordinate key points. The design of this pipeline employs a detector-tracker setup, and it consists of a lightweight body pose detector followed by a pose tracker network. The tracker predicts the keypoint positions; if the tracker indicates that there is no person in the scene, the detector will be re-run on the next frame.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">As mentioned earlier, the coordinates (x,y,z) of 33 skeleton key points can be calculated using this model. However, for this study, we did not need all these key points and used only 12. Selected key points include: (1) left_shoulder, (2) right_shoulder, (3) left_elbow, (4) right_elbow, (5) left_wrist, (6) right_wrist, (7) left_hip, (8) right_hip, (9) left_knee, (10) right_knee, (11) left_ankle, and (12) right_ankle (See Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3 Pose Estimator and Create a dataset ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="extracted/5161964/img/image15.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="240" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>BlazePose landmarks</figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">A Blazepose model is available in three variations: lite, full, and heavy. From lite to heavy, model accuracy increases, and inference speed decreases. We chose the heavy model because accuracy is important for our problem.
</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">By running BlazePose in single-person pose estimation mode, we created data for each video frame containing 12 key points for the (x, y, z) axes. Since the data generation depends on each video frame, the generated data is time-series data in which the time is the number of frames per video (Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3 Pose Estimator and Create a dataset ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S3.F3.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/image34.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_square" width="275" height="316" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>X</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S3.F3.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/image23.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_square" width="275" height="338" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Y</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Time series made of physical activity chin-ups for axis (a) x and (b) y</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Data Augmentation Using the Rotation Matrix</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">Since a person may be at different angles in front of the camera (See Figure <a href="#S3.F4" title="Figure 4 ‣ 3.4 Data Augmentation Using the Rotation Matrix ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), this can cause ambiguity in detecting movements. In this study, we overcome the ambiguity in the camera perspective by using data augmentation based on <em id="S3.SS4.p1.1.1" class="ltx_emph ltx_font_italic">Rotation Matrix</em>. By using the rotation matrix, different camera angles could be simulated, and this could help a classification algorithm to see different samples during its training time.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="extracted/5161964/img/image48.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Different images of dumbbell lunge from different camera perspectives</figcaption>
</figure>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Figure <a href="#S3.F5" title="Figure 5 ‣ 3.4 Data Augmentation Using the Rotation Matrix ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents a shape of the human body that has three degrees of freedom: Pitch, Roll, and Yaw. However, in this study, we only use the degree of Yaw freedom for data augmentation. We rotate the image around the <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.1.m1.1d">italic_y</annotation></semantics></math>-Axis by computing the following equation:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="R_{y}(\theta)=\begin{bmatrix}cos\theta&amp;0&amp;\sin\theta\\
0&amp;1&amp;0\\
-\sin\theta&amp;0&amp;\cos\theta\\
\end{bmatrix}\begin{bmatrix}x\\
y\\
z\\
\end{bmatrix}=\begin{bmatrix}x^{\prime}\\
y^{\prime}\\
z^{\prime}\\
\end{bmatrix}" display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.5" xref="S3.E1.m1.4.5.cmml"><mrow id="S3.E1.m1.4.5.2" xref="S3.E1.m1.4.5.2.cmml"><msub id="S3.E1.m1.4.5.2.2" xref="S3.E1.m1.4.5.2.2.cmml"><mi id="S3.E1.m1.4.5.2.2.2" xref="S3.E1.m1.4.5.2.2.2.cmml">R</mi><mi id="S3.E1.m1.4.5.2.2.3" xref="S3.E1.m1.4.5.2.2.3.cmml">y</mi></msub><mo id="S3.E1.m1.4.5.2.1" xref="S3.E1.m1.4.5.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.4.5.2.3.2" xref="S3.E1.m1.4.5.2.cmml"><mo stretchy="false" id="S3.E1.m1.4.5.2.3.2.1" xref="S3.E1.m1.4.5.2.cmml">(</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">θ</mi><mo stretchy="false" id="S3.E1.m1.4.5.2.3.2.2" xref="S3.E1.m1.4.5.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.5.3" xref="S3.E1.m1.4.5.3.cmml">=</mo><mrow id="S3.E1.m1.4.5.4" xref="S3.E1.m1.4.5.4.cmml"><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mtr id="S3.E1.m1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1b" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">c</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">o</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.4.cmml">s</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1b" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.1.1.1.5.cmml">θ</mi></mrow></mtd><mtd id="S3.E1.m1.1.1.1.1c" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.1.2.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.1.1.1.1d" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.3.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.3.1.1" xref="S3.E1.m1.1.1.1.1.1.3.1.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E1.m1.1.1.1.1.1.3.1a" xref="S3.E1.m1.1.1.1.1.1.3.1.cmml">⁡</mo><mi id="S3.E1.m1.1.1.1.1.1.3.1.2" xref="S3.E1.m1.1.1.1.1.1.3.1.2.cmml">θ</mi></mrow></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1e" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1f" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.2.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.1.1.1.1g" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.2.2.1" xref="S3.E1.m1.1.1.1.1.2.2.1.cmml">1</mn></mtd><mtd id="S3.E1.m1.1.1.1.1h" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.2.3.1.cmml">0</mn></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1i" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1j" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.3.1.1" xref="S3.E1.m1.1.1.1.1.3.1.1.cmml"><mo rspace="0.167em" id="S3.E1.m1.1.1.1.1.3.1.1a" xref="S3.E1.m1.1.1.1.1.3.1.1.cmml">−</mo><mrow id="S3.E1.m1.1.1.1.1.3.1.1.2" xref="S3.E1.m1.1.1.1.1.3.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.1.1.2.1" xref="S3.E1.m1.1.1.1.1.3.1.1.2.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E1.m1.1.1.1.1.3.1.1.2a" xref="S3.E1.m1.1.1.1.1.3.1.1.2.cmml">⁡</mo><mi id="S3.E1.m1.1.1.1.1.3.1.1.2.2" xref="S3.E1.m1.1.1.1.1.3.1.1.2.2.cmml">θ</mi></mrow></mrow></mtd><mtd id="S3.E1.m1.1.1.1.1k" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.3.2.1" xref="S3.E1.m1.1.1.1.1.3.2.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.1.1.1.1l" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.1.1" xref="S3.E1.m1.1.1.1.1.3.3.1.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E1.m1.1.1.1.1.3.3.1a" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">⁡</mo><mi id="S3.E1.m1.1.1.1.1.3.3.1.2" xref="S3.E1.m1.1.1.1.1.3.3.1.2.cmml">θ</mi></mrow></mtd></mtr></mtable><mo id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.2.1.cmml">]</mo></mrow><mo id="S3.E1.m1.4.5.4.1" xref="S3.E1.m1.4.5.4.1.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.2.cmml"><mo id="S3.E1.m1.2.2.3.1" xref="S3.E1.m1.2.2.2.1.cmml">[</mo><mtable displaystyle="true" rowspacing="0pt" id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mtr id="S3.E1.m1.2.2.1.1a" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1b" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml">x</mi></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1c" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1d" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.cmml">y</mi></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1e" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1f" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.3.1.1" xref="S3.E1.m1.2.2.1.1.3.1.1.cmml">z</mi></mtd></mtr></mtable><mo id="S3.E1.m1.2.2.3.2" xref="S3.E1.m1.2.2.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.4.5.5" xref="S3.E1.m1.4.5.5.cmml">=</mo><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.2.cmml"><mo id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.2.1.cmml">[</mo><mtable displaystyle="true" rowspacing="0pt" id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mtr id="S3.E1.m1.3.3.1.1a" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1b" xref="S3.E1.m1.3.3.1.1.cmml"><msup id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml">x</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.3.cmml">′</mo></msup></mtd></mtr><mtr id="S3.E1.m1.3.3.1.1c" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1d" xref="S3.E1.m1.3.3.1.1.cmml"><msup id="S3.E1.m1.3.3.1.1.2.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.2.1.1.2" xref="S3.E1.m1.3.3.1.1.2.1.1.2.cmml">y</mi><mo id="S3.E1.m1.3.3.1.1.2.1.1.3" xref="S3.E1.m1.3.3.1.1.2.1.1.3.cmml">′</mo></msup></mtd></mtr><mtr id="S3.E1.m1.3.3.1.1e" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1f" xref="S3.E1.m1.3.3.1.1.cmml"><msup id="S3.E1.m1.3.3.1.1.3.1.1" xref="S3.E1.m1.3.3.1.1.3.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.3.1.1.2" xref="S3.E1.m1.3.3.1.1.3.1.1.2.cmml">z</mi><mo id="S3.E1.m1.3.3.1.1.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.1.1.3.cmml">′</mo></msup></mtd></mtr></mtable><mo id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.5.cmml" xref="S3.E1.m1.4.5"><and id="S3.E1.m1.4.5a.cmml" xref="S3.E1.m1.4.5"></and><apply id="S3.E1.m1.4.5b.cmml" xref="S3.E1.m1.4.5"><eq id="S3.E1.m1.4.5.3.cmml" xref="S3.E1.m1.4.5.3"></eq><apply id="S3.E1.m1.4.5.2.cmml" xref="S3.E1.m1.4.5.2"><times id="S3.E1.m1.4.5.2.1.cmml" xref="S3.E1.m1.4.5.2.1"></times><apply id="S3.E1.m1.4.5.2.2.cmml" xref="S3.E1.m1.4.5.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.2.2.1.cmml" xref="S3.E1.m1.4.5.2.2">subscript</csymbol><ci id="S3.E1.m1.4.5.2.2.2.cmml" xref="S3.E1.m1.4.5.2.2.2">𝑅</ci><ci id="S3.E1.m1.4.5.2.2.3.cmml" xref="S3.E1.m1.4.5.2.2.3">𝑦</ci></apply><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝜃</ci></apply><apply id="S3.E1.m1.4.5.4.cmml" xref="S3.E1.m1.4.5.4"><times id="S3.E1.m1.4.5.4.1.cmml" xref="S3.E1.m1.4.5.4.1"></times><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="latexml" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><matrixrow id="S3.E1.m1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">𝑐</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">𝑜</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.4">𝑠</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.5">𝜃</ci></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.2.1">0</cn><apply id="S3.E1.m1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.3.1"><sin id="S3.E1.m1.1.1.1.1.1.3.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.3.1.1"></sin><ci id="S3.E1.m1.1.1.1.1.1.3.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.3.1.2">𝜃</ci></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1.1"><cn type="integer" id="S3.E1.m1.1.1.1.1.2.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1">0</cn><cn type="integer" id="S3.E1.m1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.1">1</cn><cn type="integer" id="S3.E1.m1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.3.1">0</cn></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1c.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.3.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1"><minus id="S3.E1.m1.1.1.1.1.3.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.3.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.2"><sin id="S3.E1.m1.1.1.1.1.3.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.2.1"></sin><ci id="S3.E1.m1.1.1.1.1.3.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.2.2">𝜃</ci></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.1">0</cn><apply id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1"><cos id="S3.E1.m1.1.1.1.1.3.3.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1.1"></cos><ci id="S3.E1.m1.1.1.1.1.3.3.1.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1.2">𝜃</ci></apply></matrixrow></matrix></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.3"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.3.1">matrix</csymbol><matrix id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><matrixrow id="S3.E1.m1.2.2.1.1a.cmml" xref="S3.E1.m1.2.2.1.1"><ci id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1">𝑥</ci></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1b.cmml" xref="S3.E1.m1.2.2.1.1"><ci id="S3.E1.m1.2.2.1.1.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1">𝑦</ci></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1c.cmml" xref="S3.E1.m1.2.2.1.1"><ci id="S3.E1.m1.2.2.1.1.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1.1">𝑧</ci></matrixrow></matrix></apply></apply></apply><apply id="S3.E1.m1.4.5c.cmml" xref="S3.E1.m1.4.5"><eq id="S3.E1.m1.4.5.5.cmml" xref="S3.E1.m1.4.5.5"></eq><share href="#S3.E1.m1.4.5.4.cmml" id="S3.E1.m1.4.5d.cmml" xref="S3.E1.m1.4.5"></share><apply id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.3"><csymbol cd="latexml" id="S3.E1.m1.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.1">matrix</csymbol><matrix id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1"><matrixrow id="S3.E1.m1.3.3.1.1a.cmml" xref="S3.E1.m1.3.3.1.1"><apply id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3">′</ci></apply></matrixrow><matrixrow id="S3.E1.m1.3.3.1.1b.cmml" xref="S3.E1.m1.3.3.1.1"><apply id="S3.E1.m1.3.3.1.1.2.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.2">𝑦</ci><ci id="S3.E1.m1.3.3.1.1.2.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.3">′</ci></apply></matrixrow><matrixrow id="S3.E1.m1.3.3.1.1c.cmml" xref="S3.E1.m1.3.3.1.1"><apply id="S3.E1.m1.3.3.1.1.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1.3.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.3.1.1">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.3.1.1.2">𝑧</ci><ci id="S3.E1.m1.3.3.1.1.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3.1.1.3">′</ci></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">R_{y}(\theta)=\begin{bmatrix}cos\theta&amp;0&amp;\sin\theta\\
0&amp;1&amp;0\\
-\sin\theta&amp;0&amp;\cos\theta\\
\end{bmatrix}\begin{bmatrix}x\\
y\\
z\\
\end{bmatrix}=\begin{bmatrix}x^{\prime}\\
y^{\prime}\\
z^{\prime}\\
\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">italic_R start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ( italic_θ ) = [ start_ARG start_ROW start_CELL italic_c italic_o italic_s italic_θ end_CELL start_CELL 0 end_CELL start_CELL roman_sin italic_θ end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL - roman_sin italic_θ end_CELL start_CELL 0 end_CELL start_CELL roman_cos italic_θ end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL italic_x end_CELL end_ROW start_ROW start_CELL italic_y end_CELL end_ROW start_ROW start_CELL italic_z end_CELL end_ROW end_ARG ] = [ start_ARG start_ROW start_CELL italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL italic_z start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p2.12" class="ltx_p">In this equation <math id="S3.SS4.p2.2.m1.1" class="ltx_Math" alttext="R_{y}(\theta)" display="inline"><semantics id="S3.SS4.p2.2.m1.1a"><mrow id="S3.SS4.p2.2.m1.1.2" xref="S3.SS4.p2.2.m1.1.2.cmml"><msub id="S3.SS4.p2.2.m1.1.2.2" xref="S3.SS4.p2.2.m1.1.2.2.cmml"><mi id="S3.SS4.p2.2.m1.1.2.2.2" xref="S3.SS4.p2.2.m1.1.2.2.2.cmml">R</mi><mi id="S3.SS4.p2.2.m1.1.2.2.3" xref="S3.SS4.p2.2.m1.1.2.2.3.cmml">y</mi></msub><mo id="S3.SS4.p2.2.m1.1.2.1" xref="S3.SS4.p2.2.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS4.p2.2.m1.1.2.3.2" xref="S3.SS4.p2.2.m1.1.2.cmml"><mo stretchy="false" id="S3.SS4.p2.2.m1.1.2.3.2.1" xref="S3.SS4.p2.2.m1.1.2.cmml">(</mo><mi id="S3.SS4.p2.2.m1.1.1" xref="S3.SS4.p2.2.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.SS4.p2.2.m1.1.2.3.2.2" xref="S3.SS4.p2.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m1.1b"><apply id="S3.SS4.p2.2.m1.1.2.cmml" xref="S3.SS4.p2.2.m1.1.2"><times id="S3.SS4.p2.2.m1.1.2.1.cmml" xref="S3.SS4.p2.2.m1.1.2.1"></times><apply id="S3.SS4.p2.2.m1.1.2.2.cmml" xref="S3.SS4.p2.2.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m1.1.2.2.1.cmml" xref="S3.SS4.p2.2.m1.1.2.2">subscript</csymbol><ci id="S3.SS4.p2.2.m1.1.2.2.2.cmml" xref="S3.SS4.p2.2.m1.1.2.2.2">𝑅</ci><ci id="S3.SS4.p2.2.m1.1.2.2.3.cmml" xref="S3.SS4.p2.2.m1.1.2.2.3">𝑦</ci></apply><ci id="S3.SS4.p2.2.m1.1.1.cmml" xref="S3.SS4.p2.2.m1.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m1.1c">R_{y}(\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.2.m1.1d">italic_R start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ( italic_θ )</annotation></semantics></math> represents a rotation matrix around the y-axis. It is a 3x3 matrix that describes the transformation of coordinates after rotating them by an angle <math id="S3.SS4.p2.3.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS4.p2.3.m2.1a"><mi id="S3.SS4.p2.3.m2.1.1" xref="S3.SS4.p2.3.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m2.1b"><ci id="S3.SS4.p2.3.m2.1.1.cmml" xref="S3.SS4.p2.3.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.3.m2.1d">italic_θ</annotation></semantics></math> around the y-axis. <math id="S3.SS4.p2.4.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS4.p2.4.m3.1a"><mi id="S3.SS4.p2.4.m3.1.1" xref="S3.SS4.p2.4.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m3.1b"><ci id="S3.SS4.p2.4.m3.1.1.cmml" xref="S3.SS4.p2.4.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.4.m3.1d">italic_θ</annotation></semantics></math> represents the angle of rotation around the y-axis. <math id="S3.SS4.p2.5.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS4.p2.5.m4.1a"><mi id="S3.SS4.p2.5.m4.1.1" xref="S3.SS4.p2.5.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m4.1b"><ci id="S3.SS4.p2.5.m4.1.1.cmml" xref="S3.SS4.p2.5.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.5.m4.1d">italic_x</annotation></semantics></math>, <math id="S3.SS4.p2.6.m5.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS4.p2.6.m5.1a"><mi id="S3.SS4.p2.6.m5.1.1" xref="S3.SS4.p2.6.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m5.1b"><ci id="S3.SS4.p2.6.m5.1.1.cmml" xref="S3.SS4.p2.6.m5.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m5.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.6.m5.1d">italic_y</annotation></semantics></math>, and <math id="S3.SS4.p2.7.m6.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS4.p2.7.m6.1a"><mi id="S3.SS4.p2.7.m6.1.1" xref="S3.SS4.p2.7.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.7.m6.1b"><ci id="S3.SS4.p2.7.m6.1.1.cmml" xref="S3.SS4.p2.7.m6.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.7.m6.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.7.m6.1d">italic_z</annotation></semantics></math> represent the original coordinates of a point in 3D space. <math id="S3.SS4.p2.8.m7.1" class="ltx_Math" alttext="x^{\prime}" display="inline"><semantics id="S3.SS4.p2.8.m7.1a"><msup id="S3.SS4.p2.8.m7.1.1" xref="S3.SS4.p2.8.m7.1.1.cmml"><mi id="S3.SS4.p2.8.m7.1.1.2" xref="S3.SS4.p2.8.m7.1.1.2.cmml">x</mi><mo id="S3.SS4.p2.8.m7.1.1.3" xref="S3.SS4.p2.8.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.8.m7.1b"><apply id="S3.SS4.p2.8.m7.1.1.cmml" xref="S3.SS4.p2.8.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.8.m7.1.1.1.cmml" xref="S3.SS4.p2.8.m7.1.1">superscript</csymbol><ci id="S3.SS4.p2.8.m7.1.1.2.cmml" xref="S3.SS4.p2.8.m7.1.1.2">𝑥</ci><ci id="S3.SS4.p2.8.m7.1.1.3.cmml" xref="S3.SS4.p2.8.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.8.m7.1c">x^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.8.m7.1d">italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math id="S3.SS4.p2.9.m8.1" class="ltx_Math" alttext="y^{\prime}" display="inline"><semantics id="S3.SS4.p2.9.m8.1a"><msup id="S3.SS4.p2.9.m8.1.1" xref="S3.SS4.p2.9.m8.1.1.cmml"><mi id="S3.SS4.p2.9.m8.1.1.2" xref="S3.SS4.p2.9.m8.1.1.2.cmml">y</mi><mo id="S3.SS4.p2.9.m8.1.1.3" xref="S3.SS4.p2.9.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.9.m8.1b"><apply id="S3.SS4.p2.9.m8.1.1.cmml" xref="S3.SS4.p2.9.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.9.m8.1.1.1.cmml" xref="S3.SS4.p2.9.m8.1.1">superscript</csymbol><ci id="S3.SS4.p2.9.m8.1.1.2.cmml" xref="S3.SS4.p2.9.m8.1.1.2">𝑦</ci><ci id="S3.SS4.p2.9.m8.1.1.3.cmml" xref="S3.SS4.p2.9.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.9.m8.1c">y^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.9.m8.1d">italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math id="S3.SS4.p2.10.m9.1" class="ltx_Math" alttext="z^{\prime}" display="inline"><semantics id="S3.SS4.p2.10.m9.1a"><msup id="S3.SS4.p2.10.m9.1.1" xref="S3.SS4.p2.10.m9.1.1.cmml"><mi id="S3.SS4.p2.10.m9.1.1.2" xref="S3.SS4.p2.10.m9.1.1.2.cmml">z</mi><mo id="S3.SS4.p2.10.m9.1.1.3" xref="S3.SS4.p2.10.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.10.m9.1b"><apply id="S3.SS4.p2.10.m9.1.1.cmml" xref="S3.SS4.p2.10.m9.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.10.m9.1.1.1.cmml" xref="S3.SS4.p2.10.m9.1.1">superscript</csymbol><ci id="S3.SS4.p2.10.m9.1.1.2.cmml" xref="S3.SS4.p2.10.m9.1.1.2">𝑧</ci><ci id="S3.SS4.p2.10.m9.1.1.3.cmml" xref="S3.SS4.p2.10.m9.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.10.m9.1c">z^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.10.m9.1d">italic_z start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> represent the transformed coordinates after applying the rotation matrix <math id="S3.SS4.p2.11.m10.1" class="ltx_Math" alttext="R_{y}(\theta)" display="inline"><semantics id="S3.SS4.p2.11.m10.1a"><mrow id="S3.SS4.p2.11.m10.1.2" xref="S3.SS4.p2.11.m10.1.2.cmml"><msub id="S3.SS4.p2.11.m10.1.2.2" xref="S3.SS4.p2.11.m10.1.2.2.cmml"><mi id="S3.SS4.p2.11.m10.1.2.2.2" xref="S3.SS4.p2.11.m10.1.2.2.2.cmml">R</mi><mi id="S3.SS4.p2.11.m10.1.2.2.3" xref="S3.SS4.p2.11.m10.1.2.2.3.cmml">y</mi></msub><mo id="S3.SS4.p2.11.m10.1.2.1" xref="S3.SS4.p2.11.m10.1.2.1.cmml">⁢</mo><mrow id="S3.SS4.p2.11.m10.1.2.3.2" xref="S3.SS4.p2.11.m10.1.2.cmml"><mo stretchy="false" id="S3.SS4.p2.11.m10.1.2.3.2.1" xref="S3.SS4.p2.11.m10.1.2.cmml">(</mo><mi id="S3.SS4.p2.11.m10.1.1" xref="S3.SS4.p2.11.m10.1.1.cmml">θ</mi><mo stretchy="false" id="S3.SS4.p2.11.m10.1.2.3.2.2" xref="S3.SS4.p2.11.m10.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.11.m10.1b"><apply id="S3.SS4.p2.11.m10.1.2.cmml" xref="S3.SS4.p2.11.m10.1.2"><times id="S3.SS4.p2.11.m10.1.2.1.cmml" xref="S3.SS4.p2.11.m10.1.2.1"></times><apply id="S3.SS4.p2.11.m10.1.2.2.cmml" xref="S3.SS4.p2.11.m10.1.2.2"><csymbol cd="ambiguous" id="S3.SS4.p2.11.m10.1.2.2.1.cmml" xref="S3.SS4.p2.11.m10.1.2.2">subscript</csymbol><ci id="S3.SS4.p2.11.m10.1.2.2.2.cmml" xref="S3.SS4.p2.11.m10.1.2.2.2">𝑅</ci><ci id="S3.SS4.p2.11.m10.1.2.2.3.cmml" xref="S3.SS4.p2.11.m10.1.2.2.3">𝑦</ci></apply><ci id="S3.SS4.p2.11.m10.1.1.cmml" xref="S3.SS4.p2.11.m10.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.11.m10.1c">R_{y}(\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.11.m10.1d">italic_R start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ( italic_θ )</annotation></semantics></math> to the original coordinates <math id="S3.SS4.p2.12.m11.3" class="ltx_Math" alttext="(x,y,z)" display="inline"><semantics id="S3.SS4.p2.12.m11.3a"><mrow id="S3.SS4.p2.12.m11.3.4.2" xref="S3.SS4.p2.12.m11.3.4.1.cmml"><mo stretchy="false" id="S3.SS4.p2.12.m11.3.4.2.1" xref="S3.SS4.p2.12.m11.3.4.1.cmml">(</mo><mi id="S3.SS4.p2.12.m11.1.1" xref="S3.SS4.p2.12.m11.1.1.cmml">x</mi><mo id="S3.SS4.p2.12.m11.3.4.2.2" xref="S3.SS4.p2.12.m11.3.4.1.cmml">,</mo><mi id="S3.SS4.p2.12.m11.2.2" xref="S3.SS4.p2.12.m11.2.2.cmml">y</mi><mo id="S3.SS4.p2.12.m11.3.4.2.3" xref="S3.SS4.p2.12.m11.3.4.1.cmml">,</mo><mi id="S3.SS4.p2.12.m11.3.3" xref="S3.SS4.p2.12.m11.3.3.cmml">z</mi><mo stretchy="false" id="S3.SS4.p2.12.m11.3.4.2.4" xref="S3.SS4.p2.12.m11.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.12.m11.3b"><vector id="S3.SS4.p2.12.m11.3.4.1.cmml" xref="S3.SS4.p2.12.m11.3.4.2"><ci id="S3.SS4.p2.12.m11.1.1.cmml" xref="S3.SS4.p2.12.m11.1.1">𝑥</ci><ci id="S3.SS4.p2.12.m11.2.2.cmml" xref="S3.SS4.p2.12.m11.2.2">𝑦</ci><ci id="S3.SS4.p2.12.m11.3.3.cmml" xref="S3.SS4.p2.12.m11.3.3">𝑧</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.12.m11.3c">(x,y,z)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.12.m11.3d">( italic_x , italic_y , italic_z )</annotation></semantics></math>.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="extracted/5161964/img/1.jpg" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="360" height="325" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Pitch, Roll, and Yaw describe the three degrees of freedom in the human body.</figcaption>
</figure>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">To implement the data augmentation, we rotated <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">italic_y</annotation></semantics></math>-Axis 180 degrees (increasing by one degree) clockwise and 180 degrees counterclockwise. Hence, in any view of the person in the proximity of the camera, we can have a different perspective in 360 degrees. This simple but effective data augmentation helps the algorithm identify and compare physical activities from different perspectives in front of the camera. Figure <a href="#S3.F6" title="Figure 6 ‣ 3.4 Data Augmentation Using the Rotation Matrix ‣ 3 Materials and Methods ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> is a representation of several different angles created for the 5 Physical Activities in the dataset.
</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="x2.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="830" height="1074" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Representation of the y-axis rotation at different angles of a video frame for physical activity (a) squats, (b) chin-ups, (c) dumbbell lunges, (d) hack squats, and (e) angled leg presses. The zero angle represents the pose in the original state, and the angles 5, 10, 15, 20, and 30 are the data generated from it.</figcaption>
</figure>
<div id="S3.SS4.p4" class="ltx_para ltx_noindent">
<p id="S3.SS4.p4.1" class="ltx_p">Nevertheless, it should be noted that applying rotation could distort the body structure and real position of the body joints. Since many algorithm that benefits from pose estimation focus on identifying the movements and not exactly the position of joints, the distortion caused by rotation does not affect the accuracy of these algorithms. Therefore, we recommend tolerating this distortion for the sake of the data augmentation.
Nevertheless, it should be noted that applying rotation could distort the body structure and real position of the body joints. Since many algorithms benefit from pose estimation and try to identify the movements and not exactly the position of joints, the distortion caused by rotation does not affect the accuracy of these algorithms. Therefore, we recommend tolerating this distortion for the sake of the data augmentation.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Data preprocessing and Feature Reduction</h3>

<div id="S3.SS5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS5.p1.1" class="ltx_p">In our study, the joints that are extracted from each video frame by the pose estimation algorithm are considered a feature. We have reduced the number of features for two main reasons, which are described as follows:</p>
</div>
<div id="S3.SS5.p2" class="ltx_para ltx_noindent">
<p id="S3.SS5.p2.1" class="ltx_p">First, since each frame has 12 joints in three dimensions (x, y, z), the number of features to build a model has too many dimensions. To reduce the number of features in the first step, the z-dimensions were removed.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para ltx_noindent">
<p id="S3.SS5.p3.1" class="ltx_p">A second reason is that because the person repeats a given exercise multiple times, each complete execution of the movement produces the same time series pattern. As a result, the type of movement can only be determined by having it repeated once on video. Hence, we limited the video frame count to 50. By limiting the number of frames, the number of redundant and repetitive features is reduced.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para ltx_noindent">
<p id="S3.SS5.p4.1" class="ltx_p">As a result, only the first 50 video frames of each of the extracted x and y dimensions were used. With 12 joints, each has x and y dimensions, by selecting the first 50 frames of each sample, a time series with a length (number of features) of 1200 was constructed.</p>
</div>
<div id="S3.SS5.p5" class="ltx_para ltx_noindent">
<p id="S3.SS5.p5.1" class="ltx_p">However, the results of the experiments have shown that by removing the right shoulder feature, the classification algorithm improves the accuracy for detecting the type of exercise detection (The experimental results can be seen in section <a href="#S4.SS2" title="4.2 Key points impacts ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). Therefore, we used only 11 joints, and as a result, the number of features was reduced from 1200 to 1100.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Evaluation Measures</h3>

<div id="S3.SS6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.p1.1" class="ltx_p">An important aspect that should be considered after creating a machine-learning model is how to generalize the model to unseen data. It must be ensured that the model is efficient and the results of its predictions can be trusted. This will be achieved via evaluation. To evaluate our approach, we have used the following performance metrics: accuracy, precision, recall, and F1-score.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>EXPERIMENTAL RESULTS</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparison Between the Models</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">In the study, Logistic Regression, Gaussian Naive Bayes (GNB), Decision Tree, Support Vector Machine (SVM) with RBF kernel, K Nearest Neighbor (KNN), and Stochastic Gradient Descent on Support Vector Machine (SVM-SGD) were used for our experiments.
Classifiers were implemented using the scikit-learn package of Python <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://scikit-learn.org</span></span></span>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Figures <a href="#S4.F7" title="Figure 7 ‣ 4.1 Comparison Between the Models ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> to <a href="#S4.F12" title="Figure 12 ‣ 4.1 Comparison Between the Models ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> show the confusion matrix of Logistic Regression, Gaussian Naive Bayes, Decision Tree, Support Vector Machines, K Nearest Neighbor (kNN), and SVM-SGD Classifier on no data augmentation and pose data augmentation. Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Comparison Between the Models ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results of our experiments for different models based on the accuracy, precision, recall, and F1-score metrics. As presented in Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Comparison Between the Models ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the SVM-SGD classifier has obtained the best result in all metrics using the Pose estimation method with a score of 96%. Note the significant differences between augmented and not-augmented data in the quality of pose data classification. The SVM-SGD classifier, which is the top classifier, has 32% accuracy using the pose data augmentation without augmentation versus using augmentation. As the confusion matrix in Figure <a href="#S4.F12" title="Figure 12 ‣ 4.1 Comparison Between the Models ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> presents, this classifier has not been able to classify only one sample using the Pose data Augmentation method correctly; it has also predicted correctly in other test samples.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>comparison of the models</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Classifier</span></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">F1-score</span></td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">Precision</span></td>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<td id="S4.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.1.2.2.1.1" class="ltx_text">Logistic Regression</span></td>
<td id="S4.T3.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T3.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.68</td>
<td id="S4.T3.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.67</td>
<td id="S4.T3.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.66</td>
<td id="S4.T3.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.72</td>
</tr>
<tr id="S4.T3.1.3.3" class="ltx_tr">
<td id="S4.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T3.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.92</td>
<td id="S4.T3.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.91</td>
<td id="S4.T3.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.92</td>
<td id="S4.T3.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.93</td>
</tr>
<tr id="S4.T3.1.4.4" class="ltx_tr">
<td id="S4.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.1.4.4.1.1" class="ltx_text">Gaussian Naive Bayes</span></td>
<td id="S4.T3.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T3.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td id="S4.T3.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.65</td>
<td id="S4.T3.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td id="S4.T3.1.4.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.72</td>
</tr>
<tr id="S4.T3.1.5.5" class="ltx_tr">
<td id="S4.T3.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T3.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.68</td>
<td id="S4.T3.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.67</td>
<td id="S4.T3.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.67</td>
<td id="S4.T3.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.72</td>
</tr>
<tr id="S4.T3.1.6.6" class="ltx_tr">
<td id="S4.T3.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.1.6.6.1.1" class="ltx_text">Decision Tree</span></td>
<td id="S4.T3.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T3.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.40</td>
<td id="S4.T3.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.32</td>
<td id="S4.T3.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.41</td>
<td id="S4.T3.1.6.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.28</td>
</tr>
<tr id="S4.T3.1.7.7" class="ltx_tr">
<td id="S4.T3.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T3.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.48</td>
<td id="S4.T3.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.45</td>
<td id="S4.T3.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.50</td>
<td id="S4.T3.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.45</td>
</tr>
<tr id="S4.T3.1.8.8" class="ltx_tr">
<td id="S4.T3.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.1.8.8.1.1" class="ltx_text">SVM</span></td>
<td id="S4.T3.1.8.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T3.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td id="S4.T3.1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.65</td>
<td id="S4.T3.1.8.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td id="S4.T3.1.8.8.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.72</td>
</tr>
<tr id="S4.T3.1.9.9" class="ltx_tr">
<td id="S4.T3.1.9.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T3.1.9.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td id="S4.T3.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.62</td>
<td id="S4.T3.1.9.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.62</td>
<td id="S4.T3.1.9.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.69</td>
</tr>
<tr id="S4.T3.1.10.10" class="ltx_tr">
<td id="S4.T3.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.1.10.10.1.1" class="ltx_text">KNN</span></td>
<td id="S4.T3.1.10.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T3.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td id="S4.T3.1.10.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.65</td>
<td id="S4.T3.1.10.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td id="S4.T3.1.10.10.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.72</td>
</tr>
<tr id="S4.T3.1.11.11" class="ltx_tr">
<td id="S4.T3.1.11.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T3.1.11.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.80</td>
<td id="S4.T3.1.11.11.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.79</td>
<td id="S4.T3.1.11.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.79</td>
<td id="S4.T3.1.11.11.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.83</td>
</tr>
<tr id="S4.T3.1.12.12" class="ltx_tr">
<td id="S4.T3.1.12.12.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.1.12.12.1.1" class="ltx_text ltx_font_bold">SVM-SGD</span></td>
<td id="S4.T3.1.12.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T3.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td id="S4.T3.1.12.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.56</td>
<td id="S4.T3.1.12.12.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.61</td>
<td id="S4.T3.1.12.12.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.54</td>
</tr>
<tr id="S4.T3.1.13.13" class="ltx_tr">
<td id="S4.T3.1.13.13.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T3.1.13.13.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.1.13.13.2.1" class="ltx_text ltx_font_bold">0.96</span></td>
<td id="S4.T3.1.13.13.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.1.13.13.3.1" class="ltx_text ltx_font_bold">0.96</span></td>
<td id="S4.T3.1.13.13.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.1.13.13.4.1" class="ltx_text ltx_font_bold">0.96</span></td>
<td id="S4.T3.1.13.13.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.1.13.13.5.1" class="ltx_text ltx_font_bold">0.96</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F7.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/L1.png" id="S4.F7.sf1.g1" class="ltx_graphics ltx_img_square" width="275" height="284" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>No data Augmentation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F7.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/L2.png" id="S4.F7.sf2.g1" class="ltx_graphics ltx_img_square" width="275" height="284" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Pose data Augmentation</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Confusion matrix of Logistic Regression</figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F8.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/GN1.png" id="S4.F8.sf1.g1" class="ltx_graphics ltx_img_square" width="275" height="276" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>No data Augmentation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F8.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/GN2.png" id="S4.F8.sf2.g1" class="ltx_graphics ltx_img_square" width="275" height="284" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Pose data Augmentation</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Confusion matrix of Gaussian Naive Bayes </figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F9.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/G1.png" id="S4.F9.sf1.g1" class="ltx_graphics ltx_img_square" width="275" height="284" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>No data Augmentation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F9.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/G2.png" id="S4.F9.sf2.g1" class="ltx_graphics ltx_img_square" width="275" height="284" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Pose data Augmentation</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Confusion matrix of DecisionTree </figcaption>
</figure>
<figure id="S4.F10" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F10.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/S1.png" id="S4.F10.sf1.g1" class="ltx_graphics ltx_img_square" width="275" height="276" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>No data Augmentation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F10.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/S2.png" id="S4.F10.sf2.g1" class="ltx_graphics ltx_img_square" width="275" height="284" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Pose data Augmentation</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Confusion matrix of SVM </figcaption>
</figure>
<figure id="S4.F11" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F11.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/K1.png" id="S4.F11.sf1.g1" class="ltx_graphics ltx_img_square" width="275" height="276" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>No data Augmentation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F11.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/K2.png" id="S4.F11.sf2.g1" class="ltx_graphics ltx_img_square" width="275" height="284" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Pose data Augmentation</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Confusion matrix of KNN </figcaption>
</figure>
<figure id="S4.F12" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F12.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/SG1.png" id="S4.F12.sf1.g1" class="ltx_graphics ltx_img_square" width="275" height="284" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>No data Augmentation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F12.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5161964/img/SG2.png" id="S4.F12.sf2.g1" class="ltx_graphics ltx_img_square" width="275" height="284" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Pose data Augmentation</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Confusion matrix of SVM-SGD </figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Key points impacts</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Key points impacts ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> displays results obtained from listed algorithms, using different numbers of key points. It can be observed that the SVM_SGD algorithm yields the best result with 11 key points, specifically when the right shoulder key point is eliminated from the total of 12 key points. The removal of the right shoulder key point potentially improved the performance of the SVM_SGD algorithm by reducing noise or enhancing the discriminatory power of the remaining key points. This strategic adjustment improved the algorithm’s accuracy and effectiveness, as demonstrated in the results presented in Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Key points impacts ‣ 4 EXPERIMENTAL RESULTS ‣ Augmenting Vision-based Human Pose Estimation with Rotation Matrix" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The results with different key points. "12" represents all key points, "11" represents the key points when the right shoulder is removed, "6 left" represents the key points on the left side of the body, and "6 right" represents the key points on the right side of the body.</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T4.1.1.1.1.1" class="ltx_text">Classifier</span></td>
<td id="S4.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T4.1.1.1.2.1" class="ltx_text">Method</span></td>
<td id="S4.T4.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" colspan="4">Accuracy</td>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<td id="S4.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" colspan="4">The number of key points</td>
</tr>
<tr id="S4.T4.1.3.3" class="ltx_tr">
<td id="S4.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T4.1.3.3.1.1" class="ltx_text ltx_font_bold">12</span></td>
<td id="S4.T4.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T4.1.3.3.2.1" class="ltx_text ltx_font_bold">11</span></td>
<td id="S4.T4.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T4.1.3.3.3.1" class="ltx_text ltx_font_bold">6 left</span></td>
<td id="S4.T4.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T4.1.3.3.4.1" class="ltx_text ltx_font_bold">6 right</span></td>
</tr>
<tr id="S4.T4.1.4.4" class="ltx_tr">
<td id="S4.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.1.4.4.1.1" class="ltx_text">Logistic Regression</span></td>
<td id="S4.T4.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T4.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68</td>
<td id="S4.T4.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68</td>
<td id="S4.T4.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">64</td>
<td id="S4.T4.1.4.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">60</td>
</tr>
<tr id="S4.T4.1.5.5" class="ltx_tr">
<td id="S4.T4.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T4.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">88</td>
<td id="S4.T4.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">92</td>
<td id="S4.T4.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">84</td>
<td id="S4.T4.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">72</td>
</tr>
<tr id="S4.T4.1.6.6" class="ltx_tr">
<td id="S4.T4.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.1.6.6.1.1" class="ltx_text">Gaussian Naive Bayes</span></td>
<td id="S4.T4.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T4.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68</td>
<td id="S4.T4.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">64</td>
<td id="S4.T4.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">60</td>
<td id="S4.T4.1.6.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
</tr>
<tr id="S4.T4.1.7.7" class="ltx_tr">
<td id="S4.T4.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T4.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68</td>
<td id="S4.T4.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68</td>
<td id="S4.T4.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68</td>
<td id="S4.T4.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">64</td>
</tr>
<tr id="S4.T4.1.8.8" class="ltx_tr">
<td id="S4.T4.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.1.8.8.1.1" class="ltx_text">Decision Tree</span></td>
<td id="S4.T4.1.8.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T4.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
<td id="S4.T4.1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">40</td>
<td id="S4.T4.1.8.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">52</td>
<td id="S4.T4.1.8.8.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">32</td>
</tr>
<tr id="S4.T4.1.9.9" class="ltx_tr">
<td id="S4.T4.1.9.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T4.1.9.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">36</td>
<td id="S4.T4.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
<td id="S4.T4.1.9.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">56</td>
<td id="S4.T4.1.9.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">40</td>
</tr>
<tr id="S4.T4.1.10.10" class="ltx_tr">
<td id="S4.T4.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.1.10.10.1.1" class="ltx_text">SVM</span></td>
<td id="S4.T4.1.10.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T4.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68</td>
<td id="S4.T4.1.10.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">64</td>
<td id="S4.T4.1.10.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">60</td>
<td id="S4.T4.1.10.10.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
</tr>
<tr id="S4.T4.1.11.11" class="ltx_tr">
<td id="S4.T4.1.11.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T4.1.11.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">64</td>
<td id="S4.T4.1.11.11.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">64</td>
<td id="S4.T4.1.11.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">72</td>
<td id="S4.T4.1.11.11.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">56</td>
</tr>
<tr id="S4.T4.1.12.12" class="ltx_tr">
<td id="S4.T4.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.1.12.12.1.1" class="ltx_text">KNN</span></td>
<td id="S4.T4.1.12.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T4.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68</td>
<td id="S4.T4.1.12.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">64</td>
<td id="S4.T4.1.12.12.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">60</td>
<td id="S4.T4.1.12.12.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
</tr>
<tr id="S4.T4.1.13.13" class="ltx_tr">
<td id="S4.T4.1.13.13.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T4.1.13.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">80</td>
<td id="S4.T4.1.13.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">80</td>
<td id="S4.T4.1.13.13.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">80</td>
<td id="S4.T4.1.13.13.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">64</td>
</tr>
<tr id="S4.T4.1.14.14" class="ltx_tr">
<td id="S4.T4.1.14.14.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.1.14.14.1.1" class="ltx_text">SVM-SGD</span></td>
<td id="S4.T4.1.14.14.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No data Augmentation</td>
<td id="S4.T4.1.14.14.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">56</td>
<td id="S4.T4.1.14.14.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">64</td>
<td id="S4.T4.1.14.14.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">28</td>
<td id="S4.T4.1.14.14.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
</tr>
<tr id="S4.T4.1.15.15" class="ltx_tr">
<td id="S4.T4.1.15.15.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Pose data Augmentation</td>
<td id="S4.T4.1.15.15.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">76</td>
<td id="S4.T4.1.15.15.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T4.1.15.15.3.1" class="ltx_text ltx_font_bold">96</span></td>
<td id="S4.T4.1.15.15.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">72</td>
<td id="S4.T4.1.15.15.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">64</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison with State-of-the-Art Models</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Rishan et al. (<a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Yadav et al. (<a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite>, they have reached the accuracies of 99.91 and 99.38 respectively on the same data set. Although their dataset was trained and evaluated on a small number of samples, i.e. 80 videos, however, the movement recognition was focused on yoga movements and their recognition is easier compared to complex fitness movements. <cite class="ltx_cite ltx_citemacro_cite">Militaru et al. (<a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite>, it reached an accuracy of 99 on the Fitness dataset. However, this accuracy was obtained only in 2 classes. <cite class="ltx_cite ltx_citemacro_cite">Nagarkoti et al. (<a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite>, a system is presented that analyzes the angle between pairs of limbs to detect errors and provide the user with the correct function. However, authors did not specified the accuracy of their system for 2D motion and their system has only been tested for 2 moves.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Ou and Wu (<a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib72" title="" class="ltx_ref">2020</a>)</cite>, the accuracies of 93.6 and 95.69 have been reached on the Fitness dataset, with the number of classes being 5 and 7, respectively. However, the number of video samples was very high. Our model has been able to reach an accuracy of 96 with far fewer videos in 5 classes.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>CONCLUSIONS AND FUTURE WORK</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In this study, we propose a data augmentation for pose data extraction. We use pose estimation to extract body points (jpits) from each input image by using the BlazePose estimation algorithm. To overcome the ambiguity of the camera, we created 360 data for each instance of the training data using the Rotation Matrix. We created data 180 degrees clockwise and 180 degrees counterclockwise. Next, we developed and tested several different machine-learning models on a test dataset to evaluate the impact of data augmentation on the pose classification task. Experimental results showed that the Gradient Descent on the Support Vector Machines algorithm provides the best accuracy to classify data from human pose keypoints.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">Although this model has a high ability to detect movements, On the other hand, this model is limited to a small number of movements. To overcome the existing limitation, future research endeavors should concentrate on collecting more extensive and diverse datasets that encompass a broader spectrum of motions. This dataset expansion will play a crucial role in improving the model’s performance by exposing it to various movement patterns. Consequently, the model will be able to learn and classify movements more effectively, leading to enhanced accuracy. In addition to dataset expansion, future work should also explore the combination of multiple data augmentation techniques. The dataset can be further enriched by incorporating various augmentation methods, such as rotation, scaling, flipping, and noise addition. This augmentation process introduces additional variations in the data, enabling the model to generalize better and improve its classification accuracy.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">To achieve optimal results, future research should consider the synergistic combination of both data augmentation techniques and dataset expansion. By expanding the dataset and employing diverse augmentation strategies, the model’s accuracy can be significantly improved, allowing for the classification of a wider range of movements.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amini et al. [2021]</span>
<span class="ltx_bibblock">
A. Amini, H. Farazi, and S. Behnke.

</span>
<span class="ltx_bibblock">Real-time pose estimation from images for multiple humanoid robots.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Robot World Cup</em>, pages 91–102. Springer, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka et al. [2014]</span>
<span class="ltx_bibblock">
M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele.

</span>
<span class="ltx_bibblock">2d human pose estimation: New benchmark and state of the art
analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on computer Vision and
Pattern Recognition</em>, pages 3686–3693, 2014.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azmi et al. [2020]</span>
<span class="ltx_bibblock">
N. M. W. Azmi, I. F. Albakri, N. M. Suaib, M. S. M. Rahim, and H. Yu.

</span>
<span class="ltx_bibblock">3d motion and skeleton construction from monocular video.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Computational Science and Technology: 6th ICCST 2019, Kota
Kinabalu, Malaysia, 29-30 August 2019</em>, pages 75–84. Springer, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bazarevsky et al. [2020]</span>
<span class="ltx_bibblock">
V. Bazarevsky, I. Grishchenko, K. Raveendran, T. Zhu, F. Zhang, and
M. Grundmann.

</span>
<span class="ltx_bibblock">Blazepose: On-device real-time body pose tracking.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.10204</em>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beddiar et al. [2020]</span>
<span class="ltx_bibblock">
D. R. Beddiar, B. Nini, M. Sabokrou, and A. Hadid.

</span>
<span class="ltx_bibblock">Vision-based human activity recognition: a survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Multimedia Tools and Applications</em>, 79(41-42):30509–30555, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Butola et al. [2020]</span>
<span class="ltx_bibblock">
L. K. Butola, R. Ambad, N. Gusain, and A. Dhok.

</span>
<span class="ltx_bibblock">Indoor activities for physical fitness during lockdown.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Journal of critical reviews</em>, 7(10):542–545, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2017]</span>
<span class="ltx_bibblock">
Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh.

</span>
<span class="ltx_bibblock">Realtime multi-person 2d pose estimation using part affinity fields.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 7291–7299, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2019]</span>
<span class="ltx_bibblock">
Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.

</span>
<span class="ltx_bibblock">Openpose: Realtime multi-person 2d pose estimation using part
affinity fields, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Yang [2020]</span>
<span class="ltx_bibblock">
S. Chen and R. R. Yang.

</span>
<span class="ltx_bibblock">Pose trainer: correcting exercise posture using pose estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.11718</em>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dang et al. [2020]</span>
<span class="ltx_bibblock">
L. M. Dang, K. Min, H. Wang, M. J. Piran, C. H. Lee, and H. Moon.

</span>
<span class="ltx_bibblock">Sensor-based and vision-based human activity recognition: A
comprehensive survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em>, 108:107561, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dawange et al. [2021]</span>
<span class="ltx_bibblock">
S. S. Dawange, A. M. Chavan, A. C. Dusane, and H. P. Bhabad.

</span>
<span class="ltx_bibblock">workout analysis using mediapipe blazepose and machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Journal of Engineering Development and Research
(IJEDR)</em>, 9(12):294–297, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichner et al. [2012]</span>
<span class="ltx_bibblock">
M. Eichner, M. Marin-Jimenez, A. Zisserman, and V. Ferrari.

</span>
<span class="ltx_bibblock">2d articulated human pose estimation and retrieval in (almost)
unconstrained still images.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 99:190–214, 2012.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eivindsen and Kristensen [2020]</span>
<span class="ltx_bibblock">
J. E. Eivindsen and B. Y. Kristensen.

</span>
<span class="ltx_bibblock">Human pose estimation assisted fitness technique evaluation system.

</span>
<span class="ltx_bibblock">Master’s thesis, NTNU, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganesh et al. [2020]</span>
<span class="ltx_bibblock">
P. Ganesh, R. E. Idgahi, C. B. Venkatesh, A. R. Babu, and M. Kyrarini.

</span>
<span class="ltx_bibblock">Personalized system for human gym activity recognition using an rgb
camera.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th ACM International Conference on
PErvasive Technologies Related to Assistive Environments</em>, pages 1–7, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. [2021]</span>
<span class="ltx_bibblock">
K. Gong, J. Zhang, and J. Feng.

</span>
<span class="ltx_bibblock">Poseaug: A differentiable pose augmentation framework for 3d human
pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 8575–8584, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Groos et al. [2021]</span>
<span class="ltx_bibblock">
D. Groos, H. Ramampiaro, and E. A. Ihlen.

</span>
<span class="ltx_bibblock">Efficientpose: Scalable single-person pose estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Applied intelligence</em>, 51:2518–2533, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Handa et al. [2019]</span>
<span class="ltx_bibblock">
H. Handa, S. Ando, T. Ichikawa, R. Yamamoto, and M. Otani.

</span>
<span class="ltx_bibblock">Development of privacy protection monitoring systems using skeleton
models and their evaluation on the viewpoint of fuben-eki.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Human-Computer Interaction. Recognition and Interaction
Technologies: Thematic Area, HCI 2019, Held as Part of the 21st HCI
International Conference, HCII 2019, Orlando, FL, USA, July 26–31, 2019,
Proceedings, Part II 21</em>, pages 235–246. Springer, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haque et al. [2016]</span>
<span class="ltx_bibblock">
A. Haque, B. Peng, Z. Luo, A. Alahi, S. Yeung, and L. Fei-Fei.

</span>
<span class="ltx_bibblock">Towards viewpoint invariant 3d human pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14</em>,
pages 160–177. Springer, 2016.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2015]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition, 2015.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu and Lv [2022]</span>
<span class="ltx_bibblock">
Z. Hu and C. Lv.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Vision-Based Human Activity Recognition</em>.

</span>
<span class="ltx_bibblock">Springer, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hussain et al. [2019]</span>
<span class="ltx_bibblock">
Z. Hussain, M. Sheng, and W. E. Zhang.

</span>
<span class="ltx_bibblock">Different approaches for human activity recognition: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.05074</em>, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ionescu et al. [2013a]</span>
<span class="ltx_bibblock">
C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu.

</span>
<span class="ltx_bibblock">Human3. 6m: Large scale datasets and predictive methods for 3d human
sensing in natural environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, 36(7):1325–1339, 2013a.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ionescu et al. [2013b]</span>
<span class="ltx_bibblock">
C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu.

</span>
<span class="ltx_bibblock">Human3. 6m: Large scale datasets and predictive methods for 3d human
sensing in natural environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, 36(7):1325–1339, 2013b.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang [2012]</span>
<span class="ltx_bibblock">
H. Jiang.

</span>
<span class="ltx_bibblock">Finding people using scale, rotation and articulation invariant
matching.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2012: 12th European Conference on
Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part IV
12</em>, pages 388–401. Springer, 2012.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and Everingham [2010]</span>
<span class="ltx_bibblock">
S. Johnson and M. Everingham.

</span>
<span class="ltx_bibblock">Clustered pose and nonlinear appearance models for human pose
estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">bmvc</em>, volume 2, page 5. Aberystwyth, UK, 2010.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jung et al. [2007]</span>
<span class="ltx_bibblock">
D. J. Jung, K. S. Kwon, and H. J. Kim.

</span>
<span class="ltx_bibblock">Human pose estimation using a mixture of gaussians based image
modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Human-Computer Interaction. HCI Intelligent Multimodal
Interaction Environments: 12th International Conference, HCI International
2007, Beijing, China, July 22-27, 2007, Proceedings, Part III 12</em>, pages
649–658. Springer, 2007.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaur et al. [2020]</span>
<span class="ltx_bibblock">
H. Kaur, T. Singh, Y. K. Arya, and S. Mittal.

</span>
<span class="ltx_bibblock">Physical fitness and exercise during the covid-19 pandemic: A
qualitative enquiry.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Frontiers in psychology</em>, page 2943, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. [2020]</span>
<span class="ltx_bibblock">
U. A. Khan, I. A. Khan, A. Din, W. Jadoon, R. N. Jadoon, M. A. Khan, F. G.
Khan, and A. N. Khan.

</span>
<span class="ltx_bibblock">Towards a complete set of gym exercises detection using smartphone
sensors.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Scientific Programming</em>, 2020:1–12, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khurana et al. [2018]</span>
<span class="ltx_bibblock">
R. Khurana, K. Ahuja, Z. Yu, J. Mankoff, C. Harrison, and M. Goel.

</span>
<span class="ltx_bibblock">Gymcam: Detecting, recognizing and tracking simultaneous exercises in
unconstrained scenes.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive, Mobile, Wearable and
Ubiquitous Technologies</em>, 2(4):1–17, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocabas et al. [2018]</span>
<span class="ltx_bibblock">
M. Kocabas, S. Karagoz, and E. Akbas.

</span>
<span class="ltx_bibblock">Multiposenet: Fast multi-person pose estimation using pose residual
network.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</em>, pages 417–433, 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocabas et al. [2019a]</span>
<span class="ltx_bibblock">
M. Kocabas, S. Karagoz, and E. Akbas.

</span>
<span class="ltx_bibblock">Self-supervised learning of 3d human pose using multi-view geometry.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 1077–1086, 2019a.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocabas et al. [2019b]</span>
<span class="ltx_bibblock">
M. Kocabas, S. Karagoz, and E. Akbas.

</span>
<span class="ltx_bibblock">Self-supervised learning of 3d human pose using multi-view geometry.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 1077–1086, 2019b.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kostrikov and Gall [2014]</span>
<span class="ltx_bibblock">
I. Kostrikov and J. Gall.

</span>
<span class="ltx_bibblock">Depth sweep regression forests for estimating 3d human pose from
images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">BMVC</em>, volume 1, page 5. Nottingham, UK, 2014.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021a]</span>
<span class="ltx_bibblock">
L. Li, R. Bai, S. Zhang, and Q. Zhou.

</span>
<span class="ltx_bibblock">A robust watermarking algorithm for video game artwork based on pose
estimation neural network.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Advances in Artificial Intelligence and Security: 7th
International Conference, ICAIS 2021, Dublin, Ireland, July 19-23, 2021,
Proceedings, Part III 7</em>, pages 217–229. Springer, 2021a.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019a]</span>
<span class="ltx_bibblock">
S. Li, Z. Fang, W.-F. Song, A.-M. Hao, and H. Qin.

</span>
<span class="ltx_bibblock">Bidirectional optimization coupled lightweight networks for efficient
and robust multi-person 2d pose estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Journal of Computer Science and Technology</em>, 34:522–536, 2019a.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020a]</span>
<span class="ltx_bibblock">
S. Li, L. Ke, K. Pratama, Y.-W. Tai, C.-K. Tang, and K.-T. Cheng.

</span>
<span class="ltx_bibblock">Cascaded deep monocular 3d human pose estimation with evolutionary
training data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 6173–6183, 2020a.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020b]</span>
<span class="ltx_bibblock">
S. Li, L. Ke, K. Pratama, Y.-W. Tai, C.-K. Tang, and K.-T. Cheng.

</span>
<span class="ltx_bibblock">Cascaded deep monocular 3d human pose estimation with evolutionary
training data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 6173–6183, 2020b.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019b]</span>
<span class="ltx_bibblock">
W. Li, Z. Wang, B. Yin, Q. Peng, Y. Du, T. Xiao, G. Yu, H. Lu, Y. Wei, and
J. Sun.

</span>
<span class="ltx_bibblock">Rethinking on multi-stage networks for human pose estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.00148</em>, 2019b.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Wang [2022]</span>
<span class="ltx_bibblock">
Y. Li and L. Wang.

</span>
<span class="ltx_bibblock">Human activity recognition based on residual network and bilstm.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, 22(2):635, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021b]</span>
<span class="ltx_bibblock">
Y. Li, J. Li, K. Wang, T. Cao, and H. Wang.

</span>
<span class="ltx_bibblock">Smartphones-based non-contact children’s posture evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Wireless Sensor Networks: 15th China Conference, CWSN 2021,
Guilin, China, October 22–25, 2021, Revised Selected Papers 15</em>, pages
70–83. Springer, 2021b.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019c]</span>
<span class="ltx_bibblock">
Z. Li, X. Wang, F. Wang, and P. Jiang.

</span>
<span class="ltx_bibblock">On boosting single-frame 3d human pose estimation via monocular
videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision</em>, pages 2192–2201, 2019c.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2014]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>, pages
740–755. Springer, 2014.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019]</span>
<span class="ltx_bibblock">
J. Liu, H. Ding, A. Shahroudy, L.-Y. Duan, X. Jiang, G. Wang, and A. C. Kot.

</span>
<span class="ltx_bibblock">Feature boosting network for 3d pose estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, 42(2):494–501, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2022]</span>
<span class="ltx_bibblock">
W. Liu, Q. Bao, Y. Sun, and T. Mei.

</span>
<span class="ltx_bibblock">Recent advances of monocular 2d and 3d human pose estimation: A deep
learning perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 55(4):1–41, 2022.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021]</span>
<span class="ltx_bibblock">
Z. Liu, H. Chen, R. Feng, S. Wu, S. Ji, B. Yang, and X. Wang.

</span>
<span class="ltx_bibblock">Deep dual consecutive network for human pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 525–534, 2021.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez et al. [2017]</span>
<span class="ltx_bibblock">
J. Martinez, R. Hossain, J. Romero, and J. J. Little.

</span>
<span class="ltx_bibblock">A simple yet effective baseline for 3d human pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2640–2649, 2017.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. [2017]</span>
<span class="ltx_bibblock">
D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shafiei, H.-P. Seidel,
W. Xu, D. Casas, and C. Theobalt.

</span>
<span class="ltx_bibblock">Vnect: Real-time 3d human pose estimation with a single rgb camera.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Acm transactions on graphics (tog)</em>, 36(4):1–14, 2017.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Militaru et al. [2020]</span>
<span class="ltx_bibblock">
C. Militaru, M.-D. Militaru, and K.-I. Benta.

</span>
<span class="ltx_bibblock">Physical exercise form correction using neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Companion Publication of the 2020 International Conference
on Multimodal Interaction</em>, pages 240–244, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. [2022]</span>
<span class="ltx_bibblock">
S. Mishra, J. Channegowda, and K. J. Swaroop.

</span>
<span class="ltx_bibblock">An insight into human pose estimation and its applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Bioinformatics and Medical Applications: Big Data Using Deep
Learning Algorithms</em>, pages 147–169, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al. [2019]</span>
<span class="ltx_bibblock">
G. Moon, J. Y. Chang, and K. M. Lee.

</span>
<span class="ltx_bibblock">Camera distance-aware top-down approach for 3d multi-person pose
estimation from a single rgb image.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision</em>, pages 10133–10142, 2019.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller [2007]</span>
<span class="ltx_bibblock">
M. Müller.

</span>
<span class="ltx_bibblock">Dynamic time warping.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Information retrieval for music and motion</em>, pages 69–84,
2007.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagarkoti et al. [2019]</span>
<span class="ltx_bibblock">
A. Nagarkoti, R. Teotia, A. K. Mahale, and P. K. Das.

</span>
<span class="ltx_bibblock">Realtime indoor workout analysis using machine learning &amp; computer
vision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">2019 41st Annual international conference of the IEEE
engineering in medicine and biology society (EMBC)</em>, pages 1440–1443. IEEE,
2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ou and Wu [2020]</span>
<span class="ltx_bibblock">
J. Ou and H. Wu.

</span>
<span class="ltx_bibblock">Efficient human pose estimation with depthwise separable convolution
and person centroid guided joint grouping.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Chinese Conference on Pattern Recognition and Computer
Vision (PRCV)</em>, pages 626–638. Springer, 2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parekh and Patel [2021]</span>
<span class="ltx_bibblock">
P. Parekh and A. Patel.

</span>
<span class="ltx_bibblock">Deep learning-based 2d and 3d human pose estimation: a survey.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of Second International Conference on Computing,
Communications, and Cyber-Security: IC4S 2020</em>, pages 541–556. Springer,
2021.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavllo et al. [2019a]</span>
<span class="ltx_bibblock">
D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli.

</span>
<span class="ltx_bibblock">3d human pose estimation in video with temporal convolutions and
semi-supervised training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 7753–7762, 2019a.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavllo et al. [2019b]</span>
<span class="ltx_bibblock">
D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli.

</span>
<span class="ltx_bibblock">3d human pose estimation in video with temporal convolutions and
semi-supervised training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 7753–7762, 2019b.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raaj et al. [2019]</span>
<span class="ltx_bibblock">
Y. Raaj, H. Idrees, G. Hidalgo, and Y. Sheikh.

</span>
<span class="ltx_bibblock">Efficient online multi-person 2d pose tracking with recurrent
spatio-temporal affinity fields.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 4620–4628, 2019.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rawassizadeh and Rong [2023]</span>
<span class="ltx_bibblock">
R. Rawassizadeh and Y. Rong.

</span>
<span class="ltx_bibblock">Odsearch: Fast and resource efficient on-device natural language
search for fitness trackers’ data.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive, Mobile, Wearable and
Ubiquitous Technologies</em>, 6(4):1–25, 2023.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhodin et al. [2018a]</span>
<span class="ltx_bibblock">
H. Rhodin, J. Spörri, I. Katircioglu, V. Constantin, F. Meyer,
E. Müller, M. Salzmann, and P. Fua.

</span>
<span class="ltx_bibblock">Learning monocular 3d human pose estimation from multi-view images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 8437–8446, 2018a.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhodin et al. [2018b]</span>
<span class="ltx_bibblock">
H. Rhodin, J. Spörri, I. Katircioglu, V. Constantin, F. Meyer,
E. Müller, M. Salzmann, and P. Fua.

</span>
<span class="ltx_bibblock">Learning monocular 3d human pose estimation from multi-view images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 8437–8446, 2018b.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rishan et al. [2020]</span>
<span class="ltx_bibblock">
F. Rishan, B. De Silva, S. Alawathugoda, S. Nijabdeen, L. Rupasinghe, and
C. Liyanapathirana.

</span>
<span class="ltx_bibblock">Infinity yoga tutor: Yoga posture detection and correction system.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">2020 5th International conference on information technology
research (ICITR)</em>, pages 1–6. IEEE, 2020.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodrigues et al. [2019]</span>
<span class="ltx_bibblock">
J. M. Rodrigues, R. J. Veiga, R. Bajireanu, R. Lam, P. J. Cardoso, and P. Bica.

</span>
<span class="ltx_bibblock">Ar contents superimposition on walls and persons.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Universal Access in Human-Computer Interaction. Theory,
Methods and Tools: 13th International Conference, UAHCI 2019, Held as Part of
the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July
26–31, 2019, Proceedings, Part I 21</em>, pages 628–645. Springer, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rogez and Schmid [2016]</span>
<span class="ltx_bibblock">
G. Rogez and C. Schmid.

</span>
<span class="ltx_bibblock">Mocap-guided data augmentation for 3d pose estimation in the wild.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 29, 2016.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. [2019]</span>
<span class="ltx_bibblock">
S. Sharma, P. T. Varigonda, P. Bindal, A. Sharma, and A. Jain.

</span>
<span class="ltx_bibblock">Monocular 3d human pose estimation by generation and ordinal ranking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision</em>, pages 2325–2334, 2019.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shorten and Khoshgoftaar [2019]</span>
<span class="ltx_bibblock">
C. Shorten and T. M. Khoshgoftaar.

</span>
<span class="ltx_bibblock">A survey on image data augmentation for deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Journal of big data</em>, 6(1):1–48, 2019.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2020]</span>
<span class="ltx_bibblock">
J. Sun, M. Wang, X. Zhao, and D. Zhang.

</span>
<span class="ltx_bibblock">Multi-view pose generator based on deep learning for monocular 3d
human pose estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Symmetry</em>, 12(7):1116, 2020.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tee et al. [2022]</span>
<span class="ltx_bibblock">
W. Z. Tee, R. Dave, J. Seliya, and M. Vanamala.

</span>
<span class="ltx_bibblock">A close look into human activity recognition models using deep
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">2022 3rd International Conference on Computing, Networks and
Internet of Things (CNIOT)</em>, pages 201–206. IEEE, 2022.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu et al. [2020]</span>
<span class="ltx_bibblock">
H. Tu, C. Wang, and W. Zeng.

</span>
<span class="ltx_bibblock">Voxelpose: Towards multi-camera 3d human pose estimation in wild
environment.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16</em>, pages 197–212.
Springer, 2020.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varadarajan et al. [2018]</span>
<span class="ltx_bibblock">
S. Varadarajan, P. Datta, and O. Tickoo.

</span>
<span class="ltx_bibblock">A greedy part assignment algorithm for real-time multi-person 2d pose
estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Winter Conference on Applications of Computer
Vision (WACV)</em>, pages 418–426. IEEE, 2018.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Von Marcard et al. [2018]</span>
<span class="ltx_bibblock">
T. Von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and G. Pons-Moll.

</span>
<span class="ltx_bibblock">Recovering accurate 3d human pose in the wild using imus and a moving
camera.

</span>
<span class="ltx_bibblock">In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</em>, pages 601–617, 2018.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vosoughi and Amer [2018]</span>
<span class="ltx_bibblock">
S. Vosoughi and M. A. Amer.

</span>
<span class="ltx_bibblock">Deep 3d human pose estimation under partial body presence.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">2018 25th IEEE International Conference on Image Processing
(ICIP)</em>, pages 569–573. IEEE, 2018.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020]</span>
<span class="ltx_bibblock">
C. Wang, J. Li, W. Liu, C. Qian, and C. Lu.

</span>
<span class="ltx_bibblock">Hmor: Hierarchical multi-person ordinal relations for monocular
multi-person 3d pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16</em>, pages 242–259.
Springer, 2020.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Liang [2022]</span>
<span class="ltx_bibblock">
J. Wang and S. Liang.

</span>
<span class="ltx_bibblock">Pose-enhanced relation feature for action recognition in still
images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">International Conference on Multimedia Modeling</em>, pages
154–165. Springer, 2022.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021a]</span>
<span class="ltx_bibblock">
T. Wang, Y. Gan, S. D. Arena, L. T. Chitkushev, G. Zhang, and R. Rawassizadeh.

</span>
<span class="ltx_bibblock">Advances for indoor fitness tracking, coaching, and motivation: A
review of existing technological advances.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">IEEE Systems, Man, and Cybernetics Magazine</em>, 7(1):4–14, 2021a.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021b]</span>
<span class="ltx_bibblock">
T. Wang, Y. Gan, S. D. Arena, L. T. Chitkushev, G. Zhang, and R. Rawassizadeh.

</span>
<span class="ltx_bibblock">Advances for indoor fitness tracking, coaching, and motivation: A
review of existing technological advances.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">IEEE Systems, Man, and Cybernetics Magazine</em>, 7(1):4–14, 2021b.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2021]</span>
<span class="ltx_bibblock">
F. Xu, K. Davila, S. Setlur, and V. Govindaraju.

</span>
<span class="ltx_bibblock">Skeleton-based methods for speaker action classification on lecture
videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">International Conference on Pattern Recognition</em>, pages
250–264. Springer, 2021.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yadav et al. [2019]</span>
<span class="ltx_bibblock">
S. K. Yadav, A. Singh, A. Gupta, and J. L. Raheja.

</span>
<span class="ltx_bibblock">Real-time yoga recognition using deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Neural Computing and Applications</em>, 31:9349–9361,
2019.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2018]</span>
<span class="ltx_bibblock">
W. Yang, W. Ouyang, X. Wang, J. Ren, H. Li, and X. Wang.

</span>
<span class="ltx_bibblock">3d human pose estimation in the wild by adversarial learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 5255–5264, 2018.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasin et al. [2016]</span>
<span class="ltx_bibblock">
H. Yasin, U. Iqbal, B. Kruger, A. Weber, and J. Gall.

</span>
<span class="ltx_bibblock">A dual-source approach for 3d pose estimation from a single image.

</span>
<span class="ltx_bibblock">In <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 4948–4956, 2016.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2021]</span>
<span class="ltx_bibblock">
Q. Yu, H. Wang, F. Laamarti, and A. El Saddik.

</span>
<span class="ltx_bibblock">Deep learning-enabled multitask system for exercise recognition and
counting.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Multimodal Technologies and Interaction</em>, 5(9):55, 2021.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan and Du [2021]</span>
<span class="ltx_bibblock">
Z. Yuan and S. Du.

</span>
<span class="ltx_bibblock">Jointpose: Jointly optimizing evolutionary data augmentation and
prediction neural network for 3d human pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Artificial Neural Networks and Machine Learning–ICANN 2021:
30th International Conference on Artificial Neural Networks, Bratislava,
Slovakia, September 14–17, 2021, Proceedings, Part III 30</em>, pages 368–379.
Springer, 2021.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2021]</span>
<span class="ltx_bibblock">
F. Zhang, X. Zhu, and C. Wang.

</span>
<span class="ltx_bibblock">Single person pose estimation: a survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.10056</em>, 2021.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2019]</span>
<span class="ltx_bibblock">
L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. N. Metaxas.

</span>
<span class="ltx_bibblock">Semantic graph convolutional networks for 3d human pose regression.

</span>
<span class="ltx_bibblock">In <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 3425–3435, 2019.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2020]</span>
<span class="ltx_bibblock">
Y. Zhao, Z. Luo, C. Quan, D. Liu, and G. Wang.

</span>
<span class="ltx_bibblock">Lite hourglass network for multi-person pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">MultiMedia Modeling: 26th International Conference, MMM
2020, Daejeon, South Korea, January 5–8, 2020, Proceedings, Part II 26</em>,
pages 226–238. Springer, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  9 18:15:13 2023 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
