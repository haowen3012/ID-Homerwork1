<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark</title>
<!--Generated on Thu Jul 18 22:26:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Human Pose Estimation 4D Radar Dataset Benchmark" lang="en" name="keywords"/>
<base href="/html/2407.13930v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S1" title="In RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S2" title="In RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S2.SS1" title="In 2 Related Works ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>3D Human Pose Estimation Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S2.SS2" title="In 2 Related Works ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Radar-based Human Pose Estimation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S3" title="In RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>RT-Pose Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S3.SS1" title="In 3 RT-Pose Dataset ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Sensors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S3.SS2" title="In 3 RT-Pose Dataset ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S3.SS3" title="In 3 RT-Pose Dataset ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data Processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S3.SS4" title="In 3 RT-Pose Dataset ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Annotation Workflow</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S4" title="In RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>HRRadarPose</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5" title="In RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.SS0.SSS0.Px1" title="In 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title">Evaluation metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.SS0.SSS0.Px2" title="In 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title">Baselines Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.SS0.SSS0.Px3" title="In 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title">Action complexity Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.SS0.SSS0.Px4" title="In 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title">Qualitative Result</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.SS0.SSS0.Px5" title="In 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title">Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S6" title="In RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitation and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S7" title="In RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span> National Cheng Kung University </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of Washington

<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>n28081527@gs.ncku.edu.tw<sup class="ltx_sup" id="id2.1.1">1</sup>,
<br class="ltx_break"/>{andyhci, shengyao, zyjiang, wchai, hwhuang, hwang}@uw.edu<sup class="ltx_sup" id="id2.1.2">2</sup>, 
<br class="ltx_break"/>cllin@ee.ncku.edu.tw<sup class="ltx_sup" id="id2.1.3">1</sup>
</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuan-Hao Ho<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0000-0003-1089-5509
</span><span class="ltx_author_notes">1*1*</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jen-Hao Cheng<span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0002-6970-3738
</span><span class="ltx_author_notes">2*2*</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sheng Yao Kuan<span class="ltx_ERROR undefined" id="id4.1.id1">\orcidlink</span>0009-0001-5054-3033
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhongyu Jiang<span class="ltx_ERROR undefined" id="id5.1.id1">\orcidlink</span>0000-0003-4462-6497
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenhao Chai<span class="ltx_ERROR undefined" id="id6.1.id1">\orcidlink</span>0000-0003-2611-0008
Hsiang-Wei Huang<span class="ltx_ERROR undefined" id="id7.2.id2">\orcidlink</span>0009-0009-2474-8869
</span><span class="ltx_author_notes">2222</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chih-Lung Lin
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/>Jenq-Neng Hwang<span class="ltx_ERROR undefined" id="id8.1.id1">\orcidlink</span>0000-0002-8877-2421

<br class="ltx_break"/>* Indicates equal contribution
</span><span class="ltx_author_notes">22</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">Traditional methods for human localization and pose estimation (HPE), which mainly rely on RGB images as an input modality, confront substantial limitations in real-world applications due to privacy concerns. In contrast, radar-based HPE methods emerge as a promising alternative, characterized by distinctive attributes such as through-wall recognition and privacy-preserving, rendering the method more conducive to practical deployments. This paper presents a Radar Tensor-based human pose (RT-Pose) dataset and an open-source benchmarking framework. RT-Pose dataset comprises 4D radar tensors, LiDAR point clouds, and RGB images, and is collected for a total of 72k frames across 240 sequences with six different complexity level actions. The 4D radar tensor provides raw spatio-temporal information, differentiating it from other radar point cloud-based datasets.
We develop an annotation process, which uses RGB images and LiDAR point clouds to accurately label 3D human skeletons.
In addition, we propose HRRadarPose, the first single-stage architecture that extracts the high-resolution representation of 4D radar tensors in 3D space to aid human keypoint estimation.
HRRadarPose outperforms previous radar-based HPE work on the RT-Pose benchmark.
The overall HRRadarPose performance on the RT-Pose dataset, as reflected in a mean per joint position error (MPJPE) of <math alttext="9.91cm" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">9.91</mn><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml">c</mi><mo id="id1.1.m1.1.1.1a" xref="id1.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="id1.1.m1.1.1.4" xref="id1.1.m1.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><times id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></times><cn id="id1.1.m1.1.1.2.cmml" type="float" xref="id1.1.m1.1.1.2">9.91</cn><ci id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3">ùëê</ci><ci id="id1.1.m1.1.1.4.cmml" xref="id1.1.m1.1.1.4">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">9.91cm</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">9.91 italic_c italic_m</annotation></semantics></math>, indicates the persistent challenges in achieving accurate HPE in complex real-world scenarios. RT-Pose is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/uwipl/RT-Pose" title="">https://huggingface.co/datasets/uwipl/RT-Pose</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Human Pose Estimation 4D Radar Dataset Benchmark
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Human localization and 3D pose estimation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib49" title="">49</a>]</cite> are indispensable in Augmented/Virtual Reality¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib5" title="">5</a>]</cite>, human-computer interaction¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib18" title="">18</a>]</cite>, healthcare¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib29" title="">29</a>]</cite>.
However, capturing human poses across diverse scenarios and generating comprehensive 3D representations for individuals in varying poses present significant challenges. Camera based motion capture systems are often used to collect human body posture¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib28" title="">28</a>]</cite>. However, these systems are sensitive to light and require careful multi-camera calibration, making them unreliable for outdoor scenarios and unable to provide overall pose estimation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib7" title="">7</a>]</cite>.
Furthermore, due to privacy concerns, deploying its application in home or long-term care center scenarios poses challenges.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Radar-based method emerges as a promising alternative, characterized by distinctive attributes such as through-wall recognition¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib58" title="">58</a>]</cite>.
In addition, radar is robust to lighting conditions and resilient to various weather conditions and occlusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib47" title="">47</a>]</cite>.
These characteristics make it especially suitable for safety- and privacy-critical applications¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib39" title="">39</a>]</cite>.
In smart automotive applications, radar, as a complementary sensing modality, can complement adverse scenarios such as low-lighting environments and adverse weather¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib25" title="">25</a>]</cite>, challenging RGB-based sensing.
In healthcare applications, RGB-based Human Pose Estimation (HPE)poses privacy risks and is hindered by occlusion, prompting a demand for radar in indoor care environments.
Considering the advantages and prospects of radar technology, it is indispensable to provide a dataset that contains divwerse scenarios for fostering radar-based HPE research.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The previous radar-based HPE methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib14" title="">14</a>]</cite> often utilize Constant False Alarm Rate (CFAR) techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib46" title="">46</a>]</cite> to extract radar point clouds. However, the effectiveness of CFAR-based point cloud extraction can be easily influenced by variations in radar module types and hardware parameters. Furthermore, different human body reflections in radar signals across diverse environments may require specific adjustments to CFAR parameters, making it challenging to generalize their applications.
To address this issue, some works decompose the 4D radar tensor into vertical and horizontal directions as the input of the model to estimate human pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib50" title="">50</a>]</cite>. This approach computes the magnitude of radar signals for both directions, which effectively reduces data loss during the conversion to point cloud. However, a 4D radar tensor with Doppler values contains the velocity information, which is more informative than a decomposed radar tensor¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib10" title="">10</a>]</cite>. Therefore, this work focuses on exploring the use of raw 4D radar tensors for HPE and establishing a benchmark based on this data format.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The presented radar tensor-based human pose (RT-Pose) dataset is the first benchmark to integrate calibrated 4D radar tensors, RGB images, and LiDAR point clouds data in the field of HPE. Releasing a multi-modality dataset marks a significant advancement, particularly in the domain of human behavior analyses. There are
a total of 72k frames in 240 sequences in the dataset. The actions are organized into sequences of increasing complexity, providing a realistic motion distribution. All experiments are conducted in 5 environmental conditions in 8 scenarios, which ensures the variety of scenarios for better comprehension of the model. Along with the release of this comprehensive RT-Pose dataset, we also build upon the High-Resolution Network (HRNet)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib44" title="">44</a>]</cite> to propose HRRadarPose model, a robust baseline for 3D HPE utilizing 4D radar tensor data.
Furthermore, our study also includes a comparative evaluation of different radar data types within our proposed model, as well as benchmarking against previous radar-based methods. These results demonstrate the advantages of employing the 4D radar tensor method over traditional radar point cloud methods, especially in complex actions and diverse scenarios. The 4D radar tensor-based approach presents a challenging but promising direction for future research. In summary, our contributions include:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">The first dataset provides calibrated 4D radar tensors, LiDAR point clouds, and RGB images for human localization and 3D pose estimation of complex actions in diverse scenarios.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We develop a reliable 3D human pose annotation workflow by joint optimization with LiDAR point cloud and RGB images for both outdoor and indoor scenarios.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We propose HRRadarPose, the first single-stage human pose estimator specifically designed to learn 4D radar tensors‚Äô representation for the human pose estimation task.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.18.2.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S2.T1.2.1" style="font-size:90%;">Comparison of 3D human pose estimation datasets. The first group of rows shows RGB and LiDAR datasets. The second group of rows presents datasets with radar. The radar point cloud is denoted as RPC, and the 4D radar tensor is denoted as 4D-RT.
<math alttext="*" class="ltx_Math" display="inline" id="S2.T1.2.1.m1.1"><semantics id="S2.T1.2.1.m1.1b"><mo id="S2.T1.2.1.m1.1.1" xref="S2.T1.2.1.m1.1.1.cmml">‚àó</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.1.m1.1c"><times id="S2.T1.2.1.m1.1.1.cmml" xref="S2.T1.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.1.m1.1d">*</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.1.m1.1e">‚àó</annotation></semantics></math> indicates a modified 4D radar tensor with a specific velocity range.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.16" style="width:433.6pt;height:99.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-99.9pt,22.7pt) scale(0.684587087030117,0.684587087030117) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.16.14">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.16.14.15.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.16.14.15.1.1">Dataset</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.16.14.15.1.2">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.16.14.15.1.3">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.16.14.15.1.4">LiDAR</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.16.14.15.1.5">RPC</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.16.14.15.1.6">4D-RT</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.16.14.15.1.7"># Scenes</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.16.14.15.1.8"># Actions</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.16.14.15.1.9"># Seqs</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.16.14.15.1.10"># FPS</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.16.14.15.1.11">¬†Time (min)</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.3.1.1.2">Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib19" title="">19</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.1.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.3.1.1.1.m1.1"><semantics id="S2.T1.3.1.1.1.m1.1a"><mi id="S2.T1.3.1.1.1.m1.1.1" mathvariant="normal" xref="S2.T1.3.1.1.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.3.1.1.1.m1.1b"><ci id="S2.T1.3.1.1.1.m1.1.1.cmml" xref="S2.T1.3.1.1.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.1.1.1.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td ltx_border_t" id="S2.T1.3.1.1.3"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.3.1.1.4"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.3.1.1.5"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.3.1.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.1.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.1.8">15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.1.9">839</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.1.10">50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.1.11">1200</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.5.3.3.3">HSC4D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib12" title="">12</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.2.2.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.4.2.2.1.m1.1"><semantics id="S2.T1.4.2.2.1.m1.1a"><mi id="S2.T1.4.2.2.1.m1.1.1" mathvariant="normal" xref="S2.T1.4.2.2.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.4.2.2.1.m1.1b"><ci id="S2.T1.4.2.2.1.m1.1.1.cmml" xref="S2.T1.4.2.2.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.2.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.2.2.1.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T1.5.3.3.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.3.3.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.5.3.3.2.m1.1"><semantics id="S2.T1.5.3.3.2.m1.1a"><mi id="S2.T1.5.3.3.2.m1.1.1" mathvariant="normal" xref="S2.T1.5.3.3.2.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.5.3.3.2.m1.1b"><ci id="S2.T1.5.3.3.2.m1.1.1.cmml" xref="S2.T1.5.3.3.2.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.3.3.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.3.3.2.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T1.5.3.3.5"></td>
<td class="ltx_td" id="S2.T1.5.3.3.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.3.3.7">3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.3.3.8">20</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.3.3.9">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.3.3.10">20</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.3.3.11">42</td>
</tr>
<tr class="ltx_tr" id="S2.T1.7.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.7.5.5.3">SLOPER4D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib11" title="">11</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.6.4.4.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.6.4.4.1.m1.1"><semantics id="S2.T1.6.4.4.1.m1.1a"><mi id="S2.T1.6.4.4.1.m1.1.1" mathvariant="normal" xref="S2.T1.6.4.4.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.6.4.4.1.m1.1b"><ci id="S2.T1.6.4.4.1.m1.1.1.cmml" xref="S2.T1.6.4.4.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.4.4.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.6.4.4.1.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T1.7.5.5.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.5.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.7.5.5.2.m1.1"><semantics id="S2.T1.7.5.5.2.m1.1a"><mi id="S2.T1.7.5.5.2.m1.1.1" mathvariant="normal" xref="S2.T1.7.5.5.2.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.7.5.5.2.m1.1b"><ci id="S2.T1.7.5.5.2.m1.1.1.cmml" xref="S2.T1.7.5.5.2.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.5.5.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.7.5.5.2.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T1.7.5.5.5"></td>
<td class="ltx_td" id="S2.T1.7.5.5.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.5.7">10</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.5.8">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.5.9">15</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.5.10">20</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.5.11">83</td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.8.6.6.2">MARS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib4" title="">4</a>]</cite>
</th>
<td class="ltx_td ltx_border_t" id="S2.T1.8.6.6.3"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.8.6.6.4"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.8.6.6.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.8.6.6.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.8.6.6.1.m1.1"><semantics id="S2.T1.8.6.6.1.m1.1a"><mi id="S2.T1.8.6.6.1.m1.1.1" mathvariant="normal" xref="S2.T1.8.6.6.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.8.6.6.1.m1.1b"><ci id="S2.T1.8.6.6.1.m1.1.1.cmml" xref="S2.T1.8.6.6.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.6.6.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.8.6.6.1.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td ltx_border_t" id="S2.T1.8.6.6.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.8.6.6.7">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.8.6.6.8">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.8.6.6.9">80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.8.6.6.10">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.8.6.6.11">80</td>
</tr>
<tr class="ltx_tr" id="S2.T1.11.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.11.9.9.4">mRI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib3" title="">3</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.9.7.7.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.9.7.7.1.m1.1"><semantics id="S2.T1.9.7.7.1.m1.1a"><mi id="S2.T1.9.7.7.1.m1.1.1" mathvariant="normal" xref="S2.T1.9.7.7.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.9.7.7.1.m1.1b"><ci id="S2.T1.9.7.7.1.m1.1.1.cmml" xref="S2.T1.9.7.7.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.7.7.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.9.7.7.1.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S2.T1.10.8.8.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.10.8.8.2.m1.1"><semantics id="S2.T1.10.8.8.2.m1.1a"><mi id="S2.T1.10.8.8.2.m1.1.1" mathvariant="normal" xref="S2.T1.10.8.8.2.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.10.8.8.2.m1.1b"><ci id="S2.T1.10.8.8.2.m1.1.1.cmml" xref="S2.T1.10.8.8.2.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.8.8.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.10.8.8.2.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T1.11.9.9.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.11.9.9.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.11.9.9.3.m1.1"><semantics id="S2.T1.11.9.9.3.m1.1a"><mi id="S2.T1.11.9.9.3.m1.1.1" mathvariant="normal" xref="S2.T1.11.9.9.3.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.11.9.9.3.m1.1b"><ci id="S2.T1.11.9.9.3.m1.1.1.cmml" xref="S2.T1.11.9.9.3.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.9.9.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.11.9.9.3.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T1.11.9.9.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.11.9.9.7">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.11.9.9.8">12</td>
<td class="ltx_td ltx_align_center" id="S2.T1.11.9.9.9">300</td>
<td class="ltx_td ltx_align_center" id="S2.T1.11.9.9.10">10</td>
<td class="ltx_td ltx_align_center" id="S2.T1.11.9.9.11">264</td>
</tr>
<tr class="ltx_tr" id="S2.T1.13.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.13.11.11.3">HuPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib26" title="">26</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.12.10.10.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.12.10.10.1.m1.1"><semantics id="S2.T1.12.10.10.1.m1.1a"><mi id="S2.T1.12.10.10.1.m1.1.1" mathvariant="normal" xref="S2.T1.12.10.10.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.12.10.10.1.m1.1b"><ci id="S2.T1.12.10.10.1.m1.1.1.cmml" xref="S2.T1.12.10.10.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.10.10.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.12.10.10.1.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T1.13.11.11.4"></td>
<td class="ltx_td" id="S2.T1.13.11.11.5"></td>
<td class="ltx_td" id="S2.T1.13.11.11.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.13.11.11.2"><math alttext="\checkmark*" class="ltx_math_unparsed" display="inline" id="S2.T1.13.11.11.2.m1.1"><semantics id="S2.T1.13.11.11.2.m1.1a"><mrow id="S2.T1.13.11.11.2.m1.1b"><mi id="S2.T1.13.11.11.2.m1.1.1" mathvariant="normal">‚úì</mi><mo id="S2.T1.13.11.11.2.m1.1.2" lspace="0.222em">‚àó</mo></mrow><annotation encoding="application/x-tex" id="S2.T1.13.11.11.2.m1.1c">\checkmark*</annotation><annotation encoding="application/x-llamapun" id="S2.T1.13.11.11.2.m1.1d">‚úì ‚àó</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S2.T1.13.11.11.7">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.13.11.11.8">3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.13.11.11.9">235</td>
<td class="ltx_td ltx_align_center" id="S2.T1.13.11.11.10">10</td>
<td class="ltx_td ltx_align_center" id="S2.T1.13.11.11.11">240</td>
</tr>
<tr class="ltx_tr" id="S2.T1.16.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.16.14.14.4">RT-Pose¬†(Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.14.12.12.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.14.12.12.1.m1.1"><semantics id="S2.T1.14.12.12.1.m1.1a"><mi id="S2.T1.14.12.12.1.m1.1.1" mathvariant="normal" xref="S2.T1.14.12.12.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.14.12.12.1.m1.1b"><ci id="S2.T1.14.12.12.1.m1.1.1.cmml" xref="S2.T1.14.12.12.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.12.12.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.14.12.12.1.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S2.T1.16.14.14.5"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.15.13.13.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.15.13.13.2.m1.1"><semantics id="S2.T1.15.13.13.2.m1.1a"><mi id="S2.T1.15.13.13.2.m1.1.1" mathvariant="normal" xref="S2.T1.15.13.13.2.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.15.13.13.2.m1.1b"><ci id="S2.T1.15.13.13.2.m1.1.1.cmml" xref="S2.T1.15.13.13.2.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.15.13.13.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.15.13.13.2.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S2.T1.16.14.14.6"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.16.14.14.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S2.T1.16.14.14.3.m1.1"><semantics id="S2.T1.16.14.14.3.m1.1a"><mi id="S2.T1.16.14.14.3.m1.1.1" mathvariant="normal" xref="S2.T1.16.14.14.3.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S2.T1.16.14.14.3.m1.1b"><ci id="S2.T1.16.14.14.3.m1.1.1.cmml" xref="S2.T1.16.14.14.3.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.16.14.14.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S2.T1.16.14.14.3.m1.1d">‚úì</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.16.14.14.7">40</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.16.14.14.8">6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.16.14.14.9">240</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.16.14.14.10">10</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.16.14.14.11">120</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>3D Human Pose Estimation Datasets</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">As one of the fundamental tasks in the computer vision field, there are lots of works introducing datasets and benchmarks for 3D HPE, as shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S2.T1" title="Table 1 ‚Ä£ 2 Related Works ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">1</span></a>. Human3.6M¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib19" title="">19</a>]</cite> is the first large-scale dataset for 3D human sensing in lab environments, which contains 3.6-million frames of corresponding 2D and 3D human poses from mocap captured videos of 5 female and 6 male subjects. Compared to Human3.6M, MPI-INF-3DHP¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib32" title="">32</a>]</cite> is a more challenging 3D human pose dataset captured in the wild. There are 8 subjects with 8 actions captured by 14 cameras covering a greater diversity of poses. 3DPW¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib41" title="">41</a>]</cite>, the first one to include video footage taken from a moving phone camera, includes 60 video sequences. Recently, several works have explored the possibility of HPE from other modalities like radar and LiDAR. mmBody¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib7" title="">7</a>]</cite> consists of synchronized and calibrated mmWave radar point clouds and RGB-D images in different scenes, as well as skeleton/mesh annotations for humans in the scenes.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">mRI¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib3" title="">3</a>]</cite>, a multi-modal 3D HPE dataset using mmWave, RGB-D, and inertial sensors, generates a radar point cloud dataset based on the CFAR method. There are 12 rehabilitation exercises selected to evaluate radar application in home-based health monitoring.
HuPR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib26" title="">26</a>]</cite> collects 235 sequences of data by calibrated radar and camera for 2D HPE in an indoor environment.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">All open datasets for radar-based human pose estimation are collected within a limited detection area, which contradicts the characteristics of radar applications for tracking. Although detecting within a small area may ensure system accuracy, it limits the system‚Äôs practical application.
Furthermore, to the best of our knowledge, our dataset is the first to include radar, LiDAR, and RGB sequence information with 3D pose annotations. This allows our dataset to provide pose annotations for both indoor and outdoor environments, distinguishing it from existing datasets.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Radar-based Human Pose Estimation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In spite of the popularity of RGB-based HPE applications, privacy concerns have driven the exploration of radio or radar modalities for HPE. RF-Pose¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib54" title="">54</a>]</cite> is a pioneering application in 2D HPE utilizing radio frequency signals. RF-Pose demonstrates the feasibility of pose detection in through-wall scenarios and provide the possibility of predicting the 3D pose using radio signals.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">On the other hand, frequency-modulated continuous wave (FMCW) radar, which operates in the millimeter-wave band (30-300GHz), has been shown to be suitable for detection and HPE tasks.
The radar point cloud is widely used to represent the radar signal in HPE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib37" title="">37</a>]</cite>. mmMesh¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib48" title="">48</a>]</cite> introduces a temporal model structure that concurrently captures global and local 3D point cloud structures in spatial dimensions, ensuring precise localization performance for pose estimation. Nonetheless, in cluttered environments, the probability and intensity of radar signal reflections from the human body decrease. Consequently, implementing point cloud-based methods in real-world applications becomes challenging.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Compared to the radar point cloud methods,
utilizing 4D tensor radar signal proves to be more informative and reliable
¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib56" title="">56</a>]</cite>. RF-pose 3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib55" title="">55</a>]</cite> is the first work using the 4D RF tensor to predict 3D skeletons in 22 different locations. However, the used RF system is not commercially available, which is hard to reproduce. HuPR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib26" title="">26</a>]</cite> utilizes two pieces of well-calibrated TI mmWave radar modules to capture vertical and horizontal radar signals, enhancing the resolution of radar elevation signals to improve pose estimation performance. RPM2.0¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib47" title="">47</a>]</cite> adopts a similar hardware setup to HuPR but utilizes the HRNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib44" title="">44</a>]</cite> as the backbone and includes an attention model to reconstruct missing keypoints. However, their two-stage model is trained by only walking pose, restricting the actions‚Äô complexity for achieving better pose estimation performance.
The methods proposed in HuPR and RPM2.0 fuse information from both radar modules.
While the dual-radar-module configuration potentially captures fine-grained information, its intricate setup poses reproducibility challenges.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Unlike previous works, our system employs a single radar module for vertical and horizontal sensing, simplifying the sensors‚Äô calibration and synchronization. We use 4D radar tensor as training data for 3D HPE, reducing the likelihood of data loss during radar point generation. Moreover, RT-Pose dataset includes complex actions and is recorded in diverse scenes, expanding the applicability to real-world scenarios.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>RT-Pose Dataset</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="567" id="S3.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">Experimental hardware setup in an indoor environment for data collection.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To construct the dataset, we recruit 10 participants to perform 6 types of actions. The experiments are divided into two categories. In the first category, participants are required to perform actions while standing, including waving, lifting legs, and random poses to increase the complexity of the actions. A video sequence of random poses involves a subject randomly performing one possible movement at a time, including stretching, bending, twisting, etc., each lasting 3 to 5 seconds. The second category focuses on walking, with additional actions added during the process, such as walking and waving or sitting down after walking. Each type of action is inherited from previous radar-based datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib4" title="">4</a>]</cite>. Each sequence contains one action for 30 seconds, providing a variety of different difficulty levels.
As a result, people can thoroughly evaluate their 3D HPE methods on a broad distribution of actions with diverse difficulties.
Furthermore, we provide the dataset development kit to facilitate the usage and evaluation of future methods.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="177" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Data distribution for RT-Pose dataset: (a) Activities; (b) Environmental conditions; (c) Occlusion conditions</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Sensors</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The data collection hardware system comprises two RGB cameras, a non-repetitive horizontal scanning LiDAR, and a cascade imaging radar module, as shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S3.F1" title="Figure 1 ‚Ä£ 3 RT-Pose Dataset ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">1</span></a>.
To extract human pose information from radar signals, the radar module‚Äôs parameters have been specifically configured.
The radar operates at ten frames per second in this work. The radar module is equipped with 12 transmit (TX) and 16 receive (RX) antenna elements and operates in a multiple-input multiple-output (MIMO) mode¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib23" title="">23</a>]</cite>.
This radar module provides the azimuth and elevation angle resolutions of 1.4 degrees and 18 degrees, respectively. Furthermore, this work implements a high-frequency slope with a long ramp time to efficiently utilize the hardware‚Äôs sweep bandwidth, ranging from 77 GHz to almost 81 GHz. The longer sweep bandwidth of a chirp increases the radar range resolution¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib37" title="">37</a>]</cite>, which is 4.5 cm in this work. Each frame consists of 64 chirps used for calculating radar Doppler signals, providing a velocity resolution of 3.9 cm/s. These carefully chosen configurations ensure the system‚Äôs accuracy and reliability in capturing human poses.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Collection</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We collect the dataset in 40 scenes with indoor and outdoor environments.
Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S3.F3" title="Figure 3 ‚Ä£ 3.2 Data Collection ‚Ä£ 3 RT-Pose Dataset ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">3</span></a> shows some of the collected data.
Indoor environments include clean and cluttered conditions, while outdoor ones include normal, rainy weather, and low-lighting conditions. Each experimental condition includes 8 scenarios, which are nonocclusive, and three occlusive conditions (cardboard, cloth, and plastic pad).
Each nonocclusive and occlusive condition is recorded in single- and multi-person settings.
To consider the different fields of view (FoV) of all sensors, the sensor suite is situated 109 cm above ground, and an appropriate detection area is planned based on different scenarios.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="364" id="S3.F3.g1" src="x3.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Experimental instances across various indoor and outdoor conditions with diverse scenarios for data collection.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data Processing</h3>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="S3.F4.g1" src="x4.png" width="622"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Radar signal processing flow. The green arrow line is for radar point cloud generation and the blue line is for 4D radar tensor generation. </span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.4">The captured radar signal, referred to as 4D tensor data, is processed by a series of steps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib45" title="">45</a>]</cite>, as shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S3.F4" title="Figure 4 ‚Ä£ 3.3 Data Processing ‚Ä£ 3 RT-Pose Dataset ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">4</span></a>.
On the transmitter side, the frequency of the transmitted signals, or chirps, periodically increases based on the set frequency slope.
Consequently, the received signal exhibits a different frequency than the transmitted signal.
The frequency difference between transmitted and received signals can be used to estimate the range of the object reflecting the signal to the radar sensor by the Fast Fourier Transform (FFT).
At the beginning of the data processing, each radar frame consists of 64 chirps, resulting in 64 range estimation results. Utilizing this range information, the velocity of the responding object can be measured through FFT, a phenomenon known as the Doppler effect.
Second, the radar signal data is re-modulated according to the antenna position. This step is essential for the calculation of the angle of arrival (AoA), enabling the generation of highangular resolution results.
Third, the radar signal from the azimuth and elevation directions also undergoes FFT processing to represent velocity responses in different directions. Finally, the entire radar data is transformed from polar coordinates to Cartesian coordinates. This transformation enhances the intuitiveness for the subsequent pose estimation. In this study, the processed 4D radar tensor has the dimension of <math alttext="64\times 32\times 128\times 256" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">64</mn><mo id="S3.SS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">32</mn><mo id="S3.SS3.p1.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS3.p1.1.m1.1.1.4" xref="S3.SS3.p1.1.m1.1.1.4.cmml">128</mn><mo id="S3.SS3.p1.1.m1.1.1.1b" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS3.p1.1.m1.1.1.5" xref="S3.SS3.p1.1.m1.1.1.5.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><cn id="S3.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.2">64</cn><cn id="S3.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.3">32</cn><cn id="S3.SS3.p1.1.m1.1.1.4.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.4">128</cn><cn id="S3.SS3.p1.1.m1.1.1.5.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.5">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">64\times 32\times 128\times 256</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">64 √ó 32 √ó 128 √ó 256</annotation></semantics></math>, which correspond to velocity, the <math alttext="z" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ùëß</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_z</annotation></semantics></math>-axis, the <math alttext="y" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">ùë¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">italic_y</annotation></semantics></math>-axis, and the <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_x</annotation></semantics></math>-axis, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Annotation Workflow</h3>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S3.F5.g1" src="x5.png" width="623"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.10.5.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.8.4" style="font-size:90%;">Workflow of human localization and 3D pose ground truth annotations. Estimated 2D pose results, predicted by the pre-trained HRNet model, are denoted as <math alttext="P_{2d}" class="ltx_Math" display="inline" id="S3.F5.5.1.m1.1"><semantics id="S3.F5.5.1.m1.1b"><msub id="S3.F5.5.1.m1.1.1" xref="S3.F5.5.1.m1.1.1.cmml"><mi id="S3.F5.5.1.m1.1.1.2" xref="S3.F5.5.1.m1.1.1.2.cmml">P</mi><mrow id="S3.F5.5.1.m1.1.1.3" xref="S3.F5.5.1.m1.1.1.3.cmml"><mn id="S3.F5.5.1.m1.1.1.3.2" xref="S3.F5.5.1.m1.1.1.3.2.cmml">2</mn><mo id="S3.F5.5.1.m1.1.1.3.1" xref="S3.F5.5.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.F5.5.1.m1.1.1.3.3" xref="S3.F5.5.1.m1.1.1.3.3.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F5.5.1.m1.1c"><apply id="S3.F5.5.1.m1.1.1.cmml" xref="S3.F5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F5.5.1.m1.1.1.1.cmml" xref="S3.F5.5.1.m1.1.1">subscript</csymbol><ci id="S3.F5.5.1.m1.1.1.2.cmml" xref="S3.F5.5.1.m1.1.1.2">ùëÉ</ci><apply id="S3.F5.5.1.m1.1.1.3.cmml" xref="S3.F5.5.1.m1.1.1.3"><times id="S3.F5.5.1.m1.1.1.3.1.cmml" xref="S3.F5.5.1.m1.1.1.3.1"></times><cn id="S3.F5.5.1.m1.1.1.3.2.cmml" type="integer" xref="S3.F5.5.1.m1.1.1.3.2">2</cn><ci id="S3.F5.5.1.m1.1.1.3.3.cmml" xref="S3.F5.5.1.m1.1.1.3.3">ùëë</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.5.1.m1.1d">P_{2d}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.5.1.m1.1e">italic_P start_POSTSUBSCRIPT 2 italic_d end_POSTSUBSCRIPT</annotation></semantics></math>. The initial setting pose derived from LiDAR point clouds is denoted as <math alttext="P_{init}" class="ltx_Math" display="inline" id="S3.F5.6.2.m2.1"><semantics id="S3.F5.6.2.m2.1b"><msub id="S3.F5.6.2.m2.1.1" xref="S3.F5.6.2.m2.1.1.cmml"><mi id="S3.F5.6.2.m2.1.1.2" xref="S3.F5.6.2.m2.1.1.2.cmml">P</mi><mrow id="S3.F5.6.2.m2.1.1.3" xref="S3.F5.6.2.m2.1.1.3.cmml"><mi id="S3.F5.6.2.m2.1.1.3.2" xref="S3.F5.6.2.m2.1.1.3.2.cmml">i</mi><mo id="S3.F5.6.2.m2.1.1.3.1" xref="S3.F5.6.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.F5.6.2.m2.1.1.3.3" xref="S3.F5.6.2.m2.1.1.3.3.cmml">n</mi><mo id="S3.F5.6.2.m2.1.1.3.1b" xref="S3.F5.6.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.F5.6.2.m2.1.1.3.4" xref="S3.F5.6.2.m2.1.1.3.4.cmml">i</mi><mo id="S3.F5.6.2.m2.1.1.3.1c" xref="S3.F5.6.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.F5.6.2.m2.1.1.3.5" xref="S3.F5.6.2.m2.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F5.6.2.m2.1c"><apply id="S3.F5.6.2.m2.1.1.cmml" xref="S3.F5.6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F5.6.2.m2.1.1.1.cmml" xref="S3.F5.6.2.m2.1.1">subscript</csymbol><ci id="S3.F5.6.2.m2.1.1.2.cmml" xref="S3.F5.6.2.m2.1.1.2">ùëÉ</ci><apply id="S3.F5.6.2.m2.1.1.3.cmml" xref="S3.F5.6.2.m2.1.1.3"><times id="S3.F5.6.2.m2.1.1.3.1.cmml" xref="S3.F5.6.2.m2.1.1.3.1"></times><ci id="S3.F5.6.2.m2.1.1.3.2.cmml" xref="S3.F5.6.2.m2.1.1.3.2">ùëñ</ci><ci id="S3.F5.6.2.m2.1.1.3.3.cmml" xref="S3.F5.6.2.m2.1.1.3.3">ùëõ</ci><ci id="S3.F5.6.2.m2.1.1.3.4.cmml" xref="S3.F5.6.2.m2.1.1.3.4">ùëñ</ci><ci id="S3.F5.6.2.m2.1.1.3.5.cmml" xref="S3.F5.6.2.m2.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.6.2.m2.1d">P_{init}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.6.2.m2.1e">italic_P start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. Both <math alttext="P_{2d}" class="ltx_Math" display="inline" id="S3.F5.7.3.m3.1"><semantics id="S3.F5.7.3.m3.1b"><msub id="S3.F5.7.3.m3.1.1" xref="S3.F5.7.3.m3.1.1.cmml"><mi id="S3.F5.7.3.m3.1.1.2" xref="S3.F5.7.3.m3.1.1.2.cmml">P</mi><mrow id="S3.F5.7.3.m3.1.1.3" xref="S3.F5.7.3.m3.1.1.3.cmml"><mn id="S3.F5.7.3.m3.1.1.3.2" xref="S3.F5.7.3.m3.1.1.3.2.cmml">2</mn><mo id="S3.F5.7.3.m3.1.1.3.1" xref="S3.F5.7.3.m3.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.F5.7.3.m3.1.1.3.3" xref="S3.F5.7.3.m3.1.1.3.3.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F5.7.3.m3.1c"><apply id="S3.F5.7.3.m3.1.1.cmml" xref="S3.F5.7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F5.7.3.m3.1.1.1.cmml" xref="S3.F5.7.3.m3.1.1">subscript</csymbol><ci id="S3.F5.7.3.m3.1.1.2.cmml" xref="S3.F5.7.3.m3.1.1.2">ùëÉ</ci><apply id="S3.F5.7.3.m3.1.1.3.cmml" xref="S3.F5.7.3.m3.1.1.3"><times id="S3.F5.7.3.m3.1.1.3.1.cmml" xref="S3.F5.7.3.m3.1.1.3.1"></times><cn id="S3.F5.7.3.m3.1.1.3.2.cmml" type="integer" xref="S3.F5.7.3.m3.1.1.3.2">2</cn><ci id="S3.F5.7.3.m3.1.1.3.3.cmml" xref="S3.F5.7.3.m3.1.1.3.3">ùëë</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.7.3.m3.1d">P_{2d}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.7.3.m3.1e">italic_P start_POSTSUBSCRIPT 2 italic_d end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="P_{init}" class="ltx_Math" display="inline" id="S3.F5.8.4.m4.1"><semantics id="S3.F5.8.4.m4.1b"><msub id="S3.F5.8.4.m4.1.1" xref="S3.F5.8.4.m4.1.1.cmml"><mi id="S3.F5.8.4.m4.1.1.2" xref="S3.F5.8.4.m4.1.1.2.cmml">P</mi><mrow id="S3.F5.8.4.m4.1.1.3" xref="S3.F5.8.4.m4.1.1.3.cmml"><mi id="S3.F5.8.4.m4.1.1.3.2" xref="S3.F5.8.4.m4.1.1.3.2.cmml">i</mi><mo id="S3.F5.8.4.m4.1.1.3.1" xref="S3.F5.8.4.m4.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.F5.8.4.m4.1.1.3.3" xref="S3.F5.8.4.m4.1.1.3.3.cmml">n</mi><mo id="S3.F5.8.4.m4.1.1.3.1b" xref="S3.F5.8.4.m4.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.F5.8.4.m4.1.1.3.4" xref="S3.F5.8.4.m4.1.1.3.4.cmml">i</mi><mo id="S3.F5.8.4.m4.1.1.3.1c" xref="S3.F5.8.4.m4.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.F5.8.4.m4.1.1.3.5" xref="S3.F5.8.4.m4.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F5.8.4.m4.1c"><apply id="S3.F5.8.4.m4.1.1.cmml" xref="S3.F5.8.4.m4.1.1"><csymbol cd="ambiguous" id="S3.F5.8.4.m4.1.1.1.cmml" xref="S3.F5.8.4.m4.1.1">subscript</csymbol><ci id="S3.F5.8.4.m4.1.1.2.cmml" xref="S3.F5.8.4.m4.1.1.2">ùëÉ</ci><apply id="S3.F5.8.4.m4.1.1.3.cmml" xref="S3.F5.8.4.m4.1.1.3"><times id="S3.F5.8.4.m4.1.1.3.1.cmml" xref="S3.F5.8.4.m4.1.1.3.1"></times><ci id="S3.F5.8.4.m4.1.1.3.2.cmml" xref="S3.F5.8.4.m4.1.1.3.2">ùëñ</ci><ci id="S3.F5.8.4.m4.1.1.3.3.cmml" xref="S3.F5.8.4.m4.1.1.3.3">ùëõ</ci><ci id="S3.F5.8.4.m4.1.1.3.4.cmml" xref="S3.F5.8.4.m4.1.1.3.4">ùëñ</ci><ci id="S3.F5.8.4.m4.1.1.3.5.cmml" xref="S3.F5.8.4.m4.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.8.4.m4.1d">P_{init}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.8.4.m4.1e">italic_P start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math> are inputs into ZeDO, an optimization-based pipeline for 3D pose estimation.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">To achieve accurate human subject detection in a larger area with high-quality ground truth, our system integrates LiDAR and RGB camera data, as illustrated in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S3.F5" title="Figure 5 ‚Ä£ 3.4 Annotation Workflow ‚Ä£ 3 RT-Pose Dataset ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">5</span></a>. The images provided by the camera are processed using the HRNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib38" title="">38</a>]</cite> to extract 2D human poses. Then, a 3D HPE model, called ZeDO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib22" title="">22</a>]</cite>, utilizes the 2D poses to estimate the 3D poses as the initial 3D pseudo ground truth. However, estimating 3D poses using a monocular camera often lacks precision and stability in depth. Therefore, this work incorporates LiDAR for depth estimation of the human subject center. The LiDAR sensor captures environmental information and the subject‚Äôs point cloud data, ensuring errors are under 2 cm inside the detection range of 20 m. The human subject is manually labeled with a 3D bounding box, which can provide an accurate 3D position.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">ZeDO, a state-of-the-art (SOTA) method for cross-dataset HPE, implements an optimization-based pipeline for 3D pose estimation using 2D poses. This method requires 2D poses as input, and ZeDO is able to estimate multiple-hypothesis 3D poses. In this research, the center of the 3D bounding box provided by LiDAR is utilized as the pelvis of the initial pose, which enhances the accuracy of depth estimation compared to relying solely on the monocular camera data, and improves the performance of ZeDO. With the imported 2D pose from HRNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib43" title="">43</a>]</cite>, ZeDO iteratively refines the 3D poses. To further improve annotation accuracy, an annotation tool is developed for manual selection and optimization of each frame to retain correct 3D human poses. The 3D HPE results from ZeDO will be put in the LiDAR point cloud for visualization, which is beneficial for humans to filter and modify the error pose annotation results. About 30% of the data is removed or optimized after human correction, and this process ensures the quality of keypoints annotation. More details can be found in the supplementary material.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>HRRadarPose</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.21">Inspired by HRNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib43" title="">43</a>]</cite>, renowned for its adeptness in extracting high-resolution representations, we build an architecture, HRRadarPose, with fully 3D convolutional layers.
By treating the Doppler axis, <math alttext="D" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_D</annotation></semantics></math>, as the input channels, our model extracts volumetric spatial features along the <math alttext="Z" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">ùëç</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">italic_Z</annotation></semantics></math>, <math alttext="Y" class="ltx_Math" display="inline" id="S4.p1.3.m3.1"><semantics id="S4.p1.3.m3.1a"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.1d">italic_Y</annotation></semantics></math>, and <math alttext="X" class="ltx_Math" display="inline" id="S4.p1.4.m4.1"><semantics id="S4.p1.4.m4.1a"><mi id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><ci id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">ùëã</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.p1.4.m4.1d">italic_X</annotation></semantics></math> axes, representing vertical, horizontal, and depth directions in the Cartesian coordinate system.
The HRRadarPose architecture is designed to preserve spatial resolution and assimilate semantic information from the 4D radar tensors, maintaining high-resolution representations and enabling the exchange of information between features of different resolutions during stage transitions.
Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S4.F6" title="Figure 6 ‚Ä£ 4 HRRadarPose ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates an instance of the HRRadarPose structure with three stages.
The structure starts with using 3D convolutional layers to transform the 4D radar tensors, <math alttext="T\in\mathbb{R}^{D\times Z\times Y\times X}" class="ltx_Math" display="inline" id="S4.p1.5.m5.1"><semantics id="S4.p1.5.m5.1a"><mrow id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml"><mi id="S4.p1.5.m5.1.1.2" xref="S4.p1.5.m5.1.1.2.cmml">T</mi><mo id="S4.p1.5.m5.1.1.1" xref="S4.p1.5.m5.1.1.1.cmml">‚àà</mo><msup id="S4.p1.5.m5.1.1.3" xref="S4.p1.5.m5.1.1.3.cmml"><mi id="S4.p1.5.m5.1.1.3.2" xref="S4.p1.5.m5.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.p1.5.m5.1.1.3.3" xref="S4.p1.5.m5.1.1.3.3.cmml"><mi id="S4.p1.5.m5.1.1.3.3.2" xref="S4.p1.5.m5.1.1.3.3.2.cmml">D</mi><mo id="S4.p1.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.p1.5.m5.1.1.3.3.1.cmml">√ó</mo><mi id="S4.p1.5.m5.1.1.3.3.3" xref="S4.p1.5.m5.1.1.3.3.3.cmml">Z</mi><mo id="S4.p1.5.m5.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S4.p1.5.m5.1.1.3.3.1.cmml">√ó</mo><mi id="S4.p1.5.m5.1.1.3.3.4" xref="S4.p1.5.m5.1.1.3.3.4.cmml">Y</mi><mo id="S4.p1.5.m5.1.1.3.3.1b" lspace="0.222em" rspace="0.222em" xref="S4.p1.5.m5.1.1.3.3.1.cmml">√ó</mo><mi id="S4.p1.5.m5.1.1.3.3.5" xref="S4.p1.5.m5.1.1.3.3.5.cmml">X</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><apply id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1"><in id="S4.p1.5.m5.1.1.1.cmml" xref="S4.p1.5.m5.1.1.1"></in><ci id="S4.p1.5.m5.1.1.2.cmml" xref="S4.p1.5.m5.1.1.2">ùëá</ci><apply id="S4.p1.5.m5.1.1.3.cmml" xref="S4.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.p1.5.m5.1.1.3.1.cmml" xref="S4.p1.5.m5.1.1.3">superscript</csymbol><ci id="S4.p1.5.m5.1.1.3.2.cmml" xref="S4.p1.5.m5.1.1.3.2">‚Ñù</ci><apply id="S4.p1.5.m5.1.1.3.3.cmml" xref="S4.p1.5.m5.1.1.3.3"><times id="S4.p1.5.m5.1.1.3.3.1.cmml" xref="S4.p1.5.m5.1.1.3.3.1"></times><ci id="S4.p1.5.m5.1.1.3.3.2.cmml" xref="S4.p1.5.m5.1.1.3.3.2">ùê∑</ci><ci id="S4.p1.5.m5.1.1.3.3.3.cmml" xref="S4.p1.5.m5.1.1.3.3.3">ùëç</ci><ci id="S4.p1.5.m5.1.1.3.3.4.cmml" xref="S4.p1.5.m5.1.1.3.3.4">ùëå</ci><ci id="S4.p1.5.m5.1.1.3.3.5.cmml" xref="S4.p1.5.m5.1.1.3.3.5">ùëã</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">T\in\mathbb{R}^{D\times Z\times Y\times X}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.5.m5.1d">italic_T ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_Z √ó italic_Y √ó italic_X end_POSTSUPERSCRIPT</annotation></semantics></math>, into features, <math alttext="f_{in}\in\mathbb{R}^{C_{1}\times Z_{1}\times Y_{1}\times X_{1}}" class="ltx_Math" display="inline" id="S4.p1.6.m6.1"><semantics id="S4.p1.6.m6.1a"><mrow id="S4.p1.6.m6.1.1" xref="S4.p1.6.m6.1.1.cmml"><msub id="S4.p1.6.m6.1.1.2" xref="S4.p1.6.m6.1.1.2.cmml"><mi id="S4.p1.6.m6.1.1.2.2" xref="S4.p1.6.m6.1.1.2.2.cmml">f</mi><mrow id="S4.p1.6.m6.1.1.2.3" xref="S4.p1.6.m6.1.1.2.3.cmml"><mi id="S4.p1.6.m6.1.1.2.3.2" xref="S4.p1.6.m6.1.1.2.3.2.cmml">i</mi><mo id="S4.p1.6.m6.1.1.2.3.1" xref="S4.p1.6.m6.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.p1.6.m6.1.1.2.3.3" xref="S4.p1.6.m6.1.1.2.3.3.cmml">n</mi></mrow></msub><mo id="S4.p1.6.m6.1.1.1" xref="S4.p1.6.m6.1.1.1.cmml">‚àà</mo><msup id="S4.p1.6.m6.1.1.3" xref="S4.p1.6.m6.1.1.3.cmml"><mi id="S4.p1.6.m6.1.1.3.2" xref="S4.p1.6.m6.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.p1.6.m6.1.1.3.3" xref="S4.p1.6.m6.1.1.3.3.cmml"><msub id="S4.p1.6.m6.1.1.3.3.2" xref="S4.p1.6.m6.1.1.3.3.2.cmml"><mi id="S4.p1.6.m6.1.1.3.3.2.2" xref="S4.p1.6.m6.1.1.3.3.2.2.cmml">C</mi><mn id="S4.p1.6.m6.1.1.3.3.2.3" xref="S4.p1.6.m6.1.1.3.3.2.3.cmml">1</mn></msub><mo id="S4.p1.6.m6.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.p1.6.m6.1.1.3.3.1.cmml">√ó</mo><msub id="S4.p1.6.m6.1.1.3.3.3" xref="S4.p1.6.m6.1.1.3.3.3.cmml"><mi id="S4.p1.6.m6.1.1.3.3.3.2" xref="S4.p1.6.m6.1.1.3.3.3.2.cmml">Z</mi><mn id="S4.p1.6.m6.1.1.3.3.3.3" xref="S4.p1.6.m6.1.1.3.3.3.3.cmml">1</mn></msub><mo id="S4.p1.6.m6.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S4.p1.6.m6.1.1.3.3.1.cmml">√ó</mo><msub id="S4.p1.6.m6.1.1.3.3.4" xref="S4.p1.6.m6.1.1.3.3.4.cmml"><mi id="S4.p1.6.m6.1.1.3.3.4.2" xref="S4.p1.6.m6.1.1.3.3.4.2.cmml">Y</mi><mn id="S4.p1.6.m6.1.1.3.3.4.3" xref="S4.p1.6.m6.1.1.3.3.4.3.cmml">1</mn></msub><mo id="S4.p1.6.m6.1.1.3.3.1b" lspace="0.222em" rspace="0.222em" xref="S4.p1.6.m6.1.1.3.3.1.cmml">√ó</mo><msub id="S4.p1.6.m6.1.1.3.3.5" xref="S4.p1.6.m6.1.1.3.3.5.cmml"><mi id="S4.p1.6.m6.1.1.3.3.5.2" xref="S4.p1.6.m6.1.1.3.3.5.2.cmml">X</mi><mn id="S4.p1.6.m6.1.1.3.3.5.3" xref="S4.p1.6.m6.1.1.3.3.5.3.cmml">1</mn></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.1b"><apply id="S4.p1.6.m6.1.1.cmml" xref="S4.p1.6.m6.1.1"><in id="S4.p1.6.m6.1.1.1.cmml" xref="S4.p1.6.m6.1.1.1"></in><apply id="S4.p1.6.m6.1.1.2.cmml" xref="S4.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S4.p1.6.m6.1.1.2.1.cmml" xref="S4.p1.6.m6.1.1.2">subscript</csymbol><ci id="S4.p1.6.m6.1.1.2.2.cmml" xref="S4.p1.6.m6.1.1.2.2">ùëì</ci><apply id="S4.p1.6.m6.1.1.2.3.cmml" xref="S4.p1.6.m6.1.1.2.3"><times id="S4.p1.6.m6.1.1.2.3.1.cmml" xref="S4.p1.6.m6.1.1.2.3.1"></times><ci id="S4.p1.6.m6.1.1.2.3.2.cmml" xref="S4.p1.6.m6.1.1.2.3.2">ùëñ</ci><ci id="S4.p1.6.m6.1.1.2.3.3.cmml" xref="S4.p1.6.m6.1.1.2.3.3">ùëõ</ci></apply></apply><apply id="S4.p1.6.m6.1.1.3.cmml" xref="S4.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.p1.6.m6.1.1.3.1.cmml" xref="S4.p1.6.m6.1.1.3">superscript</csymbol><ci id="S4.p1.6.m6.1.1.3.2.cmml" xref="S4.p1.6.m6.1.1.3.2">‚Ñù</ci><apply id="S4.p1.6.m6.1.1.3.3.cmml" xref="S4.p1.6.m6.1.1.3.3"><times id="S4.p1.6.m6.1.1.3.3.1.cmml" xref="S4.p1.6.m6.1.1.3.3.1"></times><apply id="S4.p1.6.m6.1.1.3.3.2.cmml" xref="S4.p1.6.m6.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.p1.6.m6.1.1.3.3.2.1.cmml" xref="S4.p1.6.m6.1.1.3.3.2">subscript</csymbol><ci id="S4.p1.6.m6.1.1.3.3.2.2.cmml" xref="S4.p1.6.m6.1.1.3.3.2.2">ùê∂</ci><cn id="S4.p1.6.m6.1.1.3.3.2.3.cmml" type="integer" xref="S4.p1.6.m6.1.1.3.3.2.3">1</cn></apply><apply id="S4.p1.6.m6.1.1.3.3.3.cmml" xref="S4.p1.6.m6.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.p1.6.m6.1.1.3.3.3.1.cmml" xref="S4.p1.6.m6.1.1.3.3.3">subscript</csymbol><ci id="S4.p1.6.m6.1.1.3.3.3.2.cmml" xref="S4.p1.6.m6.1.1.3.3.3.2">ùëç</ci><cn id="S4.p1.6.m6.1.1.3.3.3.3.cmml" type="integer" xref="S4.p1.6.m6.1.1.3.3.3.3">1</cn></apply><apply id="S4.p1.6.m6.1.1.3.3.4.cmml" xref="S4.p1.6.m6.1.1.3.3.4"><csymbol cd="ambiguous" id="S4.p1.6.m6.1.1.3.3.4.1.cmml" xref="S4.p1.6.m6.1.1.3.3.4">subscript</csymbol><ci id="S4.p1.6.m6.1.1.3.3.4.2.cmml" xref="S4.p1.6.m6.1.1.3.3.4.2">ùëå</ci><cn id="S4.p1.6.m6.1.1.3.3.4.3.cmml" type="integer" xref="S4.p1.6.m6.1.1.3.3.4.3">1</cn></apply><apply id="S4.p1.6.m6.1.1.3.3.5.cmml" xref="S4.p1.6.m6.1.1.3.3.5"><csymbol cd="ambiguous" id="S4.p1.6.m6.1.1.3.3.5.1.cmml" xref="S4.p1.6.m6.1.1.3.3.5">subscript</csymbol><ci id="S4.p1.6.m6.1.1.3.3.5.2.cmml" xref="S4.p1.6.m6.1.1.3.3.5.2">ùëã</ci><cn id="S4.p1.6.m6.1.1.3.3.5.3.cmml" type="integer" xref="S4.p1.6.m6.1.1.3.3.5.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.1c">f_{in}\in\mathbb{R}^{C_{1}\times Z_{1}\times Y_{1}\times X_{1}}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.6.m6.1d">italic_f start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT √ó italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT √ó italic_Y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT √ó italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>.
As the network proceeds from one stage to the next, it develops additional branches through strided convolution to widen the receptive field.
Each branch <math alttext="i" class="ltx_Math" display="inline" id="S4.p1.7.m7.1"><semantics id="S4.p1.7.m7.1a"><mi id="S4.p1.7.m7.1.1" xref="S4.p1.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.p1.7.m7.1b"><ci id="S4.p1.7.m7.1.1.cmml" xref="S4.p1.7.m7.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.7.m7.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.p1.7.m7.1d">italic_i</annotation></semantics></math> employs strided convolution to elevate its input feature <math alttext="f_{i}" class="ltx_Math" display="inline" id="S4.p1.8.m8.1"><semantics id="S4.p1.8.m8.1a"><msub id="S4.p1.8.m8.1.1" xref="S4.p1.8.m8.1.1.cmml"><mi id="S4.p1.8.m8.1.1.2" xref="S4.p1.8.m8.1.1.2.cmml">f</mi><mi id="S4.p1.8.m8.1.1.3" xref="S4.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.8.m8.1b"><apply id="S4.p1.8.m8.1.1.cmml" xref="S4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S4.p1.8.m8.1.1.1.cmml" xref="S4.p1.8.m8.1.1">subscript</csymbol><ci id="S4.p1.8.m8.1.1.2.cmml" xref="S4.p1.8.m8.1.1.2">ùëì</ci><ci id="S4.p1.8.m8.1.1.3.cmml" xref="S4.p1.8.m8.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.8.m8.1c">f_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.8.m8.1d">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to a feature <math alttext="f_{i+1}" class="ltx_Math" display="inline" id="S4.p1.9.m9.1"><semantics id="S4.p1.9.m9.1a"><msub id="S4.p1.9.m9.1.1" xref="S4.p1.9.m9.1.1.cmml"><mi id="S4.p1.9.m9.1.1.2" xref="S4.p1.9.m9.1.1.2.cmml">f</mi><mrow id="S4.p1.9.m9.1.1.3" xref="S4.p1.9.m9.1.1.3.cmml"><mi id="S4.p1.9.m9.1.1.3.2" xref="S4.p1.9.m9.1.1.3.2.cmml">i</mi><mo id="S4.p1.9.m9.1.1.3.1" xref="S4.p1.9.m9.1.1.3.1.cmml">+</mo><mn id="S4.p1.9.m9.1.1.3.3" xref="S4.p1.9.m9.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.9.m9.1b"><apply id="S4.p1.9.m9.1.1.cmml" xref="S4.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S4.p1.9.m9.1.1.1.cmml" xref="S4.p1.9.m9.1.1">subscript</csymbol><ci id="S4.p1.9.m9.1.1.2.cmml" xref="S4.p1.9.m9.1.1.2">ùëì</ci><apply id="S4.p1.9.m9.1.1.3.cmml" xref="S4.p1.9.m9.1.1.3"><plus id="S4.p1.9.m9.1.1.3.1.cmml" xref="S4.p1.9.m9.1.1.3.1"></plus><ci id="S4.p1.9.m9.1.1.3.2.cmml" xref="S4.p1.9.m9.1.1.3.2">ùëñ</ci><cn id="S4.p1.9.m9.1.1.3.3.cmml" type="integer" xref="S4.p1.9.m9.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.9.m9.1c">f_{i+1}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.9.m9.1d">italic_f start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math> in the succeeding branch <math alttext="i+1" class="ltx_Math" display="inline" id="S4.p1.10.m10.1"><semantics id="S4.p1.10.m10.1a"><mrow id="S4.p1.10.m10.1.1" xref="S4.p1.10.m10.1.1.cmml"><mi id="S4.p1.10.m10.1.1.2" xref="S4.p1.10.m10.1.1.2.cmml">i</mi><mo id="S4.p1.10.m10.1.1.1" xref="S4.p1.10.m10.1.1.1.cmml">+</mo><mn id="S4.p1.10.m10.1.1.3" xref="S4.p1.10.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.10.m10.1b"><apply id="S4.p1.10.m10.1.1.cmml" xref="S4.p1.10.m10.1.1"><plus id="S4.p1.10.m10.1.1.1.cmml" xref="S4.p1.10.m10.1.1.1"></plus><ci id="S4.p1.10.m10.1.1.2.cmml" xref="S4.p1.10.m10.1.1.2">ùëñ</ci><cn id="S4.p1.10.m10.1.1.3.cmml" type="integer" xref="S4.p1.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.10.m10.1c">i+1</annotation><annotation encoding="application/x-llamapun" id="S4.p1.10.m10.1d">italic_i + 1</annotation></semantics></math>, simultaneously doubling the channels and halving the spatial dimensions, <math alttext="f_{i+1}\in\mathbb{R}^{2C_{i}\times\frac{Z_{i}}{2}\times\frac{Y_{i}}{2}\times%
\frac{X_{i}}{2}}" class="ltx_Math" display="inline" id="S4.p1.11.m11.1"><semantics id="S4.p1.11.m11.1a"><mrow id="S4.p1.11.m11.1.1" xref="S4.p1.11.m11.1.1.cmml"><msub id="S4.p1.11.m11.1.1.2" xref="S4.p1.11.m11.1.1.2.cmml"><mi id="S4.p1.11.m11.1.1.2.2" xref="S4.p1.11.m11.1.1.2.2.cmml">f</mi><mrow id="S4.p1.11.m11.1.1.2.3" xref="S4.p1.11.m11.1.1.2.3.cmml"><mi id="S4.p1.11.m11.1.1.2.3.2" xref="S4.p1.11.m11.1.1.2.3.2.cmml">i</mi><mo id="S4.p1.11.m11.1.1.2.3.1" xref="S4.p1.11.m11.1.1.2.3.1.cmml">+</mo><mn id="S4.p1.11.m11.1.1.2.3.3" xref="S4.p1.11.m11.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S4.p1.11.m11.1.1.1" xref="S4.p1.11.m11.1.1.1.cmml">‚àà</mo><msup id="S4.p1.11.m11.1.1.3" xref="S4.p1.11.m11.1.1.3.cmml"><mi id="S4.p1.11.m11.1.1.3.2" xref="S4.p1.11.m11.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.p1.11.m11.1.1.3.3" xref="S4.p1.11.m11.1.1.3.3.cmml"><mrow id="S4.p1.11.m11.1.1.3.3.2" xref="S4.p1.11.m11.1.1.3.3.2.cmml"><mn id="S4.p1.11.m11.1.1.3.3.2.2" xref="S4.p1.11.m11.1.1.3.3.2.2.cmml">2</mn><mo id="S4.p1.11.m11.1.1.3.3.2.1" xref="S4.p1.11.m11.1.1.3.3.2.1.cmml">‚Å¢</mo><msub id="S4.p1.11.m11.1.1.3.3.2.3" xref="S4.p1.11.m11.1.1.3.3.2.3.cmml"><mi id="S4.p1.11.m11.1.1.3.3.2.3.2" xref="S4.p1.11.m11.1.1.3.3.2.3.2.cmml">C</mi><mi id="S4.p1.11.m11.1.1.3.3.2.3.3" xref="S4.p1.11.m11.1.1.3.3.2.3.3.cmml">i</mi></msub></mrow><mo id="S4.p1.11.m11.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.p1.11.m11.1.1.3.3.1.cmml">√ó</mo><mfrac id="S4.p1.11.m11.1.1.3.3.3" xref="S4.p1.11.m11.1.1.3.3.3.cmml"><msub id="S4.p1.11.m11.1.1.3.3.3.2" xref="S4.p1.11.m11.1.1.3.3.3.2.cmml"><mi id="S4.p1.11.m11.1.1.3.3.3.2.2" xref="S4.p1.11.m11.1.1.3.3.3.2.2.cmml">Z</mi><mi id="S4.p1.11.m11.1.1.3.3.3.2.3" xref="S4.p1.11.m11.1.1.3.3.3.2.3.cmml">i</mi></msub><mn id="S4.p1.11.m11.1.1.3.3.3.3" xref="S4.p1.11.m11.1.1.3.3.3.3.cmml">2</mn></mfrac><mo id="S4.p1.11.m11.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S4.p1.11.m11.1.1.3.3.1.cmml">√ó</mo><mfrac id="S4.p1.11.m11.1.1.3.3.4" xref="S4.p1.11.m11.1.1.3.3.4.cmml"><msub id="S4.p1.11.m11.1.1.3.3.4.2" xref="S4.p1.11.m11.1.1.3.3.4.2.cmml"><mi id="S4.p1.11.m11.1.1.3.3.4.2.2" xref="S4.p1.11.m11.1.1.3.3.4.2.2.cmml">Y</mi><mi id="S4.p1.11.m11.1.1.3.3.4.2.3" xref="S4.p1.11.m11.1.1.3.3.4.2.3.cmml">i</mi></msub><mn id="S4.p1.11.m11.1.1.3.3.4.3" xref="S4.p1.11.m11.1.1.3.3.4.3.cmml">2</mn></mfrac><mo id="S4.p1.11.m11.1.1.3.3.1b" lspace="0.222em" rspace="0.222em" xref="S4.p1.11.m11.1.1.3.3.1.cmml">√ó</mo><mfrac id="S4.p1.11.m11.1.1.3.3.5" xref="S4.p1.11.m11.1.1.3.3.5.cmml"><msub id="S4.p1.11.m11.1.1.3.3.5.2" xref="S4.p1.11.m11.1.1.3.3.5.2.cmml"><mi id="S4.p1.11.m11.1.1.3.3.5.2.2" xref="S4.p1.11.m11.1.1.3.3.5.2.2.cmml">X</mi><mi id="S4.p1.11.m11.1.1.3.3.5.2.3" xref="S4.p1.11.m11.1.1.3.3.5.2.3.cmml">i</mi></msub><mn id="S4.p1.11.m11.1.1.3.3.5.3" xref="S4.p1.11.m11.1.1.3.3.5.3.cmml">2</mn></mfrac></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.11.m11.1b"><apply id="S4.p1.11.m11.1.1.cmml" xref="S4.p1.11.m11.1.1"><in id="S4.p1.11.m11.1.1.1.cmml" xref="S4.p1.11.m11.1.1.1"></in><apply id="S4.p1.11.m11.1.1.2.cmml" xref="S4.p1.11.m11.1.1.2"><csymbol cd="ambiguous" id="S4.p1.11.m11.1.1.2.1.cmml" xref="S4.p1.11.m11.1.1.2">subscript</csymbol><ci id="S4.p1.11.m11.1.1.2.2.cmml" xref="S4.p1.11.m11.1.1.2.2">ùëì</ci><apply id="S4.p1.11.m11.1.1.2.3.cmml" xref="S4.p1.11.m11.1.1.2.3"><plus id="S4.p1.11.m11.1.1.2.3.1.cmml" xref="S4.p1.11.m11.1.1.2.3.1"></plus><ci id="S4.p1.11.m11.1.1.2.3.2.cmml" xref="S4.p1.11.m11.1.1.2.3.2">ùëñ</ci><cn id="S4.p1.11.m11.1.1.2.3.3.cmml" type="integer" xref="S4.p1.11.m11.1.1.2.3.3">1</cn></apply></apply><apply id="S4.p1.11.m11.1.1.3.cmml" xref="S4.p1.11.m11.1.1.3"><csymbol cd="ambiguous" id="S4.p1.11.m11.1.1.3.1.cmml" xref="S4.p1.11.m11.1.1.3">superscript</csymbol><ci id="S4.p1.11.m11.1.1.3.2.cmml" xref="S4.p1.11.m11.1.1.3.2">‚Ñù</ci><apply id="S4.p1.11.m11.1.1.3.3.cmml" xref="S4.p1.11.m11.1.1.3.3"><times id="S4.p1.11.m11.1.1.3.3.1.cmml" xref="S4.p1.11.m11.1.1.3.3.1"></times><apply id="S4.p1.11.m11.1.1.3.3.2.cmml" xref="S4.p1.11.m11.1.1.3.3.2"><times id="S4.p1.11.m11.1.1.3.3.2.1.cmml" xref="S4.p1.11.m11.1.1.3.3.2.1"></times><cn id="S4.p1.11.m11.1.1.3.3.2.2.cmml" type="integer" xref="S4.p1.11.m11.1.1.3.3.2.2">2</cn><apply id="S4.p1.11.m11.1.1.3.3.2.3.cmml" xref="S4.p1.11.m11.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S4.p1.11.m11.1.1.3.3.2.3.1.cmml" xref="S4.p1.11.m11.1.1.3.3.2.3">subscript</csymbol><ci id="S4.p1.11.m11.1.1.3.3.2.3.2.cmml" xref="S4.p1.11.m11.1.1.3.3.2.3.2">ùê∂</ci><ci id="S4.p1.11.m11.1.1.3.3.2.3.3.cmml" xref="S4.p1.11.m11.1.1.3.3.2.3.3">ùëñ</ci></apply></apply><apply id="S4.p1.11.m11.1.1.3.3.3.cmml" xref="S4.p1.11.m11.1.1.3.3.3"><divide id="S4.p1.11.m11.1.1.3.3.3.1.cmml" xref="S4.p1.11.m11.1.1.3.3.3"></divide><apply id="S4.p1.11.m11.1.1.3.3.3.2.cmml" xref="S4.p1.11.m11.1.1.3.3.3.2"><csymbol cd="ambiguous" id="S4.p1.11.m11.1.1.3.3.3.2.1.cmml" xref="S4.p1.11.m11.1.1.3.3.3.2">subscript</csymbol><ci id="S4.p1.11.m11.1.1.3.3.3.2.2.cmml" xref="S4.p1.11.m11.1.1.3.3.3.2.2">ùëç</ci><ci id="S4.p1.11.m11.1.1.3.3.3.2.3.cmml" xref="S4.p1.11.m11.1.1.3.3.3.2.3">ùëñ</ci></apply><cn id="S4.p1.11.m11.1.1.3.3.3.3.cmml" type="integer" xref="S4.p1.11.m11.1.1.3.3.3.3">2</cn></apply><apply id="S4.p1.11.m11.1.1.3.3.4.cmml" xref="S4.p1.11.m11.1.1.3.3.4"><divide id="S4.p1.11.m11.1.1.3.3.4.1.cmml" xref="S4.p1.11.m11.1.1.3.3.4"></divide><apply id="S4.p1.11.m11.1.1.3.3.4.2.cmml" xref="S4.p1.11.m11.1.1.3.3.4.2"><csymbol cd="ambiguous" id="S4.p1.11.m11.1.1.3.3.4.2.1.cmml" xref="S4.p1.11.m11.1.1.3.3.4.2">subscript</csymbol><ci id="S4.p1.11.m11.1.1.3.3.4.2.2.cmml" xref="S4.p1.11.m11.1.1.3.3.4.2.2">ùëå</ci><ci id="S4.p1.11.m11.1.1.3.3.4.2.3.cmml" xref="S4.p1.11.m11.1.1.3.3.4.2.3">ùëñ</ci></apply><cn id="S4.p1.11.m11.1.1.3.3.4.3.cmml" type="integer" xref="S4.p1.11.m11.1.1.3.3.4.3">2</cn></apply><apply id="S4.p1.11.m11.1.1.3.3.5.cmml" xref="S4.p1.11.m11.1.1.3.3.5"><divide id="S4.p1.11.m11.1.1.3.3.5.1.cmml" xref="S4.p1.11.m11.1.1.3.3.5"></divide><apply id="S4.p1.11.m11.1.1.3.3.5.2.cmml" xref="S4.p1.11.m11.1.1.3.3.5.2"><csymbol cd="ambiguous" id="S4.p1.11.m11.1.1.3.3.5.2.1.cmml" xref="S4.p1.11.m11.1.1.3.3.5.2">subscript</csymbol><ci id="S4.p1.11.m11.1.1.3.3.5.2.2.cmml" xref="S4.p1.11.m11.1.1.3.3.5.2.2">ùëã</ci><ci id="S4.p1.11.m11.1.1.3.3.5.2.3.cmml" xref="S4.p1.11.m11.1.1.3.3.5.2.3">ùëñ</ci></apply><cn id="S4.p1.11.m11.1.1.3.3.5.3.cmml" type="integer" xref="S4.p1.11.m11.1.1.3.3.5.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.11.m11.1c">f_{i+1}\in\mathbb{R}^{2C_{i}\times\frac{Z_{i}}{2}\times\frac{Y_{i}}{2}\times%
\frac{X_{i}}{2}}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.11.m11.1d">italic_f start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT 2 italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT √ó divide start_ARG italic_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG √ó divide start_ARG italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG √ó divide start_ARG italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT</annotation></semantics></math>.
Within each stage, a series of <math alttext="P" class="ltx_Math" display="inline" id="S4.p1.12.m12.1"><semantics id="S4.p1.12.m12.1a"><mi id="S4.p1.12.m12.1.1" xref="S4.p1.12.m12.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.p1.12.m12.1b"><ci id="S4.p1.12.m12.1.1.cmml" xref="S4.p1.12.m12.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.12.m12.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.p1.12.m12.1d">italic_P</annotation></semantics></math> modules execute <math alttext="M" class="ltx_Math" display="inline" id="S4.p1.13.m13.1"><semantics id="S4.p1.13.m13.1a"><mi id="S4.p1.13.m13.1.1" xref="S4.p1.13.m13.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.p1.13.m13.1b"><ci id="S4.p1.13.m13.1.1.cmml" xref="S4.p1.13.m13.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.13.m13.1c">M</annotation><annotation encoding="application/x-llamapun" id="S4.p1.13.m13.1d">italic_M</annotation></semantics></math> parallel convolutions.
To align feature dimensions from different branches for information exchange, we use strided convolution followed by upsampling.
At the end of the backbone, we take the feature with the highest resolution to serve as a unified representation for the pose estimation head.
In the pose estimation head, two 3D convolutional branches are employed to process these features further.
This bifurcation generates two outputs: a distribution map signifying the confidence of locating the human center and the location offset of joint keypoints.
The right part of Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S4.F6" title="Figure 6 ‚Ä£ 4 HRRadarPose ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">6</span></a> shows the decoding of the HRRadarPose network‚Äôs output to multi-person poses.
The process starts with picking top-<math alttext="k" class="ltx_Math" display="inline" id="S4.p1.14.m14.1"><semantics id="S4.p1.14.m14.1a"><mi id="S4.p1.14.m14.1.1" xref="S4.p1.14.m14.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p1.14.m14.1b"><ci id="S4.p1.14.m14.1.1.cmml" xref="S4.p1.14.m14.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.14.m14.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p1.14.m14.1d">italic_k</annotation></semantics></math> human centers, <math alttext="C_{S}" class="ltx_Math" display="inline" id="S4.p1.15.m15.1"><semantics id="S4.p1.15.m15.1a"><msub id="S4.p1.15.m15.1.1" xref="S4.p1.15.m15.1.1.cmml"><mi id="S4.p1.15.m15.1.1.2" xref="S4.p1.15.m15.1.1.2.cmml">C</mi><mi id="S4.p1.15.m15.1.1.3" xref="S4.p1.15.m15.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.15.m15.1b"><apply id="S4.p1.15.m15.1.1.cmml" xref="S4.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S4.p1.15.m15.1.1.1.cmml" xref="S4.p1.15.m15.1.1">subscript</csymbol><ci id="S4.p1.15.m15.1.1.2.cmml" xref="S4.p1.15.m15.1.1.2">ùê∂</ci><ci id="S4.p1.15.m15.1.1.3.cmml" xref="S4.p1.15.m15.1.1.3">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.15.m15.1c">C_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.15.m15.1d">italic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math>, with confidence scores over a threshold value.
We denote these picked centers‚Äô indices as <math alttext="S" class="ltx_Math" display="inline" id="S4.p1.16.m16.1"><semantics id="S4.p1.16.m16.1a"><mi id="S4.p1.16.m16.1.1" xref="S4.p1.16.m16.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.p1.16.m16.1b"><ci id="S4.p1.16.m16.1.1.cmml" xref="S4.p1.16.m16.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.16.m16.1c">S</annotation><annotation encoding="application/x-llamapun" id="S4.p1.16.m16.1d">italic_S</annotation></semantics></math>.
Then, we query the centers corresponding joint keypoints‚Äô offsets, <math alttext="K_{S}" class="ltx_Math" display="inline" id="S4.p1.17.m17.1"><semantics id="S4.p1.17.m17.1a"><msub id="S4.p1.17.m17.1.1" xref="S4.p1.17.m17.1.1.cmml"><mi id="S4.p1.17.m17.1.1.2" xref="S4.p1.17.m17.1.1.2.cmml">K</mi><mi id="S4.p1.17.m17.1.1.3" xref="S4.p1.17.m17.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.17.m17.1b"><apply id="S4.p1.17.m17.1.1.cmml" xref="S4.p1.17.m17.1.1"><csymbol cd="ambiguous" id="S4.p1.17.m17.1.1.1.cmml" xref="S4.p1.17.m17.1.1">subscript</csymbol><ci id="S4.p1.17.m17.1.1.2.cmml" xref="S4.p1.17.m17.1.1.2">ùêæ</ci><ci id="S4.p1.17.m17.1.1.3.cmml" xref="S4.p1.17.m17.1.1.3">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.17.m17.1c">K_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.17.m17.1d">italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math>, by looking up the output map of keypoint offsets head at <math alttext="S" class="ltx_Math" display="inline" id="S4.p1.18.m18.1"><semantics id="S4.p1.18.m18.1a"><mi id="S4.p1.18.m18.1.1" xref="S4.p1.18.m18.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.p1.18.m18.1b"><ci id="S4.p1.18.m18.1.1.cmml" xref="S4.p1.18.m18.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.18.m18.1c">S</annotation><annotation encoding="application/x-llamapun" id="S4.p1.18.m18.1d">italic_S</annotation></semantics></math>.
Lastly, we decode their joint keypoint locations, <math alttext="J_{S}" class="ltx_Math" display="inline" id="S4.p1.19.m19.1"><semantics id="S4.p1.19.m19.1a"><msub id="S4.p1.19.m19.1.1" xref="S4.p1.19.m19.1.1.cmml"><mi id="S4.p1.19.m19.1.1.2" xref="S4.p1.19.m19.1.1.2.cmml">J</mi><mi id="S4.p1.19.m19.1.1.3" xref="S4.p1.19.m19.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.19.m19.1b"><apply id="S4.p1.19.m19.1.1.cmml" xref="S4.p1.19.m19.1.1"><csymbol cd="ambiguous" id="S4.p1.19.m19.1.1.1.cmml" xref="S4.p1.19.m19.1.1">subscript</csymbol><ci id="S4.p1.19.m19.1.1.2.cmml" xref="S4.p1.19.m19.1.1.2">ùêΩ</ci><ci id="S4.p1.19.m19.1.1.3.cmml" xref="S4.p1.19.m19.1.1.3">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.19.m19.1c">J_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.19.m19.1d">italic_J start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math>, by summing <math alttext="K_{S}" class="ltx_Math" display="inline" id="S4.p1.20.m20.1"><semantics id="S4.p1.20.m20.1a"><msub id="S4.p1.20.m20.1.1" xref="S4.p1.20.m20.1.1.cmml"><mi id="S4.p1.20.m20.1.1.2" xref="S4.p1.20.m20.1.1.2.cmml">K</mi><mi id="S4.p1.20.m20.1.1.3" xref="S4.p1.20.m20.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.20.m20.1b"><apply id="S4.p1.20.m20.1.1.cmml" xref="S4.p1.20.m20.1.1"><csymbol cd="ambiguous" id="S4.p1.20.m20.1.1.1.cmml" xref="S4.p1.20.m20.1.1">subscript</csymbol><ci id="S4.p1.20.m20.1.1.2.cmml" xref="S4.p1.20.m20.1.1.2">ùêæ</ci><ci id="S4.p1.20.m20.1.1.3.cmml" xref="S4.p1.20.m20.1.1.3">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.20.m20.1c">K_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.20.m20.1d">italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="C_{S}" class="ltx_Math" display="inline" id="S4.p1.21.m21.1"><semantics id="S4.p1.21.m21.1a"><msub id="S4.p1.21.m21.1.1" xref="S4.p1.21.m21.1.1.cmml"><mi id="S4.p1.21.m21.1.1.2" xref="S4.p1.21.m21.1.1.2.cmml">C</mi><mi id="S4.p1.21.m21.1.1.3" xref="S4.p1.21.m21.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.21.m21.1b"><apply id="S4.p1.21.m21.1.1.cmml" xref="S4.p1.21.m21.1.1"><csymbol cd="ambiguous" id="S4.p1.21.m21.1.1.1.cmml" xref="S4.p1.21.m21.1.1">subscript</csymbol><ci id="S4.p1.21.m21.1.1.2.cmml" xref="S4.p1.21.m21.1.1.2">ùê∂</ci><ci id="S4.p1.21.m21.1.1.3.cmml" xref="S4.p1.21.m21.1.1.3">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.21.m21.1c">C_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.21.m21.1d">italic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math>.
In scenarios with multiple people, we apply range non-maximum suppression (NMS) to reduce overlapping detections. Our method is favorable in two folds:
The HRRadarPose‚Äôs architecture gains efficiency through a single-stage workflow, avoiding complexity brought by region-proposal-based methods ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib55" title="">55</a>]</cite>. In addition, our pose estimation head inherently learns per person‚Äôs joint keypoints refraining from ambiguity of association between people‚Äôs identity and joints.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="306" id="S4.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">
HRRadarPose‚Äôs architecture consists of a 3D convolutional backbone and a pose estimation head, which generates a body center‚Äôs confidence map and joint keypoint offsets map.
To infer multi-person poses, we decode the prediction by adding the picked bodies‚Äô centers with their corresponding keypoint offsets.
</span></figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">To train the center probability head, we use the pixel-wise focal loss as the classification loss, <math alttext="L_{class}" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><msub id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">L</mi><mrow id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml"><mi id="S4.p2.1.m1.1.1.3.2" xref="S4.p2.1.m1.1.1.3.2.cmml">c</mi><mo id="S4.p2.1.m1.1.1.3.1" xref="S4.p2.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.1.m1.1.1.3.3" xref="S4.p2.1.m1.1.1.3.3.cmml">l</mi><mo id="S4.p2.1.m1.1.1.3.1a" xref="S4.p2.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.1.m1.1.1.3.4" xref="S4.p2.1.m1.1.1.3.4.cmml">a</mi><mo id="S4.p2.1.m1.1.1.3.1b" xref="S4.p2.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.1.m1.1.1.3.5" xref="S4.p2.1.m1.1.1.3.5.cmml">s</mi><mo id="S4.p2.1.m1.1.1.3.1c" xref="S4.p2.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.1.m1.1.1.3.6" xref="S4.p2.1.m1.1.1.3.6.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">ùêø</ci><apply id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3"><times id="S4.p2.1.m1.1.1.3.1.cmml" xref="S4.p2.1.m1.1.1.3.1"></times><ci id="S4.p2.1.m1.1.1.3.2.cmml" xref="S4.p2.1.m1.1.1.3.2">ùëê</ci><ci id="S4.p2.1.m1.1.1.3.3.cmml" xref="S4.p2.1.m1.1.1.3.3">ùëô</ci><ci id="S4.p2.1.m1.1.1.3.4.cmml" xref="S4.p2.1.m1.1.1.3.4">ùëé</ci><ci id="S4.p2.1.m1.1.1.3.5.cmml" xref="S4.p2.1.m1.1.1.3.5">ùë†</ci><ci id="S4.p2.1.m1.1.1.3.6.cmml" xref="S4.p2.1.m1.1.1.3.6">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">L_{class}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">italic_L start_POSTSUBSCRIPT italic_c italic_l italic_a italic_s italic_s end_POSTSUBSCRIPT</annotation></semantics></math>, defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{class}=\begin{cases}(1-P_{xyz})^{\alpha}\log(P_{xyz}),&amp;\text{if }Y_{xyz}=1,%
\\
(1-Y_{xyz})^{\beta}(P_{xyz})^{\alpha}\log(1-P_{xyz}),&amp;\text{otherwise},\end{cases}" class="ltx_Math" display="block" id="S4.E1.m1.4"><semantics id="S4.E1.m1.4a"><mrow id="S4.E1.m1.4.5" xref="S4.E1.m1.4.5.cmml"><msub id="S4.E1.m1.4.5.2" xref="S4.E1.m1.4.5.2.cmml"><mi id="S4.E1.m1.4.5.2.2" xref="S4.E1.m1.4.5.2.2.cmml">L</mi><mrow id="S4.E1.m1.4.5.2.3" xref="S4.E1.m1.4.5.2.3.cmml"><mi id="S4.E1.m1.4.5.2.3.2" xref="S4.E1.m1.4.5.2.3.2.cmml">c</mi><mo id="S4.E1.m1.4.5.2.3.1" xref="S4.E1.m1.4.5.2.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.4.5.2.3.3" xref="S4.E1.m1.4.5.2.3.3.cmml">l</mi><mo id="S4.E1.m1.4.5.2.3.1a" xref="S4.E1.m1.4.5.2.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.4.5.2.3.4" xref="S4.E1.m1.4.5.2.3.4.cmml">a</mi><mo id="S4.E1.m1.4.5.2.3.1b" xref="S4.E1.m1.4.5.2.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.4.5.2.3.5" xref="S4.E1.m1.4.5.2.3.5.cmml">s</mi><mo id="S4.E1.m1.4.5.2.3.1c" xref="S4.E1.m1.4.5.2.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.4.5.2.3.6" xref="S4.E1.m1.4.5.2.3.6.cmml">s</mi></mrow></msub><mo id="S4.E1.m1.4.5.1" xref="S4.E1.m1.4.5.1.cmml">=</mo><mrow id="S4.E1.m1.4.4" xref="S4.E1.m1.4.5.3.1.cmml"><mo id="S4.E1.m1.4.4.5" xref="S4.E1.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S4.E1.m1.4.4.4" rowspacing="0pt" xref="S4.E1.m1.4.5.3.1.cmml"><mtr id="S4.E1.m1.4.4.4a" xref="S4.E1.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4b" xref="S4.E1.m1.4.5.3.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.2.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.2.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.cmml"><msup id="S4.E1.m1.1.1.1.1.1.1.2.1.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml"><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml"><mn id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml">1</mn><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.2.cmml">P</mi><mrow id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.2" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.2.cmml">x</mi><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.3" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.3.cmml">y</mi><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.1a" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.4" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.4.cmml">z</mi></mrow></msub></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml">)</mo></mrow><mi id="S4.E1.m1.1.1.1.1.1.1.2.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.3.cmml">Œ±</mi></msup><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.3" lspace="0.167em" xref="S4.E1.m1.1.1.1.1.1.1.2.1.3.cmml">‚Å¢</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.2.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">log</mi><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1a" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.2.cmml">‚Å°</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.2.cmml"><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.2.cmml">(</mo><msub id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.2.cmml">P</mi><mrow id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.2.cmml">x</mi><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.3.cmml">y</mi><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.1a" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.4" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.4.cmml">z</mi></mrow></msub><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.2.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.1.1.2.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4c" xref="S4.E1.m1.4.5.3.1.cmml"><mrow id="S4.E1.m1.2.2.2.2.2.1.1" xref="S4.E1.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="S4.E1.m1.2.2.2.2.2.1.1.1" xref="S4.E1.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="S4.E1.m1.2.2.2.2.2.1.1.1.2" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.cmml"><mtext id="S4.E1.m1.2.2.2.2.2.1.1.1.2.2" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.2a.cmml">if¬†</mtext><mo id="S4.E1.m1.2.2.2.2.2.1.1.1.2.1" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.1.cmml">‚Å¢</mo><msub id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.cmml"><mi id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.2" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.2.cmml">Y</mi><mrow id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.cmml"><mi id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.2" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.2.cmml">x</mi><mo id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.1" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.3" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.3.cmml">y</mi><mo id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.1a" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.4" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.4.cmml">z</mi></mrow></msub></mrow><mo id="S4.E1.m1.2.2.2.2.2.1.1.1.1" xref="S4.E1.m1.2.2.2.2.2.1.1.1.1.cmml">=</mo><mn id="S4.E1.m1.2.2.2.2.2.1.1.1.3" xref="S4.E1.m1.2.2.2.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S4.E1.m1.2.2.2.2.2.1.1.2" xref="S4.E1.m1.2.2.2.2.2.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S4.E1.m1.4.4.4d" xref="S4.E1.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4e" xref="S4.E1.m1.4.5.3.1.cmml"><mrow id="S4.E1.m1.3.3.3.3.1.1.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.cmml"><mrow id="S4.E1.m1.3.3.3.3.1.1.2.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.cmml"><msup id="S4.E1.m1.3.3.3.3.1.1.2.1.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.cmml"><mrow id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.cmml"><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.cmml"><mn id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.2.cmml">1</mn><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.2.cmml">Y</mi><mrow id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.cmml"><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.2.cmml">x</mi><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.3" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.3.cmml">y</mi><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.1a" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.4" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.4.cmml">z</mi></mrow></msub></mrow><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.cmml">)</mo></mrow><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.1.3" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.3.cmml">Œ≤</mi></msup><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.4" xref="S4.E1.m1.3.3.3.3.1.1.2.1.4.cmml">‚Å¢</mo><msup id="S4.E1.m1.3.3.3.3.1.1.2.1.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.cmml"><mrow id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.cmml"><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.2" stretchy="false" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.cmml">(</mo><msub id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.cmml"><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.2.cmml">P</mi><mrow id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.cmml"><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.2.cmml">x</mi><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.3" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.3.cmml">y</mi><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.1a" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.4" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.4.cmml">z</mi></mrow></msub><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.3" stretchy="false" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.cmml">)</mo></mrow><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.2.3" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.3.cmml">Œ±</mi></msup><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.4a" lspace="0.167em" xref="S4.E1.m1.3.3.3.3.1.1.2.1.4.cmml">‚Å¢</mo><mrow id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.2.cmml"><mi id="S4.E1.m1.3.3.3.3.1.1.1" xref="S4.E1.m1.3.3.3.3.1.1.1.cmml">log</mi><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1a" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.2.cmml">‚Å°</mo><mrow id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.2.cmml"><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.2" stretchy="false" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.2.cmml">(</mo><mrow id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.cmml"><mn id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.2.cmml">1</mn><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.1.cmml">‚àí</mo><msub id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.cmml"><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.2.cmml">P</mi><mrow id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.cmml"><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.2.cmml">x</mi><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.1" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.3" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.3.cmml">y</mi><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.1a" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.4" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.4.cmml">z</mi></mrow></msub></mrow><mo id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.3" stretchy="false" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.2.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E1.m1.3.3.3.3.1.1.2.2" xref="S4.E1.m1.3.3.3.3.1.1.2.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4f" xref="S4.E1.m1.4.5.3.1.cmml"><mrow id="S4.E1.m1.4.4.4.4.2.1.3" xref="S4.E1.m1.4.4.4.4.2.1.1a.cmml"><mtext id="S4.E1.m1.4.4.4.4.2.1.1" xref="S4.E1.m1.4.4.4.4.2.1.1.cmml">otherwise</mtext><mo id="S4.E1.m1.4.4.4.4.2.1.3.1" xref="S4.E1.m1.4.4.4.4.2.1.1a.cmml">,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.4b"><apply id="S4.E1.m1.4.5.cmml" xref="S4.E1.m1.4.5"><eq id="S4.E1.m1.4.5.1.cmml" xref="S4.E1.m1.4.5.1"></eq><apply id="S4.E1.m1.4.5.2.cmml" xref="S4.E1.m1.4.5.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.5.2.1.cmml" xref="S4.E1.m1.4.5.2">subscript</csymbol><ci id="S4.E1.m1.4.5.2.2.cmml" xref="S4.E1.m1.4.5.2.2">ùêø</ci><apply id="S4.E1.m1.4.5.2.3.cmml" xref="S4.E1.m1.4.5.2.3"><times id="S4.E1.m1.4.5.2.3.1.cmml" xref="S4.E1.m1.4.5.2.3.1"></times><ci id="S4.E1.m1.4.5.2.3.2.cmml" xref="S4.E1.m1.4.5.2.3.2">ùëê</ci><ci id="S4.E1.m1.4.5.2.3.3.cmml" xref="S4.E1.m1.4.5.2.3.3">ùëô</ci><ci id="S4.E1.m1.4.5.2.3.4.cmml" xref="S4.E1.m1.4.5.2.3.4">ùëé</ci><ci id="S4.E1.m1.4.5.2.3.5.cmml" xref="S4.E1.m1.4.5.2.3.5">ùë†</ci><ci id="S4.E1.m1.4.5.2.3.6.cmml" xref="S4.E1.m1.4.5.2.3.6">ùë†</ci></apply></apply><apply id="S4.E1.m1.4.5.3.1.cmml" xref="S4.E1.m1.4.4"><csymbol cd="latexml" id="S4.E1.m1.4.5.3.1.1.cmml" xref="S4.E1.m1.4.4.5">cases</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2"><times id="S4.E1.m1.1.1.1.1.1.1.2.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.3"></times><apply id="S4.E1.m1.1.1.1.1.1.1.2.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1">superscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1"><minus id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.1"></minus><cn id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml" type="integer" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.2">1</cn><apply id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.2">ùëÉ</ci><apply id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3"><times id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.1"></times><ci id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.2">ùë•</ci><ci id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.3">ùë¶</ci><ci id="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.4.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.4">ùëß</ci></apply></apply></apply><ci id="S4.E1.m1.1.1.1.1.1.1.2.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.1.3">ùõº</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.2.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1"><log id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1"></log><apply id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.2">ùëÉ</ci><apply id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3"><times id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.1"></times><ci id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.2">ùë•</ci><ci id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.3">ùë¶</ci><ci id="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.4.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1.2.1.1.1.3.4">ùëß</ci></apply></apply></apply></apply><apply id="S4.E1.m1.2.2.2.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1"><eq id="S4.E1.m1.2.2.2.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.1"></eq><apply id="S4.E1.m1.2.2.2.2.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2"><times id="S4.E1.m1.2.2.2.2.2.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.1"></times><ci id="S4.E1.m1.2.2.2.2.2.1.1.1.2.2a.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.2"><mtext id="S4.E1.m1.2.2.2.2.2.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.2">if¬†</mtext></ci><apply id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.1.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.2.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.2">ùëå</ci><apply id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3"><times id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.1.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.1"></times><ci id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.2.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.2">ùë•</ci><ci id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.3.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.3">ùë¶</ci><ci id="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.4.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1.1.2.3.3.4">ùëß</ci></apply></apply></apply><cn id="S4.E1.m1.2.2.2.2.2.1.1.1.3.cmml" type="integer" xref="S4.E1.m1.2.2.2.2.2.1.1.1.3">1</cn></apply><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2"><times id="S4.E1.m1.3.3.3.3.1.1.2.1.4.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.4"></times><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.3.1.1.2.1.1.2.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1">superscript</csymbol><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1"><minus id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.1"></minus><cn id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.2.cmml" type="integer" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.2">1</cn><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.2">ùëå</ci><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3"><times id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.1"></times><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.2.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.2">ùë•</ci><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.3">ùë¶</ci><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.4.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.1.1.1.3.3.4">ùëß</ci></apply></apply></apply><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.1.3.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.1.3">ùõΩ</ci></apply><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.2.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.3.1.1.2.1.2.2.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2">superscript</csymbol><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1">subscript</csymbol><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.2.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.2">ùëÉ</ci><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3"><times id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.1"></times><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.2.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.2">ùë•</ci><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.3.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.3">ùë¶</ci><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.4.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.1.1.1.3.4">ùëß</ci></apply></apply><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.2.3.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.2.3">ùõº</ci></apply><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.3.2.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1"><log id="S4.E1.m1.3.3.3.3.1.1.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.1"></log><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1"><minus id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.1"></minus><cn id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.2.cmml" type="integer" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.2">1</cn><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.2.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.2">ùëÉ</ci><apply id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3"><times id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.1"></times><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.2.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.2">ùë•</ci><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.3">ùë¶</ci><ci id="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.4.cmml" xref="S4.E1.m1.3.3.3.3.1.1.2.1.3.1.1.1.3.3.4">ùëß</ci></apply></apply></apply></apply></apply><ci id="S4.E1.m1.4.4.4.4.2.1.1a.cmml" xref="S4.E1.m1.4.4.4.4.2.1.3"><mtext id="S4.E1.m1.4.4.4.4.2.1.1.cmml" xref="S4.E1.m1.4.4.4.4.2.1.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.4c">L_{class}=\begin{cases}(1-P_{xyz})^{\alpha}\log(P_{xyz}),&amp;\text{if }Y_{xyz}=1,%
\\
(1-Y_{xyz})^{\beta}(P_{xyz})^{\alpha}\log(1-P_{xyz}),&amp;\text{otherwise},\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.4d">italic_L start_POSTSUBSCRIPT italic_c italic_l italic_a italic_s italic_s end_POSTSUBSCRIPT = { start_ROW start_CELL ( 1 - italic_P start_POSTSUBSCRIPT italic_x italic_y italic_z end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_Œ± end_POSTSUPERSCRIPT roman_log ( italic_P start_POSTSUBSCRIPT italic_x italic_y italic_z end_POSTSUBSCRIPT ) , end_CELL start_CELL if italic_Y start_POSTSUBSCRIPT italic_x italic_y italic_z end_POSTSUBSCRIPT = 1 , end_CELL end_ROW start_ROW start_CELL ( 1 - italic_Y start_POSTSUBSCRIPT italic_x italic_y italic_z end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_Œ≤ end_POSTSUPERSCRIPT ( italic_P start_POSTSUBSCRIPT italic_x italic_y italic_z end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_Œ± end_POSTSUPERSCRIPT roman_log ( 1 - italic_P start_POSTSUBSCRIPT italic_x italic_y italic_z end_POSTSUBSCRIPT ) , end_CELL start_CELL otherwise , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.p2.14">where <math alttext="Y_{xyz}" class="ltx_Math" display="inline" id="S4.p2.2.m1.1"><semantics id="S4.p2.2.m1.1a"><msub id="S4.p2.2.m1.1.1" xref="S4.p2.2.m1.1.1.cmml"><mi id="S4.p2.2.m1.1.1.2" xref="S4.p2.2.m1.1.1.2.cmml">Y</mi><mrow id="S4.p2.2.m1.1.1.3" xref="S4.p2.2.m1.1.1.3.cmml"><mi id="S4.p2.2.m1.1.1.3.2" xref="S4.p2.2.m1.1.1.3.2.cmml">x</mi><mo id="S4.p2.2.m1.1.1.3.1" xref="S4.p2.2.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.2.m1.1.1.3.3" xref="S4.p2.2.m1.1.1.3.3.cmml">y</mi><mo id="S4.p2.2.m1.1.1.3.1a" xref="S4.p2.2.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.2.m1.1.1.3.4" xref="S4.p2.2.m1.1.1.3.4.cmml">z</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.2.m1.1b"><apply id="S4.p2.2.m1.1.1.cmml" xref="S4.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m1.1.1.1.cmml" xref="S4.p2.2.m1.1.1">subscript</csymbol><ci id="S4.p2.2.m1.1.1.2.cmml" xref="S4.p2.2.m1.1.1.2">ùëå</ci><apply id="S4.p2.2.m1.1.1.3.cmml" xref="S4.p2.2.m1.1.1.3"><times id="S4.p2.2.m1.1.1.3.1.cmml" xref="S4.p2.2.m1.1.1.3.1"></times><ci id="S4.p2.2.m1.1.1.3.2.cmml" xref="S4.p2.2.m1.1.1.3.2">ùë•</ci><ci id="S4.p2.2.m1.1.1.3.3.cmml" xref="S4.p2.2.m1.1.1.3.3">ùë¶</ci><ci id="S4.p2.2.m1.1.1.3.4.cmml" xref="S4.p2.2.m1.1.1.3.4">ùëß</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m1.1c">Y_{xyz}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.2.m1.1d">italic_Y start_POSTSUBSCRIPT italic_x italic_y italic_z end_POSTSUBSCRIPT</annotation></semantics></math> is the three-dimensional human center Gaussian distribution map generated by pelvis locations, <math alttext="P_{xyz}" class="ltx_Math" display="inline" id="S4.p2.3.m2.1"><semantics id="S4.p2.3.m2.1a"><msub id="S4.p2.3.m2.1.1" xref="S4.p2.3.m2.1.1.cmml"><mi id="S4.p2.3.m2.1.1.2" xref="S4.p2.3.m2.1.1.2.cmml">P</mi><mrow id="S4.p2.3.m2.1.1.3" xref="S4.p2.3.m2.1.1.3.cmml"><mi id="S4.p2.3.m2.1.1.3.2" xref="S4.p2.3.m2.1.1.3.2.cmml">x</mi><mo id="S4.p2.3.m2.1.1.3.1" xref="S4.p2.3.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.3.m2.1.1.3.3" xref="S4.p2.3.m2.1.1.3.3.cmml">y</mi><mo id="S4.p2.3.m2.1.1.3.1a" xref="S4.p2.3.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.3.m2.1.1.3.4" xref="S4.p2.3.m2.1.1.3.4.cmml">z</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.3.m2.1b"><apply id="S4.p2.3.m2.1.1.cmml" xref="S4.p2.3.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m2.1.1.1.cmml" xref="S4.p2.3.m2.1.1">subscript</csymbol><ci id="S4.p2.3.m2.1.1.2.cmml" xref="S4.p2.3.m2.1.1.2">ùëÉ</ci><apply id="S4.p2.3.m2.1.1.3.cmml" xref="S4.p2.3.m2.1.1.3"><times id="S4.p2.3.m2.1.1.3.1.cmml" xref="S4.p2.3.m2.1.1.3.1"></times><ci id="S4.p2.3.m2.1.1.3.2.cmml" xref="S4.p2.3.m2.1.1.3.2">ùë•</ci><ci id="S4.p2.3.m2.1.1.3.3.cmml" xref="S4.p2.3.m2.1.1.3.3">ùë¶</ci><ci id="S4.p2.3.m2.1.1.3.4.cmml" xref="S4.p2.3.m2.1.1.3.4">ùëß</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m2.1c">P_{xyz}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.3.m2.1d">italic_P start_POSTSUBSCRIPT italic_x italic_y italic_z end_POSTSUBSCRIPT</annotation></semantics></math> is the probability of a person‚Äôs presence at location, <math alttext="(x,y,z)" class="ltx_Math" display="inline" id="S4.p2.4.m3.3"><semantics id="S4.p2.4.m3.3a"><mrow id="S4.p2.4.m3.3.4.2" xref="S4.p2.4.m3.3.4.1.cmml"><mo id="S4.p2.4.m3.3.4.2.1" stretchy="false" xref="S4.p2.4.m3.3.4.1.cmml">(</mo><mi id="S4.p2.4.m3.1.1" xref="S4.p2.4.m3.1.1.cmml">x</mi><mo id="S4.p2.4.m3.3.4.2.2" xref="S4.p2.4.m3.3.4.1.cmml">,</mo><mi id="S4.p2.4.m3.2.2" xref="S4.p2.4.m3.2.2.cmml">y</mi><mo id="S4.p2.4.m3.3.4.2.3" xref="S4.p2.4.m3.3.4.1.cmml">,</mo><mi id="S4.p2.4.m3.3.3" xref="S4.p2.4.m3.3.3.cmml">z</mi><mo id="S4.p2.4.m3.3.4.2.4" stretchy="false" xref="S4.p2.4.m3.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m3.3b"><vector id="S4.p2.4.m3.3.4.1.cmml" xref="S4.p2.4.m3.3.4.2"><ci id="S4.p2.4.m3.1.1.cmml" xref="S4.p2.4.m3.1.1">ùë•</ci><ci id="S4.p2.4.m3.2.2.cmml" xref="S4.p2.4.m3.2.2">ùë¶</ci><ci id="S4.p2.4.m3.3.3.cmml" xref="S4.p2.4.m3.3.3">ùëß</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m3.3c">(x,y,z)</annotation><annotation encoding="application/x-llamapun" id="S4.p2.4.m3.3d">( italic_x , italic_y , italic_z )</annotation></semantics></math>, <math alttext="N" class="ltx_Math" display="inline" id="S4.p2.5.m4.1"><semantics id="S4.p2.5.m4.1a"><mi id="S4.p2.5.m4.1.1" xref="S4.p2.5.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.p2.5.m4.1b"><ci id="S4.p2.5.m4.1.1.cmml" xref="S4.p2.5.m4.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.p2.5.m4.1d">italic_N</annotation></semantics></math> is the number of foreground samples, <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.p2.6.m5.1"><semantics id="S4.p2.6.m5.1a"><mi id="S4.p2.6.m5.1.1" xref="S4.p2.6.m5.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S4.p2.6.m5.1b"><ci id="S4.p2.6.m5.1.1.cmml" xref="S4.p2.6.m5.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m5.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.p2.6.m5.1d">italic_Œ±</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="S4.p2.7.m6.1"><semantics id="S4.p2.7.m6.1a"><mi id="S4.p2.7.m6.1.1" xref="S4.p2.7.m6.1.1.cmml">Œ≤</mi><annotation-xml encoding="MathML-Content" id="S4.p2.7.m6.1b"><ci id="S4.p2.7.m6.1.1.cmml" xref="S4.p2.7.m6.1.1">ùõΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m6.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.p2.7.m6.1d">italic_Œ≤</annotation></semantics></math> are hyperparameters addressing class imbalance and focusing training on foreground samples.
To train the keypoint offset head, we construct the regression loss <math alttext="L_{reg}" class="ltx_Math" display="inline" id="S4.p2.8.m7.1"><semantics id="S4.p2.8.m7.1a"><msub id="S4.p2.8.m7.1.1" xref="S4.p2.8.m7.1.1.cmml"><mi id="S4.p2.8.m7.1.1.2" xref="S4.p2.8.m7.1.1.2.cmml">L</mi><mrow id="S4.p2.8.m7.1.1.3" xref="S4.p2.8.m7.1.1.3.cmml"><mi id="S4.p2.8.m7.1.1.3.2" xref="S4.p2.8.m7.1.1.3.2.cmml">r</mi><mo id="S4.p2.8.m7.1.1.3.1" xref="S4.p2.8.m7.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.8.m7.1.1.3.3" xref="S4.p2.8.m7.1.1.3.3.cmml">e</mi><mo id="S4.p2.8.m7.1.1.3.1a" xref="S4.p2.8.m7.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.8.m7.1.1.3.4" xref="S4.p2.8.m7.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.8.m7.1b"><apply id="S4.p2.8.m7.1.1.cmml" xref="S4.p2.8.m7.1.1"><csymbol cd="ambiguous" id="S4.p2.8.m7.1.1.1.cmml" xref="S4.p2.8.m7.1.1">subscript</csymbol><ci id="S4.p2.8.m7.1.1.2.cmml" xref="S4.p2.8.m7.1.1.2">ùêø</ci><apply id="S4.p2.8.m7.1.1.3.cmml" xref="S4.p2.8.m7.1.1.3"><times id="S4.p2.8.m7.1.1.3.1.cmml" xref="S4.p2.8.m7.1.1.3.1"></times><ci id="S4.p2.8.m7.1.1.3.2.cmml" xref="S4.p2.8.m7.1.1.3.2">ùëü</ci><ci id="S4.p2.8.m7.1.1.3.3.cmml" xref="S4.p2.8.m7.1.1.3.3">ùëí</ci><ci id="S4.p2.8.m7.1.1.3.4.cmml" xref="S4.p2.8.m7.1.1.3.4">ùëî</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.8.m7.1c">L_{reg}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.8.m7.1d">italic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT</annotation></semantics></math> by taking the average of 15 keypoints‚Äô <math alttext="\mathcal{L}_{1}" class="ltx_Math" display="inline" id="S4.p2.9.m8.1"><semantics id="S4.p2.9.m8.1a"><msub id="S4.p2.9.m8.1.1" xref="S4.p2.9.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p2.9.m8.1.1.2" xref="S4.p2.9.m8.1.1.2.cmml">‚Ñí</mi><mn id="S4.p2.9.m8.1.1.3" xref="S4.p2.9.m8.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p2.9.m8.1b"><apply id="S4.p2.9.m8.1.1.cmml" xref="S4.p2.9.m8.1.1"><csymbol cd="ambiguous" id="S4.p2.9.m8.1.1.1.cmml" xref="S4.p2.9.m8.1.1">subscript</csymbol><ci id="S4.p2.9.m8.1.1.2.cmml" xref="S4.p2.9.m8.1.1.2">‚Ñí</ci><cn id="S4.p2.9.m8.1.1.3.cmml" type="integer" xref="S4.p2.9.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.9.m8.1c">\mathcal{L}_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.9.m8.1d">caligraphic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, comparing the distance between predicted offsets and ground-truth offsets to the center location of a voxel in <math alttext="Y" class="ltx_Math" display="inline" id="S4.p2.10.m9.1"><semantics id="S4.p2.10.m9.1a"><mi id="S4.p2.10.m9.1.1" xref="S4.p2.10.m9.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S4.p2.10.m9.1b"><ci id="S4.p2.10.m9.1.1.cmml" xref="S4.p2.10.m9.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.10.m9.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S4.p2.10.m9.1d">italic_Y</annotation></semantics></math> when <math alttext="Y_{xyz}" class="ltx_Math" display="inline" id="S4.p2.11.m10.1"><semantics id="S4.p2.11.m10.1a"><msub id="S4.p2.11.m10.1.1" xref="S4.p2.11.m10.1.1.cmml"><mi id="S4.p2.11.m10.1.1.2" xref="S4.p2.11.m10.1.1.2.cmml">Y</mi><mrow id="S4.p2.11.m10.1.1.3" xref="S4.p2.11.m10.1.1.3.cmml"><mi id="S4.p2.11.m10.1.1.3.2" xref="S4.p2.11.m10.1.1.3.2.cmml">x</mi><mo id="S4.p2.11.m10.1.1.3.1" xref="S4.p2.11.m10.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.11.m10.1.1.3.3" xref="S4.p2.11.m10.1.1.3.3.cmml">y</mi><mo id="S4.p2.11.m10.1.1.3.1a" xref="S4.p2.11.m10.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.11.m10.1.1.3.4" xref="S4.p2.11.m10.1.1.3.4.cmml">z</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.11.m10.1b"><apply id="S4.p2.11.m10.1.1.cmml" xref="S4.p2.11.m10.1.1"><csymbol cd="ambiguous" id="S4.p2.11.m10.1.1.1.cmml" xref="S4.p2.11.m10.1.1">subscript</csymbol><ci id="S4.p2.11.m10.1.1.2.cmml" xref="S4.p2.11.m10.1.1.2">ùëå</ci><apply id="S4.p2.11.m10.1.1.3.cmml" xref="S4.p2.11.m10.1.1.3"><times id="S4.p2.11.m10.1.1.3.1.cmml" xref="S4.p2.11.m10.1.1.3.1"></times><ci id="S4.p2.11.m10.1.1.3.2.cmml" xref="S4.p2.11.m10.1.1.3.2">ùë•</ci><ci id="S4.p2.11.m10.1.1.3.3.cmml" xref="S4.p2.11.m10.1.1.3.3">ùë¶</ci><ci id="S4.p2.11.m10.1.1.3.4.cmml" xref="S4.p2.11.m10.1.1.3.4">ùëß</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.11.m10.1c">Y_{xyz}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.11.m10.1d">italic_Y start_POSTSUBSCRIPT italic_x italic_y italic_z end_POSTSUBSCRIPT</annotation></semantics></math> equals to <math alttext="1" class="ltx_Math" display="inline" id="S4.p2.12.m11.1"><semantics id="S4.p2.12.m11.1a"><mn id="S4.p2.12.m11.1.1" xref="S4.p2.12.m11.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.p2.12.m11.1b"><cn id="S4.p2.12.m11.1.1.cmml" type="integer" xref="S4.p2.12.m11.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.12.m11.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.p2.12.m11.1d">1</annotation></semantics></math>.
The final loss is a weighted sum of <math alttext="L_{class}" class="ltx_Math" display="inline" id="S4.p2.13.m12.1"><semantics id="S4.p2.13.m12.1a"><msub id="S4.p2.13.m12.1.1" xref="S4.p2.13.m12.1.1.cmml"><mi id="S4.p2.13.m12.1.1.2" xref="S4.p2.13.m12.1.1.2.cmml">L</mi><mrow id="S4.p2.13.m12.1.1.3" xref="S4.p2.13.m12.1.1.3.cmml"><mi id="S4.p2.13.m12.1.1.3.2" xref="S4.p2.13.m12.1.1.3.2.cmml">c</mi><mo id="S4.p2.13.m12.1.1.3.1" xref="S4.p2.13.m12.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.13.m12.1.1.3.3" xref="S4.p2.13.m12.1.1.3.3.cmml">l</mi><mo id="S4.p2.13.m12.1.1.3.1a" xref="S4.p2.13.m12.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.13.m12.1.1.3.4" xref="S4.p2.13.m12.1.1.3.4.cmml">a</mi><mo id="S4.p2.13.m12.1.1.3.1b" xref="S4.p2.13.m12.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.13.m12.1.1.3.5" xref="S4.p2.13.m12.1.1.3.5.cmml">s</mi><mo id="S4.p2.13.m12.1.1.3.1c" xref="S4.p2.13.m12.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.13.m12.1.1.3.6" xref="S4.p2.13.m12.1.1.3.6.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.13.m12.1b"><apply id="S4.p2.13.m12.1.1.cmml" xref="S4.p2.13.m12.1.1"><csymbol cd="ambiguous" id="S4.p2.13.m12.1.1.1.cmml" xref="S4.p2.13.m12.1.1">subscript</csymbol><ci id="S4.p2.13.m12.1.1.2.cmml" xref="S4.p2.13.m12.1.1.2">ùêø</ci><apply id="S4.p2.13.m12.1.1.3.cmml" xref="S4.p2.13.m12.1.1.3"><times id="S4.p2.13.m12.1.1.3.1.cmml" xref="S4.p2.13.m12.1.1.3.1"></times><ci id="S4.p2.13.m12.1.1.3.2.cmml" xref="S4.p2.13.m12.1.1.3.2">ùëê</ci><ci id="S4.p2.13.m12.1.1.3.3.cmml" xref="S4.p2.13.m12.1.1.3.3">ùëô</ci><ci id="S4.p2.13.m12.1.1.3.4.cmml" xref="S4.p2.13.m12.1.1.3.4">ùëé</ci><ci id="S4.p2.13.m12.1.1.3.5.cmml" xref="S4.p2.13.m12.1.1.3.5">ùë†</ci><ci id="S4.p2.13.m12.1.1.3.6.cmml" xref="S4.p2.13.m12.1.1.3.6">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.13.m12.1c">L_{class}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.13.m12.1d">italic_L start_POSTSUBSCRIPT italic_c italic_l italic_a italic_s italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="L_{reg}" class="ltx_Math" display="inline" id="S4.p2.14.m13.1"><semantics id="S4.p2.14.m13.1a"><msub id="S4.p2.14.m13.1.1" xref="S4.p2.14.m13.1.1.cmml"><mi id="S4.p2.14.m13.1.1.2" xref="S4.p2.14.m13.1.1.2.cmml">L</mi><mrow id="S4.p2.14.m13.1.1.3" xref="S4.p2.14.m13.1.1.3.cmml"><mi id="S4.p2.14.m13.1.1.3.2" xref="S4.p2.14.m13.1.1.3.2.cmml">r</mi><mo id="S4.p2.14.m13.1.1.3.1" xref="S4.p2.14.m13.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.14.m13.1.1.3.3" xref="S4.p2.14.m13.1.1.3.3.cmml">e</mi><mo id="S4.p2.14.m13.1.1.3.1a" xref="S4.p2.14.m13.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.p2.14.m13.1.1.3.4" xref="S4.p2.14.m13.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.14.m13.1b"><apply id="S4.p2.14.m13.1.1.cmml" xref="S4.p2.14.m13.1.1"><csymbol cd="ambiguous" id="S4.p2.14.m13.1.1.1.cmml" xref="S4.p2.14.m13.1.1">subscript</csymbol><ci id="S4.p2.14.m13.1.1.2.cmml" xref="S4.p2.14.m13.1.1.2">ùêø</ci><apply id="S4.p2.14.m13.1.1.3.cmml" xref="S4.p2.14.m13.1.1.3"><times id="S4.p2.14.m13.1.1.3.1.cmml" xref="S4.p2.14.m13.1.1.3.1"></times><ci id="S4.p2.14.m13.1.1.3.2.cmml" xref="S4.p2.14.m13.1.1.3.2">ùëü</ci><ci id="S4.p2.14.m13.1.1.3.3.cmml" xref="S4.p2.14.m13.1.1.3.3">ùëí</ci><ci id="S4.p2.14.m13.1.1.3.4.cmml" xref="S4.p2.14.m13.1.1.3.4">ùëî</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.14.m13.1c">L_{reg}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.14.m13.1d">italic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="425" id="S4.F7.g1" src="x7.png" width="748"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">Visualization of pose estimation results in test frames for six activities. The second row is the trajectory of the test sequence in BEV. The third row is the instance of the global 3D pose estimated result for each action. In both rows, the blue line is the ground truth, and the red line represents estimated results.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">15 obvious keypoints of a human body are selected, aligning with the body skeleton model of the Human3.6M dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib19" title="">19</a>]</cite>.
Each frame‚Äôs 3D pose annotation in the sequence is manually validated to ensure the quality of the dataset. Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S4.F7" title="Figure 7 ‚Ä£ 4 HRRadarPose ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">7</span></a> demonstrates the testing results across various levels of action complexity, presenting the feasibility of 3D pose estimation using the proposed baseline.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Evaluation metrics</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">We employ joint position error (JPE), which measures the mean of the Euclidean distance for every joint‚Äôs predicted keypoints and the ground truth keypoints. Localization results are reported by the mean of the root position error (MRPE) ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib42" title="">42</a>]</cite>, which is the JPE of the pelvis keypoint. For the evaluation of 3D HPE, we use Mean per joint position error (MPJPE)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib57" title="">57</a>]</cite> and absolute-MPJPE (Abs-MPJPE)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Baselines Comparison</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">We evaluate the performance of the HRRadarPose in global localization and 3D HPE on the RT-Pose dataset.
We compare our method with three baseline methods, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.T2" title="Table 2 ‚Ä£ Baselines Comparison ‚Ä£ 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">2</span></a>. mm-Pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib37" title="">37</a>]</cite> and mmMesh <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib48" title="">48</a>]</cite>, utilizing radar point clouds for 3D HPE, are re-implemented to test on the RT-Pose dataset.
Notably, mmMesh introduces a global localization model, combined with an RNN HPE module, showing more accurate estimation results in both MRPE and MPJPE compared to mm-Pose, which employs a simpler CNN model. RF-Pose 3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib55" title="">55</a>]</cite> uses the 4D radar tensor with a region proposal network to locate the subjects and estimate the 3D pose.
The experimental results show that our HRRadarPose outperforms other baseline methods. On the RT-Pose dataset, we obtain 9.93 cm in MPJPE and 9.91 cm in MRPE. We find that radar-tensor-based methods reach less error than the methods using radar point cloud.
We attribute this performance gap to complicated scenarios since it is difficult for point cloud-based methods to discriminate different key points in various poses.
The results highlight the importance of using 4D radar tensors, which preserve raw and rich spatial-temporal information for the HPE tasks.
An end-to-end architecture directly takes in 4D radar tensors to better face different human action scenarios in the real world.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.6.2.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S5.T2.2.1" style="font-size:90%;">Comparison of human localization and 3D pose estimation results of our HRRadarPose method and three baseline methods. <math alttext="\dagger" class="ltx_Math" display="inline" id="S5.T2.2.1.m1.1"><semantics id="S5.T2.2.1.m1.1b"><mo id="S5.T2.2.1.m1.1.1" xref="S5.T2.2.1.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.1.m1.1c"><ci id="S5.T2.2.1.m1.1.1.cmml" xref="S5.T2.2.1.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.1.m1.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.1.m1.1e">‚Ä†</annotation></semantics></math> represents the radar point cloud-based methods.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.4" style="width:303.5pt;height:69.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-45.2pt,10.3pt) scale(0.770399508829729,0.770399508829729) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.4.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.4.2.3.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.4.2.3.1.2">MRPE (cm)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.2.3.1.3">MPJPE (cm)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.2.3.1.4">Abs-MPJPE (cm)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.3.1.1.1">mm-Pose¬†<math alttext="\dagger" class="ltx_Math" display="inline" id="S5.T2.3.1.1.1.m1.1"><semantics id="S5.T2.3.1.1.1.m1.1a"><mo id="S5.T2.3.1.1.1.m1.1.1" xref="S5.T2.3.1.1.1.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.1.1.1.m1.1b"><ci id="S5.T2.3.1.1.1.m1.1.1.cmml" xref="S5.T2.3.1.1.1.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.1.1.1.m1.1d">‚Ä†</annotation></semantics></math>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib37" title="">37</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.3.1.1.2">102.28</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.1.3">21.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.1.4">102.7</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.4.2.2.1">mmMesh¬†<math alttext="\dagger" class="ltx_Math" display="inline" id="S5.T2.4.2.2.1.m1.1"><semantics id="S5.T2.4.2.2.1.m1.1a"><mo id="S5.T2.4.2.2.1.m1.1.1" xref="S5.T2.4.2.2.1.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.2.2.1.m1.1b"><ci id="S5.T2.4.2.2.1.m1.1.1.cmml" xref="S5.T2.4.2.2.1.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.2.2.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.2.2.1.m1.1d">‚Ä†</annotation></semantics></math>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib48" title="">48</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.4.2.2.2">62.69</th>
<td class="ltx_td ltx_align_center" id="S5.T2.4.2.2.3">13.78</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.2.2.4">66.24</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.2.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.4.2.4.1.1">RF-Pose3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib55" title="">55</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.4.2.4.1.2">29.17</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.2.4.1.3">18.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.2.4.1.4">38.05</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.2.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T2.4.2.5.2.1">HRRadarPose¬†(Ours)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T2.4.2.5.2.2"><span class="ltx_text ltx_font_bold" id="S5.T2.4.2.5.2.2.1">9.91</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.2.5.2.3"><span class="ltx_text ltx_font_bold" id="S5.T2.4.2.5.2.3.1">9.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.2.5.2.4"><span class="ltx_text ltx_font_bold" id="S5.T2.4.2.5.2.4.1">14.73</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.2.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S5.T3.3.2" style="font-size:90%;">Comparison of the performance predicted by the HRRadar Pose in different complexity level activities, presenting different key points of the pose, the results of localization, and total 3D pose estimation results.
The first group of rows lists the stand-related poses, while the other lists the walking-related poses. The complexity level increases from the top activity to the bottom within each group.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.4" style="width:433.6pt;height:122.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-39.3pt,11.1pt) scale(0.846499411334851,0.846499411334851) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.4.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T3.4.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4" id="S5.T3.4.1.1.1.2">JPE(cm)</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.4.1.1.1.3"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.1.1.1.4"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.1.1.1.5"></th>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="2" id="S5.T3.4.1.2.2.1"><span class="ltx_text" id="S5.T3.4.1.2.2.1.1">Actions</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.4.1.2.2.2">Thorax</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.4.1.2.2.3">Head</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.4.1.2.2.4">Ankle</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.4.1.2.2.5">Wrist</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.4.1.2.2.6"><span class="ltx_text" id="S5.T3.4.1.2.2.6.1">MRPE (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.4.1.2.2.7"><span class="ltx_text" id="S5.T3.4.1.2.2.7.1">MPJPE (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.4.1.2.2.8"><span class="ltx_text" id="S5.T3.4.1.2.2.8.1">Abs-MPJPE (cm)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.1.3.1">
<td class="ltx_td ltx_border_t" id="S5.T3.4.1.3.1.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.4.1.3.1.2">Waving Hand</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.3.1.3">2.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.3.1.4">3.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.3.1.5">3.61</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.1.3.1.6">25.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.1.3.1.7">3.35</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.3.1.8">6.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.3.1.9">7.97</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.4.2">
<td class="ltx_td" id="S5.T3.4.1.4.2.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.4.1.4.2.2">Lifting Leg</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.4.2.3">6.12</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.4.2.4">7.92</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.4.2.5">12.24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.4.1.4.2.6">8.88</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.4.1.4.2.7">7.55</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.4.2.8">7.06</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.4.2.9">10.37</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.5.3">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.5.3.1"><span class="ltx_text" id="S5.T3.4.1.5.3.1.1">Stand:</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.4.1.5.3.2">Random Pose</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.5.3.3">7.78</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.5.3.4">11.21</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.5.3.5">11.75</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.4.1.5.3.6">32.12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.4.1.5.3.7">7.67</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.5.3.8">12.47</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.5.3.9">15.12</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.6.4">
<td class="ltx_td ltx_border_t" id="S5.T3.4.1.6.4.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.4.1.6.4.2">Normal</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.6.4.3">4.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.6.4.4">6.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.6.4.5">13.52</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.1.6.4.6">15.98</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.1.6.4.7">13.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.6.4.8">8.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.6.4.9">16.92</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.7.5">
<td class="ltx_td" id="S5.T3.4.1.7.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.4.1.7.5.2">Siting</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.7.5.3">9.36</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.7.5.4">13.03</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.7.5.5">13.93</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.4.1.7.5.6">10.77</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.4.1.7.5.7">14.11</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.7.5.8">9.44</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.7.5.9">16.83</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.8.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.1.8.6.1"><span class="ltx_text" id="S5.T3.4.1.8.6.1.1">Walk:</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S5.T3.4.1.8.6.2">Waving Hand</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.1.8.6.3">5.74</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.1.8.6.4">10.37</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.1.8.6.5">13.53</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.4.1.8.6.6">43.16</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.4.1.8.6.7">12.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.1.8.6.8">15.28</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.1.8.6.9">21.22</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Action complexity Analysis</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">RT-pose dataset provides six difficulty levels of actions to simulate real-world scenarios, enabling comprehensive evaluation of localization and 3D HPE results.
We report the performance of the proposed HRRadarPose method in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.T3" title="Table 3 ‚Ä£ Baselines Comparison ‚Ä£ 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">3</span></a>. Generally, the joints along the human longitudinal axis, such as the thorax and the head, are more stable than those in the limbs.
This phenomenon may be attributed to the larger reflective surface of the body‚Äôs trunk compared to the limbs, providing a more reliable HPE based on radar signals.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p2.1">The MRPE of standing posture is below 7.67 cm, indicating that the pose complexity slightly influences the localization accuracy. Due to the chair‚Äôs strong radar reflection, distinguishing between human subjects and the chair poses a challenge for localization. The MPJPE of walking and sitting is 9.44 cm, showing our HRRadarPose model can reliably estimate the sitting pose. However, the MRPE of this action sequence achieves 14.11 cm, demonstrating that sitting is a key factor for evaluating the performance of localization. The MPJPE of simple actions, such as standing and waving, standing and lifting legs, are around 7 cm, and the walking is 8.93 cm, demonstrating that our HRRadarPose model is comparable to other recently published radar pose estimate methods ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib26" title="">26</a>]</cite>. However, the current model still encounters difficulty in estimating HPE in complex actions such as walking and waving.
Therefore, the RT-pose dataset is a challenging benchmark, which can enable HPE models to realistically learn diverse pose scenarios.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Qualitative Result</h4>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="372" id="S5.F8.g1" src="x8.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S5.F8.3.2" style="font-size:90%;">HRRadarPose Results in different conditions. (a), (b), and (c) are occluded by cardboard, cloth, and plastic pad, respectively. (d) is in low-lighting conditions</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.1">Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.F8" title="Figure 8 ‚Ä£ Qualitative Result ‚Ä£ 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">8</span></a> demonstrates the qualitative results of different occlusion conditions and a low-lighting instance.
Due to the hardware setting of our system, supporting synchronized two camera data, the subject‚Äôs pose can be visualized by another camera if one of them is occluded.
Therefore, the 3D annotation results can be generated even on occlusion conditions.
Our method provides reliable pose estimation results even when the radar module is blocked or with low-lighting conditions.
Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.T5" title="Table 5 ‚Ä£ Ablation Study ‚Ä£ 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">5</span></a> shows the HPE results of occlusion in normal outdoor conditions. Although the radar module is occluded, the mmWave signal is still robust enough as input for the HRRadarPose model to estimate HPE, demonstrating the characteristics of radar and the importance of using radar for HPE.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Ablation Study</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p1.1">To illustrate the advantage of modeling motion features using Doppler information by our HRRadarPose, we remove Doppler information by averaging 4D radar tensors along the Doppler axis and train the models with and without Doppler information using the same setup.
Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.T5" title="Table 5 ‚Ä£ Ablation Study ‚Ä£ 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">5</span></a> shows that with the inclusion of Doppler information, our HRRadarPose enjoys a performance gain due to its capability to extract motion features, enhancing keypoint localization.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.T5.fig1" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T5.fig1.1.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S5.T5.fig1.2.2" style="font-size:90%;"> HPE results from HRRadarPose in normal and occlusive scenarios.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.fig1.3" style="width:390.3pt;height:94.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(83.9pt,-20.4pt) scale(1.75395382032499,1.75395382032499) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.fig1.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.fig1.3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.fig1.3.1.1.1.1">Occlusion</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.fig1.3.1.1.1.2">MPJPE (cm)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.fig1.3.1.1.1.3">Abs-MPJPE (cm)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.fig1.3.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.fig1.3.1.2.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig1.3.1.2.1.2">9.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig1.3.1.2.1.3">14.52</td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig1.3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T5.fig1.3.1.3.2.1">‚úì</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig1.3.1.3.2.2">11.57</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig1.3.1.3.2.3">18.55</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.T5.fig2" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T5.fig2.1.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S5.T5.fig2.2.2" style="font-size:90%;">Ablation Study of Using Dopper information.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.fig2.3" style="width:390.3pt;height:98pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(87.6pt,-22.0pt) scale(1.81425808347222,1.81425808347222) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.fig2.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.fig2.3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.fig2.3.1.1.1.1">Doppler</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.fig2.3.1.1.1.2">MPJPE¬†(cm)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.fig2.3.1.1.1.3">Abs-MPJPE¬†(cm)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.fig2.3.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.fig2.3.1.2.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig2.3.1.2.1.2">11.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig2.3.1.2.1.3">22.34</td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig2.3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T5.fig2.3.1.3.2.1">‚úì</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig2.3.1.3.2.2">9.91</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig2.3.1.3.2.3">14.73</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T6.2.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S5.T6.3.2" style="font-size:90%;">Our Radar-Doppler Convolutional Block in HR-Net.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.4" style="width:281.9pt;height:47.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.5pt,3.3pt) scale(0.878625797953698,0.878625797953698) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T6.4.1.1.1.1">HRNet-Block-Design</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.4.1.1.1.2">MPJPE¬†(cm)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.4.1.1.1.3">Abs-MPJPE¬†(cm)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.4.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.4.1.2.1.1">Traditional Block (RGB-Vision)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.4.1.2.1.2">10.35</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.4.1.2.1.3">17.59</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T6.4.1.3.2.1">Ours (Radar-Doppler)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.4.1.3.2.2">9.93</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.4.1.3.2.3">14.73</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T7">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="S5.F9" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="276" id="S5.T7.1.g1" src="x9.png" width="582"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S5.F9.3.2" style="font-size:90%;">Illustration of two variants of CNN blocks in HRNet. (a) Ours (Radar-Doppler). (b) Traditional Block (RGB-Vision).</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.T7.fig1" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T7.fig1.1.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S5.T7.fig1.2.2" style="font-size:90%;">
Comparison of common 4D radar tensor-based HPE head and ours.
</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T7.fig1.3" style="width:390.3pt;height:69.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(44.4pt,-8.0pt) scale(1.29477883011831,1.29477883011831) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T7.fig1.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T7.fig1.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T7.fig1.3.1.1.1.1">HPE Head</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T7.fig1.3.1.1.1.2">MPJPE¬†(cm)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T7.fig1.3.1.1.1.3">Abs-MPJPE¬†(cm)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.fig1.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T7.fig1.3.1.2.1.1">RF-Pose3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib55" title="">55</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.fig1.3.1.2.1.2">15.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.fig1.3.1.2.1.3">18.08</td>
</tr>
<tr class="ltx_tr" id="S5.T7.fig1.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T7.fig1.3.1.3.2.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.fig1.3.1.3.2.2">10.24</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.fig1.3.1.3.2.3">17.63</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS0.SSS0.Px5.p2">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p2.1">Compared with RPM 2.0, our input data and backbone design are different.
The proposed HRRadarPose network extracts better feature representation from 4D radar tensors through modified convolutional blocks.
As shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.F9" title="Figure 9 ‚Ä£ Table 7 ‚Ä£ Ablation Study ‚Ä£ 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">9</span></a>, we replace the batch norm with the group norm of convolutional blocks in HRNet. Since we treat the channel-wise features as a Doppler axis, the group norm inherently handles the feature in different speed groups, which normalizes the data effectively.
Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.T6" title="Table 6 ‚Ä£ Ablation Study ‚Ä£ 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">6</span></a> shows the results, proving that the modified CNN block is more suitable for Doppler radar than the original HRNet.
Compared with our method, most previous work did not leverage Doppler information ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib50" title="">50</a>]</cite>.
We validate that motion features ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib27" title="">27</a>]</cite> are crucial for aiding deep learning models in capturing more human body characteristics over the environment.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px5.p3">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p3.1">To ablate our design choice of HPE heads, we use the same backbone, replace our pose estimation head with RF-Pose3D‚Äôs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#bib.bib55" title="">55</a>]</cite> head, and train both models for the same epochs.
RF-Pose3D formulates the HPE as a multi-keypoints classification problem, where their model outputs confidence distributions of every human body joint.
As shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2407.13930v1#S5.T7" title="Table 7 ‚Ä£ Ablation Study ‚Ä£ 5 Experiments ‚Ä£ RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark"><span class="ltx_text ltx_ref_tag">7</span></a>, our design is favorable in joint keypoints localization in the metric space, indicated by a lower Abs-MPJPE.
Moreover, lower MPJPE from our HPE head‚Äôs result validates stronger inter-joint relationships modeling capability.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitation and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">First, to leverage the input data types for training the 3D HPE model, the computational resource is a crucial part. 4D radar tensor in Cartesian coordinate consumes up to 100 MB per frame, which decreases the training speed.
Second, we limit the experimental data collection scope from 2 m to 8 m because the camera is hard to clearly capture 2D pose skeletons at a large distance and LiDAR can not easily collect completed point clouds in near vision due to the constraints of FoV.
Third, the proposed baseline HRRadarPose lacks robustness in accurately estimating poses during complex activities. Capturing human pose accurately on 4D radar tensor remains an unresolved challenge.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we propose RT-Pose, the first dataset for human pose estimation (HPE) with synchronized and calibrated 4D radar tensors, LiDAR point clouds, and RGB images.
RT-Pose provides cluttered scenarios and human activities of different complexity levels to enhance the difficulty of pose estimation for practical applications. In addition, having various modalities is beneficial for our semi-automatic 3D pose annotation optimization process. The proposed HRRadarPose is the first single-stage architecture designed for estimating human pose using 4D radar tensors as the input. The results indicate that the proposed HRradarPose outperforms previous works, in which datasets are collected with simpler actions and cleaner scenarios. Our work contributes to offering a three-modality dataset with 4D radar tensors for HPE, which is promising to be applied for complex actions and different scenes.
We hope this work encourages future development of 4D radar-based HPE methods.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank Yizhou Wang <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>http://yizhouwang.net/</span></span></span> for helping us build the data collection system and the radar data processing pipeline.
We thank Hou-I Liu <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>k39967.c@nycu.edu.tw</span></span></span>, Cheng-Yi Huang <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>n28121521@gs.ncku.edu.tw</span></span></span>, Yi Shiang Chen<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>m16121051@gs.ncku.edu.tw</span></span></span>, Chi-Yuan Chan<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>n26120587@gs.ncku.edu.tw</span></span></span>, and Yu-Hsien Lu<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>n26120587@gs.ncku.edu.tw</span></span></span> for helping us collect the RT-Pose dataset.
We thank the National Science and Technology Counci of the Republic of China, Taiwan, under Projects of NSTC 112-2917-I-006-007 for its funding support.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Adib, F., Hsu, C.Y., Mao, H., Katabi, D., Durand, F.: Rf-capture: Capturing the human figure through a wall. ACM SIGGRAPH Asia (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ahuja, K., Jiang, Y., Goel, M., Harrison, C.: Vid2doppler: Synthesizing doppler radar data from videos for training privacy-preserving activity recognition. In: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. pp. 1‚Äì10 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
An, S., Li, Y., Ogras, U.: mri: Multi-modal 3d human pose estimation dataset using mmwave, rgb-d, and inertial sensors. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) Advances in Neural Information Processing Systems. vol.¬†35, pp. 27414‚Äì27426. Curran Associates, Inc. (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
An, S., Ogras, U.Y.: Mars: mmwave-based assistive rehabilitation system for smart healthcare. ACM Transactions on Embedded Computing Systems (TECS) <span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">20</span>(5s), 1‚Äì22 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chai, W., Guo, X., Wang, G., Lu, Y.: Stablevideo: Text-driven consistency-aware diffusion video editing. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 23040‚Äì23050 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chai, W., Jiang, Z., Hwang, J.N., Wang, G.: Global adaptation meets local generalization: Unsupervised domain adaptation for 3d human pose estimation. arXiv preprint arXiv:2303.16456 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chen, A., Wang, X., Zhu, S., Li, Y., Chen, J., Ye, Q.: mmbody benchmark: 3d body reconstruction dataset and analysis for millimeter wave radar. In: Proceedings of the 30th ACM International Conference on Multimedia. pp. 3501‚Äì3510 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chen, K., Gabriel, P., Alasfour, A., Gong, C., Doyle, W.K., Devinsky, O., Friedman, D., Dugan, P., Melloni, L., Thesen, T., et¬†al.: Patient-specific pose estimation in clinical environments. IEEE journal of translational engineering in health and medicine <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">6</span>, 1‚Äì11 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Cheng, J.H., Chen, Y., Chang, T.Y., Lin, H.E., Wang, P.Y.C., Cheng, L.P.: Impossible staircase: Vertically real walking in an infinite virtual tower. In: 2021 IEEE Virtual Reality and 3D User Interfaces (VR). pp. 50‚Äì56 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Cheng, J.H., Kuan, S.Y., Liu, H.I., Latapie, H., Liu, G., Hwang, J.N.: Centerradarnet: Joint 3d object detection and tracking framework using 4d fmcw radar. In: 2024 IEEE International Conference on Image Processing (ICIP) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Dai, Y., Lin, Y., Lin, X., Wen, C., Xu, L., Yi, H., Shen, S., Ma, Y., Wang, C.: Sloper4d: A scene-aware dataset for global 4d human pose estimation in urban environments. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 682‚Äì692 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Dai, Y., Lin, Y., Wen, C., Shen, S., Xu, L., Yu, J., Ma, Y., Wang, C.: Hsc4d: Human-centered 4d scene capture in large-scale indoor-outdoor space using wearable imus and lidar. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6792‚Äì6802 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
De¬†Maio, A., Farina, A., Foglia, G.: Design and experimental validation of knowledge-based constant false alarm rate detectors. IET Radar, Sonar &amp; Navigation <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">1</span>(4), 308‚Äì316 (2007)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Endo, K., Ishikawa, T., Yamamoto, K., Ohtsuki, T.: Multi-person position estimation based on correlation between received signals using mimo fmcw radar. IEEE Access <span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">11</span>, 2610‚Äì2620 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Guzov, V., Mir, A., Sattler, T., Pons-Moll, G.: Human poseitioning system (hps): 3d human pose estimation and self-localization in large scenes from body-mounted sensors. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4318‚Äì4329 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Heath, R.W., Gonzalez-Prelcic, N., Rangan, S., Roh, W., Sayeed, A.M.: An overview of signal processing techniques for millimeter wave mimo systems. IEEE journal of selected topics in signal processing <span class="ltx_text ltx_font_bold" id="bib.bib16.1.1">10</span>(3), 436‚Äì453 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Huang, H.W., Yang, C.Y., Ramkumar, S., Huang, C.I., Hwang, J.N., Kim, P.K., Lee, K., Kim, K.: Observation centric and central distance recovery for athlete tracking. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 454‚Äì460 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Huang, H.Y., Ning, C.W., Wang, P.Y., Cheng, J.H., Cheng, L.P.: Haptic-go-round: A surrounding platform for encounter-type haptics in virtual reality experiences. In: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. CHI ‚Äô20, Association for Computing Machinery (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence <span class="ltx_text ltx_font_bold" id="bib.bib19.1.1">36</span>(7), 1325‚Äì1339 (2013)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jiang, Z., Chai, W., Li, L., Zhou, Z., Yang, C.Y., Hwang, J.N.: Unihpe: Towards unified human pose estimation via contrastive learning. arXiv preprint arXiv:2311.16477 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jiang, Z., Ji, H., Yang, C.Y., Hwang, J.N.: 2d human pose estimation calibration and keypoint visibility classification. In: ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 6095‚Äì6099. IEEE (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Jiang, Z., Zhou, Z., Li, L., Chai, W., Yang, C.Y., Hwang, J.N.: Back to optimization: Diffusion-based zero-shot 3d human pose estimation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 6142‚Äì6152 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Kim, Y., Alnujaim, I., Oh, D.: Human activity classification based on point clouds measured by millimeter wave mimo radar with deep recurrent neural networks. IEEE Sensors Journal <span class="ltx_text ltx_font_bold" id="bib.bib23.1.1">21</span>(12), 13522‚Äì13529 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Klauder, J.R., Price, A., Darlington, S., Albersheim, W.J.: The theory and design of chirp radars. Bell System Technical Journal <span class="ltx_text ltx_font_bold" id="bib.bib24.1.1">39</span>(4), 745‚Äì808 (1960)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kuan, S.Y., Cheng, J.H., Huang, H.W., Chai, W., Yang, C.Y., Wu, B.F., Hwang, J.N.: Boosting online 3d multi-object tracking through camera-radar cross check. In: IEEE Intelligent Vehicles Symposium (IV) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Lee, S.P., Kini, N.P., Peng, W.H., Ma, C.W., Hwang, J.N.: Hupr: A benchmark for human pose estimation using millimeter wave radar. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 5715‚Äì5724 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Lin, C.L., Chang, Y.M., Hung, C.C., Tu, C.D., Chuang, C.Y.: Position estimation and smooth tracking with a fuzzy-logic-based adaptive strong tracking kalman filter for capacitive touch panels. IEEE Transactions on Industrial Electronics <span class="ltx_text ltx_font_bold" id="bib.bib27.1.1">62</span>(8), 5097‚Äì5108 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Lin, C.L., Chiu, W.C., Chu, T.C., Ho, Y.H., Chen, F.H., Hsu, C.C., Hsieh, P.H., Chen, C.H., Lin, C.C.K., Sung, P.S., et¬†al.: Innovative head-mounted system based on inertial sensors and magnetometer for detecting falling movements. Sensors <span class="ltx_text ltx_font_bold" id="bib.bib28.1.1">20</span>(20), ¬†5774 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Lin, C.L., Ho, Y.H., Chiu, W.C., Chu, T.C., Liu, Y.H.: Innovative shoe-integrated system based on time-of-flight range sensors for fall detection on various terrains. IEEE Sensors Letters <span class="ltx_text ltx_font_bold" id="bib.bib29.1.1">5</span>(10), ¬†1‚Äì4 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Lin, H.Y., Chen, T.W.: Augmented reality with human body interaction based on monocular 3d pose estimation. In: International Conference on Advanced Concepts for Intelligent Vision Systems. pp. 321‚Äì331. Springer (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Liu, H., He, J.Y., Cheng, Z.Q., Xiang, W., Yang, Q., Chai, W., Wang, G., Bao, X., Luo, B., Geng, Y., et¬†al.: Posynda: Multi-hypothesis pose synthesis domain adaptation for robust 3d human pose estimation. In: Proceedings of the 31st ACM International Conference on Multimedia. pp. 5542‚Äì5551 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko, O., Xu, W., Theobalt, C.: Monocular 3d human pose estimation in the wild using improved cnn supervision. In: 2017 international conference on 3D vision (3DV). pp. 506‚Äì516. IEEE (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Neemat, S., Uysal, F., Krasnov, O., Yarovoy, A.: Reconfigurable range-doppler processing and range resolution improvement for fmcw radar. IEEE Sensors Journal <span class="ltx_text ltx_font_bold" id="bib.bib33.1.1">19</span>(20), 9294‚Äì9303 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Paek, D.H., Kong, S.H., Wijaya, K.T.: K-radar: 4d radar object detection for autonomous driving in various weather conditions. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib34.1.1">35</span>, 3819‚Äì3829 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Sengupta, A., Cao, S.: mmpose-nlp: A natural language processing approach to precise skeletal pose estimation using mmwave radars. IEEE Transactions on Neural Networks and Learning Systems (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Sengupta, A., Jin, F., Cao, S.: Nlp based skeletal pose estimation using mmwave radar point-cloud: A simulation approach. In: 2020 IEEE Radar Conference (RadarConf20). pp.¬†1‚Äì6. IEEE (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Sengupta, A., Jin, F., Zhang, R., Cao, S.: mm-pose: Real-time human skeletal posture estimation using mmwave radars and cnns. IEEE Sensors Journal <span class="ltx_text ltx_font_bold" id="bib.bib37.1.1">20</span>(17), 10032‚Äì10044 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution representation learning for human pose estimation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5693‚Äì5703 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Sun, Z., Ke, Q., Rahmani, H., Bennamoun, M., Wang, G., Liu, J.: Human action recognition from various data modalities: A review. IEEE transactions on pattern analysis and machine intelligence (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
V√©ges, M., L≈ërincz, A.: Absolute human pose estimation with depth prediction network. In: 2019 International Joint Conference on Neural Networks (IJCNN). pp.¬†1‚Äì7. IEEE (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Von¬†Marcard, T., Henschel, R., Black, M.J., Rosenhahn, B., Pons-Moll, G.: Recovering accurate 3d human pose in the wild using imus and a moving camera. In: Proceedings of the European conference on computer vision (ECCV). pp. 601‚Äì617 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Wang, J., Tan, S., Zhen, X., Xu, S., Zheng, F., He, Z., Shao, L.: Deep 3d human pose estimation: A review. Computer Vision and Image Understanding <span class="ltx_text ltx_font_bold" id="bib.bib42.1.1">210</span>, 103225 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan, M., Wang, X., Liu, W., Xiao, B.: Deep high-resolution representation learning for visual recognition. TPAMI (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan, M., Wang, X., et¬†al.: Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence <span class="ltx_text ltx_font_bold" id="bib.bib44.1.1">43</span>(10), 3349‚Äì3364 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Wang, Y., Cheng, J.H., Huang, J.T., Kuan, S.Y., Fu, Q., Ni, C., Hao, S., Wang, G., Xing, G., Liu, H., Hwang, J.N.: Vision meets mmwave radar: 3d object perception benchmark for autonomous driving. In: IEEE Intelligent Vehicles Symposium (IV) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Wang, Y., Jiang, Z., Li, Y., Hwang, J.N., Xing, G., Liu, H.: Rodnet: A real-time radar object detection network cross-supervised by camera-radar fused object 3d localization. IEEE Journal of Selected Topics in Signal Processing <span class="ltx_text ltx_font_bold" id="bib.bib46.1.1">15</span>(4), 954‚Äì967 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Xie, C., Zhang, D., Wu, Z., Yu, C., Hu, Y., Chen, Y.: Rpm 2.0: Rf-based pose machines for multi-person 3d pose estimation. IEEE Transactions on Circuits and Systems for Video Technology (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Xue, H., Ju, Y., Miao, C., Wang, Y., Wang, S., Zhang, A., Su, L.: mmmesh: Towards 3d real-time dynamic human mesh construction using millimeter-wave. In: Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services. pp. 269‚Äì282 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Yang, C.Y., Luo, J., Xia, L., Sun, Y., Qiao, N., Zhang, K., Jiang, Z., Hwang, J.N., Kuo, C.H.: Camerapose: Weakly-supervised monocular 3d human pose estimation by leveraging in-the-wild 2d annotations. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 2924‚Äì2933 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Yu, C., Zhang, D., Wu, Z., Xie, C., Lu, Z., Hu, Y., Chen, Y.: Mobirfpose: Portable rf-based 3d human pose camera. IEEE Transactions on Multimedia (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Yuan, Y., Wei, S.E., Simon, T., Kitani, K., Saragih, J.: Simpoe: Simulated character control for 3d human pose estimation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 7159‚Äì7169 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Zhang, G., Geng, X., Lin, Y.J.: Comprehensive mpoint: A method for 3d point cloud generation of human bodies utilizing fmcw mimo mm-wave radar. Sensors <span class="ltx_text ltx_font_bold" id="bib.bib52.1.1">21</span>(19), ¬†6455 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Zhang, Z., Chai, W., Jiang, Z., Ye, T., Song, M., Hwang, J.N., Wang, G.: Mpm: A unified 2d-3d human pose representation via masked pose modeling. arXiv preprint arXiv:2306.17201 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Zhao, M., Li, T., Abu¬†Alsheikh, M., Tian, Y., Zhao, H., Torralba, A., Katabi, D.: Through-wall human pose estimation using radio signals. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7356‚Äì7365 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Zhao, M., Tian, Y., Zhao, H., Alsheikh, M.A., Li, T., Hristov, R., Kabelac, Z., Katabi, D., Torralba, A.: Rf-based 3d skeletons. In: Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication. pp. 267‚Äì281 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Zhao, Y., Yarovoy, A., Fioranelli, F.: Angle-insensitive human motion and posture recognition based on 4d imaging radar and deep learning classifiers. IEEE Sensors Journal <span class="ltx_text ltx_font_bold" id="bib.bib56.1.1">22</span>(12), 12173‚Äì12182 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J., Kehtarnavaz, N., Shah, M.: Deep learning-based human pose estimation: A survey. ACM Computing Surveys <span class="ltx_text ltx_font_bold" id="bib.bib57.1.1">56</span>(1), 1‚Äì37 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Zheng, Z., Pan, J., Ni, Z., Shi, C., Ye, S., Fang, G.: Human posture reconstruction for through-the-wall radar imaging using convolutional neural networks. IEEE Geoscience and Remote Sensing Letters <span class="ltx_text ltx_font_bold" id="bib.bib58.1.1">19</span>, ¬†1‚Äì5 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Zhou, Z., Jiang, Z., Chai, W., Yang, C.Y., Li, L., Hwang, J.N.: Efficient domain adaptation via generative prior for 3d infant pose estimation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 41‚Äì49 (2024)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jul 18 22:26:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
