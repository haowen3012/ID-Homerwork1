<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model</title>
<!--Generated on Mon Aug 26 08:55:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.14111v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S1" title="In Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S2" title="In Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3" title="In Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.SS1" title="In 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>BAUST-BSL-38 Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.SS2" title="In 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>BdSL-38 Dataset <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">30</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.SS3" title="In 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>KU-BdSL <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">31</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4" title="In Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Proposed Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.SS1" title="In 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Hand Skeleton Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.SS2" title="In 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Separable TCN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.SS3" title="In 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Architecture for Branches of Spatial-Temporal Approach</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.SS3.SSS1" title="In 4.3 Architecture for Branches of Spatial-Temporal Approach ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Spatial-temporal position embedding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.SS3.SSS2" title="In 4.3 Architecture for Branches of Spatial-Temporal Approach ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Multihead Attention Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.SS3.SSS3" title="In 4.3 Architecture for Branches of Spatial-Temporal Approach ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Mask Operation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.SS4" title="In 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.SS5" title="In 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Dimension of Feature Map</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5" title="In Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.SS1" title="In 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Environmental Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.SS2" title="In 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evaluation metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.SS3" title="In 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Result Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.SS4" title="In 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>State of the Art Comparison of the proposed model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.SS5" title="In 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S6" title="In Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S7" title="In Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Sample Appendix Section</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abu Saleh Musa Miah
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Md. Al Mehedi Hasan
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Md Hadiuzzaman
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Muhammad Nazrul Islam
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jungpil Shin
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Hand gesture-based sign language recognition (SLR) is one of the most advanced applications of machine learning, and computer vision uses hand gestures. However, many standard automatic recognition systems have been developed for other languages such as English, Turkish, Arabic, British etc. However, there still are some challenges to achieving standard Bangla sign language (BSL) recognition systems. Although, in the past few years, many researchers have widely explored and studied how to address BSL problems, certain unaddressed issues still remain, such as skeleton- and transformer-based BSL recognition. In addition, the lack of evaluation of the BSL model in various concealed environmental conditions can prove the generalized property of the existing model by facing daily life signs. As a consequence, existing BSL recognition systems provide a limited perspective of their generalisation ability as they are tested on datasets containing few BSL alphabets that have a wide disparity in gestures and are easy to differentiate. To overcome these limitations, we propose a spatial-temporal attention-based BSL recognition model considering hand joint skeletons extracted from the sequence of images. The main aim of utilising hand skeleton-based BSL data is to ensure the privacy and low-resolution sequence of images, which need minimum computational cost and low hardware configurations. Our model captures discriminative structural displacements and short-range dependency based on unified joint features projected onto high-dimensional feature space. Specifically, the use of Separable TCN combined with a powerful multi-head spatial-temporal attention architecture generated high-performance accuracy. The extensive experiments with a proposed dataset and two benchmark BSL datasets with a wide range of evaluations, such as intra- and inter-dataset evaluation settings, demonstrated that our proposed models achieve competitive performance with extremely low computational complexity and run faster than existing models.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Spatial-temporal network , Depth wise separable CNN , Bangla Sign Language, Sign Language Recognition , Spatial-Temporal Attention , Hand Pose based BSL

</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>Nuclear Physics B</span></span></span>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\affiliation</span>
<p class="ltx_p" id="p1.2">[inst1]organization=School of Computer Science and Engineering,The University of Aizu,addressline=Address One,
city=Aizuwakamatsu,
postcode=965-8580,
state=Fukushima,
country=Japan.</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\affiliation</span>
<p class="ltx_p" id="p2.2">[inst2]organization=Department of Computer Science and Engineering,addressline=Rajshahi University of Engineering and Technology(RUET),
city=Rajshahi,
country=Bangladesh.
<span class="ltx_ERROR undefined" id="p2.2.1">\affiliation</span>[inst3]organization=Department of Computer Science and Engineering, Bangladesh Army University of Science and Technology(BAUST),addressline=Saidpur Cantonment, Saidpur,
city=Nilphamari,
postcode=Saidpur 5311,
state=Rangpur,
country=Bangladesh.
<span class="ltx_ERROR undefined" id="p2.2.2">\affiliation</span>[inst4]organization=Department of Computer Science and Engineering (CSE), Military Institute of Science and Technology (MIST),addressline=Mirpur Cantonment,
city=Dhaka,
postcode=Dhaka-1216,
country=Bangladesh.

</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Language is the primary medium through which one can express thoughts, ideas, and requirements to other people. Some groups are deprived of using language to express their thoughts or requirements or hear something from another person. These people are usually not able to establish communication through the common language. They use other ways to fulfil their requirements. In our society, we know them as deaf and hard-of-hearing people who are mainly incapable of speaking using oral language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib5" title="">5</a>]</cite>. In some cases, they are not completely incapable of speaking but do not like to speak because of the negative voice or undesired attention that is possible to attract a typical voice. This deafness mainly comes from birth or hearing injury, yielding to complete inability to hear or moderate inability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib7" title="">7</a>]</cite>. In addition, hearing loss or deafness is associated with ageing, noise exposure, genetics and different kinds of infections such as chronic ear, certain toxins and medications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib9" title="">9</a>]</cite>. According to a report from the World Health Organization (WHO), there are 1.33 billion people belonging to the deaf-mute community in the world, which is 18.5% of the world population <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib10" title="">10</a>]</cite> and, there are 300000 Bangladeshi people belonging to the deaf-mute community <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib12" title="">12</a>]</cite>. There are some treatments for hearing loss or deaf-mute problems if the deaf person cannot hear 25 decibels after the test  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib6" title="">6</a>]</cite>. However, most of the people in the deaf-mute community are not suitable for the treatment due to high decibels. In this situation, there are some alternative solutions for the deaf and mute community, such as hearing aids, augmentative communication devices, cochlear implants, subtitles, and sign language  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In such a situation, establishing communication using a special gesture known as sign language comes from utilized visual-manual systems such as body movement and hand movement, which convey meaningful information to interact between the deaf or mute community <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib16" title="">16</a>]</cite>. Although deaf people can generally learn sign language to establish communication among themselves, people are not interested in learning sign language to communicate with a specific group of people, like a deaf community. From this perspective, general people can get help from human interpreters. The scary and alarming side of the human interpreter is that it is costly and not easy to get expert human interpreters all the time when it is needed. As a consequence, the deaf and mute communities have been facing critical challenges in fulfilling their basic needs like medical series, socialization, education and employment interview process, etc. Besides sign language, there has been a growing demand for non-contact input interfaces in recent years because of COVID-19 and emergencies. The whole world focused on using touchless devices for every sector that allow users to input data without touching their hands to avoid various infections.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Therefore, one of the aims of the study is to use a webcam or a single camera as an input device for communication. It is very hygienic and is expected to reduce the risk of infections. The advantage of using a webcam is that there is no need to prepare the above expensive devices. Many laptops already have a camera installed, so there is no need to buy additional devices. To deal with the following challenges, sign language and hand gestures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib18" title="">18</a>]</cite>. Because of these reasons, automatic hand gestures and sign language recognition have become an important medium for both the deaf community and general people as a means of communication. Many researchers have been working to develop such automatic recognition systems by considering vision and sensor-based systems. Many researchers proposed sensor-based sign language recognition systems, among them hand gloves <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib21" title="">21</a>]</cite>, leap motion  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib22" title="">22</a>]</cite>, kinematic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib23" title="">23</a>]</cite> and accelerometers are most of them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib24" title="">24</a>]</cite>. Although a sensor-based system carries the efficiency of the skeleton data with limited movement, it needs a specialized device that is portable and has costly problems.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To solve the portability of the system, researchers focus on the vision-based system considering digital camera <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib25" title="">25</a>]</cite>, RGB depth camera  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib26" title="">26</a>]</cite>, web camera, stereo camera, or 3D camera <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib27" title="">27</a>]</cite>. Most of the sign language recognition systems have been developed using the vision-based approach with various machine learning and deep learning-based methodology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>]</cite>. Although many studies developed for another sign language, few models have been developed for BSL recognition because of the inadequacy of the freely available BSL benchmark dataset. Most of the existing models for the BSL recognition model have been evaluated with their own dataset. There may be difficulties in producing good performance with the versatile environmental test dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>]</cite>. There are few benchmark datasets available for BSL recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib31" title="">31</a>]</cite>. Rafi et al. created a benchmark BSL dataset named 38 BdSL dataset, which contains 12,160 samples; after evaluating with their deep learning algorithm, they achieved 89.60% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>]</cite>. Ishara-Lipi is another benchmark dataset for BSL recognition  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib28" title="">28</a>]</cite>. Abedin et al. proposed a concatenated CNN model to improve the performance of the BSL recognition system with the BdSL dataset, and they achieved 91.50%  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib32" title="">32</a>]</cite>. Although they applied different deep learning-based models,, the performance accuracy and efficiency of the systems may not be satisfactory for real-time implementation. To overcome the problems, Miah et al. employed a CNN-based BensignNet to improve the accuracy of the BSL recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib12" title="">12</a>]</cite>. They achieved 94.00% for the BdSL dataset and 98.20% for the KU-BdSL dataset. The main drawback of these systems is computation complexity, the complexity of the background and the lack of generalization. To overcome the generalization problems, Youme et al. proposed a CNN-based algorithm by including intra and inter-dataset evaluation settings in BSL recognition to increase the generalization property <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib33" title="">33</a>]</cite>. However these methods still face computation complexity problems and various other issues in achieving high accuracy for BSL recognition tasks like background noise, self-occlusion, partial occultation, redundant background, and viewpoint of light variation and variety of illuminations. By considering the challenges recently, many researchers employed the skeleton-based spatial-temporal attention model to recognize sign language and hand gestures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib36" title="">36</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">However, (i) the computational complexity of the existing attention-based model is so high, (ii) we did not find any skeleton-based model for the BSL recognition task, (iii) No existing methods of the BSL recognition focus on improving the generalization property except one. Although skeleton-based sign language data does not contain appearance information, it is robust to variations in viewpoint, lighting conditions, and backgrounds, making it the most suitable approach but not compromising privacy. To overcome the three challenges, we proposed a skeleton-based BSL recognition system using spatial-temporal multi-head attention followed by the Separable temporal convolutional network (Sep-TCN) to encode the robust spatiotemporal features. The main contribution of the proposed mode is given below:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">New Dataset: We created a new benchmark dataset, namely BAUST-BSL-38, to overcome the data lacking problem of the BSL domain.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Cost-effectiveness: We proposed an efficient BSL recognition system by extracting 2D skeleton key points from common images aiming to use low-specification cameras for monitoring. Experimental results proved the system’s efficiency across multiple datasets with different evaluation settings. To our knowledge, this is the first work to evaluate the BSL recognition system and other evaluations where spatiotemporal attention-based BSL is employed.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Time efficiency: As we proposed here, a SepTCN with the multi-branch of multi-head spatiotemporal attention-based feature aggregation with different attention-based networks to capture slow and fast spa-to-temporal articulations. Our model reduces the required number of parameters (1/2nd), which helps to reduce the computational complexity three times compared to existing architectures and the best-competing method. The extremely lower amount of the model parameter and the minimum floating point operations per second (FLOPS) proved that the proposed efficient system extracted powerful features without applying exorbitantly deep network layers as compared to other existing state-of-the-art spatial-temporal methods and without any noticeable drop in performance</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Generalization: Good performance with BSL recognition in evaluating inter and intra-dataset settings proved the proposed model’s lead as better-generalized solutions.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">This paper is organized as follows: Section <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S2" title="2 Related works ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">2</span></a> provides the relevant literature review. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3" title="3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">3</span></a> describes the benchmark dataset of hand skeletons used to develop this work. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4" title="4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a> describes the proposed multibranch spatial-temporal attention model. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5" title="5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">5</span></a> described the experimental results and different evaluation scenarios. Section VI concludes the paper, including some future work.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Although much research has been done on different sign language recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib2" title="">2</a>]</cite>, few are available for BSL recognition. Pixel and skeleton-based image recognition, hand pose estimation, and hand tracking recognition are mainly considered for sign language recognition. There are many researchers employed machine and deep learning algorithms for sign language recognition; among them, Aryanie et al. used Principal Component Analysis (PCA) for dimensionality reduction by including the K-Nearest Neighbor (KNN) algorithm as a classifier to recognize the American sign language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib43" title="">43</a>]</cite>.
Different researchers use support vector machine (SVM) to recognize different sign language based on local and global features, like 86% achieved for Japanese sign language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib44" title="">44</a>]</cite> and good performance achieved for Thai finger spelling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib45" title="">45</a>]</cite>. To classify Bangla sign language, many researchers used SVM, such as researchers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib46" title="">46</a>]</cite> who achieved 96.46% accuracy for 5400 images which is 6% higher than the KNN algorithm; another research team achieved good accuracy with HOG features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib47" title="">47</a>]</cite>. Uddin et al. l achieved 97.70% accuracy for the SVM with 2400 images based on the Gabor filter features, which is reduced with PCA algorithms  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib48" title="">48</a>]</cite>. Yasir et al. applied PCAN with Linear discriminant analysis (LDA) to maximize inter-class distance and minimize intra-class scatter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib49" title="">49</a>]</cite>. The same researchers extracted SIFT features from a vocabulary dataset and applied KNN and SVM to classify Bangla sign language (BSL). Although some researchers achieved good accuracy with machine learning algorithms, these systems face difficulties for large image or video datasets and most of the studies are based on the self-collection of small datasets, most of which are not available for comparative analysis.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Currently, CNN-based methods have been prosperous for sign language recognition because of the efficiency and accurate processing of large-scale complex vision information or large-size video datasets. Ahmed et al. proposed an artificial neural network (ANN) to recognize BSL based on the fingertip position feature-based concept <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib50" title="">50</a>]</cite>. Shafique et al. employed CNN-based deep learning architecture to recognize BSL recognition, where they recorded data from 25 different subjects and achieved good performance accuracy with different settings of evaluations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib51" title="">51</a>]</cite>. Hoque et al. proposed a BSL recognition system using region-based CNN, and they evaluated the model with a dataset consisting of 1000 gestures and achieved around 98.00% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib52" title="">52</a>]</cite>. Xception transformer employed by Urmee et al. to recognise BSL recognition based on 37 different gesture-based datasets and achieved around 98.00% with their dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib53" title="">53</a>]</cite>.
Moreover, CNN was employed by different researchers to recognize BSL, such as a virtual reality-based hand-tracking controller, which reduced the 2% error rate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib49" title="">49</a>]</cite>, digits classification and achieved 92.00% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib54" title="">54</a>]</cite>. To solve the inadequate dataset problems, Sanzidul et al. created a new benchmark dataset, Ishara-Lipi, with 36 gestures of BSL recognition, and they achieved 92.74% accuracy using a CNN model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>]</cite>. Hosssain et al. employed a CNN-based capsule network to improve the model’s performance and efficiency based on this dataset and achieved around 98.00% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib54" title="">54</a>]</cite>. They employed a general CNN model to classify BSL and achieved higher accuracy after evaluating the Ishara-Lipi dataset compared to the previous model. Some researchers used different kinds of transformers for BSL recognition to improve the efficiency and performance of the BSL recognition systems, such as Densenet201 architecture and zero-shot learning (ZSL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib55" title="">55</a>]</cite>, and VGG16 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib56" title="">56</a>]</cite>. Rafi et al. created a BSL benchmark dataset considering 38 alphabets, namely the BdSL-38 dataset, and after applying the VGG19 method, they achieved 89.60% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>]</cite>. Also, they employed a transformer-based network, but their performance was unsatisfactory. Abedin et al. employed deep learning-based concatenated CNN to overcome the challenges and achieved 91.50% accuracy with the previous 38BdSL dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib32" title="">32</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Also, they improved the performance accuracy, but their generalization property is not good because of the few validation settings. To increase the generalization property, miah et al. employed a CNN based BenSignNet model where they achieved 94.00% accuracy for the BdSL-38 dataset and good accuracy for the other two datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib12" title="">12</a>]</cite>. Although they achieved good performance and claimed the generalization property because of the good accuracy of the three datasets, they did not evaluate the inter-dataset or cross-evaluation criteria. Youme et al. proposed a CNN-based algorithm by including intra and inter-dataset evaluation set-tings in BSL recognition to increase the generalization property <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib33" title="">33</a>]</cite>. However, they worked to increase the generalization property of the system. Still, their computational complexity and efficiency are not good because they used pixel-based image information containing redundant background, light illumination, and partial occlusion issues. To overcome the background-related computational complexity problems more recently, researchers have focused on using a self-attention mechanism based on skeleton data instead of pixel-based information by reducing the long-range de-pendency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib59" title="">59</a>]</cite>. The main concept of the attention-based model is to extract the relationship among the pixel or skeleton points. They consider three types of simultaneous matrix, namely Query, Key, and Values, then multiply the first and third matrix, divide it by the third matrix, and use the softmax function to make a weight matrix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib35" title="">35</a>]</cite>. Some researchers combined the attention with other CNN, RNN, and LSTM networks, but it increased the long-range dependencies, which is not the main goal of the attention model  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib63" title="">63</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">In addition, one researcher combined the Spatial, temporal attention model with the residual CNN and achieved 89.20% and 93.60 accuracies for DHG and Shrec hand gesture datasets, respectively. Some researchers generated the idea of the Graph for the skeleton information and proposed a graph convolutional neural network(GCNN) to increase the efficiency and generalization property of the Sign language recognition system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib34" title="">34</a>]</cite>. Miah et al. proposed a multibranch of the Graph and general neural network to recognize hand gesture recognition by replacing actual graph structure with a deep learning-based structure. They achieved good accuracy for the gesture recognition for the DHG, SHREC and MSRA datasets. These methods also achieved good performance and efficiency in their model, but their computational complexity is still high. We proposed a skeleton-based BSL recognition system using a spatial-temporal attention model to overcome the various challenges. Where we used Sep-TCN before the spatial-temporal model to reduce the number of parameters of the model. To evaluate the model, we used the BSL dataset with inter- and intra-dataset evaluation settings to prove the system’s generalization property.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Bangla sign language (BSL) mainly comes from the American sign language (ASL) concept following the Bangla language. Although there are 39 consonants and 11 vowels in Bangla, researchers considered 38 alphabets among them as a BSL<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>]</cite>. These 38 alphabets were selected by collaborating with various deaf-mute community organizations, foundations, and educational institutions.
We experimented with three individual BSL datasets of this study, namely BAUST-BSL-38, 38 BdSL and KU-BdSL datasets described in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.SS1" title="3.1 BAUST-BSL-38 Dataset ‣ 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">3.1</span></a>, Section <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.SS2" title="3.2 BdSL-38 Dataset [30] ‣ 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">3.2</span></a>, and Section <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.SS3" title="3.3 KU-BdSL [31] ‣ 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">3.3</span></a>, respectively. The primary distinction between the BdSL dataset and the KU-BDSL dataset lies in the number of hand gesture signs they contain. Specifically, the BdSL dataset comprises 38 hand gesture signs used to represent the Bangla Sign Language Alphabet, while the KU-BDSL dataset includes 30 hand gesture signs. This disparity in the number of signs is a key differentiation between the two datasets.
In contrast, our newly created dataset also encompasses 38 hand gesture signs, aligning with the Bangla Sign Language alphabet, which mirrors the composition of the BDSL dataset. However, it is essential to highlight that the BDSL dataset has limitations in terms of the number of samples available, making it less suitable for comprehensive system experimentation and real-time deployment. To address this issue, we collected our proposed dataset from a deaf and dumb school, intentionally capturing data in diverse environments. This approach enhances the dataset’s ability to evaluate systems effectively and enables real-life deployment, particularly benefiting the deaf and dumb community.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>BAUST-BSL-38 Dataset</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Data acquisition is a crucial part of BSL recognition because it is not an efficient benchmark dataset for BSL recognition work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib21" title="">21</a>]</cite>. However, it is difficult to do this task because of the huge number of alphabets and various complexities in the BSL. To select the most usable alphabet, we used a BSL dictionary published by the National Centre for Special Education under the Ministry of Social Welfare, namely ’The Bangla Sign Language Dictionary’. To record the dataset, we collaborated with a deaf-mute organization, Proyash, Rangpur (A special education school). A sign language instructor of the proyash school, Rangpur, has instructed the subject to record our data collection. There are 38 standardized gestures for the 38 alphabets the national federation provides, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.F1" title="Figure 1 ‣ 3.1 BAUST-BSL-38 Dataset ‣ 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">1</span></a>. The proposed dataset has been recorded from mixed people of general and hearing-impaired people. In total, 15 people gave the data; among them, 10 are normal people who are a student at the Bangladesh Army University of Science and Technology (BAUST), and 5 are deaf people who are a student at the Proyas School, Rangpur. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.T1" title="Table 1 ‣ 3.1 BAUST-BSL-38 Dataset ‣ 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">1</span></a> describes the information of the people who participated in the data collection procedure. We recorded more than 600 samples for each of the 38 classes, and a total of around 22800 samples were collected. Multiple smartphone cameras have been used to collect the dataset; in most cases, it contains the single hand gesture dataset. Images were collected from the different complex backgrounds, and each image size was <math alttext="512\times 512" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">512</mn><mo id="S3.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn id="S3.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.2">512</cn><cn id="S3.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">512 × 512</annotation></semantics></math> pixels collected from different subjects. The sample image of the dataset is shown in Figure<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.F1" title="Figure 1 ‣ 3.1 BAUST-BSL-38 Dataset ‣ 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Dataset Overview</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="2" id="S3.T1.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">Gender</td>
<td class="ltx_td ltx_align_left ltx_border_t" colspan="2" id="S3.T1.1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">Hearing Imparity</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">Age Range</td>
<td class="ltx_td ltx_align_left ltx_border_t" colspan="2" id="S3.T1.1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">Sample</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">Male</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">Female</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">Deaf student</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">Normal student</td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">Individual sign</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.7" style="padding-left:3.0pt;padding-right:3.0pt;">Total</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">10</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">5</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">5</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">10</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">12-26</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">600</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.7" style="padding-left:3.0pt;padding-right:3.0pt;">22800</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="236" id="S3.F1.g1" src="extracted/5813252/BaustLipi38.jpg" width="638"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of our collected dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>BdSL-38 Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>]</cite>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">This is one of the most used benchmark datasets for BSL, which consists of 38 gestures of BSL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>]</cite>. This dataset was also recorded with the help of the National Federation of deaf people. They selected the 38 gestures by following the ’Bengali Sign Language Dictionary’. They recorded 320 images for each class, and in total, 12160 samples from the 320 people. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.F2" title="Figure 2 ‣ 3.2 BdSL-38 Dataset [30] ‣ 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrates the sample image of this dataset.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="200" id="S3.F2.g1" src="extracted/5813252/Dataset_38.jpg" width="540"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Sample images of the 38 BdSL Dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>KU-BdSL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib31" title="">31</a>]</cite>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">This is the other BSL dataset which name is the KU-BdSL Multi-scale sign language dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib31" title="">31</a>]</cite>. This dataset was recorded from 33 people, where 25 were male, and eight were female for 30 classes. Multiple smartphone cameras have been used to collect the dataset; in most cases, it contains the single hand gesture dataset. Images were collected from different complex backgrounds, and each image size was <math alttext="512\times 512" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">512</mn><mo id="S3.SS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><cn id="S3.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.2">512</cn><cn id="S3.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">512 × 512</annotation></semantics></math> pixels from different subjects. During the recording of the images, the hand image was placed in the middle position, which included versatile environmental conditions.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S3.F3" title="Figure 3 ‣ 3.3 KU-BdSL [31] ‣ 3 Dataset ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrates the sample image of the KU-BdSL dataset.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="234" id="S3.F3.g1" src="extracted/5813252/Dataset_KU.jpg" width="546"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Sample images of the KU BdSL Dataset Images.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Proposed Methodology</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The working flow architecture of the proposed method is presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.F4" title="Figure 4 ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a>. Our main aim of this study is to develop a BSL recognition model to improve the accuracy and efficiency of real-life implementation. Our target is to reduce the computational complexity in order of magnitude lower computational complexity (in terms of FLOPS) with fewer parameters (1/2) and minimum (1/3) times compared to existing architectures compared to the best-competing method. To do this, we first collected the Bangla sign language dataset and then extracted the palm’s joint key point from the image using a Media pipe. In the procedure, we extracted 2D 21 hand skeleton coordinates from BSL data images using a media pipe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib67" title="">67</a>]</cite>. Then we combined the long, long-range and short-range dependencies to improve the performance of the BSL recognition. In the hand skeleton case, we employed an attention-based spatial and temporal network. Firstly, we employed a depth-wise separable convolutional network to extract effective features and convert the single image into a sequence of images. Then we parallelly employed the three branches of the attention-based architecture to extract effective features from the skeleton-based Bangla sign language dataset.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S4.F4.2">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F4.1">-3cm-3cm
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="239" id="S4.F4.1.g1" src="x1.jpg" width="556"/>
</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Working Flow Architecture.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Hand Skeleton Estimation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The RGB image of the hand obtained from the web camera is the input to the joint estimator to estimate the 3D coordinates of the hand joint. Media pipe Hands is a reliable hand and finger tracking API developed by Google to estimate the hand skeleton coordinates of each joint from a camera  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib66" title="">66</a>]</cite>. It uses machine learning (ML) to understand 21 3D local hand marks from just one frame. The data obtained from the API consists of 21 points with x, y, and z coordinates. There is an order of these 21 coordinates as follows: The bottom point is the wrist which is the first coordinate, and from that, the thumb coordinates are in order from 1-4, namely thum cmc, thum mcp, thump ip and tip. After that, the index finger order is 5-8 from the wrist coordinate. In the same way, order 9-12 for the middle finger, 13-16 for the ring finger and 17-20 for the pinky fingers (see in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.F5" title="Figure 5 ‣ 4.1 Hand Skeleton Estimation ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">5</span></a>). Although the position of each point is not fixed on either wrist or others, the coordinated value of each point usually changes with the movement of the hand.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="S4.F5.g1" src="x2.jpg" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Mediapipe key points.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Separable TCN</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">A separable TCN, also known as a depthwise separable TCN, is a variant of the traditional Temporal Convolutional Network (TCN) architecture commonly used in deep learning for processing sequential data. The separable TCN is to reduce the number of parameters required by the traditional TCN while maintaining or improving its performance. In the traditional CNN, the kernel moves every time and needs K×K×C multiplication operations, exponentially increasing the total computation costs known as FLOPS. Separable TCN divided the full operation into two stages, namely depth-wise (DW) and pointwise (PW) convolution, which needs a minimum number of operations mathematically for each time DW needs K × K × 1, and PW needs 1 × 1 × C filter operation. This computation operation yields a sharply reduced number of operations for each kernel during the convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib68" title="">68</a>]</cite>. It also reduces the transpose operations in the convolution. Consequently, the total number of parameters of the proposed model is dramatically reduced without compromising the model’s performance and efficiency, which means this makes the model faster twice compared to the traditional system and counterparts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib69" title="">69</a>]</cite>. In our study, for the first SepTCN layer, we used a 3 × 1 filter with stride one, and for the second Sep-TCN layer, we applied 5 × 1 with stride 2, yielding results with the filter size of 3×1×1 and 1×1×1 for DW. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.F4" title="Figure 4 ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a>(b) demonstrated the filter size K × 1 × 1 where we did not compromise the original joint dimension during the computation because we assigned kernel value one instead of the K as traditional CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib69" title="">69</a>]</cite>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.F4" title="Figure 4 ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a>(b) visualizes the Sep-TCN, and Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.F6" title="Figure 6 ‣ 4.2 Separable TCN ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrates the details about the portion of the Sep-TCN where first extracted the local temporal proximity through the wider space. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.F4" title="Figure 4 ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a>(b) demonstrates the Sep-TCN, including the DW, PW and residual connection, where it is included max-pooling for enhancing the temporal feature. It mainly focuses on the dominant frame as a feature vector.</p>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S4.F6.2">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F6.1">-3cm-3cm
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="190" id="S4.F6.1.g1" src="x3.jpg" width="315"/></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Internal Structure of the separable TCN</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Architecture for Branches of Spatial-Temporal Approach </h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We previously mentioned that embedding the skeleton data with their corresponding position simultaneously provides a latent space that can be considered learning-specific feature information. After extracting the Sep-TCN feature, we applied three streams of the spatial-temporal branch to convey the corresponding private feature separately. We extracted the hand skeleton dataset from the original images using a media pipe where each image generated N joints for a single time frame. Each joint consists of a K-dimensional vector where K=3 (X, Y, and Z). For detailed understanding, we assume <math alttext="g" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_g</annotation></semantics></math> is the number of images, and after extracting skeleton points, it produced the T × N × K where t represents the time, N, K defines the number of joints and the number of channels or dimensionality. In our case, we consider T=4, N=21 and K=3.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">The first branch included one stage, spatial attention with spatial embedding, which investigates and discovers the relationship between the adjacent joints within a specific time frame. In the second stream, we included an attention model with temporal embedding where the main purpose of the second stream is to investigate and discover the interaction between the neighbour joint within more than one frame. The third branch sequentially combined two stages, namely spatial attention and temporal attention, where the first stage produced the spatial features and the second stage produced the temporal features. The third branch spatial attention stage’s main goal is to produce the spatial feature, then encode and discover the adjacent joint’s interaction with a certain frame. As a consequence, it can produce the fine-grained joint-level features of the specific gestures g. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.F7" title="Figure 7 ‣ 4.3 Architecture for Branches of Spatial-Temporal Approach ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">7</span></a> showed the inner structure of the attention and masking operation.</p>
</div>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S4.F7.2">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F7.1">-3cm-3cm
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S4.F7.1.g1" src="extracted/5813252/Attention.jpg" width="1110"/></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Internal Structure of Multihead Attention Mechanism</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Spatial-temporal position embedding</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">There are three streams of the spatial-temporal attention model where we encoded positional embedding to maintain the sequence of the joint information because there are no built-in functions in the attention model to keep the skeleton data in sequences. We can consider each gesture skeleton joint set as a data tensor to feed the attention-based spatial-temporal neural network. There is no predefined structure or order for this skeleton node to visualize the identity of the node, as a consequence, it is not possible to identify the node or gesture name. To solve a specific node’s identity crisis problem, we propose a spatial, temporal position embedding approach for generating the information based on the joint information. We performed the spatial-temporal position embedding by adding the extra dimension to the original skeleton dataset. Here, spatial embedding is a vector containing the position of all joints in a single frame. On the other hand, temporal embedding is a vector which consists of the position of the sequence of the frame in single pieces. To calculate the position embedding, we employed sine and cosine functions based on the different frequencies for embedding the position number for every skeleton node using the following Equation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib36" title="">36</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\begin{array}[]{l}P_{E}(p,2i)=sin\left(p/1000^{2i/C_{in}}\right)\\
P_{E}(p,2i+1)=cos\left(p/1000^{2i/C_{in}}\right)\end{array}" class="ltx_Math" display="block" id="S4.E1.m1.6"><semantics id="S4.E1.m1.6a"><mtable displaystyle="true" id="S4.E1.m1.6.6" rowspacing="0pt" xref="S4.E1.m1.6.6.cmml"><mtr id="S4.E1.m1.6.6a" xref="S4.E1.m1.6.6.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.6.6b" xref="S4.E1.m1.6.6.cmml"><mrow id="S4.E1.m1.3.3.3.3.3" xref="S4.E1.m1.3.3.3.3.3.cmml"><mrow id="S4.E1.m1.2.2.2.2.2.2" xref="S4.E1.m1.2.2.2.2.2.2.cmml"><msub id="S4.E1.m1.2.2.2.2.2.2.3" xref="S4.E1.m1.2.2.2.2.2.2.3.cmml"><mi id="S4.E1.m1.2.2.2.2.2.2.3.2" xref="S4.E1.m1.2.2.2.2.2.2.3.2.cmml">P</mi><mi id="S4.E1.m1.2.2.2.2.2.2.3.3" xref="S4.E1.m1.2.2.2.2.2.2.3.3.cmml">E</mi></msub><mo id="S4.E1.m1.2.2.2.2.2.2.2" xref="S4.E1.m1.2.2.2.2.2.2.2.cmml">⁢</mo><mrow id="S4.E1.m1.2.2.2.2.2.2.1.1" xref="S4.E1.m1.2.2.2.2.2.2.1.2.cmml"><mo id="S4.E1.m1.2.2.2.2.2.2.1.1.2" stretchy="false" xref="S4.E1.m1.2.2.2.2.2.2.1.2.cmml">(</mo><mi id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml">p</mi><mo id="S4.E1.m1.2.2.2.2.2.2.1.1.3" xref="S4.E1.m1.2.2.2.2.2.2.1.2.cmml">,</mo><mrow id="S4.E1.m1.2.2.2.2.2.2.1.1.1" xref="S4.E1.m1.2.2.2.2.2.2.1.1.1.cmml"><mn id="S4.E1.m1.2.2.2.2.2.2.1.1.1.2" xref="S4.E1.m1.2.2.2.2.2.2.1.1.1.2.cmml">2</mn><mo id="S4.E1.m1.2.2.2.2.2.2.1.1.1.1" xref="S4.E1.m1.2.2.2.2.2.2.1.1.1.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.2.2.2.2.1.1.1.3" xref="S4.E1.m1.2.2.2.2.2.2.1.1.1.3.cmml">i</mi></mrow><mo id="S4.E1.m1.2.2.2.2.2.2.1.1.4" stretchy="false" xref="S4.E1.m1.2.2.2.2.2.2.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.3.3.3.3.3.4" xref="S4.E1.m1.3.3.3.3.3.4.cmml">=</mo><mrow id="S4.E1.m1.3.3.3.3.3.3" xref="S4.E1.m1.3.3.3.3.3.3.cmml"><mi id="S4.E1.m1.3.3.3.3.3.3.3" xref="S4.E1.m1.3.3.3.3.3.3.3.cmml">s</mi><mo id="S4.E1.m1.3.3.3.3.3.3.2" xref="S4.E1.m1.3.3.3.3.3.3.2.cmml">⁢</mo><mi id="S4.E1.m1.3.3.3.3.3.3.4" xref="S4.E1.m1.3.3.3.3.3.3.4.cmml">i</mi><mo id="S4.E1.m1.3.3.3.3.3.3.2a" xref="S4.E1.m1.3.3.3.3.3.3.2.cmml">⁢</mo><mi id="S4.E1.m1.3.3.3.3.3.3.5" xref="S4.E1.m1.3.3.3.3.3.3.5.cmml">n</mi><mo id="S4.E1.m1.3.3.3.3.3.3.2b" xref="S4.E1.m1.3.3.3.3.3.3.2.cmml">⁢</mo><mrow id="S4.E1.m1.3.3.3.3.3.3.1.1" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.cmml"><mo id="S4.E1.m1.3.3.3.3.3.3.1.1.2" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.3.3.3.3.3.3.1.1.1" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.cmml"><mi id="S4.E1.m1.3.3.3.3.3.3.1.1.1.2" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.2.cmml">p</mi><mo id="S4.E1.m1.3.3.3.3.3.3.1.1.1.1" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.1.cmml">/</mo><msup id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.cmml"><mn id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.2" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.2.cmml">1000</mn><mrow id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.cmml"><mrow id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.cmml"><mn id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.2" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.2.cmml">2</mn><mo id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.1" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.3" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.3.cmml">i</mi></mrow><mo id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.1" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.1.cmml">/</mo><msub id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.cmml"><mi id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.2" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.2.cmml">C</mi><mrow id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.cmml"><mi id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.2" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.1" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.3" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.3.cmml">n</mi></mrow></msub></mrow></msup></mrow><mo id="S4.E1.m1.3.3.3.3.3.3.1.1.3" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S4.E1.m1.6.6c" xref="S4.E1.m1.6.6.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.6.6d" xref="S4.E1.m1.6.6.cmml"><mrow id="S4.E1.m1.6.6.6.3.3" xref="S4.E1.m1.6.6.6.3.3.cmml"><mrow id="S4.E1.m1.5.5.5.2.2.2" xref="S4.E1.m1.5.5.5.2.2.2.cmml"><msub id="S4.E1.m1.5.5.5.2.2.2.3" xref="S4.E1.m1.5.5.5.2.2.2.3.cmml"><mi id="S4.E1.m1.5.5.5.2.2.2.3.2" xref="S4.E1.m1.5.5.5.2.2.2.3.2.cmml">P</mi><mi id="S4.E1.m1.5.5.5.2.2.2.3.3" xref="S4.E1.m1.5.5.5.2.2.2.3.3.cmml">E</mi></msub><mo id="S4.E1.m1.5.5.5.2.2.2.2" xref="S4.E1.m1.5.5.5.2.2.2.2.cmml">⁢</mo><mrow id="S4.E1.m1.5.5.5.2.2.2.1.1" xref="S4.E1.m1.5.5.5.2.2.2.1.2.cmml"><mo id="S4.E1.m1.5.5.5.2.2.2.1.1.2" stretchy="false" xref="S4.E1.m1.5.5.5.2.2.2.1.2.cmml">(</mo><mi id="S4.E1.m1.4.4.4.1.1.1" xref="S4.E1.m1.4.4.4.1.1.1.cmml">p</mi><mo id="S4.E1.m1.5.5.5.2.2.2.1.1.3" xref="S4.E1.m1.5.5.5.2.2.2.1.2.cmml">,</mo><mrow id="S4.E1.m1.5.5.5.2.2.2.1.1.1" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.cmml"><mrow id="S4.E1.m1.5.5.5.2.2.2.1.1.1.2" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.cmml"><mn id="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.2" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.2.cmml">2</mn><mo id="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.1" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.1.cmml">⁢</mo><mi id="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.3" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.3.cmml">i</mi></mrow><mo id="S4.E1.m1.5.5.5.2.2.2.1.1.1.1" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.1.cmml">+</mo><mn id="S4.E1.m1.5.5.5.2.2.2.1.1.1.3" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S4.E1.m1.5.5.5.2.2.2.1.1.4" stretchy="false" xref="S4.E1.m1.5.5.5.2.2.2.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.6.6.6.3.3.4" xref="S4.E1.m1.6.6.6.3.3.4.cmml">=</mo><mrow id="S4.E1.m1.6.6.6.3.3.3" xref="S4.E1.m1.6.6.6.3.3.3.cmml"><mi id="S4.E1.m1.6.6.6.3.3.3.3" xref="S4.E1.m1.6.6.6.3.3.3.3.cmml">c</mi><mo id="S4.E1.m1.6.6.6.3.3.3.2" xref="S4.E1.m1.6.6.6.3.3.3.2.cmml">⁢</mo><mi id="S4.E1.m1.6.6.6.3.3.3.4" xref="S4.E1.m1.6.6.6.3.3.3.4.cmml">o</mi><mo id="S4.E1.m1.6.6.6.3.3.3.2a" xref="S4.E1.m1.6.6.6.3.3.3.2.cmml">⁢</mo><mi id="S4.E1.m1.6.6.6.3.3.3.5" xref="S4.E1.m1.6.6.6.3.3.3.5.cmml">s</mi><mo id="S4.E1.m1.6.6.6.3.3.3.2b" xref="S4.E1.m1.6.6.6.3.3.3.2.cmml">⁢</mo><mrow id="S4.E1.m1.6.6.6.3.3.3.1.1" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.cmml"><mo id="S4.E1.m1.6.6.6.3.3.3.1.1.2" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.6.6.6.3.3.3.1.1.1" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.cmml"><mi id="S4.E1.m1.6.6.6.3.3.3.1.1.1.2" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.2.cmml">p</mi><mo id="S4.E1.m1.6.6.6.3.3.3.1.1.1.1" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.1.cmml">/</mo><msup id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.cmml"><mn id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.2" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.2.cmml">1000</mn><mrow id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.cmml"><mrow id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.cmml"><mn id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.2" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.2.cmml">2</mn><mo id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.1" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.3" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.3.cmml">i</mi></mrow><mo id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.1" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.1.cmml">/</mo><msub id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.cmml"><mi id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.2" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.2.cmml">C</mi><mrow id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.cmml"><mi id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.2" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.1" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.3" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.3.cmml">n</mi></mrow></msub></mrow></msup></mrow><mo id="S4.E1.m1.6.6.6.3.3.3.1.1.3" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S4.E1.m1.6b"><matrix id="S4.E1.m1.6.6.cmml" xref="S4.E1.m1.6.6"><matrixrow id="S4.E1.m1.6.6a.cmml" xref="S4.E1.m1.6.6"><apply id="S4.E1.m1.3.3.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3.3"><eq id="S4.E1.m1.3.3.3.3.3.4.cmml" xref="S4.E1.m1.3.3.3.3.3.4"></eq><apply id="S4.E1.m1.2.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.2.2"><times id="S4.E1.m1.2.2.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.2.2.2"></times><apply id="S4.E1.m1.2.2.2.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.2.2.3.1.cmml" xref="S4.E1.m1.2.2.2.2.2.2.3">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.2.2.3.2.cmml" xref="S4.E1.m1.2.2.2.2.2.2.3.2">𝑃</ci><ci id="S4.E1.m1.2.2.2.2.2.2.3.3.cmml" xref="S4.E1.m1.2.2.2.2.2.2.3.3">𝐸</ci></apply><interval closure="open" id="S4.E1.m1.2.2.2.2.2.2.1.2.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.1"><ci id="S4.E1.m1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1">𝑝</ci><apply id="S4.E1.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.1.1"><times id="S4.E1.m1.2.2.2.2.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.1.1.1"></times><cn id="S4.E1.m1.2.2.2.2.2.2.1.1.1.2.cmml" type="integer" xref="S4.E1.m1.2.2.2.2.2.2.1.1.1.2">2</cn><ci id="S4.E1.m1.2.2.2.2.2.2.1.1.1.3.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.1.1.3">𝑖</ci></apply></interval></apply><apply id="S4.E1.m1.3.3.3.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3.3.3"><times id="S4.E1.m1.3.3.3.3.3.3.2.cmml" xref="S4.E1.m1.3.3.3.3.3.3.2"></times><ci id="S4.E1.m1.3.3.3.3.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3.3.3.3">𝑠</ci><ci id="S4.E1.m1.3.3.3.3.3.3.4.cmml" xref="S4.E1.m1.3.3.3.3.3.3.4">𝑖</ci><ci id="S4.E1.m1.3.3.3.3.3.3.5.cmml" xref="S4.E1.m1.3.3.3.3.3.3.5">𝑛</ci><apply id="S4.E1.m1.3.3.3.3.3.3.1.1.1.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1"><divide id="S4.E1.m1.3.3.3.3.3.3.1.1.1.1.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.1"></divide><ci id="S4.E1.m1.3.3.3.3.3.3.1.1.1.2.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.2">𝑝</ci><apply id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3">superscript</csymbol><cn id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.2.cmml" type="integer" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.2">1000</cn><apply id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3"><divide id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.1.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.1"></divide><apply id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2"><times id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.1.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.1"></times><cn id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.2.cmml" type="integer" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.2">2</cn><ci id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.3.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.2.3">𝑖</ci></apply><apply id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3">subscript</csymbol><ci id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.2">𝐶</ci><apply id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3"><times id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.1.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.1"></times><ci id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.2.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.2">𝑖</ci><ci id="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3.3.3.1.1.1.3.3.3.3.3">𝑛</ci></apply></apply></apply></apply></apply></apply></apply></matrixrow><matrixrow id="S4.E1.m1.6.6b.cmml" xref="S4.E1.m1.6.6"><apply id="S4.E1.m1.6.6.6.3.3.cmml" xref="S4.E1.m1.6.6.6.3.3"><eq id="S4.E1.m1.6.6.6.3.3.4.cmml" xref="S4.E1.m1.6.6.6.3.3.4"></eq><apply id="S4.E1.m1.5.5.5.2.2.2.cmml" xref="S4.E1.m1.5.5.5.2.2.2"><times id="S4.E1.m1.5.5.5.2.2.2.2.cmml" xref="S4.E1.m1.5.5.5.2.2.2.2"></times><apply id="S4.E1.m1.5.5.5.2.2.2.3.cmml" xref="S4.E1.m1.5.5.5.2.2.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.5.5.5.2.2.2.3.1.cmml" xref="S4.E1.m1.5.5.5.2.2.2.3">subscript</csymbol><ci id="S4.E1.m1.5.5.5.2.2.2.3.2.cmml" xref="S4.E1.m1.5.5.5.2.2.2.3.2">𝑃</ci><ci id="S4.E1.m1.5.5.5.2.2.2.3.3.cmml" xref="S4.E1.m1.5.5.5.2.2.2.3.3">𝐸</ci></apply><interval closure="open" id="S4.E1.m1.5.5.5.2.2.2.1.2.cmml" xref="S4.E1.m1.5.5.5.2.2.2.1.1"><ci id="S4.E1.m1.4.4.4.1.1.1.cmml" xref="S4.E1.m1.4.4.4.1.1.1">𝑝</ci><apply id="S4.E1.m1.5.5.5.2.2.2.1.1.1.cmml" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1"><plus id="S4.E1.m1.5.5.5.2.2.2.1.1.1.1.cmml" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.1"></plus><apply id="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.cmml" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.2"><times id="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.1.cmml" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.1"></times><cn id="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.2.cmml" type="integer" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.2">2</cn><ci id="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.3.cmml" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.2.3">𝑖</ci></apply><cn id="S4.E1.m1.5.5.5.2.2.2.1.1.1.3.cmml" type="integer" xref="S4.E1.m1.5.5.5.2.2.2.1.1.1.3">1</cn></apply></interval></apply><apply id="S4.E1.m1.6.6.6.3.3.3.cmml" xref="S4.E1.m1.6.6.6.3.3.3"><times id="S4.E1.m1.6.6.6.3.3.3.2.cmml" xref="S4.E1.m1.6.6.6.3.3.3.2"></times><ci id="S4.E1.m1.6.6.6.3.3.3.3.cmml" xref="S4.E1.m1.6.6.6.3.3.3.3">𝑐</ci><ci id="S4.E1.m1.6.6.6.3.3.3.4.cmml" xref="S4.E1.m1.6.6.6.3.3.3.4">𝑜</ci><ci id="S4.E1.m1.6.6.6.3.3.3.5.cmml" xref="S4.E1.m1.6.6.6.3.3.3.5">𝑠</ci><apply id="S4.E1.m1.6.6.6.3.3.3.1.1.1.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1"><divide id="S4.E1.m1.6.6.6.3.3.3.1.1.1.1.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.1"></divide><ci id="S4.E1.m1.6.6.6.3.3.3.1.1.1.2.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.2">𝑝</ci><apply id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.1.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3">superscript</csymbol><cn id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.2.cmml" type="integer" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.2">1000</cn><apply id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3"><divide id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.1.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.1"></divide><apply id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2"><times id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.1.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.1"></times><cn id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.2.cmml" type="integer" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.2">2</cn><ci id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.3.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.2.3">𝑖</ci></apply><apply id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3">subscript</csymbol><ci id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.2">𝐶</ci><apply id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3"><times id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.1.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.1"></times><ci id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.2.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.2">𝑖</ci><ci id="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.3.cmml" xref="S4.E1.m1.6.6.6.3.3.3.1.1.1.3.3.3.3.3">𝑛</ci></apply></apply></apply></apply></apply></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.6c">\begin{array}[]{l}P_{E}(p,2i)=sin\left(p/1000^{2i/C_{in}}\right)\\
P_{E}(p,2i+1)=cos\left(p/1000^{2i/C_{in}}\right)\end{array}</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.6d">start_ARRAY start_ROW start_CELL italic_P start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT ( italic_p , 2 italic_i ) = italic_s italic_i italic_n ( italic_p / 1000 start_POSTSUPERSCRIPT 2 italic_i / italic_C start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) end_CELL end_ROW start_ROW start_CELL italic_P start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT ( italic_p , 2 italic_i + 1 ) = italic_c italic_o italic_s ( italic_p / 1000 start_POSTSUPERSCRIPT 2 italic_i / italic_C start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) end_CELL end_ROW end_ARRAY</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p3">
<p class="ltx_p" id="S4.SS3.SSS1.p3.1">Here i and p defined the position of each individual element, the sin function we employed to calculate the position embedding for the odd index, and the cos function we used to calculate the position for the odd index. After calculating the position embedding, we added the position embedding with the initial feature vector generated by the Sep-TCN network. Consequently, the embedded value of the feature vector associated with the corresponding position identity value is generated. As we employed here, three different modules of the spatial-temporal approach produced the three different kinds of vectors, which are defined by the following formulas:</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p4">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\bar{f}_{ST(t,i)}=A_{S}\left(f_{(t,i)}+P^{S}_{(t,i)}\right)" class="ltx_Math" display="block" id="S4.E2.m1.7"><semantics id="S4.E2.m1.7a"><mrow id="S4.E2.m1.7.7" xref="S4.E2.m1.7.7.cmml"><msub id="S4.E2.m1.7.7.3" xref="S4.E2.m1.7.7.3.cmml"><mover accent="true" id="S4.E2.m1.7.7.3.2" xref="S4.E2.m1.7.7.3.2.cmml"><mi id="S4.E2.m1.7.7.3.2.2" xref="S4.E2.m1.7.7.3.2.2.cmml">f</mi><mo id="S4.E2.m1.7.7.3.2.1" xref="S4.E2.m1.7.7.3.2.1.cmml">¯</mo></mover><mrow id="S4.E2.m1.2.2.2" xref="S4.E2.m1.2.2.2.cmml"><mi id="S4.E2.m1.2.2.2.4" xref="S4.E2.m1.2.2.2.4.cmml">S</mi><mo id="S4.E2.m1.2.2.2.3" xref="S4.E2.m1.2.2.2.3.cmml">⁢</mo><mi id="S4.E2.m1.2.2.2.5" xref="S4.E2.m1.2.2.2.5.cmml">T</mi><mo id="S4.E2.m1.2.2.2.3a" xref="S4.E2.m1.2.2.2.3.cmml">⁢</mo><mrow id="S4.E2.m1.2.2.2.6.2" xref="S4.E2.m1.2.2.2.6.1.cmml"><mo id="S4.E2.m1.2.2.2.6.2.1" stretchy="false" xref="S4.E2.m1.2.2.2.6.1.cmml">(</mo><mi id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml">t</mi><mo id="S4.E2.m1.2.2.2.6.2.2" xref="S4.E2.m1.2.2.2.6.1.cmml">,</mo><mi id="S4.E2.m1.2.2.2.2" xref="S4.E2.m1.2.2.2.2.cmml">i</mi><mo id="S4.E2.m1.2.2.2.6.2.3" stretchy="false" xref="S4.E2.m1.2.2.2.6.1.cmml">)</mo></mrow></mrow></msub><mo id="S4.E2.m1.7.7.2" xref="S4.E2.m1.7.7.2.cmml">=</mo><mrow id="S4.E2.m1.7.7.1" xref="S4.E2.m1.7.7.1.cmml"><msub id="S4.E2.m1.7.7.1.3" xref="S4.E2.m1.7.7.1.3.cmml"><mi id="S4.E2.m1.7.7.1.3.2" xref="S4.E2.m1.7.7.1.3.2.cmml">A</mi><mi id="S4.E2.m1.7.7.1.3.3" xref="S4.E2.m1.7.7.1.3.3.cmml">S</mi></msub><mo id="S4.E2.m1.7.7.1.2" xref="S4.E2.m1.7.7.1.2.cmml">⁢</mo><mrow id="S4.E2.m1.7.7.1.1.1" xref="S4.E2.m1.7.7.1.1.1.1.cmml"><mo id="S4.E2.m1.7.7.1.1.1.2" xref="S4.E2.m1.7.7.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.7.7.1.1.1.1" xref="S4.E2.m1.7.7.1.1.1.1.cmml"><msub id="S4.E2.m1.7.7.1.1.1.1.2" xref="S4.E2.m1.7.7.1.1.1.1.2.cmml"><mi id="S4.E2.m1.7.7.1.1.1.1.2.2" xref="S4.E2.m1.7.7.1.1.1.1.2.2.cmml">f</mi><mrow id="S4.E2.m1.4.4.2.4" xref="S4.E2.m1.4.4.2.3.cmml"><mo id="S4.E2.m1.4.4.2.4.1" stretchy="false" xref="S4.E2.m1.4.4.2.3.cmml">(</mo><mi id="S4.E2.m1.3.3.1.1" xref="S4.E2.m1.3.3.1.1.cmml">t</mi><mo id="S4.E2.m1.4.4.2.4.2" xref="S4.E2.m1.4.4.2.3.cmml">,</mo><mi id="S4.E2.m1.4.4.2.2" xref="S4.E2.m1.4.4.2.2.cmml">i</mi><mo id="S4.E2.m1.4.4.2.4.3" stretchy="false" xref="S4.E2.m1.4.4.2.3.cmml">)</mo></mrow></msub><mo id="S4.E2.m1.7.7.1.1.1.1.1" xref="S4.E2.m1.7.7.1.1.1.1.1.cmml">+</mo><msubsup id="S4.E2.m1.7.7.1.1.1.1.3" xref="S4.E2.m1.7.7.1.1.1.1.3.cmml"><mi id="S4.E2.m1.7.7.1.1.1.1.3.2.2" xref="S4.E2.m1.7.7.1.1.1.1.3.2.2.cmml">P</mi><mrow id="S4.E2.m1.6.6.2.4" xref="S4.E2.m1.6.6.2.3.cmml"><mo id="S4.E2.m1.6.6.2.4.1" stretchy="false" xref="S4.E2.m1.6.6.2.3.cmml">(</mo><mi id="S4.E2.m1.5.5.1.1" xref="S4.E2.m1.5.5.1.1.cmml">t</mi><mo id="S4.E2.m1.6.6.2.4.2" xref="S4.E2.m1.6.6.2.3.cmml">,</mo><mi id="S4.E2.m1.6.6.2.2" xref="S4.E2.m1.6.6.2.2.cmml">i</mi><mo id="S4.E2.m1.6.6.2.4.3" stretchy="false" xref="S4.E2.m1.6.6.2.3.cmml">)</mo></mrow><mi id="S4.E2.m1.7.7.1.1.1.1.3.2.3" xref="S4.E2.m1.7.7.1.1.1.1.3.2.3.cmml">S</mi></msubsup></mrow><mo id="S4.E2.m1.7.7.1.1.1.3" xref="S4.E2.m1.7.7.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.7b"><apply id="S4.E2.m1.7.7.cmml" xref="S4.E2.m1.7.7"><eq id="S4.E2.m1.7.7.2.cmml" xref="S4.E2.m1.7.7.2"></eq><apply id="S4.E2.m1.7.7.3.cmml" xref="S4.E2.m1.7.7.3"><csymbol cd="ambiguous" id="S4.E2.m1.7.7.3.1.cmml" xref="S4.E2.m1.7.7.3">subscript</csymbol><apply id="S4.E2.m1.7.7.3.2.cmml" xref="S4.E2.m1.7.7.3.2"><ci id="S4.E2.m1.7.7.3.2.1.cmml" xref="S4.E2.m1.7.7.3.2.1">¯</ci><ci id="S4.E2.m1.7.7.3.2.2.cmml" xref="S4.E2.m1.7.7.3.2.2">𝑓</ci></apply><apply id="S4.E2.m1.2.2.2.cmml" xref="S4.E2.m1.2.2.2"><times id="S4.E2.m1.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.3"></times><ci id="S4.E2.m1.2.2.2.4.cmml" xref="S4.E2.m1.2.2.2.4">𝑆</ci><ci id="S4.E2.m1.2.2.2.5.cmml" xref="S4.E2.m1.2.2.2.5">𝑇</ci><interval closure="open" id="S4.E2.m1.2.2.2.6.1.cmml" xref="S4.E2.m1.2.2.2.6.2"><ci id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1">𝑡</ci><ci id="S4.E2.m1.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2">𝑖</ci></interval></apply></apply><apply id="S4.E2.m1.7.7.1.cmml" xref="S4.E2.m1.7.7.1"><times id="S4.E2.m1.7.7.1.2.cmml" xref="S4.E2.m1.7.7.1.2"></times><apply id="S4.E2.m1.7.7.1.3.cmml" xref="S4.E2.m1.7.7.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.7.7.1.3.1.cmml" xref="S4.E2.m1.7.7.1.3">subscript</csymbol><ci id="S4.E2.m1.7.7.1.3.2.cmml" xref="S4.E2.m1.7.7.1.3.2">𝐴</ci><ci id="S4.E2.m1.7.7.1.3.3.cmml" xref="S4.E2.m1.7.7.1.3.3">𝑆</ci></apply><apply id="S4.E2.m1.7.7.1.1.1.1.cmml" xref="S4.E2.m1.7.7.1.1.1"><plus id="S4.E2.m1.7.7.1.1.1.1.1.cmml" xref="S4.E2.m1.7.7.1.1.1.1.1"></plus><apply id="S4.E2.m1.7.7.1.1.1.1.2.cmml" xref="S4.E2.m1.7.7.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.7.7.1.1.1.1.2.1.cmml" xref="S4.E2.m1.7.7.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.7.7.1.1.1.1.2.2.cmml" xref="S4.E2.m1.7.7.1.1.1.1.2.2">𝑓</ci><interval closure="open" id="S4.E2.m1.4.4.2.3.cmml" xref="S4.E2.m1.4.4.2.4"><ci id="S4.E2.m1.3.3.1.1.cmml" xref="S4.E2.m1.3.3.1.1">𝑡</ci><ci id="S4.E2.m1.4.4.2.2.cmml" xref="S4.E2.m1.4.4.2.2">𝑖</ci></interval></apply><apply id="S4.E2.m1.7.7.1.1.1.1.3.cmml" xref="S4.E2.m1.7.7.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.7.7.1.1.1.1.3.1.cmml" xref="S4.E2.m1.7.7.1.1.1.1.3">subscript</csymbol><apply id="S4.E2.m1.7.7.1.1.1.1.3.2.cmml" xref="S4.E2.m1.7.7.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.7.7.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.7.7.1.1.1.1.3">superscript</csymbol><ci id="S4.E2.m1.7.7.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.7.7.1.1.1.1.3.2.2">𝑃</ci><ci id="S4.E2.m1.7.7.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.7.7.1.1.1.1.3.2.3">𝑆</ci></apply><interval closure="open" id="S4.E2.m1.6.6.2.3.cmml" xref="S4.E2.m1.6.6.2.4"><ci id="S4.E2.m1.5.5.1.1.cmml" xref="S4.E2.m1.5.5.1.1">𝑡</ci><ci id="S4.E2.m1.6.6.2.2.cmml" xref="S4.E2.m1.6.6.2.2">𝑖</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.7c">\bar{f}_{ST(t,i)}=A_{S}\left(f_{(t,i)}+P^{S}_{(t,i)}\right)</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.7d">over¯ start_ARG italic_f end_ARG start_POSTSUBSCRIPT italic_S italic_T ( italic_t , italic_i ) end_POSTSUBSCRIPT = italic_A start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT + italic_P start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\bar{f}_{TS(t,i)}=A_{T}\left(f_{(t,i)}+P^{T}_{(t,i)}\right)" class="ltx_Math" display="block" id="S4.E3.m1.7"><semantics id="S4.E3.m1.7a"><mrow id="S4.E3.m1.7.7" xref="S4.E3.m1.7.7.cmml"><msub id="S4.E3.m1.7.7.3" xref="S4.E3.m1.7.7.3.cmml"><mover accent="true" id="S4.E3.m1.7.7.3.2" xref="S4.E3.m1.7.7.3.2.cmml"><mi id="S4.E3.m1.7.7.3.2.2" xref="S4.E3.m1.7.7.3.2.2.cmml">f</mi><mo id="S4.E3.m1.7.7.3.2.1" xref="S4.E3.m1.7.7.3.2.1.cmml">¯</mo></mover><mrow id="S4.E3.m1.2.2.2" xref="S4.E3.m1.2.2.2.cmml"><mi id="S4.E3.m1.2.2.2.4" xref="S4.E3.m1.2.2.2.4.cmml">T</mi><mo id="S4.E3.m1.2.2.2.3" xref="S4.E3.m1.2.2.2.3.cmml">⁢</mo><mi id="S4.E3.m1.2.2.2.5" xref="S4.E3.m1.2.2.2.5.cmml">S</mi><mo id="S4.E3.m1.2.2.2.3a" xref="S4.E3.m1.2.2.2.3.cmml">⁢</mo><mrow id="S4.E3.m1.2.2.2.6.2" xref="S4.E3.m1.2.2.2.6.1.cmml"><mo id="S4.E3.m1.2.2.2.6.2.1" stretchy="false" xref="S4.E3.m1.2.2.2.6.1.cmml">(</mo><mi id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml">t</mi><mo id="S4.E3.m1.2.2.2.6.2.2" xref="S4.E3.m1.2.2.2.6.1.cmml">,</mo><mi id="S4.E3.m1.2.2.2.2" xref="S4.E3.m1.2.2.2.2.cmml">i</mi><mo id="S4.E3.m1.2.2.2.6.2.3" stretchy="false" xref="S4.E3.m1.2.2.2.6.1.cmml">)</mo></mrow></mrow></msub><mo id="S4.E3.m1.7.7.2" xref="S4.E3.m1.7.7.2.cmml">=</mo><mrow id="S4.E3.m1.7.7.1" xref="S4.E3.m1.7.7.1.cmml"><msub id="S4.E3.m1.7.7.1.3" xref="S4.E3.m1.7.7.1.3.cmml"><mi id="S4.E3.m1.7.7.1.3.2" xref="S4.E3.m1.7.7.1.3.2.cmml">A</mi><mi id="S4.E3.m1.7.7.1.3.3" xref="S4.E3.m1.7.7.1.3.3.cmml">T</mi></msub><mo id="S4.E3.m1.7.7.1.2" xref="S4.E3.m1.7.7.1.2.cmml">⁢</mo><mrow id="S4.E3.m1.7.7.1.1.1" xref="S4.E3.m1.7.7.1.1.1.1.cmml"><mo id="S4.E3.m1.7.7.1.1.1.2" xref="S4.E3.m1.7.7.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.7.7.1.1.1.1" xref="S4.E3.m1.7.7.1.1.1.1.cmml"><msub id="S4.E3.m1.7.7.1.1.1.1.2" xref="S4.E3.m1.7.7.1.1.1.1.2.cmml"><mi id="S4.E3.m1.7.7.1.1.1.1.2.2" xref="S4.E3.m1.7.7.1.1.1.1.2.2.cmml">f</mi><mrow id="S4.E3.m1.4.4.2.4" xref="S4.E3.m1.4.4.2.3.cmml"><mo id="S4.E3.m1.4.4.2.4.1" stretchy="false" xref="S4.E3.m1.4.4.2.3.cmml">(</mo><mi id="S4.E3.m1.3.3.1.1" xref="S4.E3.m1.3.3.1.1.cmml">t</mi><mo id="S4.E3.m1.4.4.2.4.2" xref="S4.E3.m1.4.4.2.3.cmml">,</mo><mi id="S4.E3.m1.4.4.2.2" xref="S4.E3.m1.4.4.2.2.cmml">i</mi><mo id="S4.E3.m1.4.4.2.4.3" stretchy="false" xref="S4.E3.m1.4.4.2.3.cmml">)</mo></mrow></msub><mo id="S4.E3.m1.7.7.1.1.1.1.1" xref="S4.E3.m1.7.7.1.1.1.1.1.cmml">+</mo><msubsup id="S4.E3.m1.7.7.1.1.1.1.3" xref="S4.E3.m1.7.7.1.1.1.1.3.cmml"><mi id="S4.E3.m1.7.7.1.1.1.1.3.2.2" xref="S4.E3.m1.7.7.1.1.1.1.3.2.2.cmml">P</mi><mrow id="S4.E3.m1.6.6.2.4" xref="S4.E3.m1.6.6.2.3.cmml"><mo id="S4.E3.m1.6.6.2.4.1" stretchy="false" xref="S4.E3.m1.6.6.2.3.cmml">(</mo><mi id="S4.E3.m1.5.5.1.1" xref="S4.E3.m1.5.5.1.1.cmml">t</mi><mo id="S4.E3.m1.6.6.2.4.2" xref="S4.E3.m1.6.6.2.3.cmml">,</mo><mi id="S4.E3.m1.6.6.2.2" xref="S4.E3.m1.6.6.2.2.cmml">i</mi><mo id="S4.E3.m1.6.6.2.4.3" stretchy="false" xref="S4.E3.m1.6.6.2.3.cmml">)</mo></mrow><mi id="S4.E3.m1.7.7.1.1.1.1.3.2.3" xref="S4.E3.m1.7.7.1.1.1.1.3.2.3.cmml">T</mi></msubsup></mrow><mo id="S4.E3.m1.7.7.1.1.1.3" xref="S4.E3.m1.7.7.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.7b"><apply id="S4.E3.m1.7.7.cmml" xref="S4.E3.m1.7.7"><eq id="S4.E3.m1.7.7.2.cmml" xref="S4.E3.m1.7.7.2"></eq><apply id="S4.E3.m1.7.7.3.cmml" xref="S4.E3.m1.7.7.3"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.3.1.cmml" xref="S4.E3.m1.7.7.3">subscript</csymbol><apply id="S4.E3.m1.7.7.3.2.cmml" xref="S4.E3.m1.7.7.3.2"><ci id="S4.E3.m1.7.7.3.2.1.cmml" xref="S4.E3.m1.7.7.3.2.1">¯</ci><ci id="S4.E3.m1.7.7.3.2.2.cmml" xref="S4.E3.m1.7.7.3.2.2">𝑓</ci></apply><apply id="S4.E3.m1.2.2.2.cmml" xref="S4.E3.m1.2.2.2"><times id="S4.E3.m1.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.3"></times><ci id="S4.E3.m1.2.2.2.4.cmml" xref="S4.E3.m1.2.2.2.4">𝑇</ci><ci id="S4.E3.m1.2.2.2.5.cmml" xref="S4.E3.m1.2.2.2.5">𝑆</ci><interval closure="open" id="S4.E3.m1.2.2.2.6.1.cmml" xref="S4.E3.m1.2.2.2.6.2"><ci id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1">𝑡</ci><ci id="S4.E3.m1.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2">𝑖</ci></interval></apply></apply><apply id="S4.E3.m1.7.7.1.cmml" xref="S4.E3.m1.7.7.1"><times id="S4.E3.m1.7.7.1.2.cmml" xref="S4.E3.m1.7.7.1.2"></times><apply id="S4.E3.m1.7.7.1.3.cmml" xref="S4.E3.m1.7.7.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.1.3.1.cmml" xref="S4.E3.m1.7.7.1.3">subscript</csymbol><ci id="S4.E3.m1.7.7.1.3.2.cmml" xref="S4.E3.m1.7.7.1.3.2">𝐴</ci><ci id="S4.E3.m1.7.7.1.3.3.cmml" xref="S4.E3.m1.7.7.1.3.3">𝑇</ci></apply><apply id="S4.E3.m1.7.7.1.1.1.1.cmml" xref="S4.E3.m1.7.7.1.1.1"><plus id="S4.E3.m1.7.7.1.1.1.1.1.cmml" xref="S4.E3.m1.7.7.1.1.1.1.1"></plus><apply id="S4.E3.m1.7.7.1.1.1.1.2.cmml" xref="S4.E3.m1.7.7.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.1.1.1.1.2.1.cmml" xref="S4.E3.m1.7.7.1.1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.7.7.1.1.1.1.2.2.cmml" xref="S4.E3.m1.7.7.1.1.1.1.2.2">𝑓</ci><interval closure="open" id="S4.E3.m1.4.4.2.3.cmml" xref="S4.E3.m1.4.4.2.4"><ci id="S4.E3.m1.3.3.1.1.cmml" xref="S4.E3.m1.3.3.1.1">𝑡</ci><ci id="S4.E3.m1.4.4.2.2.cmml" xref="S4.E3.m1.4.4.2.2">𝑖</ci></interval></apply><apply id="S4.E3.m1.7.7.1.1.1.1.3.cmml" xref="S4.E3.m1.7.7.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.1.1.1.1.3.1.cmml" xref="S4.E3.m1.7.7.1.1.1.1.3">subscript</csymbol><apply id="S4.E3.m1.7.7.1.1.1.1.3.2.cmml" xref="S4.E3.m1.7.7.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.1.1.1.1.3.2.1.cmml" xref="S4.E3.m1.7.7.1.1.1.1.3">superscript</csymbol><ci id="S4.E3.m1.7.7.1.1.1.1.3.2.2.cmml" xref="S4.E3.m1.7.7.1.1.1.1.3.2.2">𝑃</ci><ci id="S4.E3.m1.7.7.1.1.1.1.3.2.3.cmml" xref="S4.E3.m1.7.7.1.1.1.1.3.2.3">𝑇</ci></apply><interval closure="open" id="S4.E3.m1.6.6.2.3.cmml" xref="S4.E3.m1.6.6.2.4"><ci id="S4.E3.m1.5.5.1.1.cmml" xref="S4.E3.m1.5.5.1.1">𝑡</ci><ci id="S4.E3.m1.6.6.2.2.cmml" xref="S4.E3.m1.6.6.2.2">𝑖</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.7c">\bar{f}_{TS(t,i)}=A_{T}\left(f_{(t,i)}+P^{T}_{(t,i)}\right)</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.7d">over¯ start_ARG italic_f end_ARG start_POSTSUBSCRIPT italic_T italic_S ( italic_t , italic_i ) end_POSTSUBSCRIPT = italic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT + italic_P start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\bar{f}_{ST(t,i)}=A_{T}\left(P^{T}_{t,i}+A_{S}\left(f_{(t,i)}+P^{S}_{(t,i)}%
\right)\right)" class="ltx_Math" display="block" id="S4.E4.m1.9"><semantics id="S4.E4.m1.9a"><mrow id="S4.E4.m1.9.9" xref="S4.E4.m1.9.9.cmml"><msub id="S4.E4.m1.9.9.3" xref="S4.E4.m1.9.9.3.cmml"><mover accent="true" id="S4.E4.m1.9.9.3.2" xref="S4.E4.m1.9.9.3.2.cmml"><mi id="S4.E4.m1.9.9.3.2.2" xref="S4.E4.m1.9.9.3.2.2.cmml">f</mi><mo id="S4.E4.m1.9.9.3.2.1" xref="S4.E4.m1.9.9.3.2.1.cmml">¯</mo></mover><mrow id="S4.E4.m1.2.2.2" xref="S4.E4.m1.2.2.2.cmml"><mi id="S4.E4.m1.2.2.2.4" xref="S4.E4.m1.2.2.2.4.cmml">S</mi><mo id="S4.E4.m1.2.2.2.3" xref="S4.E4.m1.2.2.2.3.cmml">⁢</mo><mi id="S4.E4.m1.2.2.2.5" xref="S4.E4.m1.2.2.2.5.cmml">T</mi><mo id="S4.E4.m1.2.2.2.3a" xref="S4.E4.m1.2.2.2.3.cmml">⁢</mo><mrow id="S4.E4.m1.2.2.2.6.2" xref="S4.E4.m1.2.2.2.6.1.cmml"><mo id="S4.E4.m1.2.2.2.6.2.1" stretchy="false" xref="S4.E4.m1.2.2.2.6.1.cmml">(</mo><mi id="S4.E4.m1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.cmml">t</mi><mo id="S4.E4.m1.2.2.2.6.2.2" xref="S4.E4.m1.2.2.2.6.1.cmml">,</mo><mi id="S4.E4.m1.2.2.2.2" xref="S4.E4.m1.2.2.2.2.cmml">i</mi><mo id="S4.E4.m1.2.2.2.6.2.3" stretchy="false" xref="S4.E4.m1.2.2.2.6.1.cmml">)</mo></mrow></mrow></msub><mo id="S4.E4.m1.9.9.2" xref="S4.E4.m1.9.9.2.cmml">=</mo><mrow id="S4.E4.m1.9.9.1" xref="S4.E4.m1.9.9.1.cmml"><msub id="S4.E4.m1.9.9.1.3" xref="S4.E4.m1.9.9.1.3.cmml"><mi id="S4.E4.m1.9.9.1.3.2" xref="S4.E4.m1.9.9.1.3.2.cmml">A</mi><mi id="S4.E4.m1.9.9.1.3.3" xref="S4.E4.m1.9.9.1.3.3.cmml">T</mi></msub><mo id="S4.E4.m1.9.9.1.2" xref="S4.E4.m1.9.9.1.2.cmml">⁢</mo><mrow id="S4.E4.m1.9.9.1.1.1" xref="S4.E4.m1.9.9.1.1.1.1.cmml"><mo id="S4.E4.m1.9.9.1.1.1.2" xref="S4.E4.m1.9.9.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.9.9.1.1.1.1" xref="S4.E4.m1.9.9.1.1.1.1.cmml"><msubsup id="S4.E4.m1.9.9.1.1.1.1.3" xref="S4.E4.m1.9.9.1.1.1.1.3.cmml"><mi id="S4.E4.m1.9.9.1.1.1.1.3.2.2" xref="S4.E4.m1.9.9.1.1.1.1.3.2.2.cmml">P</mi><mrow id="S4.E4.m1.4.4.2.4" xref="S4.E4.m1.4.4.2.3.cmml"><mi id="S4.E4.m1.3.3.1.1" xref="S4.E4.m1.3.3.1.1.cmml">t</mi><mo id="S4.E4.m1.4.4.2.4.1" xref="S4.E4.m1.4.4.2.3.cmml">,</mo><mi id="S4.E4.m1.4.4.2.2" xref="S4.E4.m1.4.4.2.2.cmml">i</mi></mrow><mi id="S4.E4.m1.9.9.1.1.1.1.3.2.3" xref="S4.E4.m1.9.9.1.1.1.1.3.2.3.cmml">T</mi></msubsup><mo id="S4.E4.m1.9.9.1.1.1.1.2" xref="S4.E4.m1.9.9.1.1.1.1.2.cmml">+</mo><mrow id="S4.E4.m1.9.9.1.1.1.1.1" xref="S4.E4.m1.9.9.1.1.1.1.1.cmml"><msub id="S4.E4.m1.9.9.1.1.1.1.1.3" xref="S4.E4.m1.9.9.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.9.9.1.1.1.1.1.3.2" xref="S4.E4.m1.9.9.1.1.1.1.1.3.2.cmml">A</mi><mi id="S4.E4.m1.9.9.1.1.1.1.1.3.3" xref="S4.E4.m1.9.9.1.1.1.1.1.3.3.cmml">S</mi></msub><mo id="S4.E4.m1.9.9.1.1.1.1.1.2" xref="S4.E4.m1.9.9.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E4.m1.9.9.1.1.1.1.1.1.1" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E4.m1.9.9.1.1.1.1.1.1.1.2" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.2.2" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.2.2.cmml">f</mi><mrow id="S4.E4.m1.6.6.2.4" xref="S4.E4.m1.6.6.2.3.cmml"><mo id="S4.E4.m1.6.6.2.4.1" stretchy="false" xref="S4.E4.m1.6.6.2.3.cmml">(</mo><mi id="S4.E4.m1.5.5.1.1" xref="S4.E4.m1.5.5.1.1.cmml">t</mi><mo id="S4.E4.m1.6.6.2.4.2" xref="S4.E4.m1.6.6.2.3.cmml">,</mo><mi id="S4.E4.m1.6.6.2.2" xref="S4.E4.m1.6.6.2.2.cmml">i</mi><mo id="S4.E4.m1.6.6.2.4.3" stretchy="false" xref="S4.E4.m1.6.6.2.3.cmml">)</mo></mrow></msub><mo id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.1.cmml">+</mo><msubsup id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.2.2.cmml">P</mi><mrow id="S4.E4.m1.8.8.2.4" xref="S4.E4.m1.8.8.2.3.cmml"><mo id="S4.E4.m1.8.8.2.4.1" stretchy="false" xref="S4.E4.m1.8.8.2.3.cmml">(</mo><mi id="S4.E4.m1.7.7.1.1" xref="S4.E4.m1.7.7.1.1.cmml">t</mi><mo id="S4.E4.m1.8.8.2.4.2" xref="S4.E4.m1.8.8.2.3.cmml">,</mo><mi id="S4.E4.m1.8.8.2.2" xref="S4.E4.m1.8.8.2.2.cmml">i</mi><mo id="S4.E4.m1.8.8.2.4.3" stretchy="false" xref="S4.E4.m1.8.8.2.3.cmml">)</mo></mrow><mi id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.2.3.cmml">S</mi></msubsup></mrow><mo id="S4.E4.m1.9.9.1.1.1.1.1.1.1.3" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E4.m1.9.9.1.1.1.3" xref="S4.E4.m1.9.9.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.9b"><apply id="S4.E4.m1.9.9.cmml" xref="S4.E4.m1.9.9"><eq id="S4.E4.m1.9.9.2.cmml" xref="S4.E4.m1.9.9.2"></eq><apply id="S4.E4.m1.9.9.3.cmml" xref="S4.E4.m1.9.9.3"><csymbol cd="ambiguous" id="S4.E4.m1.9.9.3.1.cmml" xref="S4.E4.m1.9.9.3">subscript</csymbol><apply id="S4.E4.m1.9.9.3.2.cmml" xref="S4.E4.m1.9.9.3.2"><ci id="S4.E4.m1.9.9.3.2.1.cmml" xref="S4.E4.m1.9.9.3.2.1">¯</ci><ci id="S4.E4.m1.9.9.3.2.2.cmml" xref="S4.E4.m1.9.9.3.2.2">𝑓</ci></apply><apply id="S4.E4.m1.2.2.2.cmml" xref="S4.E4.m1.2.2.2"><times id="S4.E4.m1.2.2.2.3.cmml" xref="S4.E4.m1.2.2.2.3"></times><ci id="S4.E4.m1.2.2.2.4.cmml" xref="S4.E4.m1.2.2.2.4">𝑆</ci><ci id="S4.E4.m1.2.2.2.5.cmml" xref="S4.E4.m1.2.2.2.5">𝑇</ci><interval closure="open" id="S4.E4.m1.2.2.2.6.1.cmml" xref="S4.E4.m1.2.2.2.6.2"><ci id="S4.E4.m1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1">𝑡</ci><ci id="S4.E4.m1.2.2.2.2.cmml" xref="S4.E4.m1.2.2.2.2">𝑖</ci></interval></apply></apply><apply id="S4.E4.m1.9.9.1.cmml" xref="S4.E4.m1.9.9.1"><times id="S4.E4.m1.9.9.1.2.cmml" xref="S4.E4.m1.9.9.1.2"></times><apply id="S4.E4.m1.9.9.1.3.cmml" xref="S4.E4.m1.9.9.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.9.9.1.3.1.cmml" xref="S4.E4.m1.9.9.1.3">subscript</csymbol><ci id="S4.E4.m1.9.9.1.3.2.cmml" xref="S4.E4.m1.9.9.1.3.2">𝐴</ci><ci id="S4.E4.m1.9.9.1.3.3.cmml" xref="S4.E4.m1.9.9.1.3.3">𝑇</ci></apply><apply id="S4.E4.m1.9.9.1.1.1.1.cmml" xref="S4.E4.m1.9.9.1.1.1"><plus id="S4.E4.m1.9.9.1.1.1.1.2.cmml" xref="S4.E4.m1.9.9.1.1.1.1.2"></plus><apply id="S4.E4.m1.9.9.1.1.1.1.3.cmml" xref="S4.E4.m1.9.9.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.9.9.1.1.1.1.3.1.cmml" xref="S4.E4.m1.9.9.1.1.1.1.3">subscript</csymbol><apply id="S4.E4.m1.9.9.1.1.1.1.3.2.cmml" xref="S4.E4.m1.9.9.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.9.9.1.1.1.1.3.2.1.cmml" xref="S4.E4.m1.9.9.1.1.1.1.3">superscript</csymbol><ci id="S4.E4.m1.9.9.1.1.1.1.3.2.2.cmml" xref="S4.E4.m1.9.9.1.1.1.1.3.2.2">𝑃</ci><ci id="S4.E4.m1.9.9.1.1.1.1.3.2.3.cmml" xref="S4.E4.m1.9.9.1.1.1.1.3.2.3">𝑇</ci></apply><list id="S4.E4.m1.4.4.2.3.cmml" xref="S4.E4.m1.4.4.2.4"><ci id="S4.E4.m1.3.3.1.1.cmml" xref="S4.E4.m1.3.3.1.1">𝑡</ci><ci id="S4.E4.m1.4.4.2.2.cmml" xref="S4.E4.m1.4.4.2.2">𝑖</ci></list></apply><apply id="S4.E4.m1.9.9.1.1.1.1.1.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1"><times id="S4.E4.m1.9.9.1.1.1.1.1.2.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.2"></times><apply id="S4.E4.m1.9.9.1.1.1.1.1.3.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.9.9.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E4.m1.9.9.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.3.2">𝐴</ci><ci id="S4.E4.m1.9.9.1.1.1.1.1.3.3.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.3.3">𝑆</ci></apply><apply id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1"><plus id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.1"></plus><apply id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.2.2">𝑓</ci><interval closure="open" id="S4.E4.m1.6.6.2.3.cmml" xref="S4.E4.m1.6.6.2.4"><ci id="S4.E4.m1.5.5.1.1.cmml" xref="S4.E4.m1.5.5.1.1">𝑡</ci><ci id="S4.E4.m1.6.6.2.2.cmml" xref="S4.E4.m1.6.6.2.2">𝑖</ci></interval></apply><apply id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.2.2">𝑃</ci><ci id="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E4.m1.9.9.1.1.1.1.1.1.1.1.3.2.3">𝑆</ci></apply><interval closure="open" id="S4.E4.m1.8.8.2.3.cmml" xref="S4.E4.m1.8.8.2.4"><ci id="S4.E4.m1.7.7.1.1.cmml" xref="S4.E4.m1.7.7.1.1">𝑡</ci><ci id="S4.E4.m1.8.8.2.2.cmml" xref="S4.E4.m1.8.8.2.2">𝑖</ci></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.9c">\bar{f}_{ST(t,i)}=A_{T}\left(P^{T}_{t,i}+A_{S}\left(f_{(t,i)}+P^{S}_{(t,i)}%
\right)\right)</annotation><annotation encoding="application/x-llamapun" id="S4.E4.m1.9d">over¯ start_ARG italic_f end_ARG start_POSTSUBSCRIPT italic_S italic_T ( italic_t , italic_i ) end_POSTSUBSCRIPT = italic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_P start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , italic_i end_POSTSUBSCRIPT + italic_A start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT + italic_P start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p5">
<p class="ltx_p" id="S4.SS3.SSS1.p5.10">Here, Equation (<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.E2" title="In 4.3.1 Spatial-temporal position embedding ‣ 4.3 Architecture for Branches of Spatial-Temporal Approach ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">2</span></a>),(<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.E2" title="In 4.3.1 Spatial-temporal position embedding ‣ 4.3 Architecture for Branches of Spatial-Temporal Approach ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">2</span></a>),(<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.E2" title="In 4.3.1 Spatial-temporal position embedding ‣ 4.3 Architecture for Branches of Spatial-Temporal Approach ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">2</span></a>) produces the final feature <math alttext="f\bar{f}_{ST(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p5.1.m1.2"><semantics id="S4.SS3.SSS1.p5.1.m1.2a"><mrow id="S4.SS3.SSS1.p5.1.m1.2.3" xref="S4.SS3.SSS1.p5.1.m1.2.3.cmml"><mi id="S4.SS3.SSS1.p5.1.m1.2.3.2" xref="S4.SS3.SSS1.p5.1.m1.2.3.2.cmml">f</mi><mo id="S4.SS3.SSS1.p5.1.m1.2.3.1" xref="S4.SS3.SSS1.p5.1.m1.2.3.1.cmml">⁢</mo><msub id="S4.SS3.SSS1.p5.1.m1.2.3.3" xref="S4.SS3.SSS1.p5.1.m1.2.3.3.cmml"><mover accent="true" id="S4.SS3.SSS1.p5.1.m1.2.3.3.2" xref="S4.SS3.SSS1.p5.1.m1.2.3.3.2.cmml"><mi id="S4.SS3.SSS1.p5.1.m1.2.3.3.2.2" xref="S4.SS3.SSS1.p5.1.m1.2.3.3.2.2.cmml">f</mi><mo id="S4.SS3.SSS1.p5.1.m1.2.3.3.2.1" xref="S4.SS3.SSS1.p5.1.m1.2.3.3.2.1.cmml">¯</mo></mover><mrow id="S4.SS3.SSS1.p5.1.m1.2.2.2" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.cmml"><mi id="S4.SS3.SSS1.p5.1.m1.2.2.2.4" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.4.cmml">S</mi><mo id="S4.SS3.SSS1.p5.1.m1.2.2.2.3" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S4.SS3.SSS1.p5.1.m1.2.2.2.5" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.5.cmml">T</mi><mo id="S4.SS3.SSS1.p5.1.m1.2.2.2.3a" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.3.cmml">⁢</mo><mrow id="S4.SS3.SSS1.p5.1.m1.2.2.2.6.2" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.6.1.cmml"><mo id="S4.SS3.SSS1.p5.1.m1.2.2.2.6.2.1" stretchy="false" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.6.1.cmml">(</mo><mi id="S4.SS3.SSS1.p5.1.m1.1.1.1.1" xref="S4.SS3.SSS1.p5.1.m1.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS1.p5.1.m1.2.2.2.6.2.2" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.6.1.cmml">,</mo><mi id="S4.SS3.SSS1.p5.1.m1.2.2.2.2" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS1.p5.1.m1.2.2.2.6.2.3" stretchy="false" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.6.1.cmml">)</mo></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p5.1.m1.2b"><apply id="S4.SS3.SSS1.p5.1.m1.2.3.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.3"><times id="S4.SS3.SSS1.p5.1.m1.2.3.1.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.3.1"></times><ci id="S4.SS3.SSS1.p5.1.m1.2.3.2.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.3.2">𝑓</ci><apply id="S4.SS3.SSS1.p5.1.m1.2.3.3.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.3.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.1.m1.2.3.3.1.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.3.3">subscript</csymbol><apply id="S4.SS3.SSS1.p5.1.m1.2.3.3.2.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.3.3.2"><ci id="S4.SS3.SSS1.p5.1.m1.2.3.3.2.1.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.3.3.2.1">¯</ci><ci id="S4.SS3.SSS1.p5.1.m1.2.3.3.2.2.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.3.3.2.2">𝑓</ci></apply><apply id="S4.SS3.SSS1.p5.1.m1.2.2.2.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.2.2"><times id="S4.SS3.SSS1.p5.1.m1.2.2.2.3.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.3"></times><ci id="S4.SS3.SSS1.p5.1.m1.2.2.2.4.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.4">𝑆</ci><ci id="S4.SS3.SSS1.p5.1.m1.2.2.2.5.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.5">𝑇</ci><interval closure="open" id="S4.SS3.SSS1.p5.1.m1.2.2.2.6.1.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.6.2"><ci id="S4.SS3.SSS1.p5.1.m1.1.1.1.1.cmml" xref="S4.SS3.SSS1.p5.1.m1.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS1.p5.1.m1.2.2.2.2.cmml" xref="S4.SS3.SSS1.p5.1.m1.2.2.2.2">𝑖</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p5.1.m1.2c">f\bar{f}_{ST(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p5.1.m1.2d">italic_f over¯ start_ARG italic_f end_ARG start_POSTSUBSCRIPT italic_S italic_T ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="bar{f}_{TS(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p5.2.m2.2"><semantics id="S4.SS3.SSS1.p5.2.m2.2a"><mrow id="S4.SS3.SSS1.p5.2.m2.2.3" xref="S4.SS3.SSS1.p5.2.m2.2.3.cmml"><mi id="S4.SS3.SSS1.p5.2.m2.2.3.2" xref="S4.SS3.SSS1.p5.2.m2.2.3.2.cmml">b</mi><mo id="S4.SS3.SSS1.p5.2.m2.2.3.1" xref="S4.SS3.SSS1.p5.2.m2.2.3.1.cmml">⁢</mo><mi id="S4.SS3.SSS1.p5.2.m2.2.3.3" xref="S4.SS3.SSS1.p5.2.m2.2.3.3.cmml">a</mi><mo id="S4.SS3.SSS1.p5.2.m2.2.3.1a" xref="S4.SS3.SSS1.p5.2.m2.2.3.1.cmml">⁢</mo><mi id="S4.SS3.SSS1.p5.2.m2.2.3.4" xref="S4.SS3.SSS1.p5.2.m2.2.3.4.cmml">r</mi><mo id="S4.SS3.SSS1.p5.2.m2.2.3.1b" xref="S4.SS3.SSS1.p5.2.m2.2.3.1.cmml">⁢</mo><msub id="S4.SS3.SSS1.p5.2.m2.2.3.5" xref="S4.SS3.SSS1.p5.2.m2.2.3.5.cmml"><mi id="S4.SS3.SSS1.p5.2.m2.2.3.5.2" xref="S4.SS3.SSS1.p5.2.m2.2.3.5.2.cmml">f</mi><mrow id="S4.SS3.SSS1.p5.2.m2.2.2.2" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.cmml"><mi id="S4.SS3.SSS1.p5.2.m2.2.2.2.4" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.4.cmml">T</mi><mo id="S4.SS3.SSS1.p5.2.m2.2.2.2.3" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.3.cmml">⁢</mo><mi id="S4.SS3.SSS1.p5.2.m2.2.2.2.5" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.5.cmml">S</mi><mo id="S4.SS3.SSS1.p5.2.m2.2.2.2.3a" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.3.cmml">⁢</mo><mrow id="S4.SS3.SSS1.p5.2.m2.2.2.2.6.2" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.6.1.cmml"><mo id="S4.SS3.SSS1.p5.2.m2.2.2.2.6.2.1" stretchy="false" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.6.1.cmml">(</mo><mi id="S4.SS3.SSS1.p5.2.m2.1.1.1.1" xref="S4.SS3.SSS1.p5.2.m2.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS1.p5.2.m2.2.2.2.6.2.2" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.6.1.cmml">,</mo><mi id="S4.SS3.SSS1.p5.2.m2.2.2.2.2" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS1.p5.2.m2.2.2.2.6.2.3" stretchy="false" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.6.1.cmml">)</mo></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p5.2.m2.2b"><apply id="S4.SS3.SSS1.p5.2.m2.2.3.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.3"><times id="S4.SS3.SSS1.p5.2.m2.2.3.1.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.3.1"></times><ci id="S4.SS3.SSS1.p5.2.m2.2.3.2.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.3.2">𝑏</ci><ci id="S4.SS3.SSS1.p5.2.m2.2.3.3.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.3.3">𝑎</ci><ci id="S4.SS3.SSS1.p5.2.m2.2.3.4.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.3.4">𝑟</ci><apply id="S4.SS3.SSS1.p5.2.m2.2.3.5.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.3.5"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.2.m2.2.3.5.1.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.3.5">subscript</csymbol><ci id="S4.SS3.SSS1.p5.2.m2.2.3.5.2.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.3.5.2">𝑓</ci><apply id="S4.SS3.SSS1.p5.2.m2.2.2.2.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.2.2"><times id="S4.SS3.SSS1.p5.2.m2.2.2.2.3.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.3"></times><ci id="S4.SS3.SSS1.p5.2.m2.2.2.2.4.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.4">𝑇</ci><ci id="S4.SS3.SSS1.p5.2.m2.2.2.2.5.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.5">𝑆</ci><interval closure="open" id="S4.SS3.SSS1.p5.2.m2.2.2.2.6.1.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.6.2"><ci id="S4.SS3.SSS1.p5.2.m2.1.1.1.1.cmml" xref="S4.SS3.SSS1.p5.2.m2.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS1.p5.2.m2.2.2.2.2.cmml" xref="S4.SS3.SSS1.p5.2.m2.2.2.2.2">𝑖</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p5.2.m2.2c">bar{f}_{TS(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p5.2.m2.2d">italic_b italic_a italic_r italic_f start_POSTSUBSCRIPT italic_T italic_S ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="bar{f}_{ST(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p5.3.m3.2"><semantics id="S4.SS3.SSS1.p5.3.m3.2a"><mrow id="S4.SS3.SSS1.p5.3.m3.2.3" xref="S4.SS3.SSS1.p5.3.m3.2.3.cmml"><mi id="S4.SS3.SSS1.p5.3.m3.2.3.2" xref="S4.SS3.SSS1.p5.3.m3.2.3.2.cmml">b</mi><mo id="S4.SS3.SSS1.p5.3.m3.2.3.1" xref="S4.SS3.SSS1.p5.3.m3.2.3.1.cmml">⁢</mo><mi id="S4.SS3.SSS1.p5.3.m3.2.3.3" xref="S4.SS3.SSS1.p5.3.m3.2.3.3.cmml">a</mi><mo id="S4.SS3.SSS1.p5.3.m3.2.3.1a" xref="S4.SS3.SSS1.p5.3.m3.2.3.1.cmml">⁢</mo><mi id="S4.SS3.SSS1.p5.3.m3.2.3.4" xref="S4.SS3.SSS1.p5.3.m3.2.3.4.cmml">r</mi><mo id="S4.SS3.SSS1.p5.3.m3.2.3.1b" xref="S4.SS3.SSS1.p5.3.m3.2.3.1.cmml">⁢</mo><msub id="S4.SS3.SSS1.p5.3.m3.2.3.5" xref="S4.SS3.SSS1.p5.3.m3.2.3.5.cmml"><mi id="S4.SS3.SSS1.p5.3.m3.2.3.5.2" xref="S4.SS3.SSS1.p5.3.m3.2.3.5.2.cmml">f</mi><mrow id="S4.SS3.SSS1.p5.3.m3.2.2.2" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.cmml"><mi id="S4.SS3.SSS1.p5.3.m3.2.2.2.4" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.4.cmml">S</mi><mo id="S4.SS3.SSS1.p5.3.m3.2.2.2.3" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.3.cmml">⁢</mo><mi id="S4.SS3.SSS1.p5.3.m3.2.2.2.5" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.5.cmml">T</mi><mo id="S4.SS3.SSS1.p5.3.m3.2.2.2.3a" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.3.cmml">⁢</mo><mrow id="S4.SS3.SSS1.p5.3.m3.2.2.2.6.2" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.6.1.cmml"><mo id="S4.SS3.SSS1.p5.3.m3.2.2.2.6.2.1" stretchy="false" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.6.1.cmml">(</mo><mi id="S4.SS3.SSS1.p5.3.m3.1.1.1.1" xref="S4.SS3.SSS1.p5.3.m3.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS1.p5.3.m3.2.2.2.6.2.2" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.6.1.cmml">,</mo><mi id="S4.SS3.SSS1.p5.3.m3.2.2.2.2" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS1.p5.3.m3.2.2.2.6.2.3" stretchy="false" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.6.1.cmml">)</mo></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p5.3.m3.2b"><apply id="S4.SS3.SSS1.p5.3.m3.2.3.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.3"><times id="S4.SS3.SSS1.p5.3.m3.2.3.1.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.3.1"></times><ci id="S4.SS3.SSS1.p5.3.m3.2.3.2.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.3.2">𝑏</ci><ci id="S4.SS3.SSS1.p5.3.m3.2.3.3.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.3.3">𝑎</ci><ci id="S4.SS3.SSS1.p5.3.m3.2.3.4.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.3.4">𝑟</ci><apply id="S4.SS3.SSS1.p5.3.m3.2.3.5.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.3.5"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.3.m3.2.3.5.1.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.3.5">subscript</csymbol><ci id="S4.SS3.SSS1.p5.3.m3.2.3.5.2.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.3.5.2">𝑓</ci><apply id="S4.SS3.SSS1.p5.3.m3.2.2.2.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.2.2"><times id="S4.SS3.SSS1.p5.3.m3.2.2.2.3.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.3"></times><ci id="S4.SS3.SSS1.p5.3.m3.2.2.2.4.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.4">𝑆</ci><ci id="S4.SS3.SSS1.p5.3.m3.2.2.2.5.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.5">𝑇</ci><interval closure="open" id="S4.SS3.SSS1.p5.3.m3.2.2.2.6.1.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.6.2"><ci id="S4.SS3.SSS1.p5.3.m3.1.1.1.1.cmml" xref="S4.SS3.SSS1.p5.3.m3.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS1.p5.3.m3.2.2.2.2.cmml" xref="S4.SS3.SSS1.p5.3.m3.2.2.2.2">𝑖</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p5.3.m3.2c">bar{f}_{ST(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p5.3.m3.2d">italic_b italic_a italic_r italic_f start_POSTSUBSCRIPT italic_S italic_T ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> for spatial, temporal and spatial-temporal for a specific initial feature node <math alttext="v_{(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p5.4.m4.2"><semantics id="S4.SS3.SSS1.p5.4.m4.2a"><msub id="S4.SS3.SSS1.p5.4.m4.2.3" xref="S4.SS3.SSS1.p5.4.m4.2.3.cmml"><mi id="S4.SS3.SSS1.p5.4.m4.2.3.2" xref="S4.SS3.SSS1.p5.4.m4.2.3.2.cmml">v</mi><mrow id="S4.SS3.SSS1.p5.4.m4.2.2.2.4" xref="S4.SS3.SSS1.p5.4.m4.2.2.2.3.cmml"><mo id="S4.SS3.SSS1.p5.4.m4.2.2.2.4.1" stretchy="false" xref="S4.SS3.SSS1.p5.4.m4.2.2.2.3.cmml">(</mo><mi id="S4.SS3.SSS1.p5.4.m4.1.1.1.1" xref="S4.SS3.SSS1.p5.4.m4.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS1.p5.4.m4.2.2.2.4.2" xref="S4.SS3.SSS1.p5.4.m4.2.2.2.3.cmml">,</mo><mi id="S4.SS3.SSS1.p5.4.m4.2.2.2.2" xref="S4.SS3.SSS1.p5.4.m4.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS1.p5.4.m4.2.2.2.4.3" stretchy="false" xref="S4.SS3.SSS1.p5.4.m4.2.2.2.3.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p5.4.m4.2b"><apply id="S4.SS3.SSS1.p5.4.m4.2.3.cmml" xref="S4.SS3.SSS1.p5.4.m4.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.4.m4.2.3.1.cmml" xref="S4.SS3.SSS1.p5.4.m4.2.3">subscript</csymbol><ci id="S4.SS3.SSS1.p5.4.m4.2.3.2.cmml" xref="S4.SS3.SSS1.p5.4.m4.2.3.2">𝑣</ci><interval closure="open" id="S4.SS3.SSS1.p5.4.m4.2.2.2.3.cmml" xref="S4.SS3.SSS1.p5.4.m4.2.2.2.4"><ci id="S4.SS3.SSS1.p5.4.m4.1.1.1.1.cmml" xref="S4.SS3.SSS1.p5.4.m4.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS1.p5.4.m4.2.2.2.2.cmml" xref="S4.SS3.SSS1.p5.4.m4.2.2.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p5.4.m4.2c">v_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p5.4.m4.2d">italic_v start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> consequences. In each Equation <math alttext="f_{(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p5.5.m5.2"><semantics id="S4.SS3.SSS1.p5.5.m5.2a"><msub id="S4.SS3.SSS1.p5.5.m5.2.3" xref="S4.SS3.SSS1.p5.5.m5.2.3.cmml"><mi id="S4.SS3.SSS1.p5.5.m5.2.3.2" xref="S4.SS3.SSS1.p5.5.m5.2.3.2.cmml">f</mi><mrow id="S4.SS3.SSS1.p5.5.m5.2.2.2.4" xref="S4.SS3.SSS1.p5.5.m5.2.2.2.3.cmml"><mo id="S4.SS3.SSS1.p5.5.m5.2.2.2.4.1" stretchy="false" xref="S4.SS3.SSS1.p5.5.m5.2.2.2.3.cmml">(</mo><mi id="S4.SS3.SSS1.p5.5.m5.1.1.1.1" xref="S4.SS3.SSS1.p5.5.m5.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS1.p5.5.m5.2.2.2.4.2" xref="S4.SS3.SSS1.p5.5.m5.2.2.2.3.cmml">,</mo><mi id="S4.SS3.SSS1.p5.5.m5.2.2.2.2" xref="S4.SS3.SSS1.p5.5.m5.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS1.p5.5.m5.2.2.2.4.3" stretchy="false" xref="S4.SS3.SSS1.p5.5.m5.2.2.2.3.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p5.5.m5.2b"><apply id="S4.SS3.SSS1.p5.5.m5.2.3.cmml" xref="S4.SS3.SSS1.p5.5.m5.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.5.m5.2.3.1.cmml" xref="S4.SS3.SSS1.p5.5.m5.2.3">subscript</csymbol><ci id="S4.SS3.SSS1.p5.5.m5.2.3.2.cmml" xref="S4.SS3.SSS1.p5.5.m5.2.3.2">𝑓</ci><interval closure="open" id="S4.SS3.SSS1.p5.5.m5.2.2.2.3.cmml" xref="S4.SS3.SSS1.p5.5.m5.2.2.2.4"><ci id="S4.SS3.SSS1.p5.5.m5.1.1.1.1.cmml" xref="S4.SS3.SSS1.p5.5.m5.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS1.p5.5.m5.2.2.2.2.cmml" xref="S4.SS3.SSS1.p5.5.m5.2.2.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p5.5.m5.2c">f_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p5.5.m5.2d">italic_f start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> defined the initial feature which is generated by the Sep-TCN, Temporal and spatial attention approach is defined by the <math alttext="A_{T}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p5.6.m6.1"><semantics id="S4.SS3.SSS1.p5.6.m6.1a"><msub id="S4.SS3.SSS1.p5.6.m6.1.1" xref="S4.SS3.SSS1.p5.6.m6.1.1.cmml"><mi id="S4.SS3.SSS1.p5.6.m6.1.1.2" xref="S4.SS3.SSS1.p5.6.m6.1.1.2.cmml">A</mi><mi id="S4.SS3.SSS1.p5.6.m6.1.1.3" xref="S4.SS3.SSS1.p5.6.m6.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p5.6.m6.1b"><apply id="S4.SS3.SSS1.p5.6.m6.1.1.cmml" xref="S4.SS3.SSS1.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.6.m6.1.1.1.cmml" xref="S4.SS3.SSS1.p5.6.m6.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p5.6.m6.1.1.2.cmml" xref="S4.SS3.SSS1.p5.6.m6.1.1.2">𝐴</ci><ci id="S4.SS3.SSS1.p5.6.m6.1.1.3.cmml" xref="S4.SS3.SSS1.p5.6.m6.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p5.6.m6.1c">A_{T}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p5.6.m6.1d">italic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="A_{S}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p5.7.m7.1"><semantics id="S4.SS3.SSS1.p5.7.m7.1a"><msub id="S4.SS3.SSS1.p5.7.m7.1.1" xref="S4.SS3.SSS1.p5.7.m7.1.1.cmml"><mi id="S4.SS3.SSS1.p5.7.m7.1.1.2" xref="S4.SS3.SSS1.p5.7.m7.1.1.2.cmml">A</mi><mi id="S4.SS3.SSS1.p5.7.m7.1.1.3" xref="S4.SS3.SSS1.p5.7.m7.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p5.7.m7.1b"><apply id="S4.SS3.SSS1.p5.7.m7.1.1.cmml" xref="S4.SS3.SSS1.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.7.m7.1.1.1.cmml" xref="S4.SS3.SSS1.p5.7.m7.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p5.7.m7.1.1.2.cmml" xref="S4.SS3.SSS1.p5.7.m7.1.1.2">𝐴</ci><ci id="S4.SS3.SSS1.p5.7.m7.1.1.3.cmml" xref="S4.SS3.SSS1.p5.7.m7.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p5.7.m7.1c">A_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p5.7.m7.1d">italic_A start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> respectively. For a specific frame of the hand gesture is t and i-th number joint, that frame is defined here with the <math alttext="P^{S}_{(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p5.8.m8.2"><semantics id="S4.SS3.SSS1.p5.8.m8.2a"><msubsup id="S4.SS3.SSS1.p5.8.m8.2.3" xref="S4.SS3.SSS1.p5.8.m8.2.3.cmml"><mi id="S4.SS3.SSS1.p5.8.m8.2.3.2.2" xref="S4.SS3.SSS1.p5.8.m8.2.3.2.2.cmml">P</mi><mrow id="S4.SS3.SSS1.p5.8.m8.2.2.2.4" xref="S4.SS3.SSS1.p5.8.m8.2.2.2.3.cmml"><mo id="S4.SS3.SSS1.p5.8.m8.2.2.2.4.1" stretchy="false" xref="S4.SS3.SSS1.p5.8.m8.2.2.2.3.cmml">(</mo><mi id="S4.SS3.SSS1.p5.8.m8.1.1.1.1" xref="S4.SS3.SSS1.p5.8.m8.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS1.p5.8.m8.2.2.2.4.2" xref="S4.SS3.SSS1.p5.8.m8.2.2.2.3.cmml">,</mo><mi id="S4.SS3.SSS1.p5.8.m8.2.2.2.2" xref="S4.SS3.SSS1.p5.8.m8.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS1.p5.8.m8.2.2.2.4.3" stretchy="false" xref="S4.SS3.SSS1.p5.8.m8.2.2.2.3.cmml">)</mo></mrow><mi id="S4.SS3.SSS1.p5.8.m8.2.3.2.3" xref="S4.SS3.SSS1.p5.8.m8.2.3.2.3.cmml">S</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p5.8.m8.2b"><apply id="S4.SS3.SSS1.p5.8.m8.2.3.cmml" xref="S4.SS3.SSS1.p5.8.m8.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.8.m8.2.3.1.cmml" xref="S4.SS3.SSS1.p5.8.m8.2.3">subscript</csymbol><apply id="S4.SS3.SSS1.p5.8.m8.2.3.2.cmml" xref="S4.SS3.SSS1.p5.8.m8.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.8.m8.2.3.2.1.cmml" xref="S4.SS3.SSS1.p5.8.m8.2.3">superscript</csymbol><ci id="S4.SS3.SSS1.p5.8.m8.2.3.2.2.cmml" xref="S4.SS3.SSS1.p5.8.m8.2.3.2.2">𝑃</ci><ci id="S4.SS3.SSS1.p5.8.m8.2.3.2.3.cmml" xref="S4.SS3.SSS1.p5.8.m8.2.3.2.3">𝑆</ci></apply><interval closure="open" id="S4.SS3.SSS1.p5.8.m8.2.2.2.3.cmml" xref="S4.SS3.SSS1.p5.8.m8.2.2.2.4"><ci id="S4.SS3.SSS1.p5.8.m8.1.1.1.1.cmml" xref="S4.SS3.SSS1.p5.8.m8.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS1.p5.8.m8.2.2.2.2.cmml" xref="S4.SS3.SSS1.p5.8.m8.2.2.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p5.8.m8.2c">P^{S}_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p5.8.m8.2d">italic_P start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="P^{T}_{(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p5.9.m9.2"><semantics id="S4.SS3.SSS1.p5.9.m9.2a"><msubsup id="S4.SS3.SSS1.p5.9.m9.2.3" xref="S4.SS3.SSS1.p5.9.m9.2.3.cmml"><mi id="S4.SS3.SSS1.p5.9.m9.2.3.2.2" xref="S4.SS3.SSS1.p5.9.m9.2.3.2.2.cmml">P</mi><mrow id="S4.SS3.SSS1.p5.9.m9.2.2.2.4" xref="S4.SS3.SSS1.p5.9.m9.2.2.2.3.cmml"><mo id="S4.SS3.SSS1.p5.9.m9.2.2.2.4.1" stretchy="false" xref="S4.SS3.SSS1.p5.9.m9.2.2.2.3.cmml">(</mo><mi id="S4.SS3.SSS1.p5.9.m9.1.1.1.1" xref="S4.SS3.SSS1.p5.9.m9.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS1.p5.9.m9.2.2.2.4.2" xref="S4.SS3.SSS1.p5.9.m9.2.2.2.3.cmml">,</mo><mi id="S4.SS3.SSS1.p5.9.m9.2.2.2.2" xref="S4.SS3.SSS1.p5.9.m9.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS1.p5.9.m9.2.2.2.4.3" stretchy="false" xref="S4.SS3.SSS1.p5.9.m9.2.2.2.3.cmml">)</mo></mrow><mi id="S4.SS3.SSS1.p5.9.m9.2.3.2.3" xref="S4.SS3.SSS1.p5.9.m9.2.3.2.3.cmml">T</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p5.9.m9.2b"><apply id="S4.SS3.SSS1.p5.9.m9.2.3.cmml" xref="S4.SS3.SSS1.p5.9.m9.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.9.m9.2.3.1.cmml" xref="S4.SS3.SSS1.p5.9.m9.2.3">subscript</csymbol><apply id="S4.SS3.SSS1.p5.9.m9.2.3.2.cmml" xref="S4.SS3.SSS1.p5.9.m9.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.9.m9.2.3.2.1.cmml" xref="S4.SS3.SSS1.p5.9.m9.2.3">superscript</csymbol><ci id="S4.SS3.SSS1.p5.9.m9.2.3.2.2.cmml" xref="S4.SS3.SSS1.p5.9.m9.2.3.2.2">𝑃</ci><ci id="S4.SS3.SSS1.p5.9.m9.2.3.2.3.cmml" xref="S4.SS3.SSS1.p5.9.m9.2.3.2.3">𝑇</ci></apply><interval closure="open" id="S4.SS3.SSS1.p5.9.m9.2.2.2.3.cmml" xref="S4.SS3.SSS1.p5.9.m9.2.2.2.4"><ci id="S4.SS3.SSS1.p5.9.m9.1.1.1.1.cmml" xref="S4.SS3.SSS1.p5.9.m9.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS1.p5.9.m9.2.2.2.2.cmml" xref="S4.SS3.SSS1.p5.9.m9.2.2.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p5.9.m9.2c">P^{T}_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p5.9.m9.2d">italic_P start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> the output dimension where the embedding dimension is the same as the input <math alttext="f_{(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p5.10.m10.2"><semantics id="S4.SS3.SSS1.p5.10.m10.2a"><msub id="S4.SS3.SSS1.p5.10.m10.2.3" xref="S4.SS3.SSS1.p5.10.m10.2.3.cmml"><mi id="S4.SS3.SSS1.p5.10.m10.2.3.2" xref="S4.SS3.SSS1.p5.10.m10.2.3.2.cmml">f</mi><mrow id="S4.SS3.SSS1.p5.10.m10.2.2.2.4" xref="S4.SS3.SSS1.p5.10.m10.2.2.2.3.cmml"><mo id="S4.SS3.SSS1.p5.10.m10.2.2.2.4.1" stretchy="false" xref="S4.SS3.SSS1.p5.10.m10.2.2.2.3.cmml">(</mo><mi id="S4.SS3.SSS1.p5.10.m10.1.1.1.1" xref="S4.SS3.SSS1.p5.10.m10.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS1.p5.10.m10.2.2.2.4.2" xref="S4.SS3.SSS1.p5.10.m10.2.2.2.3.cmml">,</mo><mi id="S4.SS3.SSS1.p5.10.m10.2.2.2.2" xref="S4.SS3.SSS1.p5.10.m10.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS1.p5.10.m10.2.2.2.4.3" stretchy="false" xref="S4.SS3.SSS1.p5.10.m10.2.2.2.3.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p5.10.m10.2b"><apply id="S4.SS3.SSS1.p5.10.m10.2.3.cmml" xref="S4.SS3.SSS1.p5.10.m10.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p5.10.m10.2.3.1.cmml" xref="S4.SS3.SSS1.p5.10.m10.2.3">subscript</csymbol><ci id="S4.SS3.SSS1.p5.10.m10.2.3.2.cmml" xref="S4.SS3.SSS1.p5.10.m10.2.3.2">𝑓</ci><interval closure="open" id="S4.SS3.SSS1.p5.10.m10.2.2.2.3.cmml" xref="S4.SS3.SSS1.p5.10.m10.2.2.2.4"><ci id="S4.SS3.SSS1.p5.10.m10.1.1.1.1.cmml" xref="S4.SS3.SSS1.p5.10.m10.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS1.p5.10.m10.2.2.2.2.cmml" xref="S4.SS3.SSS1.p5.10.m10.2.2.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p5.10.m10.2c">f_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p5.10.m10.2d">italic_f start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> dimension.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Multihead Attention Module</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.2">In the study, we proposed three streams of attention models that included three different attention approaches which consisting of spatial, temporal and temporal-spatial attention branches. Each attention branch is combined with the position embedding for the spatial, temporal and spatial-temporal branches. Each branch took the input from the output of the Sep-TCN, and then the first and second branches calculated the spatial attention feature and temporal attention features. The third branch included two stages; the first stage took the input from the Sep-TCN output and then updated the initial feature with the spatial attention block; after that, fed the spatial feature into the temporal feature for producing the spatial-temporal feature vector. Multhihead attention we employed in each branch to perform the attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib35" title="">35</a>]</cite>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.F7" title="Figure 7 ‣ 4.3 Architecture for Branches of Spatial-Temporal Approach ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">7</span></a> showed the inner structure of the attention and masking operation. Assume the initial separable feature is <math alttext="f_{(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.1.m1.2"><semantics id="S4.SS3.SSS2.p1.1.m1.2a"><msub id="S4.SS3.SSS2.p1.1.m1.2.3" xref="S4.SS3.SSS2.p1.1.m1.2.3.cmml"><mi id="S4.SS3.SSS2.p1.1.m1.2.3.2" xref="S4.SS3.SSS2.p1.1.m1.2.3.2.cmml">f</mi><mrow id="S4.SS3.SSS2.p1.1.m1.2.2.2.4" xref="S4.SS3.SSS2.p1.1.m1.2.2.2.3.cmml"><mo id="S4.SS3.SSS2.p1.1.m1.2.2.2.4.1" stretchy="false" xref="S4.SS3.SSS2.p1.1.m1.2.2.2.3.cmml">(</mo><mi id="S4.SS3.SSS2.p1.1.m1.1.1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS2.p1.1.m1.2.2.2.4.2" xref="S4.SS3.SSS2.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S4.SS3.SSS2.p1.1.m1.2.2.2.2" xref="S4.SS3.SSS2.p1.1.m1.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS2.p1.1.m1.2.2.2.4.3" stretchy="false" xref="S4.SS3.SSS2.p1.1.m1.2.2.2.3.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.2b"><apply id="S4.SS3.SSS2.p1.1.m1.2.3.cmml" xref="S4.SS3.SSS2.p1.1.m1.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.1.m1.2.3.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.2.3">subscript</csymbol><ci id="S4.SS3.SSS2.p1.1.m1.2.3.2.cmml" xref="S4.SS3.SSS2.p1.1.m1.2.3.2">𝑓</ci><interval closure="open" id="S4.SS3.SSS2.p1.1.m1.2.2.2.3.cmml" xref="S4.SS3.SSS2.p1.1.m1.2.2.2.4"><ci id="S4.SS3.SSS2.p1.1.m1.1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS2.p1.1.m1.2.2.2.2.cmml" xref="S4.SS3.SSS2.p1.1.m1.2.2.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.2c">f_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.1.m1.2d">italic_f start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> for a specific joint node V of a specific frame g which fed to the attention mechanism for the attention layer. To perform single-head attention, we first calculate the fully connected layer to project the query, key and value vector with a separable input feature <math alttext="f_{(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.2.m2.2"><semantics id="S4.SS3.SSS2.p1.2.m2.2a"><msub id="S4.SS3.SSS2.p1.2.m2.2.3" xref="S4.SS3.SSS2.p1.2.m2.2.3.cmml"><mi id="S4.SS3.SSS2.p1.2.m2.2.3.2" xref="S4.SS3.SSS2.p1.2.m2.2.3.2.cmml">f</mi><mrow id="S4.SS3.SSS2.p1.2.m2.2.2.2.4" xref="S4.SS3.SSS2.p1.2.m2.2.2.2.3.cmml"><mo id="S4.SS3.SSS2.p1.2.m2.2.2.2.4.1" stretchy="false" xref="S4.SS3.SSS2.p1.2.m2.2.2.2.3.cmml">(</mo><mi id="S4.SS3.SSS2.p1.2.m2.1.1.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS2.p1.2.m2.2.2.2.4.2" xref="S4.SS3.SSS2.p1.2.m2.2.2.2.3.cmml">,</mo><mi id="S4.SS3.SSS2.p1.2.m2.2.2.2.2" xref="S4.SS3.SSS2.p1.2.m2.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS2.p1.2.m2.2.2.2.4.3" stretchy="false" xref="S4.SS3.SSS2.p1.2.m2.2.2.2.3.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.2.m2.2b"><apply id="S4.SS3.SSS2.p1.2.m2.2.3.cmml" xref="S4.SS3.SSS2.p1.2.m2.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.2.m2.2.3.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.2.3">subscript</csymbol><ci id="S4.SS3.SSS2.p1.2.m2.2.3.2.cmml" xref="S4.SS3.SSS2.p1.2.m2.2.3.2">𝑓</ci><interval closure="open" id="S4.SS3.SSS2.p1.2.m2.2.2.2.3.cmml" xref="S4.SS3.SSS2.p1.2.m2.2.2.2.4"><ci id="S4.SS3.SSS2.p1.2.m2.1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS2.p1.2.m2.2.2.2.2.cmml" xref="S4.SS3.SSS2.p1.2.m2.2.2.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.2.m2.2c">f_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.2.m2.2d">italic_f start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math>. The projection mechanism is performed with the following Equations (<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.E5" title="In 4.3.2 Multihead Attention Module ‣ 4.3 Architecture for Branches of Spatial-Temporal Approach ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">5</span></a>):</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="Q^{m}_{(t,i)}=W^{m}_{Q}f_{(t,i)},K^{m}_{(t,i)}=W^{m}_{K}f_{(t,i)},V^{m}_{(t,i)%
}=W^{m}_{V}f_{(t,i)}" class="ltx_Math" display="block" id="S4.E5.m1.14"><semantics id="S4.E5.m1.14a"><mrow id="S4.E5.m1.14.14.2" xref="S4.E5.m1.14.14.3.cmml"><mrow id="S4.E5.m1.13.13.1.1" xref="S4.E5.m1.13.13.1.1.cmml"><msubsup id="S4.E5.m1.13.13.1.1.2" xref="S4.E5.m1.13.13.1.1.2.cmml"><mi id="S4.E5.m1.13.13.1.1.2.2.2" xref="S4.E5.m1.13.13.1.1.2.2.2.cmml">Q</mi><mrow id="S4.E5.m1.2.2.2.4" xref="S4.E5.m1.2.2.2.3.cmml"><mo id="S4.E5.m1.2.2.2.4.1" stretchy="false" xref="S4.E5.m1.2.2.2.3.cmml">(</mo><mi id="S4.E5.m1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.cmml">t</mi><mo id="S4.E5.m1.2.2.2.4.2" xref="S4.E5.m1.2.2.2.3.cmml">,</mo><mi id="S4.E5.m1.2.2.2.2" xref="S4.E5.m1.2.2.2.2.cmml">i</mi><mo id="S4.E5.m1.2.2.2.4.3" stretchy="false" xref="S4.E5.m1.2.2.2.3.cmml">)</mo></mrow><mi id="S4.E5.m1.13.13.1.1.2.2.3" xref="S4.E5.m1.13.13.1.1.2.2.3.cmml">m</mi></msubsup><mo id="S4.E5.m1.13.13.1.1.1" xref="S4.E5.m1.13.13.1.1.1.cmml">=</mo><mrow id="S4.E5.m1.13.13.1.1.3" xref="S4.E5.m1.13.13.1.1.3.cmml"><msubsup id="S4.E5.m1.13.13.1.1.3.2" xref="S4.E5.m1.13.13.1.1.3.2.cmml"><mi id="S4.E5.m1.13.13.1.1.3.2.2.2" xref="S4.E5.m1.13.13.1.1.3.2.2.2.cmml">W</mi><mi id="S4.E5.m1.13.13.1.1.3.2.3" xref="S4.E5.m1.13.13.1.1.3.2.3.cmml">Q</mi><mi id="S4.E5.m1.13.13.1.1.3.2.2.3" xref="S4.E5.m1.13.13.1.1.3.2.2.3.cmml">m</mi></msubsup><mo id="S4.E5.m1.13.13.1.1.3.1" xref="S4.E5.m1.13.13.1.1.3.1.cmml">⁢</mo><msub id="S4.E5.m1.13.13.1.1.3.3" xref="S4.E5.m1.13.13.1.1.3.3.cmml"><mi id="S4.E5.m1.13.13.1.1.3.3.2" xref="S4.E5.m1.13.13.1.1.3.3.2.cmml">f</mi><mrow id="S4.E5.m1.4.4.2.4" xref="S4.E5.m1.4.4.2.3.cmml"><mo id="S4.E5.m1.4.4.2.4.1" stretchy="false" xref="S4.E5.m1.4.4.2.3.cmml">(</mo><mi id="S4.E5.m1.3.3.1.1" xref="S4.E5.m1.3.3.1.1.cmml">t</mi><mo id="S4.E5.m1.4.4.2.4.2" xref="S4.E5.m1.4.4.2.3.cmml">,</mo><mi id="S4.E5.m1.4.4.2.2" xref="S4.E5.m1.4.4.2.2.cmml">i</mi><mo id="S4.E5.m1.4.4.2.4.3" stretchy="false" xref="S4.E5.m1.4.4.2.3.cmml">)</mo></mrow></msub></mrow></mrow><mo id="S4.E5.m1.14.14.2.3" xref="S4.E5.m1.14.14.3a.cmml">,</mo><mrow id="S4.E5.m1.14.14.2.2.2" xref="S4.E5.m1.14.14.2.2.3.cmml"><mrow id="S4.E5.m1.14.14.2.2.1.1" xref="S4.E5.m1.14.14.2.2.1.1.cmml"><msubsup id="S4.E5.m1.14.14.2.2.1.1.2" xref="S4.E5.m1.14.14.2.2.1.1.2.cmml"><mi id="S4.E5.m1.14.14.2.2.1.1.2.2.2" xref="S4.E5.m1.14.14.2.2.1.1.2.2.2.cmml">K</mi><mrow id="S4.E5.m1.6.6.2.4" xref="S4.E5.m1.6.6.2.3.cmml"><mo id="S4.E5.m1.6.6.2.4.1" stretchy="false" xref="S4.E5.m1.6.6.2.3.cmml">(</mo><mi id="S4.E5.m1.5.5.1.1" xref="S4.E5.m1.5.5.1.1.cmml">t</mi><mo id="S4.E5.m1.6.6.2.4.2" xref="S4.E5.m1.6.6.2.3.cmml">,</mo><mi id="S4.E5.m1.6.6.2.2" xref="S4.E5.m1.6.6.2.2.cmml">i</mi><mo id="S4.E5.m1.6.6.2.4.3" stretchy="false" xref="S4.E5.m1.6.6.2.3.cmml">)</mo></mrow><mi id="S4.E5.m1.14.14.2.2.1.1.2.2.3" xref="S4.E5.m1.14.14.2.2.1.1.2.2.3.cmml">m</mi></msubsup><mo id="S4.E5.m1.14.14.2.2.1.1.1" xref="S4.E5.m1.14.14.2.2.1.1.1.cmml">=</mo><mrow id="S4.E5.m1.14.14.2.2.1.1.3" xref="S4.E5.m1.14.14.2.2.1.1.3.cmml"><msubsup id="S4.E5.m1.14.14.2.2.1.1.3.2" xref="S4.E5.m1.14.14.2.2.1.1.3.2.cmml"><mi id="S4.E5.m1.14.14.2.2.1.1.3.2.2.2" xref="S4.E5.m1.14.14.2.2.1.1.3.2.2.2.cmml">W</mi><mi id="S4.E5.m1.14.14.2.2.1.1.3.2.3" xref="S4.E5.m1.14.14.2.2.1.1.3.2.3.cmml">K</mi><mi id="S4.E5.m1.14.14.2.2.1.1.3.2.2.3" xref="S4.E5.m1.14.14.2.2.1.1.3.2.2.3.cmml">m</mi></msubsup><mo id="S4.E5.m1.14.14.2.2.1.1.3.1" xref="S4.E5.m1.14.14.2.2.1.1.3.1.cmml">⁢</mo><msub id="S4.E5.m1.14.14.2.2.1.1.3.3" xref="S4.E5.m1.14.14.2.2.1.1.3.3.cmml"><mi id="S4.E5.m1.14.14.2.2.1.1.3.3.2" xref="S4.E5.m1.14.14.2.2.1.1.3.3.2.cmml">f</mi><mrow id="S4.E5.m1.8.8.2.4" xref="S4.E5.m1.8.8.2.3.cmml"><mo id="S4.E5.m1.8.8.2.4.1" stretchy="false" xref="S4.E5.m1.8.8.2.3.cmml">(</mo><mi id="S4.E5.m1.7.7.1.1" xref="S4.E5.m1.7.7.1.1.cmml">t</mi><mo id="S4.E5.m1.8.8.2.4.2" xref="S4.E5.m1.8.8.2.3.cmml">,</mo><mi id="S4.E5.m1.8.8.2.2" xref="S4.E5.m1.8.8.2.2.cmml">i</mi><mo id="S4.E5.m1.8.8.2.4.3" stretchy="false" xref="S4.E5.m1.8.8.2.3.cmml">)</mo></mrow></msub></mrow></mrow><mo id="S4.E5.m1.14.14.2.2.2.3" xref="S4.E5.m1.14.14.2.2.3a.cmml">,</mo><mrow id="S4.E5.m1.14.14.2.2.2.2" xref="S4.E5.m1.14.14.2.2.2.2.cmml"><msubsup id="S4.E5.m1.14.14.2.2.2.2.2" xref="S4.E5.m1.14.14.2.2.2.2.2.cmml"><mi id="S4.E5.m1.14.14.2.2.2.2.2.2.2" xref="S4.E5.m1.14.14.2.2.2.2.2.2.2.cmml">V</mi><mrow id="S4.E5.m1.10.10.2.4" xref="S4.E5.m1.10.10.2.3.cmml"><mo id="S4.E5.m1.10.10.2.4.1" stretchy="false" xref="S4.E5.m1.10.10.2.3.cmml">(</mo><mi id="S4.E5.m1.9.9.1.1" xref="S4.E5.m1.9.9.1.1.cmml">t</mi><mo id="S4.E5.m1.10.10.2.4.2" xref="S4.E5.m1.10.10.2.3.cmml">,</mo><mi id="S4.E5.m1.10.10.2.2" xref="S4.E5.m1.10.10.2.2.cmml">i</mi><mo id="S4.E5.m1.10.10.2.4.3" stretchy="false" xref="S4.E5.m1.10.10.2.3.cmml">)</mo></mrow><mi id="S4.E5.m1.14.14.2.2.2.2.2.2.3" xref="S4.E5.m1.14.14.2.2.2.2.2.2.3.cmml">m</mi></msubsup><mo id="S4.E5.m1.14.14.2.2.2.2.1" xref="S4.E5.m1.14.14.2.2.2.2.1.cmml">=</mo><mrow id="S4.E5.m1.14.14.2.2.2.2.3" xref="S4.E5.m1.14.14.2.2.2.2.3.cmml"><msubsup id="S4.E5.m1.14.14.2.2.2.2.3.2" xref="S4.E5.m1.14.14.2.2.2.2.3.2.cmml"><mi id="S4.E5.m1.14.14.2.2.2.2.3.2.2.2" xref="S4.E5.m1.14.14.2.2.2.2.3.2.2.2.cmml">W</mi><mi id="S4.E5.m1.14.14.2.2.2.2.3.2.3" xref="S4.E5.m1.14.14.2.2.2.2.3.2.3.cmml">V</mi><mi id="S4.E5.m1.14.14.2.2.2.2.3.2.2.3" xref="S4.E5.m1.14.14.2.2.2.2.3.2.2.3.cmml">m</mi></msubsup><mo id="S4.E5.m1.14.14.2.2.2.2.3.1" xref="S4.E5.m1.14.14.2.2.2.2.3.1.cmml">⁢</mo><msub id="S4.E5.m1.14.14.2.2.2.2.3.3" xref="S4.E5.m1.14.14.2.2.2.2.3.3.cmml"><mi id="S4.E5.m1.14.14.2.2.2.2.3.3.2" xref="S4.E5.m1.14.14.2.2.2.2.3.3.2.cmml">f</mi><mrow id="S4.E5.m1.12.12.2.4" xref="S4.E5.m1.12.12.2.3.cmml"><mo id="S4.E5.m1.12.12.2.4.1" stretchy="false" xref="S4.E5.m1.12.12.2.3.cmml">(</mo><mi id="S4.E5.m1.11.11.1.1" xref="S4.E5.m1.11.11.1.1.cmml">t</mi><mo id="S4.E5.m1.12.12.2.4.2" xref="S4.E5.m1.12.12.2.3.cmml">,</mo><mi id="S4.E5.m1.12.12.2.2" xref="S4.E5.m1.12.12.2.2.cmml">i</mi><mo id="S4.E5.m1.12.12.2.4.3" stretchy="false" xref="S4.E5.m1.12.12.2.3.cmml">)</mo></mrow></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.14b"><apply id="S4.E5.m1.14.14.3.cmml" xref="S4.E5.m1.14.14.2"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.3a.cmml" xref="S4.E5.m1.14.14.2.3">formulae-sequence</csymbol><apply id="S4.E5.m1.13.13.1.1.cmml" xref="S4.E5.m1.13.13.1.1"><eq id="S4.E5.m1.13.13.1.1.1.cmml" xref="S4.E5.m1.13.13.1.1.1"></eq><apply id="S4.E5.m1.13.13.1.1.2.cmml" xref="S4.E5.m1.13.13.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.13.13.1.1.2.1.cmml" xref="S4.E5.m1.13.13.1.1.2">subscript</csymbol><apply id="S4.E5.m1.13.13.1.1.2.2.cmml" xref="S4.E5.m1.13.13.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.13.13.1.1.2.2.1.cmml" xref="S4.E5.m1.13.13.1.1.2">superscript</csymbol><ci id="S4.E5.m1.13.13.1.1.2.2.2.cmml" xref="S4.E5.m1.13.13.1.1.2.2.2">𝑄</ci><ci id="S4.E5.m1.13.13.1.1.2.2.3.cmml" xref="S4.E5.m1.13.13.1.1.2.2.3">𝑚</ci></apply><interval closure="open" id="S4.E5.m1.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.4"><ci id="S4.E5.m1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1">𝑡</ci><ci id="S4.E5.m1.2.2.2.2.cmml" xref="S4.E5.m1.2.2.2.2">𝑖</ci></interval></apply><apply id="S4.E5.m1.13.13.1.1.3.cmml" xref="S4.E5.m1.13.13.1.1.3"><times id="S4.E5.m1.13.13.1.1.3.1.cmml" xref="S4.E5.m1.13.13.1.1.3.1"></times><apply id="S4.E5.m1.13.13.1.1.3.2.cmml" xref="S4.E5.m1.13.13.1.1.3.2"><csymbol cd="ambiguous" id="S4.E5.m1.13.13.1.1.3.2.1.cmml" xref="S4.E5.m1.13.13.1.1.3.2">subscript</csymbol><apply id="S4.E5.m1.13.13.1.1.3.2.2.cmml" xref="S4.E5.m1.13.13.1.1.3.2"><csymbol cd="ambiguous" id="S4.E5.m1.13.13.1.1.3.2.2.1.cmml" xref="S4.E5.m1.13.13.1.1.3.2">superscript</csymbol><ci id="S4.E5.m1.13.13.1.1.3.2.2.2.cmml" xref="S4.E5.m1.13.13.1.1.3.2.2.2">𝑊</ci><ci id="S4.E5.m1.13.13.1.1.3.2.2.3.cmml" xref="S4.E5.m1.13.13.1.1.3.2.2.3">𝑚</ci></apply><ci id="S4.E5.m1.13.13.1.1.3.2.3.cmml" xref="S4.E5.m1.13.13.1.1.3.2.3">𝑄</ci></apply><apply id="S4.E5.m1.13.13.1.1.3.3.cmml" xref="S4.E5.m1.13.13.1.1.3.3"><csymbol cd="ambiguous" id="S4.E5.m1.13.13.1.1.3.3.1.cmml" xref="S4.E5.m1.13.13.1.1.3.3">subscript</csymbol><ci id="S4.E5.m1.13.13.1.1.3.3.2.cmml" xref="S4.E5.m1.13.13.1.1.3.3.2">𝑓</ci><interval closure="open" id="S4.E5.m1.4.4.2.3.cmml" xref="S4.E5.m1.4.4.2.4"><ci id="S4.E5.m1.3.3.1.1.cmml" xref="S4.E5.m1.3.3.1.1">𝑡</ci><ci id="S4.E5.m1.4.4.2.2.cmml" xref="S4.E5.m1.4.4.2.2">𝑖</ci></interval></apply></apply></apply><apply id="S4.E5.m1.14.14.2.2.3.cmml" xref="S4.E5.m1.14.14.2.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.3a.cmml" xref="S4.E5.m1.14.14.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E5.m1.14.14.2.2.1.1.cmml" xref="S4.E5.m1.14.14.2.2.1.1"><eq id="S4.E5.m1.14.14.2.2.1.1.1.cmml" xref="S4.E5.m1.14.14.2.2.1.1.1"></eq><apply id="S4.E5.m1.14.14.2.2.1.1.2.cmml" xref="S4.E5.m1.14.14.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.1.1.2.1.cmml" xref="S4.E5.m1.14.14.2.2.1.1.2">subscript</csymbol><apply id="S4.E5.m1.14.14.2.2.1.1.2.2.cmml" xref="S4.E5.m1.14.14.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.1.1.2.2.1.cmml" xref="S4.E5.m1.14.14.2.2.1.1.2">superscript</csymbol><ci id="S4.E5.m1.14.14.2.2.1.1.2.2.2.cmml" xref="S4.E5.m1.14.14.2.2.1.1.2.2.2">𝐾</ci><ci id="S4.E5.m1.14.14.2.2.1.1.2.2.3.cmml" xref="S4.E5.m1.14.14.2.2.1.1.2.2.3">𝑚</ci></apply><interval closure="open" id="S4.E5.m1.6.6.2.3.cmml" xref="S4.E5.m1.6.6.2.4"><ci id="S4.E5.m1.5.5.1.1.cmml" xref="S4.E5.m1.5.5.1.1">𝑡</ci><ci id="S4.E5.m1.6.6.2.2.cmml" xref="S4.E5.m1.6.6.2.2">𝑖</ci></interval></apply><apply id="S4.E5.m1.14.14.2.2.1.1.3.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3"><times id="S4.E5.m1.14.14.2.2.1.1.3.1.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.1"></times><apply id="S4.E5.m1.14.14.2.2.1.1.3.2.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.1.1.3.2.1.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.2">subscript</csymbol><apply id="S4.E5.m1.14.14.2.2.1.1.3.2.2.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.1.1.3.2.2.1.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.2">superscript</csymbol><ci id="S4.E5.m1.14.14.2.2.1.1.3.2.2.2.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.2.2.2">𝑊</ci><ci id="S4.E5.m1.14.14.2.2.1.1.3.2.2.3.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.2.2.3">𝑚</ci></apply><ci id="S4.E5.m1.14.14.2.2.1.1.3.2.3.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.2.3">𝐾</ci></apply><apply id="S4.E5.m1.14.14.2.2.1.1.3.3.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.1.1.3.3.1.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.3">subscript</csymbol><ci id="S4.E5.m1.14.14.2.2.1.1.3.3.2.cmml" xref="S4.E5.m1.14.14.2.2.1.1.3.3.2">𝑓</ci><interval closure="open" id="S4.E5.m1.8.8.2.3.cmml" xref="S4.E5.m1.8.8.2.4"><ci id="S4.E5.m1.7.7.1.1.cmml" xref="S4.E5.m1.7.7.1.1">𝑡</ci><ci id="S4.E5.m1.8.8.2.2.cmml" xref="S4.E5.m1.8.8.2.2">𝑖</ci></interval></apply></apply></apply><apply id="S4.E5.m1.14.14.2.2.2.2.cmml" xref="S4.E5.m1.14.14.2.2.2.2"><eq id="S4.E5.m1.14.14.2.2.2.2.1.cmml" xref="S4.E5.m1.14.14.2.2.2.2.1"></eq><apply id="S4.E5.m1.14.14.2.2.2.2.2.cmml" xref="S4.E5.m1.14.14.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.2.2.2.1.cmml" xref="S4.E5.m1.14.14.2.2.2.2.2">subscript</csymbol><apply id="S4.E5.m1.14.14.2.2.2.2.2.2.cmml" xref="S4.E5.m1.14.14.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.2.2.2.2.1.cmml" xref="S4.E5.m1.14.14.2.2.2.2.2">superscript</csymbol><ci id="S4.E5.m1.14.14.2.2.2.2.2.2.2.cmml" xref="S4.E5.m1.14.14.2.2.2.2.2.2.2">𝑉</ci><ci id="S4.E5.m1.14.14.2.2.2.2.2.2.3.cmml" xref="S4.E5.m1.14.14.2.2.2.2.2.2.3">𝑚</ci></apply><interval closure="open" id="S4.E5.m1.10.10.2.3.cmml" xref="S4.E5.m1.10.10.2.4"><ci id="S4.E5.m1.9.9.1.1.cmml" xref="S4.E5.m1.9.9.1.1">𝑡</ci><ci id="S4.E5.m1.10.10.2.2.cmml" xref="S4.E5.m1.10.10.2.2">𝑖</ci></interval></apply><apply id="S4.E5.m1.14.14.2.2.2.2.3.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3"><times id="S4.E5.m1.14.14.2.2.2.2.3.1.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.1"></times><apply id="S4.E5.m1.14.14.2.2.2.2.3.2.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.2.2.3.2.1.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.2">subscript</csymbol><apply id="S4.E5.m1.14.14.2.2.2.2.3.2.2.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.2.2.3.2.2.1.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.2">superscript</csymbol><ci id="S4.E5.m1.14.14.2.2.2.2.3.2.2.2.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.2.2.2">𝑊</ci><ci id="S4.E5.m1.14.14.2.2.2.2.3.2.2.3.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.2.2.3">𝑚</ci></apply><ci id="S4.E5.m1.14.14.2.2.2.2.3.2.3.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.2.3">𝑉</ci></apply><apply id="S4.E5.m1.14.14.2.2.2.2.3.3.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S4.E5.m1.14.14.2.2.2.2.3.3.1.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.3">subscript</csymbol><ci id="S4.E5.m1.14.14.2.2.2.2.3.3.2.cmml" xref="S4.E5.m1.14.14.2.2.2.2.3.3.2">𝑓</ci><interval closure="open" id="S4.E5.m1.12.12.2.3.cmml" xref="S4.E5.m1.12.12.2.4"><ci id="S4.E5.m1.11.11.1.1.cmml" xref="S4.E5.m1.11.11.1.1">𝑡</ci><ci id="S4.E5.m1.12.12.2.2.cmml" xref="S4.E5.m1.12.12.2.2">𝑖</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.14c">Q^{m}_{(t,i)}=W^{m}_{Q}f_{(t,i)},K^{m}_{(t,i)}=W^{m}_{K}f_{(t,i)},V^{m}_{(t,i)%
}=W^{m}_{V}f_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.E5.m1.14d">italic_Q start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT = italic_W start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT , italic_K start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT = italic_W start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT , italic_V start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT = italic_W start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p3">
<p class="ltx_p" id="S4.SS3.SSS2.p3.6">Where, <math alttext="Q^{m}_{(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p3.1.m1.2"><semantics id="S4.SS3.SSS2.p3.1.m1.2a"><msubsup id="S4.SS3.SSS2.p3.1.m1.2.3" xref="S4.SS3.SSS2.p3.1.m1.2.3.cmml"><mi id="S4.SS3.SSS2.p3.1.m1.2.3.2.2" xref="S4.SS3.SSS2.p3.1.m1.2.3.2.2.cmml">Q</mi><mrow id="S4.SS3.SSS2.p3.1.m1.2.2.2.4" xref="S4.SS3.SSS2.p3.1.m1.2.2.2.3.cmml"><mo id="S4.SS3.SSS2.p3.1.m1.2.2.2.4.1" stretchy="false" xref="S4.SS3.SSS2.p3.1.m1.2.2.2.3.cmml">(</mo><mi id="S4.SS3.SSS2.p3.1.m1.1.1.1.1" xref="S4.SS3.SSS2.p3.1.m1.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS2.p3.1.m1.2.2.2.4.2" xref="S4.SS3.SSS2.p3.1.m1.2.2.2.3.cmml">,</mo><mi id="S4.SS3.SSS2.p3.1.m1.2.2.2.2" xref="S4.SS3.SSS2.p3.1.m1.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS2.p3.1.m1.2.2.2.4.3" stretchy="false" xref="S4.SS3.SSS2.p3.1.m1.2.2.2.3.cmml">)</mo></mrow><mi id="S4.SS3.SSS2.p3.1.m1.2.3.2.3" xref="S4.SS3.SSS2.p3.1.m1.2.3.2.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.1.m1.2b"><apply id="S4.SS3.SSS2.p3.1.m1.2.3.cmml" xref="S4.SS3.SSS2.p3.1.m1.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.1.m1.2.3.1.cmml" xref="S4.SS3.SSS2.p3.1.m1.2.3">subscript</csymbol><apply id="S4.SS3.SSS2.p3.1.m1.2.3.2.cmml" xref="S4.SS3.SSS2.p3.1.m1.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.1.m1.2.3.2.1.cmml" xref="S4.SS3.SSS2.p3.1.m1.2.3">superscript</csymbol><ci id="S4.SS3.SSS2.p3.1.m1.2.3.2.2.cmml" xref="S4.SS3.SSS2.p3.1.m1.2.3.2.2">𝑄</ci><ci id="S4.SS3.SSS2.p3.1.m1.2.3.2.3.cmml" xref="S4.SS3.SSS2.p3.1.m1.2.3.2.3">𝑚</ci></apply><interval closure="open" id="S4.SS3.SSS2.p3.1.m1.2.2.2.3.cmml" xref="S4.SS3.SSS2.p3.1.m1.2.2.2.4"><ci id="S4.SS3.SSS2.p3.1.m1.1.1.1.1.cmml" xref="S4.SS3.SSS2.p3.1.m1.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS2.p3.1.m1.2.2.2.2.cmml" xref="S4.SS3.SSS2.p3.1.m1.2.2.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.1.m1.2c">Q^{m}_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p3.1.m1.2d">italic_Q start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="K^{m}_{(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p3.2.m2.2"><semantics id="S4.SS3.SSS2.p3.2.m2.2a"><msubsup id="S4.SS3.SSS2.p3.2.m2.2.3" xref="S4.SS3.SSS2.p3.2.m2.2.3.cmml"><mi id="S4.SS3.SSS2.p3.2.m2.2.3.2.2" xref="S4.SS3.SSS2.p3.2.m2.2.3.2.2.cmml">K</mi><mrow id="S4.SS3.SSS2.p3.2.m2.2.2.2.4" xref="S4.SS3.SSS2.p3.2.m2.2.2.2.3.cmml"><mo id="S4.SS3.SSS2.p3.2.m2.2.2.2.4.1" stretchy="false" xref="S4.SS3.SSS2.p3.2.m2.2.2.2.3.cmml">(</mo><mi id="S4.SS3.SSS2.p3.2.m2.1.1.1.1" xref="S4.SS3.SSS2.p3.2.m2.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS2.p3.2.m2.2.2.2.4.2" xref="S4.SS3.SSS2.p3.2.m2.2.2.2.3.cmml">,</mo><mi id="S4.SS3.SSS2.p3.2.m2.2.2.2.2" xref="S4.SS3.SSS2.p3.2.m2.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS2.p3.2.m2.2.2.2.4.3" stretchy="false" xref="S4.SS3.SSS2.p3.2.m2.2.2.2.3.cmml">)</mo></mrow><mi id="S4.SS3.SSS2.p3.2.m2.2.3.2.3" xref="S4.SS3.SSS2.p3.2.m2.2.3.2.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.2.m2.2b"><apply id="S4.SS3.SSS2.p3.2.m2.2.3.cmml" xref="S4.SS3.SSS2.p3.2.m2.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.2.m2.2.3.1.cmml" xref="S4.SS3.SSS2.p3.2.m2.2.3">subscript</csymbol><apply id="S4.SS3.SSS2.p3.2.m2.2.3.2.cmml" xref="S4.SS3.SSS2.p3.2.m2.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.2.m2.2.3.2.1.cmml" xref="S4.SS3.SSS2.p3.2.m2.2.3">superscript</csymbol><ci id="S4.SS3.SSS2.p3.2.m2.2.3.2.2.cmml" xref="S4.SS3.SSS2.p3.2.m2.2.3.2.2">𝐾</ci><ci id="S4.SS3.SSS2.p3.2.m2.2.3.2.3.cmml" xref="S4.SS3.SSS2.p3.2.m2.2.3.2.3">𝑚</ci></apply><interval closure="open" id="S4.SS3.SSS2.p3.2.m2.2.2.2.3.cmml" xref="S4.SS3.SSS2.p3.2.m2.2.2.2.4"><ci id="S4.SS3.SSS2.p3.2.m2.1.1.1.1.cmml" xref="S4.SS3.SSS2.p3.2.m2.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS2.p3.2.m2.2.2.2.2.cmml" xref="S4.SS3.SSS2.p3.2.m2.2.2.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.2.m2.2c">K^{m}_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p3.2.m2.2d">italic_K start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="V^{m}_{(t,i)}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p3.3.m3.2"><semantics id="S4.SS3.SSS2.p3.3.m3.2a"><msubsup id="S4.SS3.SSS2.p3.3.m3.2.3" xref="S4.SS3.SSS2.p3.3.m3.2.3.cmml"><mi id="S4.SS3.SSS2.p3.3.m3.2.3.2.2" xref="S4.SS3.SSS2.p3.3.m3.2.3.2.2.cmml">V</mi><mrow id="S4.SS3.SSS2.p3.3.m3.2.2.2.4" xref="S4.SS3.SSS2.p3.3.m3.2.2.2.3.cmml"><mo id="S4.SS3.SSS2.p3.3.m3.2.2.2.4.1" stretchy="false" xref="S4.SS3.SSS2.p3.3.m3.2.2.2.3.cmml">(</mo><mi id="S4.SS3.SSS2.p3.3.m3.1.1.1.1" xref="S4.SS3.SSS2.p3.3.m3.1.1.1.1.cmml">t</mi><mo id="S4.SS3.SSS2.p3.3.m3.2.2.2.4.2" xref="S4.SS3.SSS2.p3.3.m3.2.2.2.3.cmml">,</mo><mi id="S4.SS3.SSS2.p3.3.m3.2.2.2.2" xref="S4.SS3.SSS2.p3.3.m3.2.2.2.2.cmml">i</mi><mo id="S4.SS3.SSS2.p3.3.m3.2.2.2.4.3" stretchy="false" xref="S4.SS3.SSS2.p3.3.m3.2.2.2.3.cmml">)</mo></mrow><mi id="S4.SS3.SSS2.p3.3.m3.2.3.2.3" xref="S4.SS3.SSS2.p3.3.m3.2.3.2.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.3.m3.2b"><apply id="S4.SS3.SSS2.p3.3.m3.2.3.cmml" xref="S4.SS3.SSS2.p3.3.m3.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.3.m3.2.3.1.cmml" xref="S4.SS3.SSS2.p3.3.m3.2.3">subscript</csymbol><apply id="S4.SS3.SSS2.p3.3.m3.2.3.2.cmml" xref="S4.SS3.SSS2.p3.3.m3.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.3.m3.2.3.2.1.cmml" xref="S4.SS3.SSS2.p3.3.m3.2.3">superscript</csymbol><ci id="S4.SS3.SSS2.p3.3.m3.2.3.2.2.cmml" xref="S4.SS3.SSS2.p3.3.m3.2.3.2.2">𝑉</ci><ci id="S4.SS3.SSS2.p3.3.m3.2.3.2.3.cmml" xref="S4.SS3.SSS2.p3.3.m3.2.3.2.3">𝑚</ci></apply><interval closure="open" id="S4.SS3.SSS2.p3.3.m3.2.2.2.3.cmml" xref="S4.SS3.SSS2.p3.3.m3.2.2.2.4"><ci id="S4.SS3.SSS2.p3.3.m3.1.1.1.1.cmml" xref="S4.SS3.SSS2.p3.3.m3.1.1.1.1">𝑡</ci><ci id="S4.SS3.SSS2.p3.3.m3.2.2.2.2.cmml" xref="S4.SS3.SSS2.p3.3.m3.2.2.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.3.m3.2c">V^{m}_{(t,i)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p3.3.m3.2d">italic_V start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> are defined the query, key and value nodes, respectively. The weight matrix for the m-the head attention can be denoted by <math alttext="W^{m}_{Q}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p3.4.m4.1"><semantics id="S4.SS3.SSS2.p3.4.m4.1a"><msubsup id="S4.SS3.SSS2.p3.4.m4.1.1" xref="S4.SS3.SSS2.p3.4.m4.1.1.cmml"><mi id="S4.SS3.SSS2.p3.4.m4.1.1.2.2" xref="S4.SS3.SSS2.p3.4.m4.1.1.2.2.cmml">W</mi><mi id="S4.SS3.SSS2.p3.4.m4.1.1.3" xref="S4.SS3.SSS2.p3.4.m4.1.1.3.cmml">Q</mi><mi id="S4.SS3.SSS2.p3.4.m4.1.1.2.3" xref="S4.SS3.SSS2.p3.4.m4.1.1.2.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.4.m4.1b"><apply id="S4.SS3.SSS2.p3.4.m4.1.1.cmml" xref="S4.SS3.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.4.m4.1.1.1.cmml" xref="S4.SS3.SSS2.p3.4.m4.1.1">subscript</csymbol><apply id="S4.SS3.SSS2.p3.4.m4.1.1.2.cmml" xref="S4.SS3.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.4.m4.1.1.2.1.cmml" xref="S4.SS3.SSS2.p3.4.m4.1.1">superscript</csymbol><ci id="S4.SS3.SSS2.p3.4.m4.1.1.2.2.cmml" xref="S4.SS3.SSS2.p3.4.m4.1.1.2.2">𝑊</ci><ci id="S4.SS3.SSS2.p3.4.m4.1.1.2.3.cmml" xref="S4.SS3.SSS2.p3.4.m4.1.1.2.3">𝑚</ci></apply><ci id="S4.SS3.SSS2.p3.4.m4.1.1.3.cmml" xref="S4.SS3.SSS2.p3.4.m4.1.1.3">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.4.m4.1c">W^{m}_{Q}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p3.4.m4.1d">italic_W start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="W^{m}_{K}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p3.5.m5.1"><semantics id="S4.SS3.SSS2.p3.5.m5.1a"><msubsup id="S4.SS3.SSS2.p3.5.m5.1.1" xref="S4.SS3.SSS2.p3.5.m5.1.1.cmml"><mi id="S4.SS3.SSS2.p3.5.m5.1.1.2.2" xref="S4.SS3.SSS2.p3.5.m5.1.1.2.2.cmml">W</mi><mi id="S4.SS3.SSS2.p3.5.m5.1.1.3" xref="S4.SS3.SSS2.p3.5.m5.1.1.3.cmml">K</mi><mi id="S4.SS3.SSS2.p3.5.m5.1.1.2.3" xref="S4.SS3.SSS2.p3.5.m5.1.1.2.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.5.m5.1b"><apply id="S4.SS3.SSS2.p3.5.m5.1.1.cmml" xref="S4.SS3.SSS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.5.m5.1.1.1.cmml" xref="S4.SS3.SSS2.p3.5.m5.1.1">subscript</csymbol><apply id="S4.SS3.SSS2.p3.5.m5.1.1.2.cmml" xref="S4.SS3.SSS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.5.m5.1.1.2.1.cmml" xref="S4.SS3.SSS2.p3.5.m5.1.1">superscript</csymbol><ci id="S4.SS3.SSS2.p3.5.m5.1.1.2.2.cmml" xref="S4.SS3.SSS2.p3.5.m5.1.1.2.2">𝑊</ci><ci id="S4.SS3.SSS2.p3.5.m5.1.1.2.3.cmml" xref="S4.SS3.SSS2.p3.5.m5.1.1.2.3">𝑚</ci></apply><ci id="S4.SS3.SSS2.p3.5.m5.1.1.3.cmml" xref="S4.SS3.SSS2.p3.5.m5.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.5.m5.1c">W^{m}_{K}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p3.5.m5.1d">italic_W start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="W^{m}_{V}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p3.6.m6.1"><semantics id="S4.SS3.SSS2.p3.6.m6.1a"><msubsup id="S4.SS3.SSS2.p3.6.m6.1.1" xref="S4.SS3.SSS2.p3.6.m6.1.1.cmml"><mi id="S4.SS3.SSS2.p3.6.m6.1.1.2.2" xref="S4.SS3.SSS2.p3.6.m6.1.1.2.2.cmml">W</mi><mi id="S4.SS3.SSS2.p3.6.m6.1.1.3" xref="S4.SS3.SSS2.p3.6.m6.1.1.3.cmml">V</mi><mi id="S4.SS3.SSS2.p3.6.m6.1.1.2.3" xref="S4.SS3.SSS2.p3.6.m6.1.1.2.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.6.m6.1b"><apply id="S4.SS3.SSS2.p3.6.m6.1.1.cmml" xref="S4.SS3.SSS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.6.m6.1.1.1.cmml" xref="S4.SS3.SSS2.p3.6.m6.1.1">subscript</csymbol><apply id="S4.SS3.SSS2.p3.6.m6.1.1.2.cmml" xref="S4.SS3.SSS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p3.6.m6.1.1.2.1.cmml" xref="S4.SS3.SSS2.p3.6.m6.1.1">superscript</csymbol><ci id="S4.SS3.SSS2.p3.6.m6.1.1.2.2.cmml" xref="S4.SS3.SSS2.p3.6.m6.1.1.2.2">𝑊</ci><ci id="S4.SS3.SSS2.p3.6.m6.1.1.2.3.cmml" xref="S4.SS3.SSS2.p3.6.m6.1.1.2.3">𝑚</ci></apply><ci id="S4.SS3.SSS2.p3.6.m6.1.1.3.cmml" xref="S4.SS3.SSS2.p3.6.m6.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.6.m6.1c">W^{m}_{V}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p3.6.m6.1d">italic_W start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math> and this weight matrix is computed from the three stages; the first stage computes the dot product between the first two matrices and applies the softmax activation to normalize the output. In the second stage, we employed the masking operation to determine the type of attention, either spatial or temporal domain. This is mainly decided by assigning some value to the weight matrix. For example, if we determine spatial attention, we block the temporal position by assigning 0 and pass the spatial value by assigning one and vice versa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib35" title="">35</a>]</cite>. In the third stage, we multiplied the spatial or temporal feature with the value vector and produced the feature for the m-th head attention. In the same way, we computed the attention for eight heads and concatenated to produce the combined feature for the multi-head using the following Equation (<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.E6" title="In 4.3.2 Multihead Attention Module ‣ 4.3 Architecture for Branches of Spatial-Temporal Approach ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p4">
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\bar{f}_{(t,i)}=Concat[\bar{f}^{1}_{(t,i)},\bar{f}^{2}_{(t,i)},\bar{f}^{3}_{(t%
,i)},...,\bar{f}^{M}_{t,i}]" class="ltx_Math" display="block" id="S4.E6.m1.15"><semantics id="S4.E6.m1.15a"><mrow id="S4.E6.m1.15.15" xref="S4.E6.m1.15.15.cmml"><msub id="S4.E6.m1.15.15.6" xref="S4.E6.m1.15.15.6.cmml"><mover accent="true" id="S4.E6.m1.15.15.6.2" xref="S4.E6.m1.15.15.6.2.cmml"><mi id="S4.E6.m1.15.15.6.2.2" xref="S4.E6.m1.15.15.6.2.2.cmml">f</mi><mo id="S4.E6.m1.15.15.6.2.1" xref="S4.E6.m1.15.15.6.2.1.cmml">¯</mo></mover><mrow id="S4.E6.m1.2.2.2.4" xref="S4.E6.m1.2.2.2.3.cmml"><mo id="S4.E6.m1.2.2.2.4.1" stretchy="false" xref="S4.E6.m1.2.2.2.3.cmml">(</mo><mi id="S4.E6.m1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.cmml">t</mi><mo id="S4.E6.m1.2.2.2.4.2" xref="S4.E6.m1.2.2.2.3.cmml">,</mo><mi id="S4.E6.m1.2.2.2.2" xref="S4.E6.m1.2.2.2.2.cmml">i</mi><mo id="S4.E6.m1.2.2.2.4.3" stretchy="false" xref="S4.E6.m1.2.2.2.3.cmml">)</mo></mrow></msub><mo id="S4.E6.m1.15.15.5" xref="S4.E6.m1.15.15.5.cmml">=</mo><mrow id="S4.E6.m1.15.15.4" xref="S4.E6.m1.15.15.4.cmml"><mi id="S4.E6.m1.15.15.4.6" xref="S4.E6.m1.15.15.4.6.cmml">C</mi><mo id="S4.E6.m1.15.15.4.5" xref="S4.E6.m1.15.15.4.5.cmml">⁢</mo><mi id="S4.E6.m1.15.15.4.7" xref="S4.E6.m1.15.15.4.7.cmml">o</mi><mo id="S4.E6.m1.15.15.4.5a" xref="S4.E6.m1.15.15.4.5.cmml">⁢</mo><mi id="S4.E6.m1.15.15.4.8" xref="S4.E6.m1.15.15.4.8.cmml">n</mi><mo id="S4.E6.m1.15.15.4.5b" xref="S4.E6.m1.15.15.4.5.cmml">⁢</mo><mi id="S4.E6.m1.15.15.4.9" xref="S4.E6.m1.15.15.4.9.cmml">c</mi><mo id="S4.E6.m1.15.15.4.5c" xref="S4.E6.m1.15.15.4.5.cmml">⁢</mo><mi id="S4.E6.m1.15.15.4.10" xref="S4.E6.m1.15.15.4.10.cmml">a</mi><mo id="S4.E6.m1.15.15.4.5d" xref="S4.E6.m1.15.15.4.5.cmml">⁢</mo><mi id="S4.E6.m1.15.15.4.11" xref="S4.E6.m1.15.15.4.11.cmml">t</mi><mo id="S4.E6.m1.15.15.4.5e" xref="S4.E6.m1.15.15.4.5.cmml">⁢</mo><mrow id="S4.E6.m1.15.15.4.4.4" xref="S4.E6.m1.15.15.4.4.5.cmml"><mo id="S4.E6.m1.15.15.4.4.4.5" stretchy="false" xref="S4.E6.m1.15.15.4.4.5.cmml">[</mo><msubsup id="S4.E6.m1.12.12.1.1.1.1" xref="S4.E6.m1.12.12.1.1.1.1.cmml"><mover accent="true" id="S4.E6.m1.12.12.1.1.1.1.2.2" xref="S4.E6.m1.12.12.1.1.1.1.2.2.cmml"><mi id="S4.E6.m1.12.12.1.1.1.1.2.2.2" xref="S4.E6.m1.12.12.1.1.1.1.2.2.2.cmml">f</mi><mo id="S4.E6.m1.12.12.1.1.1.1.2.2.1" xref="S4.E6.m1.12.12.1.1.1.1.2.2.1.cmml">¯</mo></mover><mrow id="S4.E6.m1.4.4.2.4" xref="S4.E6.m1.4.4.2.3.cmml"><mo id="S4.E6.m1.4.4.2.4.1" stretchy="false" xref="S4.E6.m1.4.4.2.3.cmml">(</mo><mi id="S4.E6.m1.3.3.1.1" xref="S4.E6.m1.3.3.1.1.cmml">t</mi><mo id="S4.E6.m1.4.4.2.4.2" xref="S4.E6.m1.4.4.2.3.cmml">,</mo><mi id="S4.E6.m1.4.4.2.2" xref="S4.E6.m1.4.4.2.2.cmml">i</mi><mo id="S4.E6.m1.4.4.2.4.3" stretchy="false" xref="S4.E6.m1.4.4.2.3.cmml">)</mo></mrow><mn id="S4.E6.m1.12.12.1.1.1.1.2.3" xref="S4.E6.m1.12.12.1.1.1.1.2.3.cmml">1</mn></msubsup><mo id="S4.E6.m1.15.15.4.4.4.6" xref="S4.E6.m1.15.15.4.4.5.cmml">,</mo><msubsup id="S4.E6.m1.13.13.2.2.2.2" xref="S4.E6.m1.13.13.2.2.2.2.cmml"><mover accent="true" id="S4.E6.m1.13.13.2.2.2.2.2.2" xref="S4.E6.m1.13.13.2.2.2.2.2.2.cmml"><mi id="S4.E6.m1.13.13.2.2.2.2.2.2.2" xref="S4.E6.m1.13.13.2.2.2.2.2.2.2.cmml">f</mi><mo id="S4.E6.m1.13.13.2.2.2.2.2.2.1" xref="S4.E6.m1.13.13.2.2.2.2.2.2.1.cmml">¯</mo></mover><mrow id="S4.E6.m1.6.6.2.4" xref="S4.E6.m1.6.6.2.3.cmml"><mo id="S4.E6.m1.6.6.2.4.1" stretchy="false" xref="S4.E6.m1.6.6.2.3.cmml">(</mo><mi id="S4.E6.m1.5.5.1.1" xref="S4.E6.m1.5.5.1.1.cmml">t</mi><mo id="S4.E6.m1.6.6.2.4.2" xref="S4.E6.m1.6.6.2.3.cmml">,</mo><mi id="S4.E6.m1.6.6.2.2" xref="S4.E6.m1.6.6.2.2.cmml">i</mi><mo id="S4.E6.m1.6.6.2.4.3" stretchy="false" xref="S4.E6.m1.6.6.2.3.cmml">)</mo></mrow><mn id="S4.E6.m1.13.13.2.2.2.2.2.3" xref="S4.E6.m1.13.13.2.2.2.2.2.3.cmml">2</mn></msubsup><mo id="S4.E6.m1.15.15.4.4.4.7" xref="S4.E6.m1.15.15.4.4.5.cmml">,</mo><msubsup id="S4.E6.m1.14.14.3.3.3.3" xref="S4.E6.m1.14.14.3.3.3.3.cmml"><mover accent="true" id="S4.E6.m1.14.14.3.3.3.3.2.2" xref="S4.E6.m1.14.14.3.3.3.3.2.2.cmml"><mi id="S4.E6.m1.14.14.3.3.3.3.2.2.2" xref="S4.E6.m1.14.14.3.3.3.3.2.2.2.cmml">f</mi><mo id="S4.E6.m1.14.14.3.3.3.3.2.2.1" xref="S4.E6.m1.14.14.3.3.3.3.2.2.1.cmml">¯</mo></mover><mrow id="S4.E6.m1.8.8.2.4" xref="S4.E6.m1.8.8.2.3.cmml"><mo id="S4.E6.m1.8.8.2.4.1" stretchy="false" xref="S4.E6.m1.8.8.2.3.cmml">(</mo><mi id="S4.E6.m1.7.7.1.1" xref="S4.E6.m1.7.7.1.1.cmml">t</mi><mo id="S4.E6.m1.8.8.2.4.2" xref="S4.E6.m1.8.8.2.3.cmml">,</mo><mi id="S4.E6.m1.8.8.2.2" xref="S4.E6.m1.8.8.2.2.cmml">i</mi><mo id="S4.E6.m1.8.8.2.4.3" stretchy="false" xref="S4.E6.m1.8.8.2.3.cmml">)</mo></mrow><mn id="S4.E6.m1.14.14.3.3.3.3.2.3" xref="S4.E6.m1.14.14.3.3.3.3.2.3.cmml">3</mn></msubsup><mo id="S4.E6.m1.15.15.4.4.4.8" xref="S4.E6.m1.15.15.4.4.5.cmml">,</mo><mi id="S4.E6.m1.11.11" mathvariant="normal" xref="S4.E6.m1.11.11.cmml">…</mi><mo id="S4.E6.m1.15.15.4.4.4.9" xref="S4.E6.m1.15.15.4.4.5.cmml">,</mo><msubsup id="S4.E6.m1.15.15.4.4.4.4" xref="S4.E6.m1.15.15.4.4.4.4.cmml"><mover accent="true" id="S4.E6.m1.15.15.4.4.4.4.2.2" xref="S4.E6.m1.15.15.4.4.4.4.2.2.cmml"><mi id="S4.E6.m1.15.15.4.4.4.4.2.2.2" xref="S4.E6.m1.15.15.4.4.4.4.2.2.2.cmml">f</mi><mo id="S4.E6.m1.15.15.4.4.4.4.2.2.1" xref="S4.E6.m1.15.15.4.4.4.4.2.2.1.cmml">¯</mo></mover><mrow id="S4.E6.m1.10.10.2.4" xref="S4.E6.m1.10.10.2.3.cmml"><mi id="S4.E6.m1.9.9.1.1" xref="S4.E6.m1.9.9.1.1.cmml">t</mi><mo id="S4.E6.m1.10.10.2.4.1" xref="S4.E6.m1.10.10.2.3.cmml">,</mo><mi id="S4.E6.m1.10.10.2.2" xref="S4.E6.m1.10.10.2.2.cmml">i</mi></mrow><mi id="S4.E6.m1.15.15.4.4.4.4.2.3" xref="S4.E6.m1.15.15.4.4.4.4.2.3.cmml">M</mi></msubsup><mo id="S4.E6.m1.15.15.4.4.4.10" stretchy="false" xref="S4.E6.m1.15.15.4.4.5.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.15b"><apply id="S4.E6.m1.15.15.cmml" xref="S4.E6.m1.15.15"><eq id="S4.E6.m1.15.15.5.cmml" xref="S4.E6.m1.15.15.5"></eq><apply id="S4.E6.m1.15.15.6.cmml" xref="S4.E6.m1.15.15.6"><csymbol cd="ambiguous" id="S4.E6.m1.15.15.6.1.cmml" xref="S4.E6.m1.15.15.6">subscript</csymbol><apply id="S4.E6.m1.15.15.6.2.cmml" xref="S4.E6.m1.15.15.6.2"><ci id="S4.E6.m1.15.15.6.2.1.cmml" xref="S4.E6.m1.15.15.6.2.1">¯</ci><ci id="S4.E6.m1.15.15.6.2.2.cmml" xref="S4.E6.m1.15.15.6.2.2">𝑓</ci></apply><interval closure="open" id="S4.E6.m1.2.2.2.3.cmml" xref="S4.E6.m1.2.2.2.4"><ci id="S4.E6.m1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1">𝑡</ci><ci id="S4.E6.m1.2.2.2.2.cmml" xref="S4.E6.m1.2.2.2.2">𝑖</ci></interval></apply><apply id="S4.E6.m1.15.15.4.cmml" xref="S4.E6.m1.15.15.4"><times id="S4.E6.m1.15.15.4.5.cmml" xref="S4.E6.m1.15.15.4.5"></times><ci id="S4.E6.m1.15.15.4.6.cmml" xref="S4.E6.m1.15.15.4.6">𝐶</ci><ci id="S4.E6.m1.15.15.4.7.cmml" xref="S4.E6.m1.15.15.4.7">𝑜</ci><ci id="S4.E6.m1.15.15.4.8.cmml" xref="S4.E6.m1.15.15.4.8">𝑛</ci><ci id="S4.E6.m1.15.15.4.9.cmml" xref="S4.E6.m1.15.15.4.9">𝑐</ci><ci id="S4.E6.m1.15.15.4.10.cmml" xref="S4.E6.m1.15.15.4.10">𝑎</ci><ci id="S4.E6.m1.15.15.4.11.cmml" xref="S4.E6.m1.15.15.4.11">𝑡</ci><list id="S4.E6.m1.15.15.4.4.5.cmml" xref="S4.E6.m1.15.15.4.4.4"><apply id="S4.E6.m1.12.12.1.1.1.1.cmml" xref="S4.E6.m1.12.12.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.12.12.1.1.1.1.1.cmml" xref="S4.E6.m1.12.12.1.1.1.1">subscript</csymbol><apply id="S4.E6.m1.12.12.1.1.1.1.2.cmml" xref="S4.E6.m1.12.12.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.12.12.1.1.1.1.2.1.cmml" xref="S4.E6.m1.12.12.1.1.1.1">superscript</csymbol><apply id="S4.E6.m1.12.12.1.1.1.1.2.2.cmml" xref="S4.E6.m1.12.12.1.1.1.1.2.2"><ci id="S4.E6.m1.12.12.1.1.1.1.2.2.1.cmml" xref="S4.E6.m1.12.12.1.1.1.1.2.2.1">¯</ci><ci id="S4.E6.m1.12.12.1.1.1.1.2.2.2.cmml" xref="S4.E6.m1.12.12.1.1.1.1.2.2.2">𝑓</ci></apply><cn id="S4.E6.m1.12.12.1.1.1.1.2.3.cmml" type="integer" xref="S4.E6.m1.12.12.1.1.1.1.2.3">1</cn></apply><interval closure="open" id="S4.E6.m1.4.4.2.3.cmml" xref="S4.E6.m1.4.4.2.4"><ci id="S4.E6.m1.3.3.1.1.cmml" xref="S4.E6.m1.3.3.1.1">𝑡</ci><ci id="S4.E6.m1.4.4.2.2.cmml" xref="S4.E6.m1.4.4.2.2">𝑖</ci></interval></apply><apply id="S4.E6.m1.13.13.2.2.2.2.cmml" xref="S4.E6.m1.13.13.2.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.13.13.2.2.2.2.1.cmml" xref="S4.E6.m1.13.13.2.2.2.2">subscript</csymbol><apply id="S4.E6.m1.13.13.2.2.2.2.2.cmml" xref="S4.E6.m1.13.13.2.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.13.13.2.2.2.2.2.1.cmml" xref="S4.E6.m1.13.13.2.2.2.2">superscript</csymbol><apply id="S4.E6.m1.13.13.2.2.2.2.2.2.cmml" xref="S4.E6.m1.13.13.2.2.2.2.2.2"><ci id="S4.E6.m1.13.13.2.2.2.2.2.2.1.cmml" xref="S4.E6.m1.13.13.2.2.2.2.2.2.1">¯</ci><ci id="S4.E6.m1.13.13.2.2.2.2.2.2.2.cmml" xref="S4.E6.m1.13.13.2.2.2.2.2.2.2">𝑓</ci></apply><cn id="S4.E6.m1.13.13.2.2.2.2.2.3.cmml" type="integer" xref="S4.E6.m1.13.13.2.2.2.2.2.3">2</cn></apply><interval closure="open" id="S4.E6.m1.6.6.2.3.cmml" xref="S4.E6.m1.6.6.2.4"><ci id="S4.E6.m1.5.5.1.1.cmml" xref="S4.E6.m1.5.5.1.1">𝑡</ci><ci id="S4.E6.m1.6.6.2.2.cmml" xref="S4.E6.m1.6.6.2.2">𝑖</ci></interval></apply><apply id="S4.E6.m1.14.14.3.3.3.3.cmml" xref="S4.E6.m1.14.14.3.3.3.3"><csymbol cd="ambiguous" id="S4.E6.m1.14.14.3.3.3.3.1.cmml" xref="S4.E6.m1.14.14.3.3.3.3">subscript</csymbol><apply id="S4.E6.m1.14.14.3.3.3.3.2.cmml" xref="S4.E6.m1.14.14.3.3.3.3"><csymbol cd="ambiguous" id="S4.E6.m1.14.14.3.3.3.3.2.1.cmml" xref="S4.E6.m1.14.14.3.3.3.3">superscript</csymbol><apply id="S4.E6.m1.14.14.3.3.3.3.2.2.cmml" xref="S4.E6.m1.14.14.3.3.3.3.2.2"><ci id="S4.E6.m1.14.14.3.3.3.3.2.2.1.cmml" xref="S4.E6.m1.14.14.3.3.3.3.2.2.1">¯</ci><ci id="S4.E6.m1.14.14.3.3.3.3.2.2.2.cmml" xref="S4.E6.m1.14.14.3.3.3.3.2.2.2">𝑓</ci></apply><cn id="S4.E6.m1.14.14.3.3.3.3.2.3.cmml" type="integer" xref="S4.E6.m1.14.14.3.3.3.3.2.3">3</cn></apply><interval closure="open" id="S4.E6.m1.8.8.2.3.cmml" xref="S4.E6.m1.8.8.2.4"><ci id="S4.E6.m1.7.7.1.1.cmml" xref="S4.E6.m1.7.7.1.1">𝑡</ci><ci id="S4.E6.m1.8.8.2.2.cmml" xref="S4.E6.m1.8.8.2.2">𝑖</ci></interval></apply><ci id="S4.E6.m1.11.11.cmml" xref="S4.E6.m1.11.11">…</ci><apply id="S4.E6.m1.15.15.4.4.4.4.cmml" xref="S4.E6.m1.15.15.4.4.4.4"><csymbol cd="ambiguous" id="S4.E6.m1.15.15.4.4.4.4.1.cmml" xref="S4.E6.m1.15.15.4.4.4.4">subscript</csymbol><apply id="S4.E6.m1.15.15.4.4.4.4.2.cmml" xref="S4.E6.m1.15.15.4.4.4.4"><csymbol cd="ambiguous" id="S4.E6.m1.15.15.4.4.4.4.2.1.cmml" xref="S4.E6.m1.15.15.4.4.4.4">superscript</csymbol><apply id="S4.E6.m1.15.15.4.4.4.4.2.2.cmml" xref="S4.E6.m1.15.15.4.4.4.4.2.2"><ci id="S4.E6.m1.15.15.4.4.4.4.2.2.1.cmml" xref="S4.E6.m1.15.15.4.4.4.4.2.2.1">¯</ci><ci id="S4.E6.m1.15.15.4.4.4.4.2.2.2.cmml" xref="S4.E6.m1.15.15.4.4.4.4.2.2.2">𝑓</ci></apply><ci id="S4.E6.m1.15.15.4.4.4.4.2.3.cmml" xref="S4.E6.m1.15.15.4.4.4.4.2.3">𝑀</ci></apply><list id="S4.E6.m1.10.10.2.3.cmml" xref="S4.E6.m1.10.10.2.4"><ci id="S4.E6.m1.9.9.1.1.cmml" xref="S4.E6.m1.9.9.1.1">𝑡</ci><ci id="S4.E6.m1.10.10.2.2.cmml" xref="S4.E6.m1.10.10.2.2">𝑖</ci></list></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.15c">\bar{f}_{(t,i)}=Concat[\bar{f}^{1}_{(t,i)},\bar{f}^{2}_{(t,i)},\bar{f}^{3}_{(t%
,i)},...,\bar{f}^{M}_{t,i}]</annotation><annotation encoding="application/x-llamapun" id="S4.E6.m1.15d">over¯ start_ARG italic_f end_ARG start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT = italic_C italic_o italic_n italic_c italic_a italic_t [ over¯ start_ARG italic_f end_ARG start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT , over¯ start_ARG italic_f end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT , over¯ start_ARG italic_f end_ARG start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_t , italic_i ) end_POSTSUBSCRIPT , … , over¯ start_ARG italic_f end_ARG start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , italic_i end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p5">
<p class="ltx_p" id="S4.SS3.SSS2.p5.3">The total number is defined here with the M, and we consider 8 in our study. Our model produced the spatial feature <math alttext="A_{S}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p5.1.m1.1"><semantics id="S4.SS3.SSS2.p5.1.m1.1a"><msub id="S4.SS3.SSS2.p5.1.m1.1.1" xref="S4.SS3.SSS2.p5.1.m1.1.1.cmml"><mi id="S4.SS3.SSS2.p5.1.m1.1.1.2" xref="S4.SS3.SSS2.p5.1.m1.1.1.2.cmml">A</mi><mi id="S4.SS3.SSS2.p5.1.m1.1.1.3" xref="S4.SS3.SSS2.p5.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.1.m1.1b"><apply id="S4.SS3.SSS2.p5.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p5.1.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p5.1.m1.1.1.2.cmml" xref="S4.SS3.SSS2.p5.1.m1.1.1.2">𝐴</ci><ci id="S4.SS3.SSS2.p5.1.m1.1.1.3.cmml" xref="S4.SS3.SSS2.p5.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.1.m1.1c">A_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p5.1.m1.1d">italic_A start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> from the 1st branch, the temporal feature <math alttext="A_{T}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p5.2.m2.1"><semantics id="S4.SS3.SSS2.p5.2.m2.1a"><msub id="S4.SS3.SSS2.p5.2.m2.1.1" xref="S4.SS3.SSS2.p5.2.m2.1.1.cmml"><mi id="S4.SS3.SSS2.p5.2.m2.1.1.2" xref="S4.SS3.SSS2.p5.2.m2.1.1.2.cmml">A</mi><mi id="S4.SS3.SSS2.p5.2.m2.1.1.3" xref="S4.SS3.SSS2.p5.2.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.2.m2.1b"><apply id="S4.SS3.SSS2.p5.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p5.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p5.2.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1.2">𝐴</ci><ci id="S4.SS3.SSS2.p5.2.m2.1.1.3.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.2.m2.1c">A_{T}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p5.2.m2.1d">italic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> from the 2nd branch and the spatial-temporal <math alttext="A_{ST}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p5.3.m3.1"><semantics id="S4.SS3.SSS2.p5.3.m3.1a"><msub id="S4.SS3.SSS2.p5.3.m3.1.1" xref="S4.SS3.SSS2.p5.3.m3.1.1.cmml"><mi id="S4.SS3.SSS2.p5.3.m3.1.1.2" xref="S4.SS3.SSS2.p5.3.m3.1.1.2.cmml">A</mi><mrow id="S4.SS3.SSS2.p5.3.m3.1.1.3" xref="S4.SS3.SSS2.p5.3.m3.1.1.3.cmml"><mi id="S4.SS3.SSS2.p5.3.m3.1.1.3.2" xref="S4.SS3.SSS2.p5.3.m3.1.1.3.2.cmml">S</mi><mo id="S4.SS3.SSS2.p5.3.m3.1.1.3.1" xref="S4.SS3.SSS2.p5.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS3.SSS2.p5.3.m3.1.1.3.3" xref="S4.SS3.SSS2.p5.3.m3.1.1.3.3.cmml">T</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.3.m3.1b"><apply id="S4.SS3.SSS2.p5.3.m3.1.1.cmml" xref="S4.SS3.SSS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p5.3.m3.1.1.1.cmml" xref="S4.SS3.SSS2.p5.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p5.3.m3.1.1.2.cmml" xref="S4.SS3.SSS2.p5.3.m3.1.1.2">𝐴</ci><apply id="S4.SS3.SSS2.p5.3.m3.1.1.3.cmml" xref="S4.SS3.SSS2.p5.3.m3.1.1.3"><times id="S4.SS3.SSS2.p5.3.m3.1.1.3.1.cmml" xref="S4.SS3.SSS2.p5.3.m3.1.1.3.1"></times><ci id="S4.SS3.SSS2.p5.3.m3.1.1.3.2.cmml" xref="S4.SS3.SSS2.p5.3.m3.1.1.3.2">𝑆</ci><ci id="S4.SS3.SSS2.p5.3.m3.1.1.3.3.cmml" xref="S4.SS3.SSS2.p5.3.m3.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.3.m3.1c">A_{ST}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p5.3.m3.1d">italic_A start_POSTSUBSCRIPT italic_S italic_T end_POSTSUBSCRIPT</annotation></semantics></math> feature from the 3rd branch.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Mask Operation. </h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">In our study, to reduce the computational complexity, we first employed the Sep-TCN to minimize the parameters of the model and mask operation to reduce the computational complexity of the attention model sharply. In each stream of the architecture, we employed masking operations to cut down the computational cost based on spatial and temporal concepts. In the mask operation, we assign 0 and 1 based on the type of attention block where 0 means to block or cut down the dimension based on value and one means to pass the value. For the spatial attention module, we assign 1 for all the values for the spatial position and 0 for the temporal position, which means it blocks all the values for the temporal block and, consequently, produces the spatial features. In the same procedure we followed for the temporal attention block, we assigned 1 for the temporal position for passing the temporal feature and blocked the spatial feature by assigning 0 to the spatial position. The mask operation yields the reduced feature, which also sharply reduces the data block size, making the model faster and more efficient by reducing the computational cost <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib35" title="">35</a>]</cite>. In short, we can say that masking operations make Multihead Self-Attention (MHSA) either a spatial or temporal attention model. The internal mechanism for the masking operation is demonstrated in Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fi:Mask_Operation</span>.</p>
</div>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S4.F8.2">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F8.1">-3cm-3cm
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="102" id="S4.F8.1.g1" src="x4.jpg" width="501"/>
</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Internal Structure of the masking operation</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Classification</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">After concatenating three stream features, we made a final feature vector, which we fed into the classifier module. After that, we applied a classification module shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.F4" title="Figure 4 ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a>(c). Which included a fully connected layer, rectified linear activation, normalization, leaky ReLU activation and dropout layer. The fully connected layer has 128 input features and 128 output features. This rectified linear activation function applies element-wise ReLU to the input tensor. The normalization layer applies layer normalization to the input tensor. Layer normalization normalizes the features across the channel dimension for each sample in the batch. After that, a leaky ReLU activation function with a negative slope 0.1. It applies element-wise leaky ReLU to the input tensor. Where used is an argument that indicates that the operation will modify the input tensor directly without allocating additional memory. The dropout layer randomly zeroes some of the elements of the input tensor with a probability value. The overall purpose of this sequence of layers is to transform the input features into a higher-level representation that is better suited for classification. The layers use nonlinear transformations and normalization techniques to extract meaningful features from the input. The dropout layer helps prevent overfitting by randomly dropping out some input features during training.
In our model, we averaged the video to make the matrix into a vector and applied an activation layer.
As the averaging technique specifically, we used the ”row-wise averaging” technique in our implementation. This technique calculates the mean or average value of each row in the data. This operation computes the average of each row in the com array that we fed into the activation layer.
For simplicity, we finally employed a categorical cross-entropy during training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib12" title="">12</a>]</cite>, Equation(<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.E7" title="In 4.4 Classification ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">7</span></a>) showed the formula of loss and prediction calculation.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="Loss(\varnothing,\hat{\varnothing})=-\sum_{c=1}^{C}\varnothing_{c}\log\hat{%
\varnothing}_{c}" class="ltx_Math" display="block" id="S4.E7.m1.2"><semantics id="S4.E7.m1.2a"><mrow id="S4.E7.m1.2.3" xref="S4.E7.m1.2.3.cmml"><mrow id="S4.E7.m1.2.3.2" xref="S4.E7.m1.2.3.2.cmml"><mi id="S4.E7.m1.2.3.2.2" xref="S4.E7.m1.2.3.2.2.cmml">L</mi><mo id="S4.E7.m1.2.3.2.1" xref="S4.E7.m1.2.3.2.1.cmml">⁢</mo><mi id="S4.E7.m1.2.3.2.3" xref="S4.E7.m1.2.3.2.3.cmml">o</mi><mo id="S4.E7.m1.2.3.2.1a" xref="S4.E7.m1.2.3.2.1.cmml">⁢</mo><mi id="S4.E7.m1.2.3.2.4" xref="S4.E7.m1.2.3.2.4.cmml">s</mi><mo id="S4.E7.m1.2.3.2.1b" xref="S4.E7.m1.2.3.2.1.cmml">⁢</mo><mi id="S4.E7.m1.2.3.2.5" xref="S4.E7.m1.2.3.2.5.cmml">s</mi><mo id="S4.E7.m1.2.3.2.1c" xref="S4.E7.m1.2.3.2.1.cmml">⁢</mo><mrow id="S4.E7.m1.2.3.2.6.2" xref="S4.E7.m1.2.3.2.6.1.cmml"><mo id="S4.E7.m1.2.3.2.6.2.1" stretchy="false" xref="S4.E7.m1.2.3.2.6.1.cmml">(</mo><mi id="S4.E7.m1.1.1" mathvariant="normal" xref="S4.E7.m1.1.1.cmml">∅</mi><mo id="S4.E7.m1.2.3.2.6.2.2" xref="S4.E7.m1.2.3.2.6.1.cmml">,</mo><mover accent="true" id="S4.E7.m1.2.2" xref="S4.E7.m1.2.2.cmml"><mi id="S4.E7.m1.2.2.2" mathvariant="normal" xref="S4.E7.m1.2.2.2.cmml">∅</mi><mo id="S4.E7.m1.2.2.1" xref="S4.E7.m1.2.2.1.cmml">^</mo></mover><mo id="S4.E7.m1.2.3.2.6.2.3" stretchy="false" xref="S4.E7.m1.2.3.2.6.1.cmml">)</mo></mrow></mrow><mo id="S4.E7.m1.2.3.1" xref="S4.E7.m1.2.3.1.cmml">=</mo><mrow id="S4.E7.m1.2.3.3" xref="S4.E7.m1.2.3.3.cmml"><mo id="S4.E7.m1.2.3.3a" xref="S4.E7.m1.2.3.3.cmml">−</mo><mrow id="S4.E7.m1.2.3.3.2" xref="S4.E7.m1.2.3.3.2.cmml"><munderover id="S4.E7.m1.2.3.3.2.1" xref="S4.E7.m1.2.3.3.2.1.cmml"><mo id="S4.E7.m1.2.3.3.2.1.2.2" movablelimits="false" xref="S4.E7.m1.2.3.3.2.1.2.2.cmml">∑</mo><mrow id="S4.E7.m1.2.3.3.2.1.2.3" xref="S4.E7.m1.2.3.3.2.1.2.3.cmml"><mi id="S4.E7.m1.2.3.3.2.1.2.3.2" xref="S4.E7.m1.2.3.3.2.1.2.3.2.cmml">c</mi><mo id="S4.E7.m1.2.3.3.2.1.2.3.1" xref="S4.E7.m1.2.3.3.2.1.2.3.1.cmml">=</mo><mn id="S4.E7.m1.2.3.3.2.1.2.3.3" xref="S4.E7.m1.2.3.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E7.m1.2.3.3.2.1.3" xref="S4.E7.m1.2.3.3.2.1.3.cmml">C</mi></munderover><mrow id="S4.E7.m1.2.3.3.2.2" xref="S4.E7.m1.2.3.3.2.2.cmml"><msub id="S4.E7.m1.2.3.3.2.2.2" xref="S4.E7.m1.2.3.3.2.2.2.cmml"><mi id="S4.E7.m1.2.3.3.2.2.2.2" mathvariant="normal" xref="S4.E7.m1.2.3.3.2.2.2.2.cmml">∅</mi><mi id="S4.E7.m1.2.3.3.2.2.2.3" xref="S4.E7.m1.2.3.3.2.2.2.3.cmml">c</mi></msub><mo id="S4.E7.m1.2.3.3.2.2.1" lspace="0.167em" xref="S4.E7.m1.2.3.3.2.2.1.cmml">⁢</mo><mrow id="S4.E7.m1.2.3.3.2.2.3" xref="S4.E7.m1.2.3.3.2.2.3.cmml"><mi id="S4.E7.m1.2.3.3.2.2.3.1" xref="S4.E7.m1.2.3.3.2.2.3.1.cmml">log</mi><mo id="S4.E7.m1.2.3.3.2.2.3a" lspace="0.167em" xref="S4.E7.m1.2.3.3.2.2.3.cmml">⁡</mo><msub id="S4.E7.m1.2.3.3.2.2.3.2" xref="S4.E7.m1.2.3.3.2.2.3.2.cmml"><mover accent="true" id="S4.E7.m1.2.3.3.2.2.3.2.2" xref="S4.E7.m1.2.3.3.2.2.3.2.2.cmml"><mi id="S4.E7.m1.2.3.3.2.2.3.2.2.2" mathvariant="normal" xref="S4.E7.m1.2.3.3.2.2.3.2.2.2.cmml">∅</mi><mo id="S4.E7.m1.2.3.3.2.2.3.2.2.1" xref="S4.E7.m1.2.3.3.2.2.3.2.2.1.cmml">^</mo></mover><mi id="S4.E7.m1.2.3.3.2.2.3.2.3" xref="S4.E7.m1.2.3.3.2.2.3.2.3.cmml">c</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.2b"><apply id="S4.E7.m1.2.3.cmml" xref="S4.E7.m1.2.3"><eq id="S4.E7.m1.2.3.1.cmml" xref="S4.E7.m1.2.3.1"></eq><apply id="S4.E7.m1.2.3.2.cmml" xref="S4.E7.m1.2.3.2"><times id="S4.E7.m1.2.3.2.1.cmml" xref="S4.E7.m1.2.3.2.1"></times><ci id="S4.E7.m1.2.3.2.2.cmml" xref="S4.E7.m1.2.3.2.2">𝐿</ci><ci id="S4.E7.m1.2.3.2.3.cmml" xref="S4.E7.m1.2.3.2.3">𝑜</ci><ci id="S4.E7.m1.2.3.2.4.cmml" xref="S4.E7.m1.2.3.2.4">𝑠</ci><ci id="S4.E7.m1.2.3.2.5.cmml" xref="S4.E7.m1.2.3.2.5">𝑠</ci><interval closure="open" id="S4.E7.m1.2.3.2.6.1.cmml" xref="S4.E7.m1.2.3.2.6.2"><emptyset id="S4.E7.m1.1.1.cmml" xref="S4.E7.m1.1.1"></emptyset><apply id="S4.E7.m1.2.2.cmml" xref="S4.E7.m1.2.2"><ci id="S4.E7.m1.2.2.1.cmml" xref="S4.E7.m1.2.2.1">^</ci><emptyset id="S4.E7.m1.2.2.2.cmml" xref="S4.E7.m1.2.2.2"></emptyset></apply></interval></apply><apply id="S4.E7.m1.2.3.3.cmml" xref="S4.E7.m1.2.3.3"><minus id="S4.E7.m1.2.3.3.1.cmml" xref="S4.E7.m1.2.3.3"></minus><apply id="S4.E7.m1.2.3.3.2.cmml" xref="S4.E7.m1.2.3.3.2"><apply id="S4.E7.m1.2.3.3.2.1.cmml" xref="S4.E7.m1.2.3.3.2.1"><csymbol cd="ambiguous" id="S4.E7.m1.2.3.3.2.1.1.cmml" xref="S4.E7.m1.2.3.3.2.1">superscript</csymbol><apply id="S4.E7.m1.2.3.3.2.1.2.cmml" xref="S4.E7.m1.2.3.3.2.1"><csymbol cd="ambiguous" id="S4.E7.m1.2.3.3.2.1.2.1.cmml" xref="S4.E7.m1.2.3.3.2.1">subscript</csymbol><sum id="S4.E7.m1.2.3.3.2.1.2.2.cmml" xref="S4.E7.m1.2.3.3.2.1.2.2"></sum><apply id="S4.E7.m1.2.3.3.2.1.2.3.cmml" xref="S4.E7.m1.2.3.3.2.1.2.3"><eq id="S4.E7.m1.2.3.3.2.1.2.3.1.cmml" xref="S4.E7.m1.2.3.3.2.1.2.3.1"></eq><ci id="S4.E7.m1.2.3.3.2.1.2.3.2.cmml" xref="S4.E7.m1.2.3.3.2.1.2.3.2">𝑐</ci><cn id="S4.E7.m1.2.3.3.2.1.2.3.3.cmml" type="integer" xref="S4.E7.m1.2.3.3.2.1.2.3.3">1</cn></apply></apply><ci id="S4.E7.m1.2.3.3.2.1.3.cmml" xref="S4.E7.m1.2.3.3.2.1.3">𝐶</ci></apply><apply id="S4.E7.m1.2.3.3.2.2.cmml" xref="S4.E7.m1.2.3.3.2.2"><times id="S4.E7.m1.2.3.3.2.2.1.cmml" xref="S4.E7.m1.2.3.3.2.2.1"></times><apply id="S4.E7.m1.2.3.3.2.2.2.cmml" xref="S4.E7.m1.2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.3.3.2.2.2.1.cmml" xref="S4.E7.m1.2.3.3.2.2.2">subscript</csymbol><emptyset id="S4.E7.m1.2.3.3.2.2.2.2.cmml" xref="S4.E7.m1.2.3.3.2.2.2.2"></emptyset><ci id="S4.E7.m1.2.3.3.2.2.2.3.cmml" xref="S4.E7.m1.2.3.3.2.2.2.3">𝑐</ci></apply><apply id="S4.E7.m1.2.3.3.2.2.3.cmml" xref="S4.E7.m1.2.3.3.2.2.3"><log id="S4.E7.m1.2.3.3.2.2.3.1.cmml" xref="S4.E7.m1.2.3.3.2.2.3.1"></log><apply id="S4.E7.m1.2.3.3.2.2.3.2.cmml" xref="S4.E7.m1.2.3.3.2.2.3.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.3.3.2.2.3.2.1.cmml" xref="S4.E7.m1.2.3.3.2.2.3.2">subscript</csymbol><apply id="S4.E7.m1.2.3.3.2.2.3.2.2.cmml" xref="S4.E7.m1.2.3.3.2.2.3.2.2"><ci id="S4.E7.m1.2.3.3.2.2.3.2.2.1.cmml" xref="S4.E7.m1.2.3.3.2.2.3.2.2.1">^</ci><emptyset id="S4.E7.m1.2.3.3.2.2.3.2.2.2.cmml" xref="S4.E7.m1.2.3.3.2.2.3.2.2.2"></emptyset></apply><ci id="S4.E7.m1.2.3.3.2.2.3.2.3.cmml" xref="S4.E7.m1.2.3.3.2.2.3.2.3">𝑐</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.2c">Loss(\varnothing,\hat{\varnothing})=-\sum_{c=1}^{C}\varnothing_{c}\log\hat{%
\varnothing}_{c}</annotation><annotation encoding="application/x-llamapun" id="S4.E7.m1.2d">italic_L italic_o italic_s italic_s ( ∅ , over^ start_ARG ∅ end_ARG ) = - ∑ start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT ∅ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT roman_log over^ start_ARG ∅ end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS4.p2.2">Here, C is the number of classes, <math alttext="\varnothing" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" mathvariant="normal" xref="S4.SS4.p2.1.m1.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><emptyset id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\varnothing</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">∅</annotation></semantics></math> is the ground truth, and <math alttext="\hat{\varnothing}" class="ltx_Math" display="inline" id="S4.SS4.p2.2.m2.1"><semantics id="S4.SS4.p2.2.m2.1a"><mover accent="true" id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mi id="S4.SS4.p2.2.m2.1.1.2" mathvariant="normal" xref="S4.SS4.p2.2.m2.1.1.2.cmml">∅</mi><mo id="S4.SS4.p2.2.m2.1.1.1" xref="S4.SS4.p2.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><ci id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1.1">^</ci><emptyset id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">\hat{\varnothing}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.2.m2.1d">over^ start_ARG ∅ end_ARG</annotation></semantics></math>is the prediction.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Dimension of Feature Map</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.5">In our research study, we leveraged the Hand Pose Media Pipe system to extract a set of 213 skeleton points for each frame. Here, the number ’21’ represents the total count of joints in the hand, while ’3’ corresponds to the x, y, and z coordinate values associated with each of these skeletal points. For each individual trial within our dataset, we carefully structured it to encompass three frames, with the intent of creating a sequential or consecutive frame sequence within each trial. To effectively manage and process our data, we further organized these individual trials into batches, with each batch consisting of 8 trials. This bundling of data led to the dimensional representation of a single batch being the dimension becomes <math alttext="Batch\times Trial" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mrow id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml"><mrow id="S4.SS5.p1.1.m1.1.1.2" xref="S4.SS5.p1.1.m1.1.1.2.cmml"><mrow id="S4.SS5.p1.1.m1.1.1.2.2" xref="S4.SS5.p1.1.m1.1.1.2.2.cmml"><mi id="S4.SS5.p1.1.m1.1.1.2.2.2" xref="S4.SS5.p1.1.m1.1.1.2.2.2.cmml">B</mi><mo id="S4.SS5.p1.1.m1.1.1.2.2.1" xref="S4.SS5.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S4.SS5.p1.1.m1.1.1.2.2.3" xref="S4.SS5.p1.1.m1.1.1.2.2.3.cmml">a</mi><mo id="S4.SS5.p1.1.m1.1.1.2.2.1a" xref="S4.SS5.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S4.SS5.p1.1.m1.1.1.2.2.4" xref="S4.SS5.p1.1.m1.1.1.2.2.4.cmml">t</mi><mo id="S4.SS5.p1.1.m1.1.1.2.2.1b" xref="S4.SS5.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S4.SS5.p1.1.m1.1.1.2.2.5" xref="S4.SS5.p1.1.m1.1.1.2.2.5.cmml">c</mi><mo id="S4.SS5.p1.1.m1.1.1.2.2.1c" xref="S4.SS5.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S4.SS5.p1.1.m1.1.1.2.2.6" xref="S4.SS5.p1.1.m1.1.1.2.2.6.cmml">h</mi></mrow><mo id="S4.SS5.p1.1.m1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.1.m1.1.1.2.1.cmml">×</mo><mi id="S4.SS5.p1.1.m1.1.1.2.3" xref="S4.SS5.p1.1.m1.1.1.2.3.cmml">T</mi></mrow><mo id="S4.SS5.p1.1.m1.1.1.1" xref="S4.SS5.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS5.p1.1.m1.1.1.3" xref="S4.SS5.p1.1.m1.1.1.3.cmml">r</mi><mo id="S4.SS5.p1.1.m1.1.1.1a" xref="S4.SS5.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS5.p1.1.m1.1.1.4" xref="S4.SS5.p1.1.m1.1.1.4.cmml">i</mi><mo id="S4.SS5.p1.1.m1.1.1.1b" xref="S4.SS5.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS5.p1.1.m1.1.1.5" xref="S4.SS5.p1.1.m1.1.1.5.cmml">a</mi><mo id="S4.SS5.p1.1.m1.1.1.1c" xref="S4.SS5.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS5.p1.1.m1.1.1.6" xref="S4.SS5.p1.1.m1.1.1.6.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><apply id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"><times id="S4.SS5.p1.1.m1.1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1.1"></times><apply id="S4.SS5.p1.1.m1.1.1.2.cmml" xref="S4.SS5.p1.1.m1.1.1.2"><times id="S4.SS5.p1.1.m1.1.1.2.1.cmml" xref="S4.SS5.p1.1.m1.1.1.2.1"></times><apply id="S4.SS5.p1.1.m1.1.1.2.2.cmml" xref="S4.SS5.p1.1.m1.1.1.2.2"><times id="S4.SS5.p1.1.m1.1.1.2.2.1.cmml" xref="S4.SS5.p1.1.m1.1.1.2.2.1"></times><ci id="S4.SS5.p1.1.m1.1.1.2.2.2.cmml" xref="S4.SS5.p1.1.m1.1.1.2.2.2">𝐵</ci><ci id="S4.SS5.p1.1.m1.1.1.2.2.3.cmml" xref="S4.SS5.p1.1.m1.1.1.2.2.3">𝑎</ci><ci id="S4.SS5.p1.1.m1.1.1.2.2.4.cmml" xref="S4.SS5.p1.1.m1.1.1.2.2.4">𝑡</ci><ci id="S4.SS5.p1.1.m1.1.1.2.2.5.cmml" xref="S4.SS5.p1.1.m1.1.1.2.2.5">𝑐</ci><ci id="S4.SS5.p1.1.m1.1.1.2.2.6.cmml" xref="S4.SS5.p1.1.m1.1.1.2.2.6">ℎ</ci></apply><ci id="S4.SS5.p1.1.m1.1.1.2.3.cmml" xref="S4.SS5.p1.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S4.SS5.p1.1.m1.1.1.3.cmml" xref="S4.SS5.p1.1.m1.1.1.3">𝑟</ci><ci id="S4.SS5.p1.1.m1.1.1.4.cmml" xref="S4.SS5.p1.1.m1.1.1.4">𝑖</ci><ci id="S4.SS5.p1.1.m1.1.1.5.cmml" xref="S4.SS5.p1.1.m1.1.1.5">𝑎</ci><ci id="S4.SS5.p1.1.m1.1.1.6.cmml" xref="S4.SS5.p1.1.m1.1.1.6">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">Batch\times Trial</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">italic_B italic_a italic_t italic_c italic_h × italic_T italic_r italic_i italic_a italic_l</annotation></semantics></math>, where Batch is 8 here.
This dataset, with its specified dimensions, was then seamlessly integrated into the Sequential Temporal Convolutional Network (Sep TCN) architecture. The Sep TCN, adept at handling sequential data, transformed our input data into a higher-level representation. Consequently, the output from the Sep TCN exhibited a dimension of <math alttext="8\times 128\times 3\times 21" class="ltx_Math" display="inline" id="S4.SS5.p1.2.m2.1"><semantics id="S4.SS5.p1.2.m2.1a"><mrow id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml"><mn id="S4.SS5.p1.2.m2.1.1.2" xref="S4.SS5.p1.2.m2.1.1.2.cmml">8</mn><mo id="S4.SS5.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.2.m2.1.1.3" xref="S4.SS5.p1.2.m2.1.1.3.cmml">128</mn><mo id="S4.SS5.p1.2.m2.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.2.m2.1.1.4" xref="S4.SS5.p1.2.m2.1.1.4.cmml">3</mn><mo id="S4.SS5.p1.2.m2.1.1.1b" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.2.m2.1.1.5" xref="S4.SS5.p1.2.m2.1.1.5.cmml">21</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><apply id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1"><times id="S4.SS5.p1.2.m2.1.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1.1"></times><cn id="S4.SS5.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS5.p1.2.m2.1.1.2">8</cn><cn id="S4.SS5.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS5.p1.2.m2.1.1.3">128</cn><cn id="S4.SS5.p1.2.m2.1.1.4.cmml" type="integer" xref="S4.SS5.p1.2.m2.1.1.4">3</cn><cn id="S4.SS5.p1.2.m2.1.1.5.cmml" type="integer" xref="S4.SS5.p1.2.m2.1.1.5">21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">8\times 128\times 3\times 21</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.2.m2.1d">8 × 128 × 3 × 21</annotation></semantics></math>. Here, the ’128’ signifies the number of output channels, ’3’ denotes the number of frames per trial, ’21’ represents the number of joints, and ’8’ corresponds to the total number of batches. Then we fed this feature dimension into four distinct streams: (i) Spatial Attention Map with Spatial PosEmbedding (ii) Temporal Attention map with Temporal Position Embedding.
(iii) Spatial Attention Map with Spatial Position Embedding-Temporal Attention Map with Temporal Position Embedding (iv) Residual Connection. Each of these streams independently generated feature representations characterized by dimensions of <math alttext="8\times 84\times 128" class="ltx_Math" display="inline" id="S4.SS5.p1.3.m3.1"><semantics id="S4.SS5.p1.3.m3.1a"><mrow id="S4.SS5.p1.3.m3.1.1" xref="S4.SS5.p1.3.m3.1.1.cmml"><mn id="S4.SS5.p1.3.m3.1.1.2" xref="S4.SS5.p1.3.m3.1.1.2.cmml">8</mn><mo id="S4.SS5.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.3.m3.1.1.3" xref="S4.SS5.p1.3.m3.1.1.3.cmml">84</mn><mo id="S4.SS5.p1.3.m3.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.3.m3.1.1.4" xref="S4.SS5.p1.3.m3.1.1.4.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.3.m3.1b"><apply id="S4.SS5.p1.3.m3.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1"><times id="S4.SS5.p1.3.m3.1.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1.1"></times><cn id="S4.SS5.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS5.p1.3.m3.1.1.2">8</cn><cn id="S4.SS5.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS5.p1.3.m3.1.1.3">84</cn><cn id="S4.SS5.p1.3.m3.1.1.4.cmml" type="integer" xref="S4.SS5.p1.3.m3.1.1.4">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.3.m3.1c">8\times 84\times 128</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.3.m3.1d">8 × 84 × 128</annotation></semantics></math>. Subsequently, to consolidate the information from these diverse streams, we concatenated the feature representations from all four streams. This fusion resulted in a unified feature representation with dimensions of <math alttext="8\times 336\times 128" class="ltx_Math" display="inline" id="S4.SS5.p1.4.m4.1"><semantics id="S4.SS5.p1.4.m4.1a"><mrow id="S4.SS5.p1.4.m4.1.1" xref="S4.SS5.p1.4.m4.1.1.cmml"><mn id="S4.SS5.p1.4.m4.1.1.2" xref="S4.SS5.p1.4.m4.1.1.2.cmml">8</mn><mo id="S4.SS5.p1.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.4.m4.1.1.3" xref="S4.SS5.p1.4.m4.1.1.3.cmml">336</mn><mo id="S4.SS5.p1.4.m4.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.4.m4.1.1.4" xref="S4.SS5.p1.4.m4.1.1.4.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.4.m4.1b"><apply id="S4.SS5.p1.4.m4.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1"><times id="S4.SS5.p1.4.m4.1.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1.1"></times><cn id="S4.SS5.p1.4.m4.1.1.2.cmml" type="integer" xref="S4.SS5.p1.4.m4.1.1.2">8</cn><cn id="S4.SS5.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS5.p1.4.m4.1.1.3">336</cn><cn id="S4.SS5.p1.4.m4.1.1.4.cmml" type="integer" xref="S4.SS5.p1.4.m4.1.1.4">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.4.m4.1c">8\times 336\times 128</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.4.m4.1d">8 × 336 × 128</annotation></semantics></math>. This concatenated feature representation was then forwarded to the classification module, which played a pivotal role in mapping the extracted features to their respective class labels. After this classification step, we applied an averaging technique, specifically referred to as ”row-wise averaging.” This technique computed the mean along the rows of our feature representation, thereby reducing the dimensionality to <math alttext="8\times 128" class="ltx_Math" display="inline" id="S4.SS5.p1.5.m5.1"><semantics id="S4.SS5.p1.5.m5.1a"><mrow id="S4.SS5.p1.5.m5.1.1" xref="S4.SS5.p1.5.m5.1.1.cmml"><mn id="S4.SS5.p1.5.m5.1.1.2" xref="S4.SS5.p1.5.m5.1.1.2.cmml">8</mn><mo id="S4.SS5.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.5.m5.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.5.m5.1.1.3" xref="S4.SS5.p1.5.m5.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.5.m5.1b"><apply id="S4.SS5.p1.5.m5.1.1.cmml" xref="S4.SS5.p1.5.m5.1.1"><times id="S4.SS5.p1.5.m5.1.1.1.cmml" xref="S4.SS5.p1.5.m5.1.1.1"></times><cn id="S4.SS5.p1.5.m5.1.1.2.cmml" type="integer" xref="S4.SS5.p1.5.m5.1.1.2">8</cn><cn id="S4.SS5.p1.5.m5.1.1.3.cmml" type="integer" xref="S4.SS5.p1.5.m5.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.5.m5.1c">8\times 128</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.5.m5.1d">8 × 128</annotation></semantics></math>. Finally, this refined feature representation was channelled into the fully connected layer, which ultimately yielded the final output corresponding to the distinct classes in our classification task. The detailed description of the dimension of a feature map process outlines the essential steps involved in our research methodology, demonstrating the meticulous handling and transformation of input data as it traverses through our model architecture to ultimately yield the desired classification outcomes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We tuned the parameters in each machine-learning model. This section demonstrated the evaluation metrics, parameter tuning, experimental setting, results and comparison with the state-of-the-art model. After that, the reason for the distance- and angle-based features are discussed.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text" id="S5.T2.2.1" style="color:#0000FF;">Possible hyperparameters</span></figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.1.1">Hyperparameter Name</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.1.2">Proposed Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.1.3">Existing Transfer Learning</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.2.1.1">Training : Testing</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.2.1.2">70%:30%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.2.1.3">70%:30%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.2">
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.2.1">Dp rate</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.2.2">0.01</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.2.3">0.01</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.4.3">
<td class="ltx_td ltx_align_left" id="S5.T2.3.4.3.1">Learning Rate</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.4.3.2">5e-6 to 1e-3</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.4.3.3">1e-3</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.5.4">
<td class="ltx_td ltx_align_left" id="S5.T2.3.5.4.1">Optimizer</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.5.4.2">Adam</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.5.4.3">Adam</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.6.5">
<td class="ltx_td ltx_align_left" id="S5.T2.3.6.5.1">Batch Size</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.6.5.2">8 - 32</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.6.5.3">8</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.7.6">
<td class="ltx_td ltx_align_left" id="S5.T2.3.7.6.1">Epochs</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.7.6.2">100-1000</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.7.6.3">500</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.8.7">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.3.8.7.1">Patient</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.3.8.7.2">100</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.3.8.7.3">100</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Environmental Setting</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To evaluate our proposed model when split into training and testing like 70% and 30%, respectively. In addition, to prove the generalization property, we consider the inter, intra, and merge evaluation settings. Intra-dataset settings define the training and testing that come from the same dataset. Inter dataset is that one dataset is used for training, and a different dataset is used for testing. To implement the model, we used PyTorch python framework and Google Colab Pro edition environment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib70" title="">70</a>]</cite>. This was mainly developed with Tesla P100, which integrated 25GB GPU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib71" title="">71</a>]</cite>. To implement the concept of computational Graph compatibility and adaptability with minimum resources and open-source properties pytorch has become a boon to the deep learning model. For the initial preprocessing of the image, we use OpenCV python package <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib72" title="">72</a>]</cite> and Mediapipe to extract the hand key points. After extracting the hand key points, we make it a CSV file and a pickle file. To process the mathematical and statistical operation of the dataset, we used Numpy and Pandas Python packages which mainly give us various facilities for the various matrix operations. To visualize the various figures, we used the Matplotlib package. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T2" title="Table 2 ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">2</span></a> The table reveals several critical hyperparameters used in the experiment. The first column contains the hyperparameter name, the second column represents the proposed model, and the third column represents the existing Transfer Learning-based model we experimented on here for validation purposes. In the hyperparameter column, first, the ”dp rate” represents the dropout rate, typically set at 0.01 to regulate overfitting. The ”learning rate” determines the optimizer’s step size, typically within the Adam optimizer. Although not explicitly defined, ”batch size” is crucial for training, usually set when creating data loaders. The ”epochs” denote the number of training iterations, with 500 specified in this context. Additionally, ”patience” controls early stopping, though it lacks an explicit definition. While these parameters are evident, it’s essential to acknowledge that there may be other hyperparameters tied to model architecture, data preprocessing, or optimization that are relevant but not explicitly detailed in the provided code.
As a training parameter, we epochs 100-1000 and initial learning <math alttext="5\mathrm{e}{-6}to1\mathrm{e}{-3}" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mrow id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml"><mn id="S5.SS1.p1.1.m1.1.1.2.2" xref="S5.SS1.p1.1.m1.1.1.2.2.cmml">5</mn><mo id="S5.SS1.p1.1.m1.1.1.2.1" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S5.SS1.p1.1.m1.1.1.2.3" mathvariant="normal" xref="S5.SS1.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">−</mo><mrow id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml"><mn id="S5.SS1.p1.1.m1.1.1.3.2" xref="S5.SS1.p1.1.m1.1.1.3.2.cmml">6</mn><mo id="S5.SS1.p1.1.m1.1.1.3.1" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.1.m1.1.1.3.3" xref="S5.SS1.p1.1.m1.1.1.3.3.cmml">t</mi><mo id="S5.SS1.p1.1.m1.1.1.3.1a" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.1.m1.1.1.3.4" xref="S5.SS1.p1.1.m1.1.1.3.4.cmml">o</mi><mo id="S5.SS1.p1.1.m1.1.1.3.1b" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mn id="S5.SS1.p1.1.m1.1.1.3.5" xref="S5.SS1.p1.1.m1.1.1.3.5.cmml">1</mn><mo id="S5.SS1.p1.1.m1.1.1.3.1c" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.1.m1.1.1.3.6" mathvariant="normal" xref="S5.SS1.p1.1.m1.1.1.3.6.cmml">e</mi></mrow><mo id="S5.SS1.p1.1.m1.1.1.1a" xref="S5.SS1.p1.1.m1.1.1.1.cmml">−</mo><mn id="S5.SS1.p1.1.m1.1.1.4" xref="S5.SS1.p1.1.m1.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><minus id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></minus><apply id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2"><times id="S5.SS1.p1.1.m1.1.1.2.1.cmml" xref="S5.SS1.p1.1.m1.1.1.2.1"></times><cn id="S5.SS1.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.2.2">5</cn><ci id="S5.SS1.p1.1.m1.1.1.2.3.cmml" xref="S5.SS1.p1.1.m1.1.1.2.3">e</ci></apply><apply id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3"><times id="S5.SS1.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.3.1"></times><cn id="S5.SS1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.3.2">6</cn><ci id="S5.SS1.p1.1.m1.1.1.3.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3.3">𝑡</ci><ci id="S5.SS1.p1.1.m1.1.1.3.4.cmml" xref="S5.SS1.p1.1.m1.1.1.3.4">𝑜</ci><cn id="S5.SS1.p1.1.m1.1.1.3.5.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.3.5">1</cn><ci id="S5.SS1.p1.1.m1.1.1.3.6.cmml" xref="S5.SS1.p1.1.m1.1.1.3.6">e</ci></apply><cn id="S5.SS1.p1.1.m1.1.1.4.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">5\mathrm{e}{-6}to1\mathrm{e}{-3}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">5 roman_e - 6 italic_t italic_o 1 roman_e - 3</annotation></semantics></math>, which considered the higher fluctuation during the optimizer like ADAM and Nesterov momentum <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib74" title="">74</a>]</cite>. Moreover, in this study, we employed different parameter tuning operations to optimise the learning rate and optimizer with multiclass.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation metrics</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Measuring the performance of the proposed model for the BSL recognition, we used accuracy, precision, recall and F1-score, which are mainly computed from the false positive (FP), true positive (TP), false negative (FN) and true negative(TN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib12" title="">12</a>]</cite>. The calculation formula for calculating the performance matrix is given below. Accuracy is calculated according to Equation (<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.E8" title="In 5.2 Evaluation metrics ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">8</span></a>), where the total correctly predicted classes are divided by the total count.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S5.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="Accuracy=(TP+TN)/(TP+FN+FP+TN)\times 100" class="ltx_Math" display="block" id="S5.E8.m1.2"><semantics id="S5.E8.m1.2a"><mrow id="S5.E8.m1.2.2" xref="S5.E8.m1.2.2.cmml"><mrow id="S5.E8.m1.2.2.4" xref="S5.E8.m1.2.2.4.cmml"><mi id="S5.E8.m1.2.2.4.2" xref="S5.E8.m1.2.2.4.2.cmml">A</mi><mo id="S5.E8.m1.2.2.4.1" xref="S5.E8.m1.2.2.4.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.4.3" xref="S5.E8.m1.2.2.4.3.cmml">c</mi><mo id="S5.E8.m1.2.2.4.1a" xref="S5.E8.m1.2.2.4.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.4.4" xref="S5.E8.m1.2.2.4.4.cmml">c</mi><mo id="S5.E8.m1.2.2.4.1b" xref="S5.E8.m1.2.2.4.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.4.5" xref="S5.E8.m1.2.2.4.5.cmml">u</mi><mo id="S5.E8.m1.2.2.4.1c" xref="S5.E8.m1.2.2.4.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.4.6" xref="S5.E8.m1.2.2.4.6.cmml">r</mi><mo id="S5.E8.m1.2.2.4.1d" xref="S5.E8.m1.2.2.4.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.4.7" xref="S5.E8.m1.2.2.4.7.cmml">a</mi><mo id="S5.E8.m1.2.2.4.1e" xref="S5.E8.m1.2.2.4.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.4.8" xref="S5.E8.m1.2.2.4.8.cmml">c</mi><mo id="S5.E8.m1.2.2.4.1f" xref="S5.E8.m1.2.2.4.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.4.9" xref="S5.E8.m1.2.2.4.9.cmml">y</mi></mrow><mo id="S5.E8.m1.2.2.3" xref="S5.E8.m1.2.2.3.cmml">=</mo><mrow id="S5.E8.m1.2.2.2" xref="S5.E8.m1.2.2.2.cmml"><mrow id="S5.E8.m1.2.2.2.2" xref="S5.E8.m1.2.2.2.2.cmml"><mrow id="S5.E8.m1.1.1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.1.1.1.cmml"><mo id="S5.E8.m1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E8.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E8.m1.1.1.1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.1.1.1.cmml"><mrow id="S5.E8.m1.1.1.1.1.1.1.1.2" xref="S5.E8.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E8.m1.1.1.1.1.1.1.1.2.2" xref="S5.E8.m1.1.1.1.1.1.1.1.2.2.cmml">T</mi><mo id="S5.E8.m1.1.1.1.1.1.1.1.2.1" xref="S5.E8.m1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S5.E8.m1.1.1.1.1.1.1.1.2.3" xref="S5.E8.m1.1.1.1.1.1.1.1.2.3.cmml">P</mi></mrow><mo id="S5.E8.m1.1.1.1.1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S5.E8.m1.1.1.1.1.1.1.1.3" xref="S5.E8.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E8.m1.1.1.1.1.1.1.1.3.2" xref="S5.E8.m1.1.1.1.1.1.1.1.3.2.cmml">T</mi><mo id="S5.E8.m1.1.1.1.1.1.1.1.3.1" xref="S5.E8.m1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S5.E8.m1.1.1.1.1.1.1.1.3.3" xref="S5.E8.m1.1.1.1.1.1.1.1.3.3.cmml">N</mi></mrow></mrow><mo id="S5.E8.m1.1.1.1.1.1.1.3" stretchy="false" xref="S5.E8.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S5.E8.m1.2.2.2.2.3" xref="S5.E8.m1.2.2.2.2.3.cmml">/</mo><mrow id="S5.E8.m1.2.2.2.2.2.1" xref="S5.E8.m1.2.2.2.2.2.1.1.cmml"><mo id="S5.E8.m1.2.2.2.2.2.1.2" stretchy="false" xref="S5.E8.m1.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S5.E8.m1.2.2.2.2.2.1.1" xref="S5.E8.m1.2.2.2.2.2.1.1.cmml"><mrow id="S5.E8.m1.2.2.2.2.2.1.1.2" xref="S5.E8.m1.2.2.2.2.2.1.1.2.cmml"><mi id="S5.E8.m1.2.2.2.2.2.1.1.2.2" xref="S5.E8.m1.2.2.2.2.2.1.1.2.2.cmml">T</mi><mo id="S5.E8.m1.2.2.2.2.2.1.1.2.1" xref="S5.E8.m1.2.2.2.2.2.1.1.2.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.2.2.2.1.1.2.3" xref="S5.E8.m1.2.2.2.2.2.1.1.2.3.cmml">P</mi></mrow><mo id="S5.E8.m1.2.2.2.2.2.1.1.1" xref="S5.E8.m1.2.2.2.2.2.1.1.1.cmml">+</mo><mrow id="S5.E8.m1.2.2.2.2.2.1.1.3" xref="S5.E8.m1.2.2.2.2.2.1.1.3.cmml"><mi id="S5.E8.m1.2.2.2.2.2.1.1.3.2" xref="S5.E8.m1.2.2.2.2.2.1.1.3.2.cmml">F</mi><mo id="S5.E8.m1.2.2.2.2.2.1.1.3.1" xref="S5.E8.m1.2.2.2.2.2.1.1.3.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.2.2.2.1.1.3.3" xref="S5.E8.m1.2.2.2.2.2.1.1.3.3.cmml">N</mi></mrow><mo id="S5.E8.m1.2.2.2.2.2.1.1.1a" xref="S5.E8.m1.2.2.2.2.2.1.1.1.cmml">+</mo><mrow id="S5.E8.m1.2.2.2.2.2.1.1.4" xref="S5.E8.m1.2.2.2.2.2.1.1.4.cmml"><mi id="S5.E8.m1.2.2.2.2.2.1.1.4.2" xref="S5.E8.m1.2.2.2.2.2.1.1.4.2.cmml">F</mi><mo id="S5.E8.m1.2.2.2.2.2.1.1.4.1" xref="S5.E8.m1.2.2.2.2.2.1.1.4.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.2.2.2.1.1.4.3" xref="S5.E8.m1.2.2.2.2.2.1.1.4.3.cmml">P</mi></mrow><mo id="S5.E8.m1.2.2.2.2.2.1.1.1b" xref="S5.E8.m1.2.2.2.2.2.1.1.1.cmml">+</mo><mrow id="S5.E8.m1.2.2.2.2.2.1.1.5" xref="S5.E8.m1.2.2.2.2.2.1.1.5.cmml"><mi id="S5.E8.m1.2.2.2.2.2.1.1.5.2" xref="S5.E8.m1.2.2.2.2.2.1.1.5.2.cmml">T</mi><mo id="S5.E8.m1.2.2.2.2.2.1.1.5.1" xref="S5.E8.m1.2.2.2.2.2.1.1.5.1.cmml">⁢</mo><mi id="S5.E8.m1.2.2.2.2.2.1.1.5.3" xref="S5.E8.m1.2.2.2.2.2.1.1.5.3.cmml">N</mi></mrow></mrow><mo id="S5.E8.m1.2.2.2.2.2.1.3" rspace="0.055em" stretchy="false" xref="S5.E8.m1.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E8.m1.2.2.2.3" rspace="0.222em" xref="S5.E8.m1.2.2.2.3.cmml">×</mo><mn id="S5.E8.m1.2.2.2.4" xref="S5.E8.m1.2.2.2.4.cmml">100</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E8.m1.2b"><apply id="S5.E8.m1.2.2.cmml" xref="S5.E8.m1.2.2"><eq id="S5.E8.m1.2.2.3.cmml" xref="S5.E8.m1.2.2.3"></eq><apply id="S5.E8.m1.2.2.4.cmml" xref="S5.E8.m1.2.2.4"><times id="S5.E8.m1.2.2.4.1.cmml" xref="S5.E8.m1.2.2.4.1"></times><ci id="S5.E8.m1.2.2.4.2.cmml" xref="S5.E8.m1.2.2.4.2">𝐴</ci><ci id="S5.E8.m1.2.2.4.3.cmml" xref="S5.E8.m1.2.2.4.3">𝑐</ci><ci id="S5.E8.m1.2.2.4.4.cmml" xref="S5.E8.m1.2.2.4.4">𝑐</ci><ci id="S5.E8.m1.2.2.4.5.cmml" xref="S5.E8.m1.2.2.4.5">𝑢</ci><ci id="S5.E8.m1.2.2.4.6.cmml" xref="S5.E8.m1.2.2.4.6">𝑟</ci><ci id="S5.E8.m1.2.2.4.7.cmml" xref="S5.E8.m1.2.2.4.7">𝑎</ci><ci id="S5.E8.m1.2.2.4.8.cmml" xref="S5.E8.m1.2.2.4.8">𝑐</ci><ci id="S5.E8.m1.2.2.4.9.cmml" xref="S5.E8.m1.2.2.4.9">𝑦</ci></apply><apply id="S5.E8.m1.2.2.2.cmml" xref="S5.E8.m1.2.2.2"><times id="S5.E8.m1.2.2.2.3.cmml" xref="S5.E8.m1.2.2.2.3"></times><apply id="S5.E8.m1.2.2.2.2.cmml" xref="S5.E8.m1.2.2.2.2"><divide id="S5.E8.m1.2.2.2.2.3.cmml" xref="S5.E8.m1.2.2.2.2.3"></divide><apply id="S5.E8.m1.1.1.1.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1"><plus id="S5.E8.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1"></plus><apply id="S5.E8.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.2"><times id="S5.E8.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.2.1"></times><ci id="S5.E8.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S5.E8.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.2.3">𝑃</ci></apply><apply id="S5.E8.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.3"><times id="S5.E8.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.3.1"></times><ci id="S5.E8.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.3.2">𝑇</ci><ci id="S5.E8.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.3.3">𝑁</ci></apply></apply><apply id="S5.E8.m1.2.2.2.2.2.1.1.cmml" xref="S5.E8.m1.2.2.2.2.2.1"><plus id="S5.E8.m1.2.2.2.2.2.1.1.1.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.1"></plus><apply id="S5.E8.m1.2.2.2.2.2.1.1.2.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.2"><times id="S5.E8.m1.2.2.2.2.2.1.1.2.1.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.2.1"></times><ci id="S5.E8.m1.2.2.2.2.2.1.1.2.2.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.2.2">𝑇</ci><ci id="S5.E8.m1.2.2.2.2.2.1.1.2.3.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.2.3">𝑃</ci></apply><apply id="S5.E8.m1.2.2.2.2.2.1.1.3.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.3"><times id="S5.E8.m1.2.2.2.2.2.1.1.3.1.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.3.1"></times><ci id="S5.E8.m1.2.2.2.2.2.1.1.3.2.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.3.2">𝐹</ci><ci id="S5.E8.m1.2.2.2.2.2.1.1.3.3.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.3.3">𝑁</ci></apply><apply id="S5.E8.m1.2.2.2.2.2.1.1.4.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.4"><times id="S5.E8.m1.2.2.2.2.2.1.1.4.1.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.4.1"></times><ci id="S5.E8.m1.2.2.2.2.2.1.1.4.2.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.4.2">𝐹</ci><ci id="S5.E8.m1.2.2.2.2.2.1.1.4.3.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.4.3">𝑃</ci></apply><apply id="S5.E8.m1.2.2.2.2.2.1.1.5.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.5"><times id="S5.E8.m1.2.2.2.2.2.1.1.5.1.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.5.1"></times><ci id="S5.E8.m1.2.2.2.2.2.1.1.5.2.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.5.2">𝑇</ci><ci id="S5.E8.m1.2.2.2.2.2.1.1.5.3.cmml" xref="S5.E8.m1.2.2.2.2.2.1.1.5.3">𝑁</ci></apply></apply></apply><cn id="S5.E8.m1.2.2.2.4.cmml" type="integer" xref="S5.E8.m1.2.2.2.4">100</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E8.m1.2c">Accuracy=(TP+TN)/(TP+FN+FP+TN)\times 100</annotation><annotation encoding="application/x-llamapun" id="S5.E8.m1.2d">italic_A italic_c italic_c italic_u italic_r italic_a italic_c italic_y = ( italic_T italic_P + italic_T italic_N ) / ( italic_T italic_P + italic_F italic_N + italic_F italic_P + italic_T italic_N ) × 100</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS2.p2.1">Precision is calculated according to Equation (<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.E9" title="In 5.2 Evaluation metrics ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">9</span></a>), where predicted positive sign classes are divided by the total positive count.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<table class="ltx_equation ltx_eqn_table" id="S5.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="Precision=TP/(TP+FP)\times 100" class="ltx_Math" display="block" id="S5.E9.m1.1"><semantics id="S5.E9.m1.1a"><mrow id="S5.E9.m1.1.1" xref="S5.E9.m1.1.1.cmml"><mrow id="S5.E9.m1.1.1.3" xref="S5.E9.m1.1.1.3.cmml"><mi id="S5.E9.m1.1.1.3.2" xref="S5.E9.m1.1.1.3.2.cmml">P</mi><mo id="S5.E9.m1.1.1.3.1" xref="S5.E9.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.3.3" xref="S5.E9.m1.1.1.3.3.cmml">r</mi><mo id="S5.E9.m1.1.1.3.1a" xref="S5.E9.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.3.4" xref="S5.E9.m1.1.1.3.4.cmml">e</mi><mo id="S5.E9.m1.1.1.3.1b" xref="S5.E9.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.3.5" xref="S5.E9.m1.1.1.3.5.cmml">c</mi><mo id="S5.E9.m1.1.1.3.1c" xref="S5.E9.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.3.6" xref="S5.E9.m1.1.1.3.6.cmml">i</mi><mo id="S5.E9.m1.1.1.3.1d" xref="S5.E9.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.3.7" xref="S5.E9.m1.1.1.3.7.cmml">s</mi><mo id="S5.E9.m1.1.1.3.1e" xref="S5.E9.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.3.8" xref="S5.E9.m1.1.1.3.8.cmml">i</mi><mo id="S5.E9.m1.1.1.3.1f" xref="S5.E9.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.3.9" xref="S5.E9.m1.1.1.3.9.cmml">o</mi><mo id="S5.E9.m1.1.1.3.1g" xref="S5.E9.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.3.10" xref="S5.E9.m1.1.1.3.10.cmml">n</mi></mrow><mo id="S5.E9.m1.1.1.2" xref="S5.E9.m1.1.1.2.cmml">=</mo><mrow id="S5.E9.m1.1.1.1" xref="S5.E9.m1.1.1.1.cmml"><mrow id="S5.E9.m1.1.1.1.1" xref="S5.E9.m1.1.1.1.1.cmml"><mrow id="S5.E9.m1.1.1.1.1.3" xref="S5.E9.m1.1.1.1.1.3.cmml"><mi id="S5.E9.m1.1.1.1.1.3.2" xref="S5.E9.m1.1.1.1.1.3.2.cmml">T</mi><mo id="S5.E9.m1.1.1.1.1.3.1" xref="S5.E9.m1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.1.1.3.3" xref="S5.E9.m1.1.1.1.1.3.3.cmml">P</mi></mrow><mo id="S5.E9.m1.1.1.1.1.2" xref="S5.E9.m1.1.1.1.1.2.cmml">/</mo><mrow id="S5.E9.m1.1.1.1.1.1.1" xref="S5.E9.m1.1.1.1.1.1.1.1.cmml"><mo id="S5.E9.m1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E9.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E9.m1.1.1.1.1.1.1.1" xref="S5.E9.m1.1.1.1.1.1.1.1.cmml"><mrow id="S5.E9.m1.1.1.1.1.1.1.1.2" xref="S5.E9.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E9.m1.1.1.1.1.1.1.1.2.2" xref="S5.E9.m1.1.1.1.1.1.1.1.2.2.cmml">T</mi><mo id="S5.E9.m1.1.1.1.1.1.1.1.2.1" xref="S5.E9.m1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.1.1.1.1.1.2.3" xref="S5.E9.m1.1.1.1.1.1.1.1.2.3.cmml">P</mi></mrow><mo id="S5.E9.m1.1.1.1.1.1.1.1.1" xref="S5.E9.m1.1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S5.E9.m1.1.1.1.1.1.1.1.3" xref="S5.E9.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E9.m1.1.1.1.1.1.1.1.3.2" xref="S5.E9.m1.1.1.1.1.1.1.1.3.2.cmml">F</mi><mo id="S5.E9.m1.1.1.1.1.1.1.1.3.1" xref="S5.E9.m1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S5.E9.m1.1.1.1.1.1.1.1.3.3" xref="S5.E9.m1.1.1.1.1.1.1.1.3.3.cmml">P</mi></mrow></mrow><mo id="S5.E9.m1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S5.E9.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E9.m1.1.1.1.2" rspace="0.222em" xref="S5.E9.m1.1.1.1.2.cmml">×</mo><mn id="S5.E9.m1.1.1.1.3" xref="S5.E9.m1.1.1.1.3.cmml">100</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E9.m1.1b"><apply id="S5.E9.m1.1.1.cmml" xref="S5.E9.m1.1.1"><eq id="S5.E9.m1.1.1.2.cmml" xref="S5.E9.m1.1.1.2"></eq><apply id="S5.E9.m1.1.1.3.cmml" xref="S5.E9.m1.1.1.3"><times id="S5.E9.m1.1.1.3.1.cmml" xref="S5.E9.m1.1.1.3.1"></times><ci id="S5.E9.m1.1.1.3.2.cmml" xref="S5.E9.m1.1.1.3.2">𝑃</ci><ci id="S5.E9.m1.1.1.3.3.cmml" xref="S5.E9.m1.1.1.3.3">𝑟</ci><ci id="S5.E9.m1.1.1.3.4.cmml" xref="S5.E9.m1.1.1.3.4">𝑒</ci><ci id="S5.E9.m1.1.1.3.5.cmml" xref="S5.E9.m1.1.1.3.5">𝑐</ci><ci id="S5.E9.m1.1.1.3.6.cmml" xref="S5.E9.m1.1.1.3.6">𝑖</ci><ci id="S5.E9.m1.1.1.3.7.cmml" xref="S5.E9.m1.1.1.3.7">𝑠</ci><ci id="S5.E9.m1.1.1.3.8.cmml" xref="S5.E9.m1.1.1.3.8">𝑖</ci><ci id="S5.E9.m1.1.1.3.9.cmml" xref="S5.E9.m1.1.1.3.9">𝑜</ci><ci id="S5.E9.m1.1.1.3.10.cmml" xref="S5.E9.m1.1.1.3.10">𝑛</ci></apply><apply id="S5.E9.m1.1.1.1.cmml" xref="S5.E9.m1.1.1.1"><times id="S5.E9.m1.1.1.1.2.cmml" xref="S5.E9.m1.1.1.1.2"></times><apply id="S5.E9.m1.1.1.1.1.cmml" xref="S5.E9.m1.1.1.1.1"><divide id="S5.E9.m1.1.1.1.1.2.cmml" xref="S5.E9.m1.1.1.1.1.2"></divide><apply id="S5.E9.m1.1.1.1.1.3.cmml" xref="S5.E9.m1.1.1.1.1.3"><times id="S5.E9.m1.1.1.1.1.3.1.cmml" xref="S5.E9.m1.1.1.1.1.3.1"></times><ci id="S5.E9.m1.1.1.1.1.3.2.cmml" xref="S5.E9.m1.1.1.1.1.3.2">𝑇</ci><ci id="S5.E9.m1.1.1.1.1.3.3.cmml" xref="S5.E9.m1.1.1.1.1.3.3">𝑃</ci></apply><apply id="S5.E9.m1.1.1.1.1.1.1.1.cmml" xref="S5.E9.m1.1.1.1.1.1.1"><plus id="S5.E9.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E9.m1.1.1.1.1.1.1.1.1"></plus><apply id="S5.E9.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.E9.m1.1.1.1.1.1.1.1.2"><times id="S5.E9.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E9.m1.1.1.1.1.1.1.1.2.1"></times><ci id="S5.E9.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E9.m1.1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S5.E9.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E9.m1.1.1.1.1.1.1.1.2.3">𝑃</ci></apply><apply id="S5.E9.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.E9.m1.1.1.1.1.1.1.1.3"><times id="S5.E9.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E9.m1.1.1.1.1.1.1.1.3.1"></times><ci id="S5.E9.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E9.m1.1.1.1.1.1.1.1.3.2">𝐹</ci><ci id="S5.E9.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E9.m1.1.1.1.1.1.1.1.3.3">𝑃</ci></apply></apply></apply><cn id="S5.E9.m1.1.1.1.3.cmml" type="integer" xref="S5.E9.m1.1.1.1.3">100</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E9.m1.1c">Precision=TP/(TP+FP)\times 100</annotation><annotation encoding="application/x-llamapun" id="S5.E9.m1.1d">italic_P italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n = italic_T italic_P / ( italic_T italic_P + italic_F italic_P ) × 100</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS2.p3.1">The recall is calculated according to Equation (<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.E10" title="In 5.2 Evaluation metrics ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">10</span></a>), where predicted positive sign classes are divided by the total count of true positive and false negative.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<table class="ltx_equation ltx_eqn_table" id="S5.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="Recall=T_{p}/(T_{p}+F_{n})\times 100" class="ltx_Math" display="block" id="S5.E10.m1.1"><semantics id="S5.E10.m1.1a"><mrow id="S5.E10.m1.1.1" xref="S5.E10.m1.1.1.cmml"><mrow id="S5.E10.m1.1.1.3" xref="S5.E10.m1.1.1.3.cmml"><mi id="S5.E10.m1.1.1.3.2" xref="S5.E10.m1.1.1.3.2.cmml">R</mi><mo id="S5.E10.m1.1.1.3.1" xref="S5.E10.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E10.m1.1.1.3.3" xref="S5.E10.m1.1.1.3.3.cmml">e</mi><mo id="S5.E10.m1.1.1.3.1a" xref="S5.E10.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E10.m1.1.1.3.4" xref="S5.E10.m1.1.1.3.4.cmml">c</mi><mo id="S5.E10.m1.1.1.3.1b" xref="S5.E10.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E10.m1.1.1.3.5" xref="S5.E10.m1.1.1.3.5.cmml">a</mi><mo id="S5.E10.m1.1.1.3.1c" xref="S5.E10.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E10.m1.1.1.3.6" xref="S5.E10.m1.1.1.3.6.cmml">l</mi><mo id="S5.E10.m1.1.1.3.1d" xref="S5.E10.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.E10.m1.1.1.3.7" xref="S5.E10.m1.1.1.3.7.cmml">l</mi></mrow><mo id="S5.E10.m1.1.1.2" xref="S5.E10.m1.1.1.2.cmml">=</mo><mrow id="S5.E10.m1.1.1.1" xref="S5.E10.m1.1.1.1.cmml"><mrow id="S5.E10.m1.1.1.1.1" xref="S5.E10.m1.1.1.1.1.cmml"><msub id="S5.E10.m1.1.1.1.1.3" xref="S5.E10.m1.1.1.1.1.3.cmml"><mi id="S5.E10.m1.1.1.1.1.3.2" xref="S5.E10.m1.1.1.1.1.3.2.cmml">T</mi><mi id="S5.E10.m1.1.1.1.1.3.3" xref="S5.E10.m1.1.1.1.1.3.3.cmml">p</mi></msub><mo id="S5.E10.m1.1.1.1.1.2" xref="S5.E10.m1.1.1.1.1.2.cmml">/</mo><mrow id="S5.E10.m1.1.1.1.1.1.1" xref="S5.E10.m1.1.1.1.1.1.1.1.cmml"><mo id="S5.E10.m1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E10.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E10.m1.1.1.1.1.1.1.1" xref="S5.E10.m1.1.1.1.1.1.1.1.cmml"><msub id="S5.E10.m1.1.1.1.1.1.1.1.2" xref="S5.E10.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E10.m1.1.1.1.1.1.1.1.2.2" xref="S5.E10.m1.1.1.1.1.1.1.1.2.2.cmml">T</mi><mi id="S5.E10.m1.1.1.1.1.1.1.1.2.3" xref="S5.E10.m1.1.1.1.1.1.1.1.2.3.cmml">p</mi></msub><mo id="S5.E10.m1.1.1.1.1.1.1.1.1" xref="S5.E10.m1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S5.E10.m1.1.1.1.1.1.1.1.3" xref="S5.E10.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E10.m1.1.1.1.1.1.1.1.3.2" xref="S5.E10.m1.1.1.1.1.1.1.1.3.2.cmml">F</mi><mi id="S5.E10.m1.1.1.1.1.1.1.1.3.3" xref="S5.E10.m1.1.1.1.1.1.1.1.3.3.cmml">n</mi></msub></mrow><mo id="S5.E10.m1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S5.E10.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E10.m1.1.1.1.2" rspace="0.222em" xref="S5.E10.m1.1.1.1.2.cmml">×</mo><mn id="S5.E10.m1.1.1.1.3" xref="S5.E10.m1.1.1.1.3.cmml">100</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E10.m1.1b"><apply id="S5.E10.m1.1.1.cmml" xref="S5.E10.m1.1.1"><eq id="S5.E10.m1.1.1.2.cmml" xref="S5.E10.m1.1.1.2"></eq><apply id="S5.E10.m1.1.1.3.cmml" xref="S5.E10.m1.1.1.3"><times id="S5.E10.m1.1.1.3.1.cmml" xref="S5.E10.m1.1.1.3.1"></times><ci id="S5.E10.m1.1.1.3.2.cmml" xref="S5.E10.m1.1.1.3.2">𝑅</ci><ci id="S5.E10.m1.1.1.3.3.cmml" xref="S5.E10.m1.1.1.3.3">𝑒</ci><ci id="S5.E10.m1.1.1.3.4.cmml" xref="S5.E10.m1.1.1.3.4">𝑐</ci><ci id="S5.E10.m1.1.1.3.5.cmml" xref="S5.E10.m1.1.1.3.5">𝑎</ci><ci id="S5.E10.m1.1.1.3.6.cmml" xref="S5.E10.m1.1.1.3.6">𝑙</ci><ci id="S5.E10.m1.1.1.3.7.cmml" xref="S5.E10.m1.1.1.3.7">𝑙</ci></apply><apply id="S5.E10.m1.1.1.1.cmml" xref="S5.E10.m1.1.1.1"><times id="S5.E10.m1.1.1.1.2.cmml" xref="S5.E10.m1.1.1.1.2"></times><apply id="S5.E10.m1.1.1.1.1.cmml" xref="S5.E10.m1.1.1.1.1"><divide id="S5.E10.m1.1.1.1.1.2.cmml" xref="S5.E10.m1.1.1.1.1.2"></divide><apply id="S5.E10.m1.1.1.1.1.3.cmml" xref="S5.E10.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E10.m1.1.1.1.1.3.1.cmml" xref="S5.E10.m1.1.1.1.1.3">subscript</csymbol><ci id="S5.E10.m1.1.1.1.1.3.2.cmml" xref="S5.E10.m1.1.1.1.1.3.2">𝑇</ci><ci id="S5.E10.m1.1.1.1.1.3.3.cmml" xref="S5.E10.m1.1.1.1.1.3.3">𝑝</ci></apply><apply id="S5.E10.m1.1.1.1.1.1.1.1.cmml" xref="S5.E10.m1.1.1.1.1.1.1"><plus id="S5.E10.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E10.m1.1.1.1.1.1.1.1.1"></plus><apply id="S5.E10.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.E10.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E10.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E10.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.E10.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E10.m1.1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S5.E10.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E10.m1.1.1.1.1.1.1.1.2.3">𝑝</ci></apply><apply id="S5.E10.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.E10.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E10.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E10.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E10.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E10.m1.1.1.1.1.1.1.1.3.2">𝐹</ci><ci id="S5.E10.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E10.m1.1.1.1.1.1.1.1.3.3">𝑛</ci></apply></apply></apply><cn id="S5.E10.m1.1.1.1.3.cmml" type="integer" xref="S5.E10.m1.1.1.1.3">100</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E10.m1.1c">Recall=T_{p}/(T_{p}+F_{n})\times 100</annotation><annotation encoding="application/x-llamapun" id="S5.E10.m1.1d">italic_R italic_e italic_c italic_a italic_l italic_l = italic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT / ( italic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT + italic_F start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) × 100</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">F1-score is calculated according to Equation (<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.E11" title="In 5.2 Evaluation metrics ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">11</span></a>), where predicted twice of precision and recall multiplication divided by the summation of the precision and recall.</p>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<table class="ltx_equation ltx_eqn_table" id="S5.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="F1-score=(2\times Precision\times Recall)/(Precsion+Recall)" class="ltx_Math" display="block" id="S5.E11.m1.2"><semantics id="S5.E11.m1.2a"><mrow id="S5.E11.m1.2.2" xref="S5.E11.m1.2.2.cmml"><mrow id="S5.E11.m1.2.2.4" xref="S5.E11.m1.2.2.4.cmml"><mrow id="S5.E11.m1.2.2.4.2" xref="S5.E11.m1.2.2.4.2.cmml"><mi id="S5.E11.m1.2.2.4.2.2" xref="S5.E11.m1.2.2.4.2.2.cmml">F</mi><mo id="S5.E11.m1.2.2.4.2.1" xref="S5.E11.m1.2.2.4.2.1.cmml">⁢</mo><mn id="S5.E11.m1.2.2.4.2.3" xref="S5.E11.m1.2.2.4.2.3.cmml">1</mn></mrow><mo id="S5.E11.m1.2.2.4.1" xref="S5.E11.m1.2.2.4.1.cmml">−</mo><mrow id="S5.E11.m1.2.2.4.3" xref="S5.E11.m1.2.2.4.3.cmml"><mi id="S5.E11.m1.2.2.4.3.2" xref="S5.E11.m1.2.2.4.3.2.cmml">s</mi><mo id="S5.E11.m1.2.2.4.3.1" xref="S5.E11.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.4.3.3" xref="S5.E11.m1.2.2.4.3.3.cmml">c</mi><mo id="S5.E11.m1.2.2.4.3.1a" xref="S5.E11.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.4.3.4" xref="S5.E11.m1.2.2.4.3.4.cmml">o</mi><mo id="S5.E11.m1.2.2.4.3.1b" xref="S5.E11.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.4.3.5" xref="S5.E11.m1.2.2.4.3.5.cmml">r</mi><mo id="S5.E11.m1.2.2.4.3.1c" xref="S5.E11.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.4.3.6" xref="S5.E11.m1.2.2.4.3.6.cmml">e</mi></mrow></mrow><mo id="S5.E11.m1.2.2.3" xref="S5.E11.m1.2.2.3.cmml">=</mo><mrow id="S5.E11.m1.2.2.2" xref="S5.E11.m1.2.2.2.cmml"><mrow id="S5.E11.m1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.cmml"><mo id="S5.E11.m1.1.1.1.1.1.2" stretchy="false" xref="S5.E11.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E11.m1.1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.cmml"><mrow id="S5.E11.m1.1.1.1.1.1.1.2" xref="S5.E11.m1.1.1.1.1.1.1.2.cmml"><mrow id="S5.E11.m1.1.1.1.1.1.1.2.2" xref="S5.E11.m1.1.1.1.1.1.1.2.2.cmml"><mrow id="S5.E11.m1.1.1.1.1.1.1.2.2.2" xref="S5.E11.m1.1.1.1.1.1.1.2.2.2.cmml"><mn id="S5.E11.m1.1.1.1.1.1.1.2.2.2.2" xref="S5.E11.m1.1.1.1.1.1.1.2.2.2.2.cmml">2</mn><mo id="S5.E11.m1.1.1.1.1.1.1.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="S5.E11.m1.1.1.1.1.1.1.2.2.2.1.cmml">×</mo><mi id="S5.E11.m1.1.1.1.1.1.1.2.2.2.3" xref="S5.E11.m1.1.1.1.1.1.1.2.2.2.3.cmml">P</mi></mrow><mo id="S5.E11.m1.1.1.1.1.1.1.2.2.1" xref="S5.E11.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.2.2.3" xref="S5.E11.m1.1.1.1.1.1.1.2.2.3.cmml">r</mi><mo id="S5.E11.m1.1.1.1.1.1.1.2.2.1a" xref="S5.E11.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.2.2.4" xref="S5.E11.m1.1.1.1.1.1.1.2.2.4.cmml">e</mi><mo id="S5.E11.m1.1.1.1.1.1.1.2.2.1b" xref="S5.E11.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.2.2.5" xref="S5.E11.m1.1.1.1.1.1.1.2.2.5.cmml">c</mi><mo id="S5.E11.m1.1.1.1.1.1.1.2.2.1c" xref="S5.E11.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.2.2.6" xref="S5.E11.m1.1.1.1.1.1.1.2.2.6.cmml">i</mi><mo id="S5.E11.m1.1.1.1.1.1.1.2.2.1d" xref="S5.E11.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.2.2.7" xref="S5.E11.m1.1.1.1.1.1.1.2.2.7.cmml">s</mi><mo id="S5.E11.m1.1.1.1.1.1.1.2.2.1e" xref="S5.E11.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.2.2.8" xref="S5.E11.m1.1.1.1.1.1.1.2.2.8.cmml">i</mi><mo id="S5.E11.m1.1.1.1.1.1.1.2.2.1f" xref="S5.E11.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.2.2.9" xref="S5.E11.m1.1.1.1.1.1.1.2.2.9.cmml">o</mi><mo id="S5.E11.m1.1.1.1.1.1.1.2.2.1g" xref="S5.E11.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.2.2.10" xref="S5.E11.m1.1.1.1.1.1.1.2.2.10.cmml">n</mi></mrow><mo id="S5.E11.m1.1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S5.E11.m1.1.1.1.1.1.1.2.1.cmml">×</mo><mi id="S5.E11.m1.1.1.1.1.1.1.2.3" xref="S5.E11.m1.1.1.1.1.1.1.2.3.cmml">R</mi></mrow><mo id="S5.E11.m1.1.1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.3" xref="S5.E11.m1.1.1.1.1.1.1.3.cmml">e</mi><mo id="S5.E11.m1.1.1.1.1.1.1.1a" xref="S5.E11.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.4" xref="S5.E11.m1.1.1.1.1.1.1.4.cmml">c</mi><mo id="S5.E11.m1.1.1.1.1.1.1.1b" xref="S5.E11.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.5" xref="S5.E11.m1.1.1.1.1.1.1.5.cmml">a</mi><mo id="S5.E11.m1.1.1.1.1.1.1.1c" xref="S5.E11.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.6" xref="S5.E11.m1.1.1.1.1.1.1.6.cmml">l</mi><mo id="S5.E11.m1.1.1.1.1.1.1.1d" xref="S5.E11.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.E11.m1.1.1.1.1.1.1.7" xref="S5.E11.m1.1.1.1.1.1.1.7.cmml">l</mi></mrow><mo id="S5.E11.m1.1.1.1.1.1.3" stretchy="false" xref="S5.E11.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S5.E11.m1.2.2.2.3" xref="S5.E11.m1.2.2.2.3.cmml">/</mo><mrow id="S5.E11.m1.2.2.2.2.1" xref="S5.E11.m1.2.2.2.2.1.1.cmml"><mo id="S5.E11.m1.2.2.2.2.1.2" stretchy="false" xref="S5.E11.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S5.E11.m1.2.2.2.2.1.1" xref="S5.E11.m1.2.2.2.2.1.1.cmml"><mrow id="S5.E11.m1.2.2.2.2.1.1.2" xref="S5.E11.m1.2.2.2.2.1.1.2.cmml"><mi id="S5.E11.m1.2.2.2.2.1.1.2.2" xref="S5.E11.m1.2.2.2.2.1.1.2.2.cmml">P</mi><mo id="S5.E11.m1.2.2.2.2.1.1.2.1" xref="S5.E11.m1.2.2.2.2.1.1.2.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.2.3" xref="S5.E11.m1.2.2.2.2.1.1.2.3.cmml">r</mi><mo id="S5.E11.m1.2.2.2.2.1.1.2.1a" xref="S5.E11.m1.2.2.2.2.1.1.2.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.2.4" xref="S5.E11.m1.2.2.2.2.1.1.2.4.cmml">e</mi><mo id="S5.E11.m1.2.2.2.2.1.1.2.1b" xref="S5.E11.m1.2.2.2.2.1.1.2.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.2.5" xref="S5.E11.m1.2.2.2.2.1.1.2.5.cmml">c</mi><mo id="S5.E11.m1.2.2.2.2.1.1.2.1c" xref="S5.E11.m1.2.2.2.2.1.1.2.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.2.6" xref="S5.E11.m1.2.2.2.2.1.1.2.6.cmml">s</mi><mo id="S5.E11.m1.2.2.2.2.1.1.2.1d" xref="S5.E11.m1.2.2.2.2.1.1.2.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.2.7" xref="S5.E11.m1.2.2.2.2.1.1.2.7.cmml">i</mi><mo id="S5.E11.m1.2.2.2.2.1.1.2.1e" xref="S5.E11.m1.2.2.2.2.1.1.2.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.2.8" xref="S5.E11.m1.2.2.2.2.1.1.2.8.cmml">o</mi><mo id="S5.E11.m1.2.2.2.2.1.1.2.1f" xref="S5.E11.m1.2.2.2.2.1.1.2.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.2.9" xref="S5.E11.m1.2.2.2.2.1.1.2.9.cmml">n</mi></mrow><mo id="S5.E11.m1.2.2.2.2.1.1.1" xref="S5.E11.m1.2.2.2.2.1.1.1.cmml">+</mo><mrow id="S5.E11.m1.2.2.2.2.1.1.3" xref="S5.E11.m1.2.2.2.2.1.1.3.cmml"><mi id="S5.E11.m1.2.2.2.2.1.1.3.2" xref="S5.E11.m1.2.2.2.2.1.1.3.2.cmml">R</mi><mo id="S5.E11.m1.2.2.2.2.1.1.3.1" xref="S5.E11.m1.2.2.2.2.1.1.3.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.3.3" xref="S5.E11.m1.2.2.2.2.1.1.3.3.cmml">e</mi><mo id="S5.E11.m1.2.2.2.2.1.1.3.1a" xref="S5.E11.m1.2.2.2.2.1.1.3.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.3.4" xref="S5.E11.m1.2.2.2.2.1.1.3.4.cmml">c</mi><mo id="S5.E11.m1.2.2.2.2.1.1.3.1b" xref="S5.E11.m1.2.2.2.2.1.1.3.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.3.5" xref="S5.E11.m1.2.2.2.2.1.1.3.5.cmml">a</mi><mo id="S5.E11.m1.2.2.2.2.1.1.3.1c" xref="S5.E11.m1.2.2.2.2.1.1.3.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.3.6" xref="S5.E11.m1.2.2.2.2.1.1.3.6.cmml">l</mi><mo id="S5.E11.m1.2.2.2.2.1.1.3.1d" xref="S5.E11.m1.2.2.2.2.1.1.3.1.cmml">⁢</mo><mi id="S5.E11.m1.2.2.2.2.1.1.3.7" xref="S5.E11.m1.2.2.2.2.1.1.3.7.cmml">l</mi></mrow></mrow><mo id="S5.E11.m1.2.2.2.2.1.3" stretchy="false" xref="S5.E11.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E11.m1.2b"><apply id="S5.E11.m1.2.2.cmml" xref="S5.E11.m1.2.2"><eq id="S5.E11.m1.2.2.3.cmml" xref="S5.E11.m1.2.2.3"></eq><apply id="S5.E11.m1.2.2.4.cmml" xref="S5.E11.m1.2.2.4"><minus id="S5.E11.m1.2.2.4.1.cmml" xref="S5.E11.m1.2.2.4.1"></minus><apply id="S5.E11.m1.2.2.4.2.cmml" xref="S5.E11.m1.2.2.4.2"><times id="S5.E11.m1.2.2.4.2.1.cmml" xref="S5.E11.m1.2.2.4.2.1"></times><ci id="S5.E11.m1.2.2.4.2.2.cmml" xref="S5.E11.m1.2.2.4.2.2">𝐹</ci><cn id="S5.E11.m1.2.2.4.2.3.cmml" type="integer" xref="S5.E11.m1.2.2.4.2.3">1</cn></apply><apply id="S5.E11.m1.2.2.4.3.cmml" xref="S5.E11.m1.2.2.4.3"><times id="S5.E11.m1.2.2.4.3.1.cmml" xref="S5.E11.m1.2.2.4.3.1"></times><ci id="S5.E11.m1.2.2.4.3.2.cmml" xref="S5.E11.m1.2.2.4.3.2">𝑠</ci><ci id="S5.E11.m1.2.2.4.3.3.cmml" xref="S5.E11.m1.2.2.4.3.3">𝑐</ci><ci id="S5.E11.m1.2.2.4.3.4.cmml" xref="S5.E11.m1.2.2.4.3.4">𝑜</ci><ci id="S5.E11.m1.2.2.4.3.5.cmml" xref="S5.E11.m1.2.2.4.3.5">𝑟</ci><ci id="S5.E11.m1.2.2.4.3.6.cmml" xref="S5.E11.m1.2.2.4.3.6">𝑒</ci></apply></apply><apply id="S5.E11.m1.2.2.2.cmml" xref="S5.E11.m1.2.2.2"><divide id="S5.E11.m1.2.2.2.3.cmml" xref="S5.E11.m1.2.2.2.3"></divide><apply id="S5.E11.m1.1.1.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1.1"><times id="S5.E11.m1.1.1.1.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1"></times><apply id="S5.E11.m1.1.1.1.1.1.1.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2"><times id="S5.E11.m1.1.1.1.1.1.1.2.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.1"></times><apply id="S5.E11.m1.1.1.1.1.1.1.2.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2"><times id="S5.E11.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.1"></times><apply id="S5.E11.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.2"><times id="S5.E11.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.2.1"></times><cn id="S5.E11.m1.1.1.1.1.1.1.2.2.2.2.cmml" type="integer" xref="S5.E11.m1.1.1.1.1.1.1.2.2.2.2">2</cn><ci id="S5.E11.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.2.3">𝑃</ci></apply><ci id="S5.E11.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.3">𝑟</ci><ci id="S5.E11.m1.1.1.1.1.1.1.2.2.4.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.4">𝑒</ci><ci id="S5.E11.m1.1.1.1.1.1.1.2.2.5.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.5">𝑐</ci><ci id="S5.E11.m1.1.1.1.1.1.1.2.2.6.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.6">𝑖</ci><ci id="S5.E11.m1.1.1.1.1.1.1.2.2.7.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.7">𝑠</ci><ci id="S5.E11.m1.1.1.1.1.1.1.2.2.8.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.8">𝑖</ci><ci id="S5.E11.m1.1.1.1.1.1.1.2.2.9.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.9">𝑜</ci><ci id="S5.E11.m1.1.1.1.1.1.1.2.2.10.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.2.10">𝑛</ci></apply><ci id="S5.E11.m1.1.1.1.1.1.1.2.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.2.3">𝑅</ci></apply><ci id="S5.E11.m1.1.1.1.1.1.1.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.3">𝑒</ci><ci id="S5.E11.m1.1.1.1.1.1.1.4.cmml" xref="S5.E11.m1.1.1.1.1.1.1.4">𝑐</ci><ci id="S5.E11.m1.1.1.1.1.1.1.5.cmml" xref="S5.E11.m1.1.1.1.1.1.1.5">𝑎</ci><ci id="S5.E11.m1.1.1.1.1.1.1.6.cmml" xref="S5.E11.m1.1.1.1.1.1.1.6">𝑙</ci><ci id="S5.E11.m1.1.1.1.1.1.1.7.cmml" xref="S5.E11.m1.1.1.1.1.1.1.7">𝑙</ci></apply><apply id="S5.E11.m1.2.2.2.2.1.1.cmml" xref="S5.E11.m1.2.2.2.2.1"><plus id="S5.E11.m1.2.2.2.2.1.1.1.cmml" xref="S5.E11.m1.2.2.2.2.1.1.1"></plus><apply id="S5.E11.m1.2.2.2.2.1.1.2.cmml" xref="S5.E11.m1.2.2.2.2.1.1.2"><times id="S5.E11.m1.2.2.2.2.1.1.2.1.cmml" xref="S5.E11.m1.2.2.2.2.1.1.2.1"></times><ci id="S5.E11.m1.2.2.2.2.1.1.2.2.cmml" xref="S5.E11.m1.2.2.2.2.1.1.2.2">𝑃</ci><ci id="S5.E11.m1.2.2.2.2.1.1.2.3.cmml" xref="S5.E11.m1.2.2.2.2.1.1.2.3">𝑟</ci><ci id="S5.E11.m1.2.2.2.2.1.1.2.4.cmml" xref="S5.E11.m1.2.2.2.2.1.1.2.4">𝑒</ci><ci id="S5.E11.m1.2.2.2.2.1.1.2.5.cmml" xref="S5.E11.m1.2.2.2.2.1.1.2.5">𝑐</ci><ci id="S5.E11.m1.2.2.2.2.1.1.2.6.cmml" xref="S5.E11.m1.2.2.2.2.1.1.2.6">𝑠</ci><ci id="S5.E11.m1.2.2.2.2.1.1.2.7.cmml" xref="S5.E11.m1.2.2.2.2.1.1.2.7">𝑖</ci><ci id="S5.E11.m1.2.2.2.2.1.1.2.8.cmml" xref="S5.E11.m1.2.2.2.2.1.1.2.8">𝑜</ci><ci id="S5.E11.m1.2.2.2.2.1.1.2.9.cmml" xref="S5.E11.m1.2.2.2.2.1.1.2.9">𝑛</ci></apply><apply id="S5.E11.m1.2.2.2.2.1.1.3.cmml" xref="S5.E11.m1.2.2.2.2.1.1.3"><times id="S5.E11.m1.2.2.2.2.1.1.3.1.cmml" xref="S5.E11.m1.2.2.2.2.1.1.3.1"></times><ci id="S5.E11.m1.2.2.2.2.1.1.3.2.cmml" xref="S5.E11.m1.2.2.2.2.1.1.3.2">𝑅</ci><ci id="S5.E11.m1.2.2.2.2.1.1.3.3.cmml" xref="S5.E11.m1.2.2.2.2.1.1.3.3">𝑒</ci><ci id="S5.E11.m1.2.2.2.2.1.1.3.4.cmml" xref="S5.E11.m1.2.2.2.2.1.1.3.4">𝑐</ci><ci id="S5.E11.m1.2.2.2.2.1.1.3.5.cmml" xref="S5.E11.m1.2.2.2.2.1.1.3.5">𝑎</ci><ci id="S5.E11.m1.2.2.2.2.1.1.3.6.cmml" xref="S5.E11.m1.2.2.2.2.1.1.3.6">𝑙</ci><ci id="S5.E11.m1.2.2.2.2.1.1.3.7.cmml" xref="S5.E11.m1.2.2.2.2.1.1.3.7">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E11.m1.2c">F1-score=(2\times Precision\times Recall)/(Precsion+Recall)</annotation><annotation encoding="application/x-llamapun" id="S5.E11.m1.2d">italic_F 1 - italic_s italic_c italic_o italic_r italic_e = ( 2 × italic_P italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n × italic_R italic_e italic_c italic_a italic_l italic_l ) / ( italic_P italic_r italic_e italic_c italic_s italic_i italic_o italic_n + italic_R italic_e italic_c italic_a italic_l italic_l )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Result Analysis</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We evaluated the model with inter-, intra, and merge dataset evaluation settings to generate the generalisation property. We arranged seven evaluation configurations here to measure the performance of the proposed model, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T3" title="Table 3 ‣ 5.3 Result Analysis ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">3</span></a>. In Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T3" title="Table 3 ‣ 5.3 Result Analysis ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">3</span></a>, the intra-dataset evaluation setting is defined in configurations A, B, and C, which means the training and testing dataset comes from the same dataset. Inter-dataset evaluation settings are denoted with the configurations D and E, which refer to the training and test datasets from different datasets. For instance, in configuration D, the model is trained and validated on the Proposed dataset and tested with the BdSL-38 dataset; in configuration, the E model is trained and validated with the Proposed dataset before being tested on the BdSL 38 dataset. Merge dataset validation settings are defined with the configuration F and G. This refers to the fact that we train the model by merging the BdSL-38 and BAUST-BSL-38 datasets and testing with BdSL-38 and BAUST-BSL-38 datasets, respectively.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance accuracy with different Dataset configuration</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.1.1.1.1">Configuration</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.1.1.1.2">Train Dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.3">Test Dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.4">Performance [%]</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.5">Category</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.2.1.1">Configuration A</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.2.1.2">KU BdSL</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.3">KU BdSL</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.4">99.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.5">Intra dataset</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.3.2.1">Configuration B</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.3.2.2">BAUST-BSL-38</th>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.3">BAUST-BSL-38</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.4">98.00</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.5">Intra dataset</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.4.3.1">Configuration C</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.4.3.2">BdSL38</th>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.3.3">BdSL38</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.3.4">96.00</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.3.5">Intra dataset</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.5.4.1">Configuration D</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.5.4.2">BAUST-BSL-38</th>
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.4.3">BdSL38</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.4.4">32.60</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.4.5">Inter dataset</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.6.5.1">Configuration E</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.6.5.2">BdSL38</th>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.3">BAUST-BSL-38</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.4">31.60</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.5">Inter dataset</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.7.6.1">Configuration F</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.7.6.2">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.1.7.6.2.1">
<tr class="ltx_tr" id="S5.T3.1.7.6.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.7.6.2.1.1.1">BdSL38+</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.6.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.7.6.2.1.2.1">BAUST-BSL-38</td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_left" id="S5.T3.1.7.6.3">BAUST-BSL-38</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.7.6.4">95.00</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.7.6.5">Merge dataset</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T3.1.8.7.1">Configuration G</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T3.1.8.7.2">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.1.8.7.2.1">
<tr class="ltx_tr" id="S5.T3.1.8.7.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.8.7.2.1.1.1">BdSL38+</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.7.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.8.7.2.1.2.1">BAUST-BSL-38</td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.8.7.3">BdSL38</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.8.7.4">92.00</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.8.7.5">Merge dataset</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S5.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S5.F9.2">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F9.1">-3.5cm-3cm
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="224" id="S5.F9.1.g1" src="x5.jpg" width="578"/>
</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Class wise Precision, Recall, F1-score and accuracy of the KU BdSL Dataset Configuration A</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S5.F10.2">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F10.1">-3cm-3cm
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="213" id="S5.F10.1.g1" src="x6.jpg" width="556"/></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Class wise Precision, Recall, F1-score and accuracy of the proposed dataset configuration B</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S5.F11.2">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F11.1">-3cm-3cm
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="S5.F11.1.g1" src="x7.jpg" width="537"/>
</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Class wise Precision, Recall, F1-score and accuracy of the BdSL 38 dataset Configuration C. </figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">We have performed a thorough assessment of the proposed models trained on different configurations of inter-datasets and intra-datasets to determine the generalizability of the proposed model. In our work, we have considered the training set from one dataset to evaluate the trained model. Furthermore, we have tested the trained model with different dataset test sets and highlighted the generalization problem. We demonstrate that the validation partition is useful in the intra-dataset as it is drawn from the same dataset as the training set. The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T3" title="Table 3 ‣ 5.3 Result Analysis ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">3</span></a> show that the accuracy of all configurations trained on inter-datasets, which had previously performed well on intra-datasets, has significantly decreased. We demonstrated label-wise precision, recall, f1-score and performance accuracy for the intra-dataset in Figures <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.F9" title="Figure 9 ‣ 5.3 Result Analysis ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">9</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.F10" title="Figure 10 ‣ 5.3 Result Analysis ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">10</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.F11" title="Figure 11 ‣ 5.3 Result Analysis ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">11</span></a> for the KU-BdSL, Proposed, and BdSL-38 datasets, respectively.</p>
</div>
<figure class="ltx_figure" id="S5.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S5.F12.2">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F12.1">-3cm-3cm
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="S5.F12.1.g1" src="x8.jpg" width="439"/></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Accuracy and Loss curve for the configuration A. </figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.F12" title="Figure 12 ‣ 5.3 Result Analysis ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">12</span></a> illustrates the relationship between training accuracy versus testing accuracy and training loss versus testing loss, focusing specifically on configuration A among various configurations. The figure portrays a sample curve that provides insights into the model’s fit within the context of our learning algorithm’s objectives. It effectively strikes a balance between overfitting and underfitting. In our experience, an ideal fit is characterized by both training and validation losses diminishing until they reach a stable point, with minimal disparity between their final values. Typically, the model’s loss on the training dataset is lower than that on the validation dataset, leading to an anticipated gap between their respective learning curves. This gap is commonly referred to as the ”generalization gap.” Our learning curves exemplify a favourable fit, where the plot of training loss steadily decreases to a point of stability, mirroring the behaviour of the validation loss plot, which also converges to a stable value with a small, acceptable gap when compared to the training loss.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>State of the Art Comparison of the proposed model</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T4" title="Table 4 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrated the comparative analysis of the proposed model with the state-of-the-art system for the BdSL-38 dataset, where we reported the number of parameters and computational time also. Our proposed model has an order of magnitude lower computational complexity (in terms of FLOPS) and also has fewer (1/2nd) parameters compared to existing architectures. This reduces hardware requirements in terms of processing power and memory, leading to lower costs. Reduced computational complexity also results in faster run time, as shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T4" title="Table 4 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T5" title="Table 5 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">5</span></a>. Our method takes less time, about three times than the best-competing method.
It can be observed from Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T4" title="Table 4 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a> that the proposed method has reported the best results in test accuracy when compared to the other models. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>]</cite>, the author employed modified VGG19 to recognize Bengali sign language alphabets. With the learning rate 1e-3 and SGD optimizer, they achieved 89.60% accuracy. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib32" title="">32</a>]</cite>, the author employed a concatenation CNN architecture where they concatenated the feature extracted from the Bengali sign language dataset and achieved 91.52% accuracy. To implement their concatenation network, they used 10 ReLU activations, a single input layer, four max-pooling layers, two fully connected layers and a softmax activation function. The author in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib12" title="">12</a>]</cite> applied a unique CNN-based architecture, namely BenSignNet, to recognize the Bengali sign language dataset, and they achieved 94.00% accuracy where, whereas our proposed model achieved 96.00% accuracy. Apart from performance, it is clear that DenseNet, with 92.29%, outperforms the other model, whereas ResNet performs worse than all for the BdSL-38 dataset. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.F13" title="Figure 13 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">13</span></a> visualizes the label-wise performance comparison with the state-of-the-art models for the BdSL-38 dataset.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison with state-the-state of the art model for BdSL38 Dataset</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.1">Dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.2">Approach</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.3">Parameter (M)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.4">Flops (BMac)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.5">Accuracy (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T4.1.2.1.1" rowspan="8"><span class="ltx_text" id="S5.T4.1.2.1.1.1">BdSL 38</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.2.1.2">VGG19<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib75" title="">75</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.2.1.3">N/A</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.2.1.4">N/A</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.2.1.5">89.60</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<td class="ltx_td ltx_align_left" id="S5.T4.1.3.2.1">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib76" title="">76</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.3.2.2">11.68</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.3.2.3">248015</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.3.2.4">89.68</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<td class="ltx_td ltx_align_left" id="S5.T4.1.4.3.1">InceptionNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib77" title="">77</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.4.3.2">47.82</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.4.3.3">N/A</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.4.3.4">88.00</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<td class="ltx_td ltx_align_left" id="S5.T4.1.5.4.1">CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib32" title="">32</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.5.4.2">N/A</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.5.4.3">N/A</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.5.4.4">91.52</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.5">
<td class="ltx_td ltx_align_left" id="S5.T4.1.6.5.1">WaveNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib78" title="">78</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.6.5.2">N/A</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.6.5.3">N/A</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.6.5.4">92.00</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.7.6">
<td class="ltx_td ltx_align_left" id="S5.T4.1.7.6.1">DenseNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib79" title="">79</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.7.6.2">0.676</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.7.6.3">N/A</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.7.6.4">92.29</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.8.7">
<td class="ltx_td ltx_align_left" id="S5.T4.1.8.7.1">BenSignNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib12" title="">12</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.8.7.2">1.42</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.8.7.3">33088</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.8.7.4">94.00</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T4.1.9.8.1">Proposed Method</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T4.1.9.8.2">0.26639</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T4.1.9.8.3">210</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T4.1.9.8.4">96.00</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S5.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S5.F13.2">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F13.1">-3cm-3cm
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="225" id="S5.F13.1.g1" src="x9.jpg" width="561"/></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Label-wise state-of-the-art comparison for BdSL-38 dataset.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S5.T5.1">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.T5.2">-2cm-3cm


<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.2.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S5.T5.2.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.2.1.1.1.1">Dataset</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.2.1.1.1.2">Approach</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.2.1.1.1.3">Parameter (M)</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.2.1.1.1.4">Flops (BMac)</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.2.1.1.1.5">Accuracy (%)</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T5.2.1.2.1">
<span class="ltx_td ltx_border_t ltx_rowspan ltx_rowspan_2" id="S5.T5.2.1.2.1.1"></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.1.2.1.2">BenSignNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib12" title="">12</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.1.2.1.3">1.42 M</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.1.2.1.4">1537.37</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.1.2.1.5">98.20</span></span>
<span class="ltx_tr" id="S5.T5.2.1.3.2">
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.3.2.1">InceptionNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib77" title="">77</a>]</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.3.2.2">47.82</span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.3.2.3">N/A</span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.3.2.4">94.94</span></span>
<span class="ltx_tr" id="S5.T5.2.1.4.3">
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.4.3.1">KU-BdSL</span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.4.3.2">WaveNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib78" title="">78</a>]</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.4.3.3">N/A</span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.4.3.4">N/A</span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.4.3.5">90.00</span></span>
<span class="ltx_tr" id="S5.T5.2.1.5.4">
<span class="ltx_td ltx_border_b ltx_rowspan ltx_rowspan_3" id="S5.T5.2.1.5.4.1"></span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.5.4.2">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib76" title="">76</a>]</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.5.4.3">11.68</span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.5.4.4">11525</span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.5.4.5">98.21</span></span>
<span class="ltx_tr" id="S5.T5.2.1.6.5">
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.6.5.1">DenseNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib79" title="">79</a>]</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.6.5.2">0.676</span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.6.5.3">N/A</span>
<span class="ltx_td ltx_align_left" id="S5.T5.2.1.6.5.4">98.21</span></span>
<span class="ltx_tr" id="S5.T5.2.1.7.6">
<span class="ltx_td ltx_align_left ltx_border_b" id="S5.T5.2.1.7.6.1">Proposed Model</span>
<span class="ltx_td ltx_align_left ltx_border_b" id="S5.T5.2.1.7.6.2">0.26639 M</span>
<span class="ltx_td ltx_align_left ltx_border_b" id="S5.T5.2.1.7.6.3">9.78</span>
<span class="ltx_td ltx_align_left ltx_border_b" id="S5.T5.2.1.7.6.4">99.00</span></span>
</span>
</span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>State-of-the-art comparison of the proposed method for KU-BdSL dataset</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S5.F14.2">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F14.1">-3cm-3cm
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="S5.F14.1.g1" src="x10.jpg" width="545"/></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Label-wise state-of-the-art comparison for KU-BdSL dataset.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Tables <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T5" title="Table 5 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrated the comparative analysis of the proposed model with the state-of-the-art system KU-BdSL dataset, where we reported the number of parameters and computational time also. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T5" title="Table 5 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">5</span></a> visualized that the proposed model achieved 99.00% accuracy, which is better than all other existing models.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S5.T6.1">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.T6.2">-1cm-3cm


<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.2.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S5.T6.2.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.1.1.1.1">Dataset</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.1.1.1.2">Approach</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.1.1.1.3">Parameter</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.1.1.1.4">Flops</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.1.1.1.5">Accuracy (%)</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T6.2.1.2.1">
<span class="ltx_td ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_6" id="S5.T6.2.1.2.1.1"><span class="ltx_text" id="S5.T6.2.1.2.1.1.1">BAUST-BSL-38 Lab Dataset</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T6.2.1.2.1.2">VGG16<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib75" title="">75</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T6.2.1.2.1.3">14.50</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T6.2.1.2.1.4">415.17</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T6.2.1.2.1.5">79.00</span></span>
<span class="ltx_tr" id="S5.T6.2.1.3.2">
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.3.2.1">InceptionNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib77" title="">77</a>]</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.3.2.2">47.82</span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.3.2.3">N/A</span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.3.2.4">80.00</span></span>
<span class="ltx_tr" id="S5.T6.2.1.4.3">
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.4.3.1">WaveNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib78" title="">78</a>]</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.4.3.2">N/A</span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.4.3.3">N/A</span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.4.3.4">78.00</span></span>
<span class="ltx_tr" id="S5.T6.2.1.5.4">
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.5.4.1">DenseNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib79" title="">79</a>]</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.5.4.2">0.676</span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.5.4.3">N/A</span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.5.4.4">97.07</span></span>
<span class="ltx_tr" id="S5.T6.2.1.6.5">
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.6.5.1">BenSignNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib12" title="">12</a>]</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.6.5.2">1.37</span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.6.5.3">32749</span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.6.5.4">78.00</span></span>
<span class="ltx_tr" id="S5.T6.2.1.7.6">
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.7.6.1">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib76" title="">76</a>]</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.7.6.2">11.68</span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.7.6.3">245474</span>
<span class="ltx_td ltx_align_left" id="S5.T6.2.1.7.6.4">96.77</span></span>
<span class="ltx_tr" id="S5.T6.2.1.8.7">
<span class="ltx_td ltx_border_b" id="S5.T6.2.1.8.7.1"></span>
<span class="ltx_td ltx_align_left ltx_border_b" id="S5.T6.2.1.8.7.2">Proposed</span>
<span class="ltx_td ltx_align_left ltx_border_b" id="S5.T6.2.1.8.7.3">0.26639 M</span>
<span class="ltx_td ltx_align_left ltx_border_b" id="S5.T6.2.1.8.7.4">208.10</span>
<span class="ltx_td ltx_align_left ltx_border_b" id="S5.T6.2.1.8.7.5">98.00</span></span>
</span>
</span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>State-of-the-art comparison for the newly created BAUST-BSL-38 lab dataset</figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">We also demonstrate here the performance of different transfer learning model performance. Based on the Experiment WaveNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib78" title="">78</a>]</cite>, InceptionNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib77" title="">77</a>]</cite>, ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib76" title="">76</a>]</cite> and DenseNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#bib.bib79" title="">79</a>]</cite> achieved 90.00%, 94.94%, 98.21%, and 98.21% accuracy, respectively. Meanwhile, DenseNet and ResNet achieved the nearest performance for the KUBdSL dataset. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.F14" title="Figure 14 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">14</span></a> visualizes the label-wise performance comparison with the state-of-the-art models for the KUBdSL.</p>
</div>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T6" title="Table 6 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">6</span></a> showcases the performance of different state-of-the-art models on the BAUST-BSL-38 lab dataset. VGG16, with 14.50 million parameters, achieves a 79% accuracy, consuming 415.17 million FLOPs. In contrast, InceptionNet, despite a high parameter count of 47.82 million, reaches 80% accuracy and WaveNet lags slightly with a 78% accuracy. DenseNet impresses with a 97.07% accuracy, owing to a relatively low parameter footprint. BenSignNet, with 1.37 million parameters and 32,749 FLOPs, also scores a 78% accuracy. ResNet generated 96.77% accuracy accuracy but with higher parameters and FLOPs. The proposed model stands out with a stellar 98% accuracy, the lowest parameter count at 0.26639 million, and moderate FLOPs at 208.10 million, showcasing a balance of efficiency and high performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Discussion</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">Our proposed Multi-Branch Spatial-Temporal Network model, depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S4.F4" title="Figure 4 ‣ 4 Proposed Methodology ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a>, has achieved high-performance accuracy while maintaining low computational complexity. This significant reduction in computational demands leads to an order of magnitude lower complexity in FLOPs and features fewer parameters—just half the count of existing architectures. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T4" title="Table 4 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a> details the proposed model, which requires only 0.266 million parameters per batch, with each batch comprising eight trials for each dataset. It also demands 210 billion multiply-accumulate operations (BMac) as computational complexity for 2,733 batches. In comparison, the number of parameters for ResNet, Inception, BenSignNet, and DenseNet are 11.68, 47.82, 1.42, and 0.602 million, respectively. Notably, our model’s parameter count is approximately one-fourth that of BenSignNet and half of DenseNet’s, also falling below those of ResNet and Inception models. DenseNet has the lowest parameter count among existing systems at 0.602 million—roughly double that of our proposed model at 0.266 million. In terms of computational complexity measured in FLOPs, our proposed model requires 210 BMac, while methods like ResNet and BenSignNet need 248,015 and 33,088, respectively. BenSignNet has the lowest complexity at 33,088, which is still about three times higher than our proposed model’s 210 BMac. According to Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T5" title="Table 5 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">5</span></a>, the number of parameters is consistent with Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T4" title="Table 4 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a>, but computational complexity varies based on the number of batches. The KU-BdSL dataset, with 127 batches, shows our model requiring only 9.78 BMac, while methods like ResNet and BenSignNet need 11,525 and 1,537, respectively. BenSignNet’s lowest complexity is still roughly three times higher than our proposed model’s 9.78 BMac. In Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T6" title="Table 6 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">6</span></a>, the parameter number remains consistent with Tables <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T4" title="Table 4 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T5" title="Table 5 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">5</span></a>, but again, computational complexity varies by batch count. For the BAUST-BSL-38 lab dataset’s 2,705 batches, our model requires just 208.10 BMac, compared to the significantly higher demands of ResNet and BenSignNet, which require 245,474 and 32,749, respectively. BenSignNet, with the lowest complexity of 32,749, is more than three times higher than that of our proposed method at 208.10 BMac.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">The proposed model marks a substantial leap in the spatial-temporal model, striking an impressive balance between efficiency and accuracy. With a parameter count of merely 0.26639 million, it vastly undercuts the parameters of models like VGG16 and InceptionNet by an order of magnitude. This streamlined architecture directly translates into reduced memory consumption and quicker training times, ideal for deployment in resource-constrained environments. Moreover, the computational cost, in terms of FLOPs, is about a third of that required by previous models such as VGG16 and ResNet. This decrease in computational demand, without a sacrifice in performance, underscores the innovative and efficient nature of our model. It not only positions the model as more environmentally friendly due to lower energy consumption but also paves the way for real-time applications on edge devices with limited computational resources. The model’s commendable 98% accuracy rate, despite a reduced architecture, establishes a new standard for future developments, demonstrating that high efficiency does not necessitate a trade-off in performance. This reduction in hardware requirements for processing power and memory also leads to cost savings. Lower computational complexity equates to faster runtimes, as evidenced in Tables <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T4" title="Table 4 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">4</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T5" title="Table 5 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">5</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2408.14111v1#S5.T6" title="Table 6 ‣ 5.4 State of the Art Comparison of the proposed model ‣ 5 Experimental Results ‣ Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model"><span class="ltx_text ltx_ref_tag">6</span></a> with our method taking approximately one-third of the time compared to the next best-competing method. Our model, which uses 2D skeletons extracted from images taken with standard cameras and does not rely on power-intensive and costlier active sensors like the Kinect, shows that hand gesture recognition can be effectively generalized across different conditions. To validate this generalization, we assessed performance across multiple datasets under varying conditions and in both inter- and intra-evaluation settings. While most existing studies report high accuracy on one or two datasets, our use of three datasets underscores the model’s robustness and generalizability, consistently handling a variety of gestures.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this study, we introduced a lightweight BSL recognition system utilizing a spatiotemporal attention approach. Our system effectively detects BSL from 2D skeleton data, which eliminates the need for video storage or costly sensors. We have developed a potent feature aggregation technique that discerns local and global spatiotemporal attributes, leading to high accuracy in our system. The exceptional accuracy of our Bangla Sign Language recognition model stems from a unique multi-branch spatial-temporal attention mechanism that integrates four distinct and effective feature types: (i) spatial attention from the first branch, (ii) temporal attention from the second, (iii) combined spatial-temporal attention from the third, and (iv) temporal CNN features from the fourth. This innovative fusion approach is pioneering, utilizing the full range of informative cues for superior gesture classification. By fusing these features, our model captures the nuances of BSL more effectively than existing methods. Its computational efficiency and ability to integrate diverse datasets highlight its adaptability and potential for real-time application. This approach marks a significant leap forward in sign language recognition technology. The proposed architecture benefits from low computational complexity and precise graph convolution attention. Our extensive experiments across three BSL datasets have demonstrated that our model surpasses existing systems in various settings and evaluations. The results section visualizes the model’s strong performance in intra-dataset and merged dataset configurations, although it reveals less promising results in inter-dataset settings. The study also indicates that while the current datasets are sufficiently large for training the model for recognition, there could be gaps in representing diverse scenarios. Our model possesses a generalizable quality, making it suitable for deployment in real-world applications to aid the disabled community. However, the model may encounter issues, such as underfitting with smaller datasets like KU-BdSL and Ishara-Lipi etc. In the future, we plan to compile a large-scale BSL dataset encompassing a variety of scenarios to train the model, ensuring its continued high performance across diverse situations.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Sample Appendix Section</h2>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Shin, M. A. M. Hasan, A. S. M. Miah, K. Suzuki, K. Hirooka, Japanese sign language recognition by combining joint skeleton-based handcrafted and pixel-based deep learning features with machine learning classification, CMES-COMPUTER MODELING IN ENGINEERING &amp; SCIENCES (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Kakizaki, A. S. M. Miah, K. Hirooka, J. Shin, Dynamic japanese sign language recognition throw hand pose estimation using effective feature extraction and classification approach, Sensors 24 (3) (2024) 826.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Shin, A. S. Musa Miah, M. A. M. Hasan, K. Hirooka, K. Suzuki, H.-S. Lee, S.-W. Jang, Korean sign language recognition using transformer-based deep neural network, Applied Sciences 13 (5) (2023) 3029.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Shin, A. S. M. Miah, K. Suzuki, K. Hirooka, M. A. M. Hasan, Dynamic korean sign language recognition using pose estimation based and attention-based neural network, IEEE Access 11 (2023) 143501–143513.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ACCESS.2023.3343404" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ACCESS.2023.3343404</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Shin, A. S. M. Miah, Y. Akiba, K. Hirooka, N. Hassan, Y. S. Hwang, Korean sign language alphabet recognition through the integration of handcrafted and deep learning-based two-stream feature extraction approach, IEEE Access (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R. Kushalnagar, Deafness and hearing loss, Web Accessibility: A Foundation for Research (2019) 35–47.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. J. Cheok, Z. Omar, M. H. Jaward, A review of hand gesture and sign language recognition techniques, International Journal of Machine Learning and Cybernetics 10 (2019) 131–153.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. A. Rahim, A. S. M. Miah, A. Sayeed, J. Shin, Hand gesture recognition based on optimal segmentation in human-computer interaction, in: 2020 3rd IEEE International Conference on Knowledge Innovation and Invention (ICKII), IEEE, 2020, pp. 163–166.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. N. Islam, R. Jahangir, N. S. Mohim, M. Wasif-Ul-Islam, A. Ashraf, N. I. Khan, M. R. Mahjabin, A. S. M. Miah, J. Shin, A multilingual handwriting learning system for visually impaired people, IEEE Access (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T. Vos, C. Allen, M. Arora, R. M. Barber, Z. A. Bhutta, A. Brown, A. Carter, D. C. Casey, F. J. Charlson, A. Z. Chen, et al., Global, regional, and national incidence, prevalence, and years lived with disability for 310 diseases and injuries, 1990–2015: a systematic analysis for the global burden of disease study 2015, The lancet 388 (10053) (2016) 1545–1602.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Alauddin, A. H. Joarder, Deafness in bangladesh, Hearing Impairment: An Invisible Disability How You Can Live With a Hearing Impairment (2004) 64–69.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. S. M. Miah, J. Shin, M. A. M. Hasan, M. A. Rahim, Bensignnet: Bengali sign language alphabet recognition using concatenated segmentation and convolutional neural network, Applied Sciences 12 (8) (2022) 3933.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. S. M. Miah, J. Shin, M. A. M. Hasan, Y. Okuyama, A. Nobuyoshi, Dynamic hand gesture recognition using effective feature extraction and attention based deep neural network., in: 2023 IEEE 16th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC), IEEE, 2023, pp. 241–247.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. S. M. Miah, J. Shin, M. A. M. Hasan, Y. Fujimoto, A. Nobuyoshi, Skeleton-based hand gesture recognition using geometric features and spatio-temporal deep learning approach, in: 2023 11th European Workshop on Visual Information Processing (EUVIP), IEEE, 2023, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Mindess, Reading between the signs: Intercultural communication for sign language interpreters, Nicholas Brealey, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
N. Hassan, A. S. M. Miah, J. Shin, A deep bidirectional lstm model enhanced by transfer-learning-based feature extraction for dynamic human activity recognition, Applied Sciences 14 (2) (2024) 603.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
B. Mallik, M. A. Rahim, A. S. M. Miah, K. S. Yun, J. Shin, Virtual keyboard: A real-time hand gesture recognition-based character input system using lstm and mediapipe holistic., Comput. Syst. Sci. Eng. 48 (2) (2024) 555–570.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M. A. Rahim, A. S. M. Miah, H. S. Akash, J. Shin, M. I. Hossain, M. N. Hossain, An advanced deep learning based three-stream hybrid model for dynamic hand gesture recognition, arXiv preprint arXiv:2408.08035 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T. Chouhan, A. Panse, A. K. Voona, S. Sameer, Smart glove with gesture recognition ability for the hearing and speech impaired, in: 2014 IEEE Global Humanitarian Technology Conference-South Asia Satellite (GHTC-SAS), IEEE, 2014, pp. 105–110.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
K. Assaleh, T. Shanableh, M. Zourob, Low complexity classification system for glove-based arabic sign language recognition, in: Neural Information Processing: 19th International Conference, ICONIP 2012, Doha, Qatar, November 12-15, 2012, Proceedings, Part III 19, Springer, 2012, pp. 262–268.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Z. Shukor, M. F. Miskon, M. H. Jamaluddin, F. bin Ali, M. F. Asyraf, M. B. bin Bahar, et al., A new data glove approach for malaysian sign language detection, Procedia Computer Science 76 (2015) 60–67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
K. Lai, J. Konrad, P. Ishwar, A gesture-driven computer interface using kinect, in: 2012 IEEE Southwest Symposium on Image Analysis and Interpretation, IEEE, 2012, pp. 185–188.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. G. M. Almeida, F. G. Guimarães, J. A. Ramírez, Feature extraction in brazilian sign language recognition based on phonological structure and using rgb-d sensors, Expert Systems with Applications 41 (16) (2014) 7259–7271.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
H. Hongo, M. Ohya, M. Yasumoto, Y. Niwa, K. Yamamoto, Focus of attention for face and hand gesture recognition using multiple cameras, in: Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), IEEE, 2000, pp. 156–161.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Mohandes, S. A-Buraiky, T. Halawani, S. Al-Baiyat, Automation of the arabic sign language recognition, in: Proceedings. 2004 International Conference on Information and Communication Technologies: From Theory to Applications, 2004., IEEE, 2004, pp. 479–480.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K. K. Podder, M. E. Chowdhury, A. M. Tahir, Z. B. Mahbub, A. Khandakar, M. S. Hossain, M. A. Kadir, Bangla sign language (bdsl) alphabets and numerals classification using a deep learning model, Sensors 22 (2) (2022) 574.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M. J. Awan, M. S. M. Rahim, N. Salim, A. Rehman, H. Nobanee, H. Shabir, Improved deep convolutional neural network to classify osteoarthritis from anterior cruciate ligament tear using magnetic resonance imaging, Journal of Personalized Medicine 11 (11) (2021) 1163.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M. S. Islam, S. S. S. Mousumi, N. A. Jessan, A. S. A. Rabby, S. A. Hossain, Ishara-lipi: The first complete multipurposeopen access dataset of isolated characters for bangla sign language, in: 2018 International Conference on Bangla Speech and Language Processing (ICBSLP), IEEE, 2018, pp. 1–4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. T. Hoque, M. Rifat-Ut-Tauwab, M. F. Kabir, F. Sarker, M. N. Huda, K. Abdullah-Al-Mamun, Automated bangla sign language translation system: Prospects, limitations and applications, in: 2016 5th International Conference on Informatics, Electronics and Vision (ICIEV), IEEE, 2016, pp. 856–862.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. M. Rafi, N. Nawal, N. S. N. Bayev, L. Nima, C. Shahnaz, S. A. Fattah, Image-based bengali sign language alphabet recognition for deaf and dumb community, in: 2019 IEEE global humanitarian technology conference (GHTC), IEEE, 2019, pp. 1–7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. AKON, M.Z.; Nahid, Ku-bdsl: Khulna university bengali sign language dataset, in: Mendeley Data. Version 1. 2021. Available online: https://data.mendeley.com/datasets/scpvm2nbkm/1, pp. 1–7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T. Abedin, K. S. Prottoy, A. Moshruba, S. B. Hakim, Bangla sign language recognition using concatenated bdsl network, arXiv preprint arXiv:2107.11818 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S. K. Youme, T. A. Chowdhury, H. Ahamed, M. S. Abid, L. Chowdhury, N. Mohammed, Generalization of bangla sign language recognition using angular loss functions, IEEE Access 9 (2021) 165351–165365.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Y. Chen, L. Zhao, X. Peng, J. Yuan, D. N. Metaxas, Construct dynamic graphs for hand gesture recognition via spatial-temporal attention, arXiv preprint arXiv:1907.08871 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
L. Shi, Y. Zhang, J. Cheng, H. Lu, Decoupled spatial-temporal attention network for skeleton-based action-gesture recognition, in: Proceedings of the Asian Conference on Computer Vision, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. S. M. Miah, M. A. M. Hasan, J. Shin, Dynamic hand gesture recognition using multi-branch attention based graph and general deep learning model, IEEE Access (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
e. a. Jungpil Shin, Abu Saleh Musa Miah, A methodological and structural review of hand gesture recognition across diverse data modalities, arXiv preprint:2408.05436 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A. S. M. Miah, M. A. M. Hasan, Y. Tomioka, J. Shin, Hand gesture recognition for multi-culture sign language using graph and general deep learning network, IEEE Open Journal of the Computer Society (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A. S. M. Miah, M. A. M. Hasan, S. Nishimura, J. Shin, Sign language recognition using graph and general deep neural network based on large scale dataset, IEEE Access (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A. S. M. Miah, M. A. M. Hasan, J. Shin, Y. Okuyama, Y. Tomioka, <a class="ltx_ref ltx_href" href="https://www.mdpi.com/2073-431X/12/1/13" title="">Multistage spatial attention-based neural network for hand gesture recognition</a>, Computers 12 (1) (2023).

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2073-431X/12/1/13" title="">https://www.mdpi.com/2073-431X/12/1/13</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
A. S. M. Miah, M. A. M. Hasan, S.-W. Jang, H.-S. Lee, J. Shin, <a class="ltx_ref ltx_href" href="https://www.mdpi.com/2079-9292/12/13/2841" title="">Multi-stream general and graph-based deep neural networks for skeleton-based sign language recognition</a>, Electronics 12 (13) (2023).

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2079-9292/12/13/2841" title="">https://www.mdpi.com/2079-9292/12/13/2841</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A. S. M. Miah, M. A. M. Hasan, Y. Okuyama, Y. Tomioka, J. Shin, Spatial–temporal attention with graph and general neural network-based sign language recognition, Pattern Analysis and Applications 27 (2) (2024) 37.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
D. Aryanie, Y. Heryadi, American sign language-based finger-spelling recognition using k-nearest neighbors classifier, in: 2015 3rd International Conference on Information and Communication Technology (ICoICT), IEEE, 2015, pp. 533–536.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
N. Mukai, N. Harada, Y. Chang, Japanese fingerspelling recognition based on classification tree and machine learning, in: 2017 Nicograph International (NicoInt), IEEE, 2017, pp. 19–24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
T. Pariwat, P. Seresangtakul, Thai finger-spelling sign language recognition using global and local features with svm, in: 2017 9th international conference on knowledge and smart technology (KST), IEEE, 2017, pp. 116–120.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
S. K. Hasan, M. Ahmad, A new approach of sign language recognition system for bilingual users, in: 2015 International Conference on Electrical &amp; Electronic Engineering (ICEEE), IEEE, 2015, pp. 33–36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
M. Hasan, T. H. Sajib, M. Dey, A machine learning based approach for the detection and recognition of bangla sign language, in: 2016 international conference on medical engineering, health informatics and technology (MediTec), IEEE, 2016, pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
M. A. Uddin, S. A. Chowdhury, Hand sign language recognition for bangla alphabet using support vector machine, in: 2016 International Conference on Innovations in Science, Engineering and Technology (ICISET), IEEE, 2016, pp. 1–4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
F. Yasir, P. Prasad, A. Alsadoon, A. Elchouemi, S. Sreedharan, Bangla sign language recognition using convolutional neural network, in: 2017 International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT), IEEE, 2017, pp. 49–53.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
S. T. Ahmed, M. Akhand, Bangladeshi sign language recognition using fingertip position, in: 2016 International conference on medical engineering, health informatics and technology (MediTec), IEEE, 2016, pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
M. S. Islalm, M. M. Rahman, M. H. Rahman, M. Arifuzzaman, R. Sassi, M. Aktaruzzaman, Recognition bangla sign language using convolutional neural network, in: 2019 international conference on innovation and intelligence for informatics, computing, and technologies (3ICT), IEEE, 2019, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
O. Bintey Hoque, M. Imrul Jubair, M. Saiful Islam, A.-F. Akash, A. Sachie Paulson, Real time bangladeshi sign language detection using faster r-cnn, arXiv e-prints (2018) arXiv–1811.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
P. P. Urmee, M. A. Al Mashud, J. Akter, A. S. M. M. Jameel, S. Islam, Real-time bangla sign language detection using xception model with augmented dataset, in: 2019 IEEE International WIE Conference on Electrical and Computer Engineering (WIECON-ECE), IEEE, 2019, pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
M. S. Islam, S. S. S. Mousumi, N. A. Jessan, A. S. A. Rabby, S. Abujar, S. A. Hossain, Ishara-bochon: The first multipurpose open access dataset for bangla sign language isolated digits, in: Recent Trends in Image Processing and Pattern Recognition: Second International Conference, RTIP2R 2018, Solapur, India, December 21–22, 2018, Revised Selected Papers, Part I 2, Springer, 2019, pp. 420–428.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
M. M. Hasan, A. Y. Srizon, M. A. M. Hasan, Classification of bengali sign language characters by applying a novel deep convolutional neural network, in: 2020 IEEE Region 10 Symposium (TENSYMP), IEEE, 2020, pp. 1303–1306.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
R. A. Nihal, S. Rahman, N. M. Broti, S. A. Deowan, Bangla sign alphabet recognition with zero-shot and transfer learning, Pattern Recognition Letters 150 (2021) 84–93.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Y. Tian, X. Peng, L. Zhao, S. Zhang, D. N. Metaxas, Cr-gan: learning complete representations for multi-view generation, arXiv preprint arXiv:1806.11191 (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
J. Liu, A. Shahroudy, D. Xu, G. Wang, Spatio-temporal lstm with trust gates for 3d human action recognition, in: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, Springer, 2016, pp. 816–833.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information processing systems 30 (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, R. Salakhutdinov, Transformer-xl: Attentive language models beyond a fixed-length context, arXiv preprint arXiv:1901.02860 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
C. Si, W. Chen, W. Wang, L. Wang, T. Tan, An attention enhanced graph convolutional lstm network for skeleton-based action recognition, in: proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 1227–1236.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
F. Baradel, C. Wolf, J. Mille, Human action recognition: Pose-based attention draws focus to hands, in: Proceedings of the IEEE International conference on computer vision workshops, 2017, pp. 604–613.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
S. Song, C. Lan, J. Xing, W. Zeng, J. Liu, An end-to-end spatio-temporal attention model for human action recognition from skeleton data, in: Proceedings of the AAAI conference on artificial intelligence, Vol. 31, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
S. Yan, Y. Xiong, D. Lin, Spatial temporal graph convolutional networks for skeleton-based action recognition, in: Proceedings of the AAAI conference on artificial intelligence, Vol. 32, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
C. Si, Y. Jing, W. Wang, L. Wang, T. Tan, Skeleton-based action recognition with spatial reasoning and temporal stack learning, in: Proceedings of the European conference on computer vision (ECCV), 2018, pp. 103–118.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
I. Sandjaja, N. Marcos, Sign language number recognition, 2009, pp. 1503 – 1508.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/NCM.2009.357" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/NCM.2009.357</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
J. Shin, A. Matsuoka, M. A. M. Hasan, A. Y. Srizon, <a class="ltx_ref ltx_href" href="https://www.mdpi.com/1424-8220/21/17/5856" title="">American sign language alphabet recognition by extracting feature from hand pose estimation</a>, Sensors 21 (17) (2021).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3390/s21175856" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.3390/s21175856</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/1424-8220/21/17/5856" title="">https://www.mdpi.com/1424-8220/21/17/5856</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, H. Adam, Mobilenets: Efficient convolutional neural networks for mobile vision applications (2017).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1704.04861" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1704.04861</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
S. Zahan, G. M. Hassan, A. Mian, Sdfa: Structure aware discriminative feature aggregation for efficient human fall detection in video, IEEE Transactions on Industrial Informatics (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, X. Zheng, Tensorflow: Large-scale machine learning on heterogeneous distributed systems (2016).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1603.04467" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1603.04467</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
K. Tock, Google colaboratory as a platform for python coding with students, RTSRE Proceedings 2 (1) (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
S. Gollapudi, S. Gollapudi, Opencv with python, Learn Computer Vision Using OpenCV: With Deep Learning CNNs and RNNs (2019) 31–50.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
X. Glorot, Y. Bengio, Understanding the difficulty of training deep feedforward neural networks, in: Proceedings of the thirteenth international conference on artificial intelligence and statistics, JMLR Workshop and Conference Proceedings, 2010, pp. 249–256.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
T. Dozat, Incorporating nesterov momentum into adam (2016).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556 (2014).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
C. Szegedy, V. Vanhoucmiah2024sign largescaleke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the inception architecture for computer vision, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818–2826.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
T. Hao, Human activity recognition based on wavenet, in: 2021 IEEE 7th World Forum on Internet of Things (WF-IoT), IEEE, 2021, pp. 824–829.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
R. A. Mohameed, R. M. Naji, A. M. Ahmeed, D. A. Saeed, M. A. Mosleh, Automated translation for yemeni’s sign language to text usingtransfer learning-based convolutional neural networks, in: 2021 1st International Conference on Emerging Smart Technologies and Applications (eSmarTA), IEEE, 2021, pp. 1–5.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 26 08:55:12 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
