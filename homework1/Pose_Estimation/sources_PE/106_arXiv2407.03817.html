<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Markerless Multi-view 3D Human Pose Estimation: a survey</title>
<!--Generated on Thu Jul  4 10:09:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.03817v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S1" title="In Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S1.SS1" title="In 1 Introduction ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Previous literature reviews</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S1.SS2" title="In 1 Introduction ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Search Process</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S2" title="In Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S3" title="In Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4" title="In Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Multi-view approaches</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.SS1" title="In 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Single-person approaches</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.SS1.SSS1" title="In 4.1 Single-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Methods under different supervision levels</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.SS2" title="In 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Multi-person approaches</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.SS2.SSS1" title="In 4.2 Multi-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Geometric constraint-based methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.SS2.SSS2" title="In 4.2 Multi-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Voxel-based approaches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.SS2.SSS3" title="In 4.2 Multi-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Plane sweep stereo-based models</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S5" title="In Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Multi-modal approaches</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S5.SS1" title="In 5 Multi-modal approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Different type of cameras</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S5.SS2" title="In 5 Multi-modal approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Wireless sensors</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S6" title="In Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1" title="In Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Datasets - Benchmarking</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1.SS1" title="In Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Single-person 3D pose estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1.SS2" title="In Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Multi-person 3D pose estimation</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">[orcid=0000-0002-9413-3300]
<span class="ltx_ERROR undefined" id="p1.1.1">\cormark</span>[1]
<span class="ltx_ERROR undefined" id="p1.1.2">\credit</span>&lt;Credit authorship details&gt;
<span class="ltx_ERROR undefined" id="p1.1.3">\cortext</span>[1]Corresponding author</p>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">[orcid=0000-0002-6193-8540]
<span class="ltx_ERROR undefined" id="p2.1.1">\credit</span></p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">[orcid=0000-0002-4050-7880]
<span class="ltx_ERROR undefined" id="p3.1.1">\credit</span></p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">1]organization=Instituto de Engenharia de Sistemas e Computadores, Tecnologia e Ciência (INESC TEC),
addressline=Rua Dr. Roberto Frias,
postcode=4200-465,
city=Porto,
country=Portugal
2]organization=Faculdade de Engenharia da Universidade do Porto (FEUP),
addressline=Rua Dr. Roberto Frias,
postcode=4200-465,
city=Porto,
country=Portugal
3]organization=Faculdade de Ciências da Universidade do Porto (FCUP),
addressline=Rua do Campo Alegre,
postcode=1021-1055,
city=Porto,
country=Portugal</p>
</div>
<h1 class="ltx_title ltx_title_document">Markerless Multi-view 3D Human Pose Estimation: a survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ana Filipa Rodrigues Nogueira
</span><span class="ltx_author_notes">ana.f.rodrigues@inesctec.pt</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hélder P. Oliveira
</span><span class="ltx_author_notes">helder.f.oliveira@inesctec.pt</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luís F. Teixeira
</span><span class="ltx_author_notes">luisft@fe.up.pt
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">3D human pose estimation aims to reconstruct the human skeleton of all the individuals in a scene by detecting several body joints. The creation of accurate and efficient methods is required for several real-world applications including animation, human-robot interaction, surveillance systems or sports, among many others. However, several obstacles such as occlusions, random camera perspectives, or the scarcity of 3D labelled data, have been hampering the models’ performance and limiting their deployment in real-world scenarios. The higher availability of cameras has led researchers to explore multi-view solutions due to the advantage of being able to exploit different perspectives to reconstruct the pose.</p>
<p class="ltx_p" id="id2.id2">Thus, the goal of this survey is to present an overview of the methodologies used to estimate the 3D pose in multi-view settings, understand what were the strategies found to address the various challenges and also, identify their limitations. Based on the reviewed articles, it was possible to find that no method is yet capable of solving all the challenges associated with the reconstruction of the 3D pose. Due to the existing trade-off between complexity and performance, the best method depends on the application scenario. Therefore, further research is still required to develop an approach capable of quickly inferring a highly accurate 3D pose with bearable computation cost. To this goal, techniques such as active learning, methods that learn with a low level of supervision, the incorporation of temporal consistency, view selection, estimation of depth information and multi-modal approaches might be interesting strategies to keep in mind when developing a new methodology to solve this task.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
3D Human Pose Estimation <span class="ltx_ERROR undefined" id="id3.id1">\sep</span>Multi-view <span class="ltx_ERROR undefined" id="id4.id2">\sep</span>Supervision level <span class="ltx_ERROR undefined" id="id5.id3">\sep</span>Temporal consistency <span class="ltx_ERROR undefined" id="id6.id4">\sep</span>Multi-modal

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">3D human pose estimation aims to reconstruct the body configuration of all persons in a scene. Finding solutions for this task is essential for numerous applications ranging from human-robot interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib92" title=""><span class="ltx_text ltx_font_typewriter">92</span></a>]</cite>, animation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib37" title=""><span class="ltx_text ltx_font_typewriter">37</span></a>]</cite>, gaming, action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib53" title=""><span class="ltx_text ltx_font_typewriter">53</span></a>]</cite>, rehabilitation assessments, surveillance systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib23" title="">23</a>]</cite>, sports <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib60" title=""><span class="ltx_text ltx_font_typewriter">60</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib9" title="">9</a>]</cite>, live broadcasts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib28" title=""><span class="ltx_text ltx_font_typewriter">28</span></a>]</cite>, human-computer interaction, such as recognising sign language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib25" title=""><span class="ltx_text ltx_font_typewriter">25</span></a>]</cite>, among many others. As an example of application, the work of Mustafa et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib60" title=""><span class="ltx_text ltx_font_typewriter">60</span></a>]</cite> showed that the reconstruction of the 3D pose in a multi-view setting helped the creation of a method for 4D dynamic scene understanding with numerous interacting individuals, such as sports games.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Therefore, it is necessary to develop solutions that can solve this task effectively and efficiently, without the need to use markers. Because the use of markers imposes many restrictions on the application scenarios in which the methods can be used. Furthermore, placing markers over clothing can lead to incorrect readings of the person’s keypoints due to possible displacement of a marker during the performance of movements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib74" title=""><span class="ltx_text ltx_font_typewriter">74</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Methods for estimating the 2D pose have been widely explored, however, those only allowed the reconstruction of a surface pose. Thus, to capture the volume of the body of the person, it is necessary to determine the 3D pose. Nonetheless, since there have been more advances in 2D pose estimation methods, many 3D estimation algorithms, use the 2D pose estimations for each view to reconstruct the 3D pose.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The use of multiple views helps to capture the whole body geometry making it easier and more suitable for 3D pose estimation than monocular methodologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib81" title=""><span class="ltx_text ltx_font_typewriter">81</span></a>]</cite>. However, the occurrence of occlusions, poor camera calibration, lack of 3D annotated data, similarities or variations in human appearance, and difficulties in associating the multiple views and generalise to new perspectives are some challenges methods developed for multi-view systems have to overcome to accurately estimate the 3D pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib88" title=""><span class="ltx_text ltx_font_typewriter">88</span></a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Previous literature reviews</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Solutions to accurately and efficiently estimate the 3D human pose have been widely explored over the years. Thus, there are numerous surveys summarizing and evaluating most of the existing works.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">Initially, most of the methodologies were developed for single-view inputs. Therefore, there are several surveys addressing 3D pose estimation based on monocular images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib76" title=""><span class="ltx_text ltx_font_typewriter">76</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib57" title=""><span class="ltx_text ltx_font_typewriter">57</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib41" title=""><span class="ltx_text ltx_font_typewriter">41</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib54" title=""><span class="ltx_text ltx_font_typewriter">54</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib51" title=""><span class="ltx_text ltx_font_typewriter">51</span></a>]</cite>. The main findings are that, even though deep learning has brought considerable improvements to the estimation of the 3D pose, occlusions, crowded scenarios, and a lack of datasets that can realistically mimic real-world settings still limit the models’ performance and restrain their real-world employment. For future research, it is suggested the use of transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib57" title=""><span class="ltx_text ltx_font_typewriter">57</span></a>]</cite>, synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib57" title=""><span class="ltx_text ltx_font_typewriter">57</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib54" title=""><span class="ltx_text ltx_font_typewriter">54</span></a>]</cite>, models with little supervision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib54" title=""><span class="ltx_text ltx_font_typewriter">54</span></a>]</cite>, or complementary information by, for example, exploiting a multi-modal approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib76" title=""><span class="ltx_text ltx_font_typewriter">76</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib54" title=""><span class="ltx_text ltx_font_typewriter">54</span></a>]</cite>, incorporate global and local context to obtain more distinguishing characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib17" title="">17</a>]</cite>, explore the interactions between the individuals and the scene or the objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib41" title=""><span class="ltx_text ltx_font_typewriter">41</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib54" title=""><span class="ltx_text ltx_font_typewriter">54</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib51" title=""><span class="ltx_text ltx_font_typewriter">51</span></a>]</cite> or add the use of temporal information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib57" title=""><span class="ltx_text ltx_font_typewriter">57</span></a>]</cite>. In addition, multi-view geometry is pointed out as a solution for data scarcity and the ambiguities in monocular 3D pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib41" title=""><span class="ltx_text ltx_font_typewriter">41</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib54" title=""><span class="ltx_text ltx_font_typewriter">54</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib51" title=""><span class="ltx_text ltx_font_typewriter">51</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">The increased availability of multi-camera setups has prompted further research on the benefits of having various camera perspectives. As a result, several surveys, besides analysing single-view methods, also cover multi-view approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib96" title=""><span class="ltx_text ltx_font_typewriter">96</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib55" title=""><span class="ltx_text ltx_font_typewriter">55</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib70" title=""><span class="ltx_text ltx_font_typewriter">70</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib85" title=""><span class="ltx_text ltx_font_typewriter">85</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib62" title=""><span class="ltx_text ltx_font_typewriter">62</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib100" title=""><span class="ltx_text ltx_font_typewriter">100</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">47</span></a>]</cite>.
The literature review of Holte et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib34" title=""><span class="ltx_text ltx_font_typewriter">34</span></a>]</cite> is the only one found, solely dedicated to multi-view methodologies.
According to the analysed surveys, most of the problems observed in monocular settings are the same for multi-view, namely, the lack of annotated in-the-wild datasets or with rare poses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib55" title=""><span class="ltx_text ltx_font_typewriter">55</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib70" title=""><span class="ltx_text ltx_font_typewriter">70</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib85" title=""><span class="ltx_text ltx_font_typewriter">85</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib100" title=""><span class="ltx_text ltx_font_typewriter">100</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib62" title=""><span class="ltx_text ltx_font_typewriter">62</span></a>]</cite>, the difficulty in identifying the several poses in crowded scenarios, and the occlusions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib96" title=""><span class="ltx_text ltx_font_typewriter">96</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">47</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib100" title=""><span class="ltx_text ltx_font_typewriter">100</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib34" title=""><span class="ltx_text ltx_font_typewriter">34</span></a>]</cite>. Even though, overcoming the occlusions problem benefits from the different view perspectives, it still remains a challenge in some situations. Besides, the increase in the number of viewpoints leads to the need for more complex models to be able to deal with the information acquired from all perspectives. This translates into slower inference runtime, forcing a balance between complexity and speed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">47</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib100" title=""><span class="ltx_text ltx_font_typewriter">100</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">49</span></a>]</cite>. Furthermore, the bad image quality in terms of focus or blur <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib100" title=""><span class="ltx_text ltx_font_typewriter">100</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">47</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib34" title=""><span class="ltx_text ltx_font_typewriter">34</span></a>]</cite>, the inaccuracies of 2D pose estimations which affect the models that use 2D-3D lifting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib34" title=""><span class="ltx_text ltx_font_typewriter">34</span></a>]</cite> are also, factors negatively influencing the quality of the predictions. So, there are still a lot of opportunities for improvement, such as considering the human interactions either with other humans or the surrounding environment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib70" title=""><span class="ltx_text ltx_font_typewriter">70</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib85" title=""><span class="ltx_text ltx_font_typewriter">85</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib100" title=""><span class="ltx_text ltx_font_typewriter">100</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib34" title=""><span class="ltx_text ltx_font_typewriter">34</span></a>]</cite>, the increase of the generality in order for the same method to be suitable across multiple applications and independent of the presented viewpoints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">47</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib34" title=""><span class="ltx_text ltx_font_typewriter">34</span></a>]</cite>. Also, several surveys recommend the use of neural architecture search for a better and more efficient design of neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib85" title=""><span class="ltx_text ltx_font_typewriter">85</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">47</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib100" title=""><span class="ltx_text ltx_font_typewriter">100</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">49</span></a>]</cite>, taking advantage of temporal information, even the adaptation of methods from monocular to multi-view <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">47</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib22" title="">22</a>]</cite>, the use of transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib55" title=""><span class="ltx_text ltx_font_typewriter">55</span></a>]</cite> and data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib100" title=""><span class="ltx_text ltx_font_typewriter">100</span></a>]</cite>. Finally, to combat the scarcity of annotated datasets, several strategies are suggested like using unsupervised, semi-supervised or weakly-supervised models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib55" title=""><span class="ltx_text ltx_font_typewriter">55</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib85" title=""><span class="ltx_text ltx_font_typewriter">85</span></a>]</cite>, or the use of active learning to alleviate the human workload <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib62" title=""><span class="ltx_text ltx_font_typewriter">62</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p4">
<p class="ltx_p" id="S1.SS1.p4.1">Moreover, some surveys briefly mention multi-modal approaches, mainly, combining vision and <span class="ltx_glossaryref" title="">Inertial Measurement Units (IMUs)</span> sensors but also, depth sensors, point clouds or <span class="ltx_glossaryref" title="">Radio-Frequency (RF)</span>, which allow to obtain more accurate estimations, demonstrating the benefits of having complementary information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib100" title=""><span class="ltx_text ltx_font_typewriter">100</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">49</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p5">
<p class="ltx_p" id="S1.SS1.p5.1">Therefore, the scope of the present survey is in markerless methods, so, it is only going to be considered methods that do not require people to have attached sensors. Also, the literature lacks a review focus on methods which used various cameras to determine the pose. Most of the surveys that address these methods only present a relatively brief section, as can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S1.T1" title="Table 1 ‣ 1.1 Previous literature reviews ‣ 1 Introduction ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">1</span></a>.
Thus, this literature review will solely focus on 3D Human Pose Estimation based on data from multiple cameras and also, acknowledge works that use different types of sensors, such as <span class="ltx_glossaryref" title="">Red Green Blue-Depth (RGB-D)</span> cameras, <span class="ltx_glossaryref" title="">Time-of-Flight (ToF)</span> cameras or wireless devices, to obtain the 3D pose in a multi-view environment.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Table of a comparative analysis between the surveys on 3D pose estimation</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S1.T1.1">
<tr class="ltx_tr" id="S1.T1.1.1">
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_tt" id="S1.T1.1.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.1">Papers</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S1.T1.1.1.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.2.1">
<span class="ltx_p" id="S1.T1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.2.1.1.1">Year</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S1.T1.1.1.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.3.1">
<span class="ltx_p" id="S1.T1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.3.1.1.1">Monocular view methods</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S1.T1.1.1.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.4.1">
<span class="ltx_p" id="S1.T1.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.4.1.1.1">Multi-view methods</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S1.T1.1.1.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.5.1">
<span class="ltx_p" id="S1.T1.1.1.5.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.5.1.1.1">Wi-Fi, RF or other sensors</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S1.T1.1.1.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.6.1">
<span class="ltx_p" id="S1.T1.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.6.1.1.1">Datasets</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S1.T1.1.1.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.7.1">
<span class="ltx_p" id="S1.T1.1.1.7.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.7.1.1.1">Benchmarking Performance</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S1.T1.1.1.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.8.1">
<span class="ltx_p" id="S1.T1.1.1.8.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.8.1.1.1">Metrics</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.2.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.1">
<span class="ltx_p" id="S1.T1.1.2.1.1.1">Human pose estimation and activity recognition from multi-view videos: Comparative explorations of recent developments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib34" title=""><span class="ltx_text ltx_font_typewriter">34</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.2.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.2.1">
<span class="ltx_p" id="S1.T1.1.2.2.1.1">2012</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.2.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.3.1">
<span class="ltx_p" id="S1.T1.1.2.3.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.2.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.4.1">
<span class="ltx_p" id="S1.T1.1.2.4.1.1">✓<span class="ltx_text" id="S1.T1.1.2.4.1.1.1" style="font-size:70%;"> (24 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.2.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.5.1">
<span class="ltx_p" id="S1.T1.1.2.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.2.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.6.1">
<span class="ltx_p" id="S1.T1.1.2.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.2.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.7.1">
<span class="ltx_p" id="S1.T1.1.2.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.2.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.8.1">
<span class="ltx_p" id="S1.T1.1.2.8.1.1">✗</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.3.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.1.1">
<span class="ltx_p" id="S1.T1.1.3.1.1.1">A survey of human pose estimation: The body parts parsing based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib55" title=""><span class="ltx_text ltx_font_typewriter">55</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.3.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.1">
<span class="ltx_p" id="S1.T1.1.3.2.1.1">2015</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.3.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.3.1">
<span class="ltx_p" id="S1.T1.1.3.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.3.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.4.1">
<span class="ltx_p" id="S1.T1.1.3.4.1.1">✓<span class="ltx_text" id="S1.T1.1.3.4.1.1.1" style="font-size:70%;"> (6 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.3.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.5.1">
<span class="ltx_p" id="S1.T1.1.3.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.3.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.6.1">
<span class="ltx_p" id="S1.T1.1.3.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.3.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.7.1">
<span class="ltx_p" id="S1.T1.1.3.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.3.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.8.1">
<span class="ltx_p" id="S1.T1.1.3.8.1.1">✗</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.4.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.1.1">
<span class="ltx_p" id="S1.T1.1.4.1.1.1">3D Human pose estimation: A review of the literature and analysis of covariates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib70" title=""><span class="ltx_text ltx_font_typewriter">70</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.4.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.2.1">
<span class="ltx_p" id="S1.T1.1.4.2.1.1">2016</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.4.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.1">
<span class="ltx_p" id="S1.T1.1.4.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.4.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.4.1">
<span class="ltx_p" id="S1.T1.1.4.4.1.1">✓<span class="ltx_text" id="S1.T1.1.4.4.1.1.1" style="font-size:70%;"> (11 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.4.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.5.1">
<span class="ltx_p" id="S1.T1.1.4.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.4.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.6.1">
<span class="ltx_p" id="S1.T1.1.4.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.4.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.7.1">
<span class="ltx_p" id="S1.T1.1.4.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.4.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.8.1">
<span class="ltx_p" id="S1.T1.1.4.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.5.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.1.1">
<span class="ltx_p" id="S1.T1.1.5.1.1.1">A survey on monocular 3D human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib41" title=""><span class="ltx_text ltx_font_typewriter">41</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.5.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.2.1">
<span class="ltx_p" id="S1.T1.1.5.2.1.1">2020</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.5.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.3.1">
<span class="ltx_p" id="S1.T1.1.5.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.5.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.1">
<span class="ltx_p" id="S1.T1.1.5.4.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.5.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.5.1">
<span class="ltx_p" id="S1.T1.1.5.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.5.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.6.1">
<span class="ltx_p" id="S1.T1.1.5.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.5.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.7.1">
<span class="ltx_p" id="S1.T1.1.5.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.5.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.8.1">
<span class="ltx_p" id="S1.T1.1.5.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.6.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.1.1">
<span class="ltx_p" id="S1.T1.1.6.1.1.1">Monocular human pose estimation: A survey of deep learning-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib17" title="">17</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.6.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.2.1">
<span class="ltx_p" id="S1.T1.1.6.2.1.1">2020</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.6.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.3.1">
<span class="ltx_p" id="S1.T1.1.6.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.6.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.4.1">
<span class="ltx_p" id="S1.T1.1.6.4.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.6.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.1">
<span class="ltx_p" id="S1.T1.1.6.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.6.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.6.1">
<span class="ltx_p" id="S1.T1.1.6.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.6.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.7.1">
<span class="ltx_p" id="S1.T1.1.6.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.6.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.8.1">
<span class="ltx_p" id="S1.T1.1.6.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.7.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.1.1">
<span class="ltx_p" id="S1.T1.1.7.1.1.1">Deep 3D human pose estimation: A review <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib85" title=""><span class="ltx_text ltx_font_typewriter">85</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.7.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.2.1">
<span class="ltx_p" id="S1.T1.1.7.2.1.1">2021</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.7.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.3.1">
<span class="ltx_p" id="S1.T1.1.7.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.7.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.4.1">
<span class="ltx_p" id="S1.T1.1.7.4.1.1">✓<span class="ltx_text" id="S1.T1.1.7.4.1.1.1" style="font-size:70%;"> (16 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.7.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.5.1">
<span class="ltx_p" id="S1.T1.1.7.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.7.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.6.1">
<span class="ltx_p" id="S1.T1.1.7.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.7.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.7.1">
<span class="ltx_p" id="S1.T1.1.7.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.7.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.8.1">
<span class="ltx_p" id="S1.T1.1.7.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.8.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.8.1.1">
<span class="ltx_p" id="S1.T1.1.8.1.1.1">Deep Learning Methods for 3D Human Pose Estimation under Different Supervision Paradigms: A Survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib96" title=""><span class="ltx_text ltx_font_typewriter">96</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.8.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.8.2.1">
<span class="ltx_p" id="S1.T1.1.8.2.1.1">2021</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.8.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.8.3.1">
<span class="ltx_p" id="S1.T1.1.8.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.8.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.8.4.1">
<span class="ltx_p" id="S1.T1.1.8.4.1.1">✓<span class="ltx_text" id="S1.T1.1.8.4.1.1.1" style="font-size:70%;"> (11 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.8.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.8.5.1">
<span class="ltx_p" id="S1.T1.1.8.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.8.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.8.6.1">
<span class="ltx_p" id="S1.T1.1.8.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.8.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.8.7.1">
<span class="ltx_p" id="S1.T1.1.8.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.8.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.8.8.1">
<span class="ltx_p" id="S1.T1.1.8.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.9.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.9.1.1">
<span class="ltx_p" id="S1.T1.1.9.1.1.1">A review of 3D human pose estimation algorithms for markerless motion capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib22" title="">22</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.9.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.9.2.1">
<span class="ltx_p" id="S1.T1.1.9.2.1.1">2021</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.9.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.9.3.1">
<span class="ltx_p" id="S1.T1.1.9.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.9.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.9.4.1">
<span class="ltx_p" id="S1.T1.1.9.4.1.1">✓<span class="ltx_text" id="S1.T1.1.9.4.1.1.1" style="font-size:70%;"> (7 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.9.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.9.5.1">
<span class="ltx_p" id="S1.T1.1.9.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.9.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.9.6.1">
<span class="ltx_p" id="S1.T1.1.9.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.9.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.9.7.1">
<span class="ltx_p" id="S1.T1.1.9.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.9.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.9.8.1">
<span class="ltx_p" id="S1.T1.1.9.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.10.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.10.1.1">
<span class="ltx_p" id="S1.T1.1.10.1.1.1">Human pose estimation and its application to action recognition: A survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib76" title=""><span class="ltx_text ltx_font_typewriter">76</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.10.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.10.2.1">
<span class="ltx_p" id="S1.T1.1.10.2.1.1">2021</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.10.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.10.3.1">
<span class="ltx_p" id="S1.T1.1.10.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.10.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.10.4.1">
<span class="ltx_p" id="S1.T1.1.10.4.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.10.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.10.5.1">
<span class="ltx_p" id="S1.T1.1.10.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.10.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.10.6.1">
<span class="ltx_p" id="S1.T1.1.10.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.10.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.10.7.1">
<span class="ltx_p" id="S1.T1.1.10.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.10.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.10.8.1">
<span class="ltx_p" id="S1.T1.1.10.8.1.1">✗</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.11">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.11.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.11.1.1">
<span class="ltx_p" id="S1.T1.1.11.1.1.1"><span class="ltx_text" id="S1.T1.1.11.1.1.1.1" style="font-size:90%;">A review of deep learning techniques for 2D and 3D human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib8" title="">8</a>]</cite></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.11.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.11.2.1">
<span class="ltx_p" id="S1.T1.1.11.2.1.1">2021</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.11.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.11.3.1">
<span class="ltx_p" id="S1.T1.1.11.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.11.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.11.4.1">
<span class="ltx_p" id="S1.T1.1.11.4.1.1">✓<span class="ltx_text" id="S1.T1.1.11.4.1.1.1" style="font-size:70%;"> (9 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.11.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.11.5.1">
<span class="ltx_p" id="S1.T1.1.11.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.11.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.11.6.1">
<span class="ltx_p" id="S1.T1.1.11.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.11.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.11.7.1">
<span class="ltx_p" id="S1.T1.1.11.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.11.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.11.8.1">
<span class="ltx_p" id="S1.T1.1.11.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.12">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.12.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.12.1.1">
<span class="ltx_p" id="S1.T1.1.12.1.1.1">A Survey of Recent Advances on Two-Step 3D Human Pose Estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib57" title=""><span class="ltx_text ltx_font_typewriter">57</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.12.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.12.2.1">
<span class="ltx_p" id="S1.T1.1.12.2.1.1">2022</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.12.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.12.3.1">
<span class="ltx_p" id="S1.T1.1.12.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.12.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.12.4.1">
<span class="ltx_p" id="S1.T1.1.12.4.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.12.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.12.5.1">
<span class="ltx_p" id="S1.T1.1.12.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.12.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.12.6.1">
<span class="ltx_p" id="S1.T1.1.12.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.12.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.12.7.1">
<span class="ltx_p" id="S1.T1.1.12.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.12.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.12.8.1">
<span class="ltx_p" id="S1.T1.1.12.8.1.1">✗</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.13">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.13.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.13.1.1">
<span class="ltx_p" id="S1.T1.1.13.1.1.1">Human pose estimation using deep learning: review, methodologies, progress and future research directions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">47</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.13.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.13.2.1">
<span class="ltx_p" id="S1.T1.1.13.2.1.1">2022</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.13.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.13.3.1">
<span class="ltx_p" id="S1.T1.1.13.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.13.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.13.4.1">
<span class="ltx_p" id="S1.T1.1.13.4.1.1">✓<span class="ltx_text" id="S1.T1.1.13.4.1.1.1" style="font-size:70%;"> (9 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.13.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.13.5.1">
<span class="ltx_p" id="S1.T1.1.13.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.13.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.13.6.1">
<span class="ltx_p" id="S1.T1.1.13.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.13.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.13.7.1">
<span class="ltx_p" id="S1.T1.1.13.7.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.13.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.13.8.1">
<span class="ltx_p" id="S1.T1.1.13.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.14">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.14.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.14.1.1">
<span class="ltx_p" id="S1.T1.1.14.1.1.1">Recent Advances of Monocular 2D and 3D Human Pose Estimation: A Deep Learning Perspective <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib54" title=""><span class="ltx_text ltx_font_typewriter">54</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.14.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.14.2.1">
<span class="ltx_p" id="S1.T1.1.14.2.1.1">2022</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.14.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.14.3.1">
<span class="ltx_p" id="S1.T1.1.14.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.14.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.14.4.1">
<span class="ltx_p" id="S1.T1.1.14.4.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.14.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.14.5.1">
<span class="ltx_p" id="S1.T1.1.14.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.14.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.14.6.1">
<span class="ltx_p" id="S1.T1.1.14.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.14.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.14.7.1">
<span class="ltx_p" id="S1.T1.1.14.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.14.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.14.8.1">
<span class="ltx_p" id="S1.T1.1.14.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.15">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.15.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.15.1.1">
<span class="ltx_p" id="S1.T1.1.15.1.1.1">Deep Learning-Based Human Pose Estimation: A Survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib100" title=""><span class="ltx_text ltx_font_typewriter">100</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.15.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.15.2.1">
<span class="ltx_p" id="S1.T1.1.15.2.1.1">2023</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.15.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.15.3.1">
<span class="ltx_p" id="S1.T1.1.15.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.15.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.15.4.1">
<span class="ltx_p" id="S1.T1.1.15.4.1.1">✓<span class="ltx_text" id="S1.T1.1.15.4.1.1.1" style="font-size:70%;"> (26 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.15.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.15.5.1">
<span class="ltx_p" id="S1.T1.1.15.5.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.15.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.15.6.1">
<span class="ltx_p" id="S1.T1.1.15.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.15.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.15.7.1">
<span class="ltx_p" id="S1.T1.1.15.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.15.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.15.8.1">
<span class="ltx_p" id="S1.T1.1.15.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.16">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.16.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.16.1.1">
<span class="ltx_p" id="S1.T1.1.16.1.1.1">Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib62" title=""><span class="ltx_text ltx_font_typewriter">62</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.16.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.16.2.1">
<span class="ltx_p" id="S1.T1.1.16.2.1.1">2023</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.16.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.16.3.1">
<span class="ltx_p" id="S1.T1.1.16.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.16.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.16.4.1">
<span class="ltx_p" id="S1.T1.1.16.4.1.1">✓<span class="ltx_text" id="S1.T1.1.16.4.1.1.1" style="font-size:70%;"> (19 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.16.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.16.5.1">
<span class="ltx_p" id="S1.T1.1.16.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.16.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.16.6.1">
<span class="ltx_p" id="S1.T1.1.16.6.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.16.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.16.7.1">
<span class="ltx_p" id="S1.T1.1.16.7.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.16.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.16.8.1">
<span class="ltx_p" id="S1.T1.1.16.8.1.1">✗</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.17">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.17.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.17.1.1">
<span class="ltx_p" id="S1.T1.1.17.1.1.1">Overview of 3D Human Pose Estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib51" title=""><span class="ltx_text ltx_font_typewriter">51</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.17.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.17.2.1">
<span class="ltx_p" id="S1.T1.1.17.2.1.1">2023</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.17.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.17.3.1">
<span class="ltx_p" id="S1.T1.1.17.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.17.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.17.4.1">
<span class="ltx_p" id="S1.T1.1.17.4.1.1">✓<span class="ltx_text" id="S1.T1.1.17.4.1.1.1" style="font-size:70%;"> (17 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.17.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.17.5.1">
<span class="ltx_p" id="S1.T1.1.17.5.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.17.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.17.6.1">
<span class="ltx_p" id="S1.T1.1.17.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.17.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.17.7.1">
<span class="ltx_p" id="S1.T1.1.17.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.17.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.17.8.1">
<span class="ltx_p" id="S1.T1.1.17.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.18">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.18.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.18.1.1">
<span class="ltx_p" id="S1.T1.1.18.1.1.1">Vision-Based Human Pose Estimation via Deep Learning: A Survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">49</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.18.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.18.2.1">
<span class="ltx_p" id="S1.T1.1.18.2.1.1">2023</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.18.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.18.3.1">
<span class="ltx_p" id="S1.T1.1.18.3.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.18.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.18.4.1">
<span class="ltx_p" id="S1.T1.1.18.4.1.1">✓<span class="ltx_text" id="S1.T1.1.18.4.1.1.1" style="font-size:70%;"> (8 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.18.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.18.5.1">
<span class="ltx_p" id="S1.T1.1.18.5.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.18.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.18.6.1">
<span class="ltx_p" id="S1.T1.1.18.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.18.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.18.7.1">
<span class="ltx_p" id="S1.T1.1.18.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S1.T1.1.18.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.18.8.1">
<span class="ltx_p" id="S1.T1.1.18.8.1.1">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.19">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S1.T1.1.19.1" style="width:176.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.19.1.1">
<span class="ltx_p" id="S1.T1.1.19.1.1.1">Markerless Multi-view 3D Human Pose Estimation: a survey (Ours)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S1.T1.1.19.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.19.2.1">
<span class="ltx_p" id="S1.T1.1.19.2.1.1">2024</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S1.T1.1.19.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.19.3.1">
<span class="ltx_p" id="S1.T1.1.19.3.1.1">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S1.T1.1.19.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.19.4.1">
<span class="ltx_p" id="S1.T1.1.19.4.1.1">✓<span class="ltx_text" id="S1.T1.1.19.4.1.1.1" style="font-size:70%;"> (57 works)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S1.T1.1.19.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.19.5.1">
<span class="ltx_p" id="S1.T1.1.19.5.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S1.T1.1.19.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.19.6.1">
<span class="ltx_p" id="S1.T1.1.19.6.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S1.T1.1.19.7" style="width:51.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.19.7.1">
<span class="ltx_p" id="S1.T1.1.19.7.1.1">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S1.T1.1.19.8" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.19.8.1">
<span class="ltx_p" id="S1.T1.1.19.8.1.1">✓</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Search Process</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">The research process, Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S1.F1" title="Figure 1 ‣ 1.2 Search Process ‣ 1 Introduction ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">1</span></a>, was conducted in September 2023 and updated in June 2024, using the search engine for scientific articles: Scopus. Several combinations of the following keywords were used: 3D pose estimation; multi-view/multi-camera; <span class="ltx_glossaryref" title="">Channel State Information (CSI)</span>, <span class="ltx_glossaryref" title="">RF</span>, Wi-Fi, multi-sensor, multi-modal, <span class="ltx_glossaryref" title="">Light Detection And Ranging (LiDAR)</span>; data-efficiency techniques.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.1">This search process resulted in 204 works, and the following criteria were then applied to filter the results:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Remove duplicates</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Exclude the articles that are entirely unrelated to the subject by analysing the title and abstract.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Include only peer-reviewed works written in English.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Analyse the body of the text and include only the relevant works to the study at hand.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S1.SS2.p2.2">After applying the above criteria, it resulted in 73 works that were fully revised and used to construct this literature review. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S1.F2" title="Figure 2 ‣ 1.2 Search Process ‣ 1 Introduction ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">2</span></a> is possible to see the growing interest in this area.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="588" id="S1.F1.g1" src="extracted/5699987/prisma.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_glossaryref" title="">PRISMA</span> diagram of the conducted research process (adapted from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib63" title=""><span class="ltx_text ltx_font_typewriter">63</span></a>]</cite>).</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="386" id="S1.F2.g1" src="extracted/5699987/nworks_graph.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Evolution of the published works found in Scopus database for markerless 3D pose estimation in multi-camera settings or by using other types of sensors such as <span class="ltx_glossaryref" title="">RGB-D</span> cameras, <span class="ltx_glossaryref" title="">ToF</span> cameras or wireless sensors.</figcaption>
</figure>
<div class="ltx_para" id="S1.SS2.p3">
<p class="ltx_p" id="S1.SS2.p3.1">The rest of this literature review is organised in the following way: Chapter <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S2" title="2 Datasets ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">2</span></a> - Datasets, Chapter <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S3" title="3 Evaluation Metrics ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">3</span></a> - Evaluation Metrics, Chapter <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4" title="4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">4</span></a> - Multi-view approaches, Chapter <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S5" title="5 Multi-modal approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">5</span></a> - Multi-modal approaches, Chapter <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S6" title="6 Conclusion ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">6</span></a> - Conclusion.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Datasets</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The most used datasets to train and assess the performance of 3D Human Pose Estimation in multi-view settings are Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib39" title=""><span class="ltx_text ltx_font_typewriter">39</span></a>]</cite>, Campus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite>, Shelf <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite> and CMU Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib43" title=""><span class="ltx_text ltx_font_typewriter">43</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Human3.6M</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib39" title=""><span class="ltx_text ltx_font_typewriter">39</span></a>]</cite> is a commonly used dataset for estimating the 3D pose of a single person. The data was collected in a laboratory with 4 digital video cameras positioned in the corners, 1 <span class="ltx_glossaryref" title="">ToF</span> on top of one digital camera, and 10 motion cameras spread throughout the walls, with 4 on each side and 2 in the middle of the horizontal edge.
The data included 15 distinct actions: discussion, greeting, posing, walking, eating, sitting and taking photographs, among others. These actions were enacted by a total of 11 unique individuals, encompassing both male and female participants. Out of these 11 individuals, two males and two females were exclusively included in the testing set.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Campus</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite> consists of three persons interacting and being recorded by three cameras in an outdoor environment. Annotations for the body joints of the three individuals were created for cameras 1 and 2. Subsequently, these annotations were triangulated and projected to generate the annotations for the third perspective.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.3"><span class="ltx_text ltx_font_bold" id="S2.p4.3.1">Shelf</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite> comprises four people disassembling a shelf in an indoor setting which is being recorded by five cameras. Due to the consistent occlusion of one participant (Actor 4) in the majority of the frames, the pose estimation corresponding to that person is usually disregarded during the evaluation. The 3D ground-truth annotations were generated by triangulating the body joint annotations from the 2<sup class="ltx_sup" id="S2.p4.3.2"><span class="ltx_text ltx_font_italic" id="S2.p4.3.2.1">nd</span></sup>, 3<sup class="ltx_sup" id="S2.p4.3.3"><span class="ltx_text ltx_font_italic" id="S2.p4.3.3.1">rd</span></sup>, and 4<sup class="ltx_sup" id="S2.p4.3.4"><span class="ltx_text ltx_font_italic" id="S2.p4.3.4.1">th</span></sup> camera perspectives.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.p5.1.1">CMU Panoptic</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib43" title=""><span class="ltx_text ltx_font_typewriter">43</span></a>]</cite> is the dataset with the greatest number of perspectives available, providing a total of 521 views (480 VGA, 31 HD and 10 Kinetic cameras). The data was acquired in a controlled environment and includes both actions performed in groups and individually. Participants were organised into groups of a maximum of 8 elements to participate in social activities such as games like Ultimatum or Haggling, group discussions, musical performances, or dancing.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Other datasets were captured with a specific application scenario in mind, such as KTH Multiview Football <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib46" title=""><span class="ltx_text ltx_font_typewriter">46</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib45" title=""><span class="ltx_text ltx_font_typewriter">45</span></a>]</cite>, which allows tracking people on a football field, or MPII Cooking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib69" title=""><span class="ltx_text ltx_font_typewriter">69</span></a>]</cite>, to track people during their cooking process. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S2" title="2 Datasets ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">2</span></a> summarises the existing datasets and their main characteristics. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.T3" title="Table 3 ‣ 4.1.1 Methods under different supervision levels ‣ 4.1 Single-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">3</span></a>, Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.T4" title="Table 4 ‣ 4.2.3 Plane sweep stereo-based models ‣ 4.2 Multi-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">4</span></a> and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1" title="Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">A</span></a> present the benchmarking for the several datasets used in the revised works.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Summary of the found datasets for Multi-view 3D Human Pose Estimation.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S2.T2.1">
<tr class="ltx_tr" id="S2.T2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S2.T2.1.1.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.1">
<span class="ltx_p" id="S2.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.1.1">Datasets</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S2.T2.1.1.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.2.1">
<span class="ltx_p" id="S2.T2.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.2.1.1.1">Year</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S2.T2.1.1.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.3.1">
<span class="ltx_p" id="S2.T2.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.3.1.1.1">Size</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S2.T2.1.1.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.4.1">
<span class="ltx_p" id="S2.T2.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.4.1.1.1">Nº cameras</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S2.T2.1.1.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.5.1">
<span class="ltx_p" id="S2.T2.1.1.5.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.5.1.1.1">Nº subjects</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S2.T2.1.1.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.6.1">
<span class="ltx_p" id="S2.T2.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.6.1.1.1">Characteristics</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.2.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.1">
<span class="ltx_p" id="S2.T2.1.2.1.1.1">HumanEva-I <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib74" title=""><span class="ltx_text ltx_font_typewriter">74</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.2.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.2.1">
<span class="ltx_p" id="S2.T2.1.2.2.1.1">2010</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.2.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.3.1">
<span class="ltx_p" id="S2.T2.1.2.3.1.1">40 060</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.2.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.4.1">
<span class="ltx_p" id="S2.T2.1.2.4.1.1">7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.2.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.5.1">
<span class="ltx_p" id="S2.T2.1.2.5.1.1">4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.2.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.6.1">
<span class="ltx_p" id="S2.T2.1.2.6.1.1">Single-person dataset that includes 6 distinct actions: random hand movements, boxing, playing with a ball by tossing and catching, walking, running and a combined sequence in which the subject begins walking, then runs and at the end, balances on each foot alternately. The ground-truth data was obtained using a MoCap system with 6 cameras.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.3.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.1.1">
<span class="ltx_p" id="S2.T2.1.3.1.1.1">HumanEva-II <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib74" title=""><span class="ltx_text ltx_font_typewriter">74</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.3.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.1">
<span class="ltx_p" id="S2.T2.1.3.2.1.1">2010</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.3.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.3.1">
<span class="ltx_p" id="S2.T2.1.3.3.1.1">2 460</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.3.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.4.1">
<span class="ltx_p" id="S2.T2.1.3.4.1.1">4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.3.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.5.1">
<span class="ltx_p" id="S2.T2.1.3.5.1.1">2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.3.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.6.1">
<span class="ltx_p" id="S2.T2.1.3.6.1.1">Single-person performing an extended sequence of actions compared to HumanEva-I. The ground-truth motion was captured using a MoCap system with 12 cameras. This is only a test set, the models are supposed to be trained and validated with the data from HumanEva-I.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.4.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.1.1">
<span class="ltx_p" id="S2.T2.1.4.1.1.1"><span class="ltx_glossaryref" title="">Utrecht Multi-Person Motion (UMPM)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib1" title="">1</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.4.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.2.1">
<span class="ltx_p" id="S2.T2.1.4.2.1.1">2011</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.4.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.1">
<span class="ltx_p" id="S2.T2.1.4.3.1.1">400 000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.4.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.4.1">
<span class="ltx_p" id="S2.T2.1.4.4.1.1">4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.4.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.5.1">
<span class="ltx_p" id="S2.T2.1.4.5.1.1">30</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.4.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.6.1">
<span class="ltx_p" id="S2.T2.1.4.6.1.1">Consists of 9 different scenarios recorded with 1 to 4 people in the scene. Ground-truth was captured with 14 Vicon MoCap cameras.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.5.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.1.1">
<span class="ltx_p" id="S2.T2.1.5.1.1.1">KTH Multiview Football I <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib46" title=""><span class="ltx_text ltx_font_typewriter">46</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.5.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.2.1">
<span class="ltx_p" id="S2.T2.1.5.2.1.1">2012</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.5.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.3.1">
<span class="ltx_p" id="S2.T2.1.5.3.1.1">257</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.5.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.1">
<span class="ltx_p" id="S2.T2.1.5.4.1.1">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.5.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.5.1">
<span class="ltx_p" id="S2.T2.1.5.5.1.1">2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.5.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.6.1">
<span class="ltx_p" id="S2.T2.1.5.6.1.1">Professional football players during a match of the Allsvenskan league.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.6.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.1.1">
<span class="ltx_p" id="S2.T2.1.6.1.1.1">KTH Multiview Football II <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib45" title=""><span class="ltx_text ltx_font_typewriter">45</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.6.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.2.1">
<span class="ltx_p" id="S2.T2.1.6.2.1.1">2013</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.6.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.3.1">
<span class="ltx_p" id="S2.T2.1.6.3.1.1">800</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.6.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.4.1">
<span class="ltx_p" id="S2.T2.1.6.4.1.1">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.6.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.1">
<span class="ltx_p" id="S2.T2.1.6.5.1.1">2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.6.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.6.1">
<span class="ltx_p" id="S2.T2.1.6.6.1.1">Extended version of KTH Multiview Football I.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.7.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.1.1">
<span class="ltx_p" id="S2.T2.1.7.1.1.1">Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib39" title=""><span class="ltx_text ltx_font_typewriter">39</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.7.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.2.1">
<span class="ltx_p" id="S2.T2.1.7.2.1.1">2014</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.7.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.3.1">
<span class="ltx_p" id="S2.T2.1.7.3.1.1">3 600 000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.7.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.4.1">
<span class="ltx_p" id="S2.T2.1.7.4.1.1">4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.7.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.5.1">
<span class="ltx_p" id="S2.T2.1.7.5.1.1">11</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.7.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.1">
<span class="ltx_p" id="S2.T2.1.7.6.1.1">Only single-person scenarios realizing one of the 17 pre-defined actions. People have IMU sensors attached for a better annotation of the keypoints.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.8.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.1.1">
<span class="ltx_p" id="S2.T2.1.8.1.1.1">Campus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.8.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.2.1">
<span class="ltx_p" id="S2.T2.1.8.2.1.1">2014</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.8.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.3.1">
<span class="ltx_p" id="S2.T2.1.8.3.1.1">2 000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.8.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.4.1">
<span class="ltx_p" id="S2.T2.1.8.4.1.1">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.8.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.5.1">
<span class="ltx_p" id="S2.T2.1.8.5.1.1">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.8.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.6.1">
<span class="ltx_p" id="S2.T2.1.8.6.1.1">Multiple people interacting in an outdoor environment.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.9.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.9.1.1">
<span class="ltx_p" id="S2.T2.1.9.1.1.1">Shelf <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.9.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.9.2.1">
<span class="ltx_p" id="S2.T2.1.9.2.1.1">2014</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.9.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.9.3.1">
<span class="ltx_p" id="S2.T2.1.9.3.1.1">3 200</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.9.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.9.4.1">
<span class="ltx_p" id="S2.T2.1.9.4.1.1">5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.9.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.9.5.1">
<span class="ltx_p" id="S2.T2.1.9.5.1.1">4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.9.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.9.6.1">
<span class="ltx_p" id="S2.T2.1.9.6.1.1">Multiple people in a room disassembling a shelf.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.10.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.10.1.1">
<span class="ltx_p" id="S2.T2.1.10.1.1.1">CMU Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib43" title=""><span class="ltx_text ltx_font_typewriter">43</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.10.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.10.2.1">
<span class="ltx_p" id="S2.T2.1.10.2.1.1">2015</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.10.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.10.3.1">
<span class="ltx_p" id="S2.T2.1.10.3.1.1">1 500 000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.10.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.10.4.1">
<span class="ltx_p" id="S2.T2.1.10.4.1.1">521</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.10.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.10.5.1">
<span class="ltx_p" id="S2.T2.1.10.5.1.1">1-8 per frame</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.10.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.10.6.1">
<span class="ltx_p" id="S2.T2.1.10.6.1.1">Contains scenes with a single person doing a set of movements and scenarios with multiple people engaging in social activities. (521 cameras: 480 VGA camera; 31 HD cameras and 10 Kinect II Sensors).</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.11">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.11.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.11.1.1">
<span class="ltx_p" id="S2.T2.1.11.1.1.1">MPII Cooking 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib69" title=""><span class="ltx_text ltx_font_typewriter">69</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.11.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.11.2.1">
<span class="ltx_p" id="S2.T2.1.11.2.1.1">2015</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.11.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.11.3.1">
<span class="ltx_p" id="S2.T2.1.11.3.1.1">2 881 616</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.11.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.11.4.1">
<span class="ltx_p" id="S2.T2.1.11.4.1.1">8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.11.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.11.5.1">
<span class="ltx_p" id="S2.T2.1.11.5.1.1">30</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.11.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.11.6.1">
<span class="ltx_p" id="S2.T2.1.11.6.1.1">Single-person preparing several dishes.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.12">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.12.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.12.1.1">
<span class="ltx_p" id="S2.T2.1.12.1.1.1">NTU RGB+D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib72" title=""><span class="ltx_text ltx_font_typewriter">72</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.12.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.12.2.1">
<span class="ltx_p" id="S2.T2.1.12.2.1.1">2016</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.12.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.12.3.1">
<span class="ltx_p" id="S2.T2.1.12.3.1.1">4 000 000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.12.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.12.4.1">
<span class="ltx_p" id="S2.T2.1.12.4.1.1">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.12.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.12.5.1">
<span class="ltx_p" id="S2.T2.1.12.5.1.1">40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.12.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.12.6.1">
<span class="ltx_p" id="S2.T2.1.12.6.1.1">There are 60 different actions; some were performed with just one person in the scene and some required a multi-person setting. It was collected depth maps, RGB images and videos, 3D joint information, and infrared sequences.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.13">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.13.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.13.1.1">
<span class="ltx_p" id="S2.T2.1.13.1.1.1">MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib59" title=""><span class="ltx_text ltx_font_typewriter">59</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.13.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.13.2.1">
<span class="ltx_p" id="S2.T2.1.13.2.1.1">2017</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.13.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.13.3.1">
<span class="ltx_p" id="S2.T2.1.13.3.1.1">&gt;1 300 000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.13.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.13.4.1">
<span class="ltx_p" id="S2.T2.1.13.4.1.1">14</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.13.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.13.5.1">
<span class="ltx_p" id="S2.T2.1.13.5.1.1">8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.13.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.13.6.1">
<span class="ltx_p" id="S2.T2.1.13.6.1.1">Single-person performing one of the 8 pre-defined activities.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.14">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.14.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.14.1.1">
<span class="ltx_p" id="S2.T2.1.14.1.1.1">Total Capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib80" title=""><span class="ltx_text ltx_font_typewriter">80</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.14.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.14.2.1">
<span class="ltx_p" id="S2.T2.1.14.2.1.1">2017</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.14.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.14.3.1">
<span class="ltx_p" id="S2.T2.1.14.3.1.1">1 892 176</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.14.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.14.4.1">
<span class="ltx_p" id="S2.T2.1.14.4.1.1">8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.14.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.14.5.1">
<span class="ltx_p" id="S2.T2.1.14.5.1.1">5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.14.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.14.6.1">
<span class="ltx_p" id="S2.T2.1.14.6.1.1">Single-person either walking, acting, doing freestyle movements or a range of motion sequences. All frames have a multi-view video, IMU and Vicon labelling.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.15">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.15.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.15.1.1">
<span class="ltx_p" id="S2.T2.1.15.1.1.1">PKU-MMD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib19" title="">19</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.15.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.15.2.1">
<span class="ltx_p" id="S2.T2.1.15.2.1.1">2017</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.15.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.15.3.1">
<span class="ltx_p" id="S2.T2.1.15.3.1.1">5 312 580</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.15.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.15.4.1">
<span class="ltx_p" id="S2.T2.1.15.4.1.1">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.15.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.15.5.1">
<span class="ltx_p" id="S2.T2.1.15.5.1.1">66</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.15.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.15.6.1">
<span class="ltx_p" id="S2.T2.1.15.6.1.1">The subjects performed various actions by themselves and in group. Contains depth maps, RGB images, skeleton joints, infrared sequences, and RGB videos.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.16">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.16.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.16.1.1">
<span class="ltx_p" id="S2.T2.1.16.1.1.1">WILDTRACK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib13" title="">13</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.16.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.16.2.1">
<span class="ltx_p" id="S2.T2.1.16.2.1.1">2018</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.16.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.16.3.1">
<span class="ltx_p" id="S2.T2.1.16.3.1.1">36 000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.16.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.16.4.1">
<span class="ltx_p" id="S2.T2.1.16.4.1.1">7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.16.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.16.5.1">
<span class="ltx_p" id="S2.T2.1.16.5.1.1">313</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.16.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.16.6.1">
<span class="ltx_p" id="S2.T2.1.16.6.1.1">Data captured in the street in front of the ETH Zurich University’s main building.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.17">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.17.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.17.1.1">
<span class="ltx_p" id="S2.T2.1.17.1.1.1"><span class="ltx_glossaryref" title="">Multi-View Operating Room (MVOR)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib78" title=""><span class="ltx_text ltx_font_typewriter">78</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.17.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.17.2.1">
<span class="ltx_p" id="S2.T2.1.17.2.1.1">2018</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.17.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.17.3.1">
<span class="ltx_p" id="S2.T2.1.17.3.1.1">732</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.17.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.17.4.1">
<span class="ltx_p" id="S2.T2.1.17.4.1.1">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.17.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.17.5.1">
<span class="ltx_p" id="S2.T2.1.17.5.1.1">10</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T2.1.17.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.17.6.1">
<span class="ltx_p" id="S2.T2.1.17.6.1.1">Consists of images recorded during 4 days in an interventional room at the University Hospital of Strasbourg.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.18">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" id="S2.T2.1.18.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.18.1.1">
<span class="ltx_p" id="S2.T2.1.18.1.1.1">NTU RGB+D 120 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib52" title=""><span class="ltx_text ltx_font_typewriter">52</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" id="S2.T2.1.18.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.18.2.1">
<span class="ltx_p" id="S2.T2.1.18.2.1.1">2019</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" id="S2.T2.1.18.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.18.3.1">
<span class="ltx_p" id="S2.T2.1.18.3.1.1">8 000 000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" id="S2.T2.1.18.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.18.4.1">
<span class="ltx_p" id="S2.T2.1.18.4.1.1">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" id="S2.T2.1.18.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.18.5.1">
<span class="ltx_p" id="S2.T2.1.18.5.1.1">106</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" id="S2.T2.1.18.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.18.6.1">
<span class="ltx_p" id="S2.T2.1.18.6.1.1">It is an extension of NTU RGB+D and it has 60 more different actions, making a total of 120 actions.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S2.2">
<table class="ltx_tabular ltx_align_middle" id="S2.2.2">
<tr class="ltx_tr" id="S2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.1.1.1.2" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.1.1.1.2.1">
<span class="ltx_p" id="S2.1.1.1.2.1.1">How2Sign <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib25" title=""><span class="ltx_text ltx_font_typewriter">25</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.1.1.1.3" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.1.1.1.3.1">
<span class="ltx_p" id="S2.1.1.1.3.1.1">2021</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.1.1.1.1" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.1.1.1.1.1">
<span class="ltx_p" id="S2.1.1.1.1.1.1"><math alttext="\sim" class="ltx_Math" display="inline" id="S2.1.1.1.1.1.1.m1.1"><semantics id="S2.1.1.1.1.1.1.m1.1a"><mo id="S2.1.1.1.1.1.1.m1.1.1" xref="S2.1.1.1.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.1.1.1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.1.1.1.1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.1.1.1.1.1.1.m1.1d">∼</annotation></semantics></math>7 503 218</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.1.1.1.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.1.1.1.4.1">
<span class="ltx_p" id="S2.1.1.1.4.1.1">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.1.1.1.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.1.1.1.5.1">
<span class="ltx_p" id="S2.1.1.1.5.1.1">11</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.1.1.1.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.1.1.1.6.1">
<span class="ltx_p" id="S2.1.1.1.6.1.1">Consists of people doing sign language. Besides images, it also, contains speech, English transcripts, gloss, pose information and depth information.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.2.2.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.2.2.3.1" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.3.1.1">
<span class="ltx_p" id="S2.2.2.3.1.1.1">HuMoMM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib97" title=""><span class="ltx_text ltx_font_typewriter">97</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.2.2.3.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.3.2.1">
<span class="ltx_p" id="S2.2.2.3.2.1.1">2023</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.2.2.3.3" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.3.3.1">
<span class="ltx_p" id="S2.2.2.3.3.1.1">262 000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.2.2.3.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.3.4.1">
<span class="ltx_p" id="S2.2.2.3.4.1.1">5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.2.2.3.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.3.5.1">
<span class="ltx_p" id="S2.2.2.3.5.1.1">18</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.2.2.3.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.3.6.1">
<span class="ltx_p" id="S2.2.2.3.6.1.1">Includes 30 different actions: 20 performed individually and 10 in group. Contains RGB images and depth images and also, provides multi-modal annotations that comprehend 2D and 3D keypoints, SMPL parameters and action categories.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S2.2.2.2.2" style="width:73.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.2.2.1">
<span class="ltx_p" id="S2.2.2.2.2.1.1">PKUInfantV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib95" title=""><span class="ltx_text ltx_font_typewriter">95</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S2.2.2.2.3" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.2.3.1">
<span class="ltx_p" id="S2.2.2.2.3.1.1">2024</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S2.2.2.2.1" style="width:45.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.2.1.1">
<span class="ltx_p" id="S2.2.2.2.1.1.1"><math alttext="\sim" class="ltx_Math" display="inline" id="S2.2.2.2.1.1.1.m1.1"><semantics id="S2.2.2.2.1.1.1.m1.1a"><mo id="S2.2.2.2.1.1.1.m1.1.1" xref="S2.2.2.2.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.2.2.2.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.2.2.2.1.1.1.m1.1.1.cmml" xref="S2.2.2.2.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.2.2.2.1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.2.2.2.1.1.1.m1.1d">∼</annotation></semantics></math>2 000 000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S2.2.2.2.4" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.2.4.1">
<span class="ltx_p" id="S2.2.2.2.4.1.1">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S2.2.2.2.5" style="width:39.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.2.5.1">
<span class="ltx_p" id="S2.2.2.2.5.1.1">170</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S2.2.2.2.6" style="width:210.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.2.2.2.6.1">
<span class="ltx_p" id="S2.2.2.2.6.1.1">It comprises 510 videos recorded in a hospital setting, and the movements of the infant were categorized as either normal or abnormal writhing motions by a physician. This dataset led to the creation of two other datasets: one consisting of 15 924 annotated images obtained from 420 videos in the PKUInfantV dataset, and the other being a downsampled version featuring solely the segments of the videos in which the infant exhibited movement.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation Metrics</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section presents the evaluation metrics specific to human pose estimation. The most widely used metrics to evaluate the quality of the 3D pose estimations are <span class="ltx_glossaryref" title="">Percentage of Correct Parts (PCP)</span>, <span class="ltx_glossaryref" title="">Mean Per Joint Position Error (MPJPE)</span>, <span class="ltx_glossaryref" title="">Percentage of Correct Keypoints (PCK)</span>, or <span class="ltx_glossaryref" title="">Average Precision (AP)</span>. Nonetheless, other less frequently used metrics like <span class="ltx_glossaryref" title="">Mean Average Precision (mAP)</span>, <span class="ltx_glossaryref" title="">Area Under the Curve (AUC)</span>, <span class="ltx_glossaryref" title="">Mean Per Joint Angle Error (MPJAE)</span> and <span class="ltx_glossaryref" title="">Mean of the Root Position Error (MRPE)</span> are also introduced.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_glossaryref ltx_font_bold" title="">PCP</span> determines the limb detection accuracy. A limb is considered correct if the distance between the predicted limb ends and the ground-truth limb joint locations is less or equal to a certain value <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_α</annotation></semantics></math> of the limb’s length, (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S3.E1" title="In 3 Evaluation Metrics ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\frac{\left|\left|s_{p}-s_{p}^{{}^{\prime}}\right|\right|+\left|\left|e_{p}-e_%
{p}^{{}^{\prime}}\right|\right|}{2}\leq\alpha\left|\left|s_{p}-e_{p}\right|\right|" class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml">s</mi><mi id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">p</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.2.cmml">s</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.3.2.3.cmml">p</mi><msup id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.3a" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml"></mi><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.3.3.1.cmml">′</mo></msup></msubsup></mrow><mo id="S3.E1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">+</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.cmml"><mo id="S3.E1.m1.2.2.2.2.1.2" stretchy="false" xref="S3.E1.m1.2.2.2.2.2.1.cmml">‖</mo><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.2.2" xref="S3.E1.m1.2.2.2.2.1.1.2.2.cmml">e</mi><mi id="S3.E1.m1.2.2.2.2.1.1.2.3" xref="S3.E1.m1.2.2.2.2.1.1.2.3.cmml">p</mi></msub><mo id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">−</mo><msubsup id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.3.2.2" xref="S3.E1.m1.2.2.2.2.1.1.3.2.2.cmml">e</mi><mi id="S3.E1.m1.2.2.2.2.1.1.3.2.3" xref="S3.E1.m1.2.2.2.2.1.1.3.2.3.cmml">p</mi><msup id="S3.E1.m1.2.2.2.2.1.1.3.3" xref="S3.E1.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.3.3a" xref="S3.E1.m1.2.2.2.2.1.1.3.3.cmml"></mi><mo id="S3.E1.m1.2.2.2.2.1.1.3.3.1" xref="S3.E1.m1.2.2.2.2.1.1.3.3.1.cmml">′</mo></msup></msubsup></mrow><mo id="S3.E1.m1.2.2.2.2.1.3" stretchy="false" xref="S3.E1.m1.2.2.2.2.2.1.cmml">‖</mo></mrow></mrow><mn id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml">2</mn></mfrac><mo id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.2.cmml">≤</mo><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><mi id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.3.3.1.3.cmml">α</mi><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.1.cmml">‖</mo><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2.2" xref="S3.E1.m1.3.3.1.1.1.1.2.2.cmml">s</mi><mi id="S3.E1.m1.3.3.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.1.1.2.3.cmml">p</mi></msub><mo id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">−</mo><msub id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.3.2.cmml">e</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.3.3.cmml">p</mi></msub></mrow><mo id="S3.E1.m1.3.3.1.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.1.cmml">‖</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><leq id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2"></leq><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2"></divide><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><plus id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></plus><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">𝑠</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3">𝑝</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2.2">𝑠</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2.3">𝑝</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3"><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.1">′</ci></apply></apply></apply></apply><apply id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.2">norm</csymbol><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><minus id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1"></minus><apply id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.2">𝑒</ci><ci id="S3.E1.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.3">𝑝</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">superscript</csymbol><apply id="S3.E1.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.2.2">𝑒</ci><ci id="S3.E1.m1.2.2.2.2.1.1.3.2.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.2.3">𝑝</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3"><ci id="S3.E1.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3.1">′</ci></apply></apply></apply></apply></apply><cn id="S3.E1.m1.2.2.4.cmml" type="integer" xref="S3.E1.m1.2.2.4">2</cn></apply><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><times id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></times><ci id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3">𝛼</ci><apply id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><minus id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"></minus><apply id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2">𝑠</ci><ci id="S3.E1.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.3">𝑝</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2">𝑒</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.3">𝑝</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\frac{\left|\left|s_{p}-s_{p}^{{}^{\prime}}\right|\right|+\left|\left|e_{p}-e_%
{p}^{{}^{\prime}}\right|\right|}{2}\leq\alpha\left|\left|s_{p}-e_{p}\right|\right|</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">divide start_ARG | | italic_s start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT - italic_s start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT | | + | | italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT - italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT | | end_ARG start_ARG 2 end_ARG ≤ italic_α | | italic_s start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT - italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT | |</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p2.9"><math alttext="s_{p}" class="ltx_Math" display="inline" id="S3.p2.2.m1.1"><semantics id="S3.p2.2.m1.1a"><msub id="S3.p2.2.m1.1.1" xref="S3.p2.2.m1.1.1.cmml"><mi id="S3.p2.2.m1.1.1.2" xref="S3.p2.2.m1.1.1.2.cmml">s</mi><mi id="S3.p2.2.m1.1.1.3" xref="S3.p2.2.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m1.1b"><apply id="S3.p2.2.m1.1.1.cmml" xref="S3.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m1.1.1.1.cmml" xref="S3.p2.2.m1.1.1">subscript</csymbol><ci id="S3.p2.2.m1.1.1.2.cmml" xref="S3.p2.2.m1.1.1.2">𝑠</ci><ci id="S3.p2.2.m1.1.1.3.cmml" xref="S3.p2.2.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m1.1c">s_{p}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m1.1d">italic_s start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="e_{p}" class="ltx_Math" display="inline" id="S3.p2.3.m2.1"><semantics id="S3.p2.3.m2.1a"><msub id="S3.p2.3.m2.1.1" xref="S3.p2.3.m2.1.1.cmml"><mi id="S3.p2.3.m2.1.1.2" xref="S3.p2.3.m2.1.1.2.cmml">e</mi><mi id="S3.p2.3.m2.1.1.3" xref="S3.p2.3.m2.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.3.m2.1b"><apply id="S3.p2.3.m2.1.1.cmml" xref="S3.p2.3.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m2.1.1.1.cmml" xref="S3.p2.3.m2.1.1">subscript</csymbol><ci id="S3.p2.3.m2.1.1.2.cmml" xref="S3.p2.3.m2.1.1.2">𝑒</ci><ci id="S3.p2.3.m2.1.1.3.cmml" xref="S3.p2.3.m2.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m2.1c">e_{p}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m2.1d">italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> are the 3D ground-truth coordinates of the beginning and ending points of the body part <math alttext="p" class="ltx_Math" display="inline" id="S3.p2.4.m3.1"><semantics id="S3.p2.4.m3.1a"><mi id="S3.p2.4.m3.1.1" xref="S3.p2.4.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m3.1b"><ci id="S3.p2.4.m3.1.1.cmml" xref="S3.p2.4.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m3.1d">italic_p</annotation></semantics></math>.
<br class="ltx_break"/><math alttext="s_{p}^{{}^{\prime}}" class="ltx_Math" display="inline" id="S3.p2.5.m4.1"><semantics id="S3.p2.5.m4.1a"><msubsup id="S3.p2.5.m4.1.1" xref="S3.p2.5.m4.1.1.cmml"><mi id="S3.p2.5.m4.1.1.2.2" xref="S3.p2.5.m4.1.1.2.2.cmml">s</mi><mi id="S3.p2.5.m4.1.1.2.3" xref="S3.p2.5.m4.1.1.2.3.cmml">p</mi><msup id="S3.p2.5.m4.1.1.3" xref="S3.p2.5.m4.1.1.3.cmml"><mi id="S3.p2.5.m4.1.1.3a" xref="S3.p2.5.m4.1.1.3.cmml"></mi><mo id="S3.p2.5.m4.1.1.3.1" xref="S3.p2.5.m4.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.5.m4.1b"><apply id="S3.p2.5.m4.1.1.cmml" xref="S3.p2.5.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m4.1.1.1.cmml" xref="S3.p2.5.m4.1.1">superscript</csymbol><apply id="S3.p2.5.m4.1.1.2.cmml" xref="S3.p2.5.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m4.1.1.2.1.cmml" xref="S3.p2.5.m4.1.1">subscript</csymbol><ci id="S3.p2.5.m4.1.1.2.2.cmml" xref="S3.p2.5.m4.1.1.2.2">𝑠</ci><ci id="S3.p2.5.m4.1.1.2.3.cmml" xref="S3.p2.5.m4.1.1.2.3">𝑝</ci></apply><apply id="S3.p2.5.m4.1.1.3.cmml" xref="S3.p2.5.m4.1.1.3"><ci id="S3.p2.5.m4.1.1.3.1.cmml" xref="S3.p2.5.m4.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m4.1c">s_{p}^{{}^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m4.1d">italic_s start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="e_{p}^{{}^{\prime}}" class="ltx_Math" display="inline" id="S3.p2.6.m5.1"><semantics id="S3.p2.6.m5.1a"><msubsup id="S3.p2.6.m5.1.1" xref="S3.p2.6.m5.1.1.cmml"><mi id="S3.p2.6.m5.1.1.2.2" xref="S3.p2.6.m5.1.1.2.2.cmml">e</mi><mi id="S3.p2.6.m5.1.1.2.3" xref="S3.p2.6.m5.1.1.2.3.cmml">p</mi><msup id="S3.p2.6.m5.1.1.3" xref="S3.p2.6.m5.1.1.3.cmml"><mi id="S3.p2.6.m5.1.1.3a" xref="S3.p2.6.m5.1.1.3.cmml"></mi><mo id="S3.p2.6.m5.1.1.3.1" xref="S3.p2.6.m5.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.6.m5.1b"><apply id="S3.p2.6.m5.1.1.cmml" xref="S3.p2.6.m5.1.1"><csymbol cd="ambiguous" id="S3.p2.6.m5.1.1.1.cmml" xref="S3.p2.6.m5.1.1">superscript</csymbol><apply id="S3.p2.6.m5.1.1.2.cmml" xref="S3.p2.6.m5.1.1"><csymbol cd="ambiguous" id="S3.p2.6.m5.1.1.2.1.cmml" xref="S3.p2.6.m5.1.1">subscript</csymbol><ci id="S3.p2.6.m5.1.1.2.2.cmml" xref="S3.p2.6.m5.1.1.2.2">𝑒</ci><ci id="S3.p2.6.m5.1.1.2.3.cmml" xref="S3.p2.6.m5.1.1.2.3">𝑝</ci></apply><apply id="S3.p2.6.m5.1.1.3.cmml" xref="S3.p2.6.m5.1.1.3"><ci id="S3.p2.6.m5.1.1.3.1.cmml" xref="S3.p2.6.m5.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m5.1c">e_{p}^{{}^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.6.m5.1d">italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> are the corresponding estimated coordinates of the beginning and ending points of the body part <math alttext="p" class="ltx_Math" display="inline" id="S3.p2.7.m6.1"><semantics id="S3.p2.7.m6.1a"><mi id="S3.p2.7.m6.1.1" xref="S3.p2.7.m6.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.p2.7.m6.1b"><ci id="S3.p2.7.m6.1.1.cmml" xref="S3.p2.7.m6.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m6.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.p2.7.m6.1d">italic_p</annotation></semantics></math>.
<br class="ltx_break"/><math alttext="\alpha" class="ltx_Math" display="inline" id="S3.p2.8.m7.1"><semantics id="S3.p2.8.m7.1a"><mi id="S3.p2.8.m7.1.1" xref="S3.p2.8.m7.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.p2.8.m7.1b"><ci id="S3.p2.8.m7.1.1.cmml" xref="S3.p2.8.m7.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m7.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.p2.8.m7.1d">italic_α</annotation></semantics></math> is the threshold, normally defined as <math alttext="0.5" class="ltx_Math" display="inline" id="S3.p2.9.m8.1"><semantics id="S3.p2.9.m8.1a"><mn id="S3.p2.9.m8.1.1" xref="S3.p2.9.m8.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.p2.9.m8.1b"><cn id="S3.p2.9.m8.1.1.cmml" type="float" xref="S3.p2.9.m8.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.9.m8.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S3.p2.9.m8.1d">0.5</annotation></semantics></math>.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.5"><span class="ltx_glossaryref ltx_font_bold" title="">MPJPE</span>, also known as 3D error, corresponds to the average Euclidean distance between the estimated joints and the respective true joint location, (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S3.E2" title="In 3 Evaluation Metrics ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\text{MPJPE }(S)=\frac{1}{N_{S}}\sum_{i=1}^{N_{S}}\left|\left|G_{i}-P_{i}%
\right|\right|_{2}" class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mrow id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml"><mtext id="S3.E2.m1.2.2.3.2" xref="S3.E2.m1.2.2.3.2a.cmml">MPJPE </mtext><mo id="S3.E2.m1.2.2.3.1" xref="S3.E2.m1.2.2.3.1.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.3.3.2" xref="S3.E2.m1.2.2.3.cmml"><mo id="S3.E2.m1.2.2.3.3.2.1" stretchy="false" xref="S3.E2.m1.2.2.3.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">S</mi><mo id="S3.E2.m1.2.2.3.3.2.2" stretchy="false" xref="S3.E2.m1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mfrac id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml"><mn id="S3.E2.m1.2.2.1.3.2" xref="S3.E2.m1.2.2.1.3.2.cmml">1</mn><msub id="S3.E2.m1.2.2.1.3.3" xref="S3.E2.m1.2.2.1.3.3.cmml"><mi id="S3.E2.m1.2.2.1.3.3.2" xref="S3.E2.m1.2.2.1.3.3.2.cmml">N</mi><mi id="S3.E2.m1.2.2.1.3.3.3" xref="S3.E2.m1.2.2.1.3.3.3.cmml">S</mi></msub></mfrac><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><munderover id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S3.E2.m1.2.2.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.2.2.1.1.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2.3.2" xref="S3.E2.m1.2.2.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E2.m1.2.2.1.1.2.2.3.1" xref="S3.E2.m1.2.2.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.2.2.1.1.2.2.3.3" xref="S3.E2.m1.2.2.1.1.2.2.3.3.cmml">1</mn></mrow><msub id="S3.E2.m1.2.2.1.1.2.3" xref="S3.E2.m1.2.2.1.1.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.2.3.2" xref="S3.E2.m1.2.2.1.1.2.3.2.cmml">N</mi><mi id="S3.E2.m1.2.2.1.1.2.3.3" xref="S3.E2.m1.2.2.1.1.2.3.3.cmml">S</mi></msub></munderover><msub id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.cmml">G</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.cmml">P</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"></eq><apply id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"><times id="S3.E2.m1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.3.1"></times><ci id="S3.E2.m1.2.2.3.2a.cmml" xref="S3.E2.m1.2.2.3.2"><mtext id="S3.E2.m1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.3.2">MPJPE </mtext></ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑆</ci></apply><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><times id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></times><apply id="S3.E2.m1.2.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3"><divide id="S3.E2.m1.2.2.1.3.1.cmml" xref="S3.E2.m1.2.2.1.3"></divide><cn id="S3.E2.m1.2.2.1.3.2.cmml" type="integer" xref="S3.E2.m1.2.2.1.3.2">1</cn><apply id="S3.E2.m1.2.2.1.3.3.cmml" xref="S3.E2.m1.2.2.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.3.3.1.cmml" xref="S3.E2.m1.2.2.1.3.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.3.3.2.cmml" xref="S3.E2.m1.2.2.1.3.3.2">𝑁</ci><ci id="S3.E2.m1.2.2.1.3.3.3.cmml" xref="S3.E2.m1.2.2.1.3.3.3">𝑆</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1"><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2"></sum><apply id="S3.E2.m1.2.2.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.3"><eq id="S3.E2.m1.2.2.1.1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.2.2.1.1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.3.2">𝑖</ci><cn id="S3.E2.m1.2.2.1.1.2.2.3.3.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.2.2.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.2.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.2.3.2">𝑁</ci><ci id="S3.E2.m1.2.2.1.1.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3.3">𝑆</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2">𝐺</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.2">𝑃</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><cn id="S3.E2.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\text{MPJPE }(S)=\frac{1}{N_{S}}\sum_{i=1}^{N_{S}}\left|\left|G_{i}-P_{i}%
\right|\right|_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">MPJPE ( italic_S ) = divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUPERSCRIPT | | italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p3.4"><math alttext="G_{i}" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><msub id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">G</mi><mi id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">𝐺</ci><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">G_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the ground-truth position of the i-the joint.
<br class="ltx_break"/><math alttext="P_{i}" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><msub id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">P</mi><mi id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">𝑃</ci><ci id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">P_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the predicted position of the i-the joint.
<br class="ltx_break"/><math alttext="N_{S}" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><msub id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">N</mi><mi id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">𝑁</ci><ci id="S3.p3.3.m3.1.1.3.cmml" xref="S3.p3.3.m3.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">N_{S}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> is the number of joints of the <math alttext="S" class="ltx_Math" display="inline" id="S3.p3.4.m4.1"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">italic_S</annotation></semantics></math> skeleton.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Some researchers also report results using a variant of <span class="ltx_glossaryref" title="">MPJPE</span>, <span class="ltx_glossaryref" title="">Procrustes Alignment MPJPE (PA-MPJPE)</span>, which uses Procrustes alignment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib32" title=""><span class="ltx_text ltx_font_typewriter">32</span></a>]</cite> on the ground-truth and estimate joints before calculating MPJPE.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.6"><span class="ltx_glossaryref ltx_font_bold" title="">PCK</span>
consists in the % of points in which the distance between the estimated and the real value is inferior to a defined threshold, (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S3.E3" title="In 3 Evaluation Metrics ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="PCK_{k_{i}}=\frac{1}{N}\sum_{i=1}^{N}\delta\left(\left|\left|P_{k}^{i}-G_{k}^{%
i}\right|\right|_{2}^{2}\leq t_{j}\right)" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">P</mi><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">C</mi><mo id="S3.E3.m1.1.1.3.1a" xref="S3.E3.m1.1.1.3.1.cmml">⁢</mo><msub id="S3.E3.m1.1.1.3.4" xref="S3.E3.m1.1.1.3.4.cmml"><mi id="S3.E3.m1.1.1.3.4.2" xref="S3.E3.m1.1.1.3.4.2.cmml">K</mi><msub id="S3.E3.m1.1.1.3.4.3" xref="S3.E3.m1.1.1.3.4.3.cmml"><mi id="S3.E3.m1.1.1.3.4.3.2" xref="S3.E3.m1.1.1.3.4.3.2.cmml">k</mi><mi id="S3.E3.m1.1.1.3.4.3.3" xref="S3.E3.m1.1.1.3.4.3.3.cmml">i</mi></msub></msub></mrow><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mfrac id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><mn id="S3.E3.m1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.3.2.cmml">1</mn><mi id="S3.E3.m1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.3.3.cmml">N</mi></mfrac><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><munderover id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E3.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E3.m1.1.1.1.1.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.3.2" xref="S3.E3.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E3.m1.1.1.1.1.2.2.3.1" xref="S3.E3.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E3.m1.1.1.1.1.2.2.3.3" xref="S3.E3.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">δ</mi><mo id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">P</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">k</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">G</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">k</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msubsup></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml">2</mn></msubsup><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml">≤</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><times id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></times><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝑃</ci><ci id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">𝐶</ci><apply id="S3.E3.m1.1.1.3.4.cmml" xref="S3.E3.m1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.4.1.cmml" xref="S3.E3.m1.1.1.3.4">subscript</csymbol><ci id="S3.E3.m1.1.1.3.4.2.cmml" xref="S3.E3.m1.1.1.3.4.2">𝐾</ci><apply id="S3.E3.m1.1.1.3.4.3.cmml" xref="S3.E3.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.4.3.1.cmml" xref="S3.E3.m1.1.1.3.4.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.4.3.2.cmml" xref="S3.E3.m1.1.1.3.4.3.2">𝑘</ci><ci id="S3.E3.m1.1.1.3.4.3.3.cmml" xref="S3.E3.m1.1.1.3.4.3.3">𝑖</ci></apply></apply></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><divide id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3"></divide><cn id="S3.E3.m1.1.1.1.3.2.cmml" type="integer" xref="S3.E3.m1.1.1.1.3.2">1</cn><ci id="S3.E3.m1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E3.m1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E3.m1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3"><eq id="S3.E3.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E3.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.E3.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">𝛿</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><leq id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2"></leq><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑃</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑘</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2">𝐺</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3">𝑘</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><cn id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3">2</cn></apply><cn id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3">2</cn></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2">𝑡</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">PCK_{k_{i}}=\frac{1}{N}\sum_{i=1}^{N}\delta\left(\left|\left|P_{k}^{i}-G_{k}^{%
i}\right|\right|_{2}^{2}\leq t_{j}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_P italic_C italic_K start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_δ ( | | italic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT - italic_G start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p5.5"><math alttext="PCK_{k_{i}}" class="ltx_Math" display="inline" id="S3.p5.1.m1.1"><semantics id="S3.p5.1.m1.1a"><mrow id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mi id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">P</mi><mo id="S3.p5.1.m1.1.1.1" xref="S3.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">C</mi><mo id="S3.p5.1.m1.1.1.1a" xref="S3.p5.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.p5.1.m1.1.1.4" xref="S3.p5.1.m1.1.1.4.cmml"><mi id="S3.p5.1.m1.1.1.4.2" xref="S3.p5.1.m1.1.1.4.2.cmml">K</mi><msub id="S3.p5.1.m1.1.1.4.3" xref="S3.p5.1.m1.1.1.4.3.cmml"><mi id="S3.p5.1.m1.1.1.4.3.2" xref="S3.p5.1.m1.1.1.4.3.2.cmml">k</mi><mi id="S3.p5.1.m1.1.1.4.3.3" xref="S3.p5.1.m1.1.1.4.3.3.cmml">i</mi></msub></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><times id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1.1"></times><ci id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.2">𝑃</ci><ci id="S3.p5.1.m1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.3">𝐶</ci><apply id="S3.p5.1.m1.1.1.4.cmml" xref="S3.p5.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.p5.1.m1.1.1.4.1.cmml" xref="S3.p5.1.m1.1.1.4">subscript</csymbol><ci id="S3.p5.1.m1.1.1.4.2.cmml" xref="S3.p5.1.m1.1.1.4.2">𝐾</ci><apply id="S3.p5.1.m1.1.1.4.3.cmml" xref="S3.p5.1.m1.1.1.4.3"><csymbol cd="ambiguous" id="S3.p5.1.m1.1.1.4.3.1.cmml" xref="S3.p5.1.m1.1.1.4.3">subscript</csymbol><ci id="S3.p5.1.m1.1.1.4.3.2.cmml" xref="S3.p5.1.m1.1.1.4.3.2">𝑘</ci><ci id="S3.p5.1.m1.1.1.4.3.3.cmml" xref="S3.p5.1.m1.1.1.4.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">PCK_{k_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.1d">italic_P italic_C italic_K start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> is the <span class="ltx_glossaryref" title="">PCK</span> value of the i-the keypoint of the k-the skeleton.
<br class="ltx_break"/><math alttext="t_{j}" class="ltx_Math" display="inline" id="S3.p5.2.m2.1"><semantics id="S3.p5.2.m2.1a"><msub id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml"><mi id="S3.p5.2.m2.1.1.2" xref="S3.p5.2.m2.1.1.2.cmml">t</mi><mi id="S3.p5.2.m2.1.1.3" xref="S3.p5.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><apply id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p5.2.m2.1.1.1.cmml" xref="S3.p5.2.m2.1.1">subscript</csymbol><ci id="S3.p5.2.m2.1.1.2.cmml" xref="S3.p5.2.m2.1.1.2">𝑡</ci><ci id="S3.p5.2.m2.1.1.3.cmml" xref="S3.p5.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">t_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.2.m2.1d">italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the j-the defined threshold.
<br class="ltx_break"/><math alttext="P_{k}^{i}" class="ltx_Math" display="inline" id="S3.p5.3.m3.1"><semantics id="S3.p5.3.m3.1a"><msubsup id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml"><mi id="S3.p5.3.m3.1.1.2.2" xref="S3.p5.3.m3.1.1.2.2.cmml">P</mi><mi id="S3.p5.3.m3.1.1.2.3" xref="S3.p5.3.m3.1.1.2.3.cmml">k</mi><mi id="S3.p5.3.m3.1.1.3" xref="S3.p5.3.m3.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.1b"><apply id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p5.3.m3.1.1.1.cmml" xref="S3.p5.3.m3.1.1">superscript</csymbol><apply id="S3.p5.3.m3.1.1.2.cmml" xref="S3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p5.3.m3.1.1.2.1.cmml" xref="S3.p5.3.m3.1.1">subscript</csymbol><ci id="S3.p5.3.m3.1.1.2.2.cmml" xref="S3.p5.3.m3.1.1.2.2">𝑃</ci><ci id="S3.p5.3.m3.1.1.2.3.cmml" xref="S3.p5.3.m3.1.1.2.3">𝑘</ci></apply><ci id="S3.p5.3.m3.1.1.3.cmml" xref="S3.p5.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">P_{k}^{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.3.m3.1d">italic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="G_{k}^{i}" class="ltx_Math" display="inline" id="S3.p5.4.m4.1"><semantics id="S3.p5.4.m4.1a"><msubsup id="S3.p5.4.m4.1.1" xref="S3.p5.4.m4.1.1.cmml"><mi id="S3.p5.4.m4.1.1.2.2" xref="S3.p5.4.m4.1.1.2.2.cmml">G</mi><mi id="S3.p5.4.m4.1.1.2.3" xref="S3.p5.4.m4.1.1.2.3.cmml">k</mi><mi id="S3.p5.4.m4.1.1.3" xref="S3.p5.4.m4.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p5.4.m4.1b"><apply id="S3.p5.4.m4.1.1.cmml" xref="S3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p5.4.m4.1.1.1.cmml" xref="S3.p5.4.m4.1.1">superscript</csymbol><apply id="S3.p5.4.m4.1.1.2.cmml" xref="S3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p5.4.m4.1.1.2.1.cmml" xref="S3.p5.4.m4.1.1">subscript</csymbol><ci id="S3.p5.4.m4.1.1.2.2.cmml" xref="S3.p5.4.m4.1.1.2.2">𝐺</ci><ci id="S3.p5.4.m4.1.1.2.3.cmml" xref="S3.p5.4.m4.1.1.2.3">𝑘</ci></apply><ci id="S3.p5.4.m4.1.1.3.cmml" xref="S3.p5.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.4.m4.1c">G_{k}^{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.4.m4.1d">italic_G start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> represent the coordinates of the i-the joint of the k-the predicted skeleton and the ground truth of that joint, respectively.
<br class="ltx_break"/><math alttext="\delta(.)=\begin{cases}1&amp;\text{if $\left|\left|P_{k}^{i}-G_{k}^{i}\right|%
\right|_{2}^{2}\leq t_{j}$ is True}\\
0&amp;\text{otherwise}\end{cases}" class="ltx_math_unparsed" display="inline" id="S3.p5.5.m5.4"><semantics id="S3.p5.5.m5.4a"><mrow id="S3.p5.5.m5.4b"><mi id="S3.p5.5.m5.4.5">δ</mi><mrow id="S3.p5.5.m5.4.6"><mo id="S3.p5.5.m5.4.6.1" stretchy="false">(</mo><mo id="S3.p5.5.m5.4.6.2" lspace="0em" rspace="0.167em">.</mo><mo id="S3.p5.5.m5.4.6.3" stretchy="false">)</mo></mrow><mo id="S3.p5.5.m5.4.7">=</mo><mrow id="S3.p5.5.m5.4.4"><mo id="S3.p5.5.m5.4.4.5">{</mo><mtable columnspacing="5pt" id="S3.p5.5.m5.4.4.4" rowspacing="0pt"><mtr id="S3.p5.5.m5.4.4.4a"><mtd class="ltx_align_left" columnalign="left" id="S3.p5.5.m5.4.4.4b"><mn id="S3.p5.5.m5.2.2.2.2.2.1">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.p5.5.m5.4.4.4c"><mrow id="S3.p5.5.m5.1.1.1.1.1.1"><mtext id="S3.p5.5.m5.1.1.1.1.1.1a">if </mtext><mrow id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1"><msubsup id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1"><mrow id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3"><mo id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3.1" stretchy="false">‖</mo><mrow id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1"><msubsup id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.2"><mi id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.2.2.2">P</mi><mi id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.2.2.3">k</mi><mi id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.2.3">i</mi></msubsup><mo id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.1">−</mo><msubsup id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.3"><mi id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.3.2.2">G</mi><mi id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.3.2.3">k</mi><mi id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.3.3">i</mi></msubsup></mrow><mo id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3.2" stretchy="false">‖</mo></mrow><mn id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.1.3">2</mn><mn id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.1.3">2</mn></msubsup><mo id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.2">≤</mo><msub id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.3"><mi id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.3.2">t</mi><mi id="S3.p5.5.m5.1.1.1.1.1.1.1.1.m1.1.1.3.3">j</mi></msub></mrow><mtext id="S3.p5.5.m5.1.1.1.1.1.1b"> is True</mtext></mrow></mtd></mtr><mtr id="S3.p5.5.m5.4.4.4d"><mtd class="ltx_align_left" columnalign="left" id="S3.p5.5.m5.4.4.4e"><mn id="S3.p5.5.m5.3.3.3.3.1.1">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.p5.5.m5.4.4.4f"><mtext id="S3.p5.5.m5.4.4.4.4.2.1">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex" id="S3.p5.5.m5.4c">\delta(.)=\begin{cases}1&amp;\text{if $\left|\left|P_{k}^{i}-G_{k}^{i}\right|%
\right|_{2}^{2}\leq t_{j}$ is True}\\
0&amp;\text{otherwise}\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.5.m5.4d">italic_δ ( . ) = { start_ROW start_CELL 1 end_CELL start_CELL if | | italic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT - italic_G start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is True end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW</annotation></semantics></math>
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1"><span class="ltx_glossaryref ltx_font_bold" title="">AP</span> is calculated using <span class="ltx_glossaryref" title="">MPJPE</span> (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S3.E2" title="In 3 Evaluation Metrics ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">2</span></a>) as the thresholding metric between the ground truth and the predicted keypoints, (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S3.E4" title="In 3 Evaluation Metrics ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">4</span></a>).
For example, AP@50 refers to the <span class="ltx_glossaryref" title="">AP</span> calculated considering a pose correctly estimated if the <math alttext="\text{\lx@glossaries@gls@link{acronym}{mpjpe}{\leavevmode MPJPE}}&lt;50mm" class="ltx_Math" display="inline" id="S3.p6.1.m1.1"><semantics id="S3.p6.1.m1.1a"><mrow id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml"><mtext id="S3.p6.1.m1.1.1.2" xref="S3.p6.1.m1.1.1.2b.cmml"><span class="ltx_glossaryref" title="">MPJPE</span></mtext><mo id="S3.p6.1.m1.1.1.1" xref="S3.p6.1.m1.1.1.1.cmml">&lt;</mo><mrow id="S3.p6.1.m1.1.1.3" xref="S3.p6.1.m1.1.1.3.cmml"><mn id="S3.p6.1.m1.1.1.3.2" xref="S3.p6.1.m1.1.1.3.2.cmml">50</mn><mo id="S3.p6.1.m1.1.1.3.1" xref="S3.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.p6.1.m1.1.1.3.3" xref="S3.p6.1.m1.1.1.3.3.cmml">m</mi><mo id="S3.p6.1.m1.1.1.3.1a" xref="S3.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.p6.1.m1.1.1.3.4" xref="S3.p6.1.m1.1.1.3.4.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><apply id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1"><lt id="S3.p6.1.m1.1.1.1.cmml" xref="S3.p6.1.m1.1.1.1"></lt><ci id="S3.p6.1.m1.1.1.2b.cmml" xref="S3.p6.1.m1.1.1.2"><mtext id="S3.p6.1.m1.1.1.2.cmml" xref="S3.p6.1.m1.1.1.2"><span class="ltx_glossaryref" title="">MPJPE</span></mtext></ci><apply id="S3.p6.1.m1.1.1.3.cmml" xref="S3.p6.1.m1.1.1.3"><times id="S3.p6.1.m1.1.1.3.1.cmml" xref="S3.p6.1.m1.1.1.3.1"></times><cn id="S3.p6.1.m1.1.1.3.2.cmml" type="integer" xref="S3.p6.1.m1.1.1.3.2">50</cn><ci id="S3.p6.1.m1.1.1.3.3.cmml" xref="S3.p6.1.m1.1.1.3.3">𝑚</ci><ci id="S3.p6.1.m1.1.1.3.4.cmml" xref="S3.p6.1.m1.1.1.3.4">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">\text{\lx@glossaries@gls@link{acronym}{mpjpe}{\leavevmode MPJPE}}&lt;50mm</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.1d">&lt; 50 italic_m italic_m</annotation></semantics></math>.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="AP@k=\frac{1}{P}\sum_{p=1}^{P}\frac{TP_{p}}{TP_{p}+FP_{p}}" class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mrow id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.2.2" xref="S3.E4.m1.1.1.2.2.cmml">A</mi><mo id="S3.E4.m1.1.1.2.1" xref="S3.E4.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.2.3" xref="S3.E4.m1.1.1.2.3.cmml">P</mi><mo id="S3.E4.m1.1.1.2.1a" xref="S3.E4.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.2.4" mathvariant="normal" xref="S3.E4.m1.1.1.2.4.cmml">@</mi><mo id="S3.E4.m1.1.1.2.1b" xref="S3.E4.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.2.5" xref="S3.E4.m1.1.1.2.5.cmml">k</mi></mrow><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mfrac id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml"><mn id="S3.E4.m1.1.1.3.2.2" xref="S3.E4.m1.1.1.3.2.2.cmml">1</mn><mi id="S3.E4.m1.1.1.3.2.3" xref="S3.E4.m1.1.1.3.2.3.cmml">P</mi></mfrac><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">⁢</mo><mrow id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><munderover id="S3.E4.m1.1.1.3.3.1" xref="S3.E4.m1.1.1.3.3.1.cmml"><mo id="S3.E4.m1.1.1.3.3.1.2.2" movablelimits="false" xref="S3.E4.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E4.m1.1.1.3.3.1.2.3" xref="S3.E4.m1.1.1.3.3.1.2.3.cmml"><mi id="S3.E4.m1.1.1.3.3.1.2.3.2" xref="S3.E4.m1.1.1.3.3.1.2.3.2.cmml">p</mi><mo id="S3.E4.m1.1.1.3.3.1.2.3.1" xref="S3.E4.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E4.m1.1.1.3.3.1.2.3.3" xref="S3.E4.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.1.1.3.3.1.3" xref="S3.E4.m1.1.1.3.3.1.3.cmml">P</mi></munderover><mfrac id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml"><mrow id="S3.E4.m1.1.1.3.3.2.2" xref="S3.E4.m1.1.1.3.3.2.2.cmml"><mi id="S3.E4.m1.1.1.3.3.2.2.2" xref="S3.E4.m1.1.1.3.3.2.2.2.cmml">T</mi><mo id="S3.E4.m1.1.1.3.3.2.2.1" xref="S3.E4.m1.1.1.3.3.2.2.1.cmml">⁢</mo><msub id="S3.E4.m1.1.1.3.3.2.2.3" xref="S3.E4.m1.1.1.3.3.2.2.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2.2.3.2" xref="S3.E4.m1.1.1.3.3.2.2.3.2.cmml">P</mi><mi id="S3.E4.m1.1.1.3.3.2.2.3.3" xref="S3.E4.m1.1.1.3.3.2.2.3.3.cmml">p</mi></msub></mrow><mrow id="S3.E4.m1.1.1.3.3.2.3" xref="S3.E4.m1.1.1.3.3.2.3.cmml"><mrow id="S3.E4.m1.1.1.3.3.2.3.2" xref="S3.E4.m1.1.1.3.3.2.3.2.cmml"><mi id="S3.E4.m1.1.1.3.3.2.3.2.2" xref="S3.E4.m1.1.1.3.3.2.3.2.2.cmml">T</mi><mo id="S3.E4.m1.1.1.3.3.2.3.2.1" xref="S3.E4.m1.1.1.3.3.2.3.2.1.cmml">⁢</mo><msub id="S3.E4.m1.1.1.3.3.2.3.2.3" xref="S3.E4.m1.1.1.3.3.2.3.2.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2.3.2.3.2" xref="S3.E4.m1.1.1.3.3.2.3.2.3.2.cmml">P</mi><mi id="S3.E4.m1.1.1.3.3.2.3.2.3.3" xref="S3.E4.m1.1.1.3.3.2.3.2.3.3.cmml">p</mi></msub></mrow><mo id="S3.E4.m1.1.1.3.3.2.3.1" xref="S3.E4.m1.1.1.3.3.2.3.1.cmml">+</mo><mrow id="S3.E4.m1.1.1.3.3.2.3.3" xref="S3.E4.m1.1.1.3.3.2.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2.3.3.2" xref="S3.E4.m1.1.1.3.3.2.3.3.2.cmml">F</mi><mo id="S3.E4.m1.1.1.3.3.2.3.3.1" xref="S3.E4.m1.1.1.3.3.2.3.3.1.cmml">⁢</mo><msub id="S3.E4.m1.1.1.3.3.2.3.3.3" xref="S3.E4.m1.1.1.3.3.2.3.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2.3.3.3.2" xref="S3.E4.m1.1.1.3.3.2.3.3.3.2.cmml">P</mi><mi id="S3.E4.m1.1.1.3.3.2.3.3.3.3" xref="S3.E4.m1.1.1.3.3.2.3.3.3.3.cmml">p</mi></msub></mrow></mrow></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></eq><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"><times id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.2.1"></times><ci id="S3.E4.m1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.2.2">𝐴</ci><ci id="S3.E4.m1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.2.3">𝑃</ci><ci id="S3.E4.m1.1.1.2.4.cmml" xref="S3.E4.m1.1.1.2.4">@</ci><ci id="S3.E4.m1.1.1.2.5.cmml" xref="S3.E4.m1.1.1.2.5">𝑘</ci></apply><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><times id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></times><apply id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"><divide id="S3.E4.m1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.3.2"></divide><cn id="S3.E4.m1.1.1.3.2.2.cmml" type="integer" xref="S3.E4.m1.1.1.3.2.2">1</cn><ci id="S3.E4.m1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.3.2.3">𝑃</ci></apply><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><apply id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.1.1.cmml" xref="S3.E4.m1.1.1.3.3.1">superscript</csymbol><apply id="S3.E4.m1.1.1.3.3.1.2.cmml" xref="S3.E4.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.1.2.1.cmml" xref="S3.E4.m1.1.1.3.3.1">subscript</csymbol><sum id="S3.E4.m1.1.1.3.3.1.2.2.cmml" xref="S3.E4.m1.1.1.3.3.1.2.2"></sum><apply id="S3.E4.m1.1.1.3.3.1.2.3.cmml" xref="S3.E4.m1.1.1.3.3.1.2.3"><eq id="S3.E4.m1.1.1.3.3.1.2.3.1.cmml" xref="S3.E4.m1.1.1.3.3.1.2.3.1"></eq><ci id="S3.E4.m1.1.1.3.3.1.2.3.2.cmml" xref="S3.E4.m1.1.1.3.3.1.2.3.2">𝑝</ci><cn id="S3.E4.m1.1.1.3.3.1.2.3.3.cmml" type="integer" xref="S3.E4.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.1.1.3.3.1.3.cmml" xref="S3.E4.m1.1.1.3.3.1.3">𝑃</ci></apply><apply id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2"><divide id="S3.E4.m1.1.1.3.3.2.1.cmml" xref="S3.E4.m1.1.1.3.3.2"></divide><apply id="S3.E4.m1.1.1.3.3.2.2.cmml" xref="S3.E4.m1.1.1.3.3.2.2"><times id="S3.E4.m1.1.1.3.3.2.2.1.cmml" xref="S3.E4.m1.1.1.3.3.2.2.1"></times><ci id="S3.E4.m1.1.1.3.3.2.2.2.cmml" xref="S3.E4.m1.1.1.3.3.2.2.2">𝑇</ci><apply id="S3.E4.m1.1.1.3.3.2.2.3.cmml" xref="S3.E4.m1.1.1.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.2.2.3.1.cmml" xref="S3.E4.m1.1.1.3.3.2.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.2.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2.2.3.2">𝑃</ci><ci id="S3.E4.m1.1.1.3.3.2.2.3.3.cmml" xref="S3.E4.m1.1.1.3.3.2.2.3.3">𝑝</ci></apply></apply><apply id="S3.E4.m1.1.1.3.3.2.3.cmml" xref="S3.E4.m1.1.1.3.3.2.3"><plus id="S3.E4.m1.1.1.3.3.2.3.1.cmml" xref="S3.E4.m1.1.1.3.3.2.3.1"></plus><apply id="S3.E4.m1.1.1.3.3.2.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2.3.2"><times id="S3.E4.m1.1.1.3.3.2.3.2.1.cmml" xref="S3.E4.m1.1.1.3.3.2.3.2.1"></times><ci id="S3.E4.m1.1.1.3.3.2.3.2.2.cmml" xref="S3.E4.m1.1.1.3.3.2.3.2.2">𝑇</ci><apply id="S3.E4.m1.1.1.3.3.2.3.2.3.cmml" xref="S3.E4.m1.1.1.3.3.2.3.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.2.3.2.3.1.cmml" xref="S3.E4.m1.1.1.3.3.2.3.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.3.2.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2.3.2.3.2">𝑃</ci><ci id="S3.E4.m1.1.1.3.3.2.3.2.3.3.cmml" xref="S3.E4.m1.1.1.3.3.2.3.2.3.3">𝑝</ci></apply></apply><apply id="S3.E4.m1.1.1.3.3.2.3.3.cmml" xref="S3.E4.m1.1.1.3.3.2.3.3"><times id="S3.E4.m1.1.1.3.3.2.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.2.3.3.1"></times><ci id="S3.E4.m1.1.1.3.3.2.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2.3.3.2">𝐹</ci><apply id="S3.E4.m1.1.1.3.3.2.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.2.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.2.3.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.2.3.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.3.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2.3.3.3.2">𝑃</ci><ci id="S3.E4.m1.1.1.3.3.2.3.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.2.3.3.3.3">𝑝</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">AP@k=\frac{1}{P}\sum_{p=1}^{P}\frac{TP_{p}}{TP_{p}+FP_{p}}</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">italic_A italic_P @ italic_k = divide start_ARG 1 end_ARG start_ARG italic_P end_ARG ∑ start_POSTSUBSCRIPT italic_p = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT divide start_ARG italic_T italic_P start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_ARG start_ARG italic_T italic_P start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT + italic_F italic_P start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p6.2">TP corresponds to True Positive.
<br class="ltx_break"/>FP corresponds to False Positive.
<br class="ltx_break"/>P is the number of people in the scene.
<br class="ltx_break"/>k is the threshold in <math alttext="mm" class="ltx_Math" display="inline" id="S3.p6.2.m1.1"><semantics id="S3.p6.2.m1.1a"><mrow id="S3.p6.2.m1.1.1" xref="S3.p6.2.m1.1.1.cmml"><mi id="S3.p6.2.m1.1.1.2" xref="S3.p6.2.m1.1.1.2.cmml">m</mi><mo id="S3.p6.2.m1.1.1.1" xref="S3.p6.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.p6.2.m1.1.1.3" xref="S3.p6.2.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.2.m1.1b"><apply id="S3.p6.2.m1.1.1.cmml" xref="S3.p6.2.m1.1.1"><times id="S3.p6.2.m1.1.1.1.cmml" xref="S3.p6.2.m1.1.1.1"></times><ci id="S3.p6.2.m1.1.1.2.cmml" xref="S3.p6.2.m1.1.1.2">𝑚</ci><ci id="S3.p6.2.m1.1.1.3.cmml" xref="S3.p6.2.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m1.1c">mm</annotation><annotation encoding="application/x-llamapun" id="S3.p6.2.m1.1d">italic_m italic_m</annotation></semantics></math>.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1"><span class="ltx_glossaryref ltx_font_bold" title="">mAP</span> is the average value of all <span class="ltx_glossaryref" title="">AP</span> over all considered thresholds, (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S3.E5" title="In 3 Evaluation Metrics ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="mAP=\frac{1}{T}\sum_{k=1}^{T}AP@k" class="ltx_Math" display="block" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mrow id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.2.2" xref="S3.E5.m1.1.1.2.2.cmml">m</mi><mo id="S3.E5.m1.1.1.2.1" xref="S3.E5.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.2.3" xref="S3.E5.m1.1.1.2.3.cmml">A</mi><mo id="S3.E5.m1.1.1.2.1a" xref="S3.E5.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.2.4" xref="S3.E5.m1.1.1.2.4.cmml">P</mi></mrow><mo id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml">=</mo><mrow id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><mfrac id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.3.2.cmml"><mn id="S3.E5.m1.1.1.3.2.2" xref="S3.E5.m1.1.1.3.2.2.cmml">1</mn><mi id="S3.E5.m1.1.1.3.2.3" xref="S3.E5.m1.1.1.3.2.3.cmml">T</mi></mfrac><mo id="S3.E5.m1.1.1.3.1" xref="S3.E5.m1.1.1.3.1.cmml">⁢</mo><mrow id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml"><munderover id="S3.E5.m1.1.1.3.3.1" xref="S3.E5.m1.1.1.3.3.1.cmml"><mo id="S3.E5.m1.1.1.3.3.1.2.2" movablelimits="false" xref="S3.E5.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E5.m1.1.1.3.3.1.2.3" xref="S3.E5.m1.1.1.3.3.1.2.3.cmml"><mi id="S3.E5.m1.1.1.3.3.1.2.3.2" xref="S3.E5.m1.1.1.3.3.1.2.3.2.cmml">k</mi><mo id="S3.E5.m1.1.1.3.3.1.2.3.1" xref="S3.E5.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E5.m1.1.1.3.3.1.2.3.3" xref="S3.E5.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E5.m1.1.1.3.3.1.3" xref="S3.E5.m1.1.1.3.3.1.3.cmml">T</mi></munderover><mrow id="S3.E5.m1.1.1.3.3.2" xref="S3.E5.m1.1.1.3.3.2.cmml"><mi id="S3.E5.m1.1.1.3.3.2.2" xref="S3.E5.m1.1.1.3.3.2.2.cmml">A</mi><mo id="S3.E5.m1.1.1.3.3.2.1" xref="S3.E5.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.3.2.3" xref="S3.E5.m1.1.1.3.3.2.3.cmml">P</mi><mo id="S3.E5.m1.1.1.3.3.2.1a" xref="S3.E5.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.3.2.4" mathvariant="normal" xref="S3.E5.m1.1.1.3.3.2.4.cmml">@</mi><mo id="S3.E5.m1.1.1.3.3.2.1b" xref="S3.E5.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.3.2.5" xref="S3.E5.m1.1.1.3.3.2.5.cmml">k</mi></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"></eq><apply id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2"><times id="S3.E5.m1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.2.1"></times><ci id="S3.E5.m1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.2.2">𝑚</ci><ci id="S3.E5.m1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.2.3">𝐴</ci><ci id="S3.E5.m1.1.1.2.4.cmml" xref="S3.E5.m1.1.1.2.4">𝑃</ci></apply><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><times id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3.1"></times><apply id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2"><divide id="S3.E5.m1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.3.2"></divide><cn id="S3.E5.m1.1.1.3.2.2.cmml" type="integer" xref="S3.E5.m1.1.1.3.2.2">1</cn><ci id="S3.E5.m1.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.3.2.3">𝑇</ci></apply><apply id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3"><apply id="S3.E5.m1.1.1.3.3.1.cmml" xref="S3.E5.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.3.1.1.cmml" xref="S3.E5.m1.1.1.3.3.1">superscript</csymbol><apply id="S3.E5.m1.1.1.3.3.1.2.cmml" xref="S3.E5.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.3.1.2.1.cmml" xref="S3.E5.m1.1.1.3.3.1">subscript</csymbol><sum id="S3.E5.m1.1.1.3.3.1.2.2.cmml" xref="S3.E5.m1.1.1.3.3.1.2.2"></sum><apply id="S3.E5.m1.1.1.3.3.1.2.3.cmml" xref="S3.E5.m1.1.1.3.3.1.2.3"><eq id="S3.E5.m1.1.1.3.3.1.2.3.1.cmml" xref="S3.E5.m1.1.1.3.3.1.2.3.1"></eq><ci id="S3.E5.m1.1.1.3.3.1.2.3.2.cmml" xref="S3.E5.m1.1.1.3.3.1.2.3.2">𝑘</ci><cn id="S3.E5.m1.1.1.3.3.1.2.3.3.cmml" type="integer" xref="S3.E5.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E5.m1.1.1.3.3.1.3.cmml" xref="S3.E5.m1.1.1.3.3.1.3">𝑇</ci></apply><apply id="S3.E5.m1.1.1.3.3.2.cmml" xref="S3.E5.m1.1.1.3.3.2"><times id="S3.E5.m1.1.1.3.3.2.1.cmml" xref="S3.E5.m1.1.1.3.3.2.1"></times><ci id="S3.E5.m1.1.1.3.3.2.2.cmml" xref="S3.E5.m1.1.1.3.3.2.2">𝐴</ci><ci id="S3.E5.m1.1.1.3.3.2.3.cmml" xref="S3.E5.m1.1.1.3.3.2.3">𝑃</ci><ci id="S3.E5.m1.1.1.3.3.2.4.cmml" xref="S3.E5.m1.1.1.3.3.2.4">@</ci><ci id="S3.E5.m1.1.1.3.3.2.5.cmml" xref="S3.E5.m1.1.1.3.3.2.5">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">mAP=\frac{1}{T}\sum_{k=1}^{T}AP@k</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">italic_m italic_A italic_P = divide start_ARG 1 end_ARG start_ARG italic_T end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_A italic_P @ italic_k</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p7.2">T is the number of considered thresholds.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p8">
<p class="ltx_p" id="S3.p8.1"><span class="ltx_glossaryref ltx_font_bold" title="">AUC</span> integrates the curve that evaluates the model performance across all <span class="ltx_glossaryref" title="">PCK</span> thresholds.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p9">
<p class="ltx_p" id="S3.p9.7"><span class="ltx_glossaryref ltx_font_bold" title="">MPJAE</span>
measures the average, across all angles, of the absolute difference in degrees between the actual joint angles and the estimates, (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S3.E6" title="In 3 Evaluation Metrics ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\text{MPJAE}=\frac{1}{N}\sum_{i=1}^{N}\left|\left(x_{i}-x_{i}^{{}^{\prime}}%
\right)\mod\pm 180\text{\textordmasculine}\right|" class="ltx_Math" display="block" id="S3.E6.m1.1"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><mtext id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3a.cmml">MPJAE</mtext><mo id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml">=</mo><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml"><mfrac id="S3.E6.m1.1.1.1.3" xref="S3.E6.m1.1.1.1.3.cmml"><mn id="S3.E6.m1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.3.2.cmml">1</mn><mi id="S3.E6.m1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.3.3.cmml">N</mi></mfrac><mo id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><munderover id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml"><mo id="S3.E6.m1.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S3.E6.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E6.m1.1.1.1.1.2.2.3" xref="S3.E6.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.2.2.3.2" xref="S3.E6.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E6.m1.1.1.1.1.2.2.3.1" xref="S3.E6.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E6.m1.1.1.1.1.2.2.3.3" xref="S3.E6.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E6.m1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.2.cmml"><mo id="S3.E6.m1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">x</mi><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><msup id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3a" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"></mi><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">′</mo></msup></msubsup></mrow><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E6.m1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.2.cmml">mod</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.3.cmml"><mo id="S3.E6.m1.1.1.1.1.1.1.1.3a" xref="S3.E6.m1.1.1.1.1.1.1.1.3.cmml">±</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.3.2.cmml"><mn id="S3.E6.m1.1.1.1.1.1.1.1.3.2.2" xref="S3.E6.m1.1.1.1.1.1.1.1.3.2.2.cmml">180</mn><mo id="S3.E6.m1.1.1.1.1.1.1.1.3.2.1" xref="S3.E6.m1.1.1.1.1.1.1.1.3.2.1.cmml">⁢</mo><mtext id="S3.E6.m1.1.1.1.1.1.1.1.3.2.3" xref="S3.E6.m1.1.1.1.1.1.1.1.3.2.3a.cmml">º</mtext></mrow></mrow></mrow><mo id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><eq id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2"></eq><ci id="S3.E6.m1.1.1.3a.cmml" xref="S3.E6.m1.1.1.3"><mtext id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3">MPJAE</mtext></ci><apply id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><times id="S3.E6.m1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.2"></times><apply id="S3.E6.m1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.3"><divide id="S3.E6.m1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.3"></divide><cn id="S3.E6.m1.1.1.1.3.2.cmml" type="integer" xref="S3.E6.m1.1.1.1.3.2">1</cn><ci id="S3.E6.m1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1"><apply id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E6.m1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E6.m1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E6.m1.1.1.1.1.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.2.3"><eq id="S3.E6.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E6.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.E6.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E6.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E6.m1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1"><abs id="S3.E6.m1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2"></abs><apply id="S3.E6.m1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E6.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.2">modulo</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1"><minus id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.2">𝑥</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3"><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.1">′</ci></apply></apply></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.3"><csymbol cd="latexml" id="S3.E6.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.3">plus-or-minus</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.3.2"><times id="S3.E6.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.3.2.1"></times><cn id="S3.E6.m1.1.1.1.1.1.1.1.3.2.2.cmml" type="integer" xref="S3.E6.m1.1.1.1.1.1.1.1.3.2.2">180</cn><ci id="S3.E6.m1.1.1.1.1.1.1.1.3.2.3a.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.3.2.3"><mtext id="S3.E6.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.3.2.3">º</mtext></ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\text{MPJAE}=\frac{1}{N}\sum_{i=1}^{N}\left|\left(x_{i}-x_{i}^{{}^{\prime}}%
\right)\mod\pm 180\text{\textordmasculine}\right|</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.1d">MPJAE = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT | ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT ) roman_mod ± 180 º |</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p9.6"><math alttext="N" class="ltx_Math" display="inline" id="S3.p9.1.m1.1"><semantics id="S3.p9.1.m1.1a"><mi id="S3.p9.1.m1.1.1" xref="S3.p9.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p9.1.m1.1b"><ci id="S3.p9.1.m1.1.1.cmml" xref="S3.p9.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.p9.1.m1.1d">italic_N</annotation></semantics></math> is the total number of joints.
<br class="ltx_break"/><math alttext="P_{i}" class="ltx_Math" display="inline" id="S3.p9.2.m2.1"><semantics id="S3.p9.2.m2.1a"><msub id="S3.p9.2.m2.1.1" xref="S3.p9.2.m2.1.1.cmml"><mi id="S3.p9.2.m2.1.1.2" xref="S3.p9.2.m2.1.1.2.cmml">P</mi><mi id="S3.p9.2.m2.1.1.3" xref="S3.p9.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p9.2.m2.1b"><apply id="S3.p9.2.m2.1.1.cmml" xref="S3.p9.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p9.2.m2.1.1.1.cmml" xref="S3.p9.2.m2.1.1">subscript</csymbol><ci id="S3.p9.2.m2.1.1.2.cmml" xref="S3.p9.2.m2.1.1.2">𝑃</ci><ci id="S3.p9.2.m2.1.1.3.cmml" xref="S3.p9.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.2.m2.1c">P_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p9.2.m2.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the estimated pose vector.
<br class="ltx_break"/><math alttext="P_{i}^{{}^{\prime}}" class="ltx_Math" display="inline" id="S3.p9.3.m3.1"><semantics id="S3.p9.3.m3.1a"><msubsup id="S3.p9.3.m3.1.1" xref="S3.p9.3.m3.1.1.cmml"><mi id="S3.p9.3.m3.1.1.2.2" xref="S3.p9.3.m3.1.1.2.2.cmml">P</mi><mi id="S3.p9.3.m3.1.1.2.3" xref="S3.p9.3.m3.1.1.2.3.cmml">i</mi><msup id="S3.p9.3.m3.1.1.3" xref="S3.p9.3.m3.1.1.3.cmml"><mi id="S3.p9.3.m3.1.1.3a" xref="S3.p9.3.m3.1.1.3.cmml"></mi><mo id="S3.p9.3.m3.1.1.3.1" xref="S3.p9.3.m3.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="S3.p9.3.m3.1b"><apply id="S3.p9.3.m3.1.1.cmml" xref="S3.p9.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p9.3.m3.1.1.1.cmml" xref="S3.p9.3.m3.1.1">superscript</csymbol><apply id="S3.p9.3.m3.1.1.2.cmml" xref="S3.p9.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p9.3.m3.1.1.2.1.cmml" xref="S3.p9.3.m3.1.1">subscript</csymbol><ci id="S3.p9.3.m3.1.1.2.2.cmml" xref="S3.p9.3.m3.1.1.2.2">𝑃</ci><ci id="S3.p9.3.m3.1.1.2.3.cmml" xref="S3.p9.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S3.p9.3.m3.1.1.3.cmml" xref="S3.p9.3.m3.1.1.3"><ci id="S3.p9.3.m3.1.1.3.1.cmml" xref="S3.p9.3.m3.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.3.m3.1c">P_{i}^{{}^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.p9.3.m3.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> is the ground-truth pose vector.
<br class="ltx_break"/><math alttext="\mod" class="ltx_Math" display="inline" id="S3.p9.4.m4.1"><semantics id="S3.p9.4.m4.1a"><mo id="S3.p9.4.m4.1.1" xref="S3.p9.4.m4.1.1.cmml">mod</mo><annotation-xml encoding="MathML-Content" id="S3.p9.4.m4.1b"><csymbol cd="latexml" id="S3.p9.4.m4.1.1.cmml" xref="S3.p9.4.m4.1.1">modulo</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.4.m4.1c">\mod</annotation><annotation encoding="application/x-llamapun" id="S3.p9.4.m4.1d">roman_mod</annotation></semantics></math> is the modulus operator, the term <math alttext="\text{mod}\pm 180" class="ltx_Math" display="inline" id="S3.p9.5.m5.1"><semantics id="S3.p9.5.m5.1a"><mrow id="S3.p9.5.m5.1.1" xref="S3.p9.5.m5.1.1.cmml"><mtext id="S3.p9.5.m5.1.1.2" xref="S3.p9.5.m5.1.1.2a.cmml">mod</mtext><mo id="S3.p9.5.m5.1.1.1" xref="S3.p9.5.m5.1.1.1.cmml">±</mo><mn id="S3.p9.5.m5.1.1.3" xref="S3.p9.5.m5.1.1.3.cmml">180</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p9.5.m5.1b"><apply id="S3.p9.5.m5.1.1.cmml" xref="S3.p9.5.m5.1.1"><csymbol cd="latexml" id="S3.p9.5.m5.1.1.1.cmml" xref="S3.p9.5.m5.1.1.1">plus-or-minus</csymbol><ci id="S3.p9.5.m5.1.1.2a.cmml" xref="S3.p9.5.m5.1.1.2"><mtext id="S3.p9.5.m5.1.1.2.cmml" xref="S3.p9.5.m5.1.1.2">mod</mtext></ci><cn id="S3.p9.5.m5.1.1.3.cmml" type="integer" xref="S3.p9.5.m5.1.1.3">180</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.5.m5.1c">\text{mod}\pm 180</annotation><annotation encoding="application/x-llamapun" id="S3.p9.5.m5.1d">mod ± 180</annotation></semantics></math>º applies to angles brings them into the range of <math alttext="[-180\text{\textordmasculine},+180\text{\textordmasculine}]" class="ltx_Math" display="inline" id="S3.p9.6.m6.2"><semantics id="S3.p9.6.m6.2a"><mrow id="S3.p9.6.m6.2.2.2" xref="S3.p9.6.m6.2.2.3.cmml"><mo id="S3.p9.6.m6.2.2.2.3" stretchy="false" xref="S3.p9.6.m6.2.2.3.cmml">[</mo><mrow id="S3.p9.6.m6.1.1.1.1" xref="S3.p9.6.m6.1.1.1.1.cmml"><mo id="S3.p9.6.m6.1.1.1.1a" xref="S3.p9.6.m6.1.1.1.1.cmml">−</mo><mrow id="S3.p9.6.m6.1.1.1.1.2" xref="S3.p9.6.m6.1.1.1.1.2.cmml"><mn id="S3.p9.6.m6.1.1.1.1.2.2" xref="S3.p9.6.m6.1.1.1.1.2.2.cmml">180</mn><mo id="S3.p9.6.m6.1.1.1.1.2.1" xref="S3.p9.6.m6.1.1.1.1.2.1.cmml">⁢</mo><mtext id="S3.p9.6.m6.1.1.1.1.2.3" xref="S3.p9.6.m6.1.1.1.1.2.3a.cmml">º</mtext></mrow></mrow><mo id="S3.p9.6.m6.2.2.2.4" xref="S3.p9.6.m6.2.2.3.cmml">,</mo><mrow id="S3.p9.6.m6.2.2.2.2" xref="S3.p9.6.m6.2.2.2.2.cmml"><mo id="S3.p9.6.m6.2.2.2.2a" xref="S3.p9.6.m6.2.2.2.2.cmml">+</mo><mrow id="S3.p9.6.m6.2.2.2.2.2" xref="S3.p9.6.m6.2.2.2.2.2.cmml"><mn id="S3.p9.6.m6.2.2.2.2.2.2" xref="S3.p9.6.m6.2.2.2.2.2.2.cmml">180</mn><mo id="S3.p9.6.m6.2.2.2.2.2.1" xref="S3.p9.6.m6.2.2.2.2.2.1.cmml">⁢</mo><mtext id="S3.p9.6.m6.2.2.2.2.2.3" xref="S3.p9.6.m6.2.2.2.2.2.3a.cmml">º</mtext></mrow></mrow><mo id="S3.p9.6.m6.2.2.2.5" stretchy="false" xref="S3.p9.6.m6.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p9.6.m6.2b"><interval closure="closed" id="S3.p9.6.m6.2.2.3.cmml" xref="S3.p9.6.m6.2.2.2"><apply id="S3.p9.6.m6.1.1.1.1.cmml" xref="S3.p9.6.m6.1.1.1.1"><minus id="S3.p9.6.m6.1.1.1.1.1.cmml" xref="S3.p9.6.m6.1.1.1.1"></minus><apply id="S3.p9.6.m6.1.1.1.1.2.cmml" xref="S3.p9.6.m6.1.1.1.1.2"><times id="S3.p9.6.m6.1.1.1.1.2.1.cmml" xref="S3.p9.6.m6.1.1.1.1.2.1"></times><cn id="S3.p9.6.m6.1.1.1.1.2.2.cmml" type="integer" xref="S3.p9.6.m6.1.1.1.1.2.2">180</cn><ci id="S3.p9.6.m6.1.1.1.1.2.3a.cmml" xref="S3.p9.6.m6.1.1.1.1.2.3"><mtext id="S3.p9.6.m6.1.1.1.1.2.3.cmml" xref="S3.p9.6.m6.1.1.1.1.2.3">º</mtext></ci></apply></apply><apply id="S3.p9.6.m6.2.2.2.2.cmml" xref="S3.p9.6.m6.2.2.2.2"><plus id="S3.p9.6.m6.2.2.2.2.1.cmml" xref="S3.p9.6.m6.2.2.2.2"></plus><apply id="S3.p9.6.m6.2.2.2.2.2.cmml" xref="S3.p9.6.m6.2.2.2.2.2"><times id="S3.p9.6.m6.2.2.2.2.2.1.cmml" xref="S3.p9.6.m6.2.2.2.2.2.1"></times><cn id="S3.p9.6.m6.2.2.2.2.2.2.cmml" type="integer" xref="S3.p9.6.m6.2.2.2.2.2.2">180</cn><ci id="S3.p9.6.m6.2.2.2.2.2.3a.cmml" xref="S3.p9.6.m6.2.2.2.2.2.3"><mtext id="S3.p9.6.m6.2.2.2.2.2.3.cmml" xref="S3.p9.6.m6.2.2.2.2.2.3">º</mtext></ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.6.m6.2c">[-180\text{\textordmasculine},+180\text{\textordmasculine}]</annotation><annotation encoding="application/x-llamapun" id="S3.p9.6.m6.2d">[ - 180 º , + 180 º ]</annotation></semantics></math>.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p10">
<p class="ltx_p" id="S3.p10.3"><span class="ltx_glossaryref ltx_font_bold" title="">MRPE</span> consists of the mean Euclidean distance between the predicted root localization and the ground-truth root localization, (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S3.E7" title="In 3 Evaluation Metrics ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\text{MRPE}=\frac{1}{N}\sum_{i=1}^{N}\left|\left|R^{(i)^{\prime}}-R^{(i)}%
\right|\right|_{2}" class="ltx_Math" display="block" id="S3.E7.m1.3"><semantics id="S3.E7.m1.3a"><mrow id="S3.E7.m1.3.3" xref="S3.E7.m1.3.3.cmml"><mtext id="S3.E7.m1.3.3.3" xref="S3.E7.m1.3.3.3a.cmml">MRPE</mtext><mo id="S3.E7.m1.3.3.2" xref="S3.E7.m1.3.3.2.cmml">=</mo><mrow id="S3.E7.m1.3.3.1" xref="S3.E7.m1.3.3.1.cmml"><mfrac id="S3.E7.m1.3.3.1.3" xref="S3.E7.m1.3.3.1.3.cmml"><mn id="S3.E7.m1.3.3.1.3.2" xref="S3.E7.m1.3.3.1.3.2.cmml">1</mn><mi id="S3.E7.m1.3.3.1.3.3" xref="S3.E7.m1.3.3.1.3.3.cmml">N</mi></mfrac><mo id="S3.E7.m1.3.3.1.2" xref="S3.E7.m1.3.3.1.2.cmml">⁢</mo><mrow id="S3.E7.m1.3.3.1.1" xref="S3.E7.m1.3.3.1.1.cmml"><munderover id="S3.E7.m1.3.3.1.1.2" xref="S3.E7.m1.3.3.1.1.2.cmml"><mo id="S3.E7.m1.3.3.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S3.E7.m1.3.3.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E7.m1.3.3.1.1.2.2.3" xref="S3.E7.m1.3.3.1.1.2.2.3.cmml"><mi id="S3.E7.m1.3.3.1.1.2.2.3.2" xref="S3.E7.m1.3.3.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E7.m1.3.3.1.1.2.2.3.1" xref="S3.E7.m1.3.3.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E7.m1.3.3.1.1.2.2.3.3" xref="S3.E7.m1.3.3.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E7.m1.3.3.1.1.2.3" xref="S3.E7.m1.3.3.1.1.2.3.cmml">N</mi></munderover><msub id="S3.E7.m1.3.3.1.1.1" xref="S3.E7.m1.3.3.1.1.1.cmml"><mrow id="S3.E7.m1.3.3.1.1.1.1.1" xref="S3.E7.m1.3.3.1.1.1.1.2.cmml"><mo id="S3.E7.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S3.E7.m1.3.3.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E7.m1.3.3.1.1.1.1.1.1" xref="S3.E7.m1.3.3.1.1.1.1.1.1.cmml"><msup id="S3.E7.m1.3.3.1.1.1.1.1.1.2" xref="S3.E7.m1.3.3.1.1.1.1.1.1.2.cmml"><mi id="S3.E7.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E7.m1.3.3.1.1.1.1.1.1.2.2.cmml">R</mi><msup id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1.3.2" xref="S3.E7.m1.1.1.1.cmml"><mo id="S3.E7.m1.1.1.1.3.2.1" stretchy="false" xref="S3.E7.m1.1.1.1.cmml">(</mo><mi id="S3.E7.m1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml">i</mi><mo id="S3.E7.m1.1.1.1.3.2.2" stretchy="false" xref="S3.E7.m1.1.1.1.cmml">)</mo></mrow><mo id="S3.E7.m1.1.1.1.4" xref="S3.E7.m1.1.1.1.4.cmml">′</mo></msup></msup><mo id="S3.E7.m1.3.3.1.1.1.1.1.1.1" xref="S3.E7.m1.3.3.1.1.1.1.1.1.1.cmml">−</mo><msup id="S3.E7.m1.3.3.1.1.1.1.1.1.3" xref="S3.E7.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E7.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E7.m1.3.3.1.1.1.1.1.1.3.2.cmml">R</mi><mrow id="S3.E7.m1.2.2.1.3" xref="S3.E7.m1.3.3.1.1.1.1.1.1.3.cmml"><mo id="S3.E7.m1.2.2.1.3.1" stretchy="false" xref="S3.E7.m1.3.3.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E7.m1.2.2.1.1" xref="S3.E7.m1.2.2.1.1.cmml">i</mi><mo id="S3.E7.m1.2.2.1.3.2" stretchy="false" xref="S3.E7.m1.3.3.1.1.1.1.1.1.3.cmml">)</mo></mrow></msup></mrow><mo id="S3.E7.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="S3.E7.m1.3.3.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E7.m1.3.3.1.1.1.3" xref="S3.E7.m1.3.3.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.3b"><apply id="S3.E7.m1.3.3.cmml" xref="S3.E7.m1.3.3"><eq id="S3.E7.m1.3.3.2.cmml" xref="S3.E7.m1.3.3.2"></eq><ci id="S3.E7.m1.3.3.3a.cmml" xref="S3.E7.m1.3.3.3"><mtext id="S3.E7.m1.3.3.3.cmml" xref="S3.E7.m1.3.3.3">MRPE</mtext></ci><apply id="S3.E7.m1.3.3.1.cmml" xref="S3.E7.m1.3.3.1"><times id="S3.E7.m1.3.3.1.2.cmml" xref="S3.E7.m1.3.3.1.2"></times><apply id="S3.E7.m1.3.3.1.3.cmml" xref="S3.E7.m1.3.3.1.3"><divide id="S3.E7.m1.3.3.1.3.1.cmml" xref="S3.E7.m1.3.3.1.3"></divide><cn id="S3.E7.m1.3.3.1.3.2.cmml" type="integer" xref="S3.E7.m1.3.3.1.3.2">1</cn><ci id="S3.E7.m1.3.3.1.3.3.cmml" xref="S3.E7.m1.3.3.1.3.3">𝑁</ci></apply><apply id="S3.E7.m1.3.3.1.1.cmml" xref="S3.E7.m1.3.3.1.1"><apply id="S3.E7.m1.3.3.1.1.2.cmml" xref="S3.E7.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.2.1.cmml" xref="S3.E7.m1.3.3.1.1.2">superscript</csymbol><apply id="S3.E7.m1.3.3.1.1.2.2.cmml" xref="S3.E7.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.2.2.1.cmml" xref="S3.E7.m1.3.3.1.1.2">subscript</csymbol><sum id="S3.E7.m1.3.3.1.1.2.2.2.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2"></sum><apply id="S3.E7.m1.3.3.1.1.2.2.3.cmml" xref="S3.E7.m1.3.3.1.1.2.2.3"><eq id="S3.E7.m1.3.3.1.1.2.2.3.1.cmml" xref="S3.E7.m1.3.3.1.1.2.2.3.1"></eq><ci id="S3.E7.m1.3.3.1.1.2.2.3.2.cmml" xref="S3.E7.m1.3.3.1.1.2.2.3.2">𝑖</ci><cn id="S3.E7.m1.3.3.1.1.2.2.3.3.cmml" type="integer" xref="S3.E7.m1.3.3.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E7.m1.3.3.1.1.2.3.cmml" xref="S3.E7.m1.3.3.1.1.2.3">𝑁</ci></apply><apply id="S3.E7.m1.3.3.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.1.2.cmml" xref="S3.E7.m1.3.3.1.1.1">subscript</csymbol><apply id="S3.E7.m1.3.3.1.1.1.1.2.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E7.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.2">norm</csymbol><apply id="S3.E7.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.1"><minus id="S3.E7.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.1.1"></minus><apply id="S3.E7.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E7.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.1.2.2">𝑅</ci><apply id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1">superscript</csymbol><ci id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1">𝑖</ci><ci id="S3.E7.m1.1.1.1.4.cmml" xref="S3.E7.m1.1.1.1.4">′</ci></apply></apply><apply id="S3.E7.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E7.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.1.3.2">𝑅</ci><ci id="S3.E7.m1.2.2.1.1.cmml" xref="S3.E7.m1.2.2.1.1">𝑖</ci></apply></apply></apply><cn id="S3.E7.m1.3.3.1.1.1.3.cmml" type="integer" xref="S3.E7.m1.3.3.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.3c">\text{MRPE}=\frac{1}{N}\sum_{i=1}^{N}\left|\left|R^{(i)^{\prime}}-R^{(i)}%
\right|\right|_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.3d">MRPE = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT | | italic_R start_POSTSUPERSCRIPT ( italic_i ) start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT - italic_R start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p10.2"><math alttext="R^{(i)^{\prime}}" class="ltx_Math" display="inline" id="S3.p10.1.m1.1"><semantics id="S3.p10.1.m1.1a"><msup id="S3.p10.1.m1.1.2" xref="S3.p10.1.m1.1.2.cmml"><mi id="S3.p10.1.m1.1.2.2" xref="S3.p10.1.m1.1.2.2.cmml">R</mi><msup id="S3.p10.1.m1.1.1.1" xref="S3.p10.1.m1.1.1.1.cmml"><mrow id="S3.p10.1.m1.1.1.1.3.2" xref="S3.p10.1.m1.1.1.1.cmml"><mo id="S3.p10.1.m1.1.1.1.3.2.1" stretchy="false" xref="S3.p10.1.m1.1.1.1.cmml">(</mo><mi id="S3.p10.1.m1.1.1.1.1" xref="S3.p10.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.p10.1.m1.1.1.1.3.2.2" stretchy="false" xref="S3.p10.1.m1.1.1.1.cmml">)</mo></mrow><mo id="S3.p10.1.m1.1.1.1.4" xref="S3.p10.1.m1.1.1.1.4.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="S3.p10.1.m1.1b"><apply id="S3.p10.1.m1.1.2.cmml" xref="S3.p10.1.m1.1.2"><csymbol cd="ambiguous" id="S3.p10.1.m1.1.2.1.cmml" xref="S3.p10.1.m1.1.2">superscript</csymbol><ci id="S3.p10.1.m1.1.2.2.cmml" xref="S3.p10.1.m1.1.2.2">𝑅</ci><apply id="S3.p10.1.m1.1.1.1.cmml" xref="S3.p10.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.p10.1.m1.1.1.1.2.cmml" xref="S3.p10.1.m1.1.1.1">superscript</csymbol><ci id="S3.p10.1.m1.1.1.1.1.cmml" xref="S3.p10.1.m1.1.1.1.1">𝑖</ci><ci id="S3.p10.1.m1.1.1.1.4.cmml" xref="S3.p10.1.m1.1.1.1.4">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.1.m1.1c">R^{(i)^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.p10.1.m1.1d">italic_R start_POSTSUPERSCRIPT ( italic_i ) start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> is the coordinates of the predicted root.
<br class="ltx_break"/><math alttext="R^{(i)}" class="ltx_Math" display="inline" id="S3.p10.2.m2.1"><semantics id="S3.p10.2.m2.1a"><msup id="S3.p10.2.m2.1.2" xref="S3.p10.2.m2.1.2.cmml"><mi id="S3.p10.2.m2.1.2.2" xref="S3.p10.2.m2.1.2.2.cmml">R</mi><mrow id="S3.p10.2.m2.1.1.1.3" xref="S3.p10.2.m2.1.2.cmml"><mo id="S3.p10.2.m2.1.1.1.3.1" stretchy="false" xref="S3.p10.2.m2.1.2.cmml">(</mo><mi id="S3.p10.2.m2.1.1.1.1" xref="S3.p10.2.m2.1.1.1.1.cmml">i</mi><mo id="S3.p10.2.m2.1.1.1.3.2" stretchy="false" xref="S3.p10.2.m2.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p10.2.m2.1b"><apply id="S3.p10.2.m2.1.2.cmml" xref="S3.p10.2.m2.1.2"><csymbol cd="ambiguous" id="S3.p10.2.m2.1.2.1.cmml" xref="S3.p10.2.m2.1.2">superscript</csymbol><ci id="S3.p10.2.m2.1.2.2.cmml" xref="S3.p10.2.m2.1.2.2">𝑅</ci><ci id="S3.p10.2.m2.1.1.1.1.cmml" xref="S3.p10.2.m2.1.1.1.1">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.2.m2.1c">R^{(i)}</annotation><annotation encoding="application/x-llamapun" id="S3.p10.2.m2.1d">italic_R start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT</annotation></semantics></math> is the coordinates of the ground-truth root.
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Multi-view approaches</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The availability of multiple views, on the one hand, provides more information for constructing the 3D pose, on the other hand, introduces a new problem, which is how to associate these multiple views. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib88" title=""><span class="ltx_text ltx_font_typewriter">88</span></a>]</cite></p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Most of the earlier methodologies were developed to address this task in single-person scenarios, nonetheless, the majority of the more recent approaches focus on multi-person settings. Therefore, the existing methods can be divided into single-person (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.SS1" title="4.1 Single-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">4.1</span></a>) or multi-person approaches (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.SS2" title="4.2 Multi-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">4.2</span></a>), according to the number of people in the scene.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Single-person approaches</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In this section, it’s presented methods developed to retrieve the 3D pose of scenarios with just one individual.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Sigal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib74" title=""><span class="ltx_text ltx_font_typewriter">74</span></a>]</cite> introduced the HumanEva datasets to combat the lack of datasets for multi-view 3D pose estimation and provide a benchmark so that developed methods could be compared fairly. Thus, to provide intuition on how to work with their dataset, a baseline model was proposed which consisted of a Bayesian filtering method whose parameters were optimised using Sequential Importance Resampling and Annealed Particle Filtering.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">To try to create a method which provides a fast inference while still giving a good performance, Wang and Chung <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib88" title=""><span class="ltx_text ltx_font_typewriter">88</span></a>]</cite> employed a bottom-up approach by first, determining all possible body part candidates for each view. Then, used a linear-combination expression to determine which detected parts had correspondence across all views, thus decreasing the number of erroneous candidates. Finally, a belief propagation process is applied to group all the parts into a 3D pose.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Mehrizi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib58" title=""><span class="ltx_text ltx_font_typewriter">58</span></a>]</cite> followed a two-stage approach. Thus, the researchers proposed a <span class="ltx_glossaryref" title="">Deep Neural Network (DNN)</span> composed of a perceptron network to determine the 2D poses and the hierarchical texture information for each view, and by a half-hourglass with skip connections which uses the mentioned information to infer the 3D pose. Showing the importance of sharing the texture information to improve the estimation of the 3D pose.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Amin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib2" title="">2</a>]</cite> for 3D single-person estimation, used the <span class="ltx_glossaryref" title="">Pictorial Structures (PS)</span> model to estimate the 2D poses and then, triangulation to recover the 3D pose. Although the employed method has demonstrated good performance on the HumanEva-I dataset, it’s dependent on the camera setup to learn the pairwise appearance terms.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">As for Remelli et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib65" title=""><span class="ltx_text ltx_font_typewriter">65</span></a>]</cite>, they presented an efficient method of reconstructing the pose without substantially increasing the computational cost. To do so, the authors introduced a faster version of the Direct Linear Transform method to lift the predicted 2D skeletons to 3D poses. The 2D poses are generated by exploring 3D geometry and camera projections.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">Solichah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib75" title=""><span class="ltx_text ltx_font_typewriter">75</span></a>]</cite> applied the OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib11" title="">11</a>]</cite> algorithm, which allows the implementation of a real-time keypoint detector for multi-persons on single images, to forecast the 2D poses. Then, used triangulation to achieve the final 3D pose. Although the method produced satisfactory results, it requires a proper calibration of the camera settings. The approach proposed by Kadkhodamohammadi and Padoy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib44" title=""><span class="ltx_text ltx_font_typewriter">44</span></a>]</cite> uses the following pipeline: estimation of the 2D poses using a single-view detector, match of the 2D poses across views using epipolar geometry and regress the 3D pose using a multilayer neural network composed of multiple stages. The authors have shown that the inclusion of distinct perspectives enhances the model’s performance. Nonetheless, one major drawback of their method is the high reliance on the camera calibration settings and the features of the single-view pose detector.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1">The study conducted by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wan et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib82" title=""><span class="ltx_text ltx_font_typewriter">82</span></a>]</cite> seeks to improve the correlation of 2D keypoints across multiple views by employing a Multi-view Fusion module and to refine the 3D pose estimation through the utilisation of Holistic Triangulation with anatomy constraints. The Multi-view Fusion module integrates pseudo-heatmaps from various views, created based on detected keypoints in the reference view, with the initial heatmap to produce a more precise heatmap. Holistic triangulation with anatomy constraints is designed to simultaneously correlate all views through the integration of a re-projection term based on multi-view geometric constraints and a <span class="ltx_glossaryref" title="">Principal Component Analysis (PCA)</span> reconstruction term to enforce anatomical consistency. Despite its rather good performance, the model encounters challenges in reconstructing poses for unseen movements and lacks adaptability to systems featuring a different number of cameras than those used during training.</p>
</div>
<div class="ltx_para" id="S4.SS1.p9">
<p class="ltx_p" id="S4.SS1.p9.1">As evidenced by previously revised methods, numerous methodologies rely on a fixed camera setup and are unable to generalise to new perspectives. To overcome this issue, Bartol et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib4" title="">4</a>]</cite> suggested a triangulation method based on stochasticity. Which after generating the 2D poses for each view, randomly chooses a subgroup of views and combines the corresponding 2D poses through triangulation to produce the 3D pose. This second step is repeated several times to create various hypotheses, to which a score is assigned to allow the computation of the weighted average of all hypotheses and consequently, choose the most appropriate hypothesis. Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib42" title=""><span class="ltx_text ltx_font_typewriter">42</span></a>]</cite> have proposed a method, called Probabilistic Triangulation, that approximates the camera pose distribution. The parameters of this distribution are initialised using 2D heatmaps estimated from the input RGB images and are then continuously updated using Monte Carlo sampling. This approach reduces the reliance on calibration parameters, making it possible to estimate the 3D pose in uncalibrated settings. Nonetheless, it still requires the model to be trained with calibrated data.</p>
</div>
<div class="ltx_para" id="S4.SS1.p10">
<p class="ltx_p" id="S4.SS1.p10.1">Nakatsuka and Komorita <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib61" title=""><span class="ltx_text ltx_font_typewriter">61</span></a>]</cite> developed a method to obtain robust 3D pose estimations in demanding environments with limited views and low-resolution data. In this type of environment, the occurrence of occlusions and the lack of visual information are very frequent, damaging the prediction of the pose, to overcome this challenge some researchers have used temporal consistency. Nonetheless, Nakatsuka and Komorita argue that imposing temporal consistency may suppress abrupt and tiny changes which might hinder the overall model’s performance. Thereby, they proposed a method composed of two components: one for regressing the 3D pose using images as input, and the other for refining the previous 3D pose estimation, utilising a gated temporal convolution network to correct just the keypoints predicted with poor confidence. The authors claim that their approach could be further optimised by employing a lightweight 3D pose estimator.</p>
</div>
<div class="ltx_para" id="S4.SS1.p11">
<p class="ltx_p" id="S4.SS1.p11.1">Xia and Zhang introduced VitPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib89" title=""><span class="ltx_text ltx_font_typewriter">89</span></a>]</cite>, a model composed of a CNN backbone used to capture the low-level features; the encoder part of the Vision Transformer to capture the long-range relationship between the human body joints in one perspective and their association with the joints in another perspective; and the Simple Feature Fusion Network, which allows to weightily fuse views to prevent views that give poor pose predictions from harming model performance. Hence, this method provided a more robust fusion step and also, showed that the exploration of long-distance relationships enables more exact 2D pose estimations. <cite class="ltx_cite ltx_citemacro_citet">Cai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib10" title="">10</a>]</cite> also explored the fusion of features. However, the researchers focus on fusing features from multiple frames with those from various perspectives. To achieve this, a transformer encoder is used to create a global feature that combines all the features from different views and frames. Then, a transformer decoder is used to fuse the global features with the features specific from each view, yielding a more meaningful set of features to estimate the 3D pose. The employment of this fusion scheme proved to be effective in mitigating the depth uncertainty impact, in addition to demonstrating the potential of methodologies that do not require prior knowledge of camera parameters.</p>
</div>
<div class="ltx_para" id="S4.SS1.p12">
<p class="ltx_p" id="S4.SS1.p12.1">The computational load imposed by <span class="ltx_glossaryref" title="">Convolutional Neural Network (CNN)</span>-based models may limit their application in real-world scenarios. To surpass this problem, Hwang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib36" title=""><span class="ltx_text ltx_font_typewriter">36</span></a>]</cite> designed a system consisting of several edge devices and a central server. In which each edge device was responsible for estimating the 2D pose of the received image using a <span class="ltx_glossaryref" title="">CNN</span> and then, sending it along with the respective timestamp to the central server. The central server, in turn, triangulates using <span class="ltx_glossaryref" title="">Direct Linear Transform (DLT)</span> the received 2D poses that were detected at roughly the same time to produce the 3D pose. Hence, this system distributes the computational burden of the CNN among edge devices and also reduces data traffic by transmitting only the 2D poses and timestamps rather than RGB images, making it more suitable for real-time applications in the real-world.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Methods under different supervision levels</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">The scarcity of labelled datasets significantly restricts the models’ outcomes. Therefore, unsupervised, weakly-supervised, semi-supervised and self-supervised methods have been proposed to combat the dependence on 3D labelled datasets.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">An example is the work of Rhodin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib67" title=""><span class="ltx_text ltx_font_typewriter">67</span></a>]</cite> which employed a semi-supervised method capable of learning from unlabelled images, a geometry-aware representation of the human body. Then, used some supervision to learn the mapping from the 3D geometry representation to the 3D pose. Inspired by this methodology, Kundu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib48" title=""><span class="ltx_text ltx_font_typewriter">48</span></a>]</cite> introduced an unsupervised method that uses a geometry-aware bottleneck to generate a comprehensible latent space for representing the 3D poses. Rochette et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib68" title=""><span class="ltx_text ltx_font_typewriter">68</span></a>]</cite>, on the other hand, explored a weakly-supervised method which uses a multi-view consistency loss and a re-projection consistency loss to determine the 3D pose using only one image.
Similarly, Ma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib56" title=""><span class="ltx_text ltx_font_typewriter">56</span></a>]</cite> also proposed a self-supervised approach in which the loss function leverages information from more than one view to completely disentangle the camera perspective from the 3D human body skeleton and consequently, surpass the projection ambiguity issue observed in a monocular setting. During their research, the authors concluded that the use of more views throughout the training process would further boost the performance of their network.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">In the research work carried out by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhao et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib99" title=""><span class="ltx_text ltx_font_typewriter">99</span></a>]</cite>, a new loss function, Triangulation Residual Loss, has been proposed, whose objective is to minimise the total distances between view rays and 3D pose estimation retrieved through triangulation. This novel loss function aims to enforce multi-view geometric consistency, facilitating the efficient self-supervised training of the model. As for Jenni and Favaro <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib40" title=""><span class="ltx_text ltx_font_typewriter">40</span></a>]</cite>, they proposed a pre-training technique using self-supervised learning, to determine the synchronisation between two images and whether one image is inverted horizontally in relation to the other, with the objective of learning a meaningful representation suitable to be used by a network to estimate the 3D pose. Also, the authors suggest removing the background for static images to prevent irrelevant features from being learnt during the self-supervised learning task. In addition, Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib53" title=""><span class="ltx_text ltx_font_typewriter">53</span></a>]</cite> also, introduced a method which relies only on two views to reconstruct the pose, without requiring to know any camera configurations. So, their method after extracting the 2D poses of each image, used a self-supervised 3D regression network able to produce virtual views using orthogonal projections of the human body, allowing the model to thoroughly understand the human body spatial structure and the transformations necessary to project one perspective onto another.
Concerning the work of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Yin et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib95" title=""><span class="ltx_text ltx_font_typewriter">95</span></a>]</cite>, a self-supervised method incorporating temporal convolution blocks and spatio-temporal attention mechanisms has been investigated. The methodology employed involved the exploration of re-projection and view consistency methods, which enabled an accurate retrieval of the 3D pose of infants without the need for 3D labels or definition of camera calibration parameters.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1">In contrast with the previous works, Feng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib29" title=""><span class="ltx_text ltx_font_typewriter">29</span></a>]</cite> proposed a method to enhance the data labelling process instead of trying to learn from the available unlabelled data or producing new synthetic data. Feng et al. have used an Active learning-based methodology (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.F3" title="Figure 3 ‣ 4.1.1 Methods under different supervision levels ‣ 4.1 Single-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">3</span></a>) to efficiently annotate the data without considerably increasing the overall computational cost. Moreover, the researchers have shown that this strategy employed whether alone or in conjunction with self-training can boost the labelling of the data and consequently, enhance the overall 3D pose estimation outcomes.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="486" id="S4.F3.g1" src="extracted/5699987/active_learning.jpg" width="503"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Active learning process. In this process, the model interactively selects some instances and asks the human annotator to label them. Then, the model is re-trained using these new labelled instances until it reaches a defined performance threshold or the budget limit.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p5">
<p class="ltx_p" id="S4.SS1.SSS1.p5.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.T3" title="Table 3 ‣ 4.1.1 Methods under different supervision levels ‣ 4.1 Single-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results reported in the revised works that have used Human3.6M dataset to evaluate the model’s performance. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1.SS1" title="A.1 Single-person 3D pose estimation ‣ Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">A.1</span></a> shows the outcomes obtained using single-person techniques on other datasets.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Table summarising the results reported in the revised publications for the Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib39" title=""><span class="ltx_text ltx_font_typewriter">39</span></a>]</cite></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.1">
<tr class="ltx_tr" id="S4.T3.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S4.T3.1.1.2" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.2.1">
<span class="ltx_p" id="S4.T3.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.2.1.1.1">Papers</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S4.T3.1.1.3" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.3.1">
<span class="ltx_p" id="S4.T3.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.1.1.1">Year</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1">MPJPE (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.2.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.1.1">
<span class="ltx_p" id="S4.T3.1.2.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Rhodin et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib67" title=""><span class="ltx_text ltx_font_typewriter">67</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.2.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.2.1">
<span class="ltx_p" id="S4.T3.1.2.2.1.1">2018</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.3">131.70</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.3.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.1.1">
<span class="ltx_p" id="S4.T3.1.3.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Tang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib79" title=""><span class="ltx_text ltx_font_typewriter">79</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.3.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.2.1">
<span class="ltx_p" id="S4.T3.1.3.2.1.1">2018</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.3">99.70</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.4.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.1.1">
<span class="ltx_p" id="S4.T3.1.4.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Kadkhodamohammadi and Padoy</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib44" title=""><span class="ltx_text ltx_font_typewriter">44</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.4.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.2.1">
<span class="ltx_p" id="S4.T3.1.4.2.1.1">2019</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.4.3">57.90</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.5.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.1.1">
<span class="ltx_p" id="S4.T3.1.5.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Kundu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib48" title=""><span class="ltx_text ltx_font_typewriter">48</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.5.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.2.1">
<span class="ltx_p" id="S4.T3.1.5.2.1.1">2020</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.5.3">56.10</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.6.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.6.1.1">
<span class="ltx_p" id="S4.T3.1.6.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Remelli et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib65" title=""><span class="ltx_text ltx_font_typewriter">65</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.6.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.6.2.1">
<span class="ltx_p" id="S4.T3.1.6.2.1.1">2020</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.6.3">30.20</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.7.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.7.1.1">
<span class="ltx_p" id="S4.T3.1.7.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Remelli et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib65" title=""><span class="ltx_text ltx_font_typewriter">65</span></a>]</cite>
<br class="ltx_break"/>(trained with extra data from the MPII Human Pose dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib3" title="">3</a>]</cite>)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.7.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.7.2.1">
<span class="ltx_p" id="S4.T3.1.7.2.1.1">2020</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.7.3">21.00</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.8.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.8.1.1">
<span class="ltx_p" id="S4.T3.1.8.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Jenni and Favaro</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib40" title=""><span class="ltx_text ltx_font_typewriter">40</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.8.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.8.2.1">
<span class="ltx_p" id="S4.T3.1.8.2.1.1">2021</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.8.3">64.90</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.9.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.9.1.1">
<span class="ltx_p" id="S4.T3.1.9.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Liu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib53" title=""><span class="ltx_text ltx_font_typewriter">53</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.9.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.9.2.1">
<span class="ltx_p" id="S4.T3.1.9.2.1.1">2021</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.9.3">22.50</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.10.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.10.1.1">
<span class="ltx_p" id="S4.T3.1.10.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Reddy et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib64" title=""><span class="ltx_text ltx_font_typewriter">64</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.10.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.10.2.1">
<span class="ltx_p" id="S4.T3.1.10.2.1.1">2021</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.10.3">18.70</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.11">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.11.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.11.1.1">
<span class="ltx_p" id="S4.T3.1.11.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wang and Sun</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib83" title=""><span class="ltx_text ltx_font_typewriter">83</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.11.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.11.2.1">
<span class="ltx_p" id="S4.T3.1.11.2.1.1">2022</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.11.3">67.20</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.12">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.12.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.12.1.1">
<span class="ltx_p" id="S4.T3.1.12.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib84" title=""><span class="ltx_text ltx_font_typewriter">84</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.12.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.12.2.1">
<span class="ltx_p" id="S4.T3.1.12.2.1.1">2022</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.12.3">31.17</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.13">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.13.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.13.1.1">
<span class="ltx_p" id="S4.T3.1.13.1.1.1"><cite class="ltx_cite ltx_citemacro_citet">Bartol et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib4" title="">4</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.13.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.13.2.1">
<span class="ltx_p" id="S4.T3.1.13.2.1.1">2022</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.13.3">29.10</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.14">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.14.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.14.1.1">
<span class="ltx_p" id="S4.T3.1.14.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Xia and Zhang</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib89" title=""><span class="ltx_text ltx_font_typewriter">89</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.14.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.14.2.1">
<span class="ltx_p" id="S4.T3.1.14.2.1.1">2022</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.14.3">17.00</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.15">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.15.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.15.1.1">
<span class="ltx_p" id="S4.T3.1.15.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ma et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib56" title=""><span class="ltx_text ltx_font_typewriter">56</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.15.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.15.2.1">
<span class="ltx_p" id="S4.T3.1.15.2.1.1">2023</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.15.3">75.10</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.16">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.16.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.16.1.1">
<span class="ltx_p" id="S4.T3.1.16.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Jiang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib42" title=""><span class="ltx_text ltx_font_typewriter">42</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.16.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.16.2.1">
<span class="ltx_p" id="S4.T3.1.16.2.1.1">2023</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.16.3">27.80</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.17">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.17.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.17.1.1">
<span class="ltx_p" id="S4.T3.1.17.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhao et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib99" title=""><span class="ltx_text ltx_font_typewriter">99</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.17.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.17.2.1">
<span class="ltx_p" id="S4.T3.1.17.2.1.1">2023</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.17.3">25.80</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.18">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.18.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.18.1.1">
<span class="ltx_p" id="S4.T3.1.18.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wan et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib82" title=""><span class="ltx_text ltx_font_typewriter">82</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T3.1.18.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.18.2.1">
<span class="ltx_p" id="S4.T3.1.18.2.1.1">2023</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.18.3">21.10</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.19">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S4.T3.1.19.1" style="width:125.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.19.1.1">
<span class="ltx_p" id="S4.T3.1.19.1.1.1"><cite class="ltx_cite ltx_citemacro_citet">Cai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib10" title="">10</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S4.T3.1.19.2" style="width:14.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.19.2.1">
<span class="ltx_p" id="S4.T3.1.19.2.1.1">2024</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.19.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.19.3.1">7.90</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Multi-person approaches</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">This section presents the solutions developed to determine 3D poses in multi-person scenarios.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Geometric constraint-based methods</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Belagiannis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite> extended the Amin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib2" title="">2</a>]</cite> approach for multi-person pose estimation, using triangulation of the detected 2D body joints in the various views to create the 3D discrete state space with all body parts hypotheses for all humans in the scene. Then, the <span class="ltx_glossaryref" title="">3D Pictorial Structures (3DPS)</span> model was used to infer the 3D human poses from the 3D body part hypotheses in the reduced state space. Later, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib7" title="">7</a>]</cite>, the researchers introduced temporal consistency to their previous approach to account for estimating human body pose over time. The researchers argue that temporal consistency is highly important for estimating multiple human poses, where the trajectory of each individual is closely related to their body posture, as it aids in penalising false positive candidates.
<cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib15" title="">15</a>]</cite> aimed to develop a system that can evaluate the similarity between the movements of an ordinary individual and those of someone with a motor dysfunction. To achieve this, the authors integrated temporal information into <span class="ltx_glossaryref" title="">3DPS</span> to enhance the 3D pose estimates.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib6" title="">6</a>]</cite>, the authors based their work on their previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite>. Nonetheless, they used different body parametrisations by exploring the use of a <span class="ltx_glossaryref" title="">3DPS</span> with several potential functions and also, to learn the model’s parameters was used a structured <span class="ltx_glossaryref" title="">Support Vector Machine (SVM)</span> to adequately weight the different potential functions.
Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib102" title=""><span class="ltx_text ltx_font_typewriter">102</span></a>]</cite> also, explored the <span class="ltx_glossaryref" title="">PS</span> model, but for human motion tracking. To accomplish that, the researchers first, subtracted the background, then, used <span class="ltx_glossaryref" title="">Flexible Mixtures of Parts (FMP)</span>, which is based on <span class="ltx_glossaryref" title="">PS</span>, for foreground learning to detect the human body parts and finally, to track the body parts, it was utilised <span class="ltx_glossaryref" title="">Annealed Particle Filter (APF)</span>.
The latest work by Belagiannis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib6" title="">6</a>]</cite>, as mentioned, relied on a structured <span class="ltx_glossaryref" title="">SVM</span> to optimise the model parameters. In contrast, Schwarcz and Pollard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib71" title=""><span class="ltx_text ltx_font_typewriter">71</span></a>]</cite>, to simplify the hyperparameters optimisation and improve the quality of the poses estimated with triangulation, used, to determine the 3D pose, a <span class="ltx_glossaryref" title="">Conditional Random Field (CRF)</span> as a factor graph, the 3D limb locations as variables, and the limb position priors, collision terms and temporal smoothing on those joints as factors. Furthermore, for determining the 2D poses for each view, Schwarcz and Pollard took advantage of the OpenPose library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib11" title="">11</a>]</cite>.
On the other hand, Ershadi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib27" title=""><span class="ltx_text ltx_font_typewriter">27</span></a>]</cite> employed a bottom-up approach starting by leveraging from the DeeperCut model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib38" title=""><span class="ltx_text ltx_font_typewriter">38</span></a>]</cite> to detect the 2D body parts. Next, a 3D search space is constructed with all detected joints and a <span class="ltx_glossaryref" title="">Gaussian Mixture Model (GMM)</span> is applied to cluster the points in order to obtain the number of people in the scene. To infer the 3D pose, a fully connected pairwise <span class="ltx_glossaryref" title="">CRF</span> with its pairwise terms defined in the 3D space and a loopy belief propagation are utilized.
To alleviate the computational cost of deep neural networks developed to estimate the 3D pose, Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib84" title=""><span class="ltx_text ltx_font_typewriter">84</span></a>]</cite> proposed LHPE-nets. The LHPE-nets incorporate a low-span network to increase the speed of the start of the training process for the 2D pose prediction as well as a residual deep neural network trained on low-resolution data which showed to enhance network scalability and performance when compared to a ResNet-34.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">To combat the inefficiency of <span class="ltx_glossaryref" title="">PS</span>-based models, Dong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib24" title="">24</a>]</cite> proposed a multi-way matching algorithm which leverages epipolar geometry, appearance similarity and cycle consistency to reduce the state space and eliminate false detections, resulting in a more efficient matching of 2D poses between views. Later, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib23" title="">23</a>]</cite>, the researchers extend their approach to also, track the poses by introducing temporal tracking and Riemannian Extended Kalman filtering. However, their methods present some limitations, like a 2D pose can be deemed an outlier if it only appears in one view, or if there are fast movements, the tracking algorithm is unreliable.
Thus, their method could not be used in applications requiring quick movement monitoring, such as sports tracking. Bridgeman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib9" title="">9</a>]</cite>, to improve the speed of the 2D pose association across views, employed a fast greedy algorithm based only on geometric relations and then, used triangulation to generate the 3D skeletons. Although, the presented method is faster than <span class="ltx_glossaryref" title="">PS</span>-based methods, in scenarios with a small number of cameras the use of <span class="ltx_glossaryref" title="">PS</span> for joint estimation has been proven to be more precise than triangulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib9" title="">9</a>]</cite>. Demonstrating the existing trade-off between speed and accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib23" title="">23</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib92" title=""><span class="ltx_text ltx_font_typewriter">92</span></a>]</cite> focus on the application of human-robot interaction. So, for that the authors need to conjugate two tasks: estimation of the 3D human skeleton and controlling of the robot movement. Focusing on the pose estimation part, Xu et al. used a 2D pose detector to determine the pose for each view, next, used a greedy algorithm to associate the 2D poses and finally, computed the 3D pose based on paired 2D poses. To enhance efficiency and avoid pose mispairing, the authors implemented a redundancy screening phase to remove redundant pairings, as well as iteration reversal which is used to double-check the validity of the current state; if the state fails this check, the iteration returns to its backup state.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1">To tackle the issue of swiftly and accurately predicting the 3D pose in crowded scenarios, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib14" title="">14</a>]</cite> used the Jonker-Volgenant algorithm to first, associate the foot joints across the various views, and then, used that information and the human body kinematics to find the matches for the rest of the joints. Lastly, to regress the 3D pose, the researchers refined the triangulation method by introducing <span class="ltx_glossaryref" title="">Maximum A Posteriori (MAP)</span> optimisation that takes into account the uncertainties of the 2D estimations and imposes the average lengths of the 3D bones.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p6">
<p class="ltx_p" id="S4.SS2.SSS1.p6.1">Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib35" title=""><span class="ltx_text ltx_font_typewriter">35</span></a>]</cite> proposed a method which allows a back-propagation of the gradients from the 3D estimation step to the 2D pose detection, enabling end-to-end training of the model. Also, another contribution is the proposed bottom-up dynamic matching algorithm that constructs 3D pose sub-spaces by projecting each pair of 2D poses using triangulation into 3D poses. The matching 3D poses are then identified using a distance-based clustering approach.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p7">
<p class="ltx_p" id="S4.SS2.SSS1.p7.1">As for Chu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib18" title="">18</a>]</cite>, the researchers introduced a method to lessen the computational burden imposed by the matching process. The approach consisted of the use of temporal consistency and part-aware measurement to be able to exploit previously obtained 3D poses to discover better 2D-3D correspondence across perspectives. Additionally, to remove possible outliers or noisy data, a joint filter is applied, improving the robustness of the 3D reconstruction.
On the other hand, Dehaeck et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib20" title="">20</a>]</cite> showed that it’s possible to obtain a reliable cross-view matching algorithm by relying only on geometric constraints, like the Sampson error which allows to compute the distance between predicted points from two distinct views. Furthermore, the authors claim that solving the matching problem between views requires at least three perspectives, however, if only two of them are free from occlusions, the trustworthiness of the matches drops.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Xu and Kitani</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib93" title=""><span class="ltx_text ltx_font_typewriter">93</span></a>]</cite> suggested employing multi-view geometry to reduce the solution space for cross-view matching. Thus, the authors established several requirements: each individual must be within the field of view of at least two cameras, no person can have more matches than the number of available cameras, and individuals from the same view cannot be matched. Then, to enhance the association of the 2D poses across the several views, a self-validation method is used, exploiting the correspondences with higher quality from all camera pairs. Lastly, the 3D poses from different pairs of cameras are triangulated to generate the final 3D pose, which is then refined via bundle adjustment. This methodology culminates in a method robust to distinct camera settings without requiring to know any calibration parameters.
Ershadi-Nasab et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">26</span></a>]</cite> adopted an adversarial learning methodology aiming to estimate the 3D pose without requiring any camera calibration. The model is composed of a generator which estimates the 3D pose and attempts to create estimates that the discriminator cannot differentiate from the ground-truth poses. Thus, the generator calculates the 2D Euclidean distance and 2D angular difference matrices before mapping each of them to the equivalent 3D version which are subsequently utilized to generate the 3D pose. Also, for the generator to become more robust to occlusions and improve the 3D pose estimation, a Procrustes analysis was used to allow the association of the poses over several viewpoints. The discriminator compares the 3D Euclidean distance and 3D angular difference matrices of the generated pose with the ground-truth and determines if they are identical.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p8">
<p class="ltx_p" id="S4.SS2.SSS1.p8.1">Other works have explored techniques to select the best views regarding occlusions to more accurately estimate the 3D pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib79" title=""><span class="ltx_text ltx_font_typewriter">79</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib28" title=""><span class="ltx_text ltx_font_typewriter">28</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib83" title=""><span class="ltx_text ltx_font_typewriter">83</span></a>]</cite>. Fan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib28" title=""><span class="ltx_text ltx_font_typewriter">28</span></a>]</cite> developed MAO-Pose, a self-supervised method capable of managing camera positions. Therefore, the various cameras are encouraged to choose the best perspective for the visibility of the joints and to diversify their choices of viewpoints in comparison to the perspectives chosen by the other cameras, in order to promote a broad range of views. Furthermore, a communication system called Consensus is introduced to allow the cameras to exchange their position information and assist the next camera in optimising its position plan by knowing how the other cameras are going to be placed. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib83" title=""><span class="ltx_text ltx_font_typewriter">83</span></a>]</cite>, on the other hand, introduced Smart-VPoseNet, a model that relies just on one view to reconstruct the 3D pose. However, the used perspective is chosen dynamically from all available viewpoints in a multi-view system. The choosing criteria that Smart-VPoseNet follows are the number of visible joints, the degree of stretch of the human body and the level of affinity between the perspective and the model. These three criteria can be used individually or in conjunction depending on the characteristics of the dataset. Furthermore, the authors claim that their methodology provides a basis for a possible two-view fusion system.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Voxel-based approaches</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib81" title=""><span class="ltx_text ltx_font_typewriter">81</span></a>]</cite>, Tu et al. introduced VoxelPose (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.F4" title="Figure 4 ‣ 4.2.2 Voxel-based approaches ‣ 4.2 Multi-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">4</span></a>), a method that completely avoids erroneous 2D pose associations of different views, by directly working on the 3D space. Thus, the model receives as input the images from the various perspectives and generates the corresponding 2D heatmaps. Next, these heatmaps are projected into the 3D space to form a 3D feature volume which serves as input for the <span class="ltx_glossaryref" title="">Cuboid Proposal Network (CPN)</span> that will output a 3D cuboid proposal for each person in the scene. Finally, finer-grained cuboids are created, to enable a more accurate estimation of the pose, and fed to the <span class="ltx_glossaryref" title="">Pose Regression Network (PRN)</span> to obtain the 3D poses.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="315" id="S4.F4.g1" src="extracted/5699987/voxelpose_img2.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Overview of the VoxelPose architecture (image adapted from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib81" title=""><span class="ltx_text ltx_font_typewriter">81</span></a>]</cite>)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">The robustness of the method against occlusions and the superior performance compared to previous approaches has attracted the community to explore this methodology. Thus, various subsequent works are based on this architecture, such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib94" title=""><span class="ltx_text ltx_font_typewriter">94</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib104" title=""><span class="ltx_text ltx_font_typewriter">104</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib98" title=""><span class="ltx_text ltx_font_typewriter">98</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib64" title=""><span class="ltx_text ltx_font_typewriter">64</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib103" title=""><span class="ltx_text ltx_font_typewriter">103</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Deng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib21" title="">21</a>]</cite> in order to make the VoxelPose model robust to domain shift have proposed to include a domain adaptation component, a dropout mechanism and a transferable parameter learning. These new additions will enable the learning of representative features across different domains, while, also, mitigating the adverse consequences of learning domain-specific information. Nonetheless, although it improves cross-domain efficiency and reduces the need for manually labelled data, the training overhead is quite high, restricting its applicability to settings requiring highly demanding processing processes, and this becomes even more pronounced as the number of perspectives increases.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1">On the other hand, Faster VoxelPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib94" title=""><span class="ltx_text ltx_font_typewriter">94</span></a>]</cite> focused on improving the VoxelPose’s speed by avoiding the use of 3D <span class="ltx_glossaryref" title="">CNN</span>. In order to do so, the feature volume belonging to each individual had to be identified, and then, re-projected to 2D space independently. However, the method is not robust to the decrease in the number of views, leading to a significant performance drop. Thus, to address that <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhuang and Zhou</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib104" title=""><span class="ltx_text ltx_font_typewriter">104</span></a>]</cite> proposed FasterVoxelPose+, introducing two changes to the Faster VoxelPose model: a <span class="ltx_glossaryref" title="">Depth-wise Projection Decay (DPD)</span> and an <span class="ltx_glossaryref" title="">Encoder-Decoder Network (EDN)</span>. <span class="ltx_glossaryref" title="">DPD</span> is a projection technique that adds extra depth information to the projection of the 2D pose heatmaps into the 3D voxel features. While <span class="ltx_glossaryref" title="">EDN</span> consists of a 2D <span class="ltx_glossaryref" title="">CNN</span> able to combine multi-scale information, allowing 2D re-projected voxel features to be processed in different phases and consequently, reaching more accurate 3D bounding boxes estimates and 2D poses.
In the case of VoxelTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib98" title=""><span class="ltx_text ltx_font_typewriter">98</span></a>]</cite>, the authors expanded on the VoxelPose approach to include tracking 3D postures throughout time. Furthermore, a more robust 2D backbone network and extra 3D heatmap supervision of all joints are added to try to improve the 3D pose estimations and, to enhance inference speed, sparse 3D <span class="ltx_glossaryref" title="">CNN</span> s are exploited. However, when the number of cameras decreases, all of these approaches still suffer a decline in performance, with this effect being less pronounced in FasterVoxelPose+. TesseTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib64" title=""><span class="ltx_text ltx_font_typewriter">64</span></a>]</cite> is, also, less influenced by this, since it integrates temporal information via the 4D spatio-temporal <span class="ltx_glossaryref" title="">CNN</span>. This makes the model more resilient to occlusions, better at predicting joint locations, and able to cope with appearance ambiguities in just one frame.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p5">
<p class="ltx_p" id="S4.SS2.SSS2.p5.1">Regarding the work of Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib103" title=""><span class="ltx_text ltx_font_typewriter">103</span></a>]</cite>, the VoxelPose approach was also, followed to compute for each human proposal, the respective feature volume. Nonetheless, as their objective was to create a reliable method to work in crowded environments, the authors have proposed a three-stage strategy to refine the 3D poses. Thus, the initial step was to generate finer-grained and narrower feature volumes in the region around each human proposal. In the second-stage, the Voxel Hourglass Network used those feature volumes to generate 3D heatmaps and tag-maps. Then, a 3D Associative Embedding was employed to combine the information provided by the heatmaps and tag-maps to produce a coarse 3D pose. In the third-stage, a refinement layer is utilised to refine the 3D pose given by the previous step. The authors demonstrated that the incorporation of 3D tag-maps resulted in the elimination of undesired joints coming from other people in the scene which, consequently, lowered the amount of mismatched joints.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p6">
<p class="ltx_p" id="S4.SS2.SSS2.p6.1">Additionally, inspired by VoxelPose, Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib86" title=""><span class="ltx_text ltx_font_typewriter">86</span></a>]</cite> introduced MvP, a transformer-based model for solving the multi-person 3D pose estimation task. This method directly predicted the 3D skeleton by encoding the joints as learnable query embeddings and joining them with the features extracted from the input images. Furthermore, to increase the model’s performance, the suggested transformer used a projective attention mechanism based on geometrical information in conjunction with a RayConv operation to precisely fuse the cross-view data for each joint. The authors point out some drawbacks of their approach which are the lack of robustness to generalize for new camera perspectives and a high demand for training data. Another transformer-based approach, based on VoxelPose is <span class="ltx_glossaryref" title="">Volumetric Transformer Pose Estimator (VTP)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib16" title="">16</a>]</cite>. The creation of this model aimed to solve the high computational cost associated with applying self-attention directly to volumetric representations. To overcome this, <span class="ltx_glossaryref" title="">Sparse Sinkhorn Attention (SSA)</span> mechanism was exploited as it provides quasi-global local attention. To do this, the input sequence is divided into blocks, then the correlation between them is learnt, and finally, <span class="ltx_glossaryref" title="">SSA</span> uses this information to understand how to reorganise and classify the blocks. The authors have concluded that by applying <span class="ltx_glossaryref" title="">SSA</span> the memory consumption was reduced, allowing to efficiently apply self-attention to the volumetric representations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Plane sweep stereo-based models</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">To create a faster and less computationally expensive algorithm compared to VoxelPose, Lin and Lee <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib50" title=""><span class="ltx_text ltx_font_typewriter">50</span></a>]</cite> proposed a framework based on plane sweep stereo to add person-level and joint-level depth information to the 2D poses generated for each viewpoint, see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.F5" title="Figure 5 ‣ 4.2.3 Plane sweep stereo-based models ‣ 4.2 Multi-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">5</span></a>. Then, the 2D poses and respective depth information are back-projected to create the 3D pose, reducing the computational burden of pose association.
Extending this approach to work with unlabelled datasets, de França Silva et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib30" title=""><span class="ltx_text ltx_font_typewriter">30</span></a>]</cite> changed the loss function used on the previously presented plane sweep stereo method to be able to learn the 3D poses in an unsupervised way. Thus, instead of computing the difference between the generated 3D pose and the ground-truth, this novel loss function employs re-projection error between the 2D projection of the 3D pose with the 2D pose estimated for that perspective. The authors besides demonstrating the potential of this approach to learn from data without annotations, also, showed the potential of Adabelief to provide quicker convergences and better performances than the Adam optimiser. Later <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">de França Silva et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib31" title=""><span class="ltx_text ltx_font_typewriter">31</span></a>]</cite> extended their work by exploring a matching method that uses ground points related to each individual to match the target with the reference view. Additionally, the authors incorporated the smooth <math alttext="L_{1}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.1.m1.1"><semantics id="S4.SS2.SSS3.p1.1.m1.1a"><msub id="S4.SS2.SSS3.p1.1.m1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.p1.1.m1.1.1.2" xref="S4.SS2.SSS3.p1.1.m1.1.1.2.cmml">L</mi><mn id="S4.SS2.SSS3.p1.1.m1.1.1.3" xref="S4.SS2.SSS3.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.1.m1.1b"><apply id="S4.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1.2">𝐿</ci><cn id="S4.SS2.SSS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.1.m1.1c">L_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.1.m1.1d">italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> loss for computing the re-projection error of the 2D poses. These modifications yielded a significant performance enhancement compared to the previous version, while also showcasing the potential of this method to cope with unlabeled data. However, it is noteworthy that this approach necessitates the knowledge of camera parameters in order to calculate the loss.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib101" title=""><span class="ltx_text ltx_font_typewriter">101</span></a>]</cite> also, took advantage of the plane sweep stereo method to get the depth score matrices, and subsequently obtain the 3D pose by back-projecting the 2D poses with the corresponding depth information. However, to optimise the prediction of all joints, particularly those most affected by occlusions or other external factors that may impair their detection, the authors employed an attention channel mechanism and an optimal calibration method based on the dependency relationship between the joints and the individual. This strategy promoted channel-wise feature re-calibration by adjusting the weights given to each channel, allowing the model to understand the long dependency relationship between joints and improve the detection of the most difficult joints.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="292" id="S4.F5.g1" src="extracted/5699987/plane_sweep_stereo.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Overview of the plane sweep stereo-based method followed by Lin and Lee (image adapted from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib50" title=""><span class="ltx_text ltx_font_typewriter">50</span></a>]</cite>)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#S4.T4" title="Table 4 ‣ 4.2.3 Plane sweep stereo-based models ‣ 4.2 Multi-person approaches ‣ 4 Multi-view approaches ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">4</span></a> reports the results for all revised works that have used Campus or Shelf dataset to assess the performance of their model. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1.SS2" title="A.2 Multi-person 3D pose estimation ‣ Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">A.2</span></a> presents the outcomes obtained with multi-person approaches on other datasets.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Table summarising the results reported in the revised publications for the Campus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite> and Shelf <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite> datasets</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.2">
<tr class="ltx_tr" id="S4.T4.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.2.2.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.3.1">Papers</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.2.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.4.1">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1">Campus — PCP (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T4.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1">Shelf — PCP (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.m1.1"><semantics id="S4.T4.2.2.2.1.m1.1a"><mo id="S4.T4.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T4.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><ci id="S4.T4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.1.1">Actor 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.2.1">Actor 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.3.1">Actor 3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.4"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.4.1">Average</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.5"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.5.1">Actor 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.6"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.6.1">Actor 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.7.1">Actor 3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.8"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.8.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.4.1"><cite class="ltx_cite ltx_citemacro_citet">Belagiannis et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.2">2014</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.3">82.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.4">72.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.5">73.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.6">75.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.7">66.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.8">65.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.9">83.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.10">71.30</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.5.1"><cite class="ltx_cite ltx_citemacro_citet">Belagiannis et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib7" title="">7</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.2">2015</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.3">83.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.4">73.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.5">78.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.6">78.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.7">75.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.8">67.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.9">86.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.10">76.00</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.6.1"><cite class="ltx_cite ltx_citemacro_citet">Belagiannis et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib6" title="">6</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.6.2">2016</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.6.3">93.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.6.4">75.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.6.5">84.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.6.6">84.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.6.7">75.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.6.8">69.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.6.9">87.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.6.10">77.51</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.7.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Schwarcz and Pollard</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib71" title=""><span class="ltx_text ltx_font_typewriter">71</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.2">2018</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.3">86.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.4">82.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.5">88.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.6">85.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.7">88.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.8">85.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.9">91.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.10">88.92</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.8.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ershadi-Nasab et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib27" title=""><span class="ltx_text ltx_font_typewriter">27</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.2">2018</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.3">94.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.4">92.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.5">84.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.6">90.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.7">93.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.8">75.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.9">94.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.10">87.99</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.9.1"><cite class="ltx_cite ltx_citemacro_citet">Bridgeman et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib9" title="">9</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.9.2">2019</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.9.3">91.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.9.4">92.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.9.5">93.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.9.6">92.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.9.7">99.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.9.8">92.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.9.9">97.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.9.10">96.73</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.10.1"><cite class="ltx_cite ltx_citemacro_citet">Dong et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib24" title="">24</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.2">2019</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.3">97.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.4">93.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.5">98.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.6">96.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.7">98.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.8">94.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.9">97.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.10">96.90</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.11.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Tu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib81" title=""><span class="ltx_text ltx_font_typewriter">81</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.11.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.11.3">97.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.11.4">93.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.11.5">98.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.11.6">96.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.11.7">99.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.11.8">94.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.11.9">97.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.11.10">97.00</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.12.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Huang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib35" title=""><span class="ltx_text ltx_font_typewriter">35</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.12.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.12.3">97.96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.12.4">94.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.12.5">97.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.12.6">96.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.12.7">98.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.12.8">96.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.12.9">97.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.12.10">97.39</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.13.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib86" title=""><span class="ltx_text ltx_font_typewriter">86</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.13.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.13.3">98.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.13.4">94.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.13.5">97.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.13.6">96.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.13.7">99.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.13.8">95.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.13.9">97.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.13.10">97.40</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.14">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.14.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib98" title=""><span class="ltx_text ltx_font_typewriter">98</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.14.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.14.3">98.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.14.4">93.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.14.5">98.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.14.6">96.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.14.7">98.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.14.8">94.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.14.9">97.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.14.10">97.10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.15">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.15.1"><cite class="ltx_cite ltx_citemacro_citet">Chu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib18" title="">18</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.15.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.15.3">98.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.15.4">93.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.15.5">98.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.15.6">96.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.15.7">99.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.15.8">95.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.15.9">97.64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.15.10">97.39</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.16">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.16.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Lin and Lee</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib50" title=""><span class="ltx_text ltx_font_typewriter">50</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.16.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.16.3">98.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.16.4">93.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.16.5">99.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.16.6">97.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.16.7">99.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.16.8">96.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.16.9">98.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.16.10">97.90</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.17">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.17.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Reddy et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib64" title=""><span class="ltx_text ltx_font_typewriter">64</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.17.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.17.3">97.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.17.4"><span class="ltx_text ltx_font_bold" id="S4.T4.2.17.4.1">95.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.17.5">99.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.17.6">97.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.17.7">99.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.17.8">96.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.17.9">98.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.17.10">98.20</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.18">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.18.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ershadi Nasab et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">26</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.18.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.18.3">98.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.18.4">95.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.18.5">98.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.18.6">97.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.18.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.18.7.1">99.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.18.8">94.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.18.9">98.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.18.10">97.69</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.19">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.19.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">de França Silva et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib30" title=""><span class="ltx_text ltx_font_typewriter">30</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.19.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.19.3">96.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.19.4">87.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.19.5">88.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.19.6">91.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.19.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.19.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.19.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.19.10">—</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.20">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.20.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ye et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib94" title=""><span class="ltx_text ltx_font_typewriter">94</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.20.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.20.3">96.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.20.4">94.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.20.5">97.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.20.6">96.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.20.7">99.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.20.8">96.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.20.9">97.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.20.10">97.60</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.21">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.21.1"><cite class="ltx_cite ltx_citemacro_citet">Dong et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib23" title="">23</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.21.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.21.3">97.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.21.4">93.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.21.5">98.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.21.6">96.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.21.7">98.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.21.8">94.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.21.9">97.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.21.10">96.90</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.22">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.22.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhou et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib101" title=""><span class="ltx_text ltx_font_typewriter">101</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.22.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.22.3">98.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.22.4">94.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.22.5">99.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.22.6">97.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.22.7">99.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.22.8"><span class="ltx_text ltx_font_bold" id="S4.T4.2.22.8.1">97.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.22.9">98.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.22.10"><span class="ltx_text ltx_font_bold" id="S4.T4.2.22.10.1">98.30</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.23">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.23.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Xu and Kitani</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib93" title=""><span class="ltx_text ltx_font_typewriter">93</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.23.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.23.3"><span class="ltx_text ltx_font_bold" id="S4.T4.2.23.3.1">99.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.23.4">94.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.23.5"><span class="ltx_text ltx_font_bold" id="S4.T4.2.23.5.1">99.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.23.6"><span class="ltx_text ltx_font_bold" id="S4.T4.2.23.6.1">97.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.23.7">99.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.23.8">95.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.23.9"><span class="ltx_text ltx_font_bold" id="S4.T4.2.23.9.1">98.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.23.10">97.80</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.24">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.24.1"><cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib16" title="">16</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.24.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.24.3">97.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.24.4">93.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.24.5">98.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.24.6">96.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.24.7">99.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.24.8">95.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.24.9">97.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.24.10">97.30</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.25">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.25.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhuang and Zhou</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib104" title=""><span class="ltx_text ltx_font_typewriter">104</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.25.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.25.3">97.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.25.4">93.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.25.5">98.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.25.6">96.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.25.7">99.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.25.8">96.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.25.9">97.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.25.10">97.70</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.26">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.26.1"><cite class="ltx_cite ltx_citemacro_citet">Deng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib21" title="">21</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.26.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.26.3">85.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.26.4">86.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.26.5">78.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.26.6">83.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.26.7">96.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.26.8">94.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.26.9">97.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.26.10">96.10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.27">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.27.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">de França Silva et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib31" title=""><span class="ltx_text ltx_font_typewriter">31</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.27.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.27.3">98.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.27.4">93.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.27.5">98.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.27.6">96.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.27.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.27.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.27.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.27.10">—</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.28">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.2.28.1"><cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib15" title="">15</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.28.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.28.3">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.28.4">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.28.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.28.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.28.7">98.60</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.28.8">95.80</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.28.9">97.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.28.10">97.40</td>
</tr>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Multi-modal approaches</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section presents methods that have used other sensors rather than RGB cameras to estimate the 3D pose.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Different type of cameras</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Some researchers have tackled the 3D pose estimation task using types of camera sensors, such as RGB-D cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib37" title=""><span class="ltx_text ltx_font_typewriter">37</span></a>]</cite>, <span class="ltx_glossaryref" title="">ToF</span> cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib33" title=""><span class="ltx_text ltx_font_typewriter">33</span></a>]</cite>, or 360º cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib73" title=""><span class="ltx_text ltx_font_typewriter">73</span></a>]</cite> which provide images that differ from the typical RGB images.
These types of cameras can capture more information which can be advantageous for the estimation of the pose. For example, Shere et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib73" title=""><span class="ltx_text ltx_font_typewriter">73</span></a>]</cite> created a method to determine the 3D pose using two 360º cameras. The authors state that this type of camera is especially advantageous since it can capture the whole scene without requiring a huge number of cameras while simultaneously providing accurate depth information. Thus, they used temporal information to associate the 2D poses and mitigate inaccuracies. Although their method has been developed to be used with data outputted by 360º cameras, data from other types of cameras may also be used, however, it will require the data to go through a costly re-projection step.
In the case of Carraro et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib12" title="">12</a>]</cite>, they have estimated the 3D human pose using an array of <span class="ltx_glossaryref" title="">RGB-D</span> cameras. This allowed them to leverage depth information to compute the 3D pose from the 2D poses generated by using a single-view pose estimation algorithm and the cameras’ extrinsic calibration parameters. Additionally, the authors used Kalman Filters to track the poses over time. The developed system thus allows the estimation of multiple people in real-time. Hwang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib37" title=""><span class="ltx_text ltx_font_typewriter">37</span></a>]</cite> have also, taken advantage of the depth information given by the RGB-D cameras to optimise the process of the cross-view matching of the 2D poses. Then, to be able to deploy this method in real-world, the authors have employed a system similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib36" title=""><span class="ltx_text ltx_font_typewriter">36</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Regarding the work of Hassan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib33" title=""><span class="ltx_text ltx_font_typewriter">33</span></a>]</cite>, the authors have used <span class="ltx_glossaryref" title="">ToF</span> cameras which provide infra-red intensity images along with depth information. The use of infra-red intensity images allows the system to be more robust to occlusions, material reflectivity, and lighting conditions in comparison to systems that utilise RGB images. Besides, the authors have proven that the use of intensity images as input is more reliable for estimating the 2D poses and subsequently, projecting them to construct the 3D pose than the depth images. However, in case a joint cannot be determined, the model takes advantage of the depth information to get the position of the missing joint.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Wireless sensors</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">On the other hand, some researchers have opted to use other types of sensors which can overcome some limitations vision-based methods suffer. Thus, the use of wireless communications to complete this task has been explored, because they aren’t affected by occlusions, privacy concerns or bad illumination conditions which are some challenges vision-based methods have to face <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib77" title=""><span class="ltx_text ltx_font_typewriter">77</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib91" title=""><span class="ltx_text ltx_font_typewriter">91</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib90" title=""><span class="ltx_text ltx_font_typewriter">90</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Typically, systems designed to rebuild 3D poses using wireless signals, such as <span class="ltx_glossaryref" title="">RF</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib77" title=""><span class="ltx_text ltx_font_typewriter">77</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib91" title=""><span class="ltx_text ltx_font_typewriter">91</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib90" title=""><span class="ltx_text ltx_font_typewriter">90</span></a>]</cite> or Wi-Fi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib87" title=""><span class="ltx_text ltx_font_typewriter">87</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib66" title=""><span class="ltx_text ltx_font_typewriter">66</span></a>]</cite>, include a camera that is synchronised with the wireless signals. Nonetheless, the researchers only used the camera data to produce ground-truth 3D poses with which to compare the skeletons generated using wireless signals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib87" title=""><span class="ltx_text ltx_font_typewriter">87</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib66" title=""><span class="ltx_text ltx_font_typewriter">66</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib77" title=""><span class="ltx_text ltx_font_typewriter">77</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib91" title=""><span class="ltx_text ltx_font_typewriter">91</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib90" title=""><span class="ltx_text ltx_font_typewriter">90</span></a>]</cite>. The typical pipeline for approaches that use these types of signals starts with transforming the input signal into images by, for example, extracting the channel-state information or the angle of arrival spectrums. Then, a method from the computer vision domain is employed to obtain the final 3D pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib87" title=""><span class="ltx_text ltx_font_typewriter">87</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib66" title=""><span class="ltx_text ltx_font_typewriter">66</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib77" title=""><span class="ltx_text ltx_font_typewriter">77</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib91" title=""><span class="ltx_text ltx_font_typewriter">91</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib90" title=""><span class="ltx_text ltx_font_typewriter">90</span></a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In summary, most existing methods use a three-step methodology where first, the 2D pose is regressed from the input images, then, the 2D poses predicted for the different views are associated and finally, the 3D pose is constructed. However, this type of strategy can lead to unreliable 3D poses due to mismatches and erroneous 2D poses. Therefore, some methods have surpassed this problem by completely avoiding this step and working directly on the 3D space <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib81" title=""><span class="ltx_text ltx_font_typewriter">81</span></a>]</cite>. Nevertheless, operations in the volumetric space become more computationally expensive, which might restrict their application in real-world settings. On the other hand, another methodology that has been utilised to combat this problem is the selection of views. So, the model only uses, to predict the pose, the views that provide better visualisation of the human subjects. Consequently, by using just some of the available views, the computational burden is also reduced.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Another limitation that most proposed methods still have to face is the dependence on the camera setup and camera’s calibration parameters, making most existing models unable to generalise to new viewpoints. To surpass this, researchers have exploited geometric multi-view consistency, stochasticity-based triangulation, adversarial learning and self-supervised learning methods.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Regarding the scarcity of 3D labelled datasets, methods that learn using a low level of supervision have been suggested to address this challenge. Nonetheless, some of those methods still depend on some 2D labelled data. On the other hand, the use of active learning has also, been suggested to overcome this issue. This strategy allows to dynamically acquire labels for unlabelled instances to boost model accuracy while requiring the least amount of annotation labour feasible. Nevertheless, only one work was found to employ this strategy, thus, more research is still required to determine the full potential of this technique for this task.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">Even though many works have focused on solving this problem, the occurrence of occlusion still limits the model’s performance in many cases.
Temporal consistency has been widely used to track poses over time and also, to overcome the occlusion problem. Nonetheless, as pointed out by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib61" title=""><span class="ltx_text ltx_font_typewriter">61</span></a>]</cite>, the use of this approach might suppress sudden and minor movements which can negatively impact the model’s performance and also make it unsuitable for applications where the detection of those movements might be crucial, such as surveillance systems. Besides temporal consistency, the long-range dependencies between joints and the person, and the use of depth information have been proposed to improve the model’s robustness against occlusions.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">Finally, the use of more sophisticated types of cameras that can give additional information beyond RGB images has been shown to be able to provide reliable 3D pose estimations. These types of cameras have not yet been very explored, nonetheless, their employment could be advantageous because, according to the found works, the <span class="ltx_glossaryref" title="">RGB-D</span>, <span class="ltx_glossaryref" title="">ToF</span> and 360º cameras, can directly provide depth information. By providing depth information directly as input, the computational cost associated with models using depth information can be substantially reduced, as the need for depth calculation will no longer be required. Regarding other types of signals, some researchers have explored the use of wireless signals to estimate the 3D pose. However, the association of information retrieved by wireless sensors with visual information given by cameras has not yet been investigated, leaving space for an innovative multi-modal approach combining both domains.</p>
</div>
<div class="ltx_para" id="S6.p6">
<p class="ltx_p" id="S6.p6.1">Thus, it’s possible to conclude that, although various techniques have been proposed to try to overcome most challenges inherent to the estimation of the 3D pose, none of the existing approaches can balance high accuracy, quick inference, and low computational cost. So, the best model depends on the desired application, due to the trade-off between performance and complexity. Therefore, more research work is still needed in this area to find a suitable methodology for different application scenarios. The combination of a multi-modal approach with active learning may be the future for an efficient and accurate solution.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work has received funding from the European Union’s Horizon Europe research and innovation programme under the Grant Agreement 101094831 - CONVERGE project and by National Funds through the Portuguese funding agency, FCT-Foundation for Science and Technology Portugal, a PhD Grant Number 2023.02851.BD.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Datasets - Benchmarking</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">This section includes a summary of the results reported in the reviewed publications.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Single-person 3D pose estimation</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1.T5" title="Table 5 ‣ A.1 Single-person 3D pose estimation ‣ Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">5</span></a>, Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1.T6" title="Table 6 ‣ A.1 Single-person 3D pose estimation ‣ Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">6</span></a>, Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1.T7" title="Table 7 ‣ A.1 Single-person 3D pose estimation ‣ Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">7</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1.T8" title="Table 8 ‣ A.1 Single-person 3D pose estimation ‣ Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">8</span></a> present the results for the dataset MPI-INF-3DHP, KTH Multiview Football II, HumanEva and Total Capture, respectively.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance of the revised methods in the MPI-INF-3DHP dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T5.3">
<tr class="ltx_tr" id="A1.T5.3.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="A1.T5.3.3.4" style="width:71.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.3.3.4.1">
<span class="ltx_p" id="A1.T5.3.3.4.1.1"><span class="ltx_text ltx_font_bold" id="A1.T5.3.3.4.1.1.1">Papers</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.3.3.5"><span class="ltx_text ltx_font_bold" id="A1.T5.3.3.5.1">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1">PCK (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.1.1.1.1.m1.1"><semantics id="A1.T5.1.1.1.1.m1.1a"><mo id="A1.T5.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.m1.1b"><ci id="A1.T5.1.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T5.2.2.2.1">AUC (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.2.2.2.1.m1.1"><semantics id="A1.T5.2.2.2.1.m1.1a"><mo id="A1.T5.2.2.2.1.m1.1.1" stretchy="false" xref="A1.T5.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.1.m1.1b"><ci id="A1.T5.2.2.2.1.m1.1.1.cmml" xref="A1.T5.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.3.3.3"><span class="ltx_text ltx_font_bold" id="A1.T5.3.3.3.1">MPJPE (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="A1.T5.3.3.3.1.m1.1"><semantics id="A1.T5.3.3.3.1.m1.1a"><mo id="A1.T5.3.3.3.1.m1.1.1" stretchy="false" xref="A1.T5.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.3.1.m1.1b"><ci id="A1.T5.3.3.3.1.m1.1.1.cmml" xref="A1.T5.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T5.3.4.1" style="width:71.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.3.4.1.1">
<span class="ltx_p" id="A1.T5.3.4.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Kundu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib48" title=""><span class="ltx_text ltx_font_typewriter">48</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.4.2">
<span class="ltx_ERROR undefined" id="A1.T5.3.4.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib48" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.4.3"><span class="ltx_text ltx_font_bold" id="A1.T5.3.4.3.1">81.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.4.4"><span class="ltx_text ltx_font_bold" id="A1.T5.3.4.4.1">52.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.4.5"><span class="ltx_text ltx_font_bold" id="A1.T5.3.4.5.1">89.80</span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T5.3.5.1" style="width:71.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.3.5.1.1">
<span class="ltx_p" id="A1.T5.3.5.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib84" title=""><span class="ltx_text ltx_font_typewriter">84</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.5.2">
<span class="ltx_ERROR undefined" id="A1.T5.3.5.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib84" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.5.3">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.5.4">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.5.5">112.36</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T5.3.6.1" style="width:71.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.3.6.1.1">
<span class="ltx_p" id="A1.T5.3.6.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wang and Sun</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib83" title=""><span class="ltx_text ltx_font_typewriter">83</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.6.2">
<span class="ltx_ERROR undefined" id="A1.T5.3.6.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib83" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.6.3">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.6.4">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.6.5">94.70</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T5.3.7.1" style="width:71.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.3.7.1.1">
<span class="ltx_p" id="A1.T5.3.7.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ma et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib56" title=""><span class="ltx_text ltx_font_typewriter">56</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.7.2">
<span class="ltx_ERROR undefined" id="A1.T5.3.7.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib56" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.7.3">74.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.7.4">40.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.7.5">—</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="A1.T5.3.8.1" style="width:71.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.3.8.1.1">
<span class="ltx_p" id="A1.T5.3.8.1.1.1"><cite class="ltx_cite ltx_citemacro_citet">Cai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib10" title="">10</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.3.8.2">
<span class="ltx_ERROR undefined" id="A1.T5.3.8.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib10" title="">2024</a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.3.8.3">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.3.8.4">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.3.8.5">5.40</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="A1.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance of the revised methods in the KTH Multiview Football II dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T6.1">
<tr class="ltx_tr" id="A1.T6.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="A1.T6.1.1.2" rowspan="2" style="width:79.7pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.1.2.1">
<span class="ltx_p" id="A1.T6.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.2.1.1.1">Papers</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.3.1">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.4.1"># Cameras</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="A1.T6.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1">PCP (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T6.1.1.1.1.m1.1"><semantics id="A1.T6.1.1.1.1.m1.1a"><mo id="A1.T6.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T6.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T6.1.1.1.1.m1.1b"><ci id="A1.T6.1.1.1.1.m1.1.1.cmml" xref="A1.T6.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T6.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.2.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.2.1.1">Upper arms</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.2.2"><span class="ltx_text ltx_font_bold" id="A1.T6.1.2.2.1">Lower arms</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.2.3"><span class="ltx_text ltx_font_bold" id="A1.T6.1.2.3.1">Upper Legs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.2.4"><span class="ltx_text ltx_font_bold" id="A1.T6.1.2.4.1">Lower Legs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.2.5"><span class="ltx_text ltx_font_bold" id="A1.T6.1.2.5.1">All parts (average)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T6.1.3.1" rowspan="2" style="width:79.7pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.3.1.1">
<span class="ltx_p" id="A1.T6.1.3.1.1.1"><cite class="ltx_cite ltx_citemacro_citet">Belagiannis et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.2" rowspan="2"><span class="ltx_text" id="A1.T6.1.3.2.1"><span class="ltx_ERROR undefined" id="A1.T6.1.3.2.1.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">2014</a></cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.3">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.4">64.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.5">50.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.6">75.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.7">66.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.8">63.80</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.4">
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.1">3</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2">68.00</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.3">56.00</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.4">78.00</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.5">70.00</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.6">68.00</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T6.1.5.1" rowspan="2" style="width:79.7pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.5.1.1">
<span class="ltx_p" id="A1.T6.1.5.1.1.1"><cite class="ltx_cite ltx_citemacro_citet">Belagiannis et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib6" title="">6</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.5.2" rowspan="2"><span class="ltx_text" id="A1.T6.1.5.2.1"><span class="ltx_ERROR undefined" id="A1.T6.1.5.2.1.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib6" title="">2016</a></cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.5.3">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.5.4">96.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.5.5">68.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.5.6">98.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.5.7">88.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.5.8">87.50</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.6">
<td class="ltx_td ltx_align_center" id="A1.T6.1.6.1">3</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.6.2">98.00</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.6.3">72.00</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.6.4">99.00</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.6.5">92.00</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.6.6">90.30</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T6.1.7.1" style="width:79.7pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.7.1.1">
<span class="ltx_p" id="A1.T6.1.7.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ershadi-Nasab et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib27" title=""><span class="ltx_text ltx_font_typewriter">27</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.7.2">
<span class="ltx_ERROR undefined" id="A1.T6.1.7.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib27" title=""><span class="ltx_text ltx_font_typewriter">2018</span></a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.7.3">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.7.4">97.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.7.5">94.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.7.6"><span class="ltx_text ltx_font_bold" id="A1.T6.1.7.6.1">100.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.7.7">99.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.7.8">98.14</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="A1.T6.1.8.1" style="width:79.7pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.8.1.1">
<span class="ltx_p" id="A1.T6.1.8.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ershadi Nasab et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">26</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.8.2">
<span class="ltx_ERROR undefined" id="A1.T6.1.8.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">2021</span></a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.8.3">3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.8.4"><span class="ltx_text ltx_font_bold" id="A1.T6.1.8.4.1">100.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.8.5"><span class="ltx_text ltx_font_bold" id="A1.T6.1.8.5.1">99.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.8.6"><span class="ltx_text ltx_font_bold" id="A1.T6.1.8.6.1">100.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.8.7"><span class="ltx_text ltx_font_bold" id="A1.T6.1.8.7.1">99.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.8.8"><span class="ltx_text ltx_font_bold" id="A1.T6.1.8.8.1">99.80</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="A1.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>3D error (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="A1.T7.2.m1.1"><semantics id="A1.T7.2.m1.1b"><mo id="A1.T7.2.m1.1.1" stretchy="false" xref="A1.T7.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A1.T7.2.m1.1c"><ci id="A1.T7.2.m1.1.1.cmml" xref="A1.T7.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.2.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T7.2.m1.1e">↓</annotation></semantics></math> of the revised methods in the HumanEva dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T7.3">
<tr class="ltx_tr" id="A1.T7.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T7.3.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.1.1">Papers</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.3.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.2.1">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="A1.T7.3.1.3"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.3.1">Sequence 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T7.3.1.4"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.4.1">Sequence 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T7.3.1.5"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.5.1">Sequence 3</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.1"><span class="ltx_text ltx_font_bold" id="A1.T7.3.2.1.1">Walk</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.2"><span class="ltx_text ltx_font_bold" id="A1.T7.3.2.2.1">Jog</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.3"><span class="ltx_text ltx_font_bold" id="A1.T7.3.2.3.1">Balance</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.4"><span class="ltx_text ltx_font_bold" id="A1.T7.3.2.4.1">Box</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.5"><span class="ltx_text ltx_font_bold" id="A1.T7.3.2.5.1">Walk</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.6"><span class="ltx_text ltx_font_bold" id="A1.T7.3.2.6.1">Jog</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.7"><span class="ltx_text ltx_font_bold" id="A1.T7.3.2.7.1">Balance</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.8"><span class="ltx_text ltx_font_bold" id="A1.T7.3.2.8.1">Walk</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.9"><span class="ltx_text ltx_font_bold" id="A1.T7.3.2.9.1">Jog</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.10"><span class="ltx_text ltx_font_bold" id="A1.T7.3.2.10.1">Combo</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.3.3.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Sigal et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib74" title=""><span class="ltx_text ltx_font_typewriter">74</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.2">
<span class="ltx_ERROR undefined" id="A1.T7.3.3.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib74" title=""><span class="ltx_text ltx_font_typewriter">2010</span></a></cite><span class="ltx_ERROR undefined" id="A1.T7.3.3.2.2">\endNoHyper</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.3">76.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4">85.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5">86.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.7">60.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8">93.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.9">80.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.10">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.11">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.12">—</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.3.4.1"><cite class="ltx_cite ltx_citemacro_citet">Amin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib2" title="">2</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.2">
<span class="ltx_ERROR undefined" id="A1.T7.3.4.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib2" title="">2013</a></cite><span class="ltx_ERROR undefined" id="A1.T7.3.4.2.2">\endNoHyper</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.3">54.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.4">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.6"><span class="ltx_text ltx_font_bold" id="A1.T7.3.4.6.1">47.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.7">50.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.10">54.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.11">54.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.4.12"><span class="ltx_text ltx_font_bold" id="A1.T7.3.4.12.1">51.8</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.3.5.1"><cite class="ltx_cite ltx_citemacro_citet">Belagiannis et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">5</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.2">
<span class="ltx_ERROR undefined" id="A1.T7.3.5.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib5" title="">2014</a></cite><span class="ltx_ERROR undefined" id="A1.T7.3.5.2.2">\endNoHyper</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.3">68.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.4">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.6">62.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.10">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.11">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.5.12">—</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.3.6.1"><cite class="ltx_cite ltx_citemacro_citet">Belagiannis et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib6" title="">6</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.2">
<span class="ltx_ERROR undefined" id="A1.T7.3.6.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib6" title="">2016</a></cite><span class="ltx_ERROR undefined" id="A1.T7.3.6.2.2">\endNoHyper</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.3">68.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.4">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.6">62.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.10">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.11">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.6.12">—</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A1.T7.3.7.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Mehrizi et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib58" title=""><span class="ltx_text ltx_font_typewriter">58</span></a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.2">
<span class="ltx_ERROR undefined" id="A1.T7.3.7.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib58" title=""><span class="ltx_text ltx_font_typewriter">2018</span></a></cite><span class="ltx_ERROR undefined" id="A1.T7.3.7.2.2">\endNoHyper</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.3"><span class="ltx_text ltx_font_bold" id="A1.T7.3.7.3.1">40.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.4"><span class="ltx_text ltx_font_bold" id="A1.T7.3.7.4.1">43.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.7"><span class="ltx_text ltx_font_bold" id="A1.T7.3.7.7.1">23.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.8"><span class="ltx_text ltx_font_bold" id="A1.T7.3.7.8.1">45.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.10"><span class="ltx_text ltx_font_bold" id="A1.T7.3.7.10.1">33.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.11"><span class="ltx_text ltx_font_bold" id="A1.T7.3.7.11.1">30.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T7.3.7.12">—</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="A1.T8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>MPJPE (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="A1.T8.2.m1.1"><semantics id="A1.T8.2.m1.1b"><mo id="A1.T8.2.m1.1.1" stretchy="false" xref="A1.T8.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A1.T8.2.m1.1c"><ci id="A1.T8.2.m1.1.1.cmml" xref="A1.T8.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.2.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T8.2.m1.1e">↓</annotation></semantics></math> of the revised methods in the Total Capture dataset</figcaption>
<table class="ltx_tabular ltx_align_middle" id="A1.T8.3">
<tr class="ltx_tr" id="A1.T8.3.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="A1.T8.3.1.1" rowspan="3" style="width:25.6pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.1.1.1">
<span class="ltx_p" id="A1.T8.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.1.1.1.1.1">Papers</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="A1.T8.3.1.2" rowspan="3" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.1.2.1">
<span class="ltx_p" id="A1.T8.3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.1.2.1.1.1">Year</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_tt" colspan="7" id="A1.T8.3.1.3"><span class="ltx_text ltx_font_bold" id="A1.T8.3.1.3.1">Seen Cameras (1, 3, 5, 7)</span></td>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_tt" colspan="7" id="A1.T8.3.1.4"><span class="ltx_text ltx_font_bold" id="A1.T8.3.1.4.1">Unseen Cameras (2, 4, 6, 8)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.2">
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_t" colspan="3" id="A1.T8.3.2.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.2.1.1">Seen Subject 1, 2, 3</span></td>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_t" colspan="3" id="A1.T8.3.2.2"><span class="ltx_text ltx_font_bold" id="A1.T8.3.2.2.1">Unseen Subject 4, 5</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.2.3" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.2.3.1">
<span class="ltx_p" id="A1.T8.3.2.3.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.2.3.1.1.1">Avg.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_t" colspan="3" id="A1.T8.3.2.4"><span class="ltx_text ltx_font_bold" id="A1.T8.3.2.4.1">Seen Subject 1, 2, 3</span></td>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_t" colspan="3" id="A1.T8.3.2.5"><span class="ltx_text ltx_font_bold" id="A1.T8.3.2.5.1">Unseen Subject 4, 5</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.2.6" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.2.6.1">
<span class="ltx_p" id="A1.T8.3.2.6.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.2.6.1.1.1">Avg.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.1" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.1.1">
<span class="ltx_p" id="A1.T8.3.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.1.1.1.1">W2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.2" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.2.1">
<span class="ltx_p" id="A1.T8.3.3.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.2.1.1.1">FS3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.3" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.3.1">
<span class="ltx_p" id="A1.T8.3.3.3.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.3.1.1.1">A3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.4" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.4.1">
<span class="ltx_p" id="A1.T8.3.3.4.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.4.1.1.1">W2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.5" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.5.1">
<span class="ltx_p" id="A1.T8.3.3.5.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.5.1.1.1">FS3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.6" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.6.1">
<span class="ltx_p" id="A1.T8.3.3.6.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.6.1.1.1">A3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T8.3.3.7" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.8" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.8.1">
<span class="ltx_p" id="A1.T8.3.3.8.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.8.1.1.1">W2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.9" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.9.1">
<span class="ltx_p" id="A1.T8.3.3.9.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.9.1.1.1">FS3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.10" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.10.1">
<span class="ltx_p" id="A1.T8.3.3.10.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.10.1.1.1">A3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.11" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.11.1">
<span class="ltx_p" id="A1.T8.3.3.11.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.11.1.1.1">W2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.12" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.12.1">
<span class="ltx_p" id="A1.T8.3.3.12.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.12.1.1.1">FS3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.3.13" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.3.13.1">
<span class="ltx_p" id="A1.T8.3.3.13.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.13.1.1.1">A3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T8.3.3.14" style="width:17.1pt;"></td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.1" style="width:25.6pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.1.1">
<span class="ltx_p" id="A1.T8.3.4.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Remelli et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib65" title=""><span class="ltx_text ltx_font_typewriter">65</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.2" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.2.1"><span class="ltx_ERROR undefined" id="A1.T8.3.4.2.1.1">\NoHyper</span>
<span class="ltx_p ltx_align_center" id="A1.T8.3.4.2.1.2"><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib65" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a></cite><span class="ltx_ERROR undefined" id="A1.T8.3.4.2.1.2.1">\endNoHyper</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.3" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.3.1">
<span class="ltx_p" id="A1.T8.3.4.3.1.1">10.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.4" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.4.1">
<span class="ltx_p" id="A1.T8.3.4.4.1.1">30.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.5" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.5.1">
<span class="ltx_p" id="A1.T8.3.4.5.1.1">16.30</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.6" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.6.1">
<span class="ltx_p" id="A1.T8.3.4.6.1.1">27.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.7" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.7.1">
<span class="ltx_p" id="A1.T8.3.4.7.1.1">65.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.8" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.8.1">
<span class="ltx_p" id="A1.T8.3.4.8.1.1">34.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.9" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.9.1">
<span class="ltx_p" id="A1.T8.3.4.9.1.1">27.50</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.10" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.10.1">
<span class="ltx_p" id="A1.T8.3.4.10.1.1">22.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.11" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.11.1">
<span class="ltx_p" id="A1.T8.3.4.11.1.1">47.10</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.12" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.12.1">
<span class="ltx_p" id="A1.T8.3.4.12.1.1">27.80</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.13" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.13.1">
<span class="ltx_p" id="A1.T8.3.4.13.1.1">39.10</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.14" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.14.1">
<span class="ltx_p" id="A1.T8.3.4.14.1.1">75.70</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.15" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.15.1">
<span class="ltx_p" id="A1.T8.3.4.15.1.1">43.10</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.4.16" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.4.16.1">
<span class="ltx_p" id="A1.T8.3.4.16.1.1">38.20</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.1" style="width:25.6pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.1.1">
<span class="ltx_p" id="A1.T8.3.5.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wan et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib82" title=""><span class="ltx_text ltx_font_typewriter">82</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.2" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.2.1"><span class="ltx_ERROR undefined" id="A1.T8.3.5.2.1.1">\NoHyper</span>
<span class="ltx_p ltx_align_center" id="A1.T8.3.5.2.1.2"><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib82" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a></cite><span class="ltx_ERROR undefined" id="A1.T8.3.5.2.1.2.1">\endNoHyper</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.3" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.3.1">
<span class="ltx_p" id="A1.T8.3.5.3.1.1">13.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.4" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.4.1">
<span class="ltx_p" id="A1.T8.3.5.4.1.1">24.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.5" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.5.1">
<span class="ltx_p" id="A1.T8.3.5.5.1.1">17.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.6" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.6.1">
<span class="ltx_p" id="A1.T8.3.5.6.1.1">23.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.7" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.7.1">
<span class="ltx_p" id="A1.T8.3.5.7.1.1">41.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.8" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.8.1">
<span class="ltx_p" id="A1.T8.3.5.8.1.1">29.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.9" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.9.1">
<span class="ltx_p" id="A1.T8.3.5.9.1.1">23.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.10" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.10.1">
<span class="ltx_p" id="A1.T8.3.5.10.1.1">—</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.11" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.11.1">
<span class="ltx_p" id="A1.T8.3.5.11.1.1">—</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.12" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.12.1">
<span class="ltx_p" id="A1.T8.3.5.12.1.1">—</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.13" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.13.1">
<span class="ltx_p" id="A1.T8.3.5.13.1.1">—</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.14" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.14.1">
<span class="ltx_p" id="A1.T8.3.5.14.1.1">—</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.15" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.15.1">
<span class="ltx_p" id="A1.T8.3.5.15.1.1">—</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.5.16" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.5.16.1">
<span class="ltx_p" id="A1.T8.3.5.16.1.1">—</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.1" style="width:25.6pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.1.1">
<span class="ltx_p" id="A1.T8.3.6.1.1.1"><cite class="ltx_cite ltx_citemacro_citet">Cai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib10" title="">10</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.2" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.2.1"><span class="ltx_ERROR undefined" id="A1.T8.3.6.2.1.1">\NoHyper</span>
<span class="ltx_p ltx_align_center" id="A1.T8.3.6.2.1.2"><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib10" title="">2024</a></cite><span class="ltx_ERROR undefined" id="A1.T8.3.6.2.1.2.1">\endNoHyper</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.3" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.3.1">
<span class="ltx_p" id="A1.T8.3.6.3.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.3.1.1.1">5.50</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.4" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.4.1">
<span class="ltx_p" id="A1.T8.3.6.4.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.4.1.1.1">15.00</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.5" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.5.1">
<span class="ltx_p" id="A1.T8.3.6.5.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.5.1.1.1">5.68</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.6" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.6.1">
<span class="ltx_p" id="A1.T8.3.6.6.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.6.1.1.1">18.10</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.7" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.7.1">
<span class="ltx_p" id="A1.T8.3.6.7.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.7.1.1.1">37.60</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.8" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.8.1">
<span class="ltx_p" id="A1.T8.3.6.8.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.8.1.1.1">20.60</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.9" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.9.1">
<span class="ltx_p" id="A1.T8.3.6.9.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.9.1.1.1">15.00</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.10" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.10.1">
<span class="ltx_p" id="A1.T8.3.6.10.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.10.1.1.1">22.10</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.11" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.11.1">
<span class="ltx_p" id="A1.T8.3.6.11.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.11.1.1.1">35.40</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.12" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.12.1">
<span class="ltx_p" id="A1.T8.3.6.12.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.12.1.1.1">23.40</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.13" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.13.1">
<span class="ltx_p" id="A1.T8.3.6.13.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.13.1.1.1">23.20</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.14" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.14.1">
<span class="ltx_p" id="A1.T8.3.6.14.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.14.1.1.1">42.60</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.15" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.15.1">
<span class="ltx_p" id="A1.T8.3.6.15.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.15.1.1.1">28.40</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T8.3.6.16" style="width:17.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.6.16.1">
<span class="ltx_p" id="A1.T8.3.6.16.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.3.6.16.1.1.1">28.30</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_tt" colspan="16" id="A1.T8.3.7.1" style="width:25.6pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T8.3.7.1.1">
<span class="ltx_p" id="A1.T8.3.7.1.1.1" style="width:475.2pt;">Avg. - Average 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.T8.3.7.1.1.1.1">Testing sequences:</span> W2 - Walking2, FS3 - Freestyle3, A3 - Acting3.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.T8.3.7.1.1.1.2">Training sequences:</span> ROM1,2,3; Walking1,3; Freestyle1,2; Acting1,2 and Running1 using subjects 1, 2 and 3.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Multi-person 3D pose estimation</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1.T9" title="Table 9 ‣ A.2 Multi-person 3D pose estimation ‣ Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">9</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#A1.T10" title="Table 10 ‣ A.2 Multi-person 3D pose estimation ‣ Appendix A Datasets - Benchmarking ‣ Markerless Multi-view 3D Human Pose Estimation: a survey"><span class="ltx_text ltx_ref_tag">10</span></a> present the results for the dataset <span class="ltx_glossaryref" title="">UMPM</span> and CMU Panoptic, respectively.</p>
</div>
<figure class="ltx_table" id="A1.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Performance of the revised methods in the <span class="ltx_glossaryref" title="">UMPM</span> dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T9.2">
<tr class="ltx_tr" id="A1.T9.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="A1.T9.1.1.2" rowspan="2" style="width:76.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.1.1.2.1">
<span class="ltx_p" id="A1.T9.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.2.1.1.1">Papers</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T9.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.3.1">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="A1.T9.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1">PCP (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T9.1.1.1.1.m1.1"><semantics id="A1.T9.1.1.1.1.m1.1a"><mo id="A1.T9.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T9.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T9.1.1.1.1.m1.1b"><ci id="A1.T9.1.1.1.1.m1.1.1.cmml" xref="A1.T9.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T9.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.3.1"><span class="ltx_text ltx_font_bold" id="A1.T9.2.3.1.1">Head</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.3.2"><span class="ltx_text ltx_font_bold" id="A1.T9.2.3.2.1">Torso</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.3.3"><span class="ltx_text ltx_font_bold" id="A1.T9.2.3.3.1">Upper arms</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.3.4"><span class="ltx_text ltx_font_bold" id="A1.T9.2.3.4.1">Lower arms</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.3.5"><span class="ltx_text ltx_font_bold" id="A1.T9.2.3.5.1">Upper Legs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.3.6"><span class="ltx_text ltx_font_bold" id="A1.T9.2.3.6.1">Lower Legs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.3.7"><span class="ltx_text ltx_font_bold" id="A1.T9.2.3.7.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T9.2.4.1" style="width:76.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.2.4.1.1">
<span class="ltx_p" id="A1.T9.2.4.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ershadi-Nasab et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib27" title=""><span class="ltx_text ltx_font_typewriter">27</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A1.T9.2.4.2">
<span class="ltx_ERROR undefined" id="A1.T9.2.4.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib27" title=""><span class="ltx_text ltx_font_typewriter">2018</span></a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.4.3">95.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.4.4">98.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.4.5">92.42</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.4.6">84.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.4.7">94.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.4.8">86.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.4.9">91.02</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T9.2.5.1" style="width:76.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.2.5.1.1">
<span class="ltx_p" id="A1.T9.2.5.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ershadi Nasab et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">26</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A1.T9.2.5.2">
<span class="ltx_ERROR undefined" id="A1.T9.2.5.2.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">2021</span></a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.5.3"><span class="ltx_text ltx_font_bold" id="A1.T9.2.5.3.1">99.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.5.4"><span class="ltx_text ltx_font_bold" id="A1.T9.2.5.4.1">99.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.5.5"><span class="ltx_text ltx_font_bold" id="A1.T9.2.5.5.1">95.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.5.6"><span class="ltx_text ltx_font_bold" id="A1.T9.2.5.6.1">89.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.5.7"><span class="ltx_text ltx_font_bold" id="A1.T9.2.5.7.1">96.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.5.8"><span class="ltx_text ltx_font_bold" id="A1.T9.2.5.8.1">90.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.5.9"><span class="ltx_text ltx_font_bold" id="A1.T9.2.5.9.1">94.30</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="A1.T9.2.2.2" rowspan="2" style="width:76.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.2.2.2.1">
<span class="ltx_p" id="A1.T9.2.2.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.2.2.2.1.1.1">Papers</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T9.2.2.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T9.2.2.3.1">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T9.2.2.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T9.2.2.4.1">Actors</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="A1.T9.2.2.1"><span class="ltx_text ltx_font_bold" id="A1.T9.2.2.1.1">UMPM sequence: p3-chair-11 — PCP (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T9.2.2.1.1.m1.1"><semantics id="A1.T9.2.2.1.1.m1.1a"><mo id="A1.T9.2.2.1.1.m1.1.1" stretchy="false" xref="A1.T9.2.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T9.2.2.1.1.m1.1b"><ci id="A1.T9.2.2.1.1.m1.1.1.cmml" xref="A1.T9.2.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.2.2.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T9.2.2.1.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.6.1"><span class="ltx_text ltx_font_bold" id="A1.T9.2.6.1.1">Head</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.6.2"><span class="ltx_text ltx_font_bold" id="A1.T9.2.6.2.1">Torso</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.6.3"><span class="ltx_text ltx_font_bold" id="A1.T9.2.6.3.1">Upper arms</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.6.4"><span class="ltx_text ltx_font_bold" id="A1.T9.2.6.4.1">Lower arms</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.6.5"><span class="ltx_text ltx_font_bold" id="A1.T9.2.6.5.1">Upper Legs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.6.6"><span class="ltx_text ltx_font_bold" id="A1.T9.2.6.6.1">Lower Legs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.6.7"><span class="ltx_text ltx_font_bold" id="A1.T9.2.6.7.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T9.2.7.1" style="width:76.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.2.7.1.1">
<span class="ltx_p" id="A1.T9.2.7.1.1.1"><span class="ltx_ERROR undefined" id="A1.T9.2.7.1.1.1.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib27" title=""><span class="ltx_text ltx_font_typewriter">Ershadi-Nasab et al.</span></a></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.7.2" rowspan="2"><span class="ltx_text" id="A1.T9.2.7.2.1"><span class="ltx_ERROR undefined" id="A1.T9.2.7.2.1.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib27" title=""><span class="ltx_text ltx_font_typewriter">2018</span></a></cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.7.3">A1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.7.4">94.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.7.5">96.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.7.6">91.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.7.7">83.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.7.8">92.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.7.9">85.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.7.10">89.76</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.8">
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T9.2.8.1" style="width:76.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.2.8.1.1">
<span class="ltx_p" id="A1.T9.2.8.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib27" title=""><span class="ltx_text ltx_font_typewriter">27</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.8.2">A2</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.8.3">95.87</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.8.4">97.28</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.8.5">95.12</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.8.6">83.89</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.8.7">93.76</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.8.8">85.12</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.8.9">89.89</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T9.2.9.1" style="width:76.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.2.9.1.1">
<span class="ltx_p" id="A1.T9.2.9.1.1.1"><span class="ltx_ERROR undefined" id="A1.T9.2.9.1.1.1.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">Ershadi Nasab et al.</span></a></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T9.2.9.2" rowspan="2"><span class="ltx_text" id="A1.T9.2.9.2.1"><span class="ltx_ERROR undefined" id="A1.T9.2.9.2.1.1">\NoHyper</span><cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">2021</span></a></cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.9.3">A1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.9.4"><span class="ltx_text ltx_font_bold" id="A1.T9.2.9.4.1">99.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.9.5"><span class="ltx_text ltx_font_bold" id="A1.T9.2.9.5.1">99.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.9.6"><span class="ltx_text ltx_font_bold" id="A1.T9.2.9.6.1">98.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.9.7"><span class="ltx_text ltx_font_bold" id="A1.T9.2.9.7.1">91.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.9.8"><span class="ltx_text ltx_font_bold" id="A1.T9.2.9.8.1">97.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.9.9"><span class="ltx_text ltx_font_bold" id="A1.T9.2.9.9.1">96.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.9.10"><span class="ltx_text ltx_font_bold" id="A1.T9.2.9.10.1">96.65</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="A1.T9.2.10.1" style="width:76.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.2.10.1.1">
<span class="ltx_p" id="A1.T9.2.10.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">26</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.2.10.2">A2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.2.10.3"><span class="ltx_text ltx_font_bold" id="A1.T9.2.10.3.1">98.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.2.10.4"><span class="ltx_text ltx_font_bold" id="A1.T9.2.10.4.1">99.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.2.10.5"><span class="ltx_text ltx_font_bold" id="A1.T9.2.10.5.1">99.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.2.10.6"><span class="ltx_text ltx_font_bold" id="A1.T9.2.10.6.1">90.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.2.10.7"><span class="ltx_text ltx_font_bold" id="A1.T9.2.10.7.1">96.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.2.10.8"><span class="ltx_text ltx_font_bold" id="A1.T9.2.10.8.1">94.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.2.10.9"><span class="ltx_text ltx_font_bold" id="A1.T9.2.10.9.1">95.83</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="A1.T10">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Performance of the revised methods in the CMU Panoptic dataset</figcaption>
<table class="ltx_tabular ltx_align_middle" id="A1.T10.8">
<tr class="ltx_tr" id="A1.T10.8.8">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.8.8.9"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.9.1">#Views</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="A1.T10.8.8.10" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.8.10.1">
<span class="ltx_p" id="A1.T10.8.8.10.1.1"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.10.1.1.1">Paper</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.8.8.11"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.11.1">Year</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="A1.T10.1.1.1" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.1.1.1.1">
<span class="ltx_p" id="A1.T10.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T10.1.1.1.1.1.1">MPJPE (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="A1.T10.1.1.1.1.1.1.m1.1"><semantics id="A1.T10.1.1.1.1.1.1.m1.1a"><mo id="A1.T10.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T10.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A1.T10.1.1.1.1.1.1.m1.1b"><ci id="A1.T10.1.1.1.1.1.1.m1.1.1.cmml" xref="A1.T10.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.1.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T10.1.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T10.2.2.2.1">AP<math alttext="{}_{25}\uparrow" class="ltx_math_unparsed" display="inline" id="A1.T10.2.2.2.1.m1.1"><semantics id="A1.T10.2.2.2.1.m1.1a"><mmultiscripts id="A1.T10.2.2.2.1.m1.1.1"><mo id="A1.T10.2.2.2.1.m1.1.1.2" stretchy="false">↑</mo><mprescripts id="A1.T10.2.2.2.1.m1.1.1a"></mprescripts><mn id="A1.T10.2.2.2.1.m1.1.1.3">25</mn><mrow id="A1.T10.2.2.2.1.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="A1.T10.2.2.2.1.m1.1b">{}_{25}\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T10.2.2.2.1.m1.1c">start_FLOATSUBSCRIPT 25 end_FLOATSUBSCRIPT ↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.3.3.3"><span class="ltx_text ltx_font_bold" id="A1.T10.3.3.3.1">AP<math alttext="{}_{50}\uparrow" class="ltx_math_unparsed" display="inline" id="A1.T10.3.3.3.1.m1.1"><semantics id="A1.T10.3.3.3.1.m1.1a"><mmultiscripts id="A1.T10.3.3.3.1.m1.1.1"><mo id="A1.T10.3.3.3.1.m1.1.1.2" stretchy="false">↑</mo><mprescripts id="A1.T10.3.3.3.1.m1.1.1a"></mprescripts><mn id="A1.T10.3.3.3.1.m1.1.1.3">50</mn><mrow id="A1.T10.3.3.3.1.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="A1.T10.3.3.3.1.m1.1b">{}_{50}\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T10.3.3.3.1.m1.1c">start_FLOATSUBSCRIPT 50 end_FLOATSUBSCRIPT ↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.4.4.4"><span class="ltx_text ltx_font_bold" id="A1.T10.4.4.4.1">AP<math alttext="{}_{75}\uparrow" class="ltx_math_unparsed" display="inline" id="A1.T10.4.4.4.1.m1.1"><semantics id="A1.T10.4.4.4.1.m1.1a"><mmultiscripts id="A1.T10.4.4.4.1.m1.1.1"><mo id="A1.T10.4.4.4.1.m1.1.1.2" stretchy="false">↑</mo><mprescripts id="A1.T10.4.4.4.1.m1.1.1a"></mprescripts><mn id="A1.T10.4.4.4.1.m1.1.1.3">75</mn><mrow id="A1.T10.4.4.4.1.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="A1.T10.4.4.4.1.m1.1b">{}_{75}\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T10.4.4.4.1.m1.1c">start_FLOATSUBSCRIPT 75 end_FLOATSUBSCRIPT ↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.5.5.5"><span class="ltx_text ltx_font_bold" id="A1.T10.5.5.5.1">AP<math alttext="{}_{100}\uparrow" class="ltx_math_unparsed" display="inline" id="A1.T10.5.5.5.1.m1.1"><semantics id="A1.T10.5.5.5.1.m1.1a"><mmultiscripts id="A1.T10.5.5.5.1.m1.1.1"><mo id="A1.T10.5.5.5.1.m1.1.1.2" stretchy="false">↑</mo><mprescripts id="A1.T10.5.5.5.1.m1.1.1a"></mprescripts><mn id="A1.T10.5.5.5.1.m1.1.1.3">100</mn><mrow id="A1.T10.5.5.5.1.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="A1.T10.5.5.5.1.m1.1b">{}_{100}\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T10.5.5.5.1.m1.1c">start_FLOATSUBSCRIPT 100 end_FLOATSUBSCRIPT ↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.6.6.6"><span class="ltx_text ltx_font_bold" id="A1.T10.6.6.6.1">AP<math alttext="{}_{125}\uparrow" class="ltx_math_unparsed" display="inline" id="A1.T10.6.6.6.1.m1.1"><semantics id="A1.T10.6.6.6.1.m1.1a"><mmultiscripts id="A1.T10.6.6.6.1.m1.1.1"><mo id="A1.T10.6.6.6.1.m1.1.1.2" stretchy="false">↑</mo><mprescripts id="A1.T10.6.6.6.1.m1.1.1a"></mprescripts><mn id="A1.T10.6.6.6.1.m1.1.1.3">125</mn><mrow id="A1.T10.6.6.6.1.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="A1.T10.6.6.6.1.m1.1b">{}_{125}\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T10.6.6.6.1.m1.1c">start_FLOATSUBSCRIPT 125 end_FLOATSUBSCRIPT ↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.7.7.7"><span class="ltx_text ltx_font_bold" id="A1.T10.7.7.7.1">AP<math alttext="{}_{150}\uparrow" class="ltx_math_unparsed" display="inline" id="A1.T10.7.7.7.1.m1.1"><semantics id="A1.T10.7.7.7.1.m1.1a"><mmultiscripts id="A1.T10.7.7.7.1.m1.1.1"><mo id="A1.T10.7.7.7.1.m1.1.1.2" stretchy="false">↑</mo><mprescripts id="A1.T10.7.7.7.1.m1.1.1a"></mprescripts><mn id="A1.T10.7.7.7.1.m1.1.1.3">150</mn><mrow id="A1.T10.7.7.7.1.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="A1.T10.7.7.7.1.m1.1b">{}_{150}\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T10.7.7.7.1.m1.1c">start_FLOATSUBSCRIPT 150 end_FLOATSUBSCRIPT ↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.8.8.8"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.8.1">mAP <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T10.8.8.8.1.m1.1"><semantics id="A1.T10.8.8.8.1.m1.1a"><mo id="A1.T10.8.8.8.1.m1.1.1" stretchy="false" xref="A1.T10.8.8.8.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T10.8.8.8.1.m1.1b"><ci id="A1.T10.8.8.8.1.m1.1.1.cmml" xref="A1.T10.8.8.8.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.8.8.8.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T10.8.8.8.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.9">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.9.1" rowspan="3"><span class="ltx_text" id="A1.T10.8.9.1.1">2</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.9.2" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.9.2.1">
<span class="ltx_p" id="A1.T10.8.9.2.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib86" title=""><span class="ltx_text ltx_font_typewriter">86</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.9.3">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.9.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.9.4.1">
<span class="ltx_p" id="A1.T10.8.9.4.1.1">34.80</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.9.5">37.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.9.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.9.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.9.8">93.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.9.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.9.10">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.9.11">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.10.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.10.1.1">
<span class="ltx_p" id="A1.T10.8.10.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib103" title=""><span class="ltx_text ltx_font_typewriter">103</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.10.2">2023</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.10.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.10.3.1">
<span class="ltx_p" id="A1.T10.8.10.3.1.1">47.42</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.10.4">25.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.10.5">62.03</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.10.6">77.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.10.7">86.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.10.8">90.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.10.9">92.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.10.10">72.35</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.11">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.11.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.11.1.1">
<span class="ltx_p" id="A1.T10.8.11.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhuang and Zhou</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib104" title=""><span class="ltx_text ltx_font_typewriter">104</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.11.2">2023</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.11.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.11.3.1">
<span class="ltx_p" id="A1.T10.8.11.3.1.1">42.53</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.11.4">31.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.11.5">65.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.11.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.11.7">93.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.11.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.11.9">96.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.11.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.12">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.12.1" rowspan="6"><span class="ltx_text" id="A1.T10.8.12.1.1">3</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.12.2" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.12.2.1">
<span class="ltx_p" id="A1.T10.8.12.2.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Tu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib81" title=""><span class="ltx_text ltx_font_typewriter">81</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.12.3">2020</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.12.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.12.4.1">
<span class="ltx_p" id="A1.T10.8.12.4.1.1">24.29</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.12.5">58.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.12.6">93.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.12.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.12.8">98.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.12.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.12.10">99.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.12.11">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.13">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.13.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.13.1.1">
<span class="ltx_p" id="A1.T10.8.13.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib86" title=""><span class="ltx_text ltx_font_typewriter">86</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.13.2">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.13.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.13.3.1">
<span class="ltx_p" id="A1.T10.8.13.3.1.1">21.10</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.13.4">71.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.13.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.13.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.13.7">95.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.13.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.13.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.13.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.14">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.14.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.14.1.1">
<span class="ltx_p" id="A1.T10.8.14.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib98" title=""><span class="ltx_text ltx_font_typewriter">98</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.14.2">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.14.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.14.3.1">
<span class="ltx_p" id="A1.T10.8.14.3.1.1">24.93</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.14.4">49.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.14.5">92.44</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.14.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.14.7">97.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.14.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.14.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.14.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.15">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.15.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.15.1.1">
<span class="ltx_p" id="A1.T10.8.15.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ye et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib94" title=""><span class="ltx_text ltx_font_typewriter">94</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.15.2">2022</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.15.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.15.3.1">
<span class="ltx_p" id="A1.T10.8.15.3.1.1">26.13</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.15.4">53.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.15.5">91.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.15.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.15.7">97.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.15.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.15.9">98.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.15.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.16">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.16.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.16.1.1">
<span class="ltx_p" id="A1.T10.8.16.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib103" title=""><span class="ltx_text ltx_font_typewriter">103</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.16.2">2023</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.16.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.16.3.1">
<span class="ltx_p" id="A1.T10.8.16.3.1.1">33.03</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.16.4">34.98</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.16.5">76.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.16.6">89.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.16.7">93.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.16.8">96.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.16.9">97.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.16.10">81.37</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.17">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.17.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.17.1.1">
<span class="ltx_p" id="A1.T10.8.17.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhuang and Zhou</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib104" title=""><span class="ltx_text ltx_font_typewriter">104</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.17.2">2023</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.17.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.17.3.1">
<span class="ltx_p" id="A1.T10.8.17.3.1.1">24.98</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.17.4">57.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.17.5">92.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.17.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.17.7">97.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.17.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.17.9">98.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.17.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.18">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.18.1" rowspan="6"><span class="ltx_text" id="A1.T10.8.18.1.1">4</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.18.2" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.18.2.1">
<span class="ltx_p" id="A1.T10.8.18.2.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Fan et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib28" title=""><span class="ltx_text ltx_font_typewriter">28</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.18.3">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.18.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.18.4.1">
<span class="ltx_p" id="A1.T10.8.18.4.1.1">113.60</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.18.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.18.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.18.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.18.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.18.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.18.10">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.18.11">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.19">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.19.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.19.1.1">
<span class="ltx_p" id="A1.T10.8.19.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib86" title=""><span class="ltx_text ltx_font_typewriter">86</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.19.2">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.19.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.19.3.1">
<span class="ltx_p" id="A1.T10.8.19.3.1.1">19.30</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.19.4">84.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.19.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.19.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.19.7">96.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.19.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.19.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.19.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.20">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.20.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.20.1.1">
<span class="ltx_p" id="A1.T10.8.20.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib98" title=""><span class="ltx_text ltx_font_typewriter">98</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.20.2">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.20.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.20.3.1">
<span class="ltx_p" id="A1.T10.8.20.3.1.1">20.35</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.20.4">66.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.20.5">96.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.20.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.20.7">99.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.20.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.20.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.20.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.21">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.21.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.21.1.1">
<span class="ltx_p" id="A1.T10.8.21.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ye et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib94" title=""><span class="ltx_text ltx_font_typewriter">94</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.21.2">2022</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.21.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.21.3.1">
<span class="ltx_p" id="A1.T10.8.21.3.1.1">21.12</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.21.4">73.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.21.5">97.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.21.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.21.7">99.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.21.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.21.9">99.35</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.21.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.22">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.22.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.22.1.1">
<span class="ltx_p" id="A1.T10.8.22.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib103" title=""><span class="ltx_text ltx_font_typewriter">103</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.22.2">2023</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.22.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.22.3.1">
<span class="ltx_p" id="A1.T10.8.22.3.1.1">23.89</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.22.4">52.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.22.5">90.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.22.6">96.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.22.7">97.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.22.8">98.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.22.9">98.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.22.10">89.24</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.23">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.23.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.23.1.1">
<span class="ltx_p" id="A1.T10.8.23.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhuang and Zhou</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib104" title=""><span class="ltx_text ltx_font_typewriter">104</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.23.2">2023</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.23.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.23.3.1">
<span class="ltx_p" id="A1.T10.8.23.3.1.1">20.95</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.23.4">75.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.23.5">97.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.23.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.23.7">99.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.23.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.23.9">99.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.23.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.24">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.24.1" rowspan="10"><span class="ltx_text" id="A1.T10.8.24.1.1">5</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.24.2" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.24.2.1">
<span class="ltx_p" id="A1.T10.8.24.2.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Tu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib81" title=""><span class="ltx_text ltx_font_typewriter">81</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.24.3">2020</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.24.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.24.4.1">
<span class="ltx_p" id="A1.T10.8.24.4.1.1">17.68</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.24.5">83.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.24.6">98.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.24.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.24.8">99.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.24.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.24.10"><span class="ltx_text ltx_font_bold" id="A1.T10.8.24.10.1">99.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.24.11">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.25">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.25.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.25.1.1">
<span class="ltx_p" id="A1.T10.8.25.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Fan et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib28" title=""><span class="ltx_text ltx_font_typewriter">28</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.25.2">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.25.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.25.3.1">
<span class="ltx_p" id="A1.T10.8.25.3.1.1">94.21</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.25.4">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.25.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.25.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.25.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.25.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.25.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.25.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.26">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.26.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.26.1.1">
<span class="ltx_p" id="A1.T10.8.26.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Lin and Lee</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib50" title=""><span class="ltx_text ltx_font_typewriter">50</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.26.2">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.26.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.26.3.1">
<span class="ltx_p" id="A1.T10.8.26.3.1.1">16.75</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.26.4">92.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.26.5"><span class="ltx_text ltx_font_bold" id="A1.T10.8.26.5.1">98.96</span></td>
<td class="ltx_td ltx_border_t" id="A1.T10.8.26.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.26.7"><span class="ltx_text ltx_font_bold" id="A1.T10.8.26.7.1">99.81</span></td>
<td class="ltx_td ltx_border_t" id="A1.T10.8.26.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.26.9">99.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.26.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.27">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.27.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.27.1.1">
<span class="ltx_p" id="A1.T10.8.27.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Reddy et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib64" title=""><span class="ltx_text ltx_font_typewriter">64</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.27.2">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.27.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.27.3.1">
<span class="ltx_p" id="A1.T10.8.27.3.1.1"><span class="ltx_text ltx_font_bold" id="A1.T10.8.27.3.1.1.1">7.30</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.27.4">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.27.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.27.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.27.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.27.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.27.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.27.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.28">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.28.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.28.1.1">
<span class="ltx_p" id="A1.T10.8.28.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib86" title=""><span class="ltx_text ltx_font_typewriter">86</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.28.2">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.28.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.28.3.1">
<span class="ltx_p" id="A1.T10.8.28.3.1.1">15.80</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.28.4"><span class="ltx_text ltx_font_bold" id="A1.T10.8.28.4.1">92.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.28.5">96.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.28.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.28.7">97.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.28.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.28.9">97.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.28.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.29">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.29.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.29.1.1">
<span class="ltx_p" id="A1.T10.8.29.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib98" title=""><span class="ltx_text ltx_font_typewriter">98</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.29.2">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.29.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.29.3.1">
<span class="ltx_p" id="A1.T10.8.29.3.1.1">16.97</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.29.4">85.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.29.5">98.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.29.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.29.7">99.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.29.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.29.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.29.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.30">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.30.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.30.1.1">
<span class="ltx_p" id="A1.T10.8.30.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ye et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib94" title=""><span class="ltx_text ltx_font_typewriter">94</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.30.2">2022</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.30.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.30.3.1">
<span class="ltx_p" id="A1.T10.8.30.3.1.1">18.26</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.30.4">85.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.30.5">98.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.30.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.30.7">99.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.30.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.30.9">99.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.30.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.31">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.31.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.31.1.1">
<span class="ltx_p" id="A1.T10.8.31.1.1.1"><cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib16" title="">16</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.31.2">2023</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.31.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.31.3.1">
<span class="ltx_p" id="A1.T10.8.31.3.1.1">17.62</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.31.4">83.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.31.5">97.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.31.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.31.7">98.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.31.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.31.9">98.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.31.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.32">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.32.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.32.1.1">
<span class="ltx_p" id="A1.T10.8.32.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib103" title=""><span class="ltx_text ltx_font_typewriter">103</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.32.2">2023</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.32.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.32.3.1">
<span class="ltx_p" id="A1.T10.8.32.3.1.1">18.88</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.32.4">61.28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.32.5">95.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.32.6"><span class="ltx_text ltx_font_bold" id="A1.T10.8.32.6.1">98.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.32.7">99.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.32.8"><span class="ltx_text ltx_font_bold" id="A1.T10.8.32.8.1">99.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.32.9">99.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.32.10"><span class="ltx_text ltx_font_bold" id="A1.T10.8.32.10.1">92.35</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.33">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.33.1" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.33.1.1">
<span class="ltx_p" id="A1.T10.8.33.1.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhuang and Zhou</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib104" title=""><span class="ltx_text ltx_font_typewriter">104</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.33.2">2023</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.33.3" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.33.3.1">
<span class="ltx_p" id="A1.T10.8.33.3.1.1">17.42</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.33.4">86.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.33.5">98.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.33.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.33.7">99.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.33.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.33.9">99.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.33.10">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.34">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.34.1">6</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.34.2" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.34.2.1">
<span class="ltx_p" id="A1.T10.8.34.2.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Fan et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib28" title=""><span class="ltx_text ltx_font_typewriter">28</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.34.3">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.34.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.34.4.1">
<span class="ltx_p" id="A1.T10.8.34.4.1.1">87.53</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.34.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.34.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.34.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.34.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.34.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.34.10">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.34.11">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.35">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.35.1">7</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.35.2" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.35.2.1">
<span class="ltx_p" id="A1.T10.8.35.2.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Fan et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib28" title=""><span class="ltx_text ltx_font_typewriter">28</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.35.3">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.35.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.35.4.1">
<span class="ltx_p" id="A1.T10.8.35.4.1.1">84.02</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.35.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.35.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.35.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.35.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.35.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.35.10">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.35.11">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.36">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.36.1">8</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.36.2" style="width:83.9pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.36.2.1">
<span class="ltx_p" id="A1.T10.8.36.2.1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Fan et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib28" title=""><span class="ltx_text ltx_font_typewriter">28</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.36.3">2021</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A1.T10.8.36.4" style="width:34.1pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.36.4.1">
<span class="ltx_p" id="A1.T10.8.36.4.1.1">80.81</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.36.5">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.36.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.36.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.36.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.36.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.36.10">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.36.11">—</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.37">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt" colspan="11" id="A1.T10.8.37.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.8.37.1.1">
<span class="ltx_p" id="A1.T10.8.37.1.1.1" style="width:475.2pt;">
Sequences used by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Tu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib81" title=""><span class="ltx_text ltx_font_typewriter">81</span></a>], <span class="ltx_text ltx_font_typewriter">Reddy et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib64" title=""><span class="ltx_text ltx_font_typewriter">64</span></a>], <span class="ltx_text ltx_font_typewriter">Ye et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib94" title=""><span class="ltx_text ltx_font_typewriter">94</span></a>], <span class="ltx_text ltx_font_typewriter">Zhang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib98" title=""><span class="ltx_text ltx_font_typewriter">98</span></a>], Chen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib16" title="">16</a>], <span class="ltx_text ltx_font_typewriter">Lin and Lee</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib50" title=""><span class="ltx_text ltx_font_typewriter">50</span></a>], <span class="ltx_text ltx_font_typewriter">Zhuang and Zhou</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib104" title=""><span class="ltx_text ltx_font_typewriter">104</span></a>]</cite>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.T10.8.37.1.1.1.1">Train set:</span> "160422_ultimatum1", "160224_haggling1", "160226_haggling1", "161202_haggling1", "160906_ian1", "160906_ian2", "160906_ian3", "160906_band1", "160906_band2" and "160906_band3".

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.T10.8.37.1.1.1.2">Test set:</span> "160906_pizza1", "160422_haggling1", "160906_ian5" and "160906_band4".  
<br class="ltx_break"/><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wang et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib86" title=""><span class="ltx_text ltx_font_typewriter">86</span></a>]</cite> – training set includes the same sequences presented above except for "160906_band3". The test sequences are the same.

<br class="ltx_break"/><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Fan et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib28" title=""><span class="ltx_text ltx_font_typewriter">28</span></a>]</cite> – training set: "Mafia", validation set: "Mafia" and "Ultimatum" and testing set: "Mafia", "Ultimatum" and "Haggling".

<br class="ltx_break"/><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Zhu et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2407.03817v1#bib.bib103" title=""><span class="ltx_text ltx_font_typewriter">103</span></a>]</cite> – training set: "160422_ultimatum1" and testing set: "160906_pizza1".</span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van der Aa et al. [2011]</span>
<span class="ltx_bibblock">
van der Aa, N., Luo, X., Giezeman, G., Tan, R., Veltkamp, R., 2011.

</span>
<span class="ltx_bibblock">Umpm benchmark: A multi-person dataset with synchronized video and motion capture data for evaluation of articulated human motion and interaction, in: 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), pp. 1264–1269.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/ICCVW.2011.6130396" title="">10.1109/ICCVW.2011.6130396</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amin et al. [2013]</span>
<span class="ltx_bibblock">
Amin, S., Andriluka, M., Rohrbach, M., Schiele, B., 2013.

</span>
<span class="ltx_bibblock">Multi-view pictorial structures for 3d human pose estimation, in: British Machine Vision Conference.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:8474682" title="">https://api.semanticscholar.org/CorpusID:8474682</a>, doi:<a class="ltx_ref" href="https:/doi.org/10.5244/C.27.45" title="">10.5244/C.27.45</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka et al. [2014]</span>
<span class="ltx_bibblock">
Andriluka, M., Pishchulin, L., Gehler, P., Schiele, B., 2014.

</span>
<span class="ltx_bibblock">2d human pose estimation: New benchmark and state of the art analysis, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bartol et al. [2022]</span>
<span class="ltx_bibblock">
Bartol, K., Bojanić, D., Petković, T., Pribanić, T., 2022.

</span>
<span class="ltx_bibblock">Generalizable human pose triangulation, in: Proceedings of IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belagiannis et al. [2014]</span>
<span class="ltx_bibblock">
Belagiannis, V., Amin, S., Andriluka, M., Schiele, B., Navab, N., Ilic, S., 2014.

</span>
<span class="ltx_bibblock">3d pictorial structures for multiple human pose estimation, in: 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1669–1676.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/CVPR.2014.216" title="">10.1109/CVPR.2014.216</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belagiannis et al. [2016]</span>
<span class="ltx_bibblock">
Belagiannis, V., Amin, S., Andriluka, M., Schiele, B., Navab, N., Ilic, S., 2016.

</span>
<span class="ltx_bibblock">3d pictorial structures revisited: Multiple human pose estimation.

</span>
<span class="ltx_bibblock">IEEE Transactions on Pattern Analysis and Machine Intelligence 38, 1929–1942.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/TPAMI.2015.2509986" title="">10.1109/TPAMI.2015.2509986</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belagiannis et al. [2015]</span>
<span class="ltx_bibblock">
Belagiannis, V., Wang, X., Schiele, B., Fua, P., Ilic, S., Navab, N., 2015.

</span>
<span class="ltx_bibblock">Multiple human pose estimation with temporally consistent 3d pictorial structures, in: Computer Vision - ECCV 2014 Workshops, Springer International Publishing, Cham. pp. 742–754.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben Gamra and Akhloufi [2021]</span>
<span class="ltx_bibblock">
Ben Gamra, M., Akhloufi, M.A., 2021.

</span>
<span class="ltx_bibblock">A review of deep learning techniques for 2d and 3d human pose estimation.

</span>
<span class="ltx_bibblock">Image and Vision Computing 114, 104282.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0262885621001876" title="">https://www.sciencedirect.com/science/article/pii/S0262885621001876</a>, doi:<a class="ltx_ref" href="https:/doi.org/https://doi.org/10.1016/j.imavis.2021.104282" title="">https://doi.org/10.1016/j.imavis.2021.104282</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bridgeman et al. [2019]</span>
<span class="ltx_bibblock">
Bridgeman, L., Volino, M., Guillemaut, J.Y., Hilton, A., 2019.

</span>
<span class="ltx_bibblock">Multi-person 3d pose estimation and tracking in sports, in: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 2487–2496.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/CVPRW.2019.00304" title="">10.1109/CVPRW.2019.00304</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. [2024]</span>
<span class="ltx_bibblock">
Cai, Y., Zhang, W., Wu, Y., Jin, C., 2024.

</span>
<span class="ltx_bibblock">Fusionformer: A concise unified feature fusion transformer for 3d pose estimation.

</span>
<span class="ltx_bibblock">Proceedings of the AAAI Conference on Artificial Intelligence 38, 900–908.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/27849" title="">https://ojs.aaai.org/index.php/AAAI/article/view/27849</a>, doi:<a class="ltx_ref" href="https:/doi.org/10.1609/aaai.v38i2.27849" title="">10.1609/aaai.v38i2.27849</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2017]</span>
<span class="ltx_bibblock">
Cao, Z., Simon, T., Wei, S.E., Sheikh, Y., 2017.

</span>
<span class="ltx_bibblock">Realtime multi-person 2d pose estimation using part affinity fields, in: CVPR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carraro et al. [2019]</span>
<span class="ltx_bibblock">
Carraro, M., Munaro, M., Burke, J., Menegatti, E., 2019.

</span>
<span class="ltx_bibblock">Real-time marker-less multi-person 3d pose estimation in rgb-depth camera networks, in: Intelligent Autonomous Systems 15, Springer International Publishing, Cham. pp. 534–545.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chavdarova et al. [2018]</span>
<span class="ltx_bibblock">
Chavdarova, T., Baqué, P., Bouquet, S., Maksai, A., Jose, C., Bagautdinov, T., Lettry, L., Fua, P., Van Gool, L., Fleuret, F., 2018.

</span>
<span class="ltx_bibblock">Wildtrack: A multi-camera hd dataset for dense unscripted pedestrian detection, in: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5030–5039.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/CVPR.2018.00528" title="">10.1109/CVPR.2018.00528</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020a]</span>
<span class="ltx_bibblock">
Chen, H., Guo, P., Li, P., Lee, G.H., Chirikjian, G., 2020a.

</span>
<span class="ltx_bibblock">Multi-person 3d pose estimation in crowded scenes based on multi-view geometry, in: Computer Vision – ECCV 2020, Springer International Publishing, Cham. pp. 541–557.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2024]</span>
<span class="ltx_bibblock">
Chen, L., Liu, T., Gong, Z., Wang, D., 2024.

</span>
<span class="ltx_bibblock">Movement function assessment based on human pose estimation from multi-view.

</span>
<span class="ltx_bibblock">Computer Systems Science and Engineering 48, 321 – 339.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191093708&amp;doi=10.32604%2fcsse.2023.037865&amp;partnerID=40&amp;md5=9f2e54e75483da59b7207dcb224d63f5" title="">https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191093708&amp;doi=10.32604%2fcsse.2023.037865&amp;partnerID=40&amp;md5=9f2e54e75483da59b7207dcb224d63f5</a>, doi:<a class="ltx_ref" href="https:/doi.org/10.32604/csse.2023.037865" title="">10.32604/csse.2023.037865</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023]</span>
<span class="ltx_bibblock">
Chen, Y., Gu, R., Huang, O., Jia, G., 2023.

</span>
<span class="ltx_bibblock">Vtp: Volumetric transformer for multi-view multi-person 3d pose estimation.

</span>
<span class="ltx_bibblock">Applied Intelligence 53, 26568–26579.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1007/s10489-023-04805-z" title="">10.1007/s10489-023-04805-z</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020b]</span>
<span class="ltx_bibblock">
Chen, Y., Tian, Y., He, M., 2020b.

</span>
<span class="ltx_bibblock">Monocular human pose estimation: A survey of deep learning-based methods.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding 192, 102897.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1077314219301778" title="">https://www.sciencedirect.com/science/article/pii/S1077314219301778</a>, doi:<a class="ltx_ref" href="https:/doi.org/https://doi.org/10.1016/j.cviu.2019.102897" title="">https://doi.org/10.1016/j.cviu.2019.102897</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. [2021]</span>
<span class="ltx_bibblock">
Chu, H., Lee, J.H., Lee, Y.C., Hsu, C.H., Li, J.D., Chen, C.S., 2021.

</span>
<span class="ltx_bibblock">Part-aware measurement for robust multi-view multi-human 3d pose estimation and tracking, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 1472–1481.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chunhui et al. [2017]</span>
<span class="ltx_bibblock">
Chunhui, L., Yueyu, H., Yanghao, L., Sijie, S., Jiaying, L., 2017.

</span>
<span class="ltx_bibblock">Pku-mmd: A large scale benchmark for continuous multi-modal human action understanding.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1703.07475 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dehaeck et al. [2022]</span>
<span class="ltx_bibblock">
Dehaeck, S., Domken, C., Bey-Temsamani, A., Abedrabbo, G., 2022.

</span>
<span class="ltx_bibblock">A strong geometric baseline for cross-view matching of multi-person 3d pose estimation from multi-view images, in: Image Analysis and Processing – ICIAP 2022, Springer International Publishing, Cham. pp. 77–88.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2023]</span>
<span class="ltx_bibblock">
Deng, J., Yao, H., Shi, P., 2023.

</span>
<span class="ltx_bibblock">Enhanced 3d pose estimation in multi-person, multi-view scenarios through unsupervised domain adaptation with dropout discriminator.

</span>
<span class="ltx_bibblock">Sensors (Basel, Switzerland) 23.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175273967&amp;doi=10.3390%2fs23208406&amp;partnerID=40&amp;md5=71c6d12d63fcbdc0b81aaf0a0fb8576c" title="">https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175273967&amp;doi=10.3390%2fs23208406&amp;partnerID=40&amp;md5=71c6d12d63fcbdc0b81aaf0a0fb8576c</a>, doi:<a class="ltx_ref" href="https:/doi.org/10.3390/s23208406" title="">10.3390/s23208406</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desmarais et al. [2021]</span>
<span class="ltx_bibblock">
Desmarais, Y., Mottet, D., Slangen, P., Montesinos, P., 2021.

</span>
<span class="ltx_bibblock">A review of 3d human pose estimation algorithms for markerless motion capture.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding 212, 103275.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1077314221001193" title="">https://www.sciencedirect.com/science/article/pii/S1077314221001193</a>, doi:<a class="ltx_ref" href="https:/doi.org/https://doi.org/10.1016/j.cviu.2021.103275" title="">https://doi.org/10.1016/j.cviu.2021.103275</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. [2022]</span>
<span class="ltx_bibblock">
Dong, J., Fang, Q., Jiang, W., Yang, Y., Huang, Q., Bao, H., Zhou, X., 2022.

</span>
<span class="ltx_bibblock">Fast and robust multi-person 3d pose estimation and tracking from multiple views.

</span>
<span class="ltx_bibblock">IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 6981–6992.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/TPAMI.2021.3098052" title="">10.1109/TPAMI.2021.3098052</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. [2019]</span>
<span class="ltx_bibblock">
Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X., 2019.

</span>
<span class="ltx_bibblock">Fast and robust multi-person 3d pose estimation from multiple views.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/1901.04111" title="">arXiv:1901.04111</a><span class="ltx_text ltx_font_typewriter" id="bib.bib24.1.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib25.5.5.1">Duarte et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib25.7.1">
Duarte, A., Palaskar, S., Ventura, L., Ghadiyaram, D., DeHaan, K., Metze, F., Torres, J., Giro-i Nieto, X., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib25.8.1">How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language, in: Conference on Computer Vision and Pattern Recognition (CVPR).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib26.5.5.1">Ershadi Nasab et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib26.7.1">
Ershadi Nasab, S., Kasaei, S., Sanaei, E., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib26.8.1">Uncalibrated multi-view multiple humans association and 3d pose estimation by adversarial learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib26.9.1">Multimedia Tools and Applications 80, 1--28.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib26.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1007/s11042-020-09733-5" title="">10.1007/s11042-020-09733-5</a><span class="ltx_text ltx_font_typewriter" id="bib.bib26.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib27.5.5.1">Ershadi-Nasab et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib27.7.1">
Ershadi-Nasab, S., Noury, E., Kasaei, S., Sanaei, E., 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib27.8.1">Multiple human 3d pose estimation from multiview images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib27.9.1">Multimedia Tools and Applications 77, 15573--15601.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib27.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11042-017-5133-8" title="">https://doi.org/10.1007/s11042-017-5133-8</a><span class="ltx_text ltx_font_typewriter" id="bib.bib27.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1007/s11042-017-5133-8" title="">10.1007/s11042-017-5133-8</a><span class="ltx_text ltx_font_typewriter" id="bib.bib27.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib28.5.5.1">Fan et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib28.7.1">
Fan, Z., Li, X., Li, Y., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib28.8.1">Multi-agent deep reinforcement learning for online 3d human poses estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib28.9.1">Remote Sensing 13.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib28.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2072-4292/13/19/3995" title="">https://www.mdpi.com/2072-4292/13/19/3995</a><span class="ltx_text ltx_font_typewriter" id="bib.bib28.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.3390/rs13193995" title="">10.3390/rs13193995</a><span class="ltx_text ltx_font_typewriter" id="bib.bib28.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib29.5.5.1">Feng et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib29.7.1">
Feng, Q., He, K., Wen, H., Keskin, C., Ye, Y., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib29.8.1">Rethinking the data annotation process for multi-view 3d pose estimation with active learning and self-training, in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 5695--5704.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib30.5.5.1">de França Silva et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib30.7.1">
de França Silva, D.W., do Monte Lima, J.P.S., Macêdo, D., Zanchettin, C., Thomas, D.G.F., Uchiyama, H., Teichrieb, V., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib30.8.1">Unsupervised multi-view multi-person 3d pose estimation using reprojection error, in: Artificial Neural Networks and Machine Learning -- ICANN 2022, Springer Nature Switzerland, Cham. pp. 482--494.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib31.5.5.1">de França Silva et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib31.7.1">
de França Silva, D.W., Silva Do Monte Lima, J.P., Francis Thomas, D.G., Uchiyama, H., Teichrieb, V., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib31.8.1">Umvpose++: Unsupervised multi-view multi-person 3d pose estimation using ground point matching, p. 607 – 614.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib31.9.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183599409&amp;doi=10.5220%2f0011668800003417&amp;partnerID=40&amp;md5=485a64f86b8058f9915ed28e3324d291" title="">https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183599409&amp;doi=10.5220%2f0011668800003417&amp;partnerID=40&amp;md5=485a64f86b8058f9915ed28e3324d291</a><span class="ltx_text ltx_font_typewriter" id="bib.bib31.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.5220/0011668800003417" title="">10.5220/0011668800003417</a><span class="ltx_text ltx_font_typewriter" id="bib.bib31.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib32.4.4.1">Gower [1975]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib32.6.1">
Gower, J.C., 1975.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib32.7.1">Generalized procrustes analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib32.8.1">Psychometrika 40, 33--51.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib32.9.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/BF02291478" title="">https://doi.org/10.1007/BF02291478</a><span class="ltx_text ltx_font_typewriter" id="bib.bib32.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1007/BF02291478" title="">10.1007/BF02291478</a><span class="ltx_text ltx_font_typewriter" id="bib.bib32.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib33.5.5.1">Hassan et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib33.7.1">
Hassan, M., Eberhardt, J., Malorodov, S., Jäger, M., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib33.8.1">Robust multiview 3d pose estimation using time of flight cameras.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib33.9.1">IEEE Sensors Journal 22, 2672--2684.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib33.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/JSEN.2021.3133108" title="">10.1109/JSEN.2021.3133108</a><span class="ltx_text ltx_font_typewriter" id="bib.bib33.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib34.5.5.1">Holte et al. [2012]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib34.7.1">
Holte, M.B., Tran, C., Trivedi, M.M., Moeslund, T.B., 2012.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib34.8.1">Human pose estimation and activity recognition from multi-view videos: Comparative explorations of recent developments.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib34.9.1">IEEE Journal of Selected Topics in Signal Processing 6, 538--552.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib34.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/JSTSP.2012.2196975" title="">10.1109/JSTSP.2012.2196975</a><span class="ltx_text ltx_font_typewriter" id="bib.bib34.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib35.5.5.1">Huang et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib35.7.1">
Huang, C., Jiang, S., Li, Y., Zhang, Z., Traish, J., Deng, C., Ferguson, S., Da Xu, R.Y., 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib35.8.1">End-to-end dynamic matching network for multi-view multi-person 3d pose estimation, in: Computer Vision -- ECCV 2020, Springer International Publishing, Cham. pp. 477--493.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib36.5.5.1">Hwang et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib36.7.1">
Hwang, T., Kim, J., Kim, M., 2023a.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib36.8.1">A distributed real-time 3d pose estimation framework based on asynchronous multiviews.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib36.9.1">KSII Transactions on Internet and Information Systems 17, 559--575.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib36.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.3837/tiis.2023.02.015" title="">10.3837/tiis.2023.02.015</a><span class="ltx_text ltx_font_typewriter" id="bib.bib36.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib37.5.5.1">Hwang et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib37.7.1">
Hwang, T., Kim, J., Kim, M., Kim, M., 2023b.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib37.8.1">A real-time multi-person 3d pose estimation system from multiple rgb-d views for live streaming of 3d animation, in: Companion Proceedings of the 28th International Conference on Intelligent User Interfaces, Association for Computing Machinery, New York, NY, USA. p. 105–107.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib37.9.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3581754.3584144" title="">https://doi.org/10.1145/3581754.3584144</a><span class="ltx_text ltx_font_typewriter" id="bib.bib37.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1145/3581754.3584144" title="">10.1145/3581754.3584144</a><span class="ltx_text ltx_font_typewriter" id="bib.bib37.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib38.5.5.1">Insafutdinov et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib38.7.1">
Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka, M., Schiele, B., 2016.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib38.8.1">Deepercut: A deeper, stronger, and faster multi-person pose estimation model, in: Computer Vision -- ECCV 2016, Springer International Publishing, Cham. pp. 34--50.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib39.5.5.1">Ionescu et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib39.7.1">
Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C., 2014.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib39.8.1">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib39.9.1">IEEE Transactions on Pattern Analysis and Machine Intelligence 36, 1325--1339.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib40.4.4.1">Jenni and Favaro [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib40.6.1">
Jenni, S., Favaro, P., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib40.7.1">Self-supervised multi-view synchronization learning for 3d pose estimation, in: Computer Vision -- ACCV 2020, Springer International Publishing, Cham. pp. 170--187.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib41.5.5.1">Ji et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib41.7.1">
Ji, X., Fang, Q., Dong, J., Shuai, Q., Jiang, W., Zhou, X., 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib41.8.1">A survey on monocular 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib41.9.1">Virtual Reality &amp; Intelligent Hardware 2, 471--500.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib41.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S2096579620300887" title="">https://www.sciencedirect.com/science/article/pii/S2096579620300887</a><span class="ltx_text ltx_font_typewriter" id="bib.bib41.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/https://doi.org/10.1016/j.vrih.2020.04.005" title="">https://doi.org/10.1016/j.vrih.2020.04.005</a><span class="ltx_text ltx_font_typewriter" id="bib.bib41.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib42.5.5.1">Jiang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib42.7.1">
Jiang, B., Hu, L., Xia, S., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib42.8.1">Probabilistic triangulation for uncalibrated multi-view 3d human pose estimation, in: 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 14804--14814.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib42.9.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/ICCV51070.2023.01364" title="">10.1109/ICCV51070.2023.01364</a><span class="ltx_text ltx_font_typewriter" id="bib.bib42.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib43.5.5.1">Joo et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib43.7.1">
Joo, H., Simon, T., Li, X., Liu, H., Tan, L., Gui, L., Banerjee, S., Godisart, T.S., Nabbe, B., Matthews, I., Kanade, T., Nobuhara, S., Sheikh, Y., 2017.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib43.8.1">Panoptic studio: A massively multiview system for social interaction capture.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib43.9.1">IEEE Transactions on Pattern Analysis and Machine Intelligence .
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib44.4.4.1">Kadkhodamohammadi and Padoy [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib44.6.1">
Kadkhodamohammadi, A., Padoy, N., 2019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib44.7.1">A generalizable approach for multi-view 3d human pose regression.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/1804.10462" title="">arXiv:1804.10462</a><span class="ltx_text ltx_font_typewriter" id="bib.bib44.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib45.5.5.1">Kazemi et al. [2013]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib45.7.1">
Kazemi, V., Burenius, M., Azizpour, H., Sullivan, J., 2013.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib45.8.1">Multi-view body part recognition with random forests.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib45.9.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.5244/C.27.48" title="">10.5244/C.27.48</a><span class="ltx_text ltx_font_typewriter" id="bib.bib45.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib46.4.4.1">Kazemi and Sullivan [2012]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib46.6.1">
Kazemi, V., Sullivan, J., 2012.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib46.7.1">Using richer models for articulated pose estimation of footballers, in: BMVC.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib47.5.5.1">Kumar et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib47.7.1">
Kumar, P., Chauhan, S., Awasthi, L.K., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib47.8.1">Human pose estimation using deep learning: review, methodologies, progress and future research directions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib47.9.1">International Journal of Multimedia Information Retrieval 11, 1--33.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib47.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1007/s13735-022-00261-6" title="">10.1007/s13735-022-00261-6</a><span class="ltx_text ltx_font_typewriter" id="bib.bib47.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib48.5.5.1">Kundu et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib48.7.1">
Kundu, J.N., Seth, S., au2, R.M.V., Rakesh, M., Babu, R.V., Chakraborty, A., 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib48.8.1">Kinematic-structure-preserved representation for unsupervised 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2006.14107" title="">arXiv:2006.14107</a><span class="ltx_text ltx_font_typewriter" id="bib.bib48.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib49.5.5.1">Lan et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib49.7.1">
Lan, G., Wu, Y., Hu, F., Hao, Q., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib49.8.1">Vision-based human pose estimation via deep learning: A survey.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib49.9.1">IEEE Transactions on Human-Machine Systems 53, 253--268.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib49.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/THMS.2022.3219242" title="">10.1109/THMS.2022.3219242</a><span class="ltx_text ltx_font_typewriter" id="bib.bib49.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib50.4.4.1">Lin and Lee [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib50.6.1">
Lin, J., Lee, G.H., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib50.7.1">Multi-view multi-person 3d pose estimation with plane sweep stereo, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11886--11895.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib51.5.5.1">Lin et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib51.7.1">
Lin, J., Li, S., Qin, H., Wang, H., Cui, N., Jiang, Q., Jian, H., Wang, G., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib51.8.1">Overview of 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib51.9.1">CMES - Computer Modeling in Engineering and Sciences 134, 1621 – 1651.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib51.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138834832&amp;doi=10.32604%2fcmes.2022.020857&amp;partnerID=40&amp;md5=e77e2ad7b0e050ca270f5a2ce7f97d3d" title="">https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138834832&amp;doi=10.32604%2fcmes.2022.020857&amp;partnerID=40&amp;md5=e77e2ad7b0e050ca270f5a2ce7f97d3d</a><span class="ltx_text ltx_font_typewriter" id="bib.bib51.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.32604/cmes.2022.020857" title="">10.32604/cmes.2022.020857</a><span class="ltx_text ltx_font_typewriter" id="bib.bib51.12.3">. cited by: 1; All Open Access, Gold Open Access.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib52.5.5.1">Liu et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib52.7.1">
Liu, J., Shahroudy, A., Perez, M., Wang, G., Duan, L.Y., Kot, A.C., 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib52.8.1">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib52.9.1">IEEE Transactions on Pattern Analysis and Machine Intelligence 42, 2684--2701.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib52.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/TPAMI.2019.2916873" title="">10.1109/TPAMI.2019.2916873</a><span class="ltx_text ltx_font_typewriter" id="bib.bib52.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib53.5.5.1">Liu et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib53.7.1">
Liu, L., Yang, L., Chen, W., Gao, X., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib53.8.1">Dual-view 3d human pose estimation without camera parameters for action recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib53.9.1">IET Image Processing 15, 3433--3440.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib53.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12277" title="">https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12277</a><span class="ltx_text ltx_font_typewriter" id="bib.bib53.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/https://doi.org/10.1049/ipr2.12277" title="">https://doi.org/10.1049/ipr2.12277</a><span class="ltx_text ltx_font_typewriter" id="bib.bib53.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib54.5.5.1">Liu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib54.7.1">
Liu, W., Bao, Q., Sun, Y., Mei, T., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib54.8.1">Recent advances of monocular 2d and 3d human pose estimation: A deep learning perspective 55.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib54.9.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3524497" title="">https://doi.org/10.1145/3524497</a><span class="ltx_text ltx_font_typewriter" id="bib.bib54.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1145/3524497" title="">10.1145/3524497</a><span class="ltx_text ltx_font_typewriter" id="bib.bib54.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib55.5.5.1">Liu et al. [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib55.7.1">
Liu, Z., Zhu, J., Bu, J., Chen, C., 2015.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib55.8.1">A survey of human pose estimation: The body parts parsing based methods.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib55.9.1">Journal of Visual Communication and Image Representation 32, 10--19.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib55.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1047320315001121" title="">https://www.sciencedirect.com/science/article/pii/S1047320315001121</a><span class="ltx_text ltx_font_typewriter" id="bib.bib55.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/https://doi.org/10.1016/j.jvcir.2015.06.013" title="">https://doi.org/10.1016/j.jvcir.2015.06.013</a><span class="ltx_text ltx_font_typewriter" id="bib.bib55.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib56.5.5.1">Ma et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib56.7.1">
Ma, Z., Li, K., Li, Y., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib56.8.1">Self-supervised method for 3d human pose estimation with consistent shape and viewpoint factorization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib56.9.1">Applied Intelligence 53, 3864--3876.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib56.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1007/s10489-022-03714-x" title="">10.1007/s10489-022-03714-x</a><span class="ltx_text ltx_font_typewriter" id="bib.bib56.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib57.4.4.1">Manesco and Marana [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib57.6.1">
Manesco, J.R.R., Marana, A.N., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib57.7.1">A survey of recent advances on two-step 3d human pose estimation, in: Xavier-Junior, J.C., Rios, R.A. (Eds.), Intelligent Systems, Springer International Publishing, Cham. pp. 266--281.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib58.5.5.1">Mehrizi et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib58.7.1">
Mehrizi, R., Peng, X., Tang, Z., Xu, X., Metaxas, D., Li, K., 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib58.8.1">Toward marker-free 3d pose estimation in lifting: A deep multi-view solution.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/1802.01741" title="">arXiv:1802.01741</a><span class="ltx_text ltx_font_typewriter" id="bib.bib58.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib59.5.5.1">Mehta et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib59.7.1">
Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko, O., Xu, W., Theobalt, C., 2017.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib59.8.1">Monocular 3d human pose estimation in the wild using improved cnn supervision, in: 3D Vision (3DV), 2017 Fifth International Conference on, IEEE.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib59.9.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://gvv.mpi-inf.mpg.de/3dhp_dataset" title="">http://gvv.mpi-inf.mpg.de/3dhp_dataset</a><span class="ltx_text ltx_font_typewriter" id="bib.bib59.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/3dv.2017.00064" title="">10.1109/3dv.2017.00064</a><span class="ltx_text ltx_font_typewriter" id="bib.bib59.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib60.5.5.1">Mustafa et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib60.7.1">
Mustafa, A., Russell, C., Hilton, A., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib60.8.1">4d temporally coherent multi-person semantic reconstruction and segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib60.9.1">International Journal of Computer Vision 130, 1--24.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib60.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1007/s11263-022-01599-4" title="">10.1007/s11263-022-01599-4</a><span class="ltx_text ltx_font_typewriter" id="bib.bib60.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib61.4.4.1">Nakatsuka and Komorita [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib61.6.1">
Nakatsuka, C., Komorita, S., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib61.7.1">Stable 3d human pose estimation in low-resolution videos with a few views, pp. 427--433.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib61.8.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/ICRA46639.2022.9812382" title="">10.1109/ICRA46639.2022.9812382</a><span class="ltx_text ltx_font_typewriter" id="bib.bib61.9.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib62.5.5.1">Ohkawa et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib62.7.1">
Ohkawa, T., Furuta, R., Sato, Y., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib62.8.1">Efficient annotation and learning for 3d hand pose estimation: A survey.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib62.9.1">International Journal of Computer Vision , 1--14doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1007/s11263-023-01856-0" title="">10.1007/s11263-023-01856-0</a><span class="ltx_text ltx_font_typewriter" id="bib.bib62.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib63.5.5.1">Page et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib63.7.1">
Page, M.J., McKenzie, J.E., Bossuyt, P.M.M., Boutron, I., Hoffmann, T.C., Mulrow, C.D., Shamseer, L., Tetzlaff, J.M., Moher, D., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib63.8.1">Updating guidance for reporting systematic reviews: development of the prisma 2020 statement.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib63.9.1">Journal of clinical epidemiology .
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib64.5.5.1">Reddy et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib64.7.1">
Reddy, N., Guigues, L., Pishchulin, L., Eledath, J., Narasimhan, S.G., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib64.8.1">Tessetrack: End-to-end learnable multi-person articulated 3d pose tracking, in: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE Computer Society, Los Alamitos, CA, USA. pp. 15185--15195.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib64.9.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.01494" title="">https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.01494</a><span class="ltx_text ltx_font_typewriter" id="bib.bib64.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/CVPR46437.2021.01494" title="">10.1109/CVPR46437.2021.01494</a><span class="ltx_text ltx_font_typewriter" id="bib.bib64.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib65.5.5.1">Remelli et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib65.7.1">
Remelli, E., Han, S., Honari, S., Fua, P., Wang, R., 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib65.8.1">Lightweight multi-view 3d pose estimation through camera-disentangled representation.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2004.02186" title="">arXiv:2004.02186</a><span class="ltx_text ltx_font_typewriter" id="bib.bib65.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib66.5.5.1">Ren et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib66.7.1">
Ren, Y., Wang, Z., Wang, Y., Tan, S., Chen, Y., Yang, J., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib66.8.1">Gopose: 3d human pose estimation using wifi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib66.9.1">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 6, 1--25.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib66.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1145/3534605" title="">10.1145/3534605</a><span class="ltx_text ltx_font_typewriter" id="bib.bib66.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib67.5.5.1">Rhodin et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib67.7.1">
Rhodin, H., Salzmann, M., Fua, P., 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib67.8.1">Unsupervised geometry-aware representation for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/1804.01110" title="">arXiv:1804.01110</a><span class="ltx_text ltx_font_typewriter" id="bib.bib67.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib68.5.5.1">Rochette et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib68.7.1">
Rochette, G., Russell, C., Bowden, R., 2019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib68.8.1">Weakly-supervised 3d pose estimation from a single image using multi-view consistency.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/1909.06119" title="">arXiv:1909.06119</a><span class="ltx_text ltx_font_typewriter" id="bib.bib68.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib69.5.5.1">Rohrbach et al. [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib69.7.1">
Rohrbach, M., Rohrbach, A., Regneri, M., Amin, S., Andriluka, M., Pinkal, M., Schiele, B., 2015.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib69.8.1">Recognizing fine-grained and composite activities using hand-centric features and script data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib69.9.1">International Journal of Computer Vision , 1--28URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1007/s11263-015-0851-8" title="">http://dx.doi.org/10.1007/s11263-015-0851-8</a><span class="ltx_text ltx_font_typewriter" id="bib.bib69.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1007/s11263-015-0851-8" title="">10.1007/s11263-015-0851-8</a><span class="ltx_text ltx_font_typewriter" id="bib.bib69.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib70.5.5.1">Sarafianos et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib70.7.1">
Sarafianos, N., Boteanu, B., Ionescu, B., Kakadiaris, I.A., 2016.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib70.8.1">3d human pose estimation: A review of the literature and analysis of covariates.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib70.9.1">Computer Vision and Image Understanding 152, 1--20.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib70.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1077314216301369" title="">https://www.sciencedirect.com/science/article/pii/S1077314216301369</a><span class="ltx_text ltx_font_typewriter" id="bib.bib70.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/https://doi.org/10.1016/j.cviu.2016.09.002" title="">https://doi.org/10.1016/j.cviu.2016.09.002</a><span class="ltx_text ltx_font_typewriter" id="bib.bib70.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib71.4.4.1">Schwarcz and Pollard [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib71.6.1">
Schwarcz, S., Pollard, T., 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib71.7.1">3d human pose estimation from deep multi-view 2d pose, in: 2018 24th International Conference on Pattern Recognition (ICPR), IEEE.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib71.8.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1109/ICPR.2018.8545631" title="">http://dx.doi.org/10.1109/ICPR.2018.8545631</a><span class="ltx_text ltx_font_typewriter" id="bib.bib71.9.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/icpr.2018.8545631" title="">10.1109/icpr.2018.8545631</a><span class="ltx_text ltx_font_typewriter" id="bib.bib71.10.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib72.5.5.1">Shahroudy et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib72.7.1">
Shahroudy, A., Liu, J., Ng, T.T., Wang, G., 2016.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib72.8.1">Ntu rgb+d: A large scale dataset for 3d human activity analysis, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1010--1019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib72.9.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/CVPR.2016.115" title="">10.1109/CVPR.2016.115</a><span class="ltx_text ltx_font_typewriter" id="bib.bib72.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib73.5.5.1">Shere et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib73.7.1">
Shere, M., Kim, H., Hilton, A., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib73.8.1">Temporally consistent 3d human pose estimation using dual 360° cameras, in: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 81--90.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib73.9.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/WACV48630.2021.00013" title="">10.1109/WACV48630.2021.00013</a><span class="ltx_text ltx_font_typewriter" id="bib.bib73.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib74.5.5.1">Sigal et al. [2010]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib74.7.1">
Sigal, L., Balan, A., Black, M., 2010.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib74.8.1">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib74.9.1">International Journal of Computer Vision 87, 4--27.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib74.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1007/s11263-009-0273-6" title="">10.1007/s11263-009-0273-6</a><span class="ltx_text ltx_font_typewriter" id="bib.bib74.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib75.5.5.1">Solichah et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib75.7.1">
Solichah, U., Purnomo, M.H., Yuniarno, E.M., 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib75.8.1">Marker-less motion capture based on openpose model using triangulation, in: 2020 International Seminar on Intelligent Technology and Its Applications (ISITIA), pp. 217--222.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib75.9.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/ISITIA49792.2020.9163662" title="">10.1109/ISITIA49792.2020.9163662</a><span class="ltx_text ltx_font_typewriter" id="bib.bib75.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib76.5.5.1">Song et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib76.7.1">
Song, L., Yu, G., Yuan, J., Liu, Z., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib76.8.1">Human pose estimation and its application to action recognition: A survey.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib76.9.1">Journal of Visual Communication and Image Representation 76, 103055.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib76.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1047320321000262" title="">https://www.sciencedirect.com/science/article/pii/S1047320321000262</a><span class="ltx_text ltx_font_typewriter" id="bib.bib76.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/https://doi.org/10.1016/j.jvcir.2021.103055" title="">https://doi.org/10.1016/j.jvcir.2021.103055</a><span class="ltx_text ltx_font_typewriter" id="bib.bib76.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib77.5.5.1">Song et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib77.7.1">
Song, R., Zhang, D., Wu, Z., Yu, C., Xie, C., Yang, S., Hu, Y., Chen, Y., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib77.8.1">Rf-url: unsupervised representation learning for rf sensing, in: Proceedings of the 28th Annual International Conference on Mobile Computing And Networking, Association for Computing Machinery, New York, NY, USA. p. 282–295.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib77.9.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3495243.3560529" title="">https://doi.org/10.1145/3495243.3560529</a><span class="ltx_text ltx_font_typewriter" id="bib.bib77.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1145/3495243.3560529" title="">10.1145/3495243.3560529</a><span class="ltx_text ltx_font_typewriter" id="bib.bib77.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib78.5.5.1">Srivastav et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib78.7.1">
Srivastav, V., Issenhuth, T., Kadkhodamohammadi, A., de Mathelin, M., Gangi, A., Padoy, N., 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib78.8.1">Mvor: A multi-view rgb-d operating room dataset for 2d and 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/1808.08180" title="">arXiv:1808.08180</a><span class="ltx_text ltx_font_typewriter" id="bib.bib78.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib79.5.5.1">Tang et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib79.7.1">
Tang, Z., Gu, R., Hwang, J.N., 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib79.8.1">Joint multi-view people tracking and pose estimation for 3d scene reconstruction, in: 2018 IEEE International Conference on Multimedia and Expo (ICME), pp. 1--6.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib79.9.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/ICME.2018.8486576" title="">10.1109/ICME.2018.8486576</a><span class="ltx_text ltx_font_typewriter" id="bib.bib79.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib80.5.5.1">Trumble et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib80.7.1">
Trumble, M., Gilbert, A., Malleson, C., Hilton, A., Collomosse, J., 2017.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib80.8.1">Total capture: 3d human pose estimation fusing video and inertial sensors, in: 2017 British Machine Vision Conference (BMVC).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib81.5.5.1">Tu et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib81.7.1">
Tu, H., Wang, C., Zeng, W., 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib81.8.1">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2004.06239" title="">arXiv:2004.06239</a><span class="ltx_text ltx_font_typewriter" id="bib.bib81.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib82.5.5.1">Wan et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib82.7.1">
Wan, X., Chen, Z., Zhao, X., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib82.8.1">View consistency aware holistic triangulation for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib82.9.1">Computer Vision and Image Understanding 236, 103830.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib82.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1077314223002102" title="">https://www.sciencedirect.com/science/article/pii/S1077314223002102</a><span class="ltx_text ltx_font_typewriter" id="bib.bib82.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/https://doi.org/10.1016/j.cviu.2023.103830" title="">https://doi.org/10.1016/j.cviu.2023.103830</a><span class="ltx_text ltx_font_typewriter" id="bib.bib82.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib83.4.4.1">Wang and Sun [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib83.6.1">
Wang, H., Sun, M., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib83.7.1">Smart-vposenet: 3d human pose estimation models and methods based on multi-view discriminant network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib83.8.1">Knowledge-Based Systems 239, 107992.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib83.9.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0950705121011060" title="">https://www.sciencedirect.com/science/article/pii/S0950705121011060</a><span class="ltx_text ltx_font_typewriter" id="bib.bib83.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/https://doi.org/10.1016/j.knosys.2021.107992" title="">https://doi.org/10.1016/j.knosys.2021.107992</a><span class="ltx_text ltx_font_typewriter" id="bib.bib83.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib84.5.5.1">Wang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib84.7.1">
Wang, H., hui Sun, M., Zhang, H., yan Dong, L., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib84.8.1">Lhpe-nets: A lightweight 2d and 3d human pose estimation model with well-structural deep networks and multi-view pose sample simplification method.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib84.9.1">PLoS ONE 17.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib84.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1371/journal.pone.0264302" title="">https://doi.org/10.1371/journal.pone.0264302</a><span class="ltx_text ltx_font_typewriter" id="bib.bib84.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib85.5.5.1">Wang et al. [2021a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib85.7.1">
Wang, J., Tan, S., Zhen, X., Xu, S., Zheng, F., He, Z., Shao, L., 2021a.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib85.8.1">Deep 3d human pose estimation: A review.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib85.9.1">Computer Vision and Image Understanding 210, 103225.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib85.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1077314221000692" title="">https://www.sciencedirect.com/science/article/pii/S1077314221000692</a><span class="ltx_text ltx_font_typewriter" id="bib.bib85.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/https://doi.org/10.1016/j.cviu.2021.103225" title="">https://doi.org/10.1016/j.cviu.2021.103225</a><span class="ltx_text ltx_font_typewriter" id="bib.bib85.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib86.5.5.1">Wang et al. [2021b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib86.7.1">
Wang, T., Zhang, J., Cai, Y., Yan, S., Feng, J., 2021b.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib86.8.1">Direct multi-view multi-person 3d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2111.04076" title="">arXiv:2111.04076</a><span class="ltx_text ltx_font_typewriter" id="bib.bib86.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib87.5.5.1">Wang et al. [2021c]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib87.7.1">
Wang, Y., Guo, L., Lu, Z., Wen, X., Zhou, S., Meng, W., 2021c.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib87.8.1">From point to space: 3d moving human pose estimation using commodity wifi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib87.9.1">IEEE Communications Letters 25, 2235--2239.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib87.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/LCOMM.2021.3073271" title="">10.1109/LCOMM.2021.3073271</a><span class="ltx_text ltx_font_typewriter" id="bib.bib87.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib88.4.4.1">Wang and Chung [2010]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib88.6.1">
Wang, Z., Chung, R., 2010.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib88.7.1">Integrating multiple uncalibrated views for human 3d pose estimation, in: Advances in Visual Computing, Springer Berlin Heidelberg, Berlin, Heidelberg. pp. 280--290.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib89.4.4.1">Xia and Zhang [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib89.6.1">
Xia, H., Zhang, Q., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib89.7.1">Vitpose: Multi-view 3d human pose estimation with vision transformer, in: 2022 IEEE 8th International Conference on Computer and Communications (ICCC), pp. 1922--1927.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib89.8.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/ICCC56324.2022.10065997" title="">10.1109/ICCC56324.2022.10065997</a><span class="ltx_text ltx_font_typewriter" id="bib.bib89.9.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib90.5.5.1">Xie et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib90.7.1">
Xie, C., Zhang, D., Wu, Z., Yu, C., Hu, Y., Chen, Y., 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib90.8.1">Rpm 2.0: Rf-based pose machines for multi-person 3d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib90.9.1">IEEE Transactions on Circuits and Systems for Video Technology 34, 490--503.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib90.10.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/TCSVT.2023.3287329" title="">10.1109/TCSVT.2023.3287329</a><span class="ltx_text ltx_font_typewriter" id="bib.bib90.11.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib91.5.5.1">Xie et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib91.7.1">
Xie, C., Zhang, D., Wu, Z., Yu, C., Hu, Y., Sun, Q., Chen, Y., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib91.8.1">Rf-based multi-view pose machine for multi-person 3d pose estimation, in: 2023 IEEE International Conference on Multimedia and Expo (ICME), pp. 2669--2674.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib91.9.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/ICME55011.2023.00454" title="">10.1109/ICME55011.2023.00454</a><span class="ltx_text ltx_font_typewriter" id="bib.bib91.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib92.5.5.1">Xu et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib92.7.1">
Xu, C., Yu, X., Wang, Z., Ou, L., 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib92.8.1">Multi-view human pose estimation in human-robot interaction, in: IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society, pp. 4769--4775.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib92.9.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/IECON43393.2020.9255211" title="">10.1109/IECON43393.2020.9255211</a><span class="ltx_text ltx_font_typewriter" id="bib.bib92.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib93.4.4.1">Xu and Kitani [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib93.6.1">
Xu, Y., Kitani, K., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib93.7.1">Multi-view multi-person 3d pose estimation with uncalibrated camera networks, in: 33rd British Machine Vision Conference 2022, BMVC 2022, London, UK, November 21-24, 2022, BMVA Press.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib93.8.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bmvc2022.mpi-inf.mpg.de/0132.pdf" title="">https://bmvc2022.mpi-inf.mpg.de/0132.pdf</a><span class="ltx_text ltx_font_typewriter" id="bib.bib93.9.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib94.5.5.1">Ye et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib94.7.1">
Ye, H., Zhu, W., Wang, C., Wu, R., Wang, Y., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib94.8.1">Faster voxelpose: Real-time 3d human pose estimation by orthographic projection.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2207.10955" title="">arXiv:2207.10955</a><span class="ltx_text ltx_font_typewriter" id="bib.bib94.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib95.5.5.1">Yin et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib95.7.1">
Yin, W., Chen, L., Huang, X., Huang, C., Wang, Z., Bian, Y., Wan, Y., Zhou, Y., Han, T., Yi, M., 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib95.8.1">A self-supervised spatio-temporal attention network for video-based 3d infant pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib95.9.1">Medical Image Analysis 96, 103208.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib95.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1361841524001336" title="">https://www.sciencedirect.com/science/article/pii/S1361841524001336</a><span class="ltx_text ltx_font_typewriter" id="bib.bib95.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/https://doi.org/10.1016/j.media.2024.103208" title="">https://doi.org/10.1016/j.media.2024.103208</a><span class="ltx_text ltx_font_typewriter" id="bib.bib95.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib96.5.5.1">Zhang et al. [2021a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib96.7.1">
Zhang, D., Wu, Y., Guo, M., Chen, Y., 2021a.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib96.8.1">Deep learning methods for 3d human pose estimation under different supervision paradigms: A survey.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib96.9.1">Electronics 10.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib96.10.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2079-9292/10/18/2267" title="">https://www.mdpi.com/2079-9292/10/18/2267</a><span class="ltx_text ltx_font_typewriter" id="bib.bib96.11.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.3390/electronics10182267" title="">10.3390/electronics10182267</a><span class="ltx_text ltx_font_typewriter" id="bib.bib96.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib97.5.5.1">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib97.7.1">
Zhang, X., Wang, M., Zeng, M., Kang, W., Deng, F., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib97.8.1">Humomm: A multi-modal dataset and benchmark for human motion analysis, in: Lu, H., Ouyang, W., Huang, H., Lu, J., Liu, R., Dong, J., Xu, M. (Eds.), Image and Graphics, Springer Nature Switzerland, Cham. pp. 204--215.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib97.9.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1007/978-3-031-46305-1_17" title="">10.1007/978-3-031-46305-1_17</a><span class="ltx_text ltx_font_typewriter" id="bib.bib97.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib98.5.5.1">Zhang et al. [2021b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib98.7.1">
Zhang, Y., Wang, C., Wang, X., Liu, W., Zeng, W., 2021b.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib98.8.1">Voxeltrack: Multi-person 3d human pose estimation and tracking in the wild.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2108.02452" title="">arXiv:2108.02452</a><span class="ltx_text ltx_font_typewriter" id="bib.bib98.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib99.5.5.1">Zhao et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib99.7.1">
Zhao, J., Yu, T., An, L., Huang, Y., Deng, F., Dai, Q., 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib99.8.1">Triangulation residual loss for data-efficient 3d pose estimation, in: Proceedings of the 37th International Conference on Neural Information Processing Systems, Curran Associates Inc., Red Hook, NY, USA.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib100.5.5.1">Zheng et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib100.7.1">
Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J., Kehtarnavaz, N., Shah, M., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib100.8.1">Deep learning-based human pose estimation: A survey.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib100.9.1">ACM Comput. Surv. URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3603618" title="">https://doi.org/10.1145/3603618</a><span class="ltx_text ltx_font_typewriter" id="bib.bib100.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1145/3603618" title="">10.1145/3603618</a><span class="ltx_text ltx_font_typewriter" id="bib.bib100.11.3">. just Accepted.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib101.5.5.1">Zhou et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib101.7.1">
Zhou, M., Liu, R., Yi, P., Zhou, D., Zhang, Q., Wei, X., 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib101.8.1">Er-net: Efficient recalibration network for multi-view multi-person 3d pose estimation, in: 2022 8th International Conference on Virtual Reality (ICVR), pp. 298--305.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib101.9.1">doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1109/ICVR55215.2022.9847965" title="">10.1109/ICVR55215.2022.9847965</a><span class="ltx_text ltx_font_typewriter" id="bib.bib101.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib102.5.5.1">Zhu et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib102.7.1">
Zhu, A., Snoussi, H., Cherouat, A., 2014.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib102.8.1">Articulated human motion tracking with foreground learning, in: 2014 22nd European Signal Processing Conference (EUSIPCO), pp. 366--370.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib103.5.5.1">Zhu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib103.7.1">
Zhu, Z., Liu, S., Shuai, J., Du, S., Li, Y., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib103.8.1">3d associative embedding: Multi-view 3d human pose estimation in crowded scenes, in: Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things, Association for Computing Machinery, New York, NY, USA. p. 131–139.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib103.9.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3603781.3603804" title="">https://doi.org/10.1145/3603781.3603804</a><span class="ltx_text ltx_font_typewriter" id="bib.bib103.10.2">, doi:</span><a class="ltx_ref ltx_font_typewriter" href="https:/doi.org/10.1145/3603781.3603804" title="">10.1145/3603781.3603804</a><span class="ltx_text ltx_font_typewriter" id="bib.bib103.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib104.4.4.1">Zhuang and Zhou [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib104.6.1">
Zhuang, Z., Zhou, Y., 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib104.7.1">Fastervoxelpose+: Fast and accurate voxel-based 3d human pose estimation by depth-wise projection decay, p. 1763 – 1778.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib104.8.1">URL: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189627610&amp;partnerID=40&amp;md5=159b2ac1f0e5a5b11c23b439bb0410c2" title="">https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189627610&amp;partnerID=40&amp;md5=159b2ac1f0e5a5b11c23b439bb0410c2</a><span class="ltx_text ltx_font_typewriter" id="bib.bib104.9.2">.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p5">
<span class="ltx_ERROR undefined" id="p5.1">\bio</span><span class="ltx_ERROR undefined" id="p5.2">\endbio</span>
</div>
<div class="ltx_para" id="p6">
<span class="ltx_ERROR undefined" id="p6.1">\endbio</span>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jul  4 10:09:04 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
