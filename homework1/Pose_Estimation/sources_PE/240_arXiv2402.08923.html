<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture</title>
<!--Generated on Fri Feb 16 22:06:51 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.08923v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S1" title="1 Introduction ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S2" title="2 Related works ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S3" title="3 Preliminaries ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S4" title="4 System design ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>System design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S4.SS1" title="4.1 Data ‣ 4 System design ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S4.SS2" title="4.2 The LSTM Model ‣ 4 System design ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>The LSTM Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S4.SS3" title="4.3 Model Interpretation ‣ 4 System design ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Model Interpretation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S4.SS4" title="4.4 Adding The Transformer ‣ 4 System design ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Adding The Transformer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5" title="5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.SS1" title="5.1 Discussion of Model Performance ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Discussion of Model Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.SS2" title="5.2 Discussion of “Optimal” IMU Locations ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Discussion of “Optimal” IMU Locations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S6" title="6 Conclusion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2402.08923v2 [cs.LG] 16 Feb 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Varun Ramani
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Hossein Khayami
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Yang Bai
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Nakul Garg
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Nirupam Roy 
<br class="ltx_break"/>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">University of Maryland, College Park
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This paper presents a novel approach for predicting human poses using IMU data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose, which use up to 6 IMUs in conjunction with bidirectional RNNs. We introduce two main innovations: a data-driven strategy for optimal IMU placement and a transformer-based model architecture for time series analysis. Our findings indicate that our approach not only outperforms traditional 6 IMU-based biRNN models but also that the transformer architecture significantly enhances pose reconstruction from data obtained from 24 IMU locations, with equivalent performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by our optimally chosen locations, when coupled with the parallelizability and performance of transformers, provides significant improvements to the field of IMU-based pose estimation.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Human pose estimation is essential for applications in animation, gaming, healthcare, and autonomous driving. Recent integration of full-body motion capture technologies like Xsens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib3" title="">3</a>]</cite> has expanded possibilities in gaming, fitness, and rehabilitation. Despite benefits, consumer adoption remains limited due to inconveniences associated with retrofitting homes or wearing specialized suits.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Traditionally dominated by vision-based techniques utilizing cameras, pose estimation faces challenges in environments with occlusions, lighting variations, or incomplete subject visibility. Recent computer vision advancements have improved accuracy, but challenges persist in occlusion and constrained environments.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">An alternative involves leveraging Inertial Measurement Units (IMUs) in everyday devices, including smartphones, smartwatches, and wearables like activPAL<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib1" title="">1</a>]</cite>. This project explores infrastructure-free body pose estimation using IMU data. IMUs, measuring rotational velocity and linear acceleration, offer advantages over vision-based methods, being less affected by environmental factors and providing real-time pose information. Highly portable and suitable for various applications, IMUs are promising for fitness tracking, rehabilitation, and virtual reality.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The motivation for using IMUs in pose estimation stems from their ability to capture motion data directly from the subject, thus providing a high degree of freedom and flexibility. This is particularly beneficial in scenarios where camera-based systems are impractical or intrusive. Furthermore, the proliferation of consumer devices equipped with IMUs, such as smartphones and smartwatches, opens up new possibilities for accessible and ubiquitous pose estimation solutions.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this project, we push the boundaries of pose estimation by delving into the realm of sparse Inertial Measurement Unit (IMU) configurations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib14" title="">14</a>]</cite>, endeavoring to reconstruct precise user poses using a significantly reduced number of sensors compared to conventional approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib3" title="">3</a>]</cite>. This endeavor introduces a unique set of difficulties stemming from the inherent ambiguity of sparse IMU data, where a given set of IMU readings may correspond to a myriad of potential poses. The previous works on using sparse IMUs such as DIP-IMU<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib7" title="">7</a>]</cite>, TransPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib15" title="">15</a>]</cite>, and PIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib14" title="">14</a>]</cite> intuitively selected 6 joints to place sensors. However, our methodology goes beyond traditional strategic methods, incorporating an LSTM to extract more nuanced information from the sparse IMU data. Using model interpretation tools such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib2" title="">2</a>]</cite>, we find the joints that most contribute to the pose estimation and cherry-pick them for our model. Finally, we develop a novel network based on the transformer and investigate its potential as a replacement for the LSTM.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We begin by examining the Deep Inertial Poser (DIP-IMU) presented by Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib7" title="">7</a>]</cite>. This groundbreaking work introduces a method for real-time 3D human pose reconstruction using a minimal set of six body-worn Inertial Measurement Units (IMUs). The core innovation of DIP-IMU lies in its effective use of deep learning, particularly a bidirectional Recurrent Neural Network (RNN), to overcome the challenge of inferring complex human poses from sparse IMU data. By leveraging synthetic data generated from extensive motion capture databases, DIP-IMU trains its model to accurately predict human poses in a variety of real-world scenarios. This approach not only advances the field of pose estimation by reducing the need for extensive sensor setups but also demonstrates significant improvements in terms of accuracy and computational efficiency over previous methods.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Building on the work done in DIP-IMU, TransPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib13" title="">13</a>]</cite> attempts to predict translations as well as poses. This innovative approach represents a significant advancement in the realm of IMU-based pose estimation, as it not only captures the static poses but also the dynamic movements and global translations of the human body. The methodology employed by TransPose is noteworthy for its multi-stage network architecture, which systematically breaks down the task of pose estimation into more manageable sub-tasks. This design enhances the system’s ability to interpret sparse IMU data, leading to more accurate and granular pose reconstructions. TransPose’s ability to estimate global translations sets it apart from its predecessors. It utilizes a fusion of two complementary methods: a foot-ground contact estimation based on the IMU measurements, and a root velocity regressor that predicts the local velocities of the root in its coordinate frame. The combination of these methods allows for a more robust and precise estimation of the body’s movement in space, which is a critical aspect often overlooked in traditional pose estimation systems.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Finally, IMUPoser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib9" title="">9</a>]</cite> builds on the foundations laid by DIP-IMU and TransPose, pioneering an approach that integrates inertial data from widely-used consumer devices such as smartphones, smartwatches, and earbuds. IMUPoser’s algorithm is adaptable, capable of effectively utilizing various configurations of IMUs, regardless of their placement on the body. By prioritizing user convenience and the practicality of device availability, IMUPoser addresses key challenges in the field of motion capture.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">However, each of these papers presents the same opportunity for advancement - IMU locations are either chosen arbitrarily, in the case of DIP-IMU and TransPose, or somewhat inherited from the nature of the system, as in the case of IMUPoser. We will build on the pioneering work of these papers to try and figure out exactly where we can place our IMUs to achieve higher accuracy with the same, minimal number of IMUs.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminaries</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.3">First, we should define <span class="ltx_text ltx_font_bold" id="S3.p1.3.1">pose estimation</span>. Pose estimation is the task of mapping some data <math alttext="X" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_X</annotation></semantics></math>, in this case IMU data, to a set of <math alttext="n" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_n</annotation></semantics></math> (usually 24) joint rotations <math alttext="\theta^{*}" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><msup id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">θ</mi><mo id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">superscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">𝜃</ci><times id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\theta^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_θ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math>. Take a moment to think about why all possible poses can be uniquely represented by a set of joint rotations (and why information like joint positions is redundant).</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.8">To achieve this task, Inertial Measurement Units are central to our study. These devices fuse an accelerometer and gyroscope to measure linear and angular motion. A sample from an IMU contains 3D acceleration and rotation - so acceleration in <math alttext="x" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_x</annotation></semantics></math>, <math alttext="y" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_y</annotation></semantics></math>, and <math alttext="z" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">italic_z</annotation></semantics></math> directions, along with <math alttext="roll" class="ltx_Math" display="inline" id="S3.p2.4.m4.1"><semantics id="S3.p2.4.m4.1a"><mrow id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">r</mi><mo id="S3.p2.4.m4.1.1.1" xref="S3.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">o</mi><mo id="S3.p2.4.m4.1.1.1a" xref="S3.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.p2.4.m4.1.1.4" xref="S3.p2.4.m4.1.1.4.cmml">l</mi><mo id="S3.p2.4.m4.1.1.1b" xref="S3.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.p2.4.m4.1.1.5" xref="S3.p2.4.m4.1.1.5.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><times id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1"></times><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">𝑟</ci><ci id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">𝑜</ci><ci id="S3.p2.4.m4.1.1.4.cmml" xref="S3.p2.4.m4.1.1.4">𝑙</ci><ci id="S3.p2.4.m4.1.1.5.cmml" xref="S3.p2.4.m4.1.1.5">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">roll</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.1d">italic_r italic_o italic_l italic_l</annotation></semantics></math>, <math alttext="pitch" class="ltx_Math" display="inline" id="S3.p2.5.m5.1"><semantics id="S3.p2.5.m5.1a"><mrow id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">p</mi><mo id="S3.p2.5.m5.1.1.1" xref="S3.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">i</mi><mo id="S3.p2.5.m5.1.1.1a" xref="S3.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S3.p2.5.m5.1.1.4" xref="S3.p2.5.m5.1.1.4.cmml">t</mi><mo id="S3.p2.5.m5.1.1.1b" xref="S3.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S3.p2.5.m5.1.1.5" xref="S3.p2.5.m5.1.1.5.cmml">c</mi><mo id="S3.p2.5.m5.1.1.1c" xref="S3.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S3.p2.5.m5.1.1.6" xref="S3.p2.5.m5.1.1.6.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><times id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1"></times><ci id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">𝑝</ci><ci id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">𝑖</ci><ci id="S3.p2.5.m5.1.1.4.cmml" xref="S3.p2.5.m5.1.1.4">𝑡</ci><ci id="S3.p2.5.m5.1.1.5.cmml" xref="S3.p2.5.m5.1.1.5">𝑐</ci><ci id="S3.p2.5.m5.1.1.6.cmml" xref="S3.p2.5.m5.1.1.6">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">pitch</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m5.1d">italic_p italic_i italic_t italic_c italic_h</annotation></semantics></math>, and <math alttext="yaw" class="ltx_Math" display="inline" id="S3.p2.6.m6.1"><semantics id="S3.p2.6.m6.1a"><mrow id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mi id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2.cmml">y</mi><mo id="S3.p2.6.m6.1.1.1" xref="S3.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml">a</mi><mo id="S3.p2.6.m6.1.1.1a" xref="S3.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S3.p2.6.m6.1.1.4" xref="S3.p2.6.m6.1.1.4.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><times id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1.1"></times><ci id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2">𝑦</ci><ci id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3">𝑎</ci><ci id="S3.p2.6.m6.1.1.4.cmml" xref="S3.p2.6.m6.1.1.4">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">yaw</annotation><annotation encoding="application/x-llamapun" id="S3.p2.6.m6.1d">italic_y italic_a italic_w</annotation></semantics></math> rotations. For reasons discussed later, we’ll represent the gyroscope’s output as a <math alttext="3x3" class="ltx_Math" display="inline" id="S3.p2.7.m7.1"><semantics id="S3.p2.7.m7.1a"><mrow id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mn id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">3</mn><mo id="S3.p2.7.m7.1.1.1" xref="S3.p2.7.m7.1.1.1.cmml">⁢</mo><mi id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">x</mi><mo id="S3.p2.7.m7.1.1.1a" xref="S3.p2.7.m7.1.1.1.cmml">⁢</mo><mn id="S3.p2.7.m7.1.1.4" xref="S3.p2.7.m7.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><times id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1.1"></times><cn id="S3.p2.7.m7.1.1.2.cmml" type="integer" xref="S3.p2.7.m7.1.1.2">3</cn><ci id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3">𝑥</ci><cn id="S3.p2.7.m7.1.1.4.cmml" type="integer" xref="S3.p2.7.m7.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">3x3</annotation><annotation encoding="application/x-llamapun" id="S3.p2.7.m7.1d">3 italic_x 3</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S3.p2.8.1">rotation matrix</span>, so it will require 9 scalars to represent as opposed to just 3. So in our work, an IMU sample is a vector <math alttext="\in\mathcal{R}^{12}" class="ltx_Math" display="inline" id="S3.p2.8.m8.1"><semantics id="S3.p2.8.m8.1a"><mrow id="S3.p2.8.m8.1.1" xref="S3.p2.8.m8.1.1.cmml"><mi id="S3.p2.8.m8.1.1.2" xref="S3.p2.8.m8.1.1.2.cmml"></mi><mo id="S3.p2.8.m8.1.1.1" xref="S3.p2.8.m8.1.1.1.cmml">∈</mo><msup id="S3.p2.8.m8.1.1.3" xref="S3.p2.8.m8.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p2.8.m8.1.1.3.2" xref="S3.p2.8.m8.1.1.3.2.cmml">ℛ</mi><mn id="S3.p2.8.m8.1.1.3.3" xref="S3.p2.8.m8.1.1.3.3.cmml">12</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.8.m8.1b"><apply id="S3.p2.8.m8.1.1.cmml" xref="S3.p2.8.m8.1.1"><in id="S3.p2.8.m8.1.1.1.cmml" xref="S3.p2.8.m8.1.1.1"></in><csymbol cd="latexml" id="S3.p2.8.m8.1.1.2.cmml" xref="S3.p2.8.m8.1.1.2">absent</csymbol><apply id="S3.p2.8.m8.1.1.3.cmml" xref="S3.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.3.1.cmml" xref="S3.p2.8.m8.1.1.3">superscript</csymbol><ci id="S3.p2.8.m8.1.1.3.2.cmml" xref="S3.p2.8.m8.1.1.3.2">ℛ</ci><cn id="S3.p2.8.m8.1.1.3.3.cmml" type="integer" xref="S3.p2.8.m8.1.1.3.3">12</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m8.1c">\in\mathcal{R}^{12}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.8.m8.1d">∈ caligraphic_R start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT</annotation></semantics></math>.
</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Actual poses will be represented and visualized computationally using the SMPL model, which is used to represent the human body in 3D. This parametric model, built from a comprehensive database of body scans, enables the creation of versatile and realistic human figures.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">We’ll train our neural network on the AMASS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib8" title="">8</a>]</cite>, a rich collection of motion capture data. It provides sequences of poses that we can use as labels to our model.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">The heart of our project lies in processing and interpreting the orientation and acceleration data from IMUs. This time-series data, characterized by its sequential nature and temporal dependencies, is the bedrock upon which our predictive models are built. Our objective is to predict rotation data, but instead of using Euler angles, which are prone to gimbal lock and singularities, we opt for 3x3 rotation matrices. These matrices provide a more stable and comprehensive representation of three-dimensional rotations, crucial for accurate pose estimation.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>System design</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.2">To recap, we’re attempting to apply a data-driven approach to figure out optimal IMU positions. More formally, our objective is to devise a function <math alttext="F(X)\rightarrow\theta^{*}" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.2" xref="S4.p1.1.m1.1.2.cmml"><mrow id="S4.p1.1.m1.1.2.2" xref="S4.p1.1.m1.1.2.2.cmml"><mi id="S4.p1.1.m1.1.2.2.2" xref="S4.p1.1.m1.1.2.2.2.cmml">F</mi><mo id="S4.p1.1.m1.1.2.2.1" xref="S4.p1.1.m1.1.2.2.1.cmml">⁢</mo><mrow id="S4.p1.1.m1.1.2.2.3.2" xref="S4.p1.1.m1.1.2.2.cmml"><mo id="S4.p1.1.m1.1.2.2.3.2.1" stretchy="false" xref="S4.p1.1.m1.1.2.2.cmml">(</mo><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">X</mi><mo id="S4.p1.1.m1.1.2.2.3.2.2" stretchy="false" xref="S4.p1.1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S4.p1.1.m1.1.2.1" stretchy="false" xref="S4.p1.1.m1.1.2.1.cmml">→</mo><msup id="S4.p1.1.m1.1.2.3" xref="S4.p1.1.m1.1.2.3.cmml"><mi id="S4.p1.1.m1.1.2.3.2" xref="S4.p1.1.m1.1.2.3.2.cmml">θ</mi><mo id="S4.p1.1.m1.1.2.3.3" xref="S4.p1.1.m1.1.2.3.3.cmml">*</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.2.cmml" xref="S4.p1.1.m1.1.2"><ci id="S4.p1.1.m1.1.2.1.cmml" xref="S4.p1.1.m1.1.2.1">→</ci><apply id="S4.p1.1.m1.1.2.2.cmml" xref="S4.p1.1.m1.1.2.2"><times id="S4.p1.1.m1.1.2.2.1.cmml" xref="S4.p1.1.m1.1.2.2.1"></times><ci id="S4.p1.1.m1.1.2.2.2.cmml" xref="S4.p1.1.m1.1.2.2.2">𝐹</ci><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">𝑋</ci></apply><apply id="S4.p1.1.m1.1.2.3.cmml" xref="S4.p1.1.m1.1.2.3"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.2.3.1.cmml" xref="S4.p1.1.m1.1.2.3">superscript</csymbol><ci id="S4.p1.1.m1.1.2.3.2.cmml" xref="S4.p1.1.m1.1.2.3.2">𝜃</ci><times id="S4.p1.1.m1.1.2.3.3.cmml" xref="S4.p1.1.m1.1.2.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">F(X)\rightarrow\theta^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_F ( italic_X ) → italic_θ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math> that maps IMU data to a set of joint rotations, representing the
actual pose. Here, <math alttext="X" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">italic_X</annotation></semantics></math> represents a subset of the available 24 joints on the SMPL skeleton,
specifically those most pertinent to our task.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The primary question is: How do we determine the most relevant IMUs from the 24 joints available? This query may initially seem akin to a dimensionality reduction
problem. However, unlike Principal Component Analysis (PCA) that transforms
high-dimensional data into a lower-dimensional space, our goal is to ascertain
which dimensions (IMUs) can be discarded or not collected altogether.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Given this distinction, we require an alternative approach to discern which
among the 24 possible SMPL joints best explain variance in the pose labels. Common
methods like p-value testing, often used in libraries like <span class="ltx_text ltx_font_typewriter" id="S4.p3.1.1">sklearn</span>, are
impractical for our needs. P-values, indicating the likelihood of
feature-induced variance in output, are more suited to simpler models and not to
the large-scale neural networks we intend to employ.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Therefore, our methodology involves:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Predicting poses using all 24 joints.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Quantitatively assessing which of these IMUs contribute most
significantly to this task.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Excluding non-contributory IMUs.</p>
</div>
</li>
</ol>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In our study, we focus on mapping Inertial Measurement Unit (IMU) data, specifically accelerations and rotations, to full body poses. This approach is crucial in understanding and interpreting the movement dynamics captured by IMU sensors.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.4">Interestingly, the converse of the process is quite easy to calculate. Using a formula derived from TransPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib13" title="">13</a>]</cite>, we can calculate the acceleration data between two poses - i.e. we can easily find IMU data given poses. This flexibility allows for the utilization of completely synthetic data in our analysis. The formula is given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="a[i]=(v[i]+v[i+2]-2\cdot v[i+1])\cdot 3600" class="ltx_Math" display="block" id="S4.E1.m1.3"><semantics id="S4.E1.m1.3a"><mrow id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml"><mrow id="S4.E1.m1.3.3.3" xref="S4.E1.m1.3.3.3.cmml"><mi id="S4.E1.m1.3.3.3.2" xref="S4.E1.m1.3.3.3.2.cmml">a</mi><mo id="S4.E1.m1.3.3.3.1" xref="S4.E1.m1.3.3.3.1.cmml">⁢</mo><mrow id="S4.E1.m1.3.3.3.3.2" xref="S4.E1.m1.3.3.3.3.1.cmml"><mo id="S4.E1.m1.3.3.3.3.2.1" stretchy="false" xref="S4.E1.m1.3.3.3.3.1.1.cmml">[</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">i</mi><mo id="S4.E1.m1.3.3.3.3.2.2" stretchy="false" xref="S4.E1.m1.3.3.3.3.1.1.cmml">]</mo></mrow></mrow><mo id="S4.E1.m1.3.3.2" xref="S4.E1.m1.3.3.2.cmml">=</mo><mrow id="S4.E1.m1.3.3.1" xref="S4.E1.m1.3.3.1.cmml"><mrow id="S4.E1.m1.3.3.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.cmml"><mo id="S4.E1.m1.3.3.1.1.1.2" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.3.3.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.3.2" xref="S4.E1.m1.3.3.1.1.1.1.1.3.2.cmml">v</mi><mo id="S4.E1.m1.3.3.1.1.1.1.1.3.1" xref="S4.E1.m1.3.3.1.1.1.1.1.3.1.cmml">⁢</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.3.3.2" xref="S4.E1.m1.3.3.1.1.1.1.1.3.3.1.cmml"><mo id="S4.E1.m1.3.3.1.1.1.1.1.3.3.2.1" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.1.3.3.1.1.cmml">[</mo><mi id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml">i</mi><mo id="S4.E1.m1.3.3.1.1.1.1.1.3.3.2.2" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.1.3.3.1.1.cmml">]</mo></mrow></mrow><mo id="S4.E1.m1.3.3.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.1.2.cmml">+</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.cmml">v</mi><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.1.cmml">[</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">i</mi><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mn id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">2</mn></mrow><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S4.E1.m1.3.3.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.3.cmml">−</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.2.cmml"><mrow id="S4.E1.m1.3.3.1.1.1.1.2.3" xref="S4.E1.m1.3.3.1.1.1.1.2.3.cmml"><mn id="S4.E1.m1.3.3.1.1.1.1.2.3.2" xref="S4.E1.m1.3.3.1.1.1.1.2.3.2.cmml">2</mn><mo id="S4.E1.m1.3.3.1.1.1.1.2.3.1" lspace="0.222em" rspace="0.222em" xref="S4.E1.m1.3.3.1.1.1.1.2.3.1.cmml">⋅</mo><mi id="S4.E1.m1.3.3.1.1.1.1.2.3.3" xref="S4.E1.m1.3.3.1.1.1.1.2.3.3.cmml">v</mi></mrow><mo id="S4.E1.m1.3.3.1.1.1.1.2.2" xref="S4.E1.m1.3.3.1.1.1.1.2.2.cmml">⁢</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.2.1.1" xref="S4.E1.m1.3.3.1.1.1.1.2.1.2.cmml"><mo id="S4.E1.m1.3.3.1.1.1.1.2.1.1.2" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.2.1.2.1.cmml">[</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.2.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.2.cmml">i</mi><mo id="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.1.cmml">+</mo><mn id="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.3.cmml">1</mn></mrow><mo id="S4.E1.m1.3.3.1.1.1.1.2.1.1.3" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.2.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S4.E1.m1.3.3.1.1.1.3" rspace="0.055em" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E1.m1.3.3.1.2" rspace="0.222em" xref="S4.E1.m1.3.3.1.2.cmml">⋅</mo><mn id="S4.E1.m1.3.3.1.3" xref="S4.E1.m1.3.3.1.3.cmml">3600</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.3b"><apply id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3"><eq id="S4.E1.m1.3.3.2.cmml" xref="S4.E1.m1.3.3.2"></eq><apply id="S4.E1.m1.3.3.3.cmml" xref="S4.E1.m1.3.3.3"><times id="S4.E1.m1.3.3.3.1.cmml" xref="S4.E1.m1.3.3.3.1"></times><ci id="S4.E1.m1.3.3.3.2.cmml" xref="S4.E1.m1.3.3.3.2">𝑎</ci><apply id="S4.E1.m1.3.3.3.3.1.cmml" xref="S4.E1.m1.3.3.3.3.2"><csymbol cd="latexml" id="S4.E1.m1.3.3.3.3.1.1.cmml" xref="S4.E1.m1.3.3.3.3.2.1">delimited-[]</csymbol><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">𝑖</ci></apply></apply><apply id="S4.E1.m1.3.3.1.cmml" xref="S4.E1.m1.3.3.1"><ci id="S4.E1.m1.3.3.1.2.cmml" xref="S4.E1.m1.3.3.1.2">⋅</ci><apply id="S4.E1.m1.3.3.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1"><minus id="S4.E1.m1.3.3.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.3"></minus><apply id="S4.E1.m1.3.3.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1"><plus id="S4.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.2"></plus><apply id="S4.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.3"><times id="S4.E1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.3.1"></times><ci id="S4.E1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.3.2">𝑣</ci><apply id="S4.E1.m1.3.3.1.1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.3.3.2"><csymbol cd="latexml" id="S4.E1.m1.3.3.1.1.1.1.1.3.3.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.3.3.2.1">delimited-[]</csymbol><ci id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2">𝑖</ci></apply></apply><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1"><times id="S4.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2"></times><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3">𝑣</ci><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1"><plus id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1"></plus><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑖</ci><cn id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply><apply id="S4.E1.m1.3.3.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2"><times id="S4.E1.m1.3.3.1.1.1.1.2.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2.2"></times><apply id="S4.E1.m1.3.3.1.1.1.1.2.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2.3"><ci id="S4.E1.m1.3.3.1.1.1.1.2.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2.3.1">⋅</ci><cn id="S4.E1.m1.3.3.1.1.1.1.2.3.2.cmml" type="integer" xref="S4.E1.m1.3.3.1.1.1.1.2.3.2">2</cn><ci id="S4.E1.m1.3.3.1.1.1.1.2.3.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2.3.3">𝑣</ci></apply><apply id="S4.E1.m1.3.3.1.1.1.1.2.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2.1.1"><csymbol cd="latexml" id="S4.E1.m1.3.3.1.1.1.1.2.1.2.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2.1.1.2">delimited-[]</csymbol><apply id="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2.1.1.1"><plus id="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.1"></plus><ci id="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.2">𝑖</ci><cn id="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.3.cmml" type="integer" xref="S4.E1.m1.3.3.1.1.1.1.2.1.1.1.3">1</cn></apply></apply></apply></apply><cn id="S4.E1.m1.3.3.1.3.cmml" type="integer" xref="S4.E1.m1.3.3.1.3">3600</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.3c">a[i]=(v[i]+v[i+2]-2\cdot v[i+1])\cdot 3600</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.3d">italic_a [ italic_i ] = ( italic_v [ italic_i ] + italic_v [ italic_i + 2 ] - 2 ⋅ italic_v [ italic_i + 1 ] ) ⋅ 3600</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p2.3">where <math alttext="v" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">v</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">italic_v</annotation></semantics></math> represents an array of frames, and <math alttext="a[i]" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.2" xref="S4.SS1.p2.2.m2.1.2.cmml"><mi id="S4.SS1.p2.2.m2.1.2.2" xref="S4.SS1.p2.2.m2.1.2.2.cmml">a</mi><mo id="S4.SS1.p2.2.m2.1.2.1" xref="S4.SS1.p2.2.m2.1.2.1.cmml">⁢</mo><mrow id="S4.SS1.p2.2.m2.1.2.3.2" xref="S4.SS1.p2.2.m2.1.2.3.1.cmml"><mo id="S4.SS1.p2.2.m2.1.2.3.2.1" stretchy="false" xref="S4.SS1.p2.2.m2.1.2.3.1.1.cmml">[</mo><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">i</mi><mo id="S4.SS1.p2.2.m2.1.2.3.2.2" stretchy="false" xref="S4.SS1.p2.2.m2.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.2.cmml" xref="S4.SS1.p2.2.m2.1.2"><times id="S4.SS1.p2.2.m2.1.2.1.cmml" xref="S4.SS1.p2.2.m2.1.2.1"></times><ci id="S4.SS1.p2.2.m2.1.2.2.cmml" xref="S4.SS1.p2.2.m2.1.2.2">𝑎</ci><apply id="S4.SS1.p2.2.m2.1.2.3.1.cmml" xref="S4.SS1.p2.2.m2.1.2.3.2"><csymbol cd="latexml" id="S4.SS1.p2.2.m2.1.2.3.1.1.cmml" xref="S4.SS1.p2.2.m2.1.2.3.2.1">delimited-[]</csymbol><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">a[i]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">italic_a [ italic_i ]</annotation></semantics></math> is the acceleration at frame <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">italic_i</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Naturally, the rotation of each joint is equivalent to the rotation of the corresponding synthetic IMU.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">The base dataset for our study is AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib8" title="">8</a>]</cite>, an aggregation of numerous motion capture datasets. AMASS provides data in the SMPL format, a mesh model that demonstrates how a virtual "body" is contorted by a given set of joint rotations. The SMPL model is particularly significant as it contains a <math alttext="24\times 6890" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mn id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">24</mn><mo id="S4.SS1.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p4.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">6890</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><times id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></times><cn id="S4.SS1.p4.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.p4.1.m1.1.1.2">24</cn><cn id="S4.SS1.p4.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p4.1.m1.1.1.3">6890</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">24\times 6890</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">24 × 6890</annotation></semantics></math> matrix that transforms 24 joints into the 6980 vertices of the model. By running an argmax operation on each of the 24 weight sets, we can identify the vertex that most closely corresponds to each joint.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1">Joint Index</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2">Joint Name</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3">Joint Index</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4">Joint Name</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1">0</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.2.1.2">Pelvis</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.1.3">12</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.2.1.4">Neck</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.1.3.2.1">1</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.3.2.2">L Hip</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.3.2.3">13</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.3.2.4">L Collar</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.1.4.3.1">2</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.4.3.2">R Hip</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.4.3.3">14</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.4.3.4">R Collar</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.1.5.4.1">3</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.5.4.2">Spine1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.5.4.3">15</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.5.4.4">Head</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.1.6.5.1">4</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.6.5.2">L Knee</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.6.5.3">16</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.6.5.4">L Shoulder</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.1.7.6.1">5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.7.6.2">R Knee</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.7.6.3">17</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.7.6.4">R Shoulder</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.1.8.7.1">6</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.8.7.2">Spine2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.8.7.3">18</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.8.7.4">L Elbow</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.1.9.8.1">7</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.9.8.2">L Ankle</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.9.8.3">19</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.9.8.4">R Elbow</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.1.10.9.1">8</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.10.9.2">R Ankle</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.10.9.3">20</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.10.9.4">L Wrist</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.1.11.10.1">9</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.11.10.2">Spine3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.11.10.3">21</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.11.10.4">R Wrist</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.1.12.11.1">10</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.12.11.2">L Foot</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.12.11.3">22</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.12.11.4">L Hand</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" id="S4.T1.1.13.12.1">11</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S4.T1.1.13.12.2">R Foot</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.1.13.12.3">23</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S4.T1.1.13.12.4">R Hand</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Mapping of SMPL Joint Indices to Joint Names</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Once we have vertices, which are points on the SMPL mesh, and their corresponding joints, we can compute IMU data on a per-joint basis by applying the above formula to the motion of the vertices between frames.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>The LSTM Model</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">In addressing this problem, it is crucial to understand that we are dealing with a time series issue where isolated pose frames are insufficient. To synthesize IMU data effectively, one must consider the sequence of frames and their context within the surrounding frames. This calls for a model that ingests a series of <math alttext="X" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_X</annotation></semantics></math> frames of IMU data and outputs <math alttext="X" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_X</annotation></semantics></math> corresponding poses.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.3">Long Short-Term Memory (LSTM) networks are exceptionally suited for this challenge due to their ability to process sequences, retain information about previous frames, and apply this context to new ones. A prediction at any frame is a weighted consideration <math alttext="f(h,x)" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.2"><semantics id="S4.SS2.p2.1.m1.2a"><mrow id="S4.SS2.p2.1.m1.2.3" xref="S4.SS2.p2.1.m1.2.3.cmml"><mi id="S4.SS2.p2.1.m1.2.3.2" xref="S4.SS2.p2.1.m1.2.3.2.cmml">f</mi><mo id="S4.SS2.p2.1.m1.2.3.1" xref="S4.SS2.p2.1.m1.2.3.1.cmml">⁢</mo><mrow id="S4.SS2.p2.1.m1.2.3.3.2" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml"><mo id="S4.SS2.p2.1.m1.2.3.3.2.1" stretchy="false" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml">(</mo><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">h</mi><mo id="S4.SS2.p2.1.m1.2.3.3.2.2" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml">,</mo><mi id="S4.SS2.p2.1.m1.2.2" xref="S4.SS2.p2.1.m1.2.2.cmml">x</mi><mo id="S4.SS2.p2.1.m1.2.3.3.2.3" stretchy="false" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.2b"><apply id="S4.SS2.p2.1.m1.2.3.cmml" xref="S4.SS2.p2.1.m1.2.3"><times id="S4.SS2.p2.1.m1.2.3.1.cmml" xref="S4.SS2.p2.1.m1.2.3.1"></times><ci id="S4.SS2.p2.1.m1.2.3.2.cmml" xref="S4.SS2.p2.1.m1.2.3.2">𝑓</ci><interval closure="open" id="S4.SS2.p2.1.m1.2.3.3.1.cmml" xref="S4.SS2.p2.1.m1.2.3.3.2"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">ℎ</ci><ci id="S4.SS2.p2.1.m1.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2">𝑥</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.2c">f(h,x)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.2d">italic_f ( italic_h , italic_x )</annotation></semantics></math> that considers both the current frame <math alttext="x" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">italic_x</annotation></semantics></math>, along with the model’s “memory”, the hidden state <math alttext="h" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1"><semantics id="S4.SS2.p2.3.m3.1a"><mi id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><ci id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">h</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">italic_h</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Below, we have provided our model architecture as a list. Here, <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_N</annotation></semantics></math> is the number of sensors the model uses - it will either be 24 or 6.</p>
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.2">Linear Layer: Transform from shape <math alttext="N*(3*3+3)" class="ltx_Math" display="inline" id="S4.I2.i1.p1.1.m1.1"><semantics id="S4.I2.i1.p1.1.m1.1a"><mrow id="S4.I2.i1.p1.1.m1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.cmml"><mi id="S4.I2.i1.p1.1.m1.1.1.3" xref="S4.I2.i1.p1.1.m1.1.1.3.cmml">N</mi><mo id="S4.I2.i1.p1.1.m1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S4.I2.i1.p1.1.m1.1.1.2.cmml">*</mo><mrow id="S4.I2.i1.p1.1.m1.1.1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.cmml"><mo id="S4.I2.i1.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.I2.i1.p1.1.m1.1.1.1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.cmml"><mrow id="S4.I2.i1.p1.1.m1.1.1.1.1.1.2" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.cmml"><mn id="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.2" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.2.cmml">3</mn><mo id="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.1.cmml">*</mo><mn id="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.3" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.3.cmml">3</mn></mrow><mo id="S4.I2.i1.p1.1.m1.1.1.1.1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S4.I2.i1.p1.1.m1.1.1.1.1.1.3" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.3.cmml">3</mn></mrow><mo id="S4.I2.i1.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.1.m1.1b"><apply id="S4.I2.i1.p1.1.m1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1"><times id="S4.I2.i1.p1.1.m1.1.1.2.cmml" xref="S4.I2.i1.p1.1.m1.1.1.2"></times><ci id="S4.I2.i1.p1.1.m1.1.1.3.cmml" xref="S4.I2.i1.p1.1.m1.1.1.3">𝑁</ci><apply id="S4.I2.i1.p1.1.m1.1.1.1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1.1.1"><plus id="S4.I2.i1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.1"></plus><apply id="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.2"><times id="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.1"></times><cn id="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.2.cmml" type="integer" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.2">3</cn><cn id="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.3.cmml" type="integer" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.2.3">3</cn></apply><cn id="S4.I2.i1.p1.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S4.I2.i1.p1.1.m1.1.1.1.1.1.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.1.m1.1c">N*(3*3+3)</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i1.p1.1.m1.1d">italic_N * ( 3 * 3 + 3 )</annotation></semantics></math> to <math alttext="1024" class="ltx_Math" display="inline" id="S4.I2.i1.p1.2.m2.1"><semantics id="S4.I2.i1.p1.2.m2.1a"><mn id="S4.I2.i1.p1.2.m2.1.1" xref="S4.I2.i1.p1.2.m2.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.2.m2.1b"><cn id="S4.I2.i1.p1.2.m2.1.1.cmml" type="integer" xref="S4.I2.i1.p1.2.m2.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.2.m2.1c">1024</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i1.p1.2.m2.1d">1024</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.3">LSTM Layer: Accept shape <math alttext="1024" class="ltx_Math" display="inline" id="S4.I2.i2.p1.1.m1.1"><semantics id="S4.I2.i2.p1.1.m1.1a"><mn id="S4.I2.i2.p1.1.m1.1.1" xref="S4.I2.i2.p1.1.m1.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.1.m1.1b"><cn id="S4.I2.i2.p1.1.m1.1.1.cmml" type="integer" xref="S4.I2.i2.p1.1.m1.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.1.m1.1c">1024</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.1.m1.1d">1024</annotation></semantics></math> then feed it through 2 recurrent layers, each with hidden size <math alttext="1024" class="ltx_Math" display="inline" id="S4.I2.i2.p1.2.m2.1"><semantics id="S4.I2.i2.p1.2.m2.1a"><mn id="S4.I2.i2.p1.2.m2.1.1" xref="S4.I2.i2.p1.2.m2.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.2.m2.1b"><cn id="S4.I2.i2.p1.2.m2.1.1.cmml" type="integer" xref="S4.I2.i2.p1.2.m2.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.2.m2.1c">1024</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.2.m2.1d">1024</annotation></semantics></math>. Produce output of shape <math alttext="2*1024" class="ltx_Math" display="inline" id="S4.I2.i2.p1.3.m3.1"><semantics id="S4.I2.i2.p1.3.m3.1a"><mrow id="S4.I2.i2.p1.3.m3.1.1" xref="S4.I2.i2.p1.3.m3.1.1.cmml"><mn id="S4.I2.i2.p1.3.m3.1.1.2" xref="S4.I2.i2.p1.3.m3.1.1.2.cmml">2</mn><mo id="S4.I2.i2.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I2.i2.p1.3.m3.1.1.1.cmml">*</mo><mn id="S4.I2.i2.p1.3.m3.1.1.3" xref="S4.I2.i2.p1.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.3.m3.1b"><apply id="S4.I2.i2.p1.3.m3.1.1.cmml" xref="S4.I2.i2.p1.3.m3.1.1"><times id="S4.I2.i2.p1.3.m3.1.1.1.cmml" xref="S4.I2.i2.p1.3.m3.1.1.1"></times><cn id="S4.I2.i2.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.I2.i2.p1.3.m3.1.1.2">2</cn><cn id="S4.I2.i2.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.I2.i2.p1.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.3.m3.1c">2*1024</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.3.m3.1d">2 * 1024</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.2">Linear Layer: Transform from shape <math alttext="2*1024" class="ltx_Math" display="inline" id="S4.I2.i3.p1.1.m1.1"><semantics id="S4.I2.i3.p1.1.m1.1a"><mrow id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml"><mn id="S4.I2.i3.p1.1.m1.1.1.2" xref="S4.I2.i3.p1.1.m1.1.1.2.cmml">2</mn><mo id="S4.I2.i3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I2.i3.p1.1.m1.1.1.1.cmml">*</mo><mn id="S4.I2.i3.p1.1.m1.1.1.3" xref="S4.I2.i3.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><apply id="S4.I2.i3.p1.1.m1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1"><times id="S4.I2.i3.p1.1.m1.1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1.1"></times><cn id="S4.I2.i3.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.I2.i3.p1.1.m1.1.1.2">2</cn><cn id="S4.I2.i3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.I2.i3.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">2*1024</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.1.m1.1d">2 * 1024</annotation></semantics></math> to final output shape <math alttext="24*(3*3)" class="ltx_Math" display="inline" id="S4.I2.i3.p1.2.m2.1"><semantics id="S4.I2.i3.p1.2.m2.1a"><mrow id="S4.I2.i3.p1.2.m2.1.1" xref="S4.I2.i3.p1.2.m2.1.1.cmml"><mn id="S4.I2.i3.p1.2.m2.1.1.3" xref="S4.I2.i3.p1.2.m2.1.1.3.cmml">24</mn><mo id="S4.I2.i3.p1.2.m2.1.1.2" lspace="0.222em" rspace="0.222em" xref="S4.I2.i3.p1.2.m2.1.1.2.cmml">*</mo><mrow id="S4.I2.i3.p1.2.m2.1.1.1.1" xref="S4.I2.i3.p1.2.m2.1.1.1.1.1.cmml"><mo id="S4.I2.i3.p1.2.m2.1.1.1.1.2" stretchy="false" xref="S4.I2.i3.p1.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S4.I2.i3.p1.2.m2.1.1.1.1.1" xref="S4.I2.i3.p1.2.m2.1.1.1.1.1.cmml"><mn id="S4.I2.i3.p1.2.m2.1.1.1.1.1.2" xref="S4.I2.i3.p1.2.m2.1.1.1.1.1.2.cmml">3</mn><mo id="S4.I2.i3.p1.2.m2.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I2.i3.p1.2.m2.1.1.1.1.1.1.cmml">*</mo><mn id="S4.I2.i3.p1.2.m2.1.1.1.1.1.3" xref="S4.I2.i3.p1.2.m2.1.1.1.1.1.3.cmml">3</mn></mrow><mo id="S4.I2.i3.p1.2.m2.1.1.1.1.3" stretchy="false" xref="S4.I2.i3.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.2.m2.1b"><apply id="S4.I2.i3.p1.2.m2.1.1.cmml" xref="S4.I2.i3.p1.2.m2.1.1"><times id="S4.I2.i3.p1.2.m2.1.1.2.cmml" xref="S4.I2.i3.p1.2.m2.1.1.2"></times><cn id="S4.I2.i3.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.I2.i3.p1.2.m2.1.1.3">24</cn><apply id="S4.I2.i3.p1.2.m2.1.1.1.1.1.cmml" xref="S4.I2.i3.p1.2.m2.1.1.1.1"><times id="S4.I2.i3.p1.2.m2.1.1.1.1.1.1.cmml" xref="S4.I2.i3.p1.2.m2.1.1.1.1.1.1"></times><cn id="S4.I2.i3.p1.2.m2.1.1.1.1.1.2.cmml" type="integer" xref="S4.I2.i3.p1.2.m2.1.1.1.1.1.2">3</cn><cn id="S4.I2.i3.p1.2.m2.1.1.1.1.1.3.cmml" type="integer" xref="S4.I2.i3.p1.2.m2.1.1.1.1.1.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.2.m2.1c">24*(3*3)</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.2.m2.1d">24 * ( 3 * 3 )</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">RNN training was executed over 5 epochs, where each epoch took approximately 20 minutes to run on an RTX3090. Counterintuitively, note that we actually <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.1">want</span> to overfit the model - we do not implement any sort of regularization, since our desired result is actually the model conditioning heavily on a small set of features, as opposed to using all the features equally.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Model Interpretation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To figure out which features are most important, we need to leverage a model interpretation technique. Simpler models, like random forests and gradient-boosted trees, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib5" title="">5</a>]</cite> are easier to interpret, but since we work with deep learning models, it can be a little harder to understand what goes into the network’s prediction.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">We apply Captum, a powerful library for model interpretability in PyTorch. Captum offers various methods for attributing model outputs to input features, aiding in the understanding of the model’s decision-making process. Given that our model input is a tensor of size 288, representing 24 joints with 3-axis acceleration and 3x3 elements for rotations, we employ Captum’s feature masking capabilities to analyze the importance of individual IMUs.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Feature masking involves selectively hiding or masking certain features during model evaluation to observe their impact on predictions. Notably, since only a subset of methods in Captum supports feature masks, we focus on those that apply to our scenario. We tried Shapley value, Shapley value sampling, and feature ablation - we found that feature ablation was the most convenient to use and provided the best results.
</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Feature ablation is known as a <span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">perturbation approach</span>: it systematically replaces given features or groups of features with a baseline, then evaluates how much this influences predictions. Of course, this means that it requires data to operate on - unlike static analysis methods that directly inspect the parameters of the model, feature ablation requires that the dataset be passed in as a parameter. Thus, we reserved a set of test data, separate from the training dataset, that we could use for model evaluation and interpretation.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Adding The Transformer</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Transformers, as introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib12" title="">12</a>]</cite>, have recently brought about significant advancements in time series analysis. Although groundbreaking in the field of large language models, where transformers are arranged in an encoder/decoder architecture to generate coherent text, the fundamental principle of self attention underlying transformers is applicable to a wide variety of time series tasks.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.6">RNNs like LSTMs store context over time in a “hidden state” - after consuming each element <math alttext="x" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><ci id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">italic_x</annotation></semantics></math> of the input sequence, this hidden state is updated by passing it through a function <math alttext="f(x,h)\rightarrow(y,h)" class="ltx_Math" display="inline" id="S4.SS4.p2.2.m2.4"><semantics id="S4.SS4.p2.2.m2.4a"><mrow id="S4.SS4.p2.2.m2.4.5" xref="S4.SS4.p2.2.m2.4.5.cmml"><mrow id="S4.SS4.p2.2.m2.4.5.2" xref="S4.SS4.p2.2.m2.4.5.2.cmml"><mi id="S4.SS4.p2.2.m2.4.5.2.2" xref="S4.SS4.p2.2.m2.4.5.2.2.cmml">f</mi><mo id="S4.SS4.p2.2.m2.4.5.2.1" xref="S4.SS4.p2.2.m2.4.5.2.1.cmml">⁢</mo><mrow id="S4.SS4.p2.2.m2.4.5.2.3.2" xref="S4.SS4.p2.2.m2.4.5.2.3.1.cmml"><mo id="S4.SS4.p2.2.m2.4.5.2.3.2.1" stretchy="false" xref="S4.SS4.p2.2.m2.4.5.2.3.1.cmml">(</mo><mi id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">x</mi><mo id="S4.SS4.p2.2.m2.4.5.2.3.2.2" xref="S4.SS4.p2.2.m2.4.5.2.3.1.cmml">,</mo><mi id="S4.SS4.p2.2.m2.2.2" xref="S4.SS4.p2.2.m2.2.2.cmml">h</mi><mo id="S4.SS4.p2.2.m2.4.5.2.3.2.3" stretchy="false" xref="S4.SS4.p2.2.m2.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.SS4.p2.2.m2.4.5.1" stretchy="false" xref="S4.SS4.p2.2.m2.4.5.1.cmml">→</mo><mrow id="S4.SS4.p2.2.m2.4.5.3.2" xref="S4.SS4.p2.2.m2.4.5.3.1.cmml"><mo id="S4.SS4.p2.2.m2.4.5.3.2.1" stretchy="false" xref="S4.SS4.p2.2.m2.4.5.3.1.cmml">(</mo><mi id="S4.SS4.p2.2.m2.3.3" xref="S4.SS4.p2.2.m2.3.3.cmml">y</mi><mo id="S4.SS4.p2.2.m2.4.5.3.2.2" xref="S4.SS4.p2.2.m2.4.5.3.1.cmml">,</mo><mi id="S4.SS4.p2.2.m2.4.4" xref="S4.SS4.p2.2.m2.4.4.cmml">h</mi><mo id="S4.SS4.p2.2.m2.4.5.3.2.3" stretchy="false" xref="S4.SS4.p2.2.m2.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.4b"><apply id="S4.SS4.p2.2.m2.4.5.cmml" xref="S4.SS4.p2.2.m2.4.5"><ci id="S4.SS4.p2.2.m2.4.5.1.cmml" xref="S4.SS4.p2.2.m2.4.5.1">→</ci><apply id="S4.SS4.p2.2.m2.4.5.2.cmml" xref="S4.SS4.p2.2.m2.4.5.2"><times id="S4.SS4.p2.2.m2.4.5.2.1.cmml" xref="S4.SS4.p2.2.m2.4.5.2.1"></times><ci id="S4.SS4.p2.2.m2.4.5.2.2.cmml" xref="S4.SS4.p2.2.m2.4.5.2.2">𝑓</ci><interval closure="open" id="S4.SS4.p2.2.m2.4.5.2.3.1.cmml" xref="S4.SS4.p2.2.m2.4.5.2.3.2"><ci id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">𝑥</ci><ci id="S4.SS4.p2.2.m2.2.2.cmml" xref="S4.SS4.p2.2.m2.2.2">ℎ</ci></interval></apply><interval closure="open" id="S4.SS4.p2.2.m2.4.5.3.1.cmml" xref="S4.SS4.p2.2.m2.4.5.3.2"><ci id="S4.SS4.p2.2.m2.3.3.cmml" xref="S4.SS4.p2.2.m2.3.3">𝑦</ci><ci id="S4.SS4.p2.2.m2.4.4.cmml" xref="S4.SS4.p2.2.m2.4.4">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.4c">f(x,h)\rightarrow(y,h)</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.2.m2.4d">italic_f ( italic_x , italic_h ) → ( italic_y , italic_h )</annotation></semantics></math>, a function that accepts an input <math alttext="x" class="ltx_Math" display="inline" id="S4.SS4.p2.3.m3.1"><semantics id="S4.SS4.p2.3.m3.1a"><mi id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><ci id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.3.m3.1d">italic_x</annotation></semantics></math> and the current hidden state <math alttext="h" class="ltx_Math" display="inline" id="S4.SS4.p2.4.m4.1"><semantics id="S4.SS4.p2.4.m4.1a"><mi id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><ci id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">h</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.4.m4.1d">italic_h</annotation></semantics></math>, then returns an output <math alttext="y" class="ltx_Math" display="inline" id="S4.SS4.p2.5.m5.1"><semantics id="S4.SS4.p2.5.m5.1a"><mi id="S4.SS4.p2.5.m5.1.1" xref="S4.SS4.p2.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.5.m5.1b"><ci id="S4.SS4.p2.5.m5.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.5.m5.1c">y</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.5.m5.1d">italic_y</annotation></semantics></math> and a new hidden state. The output of the RNN is the last <math alttext="y" class="ltx_Math" display="inline" id="S4.SS4.p2.6.m6.1"><semantics id="S4.SS4.p2.6.m6.1a"><mi id="S4.SS4.p2.6.m6.1.1" xref="S4.SS4.p2.6.m6.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.6.m6.1b"><ci id="S4.SS4.p2.6.m6.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.6.m6.1c">y</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.6.m6.1d">italic_y</annotation></semantics></math> produced - which means that the information that is used to compute the final output of the RNN is limited to the second-to-last hidden state and the last element. Although in theory, RNNs can operate on infinitely long sequences, the actual amount of context that can be used to compute the output of a given sequence element is limited to the amount of information that can be encoded by the hidden state vector. This results in a <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.6.1">temporal dependency</span>: the context to compute the output of any sequence element must come from sequence elements close to it.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.4">More damaging is the inherently sequential nature of RNNs. The computation of hidden state <math alttext="h_{k}" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><msub id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mi id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">h</mi><mi id="S4.SS4.p3.1.m1.1.1.3" xref="S4.SS4.p3.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">ℎ</ci><ci id="S4.SS4.p3.1.m1.1.1.3.cmml" xref="S4.SS4.p3.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">h_{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> depends on <math alttext="h_{k-1}" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m2.1"><semantics id="S4.SS4.p3.2.m2.1a"><msub id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mi id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">h</mi><mrow id="S4.SS4.p3.2.m2.1.1.3" xref="S4.SS4.p3.2.m2.1.1.3.cmml"><mi id="S4.SS4.p3.2.m2.1.1.3.2" xref="S4.SS4.p3.2.m2.1.1.3.2.cmml">k</mi><mo id="S4.SS4.p3.2.m2.1.1.3.1" xref="S4.SS4.p3.2.m2.1.1.3.1.cmml">−</mo><mn id="S4.SS4.p3.2.m2.1.1.3.3" xref="S4.SS4.p3.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">ℎ</ci><apply id="S4.SS4.p3.2.m2.1.1.3.cmml" xref="S4.SS4.p3.2.m2.1.1.3"><minus id="S4.SS4.p3.2.m2.1.1.3.1.cmml" xref="S4.SS4.p3.2.m2.1.1.3.1"></minus><ci id="S4.SS4.p3.2.m2.1.1.3.2.cmml" xref="S4.SS4.p3.2.m2.1.1.3.2">𝑘</ci><cn id="S4.SS4.p3.2.m2.1.1.3.3.cmml" type="integer" xref="S4.SS4.p3.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">h_{k-1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.1d">italic_h start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT</annotation></semantics></math>. This unfortunate fact means that the computation of output <math alttext="y_{n}" class="ltx_Math" display="inline" id="S4.SS4.p3.3.m3.1"><semantics id="S4.SS4.p3.3.m3.1a"><msub id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml"><mi id="S4.SS4.p3.3.m3.1.1.2" xref="S4.SS4.p3.3.m3.1.1.2.cmml">y</mi><mi id="S4.SS4.p3.3.m3.1.1.3" xref="S4.SS4.p3.3.m3.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><apply id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.3.m3.1.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p3.3.m3.1.1.2.cmml" xref="S4.SS4.p3.3.m3.1.1.2">𝑦</ci><ci id="S4.SS4.p3.3.m3.1.1.3.cmml" xref="S4.SS4.p3.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">y_{n}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.3.m3.1d">italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> requires the computation of the <math alttext="n-1" class="ltx_Math" display="inline" id="S4.SS4.p3.4.m4.1"><semantics id="S4.SS4.p3.4.m4.1a"><mrow id="S4.SS4.p3.4.m4.1.1" xref="S4.SS4.p3.4.m4.1.1.cmml"><mi id="S4.SS4.p3.4.m4.1.1.2" xref="S4.SS4.p3.4.m4.1.1.2.cmml">n</mi><mo id="S4.SS4.p3.4.m4.1.1.1" xref="S4.SS4.p3.4.m4.1.1.1.cmml">−</mo><mn id="S4.SS4.p3.4.m4.1.1.3" xref="S4.SS4.p3.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.4.m4.1b"><apply id="S4.SS4.p3.4.m4.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1"><minus id="S4.SS4.p3.4.m4.1.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1.1"></minus><ci id="S4.SS4.p3.4.m4.1.1.2.cmml" xref="S4.SS4.p3.4.m4.1.1.2">𝑛</ci><cn id="S4.SS4.p3.4.m4.1.1.3.cmml" type="integer" xref="S4.SS4.p3.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.4.m4.1c">n-1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.4.m4.1d">italic_n - 1</annotation></semantics></math> hidden states before it, rendering parallelization impossible. We’d like a model that is capable of leveraging modern GPUs.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">Transformers address both of these issues through a mechanism known as “self attention”. Rather than sequentially computing hidden states / sequence outputs, self attention applies 3 separate transformations to the input, turning it into “query”, “key”, and “value” vectors (matrices, in practice). The query and key vectors are multiplied and transformed in a pairwise fashion, allowing the computation of a matrix called the “attention weights”. Essentially, this matrix encodes how important each element of the input is to each element of the output. It’s somewhat analogous to computing all of the hidden states in a single matrix multiplication, all at once. It can then be used to transform the “value” vector into a final output sequence.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.2">Since the attention weights are computed by running some transformation <math alttext="f(a,b)" class="ltx_Math" display="inline" id="S4.SS4.p5.1.m1.2"><semantics id="S4.SS4.p5.1.m1.2a"><mrow id="S4.SS4.p5.1.m1.2.3" xref="S4.SS4.p5.1.m1.2.3.cmml"><mi id="S4.SS4.p5.1.m1.2.3.2" xref="S4.SS4.p5.1.m1.2.3.2.cmml">f</mi><mo id="S4.SS4.p5.1.m1.2.3.1" xref="S4.SS4.p5.1.m1.2.3.1.cmml">⁢</mo><mrow id="S4.SS4.p5.1.m1.2.3.3.2" xref="S4.SS4.p5.1.m1.2.3.3.1.cmml"><mo id="S4.SS4.p5.1.m1.2.3.3.2.1" stretchy="false" xref="S4.SS4.p5.1.m1.2.3.3.1.cmml">(</mo><mi id="S4.SS4.p5.1.m1.1.1" xref="S4.SS4.p5.1.m1.1.1.cmml">a</mi><mo id="S4.SS4.p5.1.m1.2.3.3.2.2" xref="S4.SS4.p5.1.m1.2.3.3.1.cmml">,</mo><mi id="S4.SS4.p5.1.m1.2.2" xref="S4.SS4.p5.1.m1.2.2.cmml">b</mi><mo id="S4.SS4.p5.1.m1.2.3.3.2.3" stretchy="false" xref="S4.SS4.p5.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.1.m1.2b"><apply id="S4.SS4.p5.1.m1.2.3.cmml" xref="S4.SS4.p5.1.m1.2.3"><times id="S4.SS4.p5.1.m1.2.3.1.cmml" xref="S4.SS4.p5.1.m1.2.3.1"></times><ci id="S4.SS4.p5.1.m1.2.3.2.cmml" xref="S4.SS4.p5.1.m1.2.3.2">𝑓</ci><interval closure="open" id="S4.SS4.p5.1.m1.2.3.3.1.cmml" xref="S4.SS4.p5.1.m1.2.3.3.2"><ci id="S4.SS4.p5.1.m1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1">𝑎</ci><ci id="S4.SS4.p5.1.m1.2.2.cmml" xref="S4.SS4.p5.1.m1.2.2">𝑏</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.1.m1.2c">f(a,b)</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.1.m1.2d">italic_f ( italic_a , italic_b )</annotation></semantics></math> on every single pair <math alttext="(a,b)" class="ltx_Math" display="inline" id="S4.SS4.p5.2.m2.2"><semantics id="S4.SS4.p5.2.m2.2a"><mrow id="S4.SS4.p5.2.m2.2.3.2" xref="S4.SS4.p5.2.m2.2.3.1.cmml"><mo id="S4.SS4.p5.2.m2.2.3.2.1" stretchy="false" xref="S4.SS4.p5.2.m2.2.3.1.cmml">(</mo><mi id="S4.SS4.p5.2.m2.1.1" xref="S4.SS4.p5.2.m2.1.1.cmml">a</mi><mo id="S4.SS4.p5.2.m2.2.3.2.2" xref="S4.SS4.p5.2.m2.2.3.1.cmml">,</mo><mi id="S4.SS4.p5.2.m2.2.2" xref="S4.SS4.p5.2.m2.2.2.cmml">b</mi><mo id="S4.SS4.p5.2.m2.2.3.2.3" stretchy="false" xref="S4.SS4.p5.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.2.m2.2b"><interval closure="open" id="S4.SS4.p5.2.m2.2.3.1.cmml" xref="S4.SS4.p5.2.m2.2.3.2"><ci id="S4.SS4.p5.2.m2.1.1.cmml" xref="S4.SS4.p5.2.m2.1.1">𝑎</ci><ci id="S4.SS4.p5.2.m2.2.2.cmml" xref="S4.SS4.p5.2.m2.2.2">𝑏</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.2.m2.2c">(a,b)</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.2.m2.2d">( italic_a , italic_b )</annotation></semantics></math>, the context usable by each sequence element includes the entire sequence, with equal potential importance given to each element (the actual importance is the attention weight). This successfully breaks temporal dependency. Furthermore, since the attention weights are computed all at once in a single matrix multiplication, this breaks the sequential nature of RNNs and lends itself to massive parallelization.</p>
</div>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1">Therefore, we developed a transformer-based architecture to replace the LSTM. Generative applications of the transformer usually leverage both an encoder and a decoder, wherein previous sequences are used as input to the decoder in order to generate new sequences. However, since each pose sequence in our application is independent, we actually do not need a decoder at all and our model can perform with just an encoder.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>DIP-IMU Model Performance On TotalCapture</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.4.4.5"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.5.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1">
<math alttext="\mu_{ang}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2.cmml">μ</mi><mrow id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3.cmml"><mi id="S4.T2.1.1.1.m1.1.1.3.2" xref="S4.T2.1.1.1.m1.1.1.3.2.cmml">a</mi><mo id="S4.T2.1.1.1.m1.1.1.3.1" xref="S4.T2.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.1.1.1.m1.1.1.3.3" xref="S4.T2.1.1.1.m1.1.1.3.3.cmml">n</mi><mo id="S4.T2.1.1.1.m1.1.1.3.1a" xref="S4.T2.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.1.1.1.m1.1.1.3.4" xref="S4.T2.1.1.1.m1.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2">𝜇</ci><apply id="S4.T2.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3"><times id="S4.T2.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.1.1.1.m1.1.1.3.1"></times><ci id="S4.T2.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.1.1.1.m1.1.1.3.2">𝑎</ci><ci id="S4.T2.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3.3">𝑛</ci><ci id="S4.T2.1.1.1.m1.1.1.3.4.cmml" xref="S4.T2.1.1.1.m1.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\mu_{ang}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">italic_μ start_POSTSUBSCRIPT italic_a italic_n italic_g end_POSTSUBSCRIPT</annotation></semantics></math> [deg]</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2">
<math alttext="\sigma_{ang}" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><msub id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml"><mi id="S4.T2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.m1.1.1.2.cmml">σ</mi><mrow id="S4.T2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.m1.1.1.3.cmml"><mi id="S4.T2.2.2.2.m1.1.1.3.2" xref="S4.T2.2.2.2.m1.1.1.3.2.cmml">a</mi><mo id="S4.T2.2.2.2.m1.1.1.3.1" xref="S4.T2.2.2.2.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.2.2.2.m1.1.1.3.3" xref="S4.T2.2.2.2.m1.1.1.3.3.cmml">n</mi><mo id="S4.T2.2.2.2.m1.1.1.3.1a" xref="S4.T2.2.2.2.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.2.2.2.m1.1.1.3.4" xref="S4.T2.2.2.2.m1.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.m1.1.1.2">𝜎</ci><apply id="S4.T2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.2.2.2.m1.1.1.3"><times id="S4.T2.2.2.2.m1.1.1.3.1.cmml" xref="S4.T2.2.2.2.m1.1.1.3.1"></times><ci id="S4.T2.2.2.2.m1.1.1.3.2.cmml" xref="S4.T2.2.2.2.m1.1.1.3.2">𝑎</ci><ci id="S4.T2.2.2.2.m1.1.1.3.3.cmml" xref="S4.T2.2.2.2.m1.1.1.3.3">𝑛</ci><ci id="S4.T2.2.2.2.m1.1.1.3.4.cmml" xref="S4.T2.2.2.2.m1.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\sigma_{ang}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">italic_σ start_POSTSUBSCRIPT italic_a italic_n italic_g end_POSTSUBSCRIPT</annotation></semantics></math> [deg]</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.3">
<math alttext="\mu_{pos}" class="ltx_Math" display="inline" id="S4.T2.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.m1.1a"><msub id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml"><mi id="S4.T2.3.3.3.m1.1.1.2" xref="S4.T2.3.3.3.m1.1.1.2.cmml">μ</mi><mrow id="S4.T2.3.3.3.m1.1.1.3" xref="S4.T2.3.3.3.m1.1.1.3.cmml"><mi id="S4.T2.3.3.3.m1.1.1.3.2" xref="S4.T2.3.3.3.m1.1.1.3.2.cmml">p</mi><mo id="S4.T2.3.3.3.m1.1.1.3.1" xref="S4.T2.3.3.3.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.3.3.3.m1.1.1.3.3" xref="S4.T2.3.3.3.m1.1.1.3.3.cmml">o</mi><mo id="S4.T2.3.3.3.m1.1.1.3.1a" xref="S4.T2.3.3.3.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.3.3.3.m1.1.1.3.4" xref="S4.T2.3.3.3.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><apply id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.3.m1.1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T2.3.3.3.m1.1.1.2.cmml" xref="S4.T2.3.3.3.m1.1.1.2">𝜇</ci><apply id="S4.T2.3.3.3.m1.1.1.3.cmml" xref="S4.T2.3.3.3.m1.1.1.3"><times id="S4.T2.3.3.3.m1.1.1.3.1.cmml" xref="S4.T2.3.3.3.m1.1.1.3.1"></times><ci id="S4.T2.3.3.3.m1.1.1.3.2.cmml" xref="S4.T2.3.3.3.m1.1.1.3.2">𝑝</ci><ci id="S4.T2.3.3.3.m1.1.1.3.3.cmml" xref="S4.T2.3.3.3.m1.1.1.3.3">𝑜</ci><ci id="S4.T2.3.3.3.m1.1.1.3.4.cmml" xref="S4.T2.3.3.3.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\mu_{pos}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.m1.1d">italic_μ start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT</annotation></semantics></math> [cm]</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.4.4.4">
<math alttext="\sigma_{pos}" class="ltx_Math" display="inline" id="S4.T2.4.4.4.m1.1"><semantics id="S4.T2.4.4.4.m1.1a"><msub id="S4.T2.4.4.4.m1.1.1" xref="S4.T2.4.4.4.m1.1.1.cmml"><mi id="S4.T2.4.4.4.m1.1.1.2" xref="S4.T2.4.4.4.m1.1.1.2.cmml">σ</mi><mrow id="S4.T2.4.4.4.m1.1.1.3" xref="S4.T2.4.4.4.m1.1.1.3.cmml"><mi id="S4.T2.4.4.4.m1.1.1.3.2" xref="S4.T2.4.4.4.m1.1.1.3.2.cmml">p</mi><mo id="S4.T2.4.4.4.m1.1.1.3.1" xref="S4.T2.4.4.4.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.4.4.4.m1.1.1.3.3" xref="S4.T2.4.4.4.m1.1.1.3.3.cmml">o</mi><mo id="S4.T2.4.4.4.m1.1.1.3.1a" xref="S4.T2.4.4.4.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.4.4.4.m1.1.1.3.4" xref="S4.T2.4.4.4.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><apply id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.4.4.4.m1.1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">subscript</csymbol><ci id="S4.T2.4.4.4.m1.1.1.2.cmml" xref="S4.T2.4.4.4.m1.1.1.2">𝜎</ci><apply id="S4.T2.4.4.4.m1.1.1.3.cmml" xref="S4.T2.4.4.4.m1.1.1.3"><times id="S4.T2.4.4.4.m1.1.1.3.1.cmml" xref="S4.T2.4.4.4.m1.1.1.3.1"></times><ci id="S4.T2.4.4.4.m1.1.1.3.2.cmml" xref="S4.T2.4.4.4.m1.1.1.3.2">𝑝</ci><ci id="S4.T2.4.4.4.m1.1.1.3.3.cmml" xref="S4.T2.4.4.4.m1.1.1.3.3">𝑜</ci><ci id="S4.T2.4.4.4.m1.1.1.3.4.cmml" xref="S4.T2.4.4.4.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\sigma_{pos}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.m1.1d">italic_σ start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT</annotation></semantics></math> [cm]</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.4.5.1.1">SOP</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.5.1.2">22.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.5.1.3">17.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.5.1.4">8.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.5.1.5">7.57</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.6.2.1">SIP</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.6.2.2">16.98</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.6.2.3">13.26</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.6.2.4">5.97</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.6.2.5">5.50</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.7.3.1">RNN (Dropout)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.7.3.2">16.83</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.7.3.3">13.41</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.7.3.4">6.27</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.7.3.5">6.32</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.8.4.1">RNN (Acc)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.8.4.2">16.07</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.8.4.3">13.16</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.8.4.4">6.06</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.8.4.5">6.01</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.9.5.1">RNN (Acc+Dropout)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.9.5.2">16.08</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.9.5.3">13.46</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.9.5.4">6.21</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.9.5.5">6.27</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.10.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.10.6.1">BiRNN (Dropout)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.10.6.2">15.86</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.10.6.3">13.12</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.10.6.4">6.09</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.10.6.5">6.01</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.11.7.1">BiRNN (Acc)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.11.7.2">16.31</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.11.7.3">12.28</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.11.7.4">5.78</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.11.7.5">5.62</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.12.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.12.8.1">BiRNN (Acc+Dropout)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.12.8.2">15.85</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.12.8.3">12.87</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.12.8.4">5.98</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.12.8.5">6.03</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.13.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.4.13.9.1">BiRNN (after fine-tuning)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.13.9.2">16.84</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.13.9.3">13.22</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.13.9.4">6.51</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.13.9.5">6.17</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS4.p7">
<p class="ltx_p" id="S4.SS4.p7.1">Below, we have provided our model architecture as a list. Here, <math alttext="N" class="ltx_Math" display="inline" id="S4.SS4.p7.1.m1.1"><semantics id="S4.SS4.p7.1.m1.1a"><mi id="S4.SS4.p7.1.m1.1.1" xref="S4.SS4.p7.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p7.1.m1.1b"><ci id="S4.SS4.p7.1.m1.1.1.cmml" xref="S4.SS4.p7.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p7.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p7.1.m1.1d">italic_N</annotation></semantics></math> is the number of sensors the model uses - it will either be 24 or 6.</p>
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.2">Linear Layer: Transform from shape <math alttext="N*(3*3+3)" class="ltx_Math" display="inline" id="S4.I3.i1.p1.1.m1.1"><semantics id="S4.I3.i1.p1.1.m1.1a"><mrow id="S4.I3.i1.p1.1.m1.1.1" xref="S4.I3.i1.p1.1.m1.1.1.cmml"><mi id="S4.I3.i1.p1.1.m1.1.1.3" xref="S4.I3.i1.p1.1.m1.1.1.3.cmml">N</mi><mo id="S4.I3.i1.p1.1.m1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S4.I3.i1.p1.1.m1.1.1.2.cmml">*</mo><mrow id="S4.I3.i1.p1.1.m1.1.1.1.1" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.cmml"><mo id="S4.I3.i1.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.I3.i1.p1.1.m1.1.1.1.1.1" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.cmml"><mrow id="S4.I3.i1.p1.1.m1.1.1.1.1.1.2" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.cmml"><mn id="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.2" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.2.cmml">3</mn><mo id="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.1.cmml">*</mo><mn id="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.3" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.3.cmml">3</mn></mrow><mo id="S4.I3.i1.p1.1.m1.1.1.1.1.1.1" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S4.I3.i1.p1.1.m1.1.1.1.1.1.3" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.3.cmml">3</mn></mrow><mo id="S4.I3.i1.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.1.m1.1b"><apply id="S4.I3.i1.p1.1.m1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1"><times id="S4.I3.i1.p1.1.m1.1.1.2.cmml" xref="S4.I3.i1.p1.1.m1.1.1.2"></times><ci id="S4.I3.i1.p1.1.m1.1.1.3.cmml" xref="S4.I3.i1.p1.1.m1.1.1.3">𝑁</ci><apply id="S4.I3.i1.p1.1.m1.1.1.1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1.1.1"><plus id="S4.I3.i1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.1"></plus><apply id="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.2"><times id="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.1"></times><cn id="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.2.cmml" type="integer" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.2">3</cn><cn id="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.3.cmml" type="integer" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.2.3">3</cn></apply><cn id="S4.I3.i1.p1.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S4.I3.i1.p1.1.m1.1.1.1.1.1.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.1.m1.1c">N*(3*3+3)</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i1.p1.1.m1.1d">italic_N * ( 3 * 3 + 3 )</annotation></semantics></math> to <math alttext="512" class="ltx_Math" display="inline" id="S4.I3.i1.p1.2.m2.1"><semantics id="S4.I3.i1.p1.2.m2.1a"><mn id="S4.I3.i1.p1.2.m2.1.1" xref="S4.I3.i1.p1.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.2.m2.1b"><cn id="S4.I3.i1.p1.2.m2.1.1.cmml" type="integer" xref="S4.I3.i1.p1.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.2.m2.1c">512</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i1.p1.2.m2.1d">512</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.2">Positional Encoding: Since transformers have no notion of position, augment each input with information about where it occurs in the sequence as per <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib12" title="">12</a>]</cite>. Transforms from <math alttext="512" class="ltx_Math" display="inline" id="S4.I3.i2.p1.1.m1.1"><semantics id="S4.I3.i2.p1.1.m1.1a"><mn id="S4.I3.i2.p1.1.m1.1.1" xref="S4.I3.i2.p1.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.1.m1.1b"><cn id="S4.I3.i2.p1.1.m1.1.1.cmml" type="integer" xref="S4.I3.i2.p1.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.1.m1.1c">512</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.1.m1.1d">512</annotation></semantics></math> to <math alttext="512" class="ltx_Math" display="inline" id="S4.I3.i2.p1.2.m2.1"><semantics id="S4.I3.i2.p1.2.m2.1a"><mn id="S4.I3.i2.p1.2.m2.1.1" xref="S4.I3.i2.p1.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.2.m2.1b"><cn id="S4.I3.i2.p1.2.m2.1.1.cmml" type="integer" xref="S4.I3.i2.p1.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.2.m2.1c">512</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.2.m2.1d">512</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I3.i3.p1">
<p class="ltx_p" id="S4.I3.i3.p1.2">Transformer Encoder: Stack of 6 identical layers, each accepting and returning <math alttext="512" class="ltx_Math" display="inline" id="S4.I3.i3.p1.1.m1.1"><semantics id="S4.I3.i3.p1.1.m1.1a"><mn id="S4.I3.i3.p1.1.m1.1.1" xref="S4.I3.i3.p1.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.I3.i3.p1.1.m1.1b"><cn id="S4.I3.i3.p1.1.m1.1.1.cmml" type="integer" xref="S4.I3.i3.p1.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i3.p1.1.m1.1c">512</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i3.p1.1.m1.1d">512</annotation></semantics></math>. Each layer implements multi-headed self attention with 4 heads, then runs the concatenated result through a feedforward network to transform it back to <math alttext="512" class="ltx_Math" display="inline" id="S4.I3.i3.p1.2.m2.1"><semantics id="S4.I3.i3.p1.2.m2.1a"><mn id="S4.I3.i3.p1.2.m2.1.1" xref="S4.I3.i3.p1.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.I3.i3.p1.2.m2.1b"><cn id="S4.I3.i3.p1.2.m2.1.1.cmml" type="integer" xref="S4.I3.i3.p1.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i3.p1.2.m2.1c">512</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i3.p1.2.m2.1d">512</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I3.i4.p1">
<p class="ltx_p" id="S4.I3.i4.p1.2">Linear Layer: Transform from <math alttext="512" class="ltx_Math" display="inline" id="S4.I3.i4.p1.1.m1.1"><semantics id="S4.I3.i4.p1.1.m1.1a"><mn id="S4.I3.i4.p1.1.m1.1.1" xref="S4.I3.i4.p1.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.I3.i4.p1.1.m1.1b"><cn id="S4.I3.i4.p1.1.m1.1.1.cmml" type="integer" xref="S4.I3.i4.p1.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i4.p1.1.m1.1c">512</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i4.p1.1.m1.1d">512</annotation></semantics></math> back to <math alttext="24*(3*3)" class="ltx_Math" display="inline" id="S4.I3.i4.p1.2.m2.1"><semantics id="S4.I3.i4.p1.2.m2.1a"><mrow id="S4.I3.i4.p1.2.m2.1.1" xref="S4.I3.i4.p1.2.m2.1.1.cmml"><mn id="S4.I3.i4.p1.2.m2.1.1.3" xref="S4.I3.i4.p1.2.m2.1.1.3.cmml">24</mn><mo id="S4.I3.i4.p1.2.m2.1.1.2" lspace="0.222em" rspace="0.222em" xref="S4.I3.i4.p1.2.m2.1.1.2.cmml">*</mo><mrow id="S4.I3.i4.p1.2.m2.1.1.1.1" xref="S4.I3.i4.p1.2.m2.1.1.1.1.1.cmml"><mo id="S4.I3.i4.p1.2.m2.1.1.1.1.2" stretchy="false" xref="S4.I3.i4.p1.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S4.I3.i4.p1.2.m2.1.1.1.1.1" xref="S4.I3.i4.p1.2.m2.1.1.1.1.1.cmml"><mn id="S4.I3.i4.p1.2.m2.1.1.1.1.1.2" xref="S4.I3.i4.p1.2.m2.1.1.1.1.1.2.cmml">3</mn><mo id="S4.I3.i4.p1.2.m2.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I3.i4.p1.2.m2.1.1.1.1.1.1.cmml">*</mo><mn id="S4.I3.i4.p1.2.m2.1.1.1.1.1.3" xref="S4.I3.i4.p1.2.m2.1.1.1.1.1.3.cmml">3</mn></mrow><mo id="S4.I3.i4.p1.2.m2.1.1.1.1.3" stretchy="false" xref="S4.I3.i4.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i4.p1.2.m2.1b"><apply id="S4.I3.i4.p1.2.m2.1.1.cmml" xref="S4.I3.i4.p1.2.m2.1.1"><times id="S4.I3.i4.p1.2.m2.1.1.2.cmml" xref="S4.I3.i4.p1.2.m2.1.1.2"></times><cn id="S4.I3.i4.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.I3.i4.p1.2.m2.1.1.3">24</cn><apply id="S4.I3.i4.p1.2.m2.1.1.1.1.1.cmml" xref="S4.I3.i4.p1.2.m2.1.1.1.1"><times id="S4.I3.i4.p1.2.m2.1.1.1.1.1.1.cmml" xref="S4.I3.i4.p1.2.m2.1.1.1.1.1.1"></times><cn id="S4.I3.i4.p1.2.m2.1.1.1.1.1.2.cmml" type="integer" xref="S4.I3.i4.p1.2.m2.1.1.1.1.1.2">3</cn><cn id="S4.I3.i4.p1.2.m2.1.1.1.1.1.3.cmml" type="integer" xref="S4.I3.i4.p1.2.m2.1.1.1.1.1.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i4.p1.2.m2.1c">24*(3*3)</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i4.p1.2.m2.1d">24 * ( 3 * 3 )</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS4.p8">
<p class="ltx_p" id="S4.SS4.p8.1">We trained this for 5 epochs on an RTX3090, where each epoch took around 4 minutes. The transformer architecture yields performance around 5 times as fast as the LSTM.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, our objective was to attain higher pose estimation accuracy through a combination of data-driven IMU placement and a novel application of the transformer network architecture.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Discussion of Model Performance</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.2">As a result of our data-driven IMU placement strategy, both our LSTM and transformer-powered models achieve better performance on the TotalCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib11" title="">11</a>]</cite> dataset than DIP-IMU, the previous work in this domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib7" title="">7</a>]</cite>. See table <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S4.T2" title="Table 2 ‣ 4.4 Adding The Transformer ‣ 4 System design ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">2</span></a> for DIP-IMU’s results on TotalCapture - the <math alttext="\mu_{ang}" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><msub id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">μ</mi><mrow id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml"><mi id="S5.SS1.p1.1.m1.1.1.3.2" xref="S5.SS1.p1.1.m1.1.1.3.2.cmml">a</mi><mo id="S5.SS1.p1.1.m1.1.1.3.1" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.1.m1.1.1.3.3" xref="S5.SS1.p1.1.m1.1.1.3.3.cmml">n</mi><mo id="S5.SS1.p1.1.m1.1.1.3.1a" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.1.m1.1.1.3.4" xref="S5.SS1.p1.1.m1.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">𝜇</ci><apply id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3"><times id="S5.SS1.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.3.1"></times><ci id="S5.SS1.p1.1.m1.1.1.3.2.cmml" xref="S5.SS1.p1.1.m1.1.1.3.2">𝑎</ci><ci id="S5.SS1.p1.1.m1.1.1.3.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3.3">𝑛</ci><ci id="S5.SS1.p1.1.m1.1.1.3.4.cmml" xref="S5.SS1.p1.1.m1.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\mu_{ang}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_μ start_POSTSUBSCRIPT italic_a italic_n italic_g end_POSTSUBSCRIPT</annotation></semantics></math> captures local rotation errors for each type of model. The important value is <math alttext="15.85" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mn id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">15.85</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><cn id="S5.SS1.p1.2.m2.1.1.cmml" type="float" xref="S5.SS1.p1.2.m2.1.1">15.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">15.85</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">15.85</annotation></semantics></math>, which is the best local rotation error that DIP-IMU achieved on TotalCapture.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.1.1">Attribute</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.2">Value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.2.1.1">crit_type</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.2.1.2">MSELoss()</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T3.1.3.2.1">crit_score</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.1.3.2.2">0.010</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T3.1.4.3.1">pos_err</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.1.4.3.2">0.094</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T3.1.5.4.1">loc_rot_err</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.1.5.4.2">8.695</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S5.T3.1.6.5.1">global_rot_err</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T3.1.6.5.2">15.221</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Transformer TotalCapture Performance With 24 Sensors</figcaption>
</figure>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.1.1.1">Attribute</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.2">Value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.2.1.1">crit_type</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.2.1.2">MSELoss()</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T4.1.3.2.1">crit_score</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.1.3.2.2">0.025</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T4.1.4.3.1">pos_err</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.1.4.3.2">0.184</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T4.1.5.4.1">loc_rot_err</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.1.5.4.2">13.892</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S5.T4.1.6.5.1">global_rot_err</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T4.1.6.5.2">40.588</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>BiRNN TotalCapture Performance With 24 Sensors</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">To evaluate baseline performance - i.e. the best possible rotational error that our models could achieve - we ran evaluations on the models trained on 24 IMUs. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.T4" title="Table 4 ‣ 5.1 Discussion of Model Performance ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">4</span></a> captures the evaluations for the LSTM, while table <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.T3" title="Table 3 ‣ 5.1 Discussion of Model Performance ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">3</span></a> captures the evaluations for the transformer. Therefore, the ideal rotational error we hope to achieve would be 13.892 degrees with an LSTM, and 8.695 degrees with a transformer.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">That said, tables <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.T5" title="Table 5 ‣ 5.1 Discussion of Model Performance ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.T6" title="Table 6 ‣ 5.1 Discussion of Model Performance ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">6</span></a> reveal the performance of the optimized LSTM and transformer models, respectively. Notice that the optimized LSTM has a local rotation error of 13.018 on TotalCapture, while the optimized transformer has a local rotation error of 12.916. Both models yield significantly better performance than TotalCapture’s solution, while the transformer, as expected, performs slightly better than the LSTM.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>BiRNN TotalCapture Performance With 6 Sensors</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.1.1.1.1">Attribute</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.1.1.1.2">Value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.1.2.1.1">crit_type</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.2.1.2">MSELoss()</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T5.1.3.2.1">crit_score</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T5.1.3.2.2">0.023</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T5.1.4.3.1">pos_err</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T5.1.4.3.2">0.155</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T5.1.5.4.1">loc_rot_err</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T5.1.5.4.2">13.018</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S5.T5.1.6.5.1">global_rot_err</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T5.1.6.5.2">29.308</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Transformer TotalCapture Performance With 6 Sensors</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.1.1.1.1">Attribute</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.1.1.1.2">Value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.1.2.1.1">crit_type</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T6.1.2.1.2">MSELoss()</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T6.1.3.2.1">crit_score</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T6.1.3.2.2">0.021</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T6.1.4.3.1">pos_err</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T6.1.4.3.2">0.125</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T6.1.5.4.1">loc_rot_err</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T6.1.5.4.2">12.916</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S5.T6.1.6.5.1">global_rot_err</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T6.1.6.5.2">25.088</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">Do remember, however, that even though the accuracy delta between the LSTM and transformer isn’t that large, the transformer’s massively parallel nature meant that it only took a fifth of the time to train.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Discussion of “Optimal” IMU Locations</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Just looking at model evaluations hides some really interesting parts of the story. When we ran feature ablation, we observed that the most relevant IMUs were quite dataset dependent, in addition to the fact that the transformer and LSTM seemed to prefer vastly different sets of IMUs.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Start off by looking at figures <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.F1" title="Figure 1 ‣ 5.2 Discussion of “Optimal” IMU Locations ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.F2" title="Figure 2 ‣ 5.2 Discussion of “Optimal” IMU Locations ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">2</span></a>. Immediately, a disparity in preferred IMUs emerges - the BiRNN prefers the pelvis, left shoulder, left wrist, right knee, upper spine, and left knee, in that order. However, the transformer prefers the pelvis, upper spine, middle spine, left foot, right hip, and left knee, in that order. Although there are certainly some overlaps - namely the pelvis, upper spine, and left knee, three of the joints differ.</p>
</div>
<figure class="ltx_figure" id="S5.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F1.g1" src="extracted/5413950/figures/evaluation/global-birnn-ablation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Global BiRNN Feature Ablation</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F2.g1" src="extracted/5413950/figures/evaluation/global-transformer-ablation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Global Transformer Feature Ablation</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Another disparity appears on a per-dataset basis. Figures <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.F6" title="Figure 6 ‣ 5.2 Discussion of “Optimal” IMU Locations ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.F3" title="Figure 3 ‣ 5.2 Discussion of “Optimal” IMU Locations ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.F4" title="Figure 4 ‣ 5.2 Discussion of “Optimal” IMU Locations ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">4</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#S5.F5" title="Figure 5 ‣ 5.2 Discussion of “Optimal” IMU Locations ‣ 5 Results and Discussion ‣ IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture"><span class="ltx_text ltx_ref_tag">5</span></a> reveal the feature ablation results on the TotalCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib11" title="">11</a>]</cite>, ACCAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib4" title="">4</a>]</cite>, CMU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib6" title="">6</a>]</cite>, and BioMotionLab_Ntroj <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.08923v2#bib.bib10" title="">10</a>]</cite> datasets. Although each dataset does seem to yield the same set of optimal IMUs as the globally optimal BiRNN IMUs, they occur with different degrees of importance.</p>
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">ACCAD: Pelvis, left shoulder, upper spine, left knee, left wrist, right knee</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">BioMotionLab_Ntroj: Left shoulder, pelvis, left wrist, right knee, upper spine, left knee.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">CMU: Pelvis, left shoulder, left wrist, right knee, left knee, upper spine</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">TotalCapture: Pelvis, left shoulder, left wrist, right knee, upper spine, left knee</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F3.g1" src="extracted/5413950/figures/evaluation/acc-birnn-ablation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>ACCAD BiRNN Feature Ablation</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F4.g1" src="extracted/5413950/figures/evaluation/bio-birnn-ablation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>BioMotionLab_NTroj BiRNN Feature Ablation</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F5.g1" src="extracted/5413950/figures/evaluation/cmu-birnn-ablation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>CMU BiRNN Feature Ablation</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F6.g1" src="extracted/5413950/figures/evaluation/tc-birnn-ablation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>TotalCapture BiRNN Feature Ablation</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">A similar sort of disparity occurs with the transformer - take a look at the listing below:</p>
<ol class="ltx_enumerate" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1">ACCAD: Pelvis, upper spine, left knee, left foot, right knee, right elbow</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1">BioMotionLab_NTroj: Upper spine, left foot, middle spine, left knee, right foot, right hip</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1">CMU: Pelvis, upper spine, left foot, right knee, middle spine, right hip</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S5.I2.i4.p1">
<p class="ltx_p" id="S5.I2.i4.p1.1">TotalCapture: Pelvis, upper spine, middle spine, left foot, left knee, right foot</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F7.g1" src="extracted/5413950/figures/evaluation/acc-transformer-ablation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>ACCAD Transformer Feature Ablation</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F8.g1" src="extracted/5413950/figures/evaluation/bio-transformer-ablation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>BioMotionLab_NTroj Transformer Feature Ablation</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F9.g1" src="extracted/5413950/figures/evaluation/cmu-transformer-ablation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>CMU Transformer Feature Ablation</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F10.g1" src="extracted/5413950/figures/evaluation/tc-transformer-ablation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>TotalCapture Transformer Feature Ablation</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">This time, many individual joints differ as well, rather than just being reordered. This could point to the transformer’s potential for extracting finer nuances of the data - but in either case, reveals that finding globally optimal IMUs in all situations might not be possible. The vastly differing results between datasets and model architectures reveals that IMUs should be carefully chosen in a data-driven manner with regards to individual activity types. Although this paper does try to approximate an optimal set of IMUs for each model architecture, the true result of our study is that the “optimal” set of IMUs for pose estimation should be tailored to the situation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our exploration into the use of sparse IMU configurations for human pose estimation has culminated in a robust methodology that outpaces the capabilities of conventional biRNN models. By employing a data-driven strategy to determine the optimal placement of IMUs on the body and leveraging the computational strengths of the transformer architecture, we have achieved notable advancements in pose reconstruction accuracy from IMU data.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Our findings underscore the significance of IMU placement, revealing that the strategic selection of sensor locations is crucial and highly contingent on the dataset and activity type. The contrasting preferences between LSTM and transformer models for certain IMUs, along with the variations across different datasets, illuminate the complexity and situational dependency of optimal sensor placement. This work emphasizes that there is no one-size-fits-all solution; instead, IMU placement must be tailored to the specific context and objectives of the application.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Furthermore, the rapid training times and parallel processing capabilities of the transformer model present compelling advantages over traditional LSTM models. This efficiency, combined with the transformer’s proficiency in time-series analysis, positions our approach as a scalable and practical solution for real-world applications.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">In summary, this study not only contributes a novel methodological framework for IMU-based pose estimation but also provides key insights into the nuanced relationships between IMU data and human pose. Our results pave the way for future research in this domain, encouraging the development of more adaptive, efficient, and precise pose estimation systems that capitalize on the ubiquity and versatility of IMUs in consumer devices.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
activpal: Pal technologies.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Captum · Model Interpretability for PyTorch.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Motion Capture | Movella.com.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib4.1.1">Advanced Computing Center for the Arts and Design</span>.

</span>
<span class="ltx_bibblock">ACCAD MoCap Dataset.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib5.1.1">Breiman, L.</span>
</span>
<span class="ltx_bibblock">Random Forests.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.2.1">Machine Learning 45</span>, 1 (2001), 5–32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib6.1.1">Carnegie Mellon University</span>.

</span>
<span class="ltx_bibblock">CMU MoCap Dataset.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib7.1.1">Huang, Y., Kaufmann, M., Aksan, E., Black, M. J., Hilliges, O., and Pons-Moll, G.</span>
</span>
<span class="ltx_bibblock">Deep inertial poser: learning to reconstruct human pose from sparse inertial measurements in real time.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.2.1">ACM Trans. Graph. 37</span>, 6 (Dec. 2018), 1–15.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib8.1.1">Mahmood, N., Ghorbani, N., Troje, N. F., Pons-Moll, G., and Black, M. J.</span>
</span>
<span class="ltx_bibblock">AMASS: Archive of motion capture as surface shapes.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.2.1">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</span> (Oct. 2019), pp. 5441–5450.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib9.1.1">Mollyn, V., Arakawa, R., Goel, M., Harrison, C., and Ahuja, K.</span>
</span>
<span class="ltx_bibblock">IMUPoser: Full-Body Pose Estimation using IMUs in Phones, Watches, and Earbuds.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.2.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</span> (Hamburg Germany, Apr. 2023), ACM, pp. 1–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib10.1.1">Troje, N. F.</span>
</span>
<span class="ltx_bibblock">Decomposing biological motion: A framework for analysis and synthesis of human gait patterns.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.2.1">Journal of Vision 2</span>, 5 (Sept. 2002), 2–2.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib11.1.1">Trumble, M., Gilbert, A., Malleson, C., Hilton, A., and Collomosse, J.</span>
</span>
<span class="ltx_bibblock">Total capture: 3d human pose estimation fusing video and inertial sensors.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.2.1">Proceedings of the British Machine Vision Conference (BMVC)</span> (Sept. 2017), pp. 14.1–14.13.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib12.1.1">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I.</span>
</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.2.1">Advances in Neural Information Processing Systems</span> (2017), I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30, Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib13.1.1">Yang, S., Quan, Z., Nie, M., and Yang, W.</span>
</span>
<span class="ltx_bibblock">Transpose: Keypoint localization via transformer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.2.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib14.1.1">Yi, X., Zhou, Y., Habermann, M., Shimada, S., Golyanik, V., Theobalt, C., and Xu, F.</span>
</span>
<span class="ltx_bibblock">Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking from Sparse Inertial Sensors, Mar. 2022.

</span>
<span class="ltx_bibblock">arXiv:2203.08528 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib15.1.1">Yi, X., Zhou, Y., and Xu, F.</span>
</span>
<span class="ltx_bibblock">TransPose: Real-time 3D Human Translation and Pose Estimation with Six Inertial Sensors, May 2021.

</span>
<span class="ltx_bibblock">arXiv:2105.04605 [cs].

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Feb 16 22:06:51 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
