<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Deep Learning-Based Object Pose Estimation: A Comprehensive Survey</title>
<!--Generated on Fri May 31 15:08:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Object pose estimation,  deep learning,  comprehensive survey,  3D computer vision.
" lang="en" name="keywords"/>
<base href="/html/2405.07801v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S1" title="In Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2" title="In Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_text ltx_font_smallcaps">Datasets and Metrics</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS1" title="In 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span><span class="ltx_text ltx_font_italic">Datasets for Instance-Level Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS1.SSS1" title="In 2.1 Datasets for Instance-Level Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>BOP Challenge Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS1.SSS2" title="In 2.1 Datasets for Instance-Level Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Other Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS2" title="In 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span><span class="ltx_text ltx_font_italic">Datasets for Category-Level Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS2.SSS1" title="In 2.2 Datasets for Category-Level Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Rigid Objects Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS2.SSS2" title="In 2.2 Datasets for Category-Level Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Articulated Objects Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS3" title="In 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span><span class="ltx_text ltx_font_italic">Datasets for Unseen Methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS4" title="In 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span><span class="ltx_text ltx_font_italic">Metrics</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS4.SSS1" title="In 2.4 Metrics ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span>3DoF Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS4.SSS2" title="In 2.4 Metrics ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span>6DoF Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS4.SSS3" title="In 2.4 Metrics ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.3 </span>9DoF Evaluation Metric</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS4.SSS4" title="In 2.4 Metrics ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.4 </span>Other Metric</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3" title="In Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">Instance-Level Object Pose Estimation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS1" title="In 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span><span class="ltx_text ltx_font_italic">Correspondence-Based Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS1.SSS1" title="In 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Sparse Correspondence Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS1.SSS2" title="In 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Dense Correspondence Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS2" title="In 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span><span class="ltx_text ltx_font_italic">Template-Based Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS2.SSS1" title="In 3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>RGB-Based Template Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS2.SSS2" title="In 3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Point Cloud-Based Template Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS3" title="In 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span><span class="ltx_text ltx_font_italic">Voting-Based Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS3.SSS1" title="In 3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Indirect Voting Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS3.SSS2" title="In 3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Direct Voting Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS4" title="In 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span><span class="ltx_text ltx_font_italic">Regression-Based Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS4.SSS1" title="In 3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Geometry-Guided Regression Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS4.SSS2" title="In 3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Direct Regression Methods</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4" title="In Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span><span class="ltx_text ltx_font_smallcaps">Category-Level Object Pose Estimation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS1" title="In 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span><span class="ltx_text ltx_font_italic">Shape Prior-Based Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS1.SSS1" title="In 4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>NOCS Shape Alignment Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS1.SSS2" title="In 4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Direct Regress Pose Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2" title="In 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span><span class="ltx_text ltx_font_italic">Shape Prior-Free Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2.SSS1" title="In 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Depth-Guided Geometry-Aware Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2.SSS2" title="In 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>RGBD-Guided Semantic and Geometry Fusion Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2.SSS3" title="In 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Others</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5" title="In Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span><span class="ltx_text ltx_font_smallcaps">Unseen Object Pose Estimation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS1" title="In 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span><span class="ltx_text ltx_font_italic">CAD Model-Based Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS1.SSS1" title="In 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Feature Matching-Based Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS1.SSS2" title="In 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Template Matching-Based Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS2" title="In 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span><span class="ltx_text ltx_font_italic">Manual Reference View-Based Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS2.SSS1" title="In 5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Feature Matching-Based Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS2.SSS2" title="In 5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Template Matching-Based Methods</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6" title="In Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span><span class="ltx_text ltx_font_smallcaps">Applications</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS1" title="In 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span><span class="ltx_text ltx_font_italic">Robotic Manipulation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS1.SSS1" title="In 6.1 Robotic Manipulation ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Instance-Level Manipulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS1.SSS2" title="In 6.1 Robotic Manipulation ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Category-Level Manipulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS1.SSS3" title="In 6.1 Robotic Manipulation ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>Unseen Object Manipulation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS2" title="In 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span><span class="ltx_text ltx_font_italic">Augmented Reality/Virtual Reality</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS3" title="In 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span><span class="ltx_text ltx_font_italic">Aerospace</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS4" title="In 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span><span class="ltx_text ltx_font_italic">Hand-Object Interaction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS5" title="In 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span><span class="ltx_text ltx_font_italic">Autonomous Driving</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S7" title="In Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span><span class="ltx_text ltx_font_smallcaps">Conclusion and Future Direction</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Deep Learning-Based Object Pose Estimation:
<br class="ltx_break"/>A Comprehensive Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jian Liu, Wei Sun, Hui Yang, Zhiwen Zeng, Chongpei Liu, Jin Zheng, Xingyu Liu
<br class="ltx_break"/>Hossein Rahmani, Nicu Sebe, , and Ajmal Mian
</span><span class="ltx_author_notes">
Jian Liu, Wei Sun, Hui Yang, Zhiwen Zeng, and Chongpei Liu are with the National Engineering Research Center for Robot Visual Perception and Control Technology, College of Electrical and Information Engineering, and the State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body, Hunan University, Changsha 410082, China. E-mail: (jianliu, wei_sun, huiyang, zingaltern, chongpei56)@hnu.edu.cn
Jin Zheng is with the School of Architecture and Art, Central South University, Changsha, 410082, China. E-mail: zheng.jin@csu.edu.cn
Xingyu Liu is with the Department of Automation, Tsinghua University, Beijing 100084, China. E-mail: liuxy21@mails.tsinghua.edu.cn
Hossein Rahmani is with the School of Computing and Communications, Lancaster University, LA1 4YW, United Kingdom. E-mail: h.rahmani@lancaster.ac.uk
Nicu Sebe is with the Department of Information Engineering and Computer Science, University of Trento, Trento 38123, Italy. E-mail: sebe@disi.unitn.it
Ajmal Mian is with the Department of Computer Science, The University of Western Australia, WA 6009, Australia. E-mail: ajmal.mian@uwa.edu.au
This work was done while Jian Liu was a visiting Ph.D. student with The University of Western Australia, supervised by Prof. Ajmal Mian, and Chongpei Liu was a visiting Ph.D. student with the University of Trento, supervised by Prof. Nicu Sebe.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, <em class="ltx_emph ltx_font_italic" id="id1.id1.1">i.e.</em>, instance-level, category-level, and unseen object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We also keep tracing the latest works at <a class="ltx_ref ltx_href" href="https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation" title="">Awesome-Object-Pose-Estimation</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Object pose estimation, deep learning, comprehensive survey, 3D computer vision.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Object pose estimation is a fundamental computer vision problem that aims to estimate the pose of an object in a given image relative to the camera that captured the image.
Object pose estimation is a crucial technology for augmented reality<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib3" title="">3</a>]</cite>, robotic manipulation<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib5" title="">5</a>]</cite>, hand-object interaction<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib7" title="">7</a>]</cite>, etc. Depending on the application needs, the object pose is estimated up to varying degrees of freedom (DoF) such as 3DoF that only includes 3D rotation, 6DoF that additionally includes 3D translation, or 9DoF which includes estimating the 3D size of the object besides the 3D rotation and 3D translation.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In the pre-deep learning era, many hand-crafted feature-based approaches such as SIFT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib8" title="">8</a>]</cite>, FPFH<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib9" title="">9</a>]</cite>, VFH<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib10" title="">10</a>]</cite>, and Point Pair Features (PPF)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib14" title="">14</a>]</cite> were designed for object pose estimation. However, these methods exhibit deficiencies in accuracy and robustness when confronted with complex scenes<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib16" title="">16</a>]</cite>. These traditional methods have now been supplanted by data driven deep learning-based approaches that harness the power of deep neural networks to learn high-dimensional feature representations from data, leading to improved accuracy and robustness to handle complex environments.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<p class="ltx_p ltx_align_center" id="S1.F1.1"><span class="ltx_text" id="S1.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="420" id="S1.F1.1.1.g1" src="x1.png" width="830"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison of instance-level, category-level, and unseen object methods. Instance-level methods can only estimate the pose of specific object instances on which they are trained. Category-level methods can infer intra-class unseen instances rather than being limited to specific instances in the training data. In contrast, unseen object pose estimation methods have stronger generalization ability and can handle object categories not encountered during training.</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="277" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A taxonomy of this survey. Firstly, we review the datasets and evaluation metrics used to evaluate object pose estimation. Next, we review the deep learning-based methods by dividing them into three categories: instance-level, category-level, and unseen methods. Instance-level methods can be further classified into correspondence-based, template-based, voting-based, and regression-based methods. Category-level methods can be further divided into shape prior-based and shape prior-free methods. Unseen methods can be further classified into CAD model-based and manual reference view-based methods.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Deep learning-based object pose estimation methods can be divided into instance-level, category-level, and unseen object methods according to the problem formulation. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">1</span></a> shows a comparison of the three methods. Early methods were mainly instance-level <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib20" title="">20</a>]</cite>, trained to estimate the pose of specific object instances. Instance-level methods can be further divided into correspondence-based, template-based, voting-based, and regression-based methods. Since instance-level methods are trained on instance-specific data, they can estimate pose with high precision for the given object instances. However, their generalization performance is poor because they are meant to be applied only to the instances on which they are trained. Moreover, many instance-level methods<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib21" title="">21</a>]</cite> require CAD models of the objects. Recognizing these limitations, Wang <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib22" title="">22</a>]</cite> proposed the first category-level object pose and size estimation method. They generalize to intra-class unseen objects without necessitating retraining and employing CAD models during inference. Subsequent category-level methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib27" title="">27</a>]</cite> can be divided into shape prior-based and shape prior-free methods. While improving the generalization ability within a category, these category-level methods still need to collect and label extensive training data for each object category. Moreover, these methods cannot generalize to unseen object categories. To this end, some unseen object pose estimation methods have been recently proposed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib3" title="">3</a>]</cite>, which can be further classified into CAD model-based and manual reference view-based methods. These methods further enhance the generalization of object pose estimation, <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">i.e.</em>, they can be generalized to unseen objects without retraining. Nevertheless, they still need to obtain the object CAD model or annotate a few reference images of the object.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Although significant progress has been made in the area of object pose estimation, several challenges persist in current methods, such as the reliance on labeled training data, difficulty in generalizing to novel unseen objects, model compactness, and robustness in challenging scenarios. To enable readers to swiftly grasp the current state-of-the-art (SOTA) in object pose estimation and facilitate further research in this direction, it is crucial to provide a thorough review of all the relevant problem formulations. A close examination of the existing academic literature reveals a significant gap when reviewing the various problem formulations in object pose estimation. Current prevailing reviews <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib35" title="">35</a>]</cite> tend to exhibit a narrow focus, either confined to particular input modalities<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib33" title="">33</a>]</cite> or tethered to specific application domains<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib35" title="">35</a>]</cite>. Furthermore, these reviews predominantly scrutinize instance-level and category-level methods, thus neglecting the exploration of the most practical problem formulation in the domain which is unseen object pose estimation. This hinders readers from gaining a comprehensive understanding of the area. For instance, Fan <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib33" title="">33</a>]</cite> provided valuable insights into RGB image-based object pose estimation. However, their focus is limited to a singular modality, hindering readers from comprehensively understanding methods across various input modalities. Conversely, Du <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib34" title="">34</a>]</cite> exclusively examined object pose estimation within the context of the robotic grasping task, which limits the readers to understand object pose estimation only from the perspective of a single specific application.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address the above problems, we present here a comprehensive survey of recent advancements in deep learning-based methods for object pose estimation. Our survey encompasses all problem formulations, including instance-level, category-level, and unseen object pose estimation, aiming to provide readers with a holistic understanding of this field. Additionally, we discuss different domain training paradigms, application areas, evaluation metrics, and benchmark datasets, as well as report the performance of state-of-the-art methods on these benchmarks, aiding readers in selecting suitable methods for their applications. Furthermore, we also highlight prevailing trends and discuss their strengths and weaknesses, as well as identify key challenges and promising avenues for future research. The taxonomy of this survey is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our main contributions and highlights are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a <em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.1">comprehensive survey</em> of deep learning-based object pose estimation methods. This is the <em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.2">first</em> survey that covers all three problem formulations in the domain, including instance-level, category-level, and unseen object pose estimation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Our survey covers popular input data modalities (RGB images, depth images, RGBD images), the different degrees of freedom (3DoF, 6DoF, 9DoF) in output poses, object properties (rigid, articulated) for the task of pose estimation as well as tracking. It is crucial to cover all these aspects in a single survey to give a complete picture to readers, an aspect overlooked by existing surveys which only cover a few of these aspects.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We discuss different domain training paradigms, inference modes, application areas, evaluation metrics, and benchmark datasets as well as report the performance of existing SOTA methods on these benchmarks to help readers choose the most appropriate ones for deployment in their application.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We highlight popular trends in the evolution of object pose estimation techniques over the past decade and discuss their strengths and weaknesses. We also identify key challenges that are still outstanding in object pose estimation along with promising research directions to guide future efforts.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The rest of this article is organized as follows. Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2" title="2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">2</span></a> reviews the datasets and metrics used to evaluate the three categories of object pose estimation methods. We then review instance-level methods in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3" title="3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3</span></a>, category-level methods in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4" title="4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4</span></a>, and unseen object pose estimation methods in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5" title="5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5</span></a>. In the aforementioned three sections, we also discuss the training paradigms, inference modes, challenges, and popular trends associated with representative methods in the particular category. Next, Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6" title="6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">6</span></a> reviews the common applications of object pose estimation. Finally, Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S7" title="7 Conclusion and Future Direction ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">7</span></a> summarizes this article and provides an outlook on future research directions based on the challenges in the field.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Datasets and Metrics</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The advancement of deep learning-based object pose estimation is closely linked to the creation and utilization of challenging and trustworthy large-scale datasets. This section introduces commonly used mainstream object pose estimation datasets, categorized into instance-level, category-level, and unseen object pose estimation methods based on problem formulation. The chronological overview is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.F3" title="Figure 3 ‣ 2.1.2 Other Datasets ‣ 2.1 Datasets for Instance-Level Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3</span></a>. In addition, we also conduct an overview of the related evaluation metrics.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span class="ltx_text ltx_font_italic" id="S2.SS1.1.1">Datasets for Instance-Level Methods</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Since the BOP Challenge datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib36" title="">36</a>]</cite> are currently the most popular datasets for the evaluation of instance-level methods, we divide the instance-level datasets into BOP Challenge and other datasets for overview.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>BOP Challenge Datasets</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.2"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p1.2.1">Linemod Dataset (LM)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib37" title="">37</a>]</cite></span> comprises 15 RGBD sequences containing annotated RGBD images with ground-truth 6DoF object poses, object CAD models, 2D bounding boxes, and binary masks. Typically following Brachmann <em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS1.p1.2.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib38" title="">38</a>]</cite>, approximately 15<math alttext="\%" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.1.m1.1"><semantics id="S2.SS1.SSS1.p1.1.m1.1a"><mo id="S2.SS1.SSS1.p1.1.m1.1.1" xref="S2.SS1.SSS1.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.1.m1.1d">%</annotation></semantics></math> of images from each sequence are allocated for training, with the remaining 85<math alttext="\%" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.2.m2.1"><semantics id="S2.SS1.SSS1.p1.2.m2.1a"><mo id="S2.SS1.SSS1.p1.2.m2.1.1" xref="S2.SS1.SSS1.p1.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.2.m2.1b"><csymbol cd="latexml" id="S2.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.2.m2.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.2.m2.1d">%</annotation></semantics></math> reserved for testing. These sequences present challenging scenarios with cluttered scenes, texture-less objects, and varying lighting conditions, making accurate object pose estimation difficult.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p2.1.1">Linemod Occlusion Dataset (LM-O)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib39" title="">39</a>]</cite></span> is an extension of the LM dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib37" title="">37</a>]</cite> specifically designed to evaluate the performance in occlusion scenarios. This dataset consists of 1214 RGBD images from the basic sequence in the LM dataset for 8 heavily occluded objects. It is critical for evaluating and improving pose estimation algorithms in complex environments characterized by occlusion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p3.1.1">IC-MI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib40" title="">40</a>]</cite> / IC-BIN Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib41" title="">41</a>]</cite></span> contribute to texture-less object pose estimation. IC-MI comprises six objects: 2 texture-less and 4 textured household item models. IC-BIN dataset is specifically designed to address challenges posed by clutter and occlusion in robot garbage bin picking scenarios. IC-BIN includes 2 objects from the IC-MI.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p4">
<p class="ltx_p" id="S2.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p4.1.1">RU-APC Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib42" title="">42</a>]</cite></span> aims to tackle challenges in warehouse picking tasks and provides rich data for evaluating and improving the perception capabilities of robots in a warehouse automation context. The dataset comprises 10,368 registered depth and RGB images, covering 24 types of objects, which are placed in various poses within different boxes on warehouse shelves to simulate diverse experimental conditions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p5">
<p class="ltx_p" id="S2.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p5.1.1">YCB-Video Dataset (YCB-V)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib15" title="">15</a>]</cite></span> comprises 21 objects distributed across 92 RGBD videos, each video containing 3 to 9 objects from the YCB object dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib43" title="">43</a>]</cite> (totaling 50 objects). It includes 133,827 frames with a resolution of 640<math alttext="\times" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p5.1.m1.1"><semantics id="S2.SS1.SSS1.p5.1.m1.1a"><mo id="S2.SS1.SSS1.p5.1.m1.1.1" xref="S2.SS1.SSS1.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p5.1.m1.1b"><times id="S2.SS1.SSS1.p5.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p5.1.m1.1d">×</annotation></semantics></math>480, making it well-suited for both object pose estimation and tracking tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p6">
<p class="ltx_p" id="S2.SS1.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p6.1.1">T-LESS Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib44" title="">44</a>]</cite></span> is an RGBD dataset designed for texture-less objects commonly found in industrial settings. It includes 30 electrical objects with no obvious texture or distinguishable color properties. In addition, it includes images of varying resolutions. In the training set, images predominantly feature black backgrounds, while the test set showcases diverse backgrounds with varying lighting conditions and occlusions. T-LESS is challenging because of the absence of texture on objects and the intricate environmental settings.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p7">
<p class="ltx_p" id="S2.SS1.SSS1.p7.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p7.1.1">ITODD Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib45" title="">45</a>]</cite></span> includes 28 real-world industrial objects distributed across over 800 scenes with around 3,500 images. This dataset leverages two industrial 3D sensors and three high-resolution grayscale cameras to enable multi-angle observation of the scenes, providing comprehensive and detailed data for industrial object analysis and evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p8">
<p class="ltx_p" id="S2.SS1.SSS1.p8.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p8.1.1">TYO-L / TUD-L Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib36" title="">36</a>]</cite></span> focus on different lighting conditions. Specifically, TYO-L provides observation of 3 objects under 8 lighting conditions. These scenes are designed to evaluate the robustness of pose estimation algorithms to lighting variations. Unlike TYO-L, the data collection method of TUD-L involves fixing the camera and manually moving the object, providing a more realistic representation of the object’s physical movement.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p9">
<p class="ltx_p" id="S2.SS1.SSS1.p9.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p9.1.1">HB Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib46" title="">46</a>]</cite></span> covers various scenes with changes in occlusion and lighting conditions.
It comprises 33 objects, including 17 toys, 8 household items, and 8 industry-related objects, distributed across 13 diverse scenes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p10">
<p class="ltx_p" id="S2.SS1.SSS1.p10.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p10.1.1">HOPE Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib47" title="">47</a>]</cite></span> is specifically designed for household objects, containing 28 toy grocery objects. The HOPE-Image dataset includes objects from 50 scenes across 10 home/office environments. Each scene includes up to 5 lighting variations, such as backlit and obliquely directed lighting, with shadow-casting effects. Additionally, the HOPE-Video dataset comprises 10 video sequences totaling 2,038 frames, with each scene showcasing between 5 to 20 objects.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Other Datasets</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p1.1.1">YCBInEOAT Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib48" title="">48</a>]</cite></span> is designed for RGBD-based object pose tracking in robotic manipulation. It contains the ego-centric RGBD videos of a dual-arm robot manipulating the YCB objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib43" title="">43</a>]</cite>. There are 3 types of manipulation: single-arm pick-and-place, within-arm manipulation, and pick-and-place between arms. This dataset comprises annotations of ground-truth poses across 7449 frames, encompassing 5 distinct objects depicted in 9 videos.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p2.1.1">ClearPose Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib49" title="">49</a>]</cite></span> is designed for transparent objects, which are widely prevalent in daily life, presenting significant challenges to visual perception and sensing systems due to their indistinct texture features and unreliable depth information. It encompasses over 350K real-world RGBD images and 5M instance annotations across 63 household objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p3">
<p class="ltx_p" id="S2.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p3.1.1">MP6D Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib50" title="">50</a>]</cite></span> is an RGBD dataset designed for object pose estimation of metal parts, featuring 20 texture-less metal components. It consists of 20,100 real-world images with object pose labels collected from various scenarios as well as 50K synthetic images, encompassing cluttered and occluded scenes.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="168" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Chronological overview of the datasets for object pose estimation evaluation. Notably, the <span class="ltx_text" id="S2.F3.4.1" style="color:#FFBFBF;">pink arrows</span> represent the BOP Challenge datasets, which can be used to evaluate both instance-level and unseen object methods. The <span class="ltx_text" id="S2.F3.5.2" style="color:#FF0000;">red references</span> represent the datasets of articulated objects. From this, we can also see the development trend in the field of object pose estimation, <em class="ltx_emph ltx_font_italic" id="S2.F3.6.3">i.e.</em>, from instance-level methods to category-level and unseen methods.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span class="ltx_text ltx_font_italic" id="S2.SS2.1.1">Datasets for Category-Level Methods</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In this part, we divide the category-level datasets into rigid and articulated object datasets for elaboration.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Rigid Objects Datasets</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p1.1.1">CAMERA25 Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib22" title="">22</a>]</cite></span> incorporates 1085 instances across 6 object categories: bowl, bottle, can, camera, mug, and laptop. Notably, the object CAD models in CAMERA25 are sourced from the synthetic ShapeNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib51" title="">51</a>]</cite>. Each image within this dataset contains multiple instances, accompanied by segmentation masks and 9DoF pose labels.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p2.1.1">REAL275 Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib22" title="">22</a>]</cite></span> is a real-world dataset comprising 18 videos and approximately 8K RGBD images. The dataset is divided into three subsets: a training set (7 videos), a validation set (5 videos), and a testing set (6 videos). It includes 42 object instances across 6 categories, consistent with those in the CAMERA25 dataset. REAL275 is a prominent real-world dataset extensively used for category-level object pose estimation in academic research.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p3.1.1">kPAM Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib52" title="">52</a>]</cite></span> is tailored specifically for robotic applications, emphasizing the use of keypoints. Notably, it adopts a methodology involving 3D reconstruction followed by manual keypoint annotation on these reconstructions. With a total of 117 training sequences and 245 testing sequences, kPAM offers a substantial collection of data for training and evaluating algorithms related to robotic perception and manipulation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p4">
<p class="ltx_p" id="S2.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p4.1.1">TOD Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib53" title="">53</a>]</cite></span> consists of 15 transparent objects categorized into 6 classes, each annotated with pertinent 3D keypoints. It encompasses a vast collection of 48K stereo and RGBD images capturing both transparent and opaque depth variations. The primary focus of the TOD dataset is on transparent 3D object applications, providing essential resources for tasks such as object detection and pose estimation in challenging scenarios involving transparency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p5">
<p class="ltx_p" id="S2.SS2.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p5.1.1">Objectron Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib54" title="">54</a>]</cite></span> contains 15K annotated video clips with over 4M labeled images belonging to categories of bottles, books, bikes, cameras, chairs, cereal boxes, cups, laptops, and shoes. This dataset is sourced from 10 countries spanning 5 continents, ensuring diverse geographic representation. Due to its extensive content, it is highly advantageous for evaluating the RGB-based category-level object pose estimation and tracking methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p6">
<p class="ltx_p" id="S2.SS2.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p6.1.1">Wild6D Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib55" title="">55</a>]</cite></span> is a substantial real-world dataset used to assess self-supervised category-level object pose estimation methods. It offers annotations exclusively for 486 test videos with diverse backgrounds, showcasing 162 objects across 5 categories (excluding the ”can” category found in CAMERA25 and REAL275).</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p7">
<p class="ltx_p" id="S2.SS2.SSS1.p7.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p7.1.1">PhoCaL Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib56" title="">56</a>]</cite></span> incorporates both RGBD and RGB-P (Polarisation) modalities. It consists of 60 meticulously crafted 3D models representing household objects, including symmetric, transparent, and reflective items. PhoCaL focuses on 8 specific object categories across 24 sequences, deliberately introducing challenges such as occlusion and clutter.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p8">
<p class="ltx_p" id="S2.SS2.SSS1.p8.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p8.1.1">HouseCat6D Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib57" title="">57</a>]</cite></span> is a comprehensive dataset designed for multi-modal category-level object pose estimation and grasping tasks. The dataset encompasses a wide range of household object categories, featuring 194 high-quality 3D models. It includes objects of varying photometric complexity, such as transparent and reflective items, and spans 41 scenes with diverse viewpoints. The dataset is specifically curated to address challenges in object pose estimation, including occlusions and the absence of markers, making it suitable for evaluating algorithms under real-world conditions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Articulated Objects Datasets</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.p1.1.1">BMVC Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib58" title="">58</a>]</cite></span> includes 4 articulated objects: laptop, cabinet, cupboard, and toy train. Each object is modeled as a motion chain comprising components and interconnected heads. Joints are constrained to one rotational and one translational DoF. This dataset provides CAD models and accompanying text files detailing the topology of the underlying motion chain structure for each object.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.p2.1.1">RBO Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib59" title="">59</a>]</cite></span> contains 14 commonly found articulated objects in human environments, with 358 interaction sequences, resulting in a total of 67 minutes of manual manipulation under different experimental conditions, including changes in interaction type, lighting, viewpoint, and background settings.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS2.p3">
<p class="ltx_p" id="S2.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.p3.1.1">HOI4D Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib60" title="">60</a>]</cite></span> is pivotal for advancing research in category-level human-object interactions. It comprises 2.4M RGBD self-centered video frames depicting interactions between over 9 participants and 800 object instances. These instances are divided into 16 categories, including 7 rigid and 9 articulated objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS2.p4">
<p class="ltx_p" id="S2.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.p4.1.1">ReArtMix / ReArtVal Datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib61" title="">61</a>]</cite></span> are formulated to tackle the challenge of partial-level multiple articulated objects pose estimation featuring unknown kinematic structures. The ReArtMix dataset encompasses over 100,000 RGBD images rendered against diverse background scenes. The ReArtVal dataset consists of 6 real-world desktop scenes comprising over 6,000 RGBD frames.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS2.p5">
<p class="ltx_p" id="S2.SS2.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.p5.1.1">ContactArt Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib62" title="">62</a>]</cite></span> is generated using a remote operating system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib63" title="">63</a>]</cite> to manipulate articulated objects in a simulation environment. This system utilizes smartphones and laptops to precisely annotate poses and contact information. This dataset contains 5 prevalent categories of articulated objects: laptops, drawers, safes, microwaves, and trash cans, for a total of 80 instances. All object models are sourced from the PartNet dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib64" title="">64</a>]</cite>, thus promoting scalability.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span><span class="ltx_text ltx_font_italic" id="S2.SS3.1.1">Datasets for Unseen Methods</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The current mainstream datasets for evaluating unseen methods are the BOP Challenge datasets, as discussed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2.SS1.SSS1" title="2.1.1 BOP Challenge Datasets ‣ 2.1 Datasets for Instance-Level Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>. Besides these BOP Challenge datasets, there are also some datasets designed for evaluating manual reference view-based methods as follows.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">MOPED Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib65" title="">65</a>]</cite></span> is a model-free object pose estimation dataset featuring 11 household objects. It includes reference and test images that encompass all views of the objects. Each object in the test sequences is depicted in five distinct environments, with approximately 300 test images per object.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">GenMOP Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib1" title="">1</a>]</cite></span> includes 10 objects ranging from flat objects to thin structure objects. For each object, there are two video sequences collected from various backgrounds and lighting situations. Each video sequence consists of approximately 200 images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.1">OnePose Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib66" title="">66</a>]</cite></span> comprises over 450 real-world video sequences of 150 objects. These sequences are collected in a variety of background conditions and capture all angles of the objects. Each environment has an average duration of 30 seconds. The dataset is randomly partitioned into training and validation sets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p5.1.1">OnePose-LowTexture Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib2" title="">2</a>]</cite></span> is introduced as a complement to the testing set of the existing OnePose dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib66" title="">66</a>]</cite>, which predominantly features textured objects. This dataset comprises 40 household objects with low texture. For each object, there are two video sequences: one serving as the reference video and the other for testing. Each video is captured at a resolution of 1920<math alttext="\times" class="ltx_Math" display="inline" id="S2.SS3.p5.1.m1.1"><semantics id="S2.SS3.p5.1.m1.1a"><mo id="S2.SS3.p5.1.m1.1.1" xref="S2.SS3.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.1.m1.1b"><times id="S2.SS3.p5.1.m1.1.1.cmml" xref="S2.SS3.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p5.1.m1.1d">×</annotation></semantics></math>1440, 30 Frames Per Second (FPS), and approximately 30 seconds in duration.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span><span class="ltx_text ltx_font_italic" id="S2.SS4.1.1">Metrics</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">In this part, we divide the metrics into 3DoF, 6DoF, 9DoF, and other evaluation metrics for a comprehensive overview.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>3DoF Evaluation Metrics</h4>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<p class="ltx_p" id="S2.SS4.SSS1.p1.8">The geodesic distance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib67" title="">67</a>]</cite> between the ground-truth and predicted 3D rotations is a commonly used 3DoF pose estimation metric. Calculating the angle error between two rotation matrices can visually evaluate their relative deviation. It can be formulated as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{array}[]{l}d\left({R_{gt}},R\right)=\arccos\left(\frac{{tr}\left({R_{gt%
}}^{\top}R\right)-1}{2}\right)/\pi\end{array},\vspace{-0.5em}" class="ltx_Math" display="block" id="S2.E1.m1.4"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.5.2" xref="S2.E1.m1.4.4.cmml"><mtable displaystyle="true" id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml"><mtr id="S2.E1.m1.4.4a" xref="S2.E1.m1.4.4.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.4.4b" xref="S2.E1.m1.4.4.cmml"><mrow id="S2.E1.m1.4.4.4.4.4" xref="S2.E1.m1.4.4.4.4.4.cmml"><mrow id="S2.E1.m1.4.4.4.4.4.4" xref="S2.E1.m1.4.4.4.4.4.4.cmml"><mi id="S2.E1.m1.4.4.4.4.4.4.3" xref="S2.E1.m1.4.4.4.4.4.4.3.cmml">d</mi><mo id="S2.E1.m1.4.4.4.4.4.4.2" xref="S2.E1.m1.4.4.4.4.4.4.2.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.4.4.4.4.1.1" xref="S2.E1.m1.4.4.4.4.4.4.1.2.cmml"><mo id="S2.E1.m1.4.4.4.4.4.4.1.1.2" xref="S2.E1.m1.4.4.4.4.4.4.1.2.cmml">(</mo><msub id="S2.E1.m1.4.4.4.4.4.4.1.1.1" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.cmml"><mi id="S2.E1.m1.4.4.4.4.4.4.1.1.1.2" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.2.cmml">R</mi><mrow id="S2.E1.m1.4.4.4.4.4.4.1.1.1.3" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.cmml"><mi id="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.2" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.2.cmml">g</mi><mo id="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.1" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.1.cmml">⁢</mo><mi id="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.3" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.3.cmml">t</mi></mrow></msub><mo id="S2.E1.m1.4.4.4.4.4.4.1.1.3" xref="S2.E1.m1.4.4.4.4.4.4.1.2.cmml">,</mo><mi id="S2.E1.m1.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.cmml">R</mi><mo id="S2.E1.m1.4.4.4.4.4.4.1.1.4" xref="S2.E1.m1.4.4.4.4.4.4.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.4.4.4.4.5" xref="S2.E1.m1.4.4.4.4.4.5.cmml">=</mo><mrow id="S2.E1.m1.4.4.4.4.4.6" xref="S2.E1.m1.4.4.4.4.4.6.cmml"><mrow id="S2.E1.m1.4.4.4.4.4.6.2.2" xref="S2.E1.m1.4.4.4.4.4.6.2.1.cmml"><mi id="S2.E1.m1.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.cmml">arccos</mi><mo id="S2.E1.m1.4.4.4.4.4.6.2.2a" xref="S2.E1.m1.4.4.4.4.4.6.2.1.cmml">⁡</mo><mrow id="S2.E1.m1.4.4.4.4.4.6.2.2.1" xref="S2.E1.m1.4.4.4.4.4.6.2.1.cmml"><mo id="S2.E1.m1.4.4.4.4.4.6.2.2.1.1" xref="S2.E1.m1.4.4.4.4.4.6.2.1.cmml">(</mo><mstyle displaystyle="false" id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mfrac id="S2.E1.m1.1.1.1.1.1.1a" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml">t</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.1.1.1.1.4.cmml">r</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.2a" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mmultiscripts id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">R</mi><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">g</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">t</mi></mrow><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2a" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"></mrow><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2b" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">⊤</mo></mmultiscripts><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">R</mi></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.2.cmml">−</mo><mn id="S2.E1.m1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mn id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">2</mn></mfrac></mstyle><mo id="S2.E1.m1.4.4.4.4.4.6.2.2.1.2" xref="S2.E1.m1.4.4.4.4.4.6.2.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.4.4.4.4.6.1" xref="S2.E1.m1.4.4.4.4.4.6.1.cmml">/</mo><mi id="S2.E1.m1.4.4.4.4.4.6.3" xref="S2.E1.m1.4.4.4.4.4.6.3.cmml">π</mi></mrow></mrow></mtd></mtr></mtable><mo id="S2.E1.m1.4.5.2.1" lspace="0.167em" xref="S2.E1.m1.4.4.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><matrix id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.5.2"><matrixrow id="S2.E1.m1.4.4a.cmml" xref="S2.E1.m1.4.5.2"><apply id="S2.E1.m1.4.4.4.4.4.cmml" xref="S2.E1.m1.4.4.4.4.4"><eq id="S2.E1.m1.4.4.4.4.4.5.cmml" xref="S2.E1.m1.4.4.4.4.4.5"></eq><apply id="S2.E1.m1.4.4.4.4.4.4.cmml" xref="S2.E1.m1.4.4.4.4.4.4"><times id="S2.E1.m1.4.4.4.4.4.4.2.cmml" xref="S2.E1.m1.4.4.4.4.4.4.2"></times><ci id="S2.E1.m1.4.4.4.4.4.4.3.cmml" xref="S2.E1.m1.4.4.4.4.4.4.3">𝑑</ci><interval closure="open" id="S2.E1.m1.4.4.4.4.4.4.1.2.cmml" xref="S2.E1.m1.4.4.4.4.4.4.1.1"><apply id="S2.E1.m1.4.4.4.4.4.4.1.1.1.cmml" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.4.4.4.1.1.1.1.cmml" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1">subscript</csymbol><ci id="S2.E1.m1.4.4.4.4.4.4.1.1.1.2.cmml" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.2">𝑅</ci><apply id="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.cmml" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.3"><times id="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.1"></times><ci id="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.2.cmml" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.2">𝑔</ci><ci id="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.3.cmml" xref="S2.E1.m1.4.4.4.4.4.4.1.1.1.3.3">𝑡</ci></apply></apply><ci id="S2.E1.m1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2">𝑅</ci></interval></apply><apply id="S2.E1.m1.4.4.4.4.4.6.cmml" xref="S2.E1.m1.4.4.4.4.4.6"><divide id="S2.E1.m1.4.4.4.4.4.6.1.cmml" xref="S2.E1.m1.4.4.4.4.4.6.1"></divide><apply id="S2.E1.m1.4.4.4.4.4.6.2.1.cmml" xref="S2.E1.m1.4.4.4.4.4.6.2.2"><arccos id="S2.E1.m1.3.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3"></arccos><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1"><divide id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1"></divide><apply id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1"><minus id="S2.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.2"></minus><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3">𝑡</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.4">𝑟</ci><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑅</ci><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3"><times id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1"></times><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2">𝑔</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3">𝑡</ci></apply></apply><csymbol cd="latexml" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3">top</csymbol></apply><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3">𝑅</ci></apply></apply><cn id="S2.E1.m1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.1.1.1.3">1</cn></apply><cn id="S2.E1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.1.1.3">2</cn></apply></apply><ci id="S2.E1.m1.4.4.4.4.4.6.3.cmml" xref="S2.E1.m1.4.4.4.4.4.6.3">𝜋</ci></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">\begin{array}[]{l}d\left({R_{gt}},R\right)=\arccos\left(\frac{{tr}\left({R_{gt%
}}^{\top}R\right)-1}{2}\right)/\pi\end{array},\vspace{-0.5em}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.4d">start_ARRAY start_ROW start_CELL italic_d ( italic_R start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT , italic_R ) = roman_arccos ( divide start_ARG italic_t italic_r ( italic_R start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_R ) - 1 end_ARG start_ARG 2 end_ARG ) / italic_π end_CELL end_ROW end_ARRAY ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.SSS1.p1.4">where <math alttext="{R_{gt}}" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.1.m1.1"><semantics id="S2.SS4.SSS1.p1.1.m1.1a"><msub id="S2.SS4.SSS1.p1.1.m1.1.1" xref="S2.SS4.SSS1.p1.1.m1.1.1.cmml"><mi id="S2.SS4.SSS1.p1.1.m1.1.1.2" xref="S2.SS4.SSS1.p1.1.m1.1.1.2.cmml">R</mi><mrow id="S2.SS4.SSS1.p1.1.m1.1.1.3" xref="S2.SS4.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S2.SS4.SSS1.p1.1.m1.1.1.3.2" xref="S2.SS4.SSS1.p1.1.m1.1.1.3.2.cmml">g</mi><mo id="S2.SS4.SSS1.p1.1.m1.1.1.3.1" xref="S2.SS4.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS4.SSS1.p1.1.m1.1.1.3.3" xref="S2.SS4.SSS1.p1.1.m1.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.1.m1.1b"><apply id="S2.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1.2">𝑅</ci><apply id="S2.SS4.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1.3"><times id="S2.SS4.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1.3.1"></times><ci id="S2.SS4.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1.3.2">𝑔</ci><ci id="S2.SS4.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.1.m1.1c">{R_{gt}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.1.m1.1d">italic_R start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="R" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.2.m2.1"><semantics id="S2.SS4.SSS1.p1.2.m2.1a"><mi id="S2.SS4.SSS1.p1.2.m2.1.1" xref="S2.SS4.SSS1.p1.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.2.m2.1b"><ci id="S2.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS1.p1.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.2.m2.1c">R</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.2.m2.1d">italic_R</annotation></semantics></math> denote the ground-truth and predicted 3D rotations, respectively. <math alttext="\top" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.3.m3.1"><semantics id="S2.SS4.SSS1.p1.3.m3.1a"><mo id="S2.SS4.SSS1.p1.3.m3.1.1" xref="S2.SS4.SSS1.p1.3.m3.1.1.cmml">⊤</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.3.m3.1b"><csymbol cd="latexml" id="S2.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1">top</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.3.m3.1c">\top</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.3.m3.1d">⊤</annotation></semantics></math> represents matrix transpose. <math alttext="tr" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.4.m4.1"><semantics id="S2.SS4.SSS1.p1.4.m4.1a"><mrow id="S2.SS4.SSS1.p1.4.m4.1.1" xref="S2.SS4.SSS1.p1.4.m4.1.1.cmml"><mi id="S2.SS4.SSS1.p1.4.m4.1.1.2" xref="S2.SS4.SSS1.p1.4.m4.1.1.2.cmml">t</mi><mo id="S2.SS4.SSS1.p1.4.m4.1.1.1" xref="S2.SS4.SSS1.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S2.SS4.SSS1.p1.4.m4.1.1.3" xref="S2.SS4.SSS1.p1.4.m4.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.4.m4.1b"><apply id="S2.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS4.SSS1.p1.4.m4.1.1"><times id="S2.SS4.SSS1.p1.4.m4.1.1.1.cmml" xref="S2.SS4.SSS1.p1.4.m4.1.1.1"></times><ci id="S2.SS4.SSS1.p1.4.m4.1.1.2.cmml" xref="S2.SS4.SSS1.p1.4.m4.1.1.2">𝑡</ci><ci id="S2.SS4.SSS1.p1.4.m4.1.1.3.cmml" xref="S2.SS4.SSS1.p1.4.m4.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.4.m4.1c">tr</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.4.m4.1d">italic_t italic_r</annotation></semantics></math> denotes the trace of a matrix, which refers to the sum of the elements on the main diagonal. Typically, 3D rotation estimation accuracy is defined as the percentage of objects whose angle error is below a specific threshold and whose predicted class is correct. It can be expressed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Acc.=\left\{{\begin{array}[]{*{20}{c}}{1,}\\
{0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20%
}{c}}{d({R_{gt}},R)&lt;\lambda\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}c=c%
_{gt}}\end{array}}\\
{{\rm{otherwise}}}\end{array}," class="ltx_math_unparsed" display="block" id="S2.E2.m1.4"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4b"><mi id="S2.E2.m1.4.5">A</mi><mi id="S2.E2.m1.4.6">c</mi><mi id="S2.E2.m1.4.7">c</mi><mo id="S2.E2.m1.4.8" lspace="0em" rspace="0.0835em">.</mo><mo id="S2.E2.m1.4.9" lspace="0.0835em">=</mo><mrow id="S2.E2.m1.4.10"><mo id="S2.E2.m1.4.10.1">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E2.m1.2.2" rowspacing="0pt"><mtr id="S2.E2.m1.2.2a"><mtd id="S2.E2.m1.2.2b"><mrow id="S2.E2.m1.1.1.1.1.1.3"><mn id="S2.E2.m1.1.1.1.1.1.1">1</mn><mo id="S2.E2.m1.1.1.1.1.1.3.1">,</mo></mrow></mtd><mtd id="S2.E2.m1.2.2c"></mtd><mtd id="S2.E2.m1.2.2d"></mtd><mtd id="S2.E2.m1.2.2e"></mtd><mtd id="S2.E2.m1.2.2f"></mtd><mtd id="S2.E2.m1.2.2g"></mtd><mtd id="S2.E2.m1.2.2h"></mtd><mtd id="S2.E2.m1.2.2i"></mtd><mtd id="S2.E2.m1.2.2j"></mtd><mtd id="S2.E2.m1.2.2k"></mtd><mtd id="S2.E2.m1.2.2l"></mtd><mtd id="S2.E2.m1.2.2m"></mtd><mtd id="S2.E2.m1.2.2n"></mtd><mtd id="S2.E2.m1.2.2o"></mtd><mtd id="S2.E2.m1.2.2p"></mtd><mtd id="S2.E2.m1.2.2q"></mtd><mtd id="S2.E2.m1.2.2r"></mtd><mtd id="S2.E2.m1.2.2s"></mtd><mtd id="S2.E2.m1.2.2t"></mtd><mtd id="S2.E2.m1.2.2u"></mtd></mtr><mtr id="S2.E2.m1.2.2v"><mtd id="S2.E2.m1.2.2w"><mrow id="S2.E2.m1.2.2.2.1.1.3"><mn id="S2.E2.m1.2.2.2.1.1.1">0</mn><mo id="S2.E2.m1.2.2.2.1.1.3.1">,</mo></mrow></mtd><mtd id="S2.E2.m1.2.2x"></mtd><mtd id="S2.E2.m1.2.2y"></mtd><mtd id="S2.E2.m1.2.2z"></mtd><mtd id="S2.E2.m1.2.2aa"></mtd><mtd id="S2.E2.m1.2.2ab"></mtd><mtd id="S2.E2.m1.2.2ac"></mtd><mtd id="S2.E2.m1.2.2ad"></mtd><mtd id="S2.E2.m1.2.2ae"></mtd><mtd id="S2.E2.m1.2.2af"></mtd><mtd id="S2.E2.m1.2.2ag"></mtd><mtd id="S2.E2.m1.2.2ah"></mtd><mtd id="S2.E2.m1.2.2ai"></mtd><mtd id="S2.E2.m1.2.2aj"></mtd><mtd id="S2.E2.m1.2.2ak"></mtd><mtd id="S2.E2.m1.2.2al"></mtd><mtd id="S2.E2.m1.2.2am"></mtd><mtd id="S2.E2.m1.2.2an"></mtd><mtd id="S2.E2.m1.2.2ao"></mtd><mtd id="S2.E2.m1.2.2ap"></mtd></mtr></mtable><mtable columnspacing="5pt" displaystyle="true" id="S2.E2.m1.4.4" rowspacing="0pt"><mtr id="S2.E2.m1.4.4a"><mtd id="S2.E2.m1.4.4b"><mrow id="S2.E2.m1.4.4.2.2.2"><mi id="S2.E2.m1.4.4.2.2.2.4">if</mi><mo id="S2.E2.m1.4.4.2.2.2.3" lspace="0.167em">⁢</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E2.m1.4.4.2.2.2.2"><mtr id="S2.E2.m1.4.4.2.2.2.2a"><mtd id="S2.E2.m1.4.4.2.2.2.2b"><mrow id="S2.E2.m1.4.4.2.2.2.2.2.2.2"><mrow id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2"><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.3">d</mi><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.2">⁢</mo><mrow id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1"><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.2" stretchy="false">(</mo><msub id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1"><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1.2">R</mi><mrow id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1.3"><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1.3.2">g</mi><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1.3.1">⁢</mo><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1.3.3">t</mi></mrow></msub><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.3">,</mo><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1">R</mi><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.4" stretchy="false">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.4">&lt;</mo><mrow id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5"><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.2">λ</mi><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.1" lspace="0.167em">⁢</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3"><mtr id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3a"><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3b"><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3.1.1.1">and</mi></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3c"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3d"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3e"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3f"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3g"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3h"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3i"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3j"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3k"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3l"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3m"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3n"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3o"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3p"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3q"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3r"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3s"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3t"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3u"></mtd></mtr></mtable><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.1a" lspace="0.167em">⁢</mo><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.4">c</mi></mrow><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.6">=</mo><msub id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7"><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7.2">c</mi><mrow id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7.3"><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7.3.2">g</mi><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7.3.1">⁢</mo><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7.3.3">t</mi></mrow></msub></mrow></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2c"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2d"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2e"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2f"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2g"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2h"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2i"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2j"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2k"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2l"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2m"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2n"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2o"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2p"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2q"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2r"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2s"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2t"></mtd><mtd id="S2.E2.m1.4.4.2.2.2.2u"></mtd></mtr></mtable></mrow></mtd><mtd id="S2.E2.m1.4.4c"></mtd><mtd id="S2.E2.m1.4.4d"></mtd><mtd id="S2.E2.m1.4.4e"></mtd><mtd id="S2.E2.m1.4.4f"></mtd><mtd id="S2.E2.m1.4.4g"></mtd><mtd id="S2.E2.m1.4.4h"></mtd><mtd id="S2.E2.m1.4.4i"></mtd><mtd id="S2.E2.m1.4.4j"></mtd><mtd id="S2.E2.m1.4.4k"></mtd><mtd id="S2.E2.m1.4.4l"></mtd><mtd id="S2.E2.m1.4.4m"></mtd><mtd id="S2.E2.m1.4.4n"></mtd><mtd id="S2.E2.m1.4.4o"></mtd><mtd id="S2.E2.m1.4.4p"></mtd><mtd id="S2.E2.m1.4.4q"></mtd><mtd id="S2.E2.m1.4.4r"></mtd><mtd id="S2.E2.m1.4.4s"></mtd><mtd id="S2.E2.m1.4.4t"></mtd><mtd id="S2.E2.m1.4.4u"></mtd></mtr><mtr id="S2.E2.m1.4.4v"><mtd id="S2.E2.m1.4.4w"><mi id="S2.E2.m1.4.4.3.1.1">otherwise</mi></mtd><mtd id="S2.E2.m1.4.4x"></mtd><mtd id="S2.E2.m1.4.4y"></mtd><mtd id="S2.E2.m1.4.4z"></mtd><mtd id="S2.E2.m1.4.4aa"></mtd><mtd id="S2.E2.m1.4.4ab"></mtd><mtd id="S2.E2.m1.4.4ac"></mtd><mtd id="S2.E2.m1.4.4ad"></mtd><mtd id="S2.E2.m1.4.4ae"></mtd><mtd id="S2.E2.m1.4.4af"></mtd><mtd id="S2.E2.m1.4.4ag"></mtd><mtd id="S2.E2.m1.4.4ah"></mtd><mtd id="S2.E2.m1.4.4ai"></mtd><mtd id="S2.E2.m1.4.4aj"></mtd><mtd id="S2.E2.m1.4.4ak"></mtd><mtd id="S2.E2.m1.4.4al"></mtd><mtd id="S2.E2.m1.4.4am"></mtd><mtd id="S2.E2.m1.4.4an"></mtd><mtd id="S2.E2.m1.4.4ao"></mtd><mtd id="S2.E2.m1.4.4ap"></mtd></mtr></mtable><mo id="S2.E2.m1.4.10.2" lspace="0.167em">,</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.E2.m1.4c">Acc.=\left\{{\begin{array}[]{*{20}{c}}{1,}\\
{0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20%
}{c}}{d({R_{gt}},R)&lt;\lambda\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}c=c%
_{gt}}\end{array}}\\
{{\rm{otherwise}}}\end{array},</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.4d">italic_A italic_c italic_c . = { start_ARRAY start_ROW start_CELL 1 , end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL 0 , end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY start_ARRAY start_ROW start_CELL roman_if start_ARRAY start_ROW start_CELL italic_d ( italic_R start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT , italic_R ) &lt; italic_λ start_ARRAY start_ROW start_CELL roman_and end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY italic_c = italic_c start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL roman_otherwise end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.SSS1.p1.7">where <math alttext="c" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.5.m1.1"><semantics id="S2.SS4.SSS1.p1.5.m1.1a"><mi id="S2.SS4.SSS1.p1.5.m1.1.1" xref="S2.SS4.SSS1.p1.5.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.5.m1.1b"><ci id="S2.SS4.SSS1.p1.5.m1.1.1.cmml" xref="S2.SS4.SSS1.p1.5.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.5.m1.1c">c</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.5.m1.1d">italic_c</annotation></semantics></math>, <math alttext="c_{gt}" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.6.m2.1"><semantics id="S2.SS4.SSS1.p1.6.m2.1a"><msub id="S2.SS4.SSS1.p1.6.m2.1.1" xref="S2.SS4.SSS1.p1.6.m2.1.1.cmml"><mi id="S2.SS4.SSS1.p1.6.m2.1.1.2" xref="S2.SS4.SSS1.p1.6.m2.1.1.2.cmml">c</mi><mrow id="S2.SS4.SSS1.p1.6.m2.1.1.3" xref="S2.SS4.SSS1.p1.6.m2.1.1.3.cmml"><mi id="S2.SS4.SSS1.p1.6.m2.1.1.3.2" xref="S2.SS4.SSS1.p1.6.m2.1.1.3.2.cmml">g</mi><mo id="S2.SS4.SSS1.p1.6.m2.1.1.3.1" xref="S2.SS4.SSS1.p1.6.m2.1.1.3.1.cmml">⁢</mo><mi id="S2.SS4.SSS1.p1.6.m2.1.1.3.3" xref="S2.SS4.SSS1.p1.6.m2.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.6.m2.1b"><apply id="S2.SS4.SSS1.p1.6.m2.1.1.cmml" xref="S2.SS4.SSS1.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS1.p1.6.m2.1.1.1.cmml" xref="S2.SS4.SSS1.p1.6.m2.1.1">subscript</csymbol><ci id="S2.SS4.SSS1.p1.6.m2.1.1.2.cmml" xref="S2.SS4.SSS1.p1.6.m2.1.1.2">𝑐</ci><apply id="S2.SS4.SSS1.p1.6.m2.1.1.3.cmml" xref="S2.SS4.SSS1.p1.6.m2.1.1.3"><times id="S2.SS4.SSS1.p1.6.m2.1.1.3.1.cmml" xref="S2.SS4.SSS1.p1.6.m2.1.1.3.1"></times><ci id="S2.SS4.SSS1.p1.6.m2.1.1.3.2.cmml" xref="S2.SS4.SSS1.p1.6.m2.1.1.3.2">𝑔</ci><ci id="S2.SS4.SSS1.p1.6.m2.1.1.3.3.cmml" xref="S2.SS4.SSS1.p1.6.m2.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.6.m2.1c">c_{gt}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.6.m2.1d">italic_c start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\lambda" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.7.m3.1"><semantics id="S2.SS4.SSS1.p1.7.m3.1a"><mi id="S2.SS4.SSS1.p1.7.m3.1.1" xref="S2.SS4.SSS1.p1.7.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.7.m3.1b"><ci id="S2.SS4.SSS1.p1.7.m3.1.1.cmml" xref="S2.SS4.SSS1.p1.7.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.7.m3.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.7.m3.1d">italic_λ</annotation></semantics></math> denote the predicted class, ground-truth class and predefined threshold, respectively.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>6DoF Evaluation Metrics</h4>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.22">Currently, the BOP metric (<em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS2.p1.22.1">BOP-M</em>)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib36" title="">36</a>]</cite> is the most popular metric, which is the Average Recall (<em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS2.p1.22.2">AR</em>) of the Visible Surface Discrepancy (VSD), Maximum Symmetry-Aware Surface Distance (MSSD), and Maximum Symmetry-Aware Projection Distance (MSPD) metrics. Specifically, the <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS2.p1.22.3">VSD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib36" title="">36</a>]</cite></span> metric treats poses that are indistinguishable in shape as equivalent by only measuring the misalignment of the visible object surface. It can be expressed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{array}[]{l}{e_{VSD}}\left({\hat{D},\bar{D},\hat{V},\bar{V},\tau}\right)%
=\\
av{g_{p\in\hat{V}\cup\bar{V}}}\left\{{\begin{array}[]{*{20}{l}}{0,}\\
{1,}\end{array}}\right.\begin{array}[]{*{20}{l}}{{\rm{if}}\begin{array}[]{*{20%
}{l}}{p\in\hat{V}\cap\bar{V}\wedge|\hat{D}\left(p\right)-\bar{D}\left(p\right)%
|&lt;\tau}\end{array}}\\
{{\rm{otherwise}}}\end{array},\end{array}\vspace{-0.5em}" class="ltx_Math" display="block" id="S2.E3.m1.11"><semantics id="S2.E3.m1.11a"><mtable displaystyle="true" id="S2.E3.m1.11.11" rowspacing="0pt" xref="S2.E3.m1.11.11.cmml"><mtr id="S2.E3.m1.11.11a" xref="S2.E3.m1.11.11.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.11.11b" xref="S2.E3.m1.11.11.cmml"><mrow id="S2.E3.m1.5.5.5.5.5" xref="S2.E3.m1.5.5.5.5.5.cmml"><mrow id="S2.E3.m1.5.5.5.5.5.7" xref="S2.E3.m1.5.5.5.5.5.7.cmml"><msub id="S2.E3.m1.5.5.5.5.5.7.2" xref="S2.E3.m1.5.5.5.5.5.7.2.cmml"><mi id="S2.E3.m1.5.5.5.5.5.7.2.2" xref="S2.E3.m1.5.5.5.5.5.7.2.2.cmml">e</mi><mrow id="S2.E3.m1.5.5.5.5.5.7.2.3" xref="S2.E3.m1.5.5.5.5.5.7.2.3.cmml"><mi id="S2.E3.m1.5.5.5.5.5.7.2.3.2" xref="S2.E3.m1.5.5.5.5.5.7.2.3.2.cmml">V</mi><mo id="S2.E3.m1.5.5.5.5.5.7.2.3.1" xref="S2.E3.m1.5.5.5.5.5.7.2.3.1.cmml">⁢</mo><mi id="S2.E3.m1.5.5.5.5.5.7.2.3.3" xref="S2.E3.m1.5.5.5.5.5.7.2.3.3.cmml">S</mi><mo id="S2.E3.m1.5.5.5.5.5.7.2.3.1a" xref="S2.E3.m1.5.5.5.5.5.7.2.3.1.cmml">⁢</mo><mi id="S2.E3.m1.5.5.5.5.5.7.2.3.4" xref="S2.E3.m1.5.5.5.5.5.7.2.3.4.cmml">D</mi></mrow></msub><mo id="S2.E3.m1.5.5.5.5.5.7.1" xref="S2.E3.m1.5.5.5.5.5.7.1.cmml">⁢</mo><mrow id="S2.E3.m1.5.5.5.5.5.7.3.2" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml"><mo id="S2.E3.m1.5.5.5.5.5.7.3.2.1" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">(</mo><mover accent="true" id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml">D</mi><mo id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.cmml">^</mo></mover><mo id="S2.E3.m1.5.5.5.5.5.7.3.2.2" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">,</mo><mover accent="true" id="S2.E3.m1.2.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.2.cmml"><mi id="S2.E3.m1.2.2.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.2.2.cmml">D</mi><mo id="S2.E3.m1.2.2.2.2.2.2.1" xref="S2.E3.m1.2.2.2.2.2.2.1.cmml">¯</mo></mover><mo id="S2.E3.m1.5.5.5.5.5.7.3.2.3" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">,</mo><mover accent="true" id="S2.E3.m1.3.3.3.3.3.3" xref="S2.E3.m1.3.3.3.3.3.3.cmml"><mi id="S2.E3.m1.3.3.3.3.3.3.2" xref="S2.E3.m1.3.3.3.3.3.3.2.cmml">V</mi><mo id="S2.E3.m1.3.3.3.3.3.3.1" xref="S2.E3.m1.3.3.3.3.3.3.1.cmml">^</mo></mover><mo id="S2.E3.m1.5.5.5.5.5.7.3.2.4" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">,</mo><mover accent="true" id="S2.E3.m1.4.4.4.4.4.4" xref="S2.E3.m1.4.4.4.4.4.4.cmml"><mi id="S2.E3.m1.4.4.4.4.4.4.2" xref="S2.E3.m1.4.4.4.4.4.4.2.cmml">V</mi><mo id="S2.E3.m1.4.4.4.4.4.4.1" xref="S2.E3.m1.4.4.4.4.4.4.1.cmml">¯</mo></mover><mo id="S2.E3.m1.5.5.5.5.5.7.3.2.5" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">,</mo><mi id="S2.E3.m1.5.5.5.5.5.5" xref="S2.E3.m1.5.5.5.5.5.5.cmml">τ</mi><mo id="S2.E3.m1.5.5.5.5.5.7.3.2.6" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.5.5.5.5.5.6" xref="S2.E3.m1.5.5.5.5.5.6.cmml">=</mo><mi id="S2.E3.m1.5.5.5.5.5.8" xref="S2.E3.m1.5.5.5.5.5.8.cmml"></mi></mrow></mtd></mtr><mtr id="S2.E3.m1.11.11c" xref="S2.E3.m1.11.11.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.11.11d" xref="S2.E3.m1.11.11.cmml"><mrow id="S2.E3.m1.11.11.11.6.6.6" xref="S2.E3.m1.11.11.11.6.6.6.1.cmml"><mrow id="S2.E3.m1.11.11.11.6.6.6.1" xref="S2.E3.m1.11.11.11.6.6.6.1.cmml"><mi id="S2.E3.m1.11.11.11.6.6.6.1.2" xref="S2.E3.m1.11.11.11.6.6.6.1.2.cmml">a</mi><mo id="S2.E3.m1.11.11.11.6.6.6.1.1" xref="S2.E3.m1.11.11.11.6.6.6.1.1.cmml">⁢</mo><mi id="S2.E3.m1.11.11.11.6.6.6.1.3" xref="S2.E3.m1.11.11.11.6.6.6.1.3.cmml">v</mi><mo id="S2.E3.m1.11.11.11.6.6.6.1.1a" xref="S2.E3.m1.11.11.11.6.6.6.1.1.cmml">⁢</mo><msub id="S2.E3.m1.11.11.11.6.6.6.1.4" xref="S2.E3.m1.11.11.11.6.6.6.1.4.cmml"><mi id="S2.E3.m1.11.11.11.6.6.6.1.4.2" xref="S2.E3.m1.11.11.11.6.6.6.1.4.2.cmml">g</mi><mrow id="S2.E3.m1.11.11.11.6.6.6.1.4.3" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.cmml"><mi id="S2.E3.m1.11.11.11.6.6.6.1.4.3.2" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.2.cmml">p</mi><mo id="S2.E3.m1.11.11.11.6.6.6.1.4.3.1" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.1.cmml">∈</mo><mrow id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.cmml"><mover accent="true" id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.cmml"><mi id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.2" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.2.cmml">V</mi><mo id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.1" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.1.cmml">^</mo></mover><mo id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.1" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.1.cmml">∪</mo><mover accent="true" id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.cmml"><mi id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.2" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.2.cmml">V</mi><mo id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.1" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.1.cmml">¯</mo></mover></mrow></mrow></msub><mo id="S2.E3.m1.11.11.11.6.6.6.1.1b" xref="S2.E3.m1.11.11.11.6.6.6.1.1.cmml">⁢</mo><mrow id="S2.E3.m1.11.11.11.6.6.6.1.5.2" xref="S2.E3.m1.11.11.11.6.6.6.1.5.1.cmml"><mo id="S2.E3.m1.11.11.11.6.6.6.1.5.2.1" xref="S2.E3.m1.11.11.11.6.6.6.1.5.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E3.m1.7.7.7.2.2.2" rowspacing="0pt" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mtr id="S2.E3.m1.7.7.7.2.2.2a" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.7.7.7.2.2.2b" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mrow id="S2.E3.m1.6.6.6.1.1.1.1.1.1.3" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mn id="S2.E3.m1.6.6.6.1.1.1.1.1.1.1" xref="S2.E3.m1.6.6.6.1.1.1.1.1.1.1.cmml">0</mn><mo id="S2.E3.m1.6.6.6.1.1.1.1.1.1.3.1" xref="S2.E3.m1.7.7.7.2.2.2.cmml">,</mo></mrow></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2c" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2d" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2e" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2f" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2g" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2h" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2i" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2j" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2k" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2l" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2m" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2n" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2o" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2p" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2q" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2r" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2s" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2t" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2u" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd></mtr><mtr id="S2.E3.m1.7.7.7.2.2.2v" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.7.7.7.2.2.2w" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mrow id="S2.E3.m1.7.7.7.2.2.2.2.1.1.3" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mn id="S2.E3.m1.7.7.7.2.2.2.2.1.1.1" xref="S2.E3.m1.7.7.7.2.2.2.2.1.1.1.cmml">1</mn><mo id="S2.E3.m1.7.7.7.2.2.2.2.1.1.3.1" xref="S2.E3.m1.7.7.7.2.2.2.cmml">,</mo></mrow></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2x" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2y" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2z" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2aa" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2ab" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2ac" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2ad" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2ae" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2af" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2ag" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2ah" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2ai" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2aj" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2ak" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2al" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2am" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2an" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2ao" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd><mtd id="S2.E3.m1.7.7.7.2.2.2ap" xref="S2.E3.m1.7.7.7.2.2.2.cmml"></mtd></mtr></mtable><mi id="S2.E3.m1.11.11.11.6.6.6.1.5.2.2" xref="S2.E3.m1.11.11.11.6.6.6.1.5.1.1.cmml"></mi></mrow><mo id="S2.E3.m1.11.11.11.6.6.6.1.1c" lspace="0.167em" xref="S2.E3.m1.11.11.11.6.6.6.1.1.cmml">⁢</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E3.m1.10.10.10.5.5.5" rowspacing="0pt" xref="S2.E3.m1.10.10.10.5.5.5.cmml"><mtr id="S2.E3.m1.10.10.10.5.5.5a" xref="S2.E3.m1.10.10.10.5.5.5.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.10.10.10.5.5.5b" xref="S2.E3.m1.10.10.10.5.5.5.cmml"><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.cmml"><mi id="S2.E3.m1.10.10.10.5.5.5.3.3.3.5" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.5.cmml">if</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.4" lspace="0.167em" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.4.cmml">⁢</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"><mtr id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3a" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3b" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.cmml"><mi id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.5" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.5.cmml">p</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.6" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.6.cmml">∈</mo><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.cmml"><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.cmml"><mover accent="true" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.cmml"><mi id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.2.cmml">V</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.1.cmml">^</mo></mover><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.1.cmml">∩</mo><mover accent="true" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.cmml"><mi id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.2.cmml">V</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.1.cmml">¯</mo></mover></mrow><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.2.cmml">∧</mo><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.2.cmml"><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.2" stretchy="false" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.2.1.cmml">|</mo><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.cmml"><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.cmml"><mover accent="true" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.cmml"><mi id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.2.cmml">D</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.1.cmml">^</mo></mover><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.1.cmml">⁢</mo><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.3.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.cmml"><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.3.2.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.cmml">(</mo><mi id="S2.E3.m1.8.8.8.3.3.3.1.1.1.1.1.1.1.1" xref="S2.E3.m1.8.8.8.3.3.3.1.1.1.1.1.1.1.1.cmml">p</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.3.2.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.1.cmml">−</mo><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.cmml"><mover accent="true" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.cmml"><mi id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.2.cmml">D</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.1.cmml">¯</mo></mover><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.1.cmml">⁢</mo><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.3.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.cmml"><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.3.2.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.cmml">(</mo><mi id="S2.E3.m1.9.9.9.4.4.4.2.2.2.2.2.2.2.2" xref="S2.E3.m1.9.9.9.4.4.4.2.2.2.2.2.2.2.2.cmml">p</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.3.2.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.3" stretchy="false" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.2.1.cmml">|</mo></mrow></mrow><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.7" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.7.cmml">&lt;</mo><mi id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.8" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.8.cmml">τ</mi></mrow></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3c" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3d" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3e" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3f" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3g" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3h" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3i" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3j" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3k" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3l" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3m" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3n" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3o" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3p" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3q" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3r" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3s" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3t" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3u" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"></mtd></mtr></mtable></mrow></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5c" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5d" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5e" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5f" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5g" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5h" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5i" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5j" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5k" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5l" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5m" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5n" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5o" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5p" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5q" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5r" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5s" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5t" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5u" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd></mtr><mtr id="S2.E3.m1.10.10.10.5.5.5v" xref="S2.E3.m1.10.10.10.5.5.5.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.10.10.10.5.5.5w" xref="S2.E3.m1.10.10.10.5.5.5.cmml"><mi id="S2.E3.m1.10.10.10.5.5.5.4.1.1" xref="S2.E3.m1.10.10.10.5.5.5.4.1.1.cmml">otherwise</mi></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5x" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5y" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5z" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5aa" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5ab" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5ac" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5ad" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5ae" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5af" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5ag" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5ah" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5ai" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5aj" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5ak" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5al" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5am" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5an" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5ao" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd><mtd id="S2.E3.m1.10.10.10.5.5.5ap" xref="S2.E3.m1.10.10.10.5.5.5.cmml"></mtd></mtr></mtable></mrow><mo id="S2.E3.m1.11.11.11.6.6.6.2" lspace="0.167em" xref="S2.E3.m1.11.11.11.6.6.6.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E3.m1.11b"><matrix id="S2.E3.m1.11.11.cmml" xref="S2.E3.m1.11.11"><matrixrow id="S2.E3.m1.11.11a.cmml" xref="S2.E3.m1.11.11"><apply id="S2.E3.m1.5.5.5.5.5.cmml" xref="S2.E3.m1.5.5.5.5.5"><eq id="S2.E3.m1.5.5.5.5.5.6.cmml" xref="S2.E3.m1.5.5.5.5.5.6"></eq><apply id="S2.E3.m1.5.5.5.5.5.7.cmml" xref="S2.E3.m1.5.5.5.5.5.7"><times id="S2.E3.m1.5.5.5.5.5.7.1.cmml" xref="S2.E3.m1.5.5.5.5.5.7.1"></times><apply id="S2.E3.m1.5.5.5.5.5.7.2.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.5.5.5.7.2.1.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2">subscript</csymbol><ci id="S2.E3.m1.5.5.5.5.5.7.2.2.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2.2">𝑒</ci><apply id="S2.E3.m1.5.5.5.5.5.7.2.3.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2.3"><times id="S2.E3.m1.5.5.5.5.5.7.2.3.1.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2.3.1"></times><ci id="S2.E3.m1.5.5.5.5.5.7.2.3.2.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2.3.2">𝑉</ci><ci id="S2.E3.m1.5.5.5.5.5.7.2.3.3.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2.3.3">𝑆</ci><ci id="S2.E3.m1.5.5.5.5.5.7.2.3.4.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2.3.4">𝐷</ci></apply></apply><vector id="S2.E3.m1.5.5.5.5.5.7.3.1.cmml" xref="S2.E3.m1.5.5.5.5.5.7.3.2"><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1"><ci id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1">^</ci><ci id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">𝐷</ci></apply><apply id="S2.E3.m1.2.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2.2"><ci id="S2.E3.m1.2.2.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.2.2.2.1">¯</ci><ci id="S2.E3.m1.2.2.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2">𝐷</ci></apply><apply id="S2.E3.m1.3.3.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3.3.3"><ci id="S2.E3.m1.3.3.3.3.3.3.1.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1">^</ci><ci id="S2.E3.m1.3.3.3.3.3.3.2.cmml" xref="S2.E3.m1.3.3.3.3.3.3.2">𝑉</ci></apply><apply id="S2.E3.m1.4.4.4.4.4.4.cmml" xref="S2.E3.m1.4.4.4.4.4.4"><ci id="S2.E3.m1.4.4.4.4.4.4.1.cmml" xref="S2.E3.m1.4.4.4.4.4.4.1">¯</ci><ci id="S2.E3.m1.4.4.4.4.4.4.2.cmml" xref="S2.E3.m1.4.4.4.4.4.4.2">𝑉</ci></apply><ci id="S2.E3.m1.5.5.5.5.5.5.cmml" xref="S2.E3.m1.5.5.5.5.5.5">𝜏</ci></vector></apply><csymbol cd="latexml" id="S2.E3.m1.5.5.5.5.5.8.cmml" xref="S2.E3.m1.5.5.5.5.5.8">absent</csymbol></apply></matrixrow><matrixrow id="S2.E3.m1.11.11b.cmml" xref="S2.E3.m1.11.11"><apply id="S2.E3.m1.11.11.11.6.6.6.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6"><times id="S2.E3.m1.11.11.11.6.6.6.1.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.1"></times><ci id="S2.E3.m1.11.11.11.6.6.6.1.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.2">𝑎</ci><ci id="S2.E3.m1.11.11.11.6.6.6.1.3.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.3">𝑣</ci><apply id="S2.E3.m1.11.11.11.6.6.6.1.4.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4"><csymbol cd="ambiguous" id="S2.E3.m1.11.11.11.6.6.6.1.4.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4">subscript</csymbol><ci id="S2.E3.m1.11.11.11.6.6.6.1.4.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.2">𝑔</ci><apply id="S2.E3.m1.11.11.11.6.6.6.1.4.3.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3"><in id="S2.E3.m1.11.11.11.6.6.6.1.4.3.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.1"></in><ci id="S2.E3.m1.11.11.11.6.6.6.1.4.3.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.2">𝑝</ci><apply id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3"><union id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.1"></union><apply id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2"><ci id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.1">^</ci><ci id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.2">𝑉</ci></apply><apply id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3"><ci id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.1">¯</ci><ci id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.2">𝑉</ci></apply></apply></apply></apply><apply id="S2.E3.m1.11.11.11.6.6.6.1.5.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.5.2"><csymbol cd="latexml" id="S2.E3.m1.11.11.11.6.6.6.1.5.1.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.5.2.1">cases</csymbol><matrix id="S2.E3.m1.7.7.7.2.2.2.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><matrixrow id="S2.E3.m1.7.7.7.2.2.2a.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><cn id="S2.E3.m1.6.6.6.1.1.1.1.1.1.1.cmml" type="integer" xref="S2.E3.m1.6.6.6.1.1.1.1.1.1.1">0</cn><cerror id="S2.E3.m1.7.7.7.2.2.2b.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2c.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2d.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2e.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2f.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2g.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2h.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2i.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2j.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2k.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2l.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2m.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2n.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2o.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2p.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2q.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2r.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2s.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2t.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2u.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2v.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2w.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2x.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2y.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2z.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2aa.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2ab.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ac.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2ad.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ae.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2af.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ag.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2ah.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ai.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2aj.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ak.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2al.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2am.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror></matrixrow><matrixrow id="S2.E3.m1.7.7.7.2.2.2an.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><cn id="S2.E3.m1.7.7.7.2.2.2.2.1.1.1.cmml" type="integer" xref="S2.E3.m1.7.7.7.2.2.2.2.1.1.1">1</cn><cerror id="S2.E3.m1.7.7.7.2.2.2ao.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ap.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2aq.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ar.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2as.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2at.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2au.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2av.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2aw.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ax.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2ay.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2az.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2ba.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bb.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2bc.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bd.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2be.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bf.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2bg.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bh.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2bi.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bj.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2bk.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bl.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2bm.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bn.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2bo.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bp.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2bq.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2br.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2bs.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bt.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2bu.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bv.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2bw.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bx.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2by.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2bz.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><matrix id="S2.E3.m1.10.10.10.5.5.5.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><matrixrow id="S2.E3.m1.10.10.10.5.5.5a.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3"><times id="S2.E3.m1.10.10.10.5.5.5.3.3.3.4.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.4"></times><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.5.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.5">if</ci><matrix id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><matrixrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3a.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3"><and id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3a.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3"></and><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3b.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3"><in id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.6.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.6"></in><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.5.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.5">𝑝</ci><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3"><and id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.2"></and><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3"><intersect id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.1"></intersect><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2"><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.1">^</ci><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.2">𝑉</ci></apply><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3"><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.1">¯</ci><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.2">𝑉</ci></apply></apply><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1"><abs id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.2.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.2"></abs><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1"><minus id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.1"></minus><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2"><times id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.1"></times><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2"><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.1">^</ci><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.2">𝐷</ci></apply><ci id="S2.E3.m1.8.8.8.3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.8.8.8.3.3.3.1.1.1.1.1.1.1.1">𝑝</ci></apply><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3"><times id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.1"></times><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2"><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.1">¯</ci><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.2">𝐷</ci></apply><ci id="S2.E3.m1.9.9.9.4.4.4.2.2.2.2.2.2.2.2.cmml" xref="S2.E3.m1.9.9.9.4.4.4.2.2.2.2.2.2.2.2">𝑝</ci></apply></apply></apply></apply></apply><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3c.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3"><lt id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.7.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.7"></lt><share href="https://arxiv.org/html/2405.07801v3#S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.cmml" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3d.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3"></share><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.8.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.8">𝜏</ci></apply></apply><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3b.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3c.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3d.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3e.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3f.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3g.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3h.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3i.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3j.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3k.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3l.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3m.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3n.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3o.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3p.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3q.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3r.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3s.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3t.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3u.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3v.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3w.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3x.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3y.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3z.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3aa.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ab.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ac.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ad.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ae.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3af.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ag.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ah.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ai.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3aj.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ak.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3al.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3am.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><cerror id="S2.E3.m1.10.10.10.5.5.5b.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5c.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5d.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5e.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5f.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5g.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5h.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5i.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5j.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5k.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5l.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5m.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5n.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5o.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5p.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5q.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5r.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5s.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5t.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5u.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5v.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5w.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5x.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5y.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5z.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5aa.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5ab.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5ac.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5ad.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5ae.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5af.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5ag.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5ah.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5ai.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5aj.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5ak.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5al.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5am.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror></matrixrow><matrixrow id="S2.E3.m1.10.10.10.5.5.5an.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><ci id="S2.E3.m1.10.10.10.5.5.5.4.1.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.4.1.1">otherwise</ci><cerror id="S2.E3.m1.10.10.10.5.5.5ao.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5ap.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5aq.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5ar.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5as.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5at.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5au.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5av.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5aw.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5ax.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5ay.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5az.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5ba.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bb.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5bc.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bd.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5be.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bf.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5bg.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bh.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5bi.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bj.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5bk.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bl.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5bm.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bn.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5bo.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bp.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5bq.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5br.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5bs.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bt.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5bu.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bv.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5bw.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bx.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.10.10.10.5.5.5by.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5bz.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.11c">\begin{array}[]{l}{e_{VSD}}\left({\hat{D},\bar{D},\hat{V},\bar{V},\tau}\right)%
=\\
av{g_{p\in\hat{V}\cup\bar{V}}}\left\{{\begin{array}[]{*{20}{l}}{0,}\\
{1,}\end{array}}\right.\begin{array}[]{*{20}{l}}{{\rm{if}}\begin{array}[]{*{20%
}{l}}{p\in\hat{V}\cap\bar{V}\wedge|\hat{D}\left(p\right)-\bar{D}\left(p\right)%
|&lt;\tau}\end{array}}\\
{{\rm{otherwise}}}\end{array},\end{array}\vspace{-0.5em}</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.11d">start_ARRAY start_ROW start_CELL italic_e start_POSTSUBSCRIPT italic_V italic_S italic_D end_POSTSUBSCRIPT ( over^ start_ARG italic_D end_ARG , over¯ start_ARG italic_D end_ARG , over^ start_ARG italic_V end_ARG , over¯ start_ARG italic_V end_ARG , italic_τ ) = end_CELL end_ROW start_ROW start_CELL italic_a italic_v italic_g start_POSTSUBSCRIPT italic_p ∈ over^ start_ARG italic_V end_ARG ∪ over¯ start_ARG italic_V end_ARG end_POSTSUBSCRIPT { start_ARRAY start_ROW start_CELL 0 , end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL 1 , end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY start_ARRAY start_ROW start_CELL roman_if start_ARRAY start_ROW start_CELL italic_p ∈ over^ start_ARG italic_V end_ARG ∩ over¯ start_ARG italic_V end_ARG ∧ | over^ start_ARG italic_D end_ARG ( italic_p ) - over¯ start_ARG italic_D end_ARG ( italic_p ) | &lt; italic_τ end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL roman_otherwise end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY , end_CELL end_ROW end_ARRAY</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.SSS2.p1.17">where the symbols <math alttext="\hat{D}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.1.m1.1"><semantics id="S2.SS4.SSS2.p1.1.m1.1a"><mover accent="true" id="S2.SS4.SSS2.p1.1.m1.1.1" xref="S2.SS4.SSS2.p1.1.m1.1.1.cmml"><mi id="S2.SS4.SSS2.p1.1.m1.1.1.2" xref="S2.SS4.SSS2.p1.1.m1.1.1.2.cmml">D</mi><mo id="S2.SS4.SSS2.p1.1.m1.1.1.1" xref="S2.SS4.SSS2.p1.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.1.m1.1b"><apply id="S2.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.1.1"><ci id="S2.SS4.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.1.1.1">^</ci><ci id="S2.SS4.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.1.1.2">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.1.m1.1c">\hat{D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.1.m1.1d">over^ start_ARG italic_D end_ARG</annotation></semantics></math> and <math alttext="\bar{D}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.2.m2.1"><semantics id="S2.SS4.SSS2.p1.2.m2.1a"><mover accent="true" id="S2.SS4.SSS2.p1.2.m2.1.1" xref="S2.SS4.SSS2.p1.2.m2.1.1.cmml"><mi id="S2.SS4.SSS2.p1.2.m2.1.1.2" xref="S2.SS4.SSS2.p1.2.m2.1.1.2.cmml">D</mi><mo id="S2.SS4.SSS2.p1.2.m2.1.1.1" xref="S2.SS4.SSS2.p1.2.m2.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.2.m2.1b"><apply id="S2.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.1.1"><ci id="S2.SS4.SSS2.p1.2.m2.1.1.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.1.1.1">¯</ci><ci id="S2.SS4.SSS2.p1.2.m2.1.1.2.cmml" xref="S2.SS4.SSS2.p1.2.m2.1.1.2">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.2.m2.1c">\bar{D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.2.m2.1d">over¯ start_ARG italic_D end_ARG</annotation></semantics></math> represent distance maps generated by rendering the object model <math alttext="M" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.3.m3.1"><semantics id="S2.SS4.SSS2.p1.3.m3.1a"><mi id="S2.SS4.SSS2.p1.3.m3.1.1" xref="S2.SS4.SSS2.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.3.m3.1b"><ci id="S2.SS4.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS4.SSS2.p1.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.3.m3.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.3.m3.1d">italic_M</annotation></semantics></math> in two different poses: <math alttext="\hat{P}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.4.m4.1"><semantics id="S2.SS4.SSS2.p1.4.m4.1a"><mover accent="true" id="S2.SS4.SSS2.p1.4.m4.1.1" xref="S2.SS4.SSS2.p1.4.m4.1.1.cmml"><mi id="S2.SS4.SSS2.p1.4.m4.1.1.2" xref="S2.SS4.SSS2.p1.4.m4.1.1.2.cmml">P</mi><mo id="S2.SS4.SSS2.p1.4.m4.1.1.1" xref="S2.SS4.SSS2.p1.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.4.m4.1b"><apply id="S2.SS4.SSS2.p1.4.m4.1.1.cmml" xref="S2.SS4.SSS2.p1.4.m4.1.1"><ci id="S2.SS4.SSS2.p1.4.m4.1.1.1.cmml" xref="S2.SS4.SSS2.p1.4.m4.1.1.1">^</ci><ci id="S2.SS4.SSS2.p1.4.m4.1.1.2.cmml" xref="S2.SS4.SSS2.p1.4.m4.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.4.m4.1c">\hat{P}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.4.m4.1d">over^ start_ARG italic_P end_ARG</annotation></semantics></math> (an estimated pose) and <math alttext="\bar{P}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.5.m5.1"><semantics id="S2.SS4.SSS2.p1.5.m5.1a"><mover accent="true" id="S2.SS4.SSS2.p1.5.m5.1.1" xref="S2.SS4.SSS2.p1.5.m5.1.1.cmml"><mi id="S2.SS4.SSS2.p1.5.m5.1.1.2" xref="S2.SS4.SSS2.p1.5.m5.1.1.2.cmml">P</mi><mo id="S2.SS4.SSS2.p1.5.m5.1.1.1" xref="S2.SS4.SSS2.p1.5.m5.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.5.m5.1b"><apply id="S2.SS4.SSS2.p1.5.m5.1.1.cmml" xref="S2.SS4.SSS2.p1.5.m5.1.1"><ci id="S2.SS4.SSS2.p1.5.m5.1.1.1.cmml" xref="S2.SS4.SSS2.p1.5.m5.1.1.1">¯</ci><ci id="S2.SS4.SSS2.p1.5.m5.1.1.2.cmml" xref="S2.SS4.SSS2.p1.5.m5.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.5.m5.1c">\bar{P}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.5.m5.1d">over¯ start_ARG italic_P end_ARG</annotation></semantics></math> (the ground-truth pose), respectively. In these maps, each pixel <math alttext="p" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.6.m6.1"><semantics id="S2.SS4.SSS2.p1.6.m6.1a"><mi id="S2.SS4.SSS2.p1.6.m6.1.1" xref="S2.SS4.SSS2.p1.6.m6.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.6.m6.1b"><ci id="S2.SS4.SSS2.p1.6.m6.1.1.cmml" xref="S2.SS4.SSS2.p1.6.m6.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.6.m6.1c">p</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.6.m6.1d">italic_p</annotation></semantics></math> stores the distance from the camera center to a 3D point <math alttext="{{\rm{x}}_{p}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.7.m7.1"><semantics id="S2.SS4.SSS2.p1.7.m7.1a"><msub id="S2.SS4.SSS2.p1.7.m7.1.1" xref="S2.SS4.SSS2.p1.7.m7.1.1.cmml"><mi id="S2.SS4.SSS2.p1.7.m7.1.1.2" mathvariant="normal" xref="S2.SS4.SSS2.p1.7.m7.1.1.2.cmml">x</mi><mi id="S2.SS4.SSS2.p1.7.m7.1.1.3" xref="S2.SS4.SSS2.p1.7.m7.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.7.m7.1b"><apply id="S2.SS4.SSS2.p1.7.m7.1.1.cmml" xref="S2.SS4.SSS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.7.m7.1.1.1.cmml" xref="S2.SS4.SSS2.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p1.7.m7.1.1.2.cmml" xref="S2.SS4.SSS2.p1.7.m7.1.1.2">x</ci><ci id="S2.SS4.SSS2.p1.7.m7.1.1.3.cmml" xref="S2.SS4.SSS2.p1.7.m7.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.7.m7.1c">{{\rm{x}}_{p}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.7.m7.1d">roman_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> that projects onto <math alttext="p" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.8.m8.1"><semantics id="S2.SS4.SSS2.p1.8.m8.1a"><mi id="S2.SS4.SSS2.p1.8.m8.1.1" xref="S2.SS4.SSS2.p1.8.m8.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.8.m8.1b"><ci id="S2.SS4.SSS2.p1.8.m8.1.1.cmml" xref="S2.SS4.SSS2.p1.8.m8.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.8.m8.1c">p</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.8.m8.1d">italic_p</annotation></semantics></math>. These distance values are derived from depth maps, which are typical outputs of sensors like Kinect, containing the <math alttext="Z" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.9.m9.1"><semantics id="S2.SS4.SSS2.p1.9.m9.1a"><mi id="S2.SS4.SSS2.p1.9.m9.1.1" xref="S2.SS4.SSS2.p1.9.m9.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.9.m9.1b"><ci id="S2.SS4.SSS2.p1.9.m9.1.1.cmml" xref="S2.SS4.SSS2.p1.9.m9.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.9.m9.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.9.m9.1d">italic_Z</annotation></semantics></math> coordinate of <math alttext="{{\rm{x}}_{p}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.10.m10.1"><semantics id="S2.SS4.SSS2.p1.10.m10.1a"><msub id="S2.SS4.SSS2.p1.10.m10.1.1" xref="S2.SS4.SSS2.p1.10.m10.1.1.cmml"><mi id="S2.SS4.SSS2.p1.10.m10.1.1.2" mathvariant="normal" xref="S2.SS4.SSS2.p1.10.m10.1.1.2.cmml">x</mi><mi id="S2.SS4.SSS2.p1.10.m10.1.1.3" xref="S2.SS4.SSS2.p1.10.m10.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.10.m10.1b"><apply id="S2.SS4.SSS2.p1.10.m10.1.1.cmml" xref="S2.SS4.SSS2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.10.m10.1.1.1.cmml" xref="S2.SS4.SSS2.p1.10.m10.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p1.10.m10.1.1.2.cmml" xref="S2.SS4.SSS2.p1.10.m10.1.1.2">x</ci><ci id="S2.SS4.SSS2.p1.10.m10.1.1.3.cmml" xref="S2.SS4.SSS2.p1.10.m10.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.10.m10.1c">{{\rm{x}}_{p}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.10.m10.1d">roman_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math>. These distance maps are compared with the distance map <math alttext="{D_{I}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.11.m11.1"><semantics id="S2.SS4.SSS2.p1.11.m11.1a"><msub id="S2.SS4.SSS2.p1.11.m11.1.1" xref="S2.SS4.SSS2.p1.11.m11.1.1.cmml"><mi id="S2.SS4.SSS2.p1.11.m11.1.1.2" xref="S2.SS4.SSS2.p1.11.m11.1.1.2.cmml">D</mi><mi id="S2.SS4.SSS2.p1.11.m11.1.1.3" xref="S2.SS4.SSS2.p1.11.m11.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.11.m11.1b"><apply id="S2.SS4.SSS2.p1.11.m11.1.1.cmml" xref="S2.SS4.SSS2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.11.m11.1.1.1.cmml" xref="S2.SS4.SSS2.p1.11.m11.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p1.11.m11.1.1.2.cmml" xref="S2.SS4.SSS2.p1.11.m11.1.1.2">𝐷</ci><ci id="S2.SS4.SSS2.p1.11.m11.1.1.3.cmml" xref="S2.SS4.SSS2.p1.11.m11.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.11.m11.1c">{D_{I}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.11.m11.1d">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> of the test image <math alttext="I" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.12.m12.1"><semantics id="S2.SS4.SSS2.p1.12.m12.1a"><mi id="S2.SS4.SSS2.p1.12.m12.1.1" xref="S2.SS4.SSS2.p1.12.m12.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.12.m12.1b"><ci id="S2.SS4.SSS2.p1.12.m12.1.1.cmml" xref="S2.SS4.SSS2.p1.12.m12.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.12.m12.1c">I</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.12.m12.1d">italic_I</annotation></semantics></math> to derive visibility masks <math alttext="\hat{V}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.13.m13.1"><semantics id="S2.SS4.SSS2.p1.13.m13.1a"><mover accent="true" id="S2.SS4.SSS2.p1.13.m13.1.1" xref="S2.SS4.SSS2.p1.13.m13.1.1.cmml"><mi id="S2.SS4.SSS2.p1.13.m13.1.1.2" xref="S2.SS4.SSS2.p1.13.m13.1.1.2.cmml">V</mi><mo id="S2.SS4.SSS2.p1.13.m13.1.1.1" xref="S2.SS4.SSS2.p1.13.m13.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.13.m13.1b"><apply id="S2.SS4.SSS2.p1.13.m13.1.1.cmml" xref="S2.SS4.SSS2.p1.13.m13.1.1"><ci id="S2.SS4.SSS2.p1.13.m13.1.1.1.cmml" xref="S2.SS4.SSS2.p1.13.m13.1.1.1">^</ci><ci id="S2.SS4.SSS2.p1.13.m13.1.1.2.cmml" xref="S2.SS4.SSS2.p1.13.m13.1.1.2">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.13.m13.1c">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.13.m13.1d">over^ start_ARG italic_V end_ARG</annotation></semantics></math> and <math alttext="\bar{V}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.14.m14.1"><semantics id="S2.SS4.SSS2.p1.14.m14.1a"><mover accent="true" id="S2.SS4.SSS2.p1.14.m14.1.1" xref="S2.SS4.SSS2.p1.14.m14.1.1.cmml"><mi id="S2.SS4.SSS2.p1.14.m14.1.1.2" xref="S2.SS4.SSS2.p1.14.m14.1.1.2.cmml">V</mi><mo id="S2.SS4.SSS2.p1.14.m14.1.1.1" xref="S2.SS4.SSS2.p1.14.m14.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.14.m14.1b"><apply id="S2.SS4.SSS2.p1.14.m14.1.1.cmml" xref="S2.SS4.SSS2.p1.14.m14.1.1"><ci id="S2.SS4.SSS2.p1.14.m14.1.1.1.cmml" xref="S2.SS4.SSS2.p1.14.m14.1.1.1">¯</ci><ci id="S2.SS4.SSS2.p1.14.m14.1.1.2.cmml" xref="S2.SS4.SSS2.p1.14.m14.1.1.2">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.14.m14.1c">\bar{V}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.14.m14.1d">over¯ start_ARG italic_V end_ARG</annotation></semantics></math>. These masks identify pixels where the model <math alttext="M" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.15.m15.1"><semantics id="S2.SS4.SSS2.p1.15.m15.1a"><mi id="S2.SS4.SSS2.p1.15.m15.1.1" xref="S2.SS4.SSS2.p1.15.m15.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.15.m15.1b"><ci id="S2.SS4.SSS2.p1.15.m15.1.1.cmml" xref="S2.SS4.SSS2.p1.15.m15.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.15.m15.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.15.m15.1d">italic_M</annotation></semantics></math> is visible in the image <math alttext="I" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.16.m16.1"><semantics id="S2.SS4.SSS2.p1.16.m16.1a"><mi id="S2.SS4.SSS2.p1.16.m16.1.1" xref="S2.SS4.SSS2.p1.16.m16.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.16.m16.1b"><ci id="S2.SS4.SSS2.p1.16.m16.1.1.cmml" xref="S2.SS4.SSS2.p1.16.m16.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.16.m16.1c">I</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.16.m16.1d">italic_I</annotation></semantics></math>. The parameter <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.17.m17.1"><semantics id="S2.SS4.SSS2.p1.17.m17.1a"><mi id="S2.SS4.SSS2.p1.17.m17.1.1" xref="S2.SS4.SSS2.p1.17.m17.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.17.m17.1b"><ci id="S2.SS4.SSS2.p1.17.m17.1.1.cmml" xref="S2.SS4.SSS2.p1.17.m17.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.17.m17.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.17.m17.1d">italic_τ</annotation></semantics></math> represents the tolerance for misalignment. In addition, the <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS2.p1.17.1">MSSD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib36" title="">36</a>]</cite></span> metric is a suitable factor for determining the likelihood of successful robotic manipulation and is not significantly affected by object geometry or surface sampling density. It can be formulated as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{e_{MSSD}}\left({\hat{P},\bar{P},{S_{M}},{V_{M}}}\right)={\min_{S\in{S_{M}}}}{%
\max_{{\rm{x}}\in{V_{M}}}}{\left\|{\hat{P}{\rm{x}}-\bar{P}S{\rm{x}}}\right\|_{%
2}}," class="ltx_Math" display="block" id="S2.E4.m1.3"><semantics id="S2.E4.m1.3a"><mrow id="S2.E4.m1.3.3.1" xref="S2.E4.m1.3.3.1.1.cmml"><mrow id="S2.E4.m1.3.3.1.1" xref="S2.E4.m1.3.3.1.1.cmml"><mrow id="S2.E4.m1.3.3.1.1.2" xref="S2.E4.m1.3.3.1.1.2.cmml"><msub id="S2.E4.m1.3.3.1.1.2.4" xref="S2.E4.m1.3.3.1.1.2.4.cmml"><mi id="S2.E4.m1.3.3.1.1.2.4.2" xref="S2.E4.m1.3.3.1.1.2.4.2.cmml">e</mi><mrow id="S2.E4.m1.3.3.1.1.2.4.3" xref="S2.E4.m1.3.3.1.1.2.4.3.cmml"><mi id="S2.E4.m1.3.3.1.1.2.4.3.2" xref="S2.E4.m1.3.3.1.1.2.4.3.2.cmml">M</mi><mo id="S2.E4.m1.3.3.1.1.2.4.3.1" xref="S2.E4.m1.3.3.1.1.2.4.3.1.cmml">⁢</mo><mi id="S2.E4.m1.3.3.1.1.2.4.3.3" xref="S2.E4.m1.3.3.1.1.2.4.3.3.cmml">S</mi><mo id="S2.E4.m1.3.3.1.1.2.4.3.1a" xref="S2.E4.m1.3.3.1.1.2.4.3.1.cmml">⁢</mo><mi id="S2.E4.m1.3.3.1.1.2.4.3.4" xref="S2.E4.m1.3.3.1.1.2.4.3.4.cmml">S</mi><mo id="S2.E4.m1.3.3.1.1.2.4.3.1b" xref="S2.E4.m1.3.3.1.1.2.4.3.1.cmml">⁢</mo><mi id="S2.E4.m1.3.3.1.1.2.4.3.5" xref="S2.E4.m1.3.3.1.1.2.4.3.5.cmml">D</mi></mrow></msub><mo id="S2.E4.m1.3.3.1.1.2.3" xref="S2.E4.m1.3.3.1.1.2.3.cmml">⁢</mo><mrow id="S2.E4.m1.3.3.1.1.2.2.2" xref="S2.E4.m1.3.3.1.1.2.2.3.cmml"><mo id="S2.E4.m1.3.3.1.1.2.2.2.3" xref="S2.E4.m1.3.3.1.1.2.2.3.cmml">(</mo><mover accent="true" id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml"><mi id="S2.E4.m1.1.1.2" xref="S2.E4.m1.1.1.2.cmml">P</mi><mo id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.cmml">^</mo></mover><mo id="S2.E4.m1.3.3.1.1.2.2.2.4" xref="S2.E4.m1.3.3.1.1.2.2.3.cmml">,</mo><mover accent="true" id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml"><mi id="S2.E4.m1.2.2.2" xref="S2.E4.m1.2.2.2.cmml">P</mi><mo id="S2.E4.m1.2.2.1" xref="S2.E4.m1.2.2.1.cmml">¯</mo></mover><mo id="S2.E4.m1.3.3.1.1.2.2.2.5" xref="S2.E4.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="S2.E4.m1.3.3.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.2.cmml">S</mi><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.cmml">M</mi></msub><mo id="S2.E4.m1.3.3.1.1.2.2.2.6" xref="S2.E4.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="S2.E4.m1.3.3.1.1.2.2.2.2" xref="S2.E4.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S2.E4.m1.3.3.1.1.2.2.2.2.2" xref="S2.E4.m1.3.3.1.1.2.2.2.2.2.cmml">V</mi><mi id="S2.E4.m1.3.3.1.1.2.2.2.2.3" xref="S2.E4.m1.3.3.1.1.2.2.2.2.3.cmml">M</mi></msub><mo id="S2.E4.m1.3.3.1.1.2.2.2.7" xref="S2.E4.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.3.3.1.1.4" xref="S2.E4.m1.3.3.1.1.4.cmml">=</mo><mrow id="S2.E4.m1.3.3.1.1.3" xref="S2.E4.m1.3.3.1.1.3.cmml"><munder id="S2.E4.m1.3.3.1.1.3.2" xref="S2.E4.m1.3.3.1.1.3.2.cmml"><mi id="S2.E4.m1.3.3.1.1.3.2.2" xref="S2.E4.m1.3.3.1.1.3.2.2.cmml">min</mi><mrow id="S2.E4.m1.3.3.1.1.3.2.3" xref="S2.E4.m1.3.3.1.1.3.2.3.cmml"><mi id="S2.E4.m1.3.3.1.1.3.2.3.2" xref="S2.E4.m1.3.3.1.1.3.2.3.2.cmml">S</mi><mo id="S2.E4.m1.3.3.1.1.3.2.3.1" xref="S2.E4.m1.3.3.1.1.3.2.3.1.cmml">∈</mo><msub id="S2.E4.m1.3.3.1.1.3.2.3.3" xref="S2.E4.m1.3.3.1.1.3.2.3.3.cmml"><mi id="S2.E4.m1.3.3.1.1.3.2.3.3.2" xref="S2.E4.m1.3.3.1.1.3.2.3.3.2.cmml">S</mi><mi id="S2.E4.m1.3.3.1.1.3.2.3.3.3" xref="S2.E4.m1.3.3.1.1.3.2.3.3.3.cmml">M</mi></msub></mrow></munder><mo id="S2.E4.m1.3.3.1.1.3a" lspace="0.167em" xref="S2.E4.m1.3.3.1.1.3.cmml">⁡</mo><mrow id="S2.E4.m1.3.3.1.1.3.1" xref="S2.E4.m1.3.3.1.1.3.1.cmml"><munder id="S2.E4.m1.3.3.1.1.3.1.2" xref="S2.E4.m1.3.3.1.1.3.1.2.cmml"><mi id="S2.E4.m1.3.3.1.1.3.1.2.2" xref="S2.E4.m1.3.3.1.1.3.1.2.2.cmml">max</mi><mrow id="S2.E4.m1.3.3.1.1.3.1.2.3" xref="S2.E4.m1.3.3.1.1.3.1.2.3.cmml"><mi id="S2.E4.m1.3.3.1.1.3.1.2.3.2" mathvariant="normal" xref="S2.E4.m1.3.3.1.1.3.1.2.3.2.cmml">x</mi><mo id="S2.E4.m1.3.3.1.1.3.1.2.3.1" xref="S2.E4.m1.3.3.1.1.3.1.2.3.1.cmml">∈</mo><msub id="S2.E4.m1.3.3.1.1.3.1.2.3.3" xref="S2.E4.m1.3.3.1.1.3.1.2.3.3.cmml"><mi id="S2.E4.m1.3.3.1.1.3.1.2.3.3.2" xref="S2.E4.m1.3.3.1.1.3.1.2.3.3.2.cmml">V</mi><mi id="S2.E4.m1.3.3.1.1.3.1.2.3.3.3" xref="S2.E4.m1.3.3.1.1.3.1.2.3.3.3.cmml">M</mi></msub></mrow></munder><mo id="S2.E4.m1.3.3.1.1.3.1a" xref="S2.E4.m1.3.3.1.1.3.1.cmml">⁡</mo><msub id="S2.E4.m1.3.3.1.1.3.1.1" xref="S2.E4.m1.3.3.1.1.3.1.1.cmml"><mrow id="S2.E4.m1.3.3.1.1.3.1.1.1.1" xref="S2.E4.m1.3.3.1.1.3.1.1.1.2.cmml"><mo id="S2.E4.m1.3.3.1.1.3.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.3.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.cmml"><mrow id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.cmml"><mover accent="true" id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2.cmml"><mi id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2.2" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2.2.cmml">P</mi><mo id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2.1" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mo id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.1" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.3" mathvariant="normal" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.3.cmml">x</mi></mrow><mo id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.cmml"><mover accent="true" id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2.cmml"><mi id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2.2" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2.2.cmml">P</mi><mo id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2.1" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2.1.cmml">¯</mo></mover><mo id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.1" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.3" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.3.cmml">S</mi><mo id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.1a" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.4" mathvariant="normal" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.4.cmml">x</mi></mrow></mrow><mo id="S2.E4.m1.3.3.1.1.3.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.3.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.E4.m1.3.3.1.1.3.1.1.3" xref="S2.E4.m1.3.3.1.1.3.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><mo id="S2.E4.m1.3.3.1.2" xref="S2.E4.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.3b"><apply id="S2.E4.m1.3.3.1.1.cmml" xref="S2.E4.m1.3.3.1"><eq id="S2.E4.m1.3.3.1.1.4.cmml" xref="S2.E4.m1.3.3.1.1.4"></eq><apply id="S2.E4.m1.3.3.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.2"><times id="S2.E4.m1.3.3.1.1.2.3.cmml" xref="S2.E4.m1.3.3.1.1.2.3"></times><apply id="S2.E4.m1.3.3.1.1.2.4.cmml" xref="S2.E4.m1.3.3.1.1.2.4"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.2.4.1.cmml" xref="S2.E4.m1.3.3.1.1.2.4">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.2.4.2.cmml" xref="S2.E4.m1.3.3.1.1.2.4.2">𝑒</ci><apply id="S2.E4.m1.3.3.1.1.2.4.3.cmml" xref="S2.E4.m1.3.3.1.1.2.4.3"><times id="S2.E4.m1.3.3.1.1.2.4.3.1.cmml" xref="S2.E4.m1.3.3.1.1.2.4.3.1"></times><ci id="S2.E4.m1.3.3.1.1.2.4.3.2.cmml" xref="S2.E4.m1.3.3.1.1.2.4.3.2">𝑀</ci><ci id="S2.E4.m1.3.3.1.1.2.4.3.3.cmml" xref="S2.E4.m1.3.3.1.1.2.4.3.3">𝑆</ci><ci id="S2.E4.m1.3.3.1.1.2.4.3.4.cmml" xref="S2.E4.m1.3.3.1.1.2.4.3.4">𝑆</ci><ci id="S2.E4.m1.3.3.1.1.2.4.3.5.cmml" xref="S2.E4.m1.3.3.1.1.2.4.3.5">𝐷</ci></apply></apply><vector id="S2.E4.m1.3.3.1.1.2.2.3.cmml" xref="S2.E4.m1.3.3.1.1.2.2.2"><apply id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1"><ci id="S2.E4.m1.1.1.1.cmml" xref="S2.E4.m1.1.1.1">^</ci><ci id="S2.E4.m1.1.1.2.cmml" xref="S2.E4.m1.1.1.2">𝑃</ci></apply><apply id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2"><ci id="S2.E4.m1.2.2.1.cmml" xref="S2.E4.m1.2.2.1">¯</ci><ci id="S2.E4.m1.2.2.2.cmml" xref="S2.E4.m1.2.2.2">𝑃</ci></apply><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.2">𝑆</ci><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3">𝑀</ci></apply><apply id="S2.E4.m1.3.3.1.1.2.2.2.2.cmml" xref="S2.E4.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S2.E4.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S2.E4.m1.3.3.1.1.2.2.2.2.2">𝑉</ci><ci id="S2.E4.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S2.E4.m1.3.3.1.1.2.2.2.2.3">𝑀</ci></apply></vector></apply><apply id="S2.E4.m1.3.3.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.3"><apply id="S2.E4.m1.3.3.1.1.3.2.cmml" xref="S2.E4.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.3.2.1.cmml" xref="S2.E4.m1.3.3.1.1.3.2">subscript</csymbol><min id="S2.E4.m1.3.3.1.1.3.2.2.cmml" xref="S2.E4.m1.3.3.1.1.3.2.2"></min><apply id="S2.E4.m1.3.3.1.1.3.2.3.cmml" xref="S2.E4.m1.3.3.1.1.3.2.3"><in id="S2.E4.m1.3.3.1.1.3.2.3.1.cmml" xref="S2.E4.m1.3.3.1.1.3.2.3.1"></in><ci id="S2.E4.m1.3.3.1.1.3.2.3.2.cmml" xref="S2.E4.m1.3.3.1.1.3.2.3.2">𝑆</ci><apply id="S2.E4.m1.3.3.1.1.3.2.3.3.cmml" xref="S2.E4.m1.3.3.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.3.2.3.3.1.cmml" xref="S2.E4.m1.3.3.1.1.3.2.3.3">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.3.2.3.3.2.cmml" xref="S2.E4.m1.3.3.1.1.3.2.3.3.2">𝑆</ci><ci id="S2.E4.m1.3.3.1.1.3.2.3.3.3.cmml" xref="S2.E4.m1.3.3.1.1.3.2.3.3.3">𝑀</ci></apply></apply></apply><apply id="S2.E4.m1.3.3.1.1.3.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1"><apply id="S2.E4.m1.3.3.1.1.3.1.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.3.1.2.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.2">subscript</csymbol><max id="S2.E4.m1.3.3.1.1.3.1.2.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.2.2"></max><apply id="S2.E4.m1.3.3.1.1.3.1.2.3.cmml" xref="S2.E4.m1.3.3.1.1.3.1.2.3"><in id="S2.E4.m1.3.3.1.1.3.1.2.3.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.2.3.1"></in><ci id="S2.E4.m1.3.3.1.1.3.1.2.3.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.2.3.2">x</ci><apply id="S2.E4.m1.3.3.1.1.3.1.2.3.3.cmml" xref="S2.E4.m1.3.3.1.1.3.1.2.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.3.1.2.3.3.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.2.3.3">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.3.1.2.3.3.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.2.3.3.2">𝑉</ci><ci id="S2.E4.m1.3.3.1.1.3.1.2.3.3.3.cmml" xref="S2.E4.m1.3.3.1.1.3.1.2.3.3.3">𝑀</ci></apply></apply></apply><apply id="S2.E4.m1.3.3.1.1.3.1.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.3.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1">subscript</csymbol><apply id="S2.E4.m1.3.3.1.1.3.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.3.3.1.1.3.1.1.1.2.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.2">norm</csymbol><apply id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1"><minus id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.1"></minus><apply id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2"><times id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.1"></times><apply id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2"><ci id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2.1">^</ci><ci id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.2.2">𝑃</ci></apply><ci id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.2.3">x</ci></apply><apply id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3"><times id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.1"></times><apply id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2"><ci id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2.1.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2.1">¯</ci><ci id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2.2.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.2.2">𝑃</ci></apply><ci id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.3">𝑆</ci><ci id="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.4.cmml" xref="S2.E4.m1.3.3.1.1.3.1.1.1.1.1.3.4">x</ci></apply></apply></apply><cn id="S2.E4.m1.3.3.1.1.3.1.1.3.cmml" type="integer" xref="S2.E4.m1.3.3.1.1.3.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.3c">{e_{MSSD}}\left({\hat{P},\bar{P},{S_{M}},{V_{M}}}\right)={\min_{S\in{S_{M}}}}{%
\max_{{\rm{x}}\in{V_{M}}}}{\left\|{\hat{P}{\rm{x}}-\bar{P}S{\rm{x}}}\right\|_{%
2}},</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.3d">italic_e start_POSTSUBSCRIPT italic_M italic_S italic_S italic_D end_POSTSUBSCRIPT ( over^ start_ARG italic_P end_ARG , over¯ start_ARG italic_P end_ARG , italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) = roman_min start_POSTSUBSCRIPT italic_S ∈ italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT roman_x ∈ italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ over^ start_ARG italic_P end_ARG roman_x - over¯ start_ARG italic_P end_ARG italic_S roman_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.SSS2.p1.20">where the set <math alttext="{{S_{M}}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.18.m1.1"><semantics id="S2.SS4.SSS2.p1.18.m1.1a"><msub id="S2.SS4.SSS2.p1.18.m1.1.1" xref="S2.SS4.SSS2.p1.18.m1.1.1.cmml"><mi id="S2.SS4.SSS2.p1.18.m1.1.1.2" xref="S2.SS4.SSS2.p1.18.m1.1.1.2.cmml">S</mi><mi id="S2.SS4.SSS2.p1.18.m1.1.1.3" xref="S2.SS4.SSS2.p1.18.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.18.m1.1b"><apply id="S2.SS4.SSS2.p1.18.m1.1.1.cmml" xref="S2.SS4.SSS2.p1.18.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.18.m1.1.1.1.cmml" xref="S2.SS4.SSS2.p1.18.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p1.18.m1.1.1.2.cmml" xref="S2.SS4.SSS2.p1.18.m1.1.1.2">𝑆</ci><ci id="S2.SS4.SSS2.p1.18.m1.1.1.3.cmml" xref="S2.SS4.SSS2.p1.18.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.18.m1.1c">{{S_{M}}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.18.m1.1d">italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> comprises global symmetry transformations for the object model <math alttext="M" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.19.m2.1"><semantics id="S2.SS4.SSS2.p1.19.m2.1a"><mi id="S2.SS4.SSS2.p1.19.m2.1.1" xref="S2.SS4.SSS2.p1.19.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.19.m2.1b"><ci id="S2.SS4.SSS2.p1.19.m2.1.1.cmml" xref="S2.SS4.SSS2.p1.19.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.19.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.19.m2.1d">italic_M</annotation></semantics></math>, while <math alttext="{{V_{M}}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.20.m3.1"><semantics id="S2.SS4.SSS2.p1.20.m3.1a"><msub id="S2.SS4.SSS2.p1.20.m3.1.1" xref="S2.SS4.SSS2.p1.20.m3.1.1.cmml"><mi id="S2.SS4.SSS2.p1.20.m3.1.1.2" xref="S2.SS4.SSS2.p1.20.m3.1.1.2.cmml">V</mi><mi id="S2.SS4.SSS2.p1.20.m3.1.1.3" xref="S2.SS4.SSS2.p1.20.m3.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.20.m3.1b"><apply id="S2.SS4.SSS2.p1.20.m3.1.1.cmml" xref="S2.SS4.SSS2.p1.20.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.20.m3.1.1.1.cmml" xref="S2.SS4.SSS2.p1.20.m3.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p1.20.m3.1.1.2.cmml" xref="S2.SS4.SSS2.p1.20.m3.1.1.2">𝑉</ci><ci id="S2.SS4.SSS2.p1.20.m3.1.1.3.cmml" xref="S2.SS4.SSS2.p1.20.m3.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.20.m3.1c">{{V_{M}}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.20.m3.1d">italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> represents the vertices of the model. Furthermore, the <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS2.p1.20.1">MSPD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib36" title="">36</a>]</cite></span> metric is ideal for evaluating RGB-only methods in augmented reality, focusing on perceivable discrepancies and excluding alignment along the optical (Z) axis, which can be represented as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{array}[]{l}{e_{MSPD}}\left({\hat{P},\bar{P},{S_{M}},{V_{M}}}\right)=\\
{\min_{S\in{S_{M}}}}{\max_{{\rm{x}}\in{V_{M}}}}{\left\|{proj\left({\hat{P}{\rm%
{x}}}\right)-proj\left({\bar{P}S{\rm{x}}}\right)}\right\|_{2}},\end{array}%
\vspace{-0.5em}" class="ltx_Math" display="block" id="S2.E5.m1.5"><semantics id="S2.E5.m1.5a"><mtable displaystyle="true" id="S2.E5.m1.5.5" rowspacing="0pt" xref="S2.E5.m1.5.5.cmml"><mtr id="S2.E5.m1.5.5a" xref="S2.E5.m1.5.5.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.5.5b" xref="S2.E5.m1.5.5.cmml"><mrow id="S2.E5.m1.4.4.4.4.4" xref="S2.E5.m1.4.4.4.4.4.cmml"><mrow id="S2.E5.m1.4.4.4.4.4.4" xref="S2.E5.m1.4.4.4.4.4.4.cmml"><msub id="S2.E5.m1.4.4.4.4.4.4.4" xref="S2.E5.m1.4.4.4.4.4.4.4.cmml"><mi id="S2.E5.m1.4.4.4.4.4.4.4.2" xref="S2.E5.m1.4.4.4.4.4.4.4.2.cmml">e</mi><mrow id="S2.E5.m1.4.4.4.4.4.4.4.3" xref="S2.E5.m1.4.4.4.4.4.4.4.3.cmml"><mi id="S2.E5.m1.4.4.4.4.4.4.4.3.2" xref="S2.E5.m1.4.4.4.4.4.4.4.3.2.cmml">M</mi><mo id="S2.E5.m1.4.4.4.4.4.4.4.3.1" xref="S2.E5.m1.4.4.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S2.E5.m1.4.4.4.4.4.4.4.3.3" xref="S2.E5.m1.4.4.4.4.4.4.4.3.3.cmml">S</mi><mo id="S2.E5.m1.4.4.4.4.4.4.4.3.1a" xref="S2.E5.m1.4.4.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S2.E5.m1.4.4.4.4.4.4.4.3.4" xref="S2.E5.m1.4.4.4.4.4.4.4.3.4.cmml">P</mi><mo id="S2.E5.m1.4.4.4.4.4.4.4.3.1b" xref="S2.E5.m1.4.4.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S2.E5.m1.4.4.4.4.4.4.4.3.5" xref="S2.E5.m1.4.4.4.4.4.4.4.3.5.cmml">D</mi></mrow></msub><mo id="S2.E5.m1.4.4.4.4.4.4.3" xref="S2.E5.m1.4.4.4.4.4.4.3.cmml">⁢</mo><mrow id="S2.E5.m1.4.4.4.4.4.4.2.2" xref="S2.E5.m1.4.4.4.4.4.4.2.3.cmml"><mo id="S2.E5.m1.4.4.4.4.4.4.2.2.3" xref="S2.E5.m1.4.4.4.4.4.4.2.3.cmml">(</mo><mover accent="true" id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.2.cmml">P</mi><mo id="S2.E5.m1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml">^</mo></mover><mo id="S2.E5.m1.4.4.4.4.4.4.2.2.4" xref="S2.E5.m1.4.4.4.4.4.4.2.3.cmml">,</mo><mover accent="true" id="S2.E5.m1.2.2.2.2.2.2" xref="S2.E5.m1.2.2.2.2.2.2.cmml"><mi id="S2.E5.m1.2.2.2.2.2.2.2" xref="S2.E5.m1.2.2.2.2.2.2.2.cmml">P</mi><mo id="S2.E5.m1.2.2.2.2.2.2.1" xref="S2.E5.m1.2.2.2.2.2.2.1.cmml">¯</mo></mover><mo id="S2.E5.m1.4.4.4.4.4.4.2.2.5" xref="S2.E5.m1.4.4.4.4.4.4.2.3.cmml">,</mo><msub id="S2.E5.m1.3.3.3.3.3.3.1.1.1" xref="S2.E5.m1.3.3.3.3.3.3.1.1.1.cmml"><mi id="S2.E5.m1.3.3.3.3.3.3.1.1.1.2" xref="S2.E5.m1.3.3.3.3.3.3.1.1.1.2.cmml">S</mi><mi id="S2.E5.m1.3.3.3.3.3.3.1.1.1.3" xref="S2.E5.m1.3.3.3.3.3.3.1.1.1.3.cmml">M</mi></msub><mo id="S2.E5.m1.4.4.4.4.4.4.2.2.6" xref="S2.E5.m1.4.4.4.4.4.4.2.3.cmml">,</mo><msub id="S2.E5.m1.4.4.4.4.4.4.2.2.2" xref="S2.E5.m1.4.4.4.4.4.4.2.2.2.cmml"><mi id="S2.E5.m1.4.4.4.4.4.4.2.2.2.2" xref="S2.E5.m1.4.4.4.4.4.4.2.2.2.2.cmml">V</mi><mi id="S2.E5.m1.4.4.4.4.4.4.2.2.2.3" xref="S2.E5.m1.4.4.4.4.4.4.2.2.2.3.cmml">M</mi></msub><mo id="S2.E5.m1.4.4.4.4.4.4.2.2.7" xref="S2.E5.m1.4.4.4.4.4.4.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.4.4.4.4.4.5" xref="S2.E5.m1.4.4.4.4.4.5.cmml">=</mo><mi id="S2.E5.m1.4.4.4.4.4.6" xref="S2.E5.m1.4.4.4.4.4.6.cmml"></mi></mrow></mtd></mtr><mtr id="S2.E5.m1.5.5c" xref="S2.E5.m1.5.5.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.5.5d" xref="S2.E5.m1.5.5.cmml"><mrow id="S2.E5.m1.5.5.5.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.cmml"><mrow id="S2.E5.m1.5.5.5.1.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.cmml"><msub id="S2.E5.m1.5.5.5.1.1.1.1.2" xref="S2.E5.m1.5.5.5.1.1.1.1.2.cmml"><mi id="S2.E5.m1.5.5.5.1.1.1.1.2.2" xref="S2.E5.m1.5.5.5.1.1.1.1.2.2.cmml">min</mi><mrow id="S2.E5.m1.5.5.5.1.1.1.1.2.3" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.cmml"><mi id="S2.E5.m1.5.5.5.1.1.1.1.2.3.2" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.2.cmml">S</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.2.3.1" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.1.cmml">∈</mo><msub id="S2.E5.m1.5.5.5.1.1.1.1.2.3.3" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.cmml"><mi id="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.2" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.2.cmml">S</mi><mi id="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.3" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.3.cmml">M</mi></msub></mrow></msub><mo id="S2.E5.m1.5.5.5.1.1.1.1a" lspace="0.167em" xref="S2.E5.m1.5.5.5.1.1.1.1.cmml">⁡</mo><mrow id="S2.E5.m1.5.5.5.1.1.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.cmml"><msub id="S2.E5.m1.5.5.5.1.1.1.1.1.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.cmml"><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.2.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.2.cmml">max</mi><mrow id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.cmml"><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.2" mathvariant="normal" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.2.cmml">x</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.1.cmml">∈</mo><msub id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.cmml"><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.2.cmml">V</mi><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.3.cmml">M</mi></msub></mrow></msub><mo id="S2.E5.m1.5.5.5.1.1.1.1.1a" xref="S2.E5.m1.5.5.5.1.1.1.1.1.cmml">⁡</mo><msub id="S2.E5.m1.5.5.5.1.1.1.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.cmml"><mrow id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.2.cmml"><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.3.cmml">p</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.4" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.4.cmml">r</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.2a" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.5" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.5.cmml">o</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.2b" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.6" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.6.cmml">j</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.2c" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">P</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.3" mathvariant="normal" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.3.cmml">p</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.2.cmml">⁢</mo><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.4" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.4.cmml">r</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.2a" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.2.cmml">⁢</mo><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.5" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.5.cmml">o</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.2b" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.2.cmml">⁢</mo><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.6" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.6.cmml">j</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.2c" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.2.cmml">⁢</mo><mrow id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mover accent="true" id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml"><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2.2" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2.2.cmml">P</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2.1.cmml">¯</mo></mover><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.1" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml">⁢</mo><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml">S</mi><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.1a" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml">⁢</mo><mi id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.4" mathvariant="normal" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.4.cmml">x</mi></mrow><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.E5.m1.5.5.5.1.1.1.1.1.1.3" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow><mo id="S2.E5.m1.5.5.5.1.1.1.2" xref="S2.E5.m1.5.5.5.1.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E5.m1.5b"><matrix id="S2.E5.m1.5.5.cmml" xref="S2.E5.m1.5.5"><matrixrow id="S2.E5.m1.5.5a.cmml" xref="S2.E5.m1.5.5"><apply id="S2.E5.m1.4.4.4.4.4.cmml" xref="S2.E5.m1.4.4.4.4.4"><eq id="S2.E5.m1.4.4.4.4.4.5.cmml" xref="S2.E5.m1.4.4.4.4.4.5"></eq><apply id="S2.E5.m1.4.4.4.4.4.4.cmml" xref="S2.E5.m1.4.4.4.4.4.4"><times id="S2.E5.m1.4.4.4.4.4.4.3.cmml" xref="S2.E5.m1.4.4.4.4.4.4.3"></times><apply id="S2.E5.m1.4.4.4.4.4.4.4.cmml" xref="S2.E5.m1.4.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S2.E5.m1.4.4.4.4.4.4.4.1.cmml" xref="S2.E5.m1.4.4.4.4.4.4.4">subscript</csymbol><ci id="S2.E5.m1.4.4.4.4.4.4.4.2.cmml" xref="S2.E5.m1.4.4.4.4.4.4.4.2">𝑒</ci><apply id="S2.E5.m1.4.4.4.4.4.4.4.3.cmml" xref="S2.E5.m1.4.4.4.4.4.4.4.3"><times id="S2.E5.m1.4.4.4.4.4.4.4.3.1.cmml" xref="S2.E5.m1.4.4.4.4.4.4.4.3.1"></times><ci id="S2.E5.m1.4.4.4.4.4.4.4.3.2.cmml" xref="S2.E5.m1.4.4.4.4.4.4.4.3.2">𝑀</ci><ci id="S2.E5.m1.4.4.4.4.4.4.4.3.3.cmml" xref="S2.E5.m1.4.4.4.4.4.4.4.3.3">𝑆</ci><ci id="S2.E5.m1.4.4.4.4.4.4.4.3.4.cmml" xref="S2.E5.m1.4.4.4.4.4.4.4.3.4">𝑃</ci><ci id="S2.E5.m1.4.4.4.4.4.4.4.3.5.cmml" xref="S2.E5.m1.4.4.4.4.4.4.4.3.5">𝐷</ci></apply></apply><vector id="S2.E5.m1.4.4.4.4.4.4.2.3.cmml" xref="S2.E5.m1.4.4.4.4.4.4.2.2"><apply id="S2.E5.m1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1"><ci id="S2.E5.m1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1">^</ci><ci id="S2.E5.m1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2">𝑃</ci></apply><apply id="S2.E5.m1.2.2.2.2.2.2.cmml" xref="S2.E5.m1.2.2.2.2.2.2"><ci id="S2.E5.m1.2.2.2.2.2.2.1.cmml" xref="S2.E5.m1.2.2.2.2.2.2.1">¯</ci><ci id="S2.E5.m1.2.2.2.2.2.2.2.cmml" xref="S2.E5.m1.2.2.2.2.2.2.2">𝑃</ci></apply><apply id="S2.E5.m1.3.3.3.3.3.3.1.1.1.cmml" xref="S2.E5.m1.3.3.3.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.3.3.3.3.3.3.1.1.1.1.cmml" xref="S2.E5.m1.3.3.3.3.3.3.1.1.1">subscript</csymbol><ci id="S2.E5.m1.3.3.3.3.3.3.1.1.1.2.cmml" xref="S2.E5.m1.3.3.3.3.3.3.1.1.1.2">𝑆</ci><ci id="S2.E5.m1.3.3.3.3.3.3.1.1.1.3.cmml" xref="S2.E5.m1.3.3.3.3.3.3.1.1.1.3">𝑀</ci></apply><apply id="S2.E5.m1.4.4.4.4.4.4.2.2.2.cmml" xref="S2.E5.m1.4.4.4.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S2.E5.m1.4.4.4.4.4.4.2.2.2.1.cmml" xref="S2.E5.m1.4.4.4.4.4.4.2.2.2">subscript</csymbol><ci id="S2.E5.m1.4.4.4.4.4.4.2.2.2.2.cmml" xref="S2.E5.m1.4.4.4.4.4.4.2.2.2.2">𝑉</ci><ci id="S2.E5.m1.4.4.4.4.4.4.2.2.2.3.cmml" xref="S2.E5.m1.4.4.4.4.4.4.2.2.2.3">𝑀</ci></apply></vector></apply><csymbol cd="latexml" id="S2.E5.m1.4.4.4.4.4.6.cmml" xref="S2.E5.m1.4.4.4.4.4.6">absent</csymbol></apply></matrixrow><matrixrow id="S2.E5.m1.5.5b.cmml" xref="S2.E5.m1.5.5"><apply id="S2.E5.m1.5.5.5.1.1.1.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1"><apply id="S2.E5.m1.5.5.5.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.5.1.1.1.1.2.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.2">subscript</csymbol><min id="S2.E5.m1.5.5.5.1.1.1.1.2.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.2.2"></min><apply id="S2.E5.m1.5.5.5.1.1.1.1.2.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3"><in id="S2.E5.m1.5.5.5.1.1.1.1.2.3.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.1"></in><ci id="S2.E5.m1.5.5.5.1.1.1.1.2.3.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.2">𝑆</ci><apply id="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.3">subscript</csymbol><ci id="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.2">𝑆</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.2.3.3.3">𝑀</ci></apply></apply></apply><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1"><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.5.1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2">subscript</csymbol><max id="S2.E5.m1.5.5.5.1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.2"></max><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3"><in id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.1"></in><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.2">x</ci><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3">subscript</csymbol><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.2">𝑉</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.2.3.3.3">𝑀</ci></apply></apply></apply><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.5.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1">subscript</csymbol><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1"><minus id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.3"></minus><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1"><times id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.3">𝑝</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.4">𝑟</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.5">𝑜</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.6.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.6">𝑗</ci><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑃</ci></apply><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.3">x</ci></apply></apply><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2"><times id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.2"></times><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.3">𝑝</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.4">𝑟</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.5.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.5">𝑜</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.6.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.6">𝑗</ci><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1"><times id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.1"></times><apply id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2"><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2.1.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2.1">¯</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2.2.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.2.2">𝑃</ci></apply><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.3">𝑆</ci><ci id="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.4.cmml" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.1.1.1.2.1.1.1.4">x</ci></apply></apply></apply></apply><cn id="S2.E5.m1.5.5.5.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E5.m1.5.5.5.1.1.1.1.1.1.3">2</cn></apply></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.5c">\begin{array}[]{l}{e_{MSPD}}\left({\hat{P},\bar{P},{S_{M}},{V_{M}}}\right)=\\
{\min_{S\in{S_{M}}}}{\max_{{\rm{x}}\in{V_{M}}}}{\left\|{proj\left({\hat{P}{\rm%
{x}}}\right)-proj\left({\bar{P}S{\rm{x}}}\right)}\right\|_{2}},\end{array}%
\vspace{-0.5em}</annotation><annotation encoding="application/x-llamapun" id="S2.E5.m1.5d">start_ARRAY start_ROW start_CELL italic_e start_POSTSUBSCRIPT italic_M italic_S italic_P italic_D end_POSTSUBSCRIPT ( over^ start_ARG italic_P end_ARG , over¯ start_ARG italic_P end_ARG , italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) = end_CELL end_ROW start_ROW start_CELL roman_min start_POSTSUBSCRIPT italic_S ∈ italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT roman_x ∈ italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ italic_p italic_r italic_o italic_j ( over^ start_ARG italic_P end_ARG roman_x ) - italic_p italic_r italic_o italic_j ( over¯ start_ARG italic_P end_ARG italic_S roman_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , end_CELL end_ROW end_ARRAY</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.SSS2.p1.21">where the function <math alttext="proj()" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.21.m1.1"><semantics id="S2.SS4.SSS2.p1.21.m1.1a"><mrow id="S2.SS4.SSS2.p1.21.m1.1.1" xref="S2.SS4.SSS2.p1.21.m1.1.1.cmml"><mi id="S2.SS4.SSS2.p1.21.m1.1.1.2" xref="S2.SS4.SSS2.p1.21.m1.1.1.2.cmml">p</mi><mo id="S2.SS4.SSS2.p1.21.m1.1.1.1" xref="S2.SS4.SSS2.p1.21.m1.1.1.1.cmml">⁢</mo><mi id="S2.SS4.SSS2.p1.21.m1.1.1.3" xref="S2.SS4.SSS2.p1.21.m1.1.1.3.cmml">r</mi><mo id="S2.SS4.SSS2.p1.21.m1.1.1.1a" xref="S2.SS4.SSS2.p1.21.m1.1.1.1.cmml">⁢</mo><mi id="S2.SS4.SSS2.p1.21.m1.1.1.4" xref="S2.SS4.SSS2.p1.21.m1.1.1.4.cmml">o</mi><mo id="S2.SS4.SSS2.p1.21.m1.1.1.1b" xref="S2.SS4.SSS2.p1.21.m1.1.1.1.cmml">⁢</mo><mi id="S2.SS4.SSS2.p1.21.m1.1.1.5" xref="S2.SS4.SSS2.p1.21.m1.1.1.5.cmml">j</mi><mo id="S2.SS4.SSS2.p1.21.m1.1.1.1c" xref="S2.SS4.SSS2.p1.21.m1.1.1.1.cmml">⁢</mo><mrow id="S2.SS4.SSS2.p1.21.m1.1.1.6.2" xref="S2.SS4.SSS2.p1.21.m1.1.1.cmml"><mo id="S2.SS4.SSS2.p1.21.m1.1.1.6.2.1" stretchy="false" xref="S2.SS4.SSS2.p1.21.m1.1.1.6.1.cmml">(</mo><mo id="S2.SS4.SSS2.p1.21.m1.1.1.6.2.2" stretchy="false" xref="S2.SS4.SSS2.p1.21.m1.1.1.6.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.21.m1.1b"><apply id="S2.SS4.SSS2.p1.21.m1.1.1.cmml" xref="S2.SS4.SSS2.p1.21.m1.1.1"><times id="S2.SS4.SSS2.p1.21.m1.1.1.1.cmml" xref="S2.SS4.SSS2.p1.21.m1.1.1.1"></times><ci id="S2.SS4.SSS2.p1.21.m1.1.1.2.cmml" xref="S2.SS4.SSS2.p1.21.m1.1.1.2">𝑝</ci><ci id="S2.SS4.SSS2.p1.21.m1.1.1.3.cmml" xref="S2.SS4.SSS2.p1.21.m1.1.1.3">𝑟</ci><ci id="S2.SS4.SSS2.p1.21.m1.1.1.4.cmml" xref="S2.SS4.SSS2.p1.21.m1.1.1.4">𝑜</ci><ci id="S2.SS4.SSS2.p1.21.m1.1.1.5.cmml" xref="S2.SS4.SSS2.p1.21.m1.1.1.5">𝑗</ci><list id="S2.SS4.SSS2.p1.21.m1.1.1.6.1.cmml" xref="S2.SS4.SSS2.p1.21.m1.1.1.6.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.21.m1.1c">proj()</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.21.m1.1d">italic_p italic_r italic_o italic_j ( )</annotation></semantics></math> represents the 2D projection (pixel-level), and the other symbols have the same meanings as in MSSD.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p2">
<p class="ltx_p" id="S2.SS4.SSS2.p2.5">Besides the <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS2.p2.5.1">BOP-M</em>, the average point distance (ADD)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib37" title="">37</a>]</cite> and average closest point distance (ADD-S)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib37" title="">37</a>]</cite> are also commonly leveraged to evaluate the performance of 6DoF object pose estimation. They can intuitively quantify the geometric error between the estimated and the ground-truth poses by computing the average distance between corresponding points on the object CAD model. Specifically, the ADD metric is designed for asymmetric objects, while the ADD-S metric is designed explicitly for symmetric objects. Given the ground-truth rotation <math alttext="{R_{gt}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p2.1.m1.1"><semantics id="S2.SS4.SSS2.p2.1.m1.1a"><msub id="S2.SS4.SSS2.p2.1.m1.1.1" xref="S2.SS4.SSS2.p2.1.m1.1.1.cmml"><mi id="S2.SS4.SSS2.p2.1.m1.1.1.2" xref="S2.SS4.SSS2.p2.1.m1.1.1.2.cmml">R</mi><mrow id="S2.SS4.SSS2.p2.1.m1.1.1.3" xref="S2.SS4.SSS2.p2.1.m1.1.1.3.cmml"><mi id="S2.SS4.SSS2.p2.1.m1.1.1.3.2" xref="S2.SS4.SSS2.p2.1.m1.1.1.3.2.cmml">g</mi><mo id="S2.SS4.SSS2.p2.1.m1.1.1.3.1" xref="S2.SS4.SSS2.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS4.SSS2.p2.1.m1.1.1.3.3" xref="S2.SS4.SSS2.p2.1.m1.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.1.m1.1b"><apply id="S2.SS4.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.1.m1.1.1.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p2.1.m1.1.1.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.1.1.2">𝑅</ci><apply id="S2.SS4.SSS2.p2.1.m1.1.1.3.cmml" xref="S2.SS4.SSS2.p2.1.m1.1.1.3"><times id="S2.SS4.SSS2.p2.1.m1.1.1.3.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.1.1.3.1"></times><ci id="S2.SS4.SSS2.p2.1.m1.1.1.3.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.1.1.3.2">𝑔</ci><ci id="S2.SS4.SSS2.p2.1.m1.1.1.3.3.cmml" xref="S2.SS4.SSS2.p2.1.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.1.m1.1c">{R_{gt}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p2.1.m1.1d">italic_R start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and translation <math alttext="{t_{gt}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p2.2.m2.1"><semantics id="S2.SS4.SSS2.p2.2.m2.1a"><msub id="S2.SS4.SSS2.p2.2.m2.1.1" xref="S2.SS4.SSS2.p2.2.m2.1.1.cmml"><mi id="S2.SS4.SSS2.p2.2.m2.1.1.2" xref="S2.SS4.SSS2.p2.2.m2.1.1.2.cmml">t</mi><mrow id="S2.SS4.SSS2.p2.2.m2.1.1.3" xref="S2.SS4.SSS2.p2.2.m2.1.1.3.cmml"><mi id="S2.SS4.SSS2.p2.2.m2.1.1.3.2" xref="S2.SS4.SSS2.p2.2.m2.1.1.3.2.cmml">g</mi><mo id="S2.SS4.SSS2.p2.2.m2.1.1.3.1" xref="S2.SS4.SSS2.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S2.SS4.SSS2.p2.2.m2.1.1.3.3" xref="S2.SS4.SSS2.p2.2.m2.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.2.m2.1b"><apply id="S2.SS4.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.2.m2.1.1.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p2.2.m2.1.1.2.cmml" xref="S2.SS4.SSS2.p2.2.m2.1.1.2">𝑡</ci><apply id="S2.SS4.SSS2.p2.2.m2.1.1.3.cmml" xref="S2.SS4.SSS2.p2.2.m2.1.1.3"><times id="S2.SS4.SSS2.p2.2.m2.1.1.3.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.1.1.3.1"></times><ci id="S2.SS4.SSS2.p2.2.m2.1.1.3.2.cmml" xref="S2.SS4.SSS2.p2.2.m2.1.1.3.2">𝑔</ci><ci id="S2.SS4.SSS2.p2.2.m2.1.1.3.3.cmml" xref="S2.SS4.SSS2.p2.2.m2.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.2.m2.1c">{t_{gt}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p2.2.m2.1d">italic_t start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, as well as the estimated rotation <math alttext="R" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p2.3.m3.1"><semantics id="S2.SS4.SSS2.p2.3.m3.1a"><mi id="S2.SS4.SSS2.p2.3.m3.1.1" xref="S2.SS4.SSS2.p2.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.3.m3.1b"><ci id="S2.SS4.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS4.SSS2.p2.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.3.m3.1c">R</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p2.3.m3.1d">italic_R</annotation></semantics></math> and translation <math alttext="t" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p2.4.m4.1"><semantics id="S2.SS4.SSS2.p2.4.m4.1a"><mi id="S2.SS4.SSS2.p2.4.m4.1.1" xref="S2.SS4.SSS2.p2.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.4.m4.1b"><ci id="S2.SS4.SSS2.p2.4.m4.1.1.cmml" xref="S2.SS4.SSS2.p2.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p2.4.m4.1d">italic_t</annotation></semantics></math>, ADD calculates the average pairwise distance between the 3D model points <math alttext="{x\in O}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p2.5.m5.1"><semantics id="S2.SS4.SSS2.p2.5.m5.1a"><mrow id="S2.SS4.SSS2.p2.5.m5.1.1" xref="S2.SS4.SSS2.p2.5.m5.1.1.cmml"><mi id="S2.SS4.SSS2.p2.5.m5.1.1.2" xref="S2.SS4.SSS2.p2.5.m5.1.1.2.cmml">x</mi><mo id="S2.SS4.SSS2.p2.5.m5.1.1.1" xref="S2.SS4.SSS2.p2.5.m5.1.1.1.cmml">∈</mo><mi id="S2.SS4.SSS2.p2.5.m5.1.1.3" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.cmml">O</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.5.m5.1b"><apply id="S2.SS4.SSS2.p2.5.m5.1.1.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1"><in id="S2.SS4.SSS2.p2.5.m5.1.1.1.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1.1"></in><ci id="S2.SS4.SSS2.p2.5.m5.1.1.2.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1.2">𝑥</ci><ci id="S2.SS4.SSS2.p2.5.m5.1.1.3.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.5.m5.1c">{x\in O}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p2.5.m5.1d">italic_x ∈ italic_O</annotation></semantics></math> corresponding to the transformation between the ground truth and estimated pose:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="ADD=\mathop{avg}\limits_{x\in O}\left\|{\left({{R_{gt}}x+{t_{gt}}}\right)-%
\left({Rx+t}\right)}\right\|.\vspace{-0.5em}" class="ltx_Math" display="block" id="S2.E6.m1.1"><semantics id="S2.E6.m1.1a"><mrow id="S2.E6.m1.1.1.1" xref="S2.E6.m1.1.1.1.1.cmml"><mrow id="S2.E6.m1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.cmml"><mrow id="S2.E6.m1.1.1.1.1.3" xref="S2.E6.m1.1.1.1.1.3.cmml"><mi id="S2.E6.m1.1.1.1.1.3.2" xref="S2.E6.m1.1.1.1.1.3.2.cmml">A</mi><mo id="S2.E6.m1.1.1.1.1.3.1" xref="S2.E6.m1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.E6.m1.1.1.1.1.3.3" xref="S2.E6.m1.1.1.1.1.3.3.cmml">D</mi><mo id="S2.E6.m1.1.1.1.1.3.1a" xref="S2.E6.m1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.E6.m1.1.1.1.1.3.4" xref="S2.E6.m1.1.1.1.1.3.4.cmml">D</mi></mrow><mo id="S2.E6.m1.1.1.1.1.2" xref="S2.E6.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E6.m1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.cmml"><munder id="S2.E6.m1.1.1.1.1.1.2" xref="S2.E6.m1.1.1.1.1.1.2.cmml"><mrow id="S2.E6.m1.1.1.1.1.1.2.2" xref="S2.E6.m1.1.1.1.1.1.2.2.cmml"><mi id="S2.E6.m1.1.1.1.1.1.2.2.2" xref="S2.E6.m1.1.1.1.1.1.2.2.2.cmml">a</mi><mo id="S2.E6.m1.1.1.1.1.1.2.2.1" xref="S2.E6.m1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S2.E6.m1.1.1.1.1.1.2.2.3" xref="S2.E6.m1.1.1.1.1.1.2.2.3.cmml">v</mi><mo id="S2.E6.m1.1.1.1.1.1.2.2.1a" xref="S2.E6.m1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S2.E6.m1.1.1.1.1.1.2.2.4" xref="S2.E6.m1.1.1.1.1.1.2.2.4.cmml">g</mi></mrow><mrow id="S2.E6.m1.1.1.1.1.1.2.3" xref="S2.E6.m1.1.1.1.1.1.2.3.cmml"><mi id="S2.E6.m1.1.1.1.1.1.2.3.2" xref="S2.E6.m1.1.1.1.1.1.2.3.2.cmml">x</mi><mo id="S2.E6.m1.1.1.1.1.1.2.3.1" xref="S2.E6.m1.1.1.1.1.1.2.3.1.cmml">∈</mo><mi id="S2.E6.m1.1.1.1.1.1.2.3.3" xref="S2.E6.m1.1.1.1.1.1.2.3.3.cmml">O</mi></mrow></munder><mrow id="S2.E6.m1.1.1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.2.cmml"><mo id="S2.E6.m1.1.1.1.1.1.1.1.2" xref="S2.E6.m1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">R</mi><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">g</mi><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">⁢</mo><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">t</mi></mrow></msub><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">x</mi></mrow><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">g</mi><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.cmml"><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.cmml">(</mo><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.cmml"><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml"><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.cmml">R</mi><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml">⁢</mo><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.3.cmml">x</mi></mrow><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml">+</mo><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.3.cmml">t</mi></mrow><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E6.m1.1.1.1.1.1.1.1.3" xref="S2.E6.m1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow></mrow></mrow><mo id="S2.E6.m1.1.1.1.2" lspace="0em" xref="S2.E6.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E6.m1.1b"><apply id="S2.E6.m1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1"><eq id="S2.E6.m1.1.1.1.1.2.cmml" xref="S2.E6.m1.1.1.1.1.2"></eq><apply id="S2.E6.m1.1.1.1.1.3.cmml" xref="S2.E6.m1.1.1.1.1.3"><times id="S2.E6.m1.1.1.1.1.3.1.cmml" xref="S2.E6.m1.1.1.1.1.3.1"></times><ci id="S2.E6.m1.1.1.1.1.3.2.cmml" xref="S2.E6.m1.1.1.1.1.3.2">𝐴</ci><ci id="S2.E6.m1.1.1.1.1.3.3.cmml" xref="S2.E6.m1.1.1.1.1.3.3">𝐷</ci><ci id="S2.E6.m1.1.1.1.1.3.4.cmml" xref="S2.E6.m1.1.1.1.1.3.4">𝐷</ci></apply><apply id="S2.E6.m1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1"><apply id="S2.E6.m1.1.1.1.1.1.2.cmml" xref="S2.E6.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.1.1.2.1.cmml" xref="S2.E6.m1.1.1.1.1.1.2">subscript</csymbol><apply id="S2.E6.m1.1.1.1.1.1.2.2.cmml" xref="S2.E6.m1.1.1.1.1.1.2.2"><times id="S2.E6.m1.1.1.1.1.1.2.2.1.cmml" xref="S2.E6.m1.1.1.1.1.1.2.2.1"></times><ci id="S2.E6.m1.1.1.1.1.1.2.2.2.cmml" xref="S2.E6.m1.1.1.1.1.1.2.2.2">𝑎</ci><ci id="S2.E6.m1.1.1.1.1.1.2.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.2.2.3">𝑣</ci><ci id="S2.E6.m1.1.1.1.1.1.2.2.4.cmml" xref="S2.E6.m1.1.1.1.1.1.2.2.4">𝑔</ci></apply><apply id="S2.E6.m1.1.1.1.1.1.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.2.3"><in id="S2.E6.m1.1.1.1.1.1.2.3.1.cmml" xref="S2.E6.m1.1.1.1.1.1.2.3.1"></in><ci id="S2.E6.m1.1.1.1.1.1.2.3.2.cmml" xref="S2.E6.m1.1.1.1.1.1.2.3.2">𝑥</ci><ci id="S2.E6.m1.1.1.1.1.1.2.3.3.cmml" xref="S2.E6.m1.1.1.1.1.1.2.3.3">𝑂</ci></apply></apply><apply id="S2.E6.m1.1.1.1.1.1.1.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E6.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1"><minus id="S2.E6.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.3"></minus><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1"><plus id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2"><times id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑅</ci><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3"><times id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1"></times><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2">𝑔</ci><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3">𝑡</ci></apply></apply><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑥</ci></apply><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑡</ci><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3"><times id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2">𝑔</ci><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1"><plus id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.1"></plus><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2"><times id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.1"></times><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.2">𝑅</ci><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.2.3">𝑥</ci></apply><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.1.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m1.1c">ADD=\mathop{avg}\limits_{x\in O}\left\|{\left({{R_{gt}}x+{t_{gt}}}\right)-%
\left({Rx+t}\right)}\right\|.\vspace{-0.5em}</annotation><annotation encoding="application/x-llamapun" id="S2.E6.m1.1d">italic_A italic_D italic_D = start_BIGOP italic_a italic_v italic_g end_BIGOP start_POSTSUBSCRIPT italic_x ∈ italic_O end_POSTSUBSCRIPT ∥ ( italic_R start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT italic_x + italic_t start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT ) - ( italic_R italic_x + italic_t ) ∥ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.SSS2.p2.6">For symmetric objects, the matching between points in certain views is inherently ambiguous. Therefore, the average distance is calculated using the nearest point distance as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="ADD{\rm{-}}S=\mathop{avg}\limits_{{x_{1}}\in O}\mathop{\min}\limits_{{x_{2}}%
\in O}\left\|{\left({{R_{gt}}{x_{1}}+{t_{gt}}}\right)-\left({R{x_{2}}+t}\right%
)}\right\|.\vspace{-0.5em}" class="ltx_Math" display="block" id="S2.E7.m1.1"><semantics id="S2.E7.m1.1a"><mrow id="S2.E7.m1.1.1.1" xref="S2.E7.m1.1.1.1.1.cmml"><mrow id="S2.E7.m1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.cmml"><mrow id="S2.E7.m1.1.1.1.1.3" xref="S2.E7.m1.1.1.1.1.3.cmml"><mrow id="S2.E7.m1.1.1.1.1.3.2" xref="S2.E7.m1.1.1.1.1.3.2.cmml"><mi id="S2.E7.m1.1.1.1.1.3.2.2" xref="S2.E7.m1.1.1.1.1.3.2.2.cmml">A</mi><mo id="S2.E7.m1.1.1.1.1.3.2.1" xref="S2.E7.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S2.E7.m1.1.1.1.1.3.2.3" xref="S2.E7.m1.1.1.1.1.3.2.3.cmml">D</mi><mo id="S2.E7.m1.1.1.1.1.3.2.1a" xref="S2.E7.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S2.E7.m1.1.1.1.1.3.2.4" xref="S2.E7.m1.1.1.1.1.3.2.4.cmml">D</mi></mrow><mo id="S2.E7.m1.1.1.1.1.3.1" xref="S2.E7.m1.1.1.1.1.3.1.cmml">−</mo><mi id="S2.E7.m1.1.1.1.1.3.3" xref="S2.E7.m1.1.1.1.1.3.3.cmml">S</mi></mrow><mo id="S2.E7.m1.1.1.1.1.2" xref="S2.E7.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E7.m1.1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.1.cmml"><munder id="S2.E7.m1.1.1.1.1.1.2" xref="S2.E7.m1.1.1.1.1.1.2.cmml"><mrow id="S2.E7.m1.1.1.1.1.1.2.2" xref="S2.E7.m1.1.1.1.1.1.2.2.cmml"><mi id="S2.E7.m1.1.1.1.1.1.2.2.2" xref="S2.E7.m1.1.1.1.1.1.2.2.2.cmml">a</mi><mo id="S2.E7.m1.1.1.1.1.1.2.2.1" xref="S2.E7.m1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S2.E7.m1.1.1.1.1.1.2.2.3" xref="S2.E7.m1.1.1.1.1.1.2.2.3.cmml">v</mi><mo id="S2.E7.m1.1.1.1.1.1.2.2.1a" xref="S2.E7.m1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S2.E7.m1.1.1.1.1.1.2.2.4" xref="S2.E7.m1.1.1.1.1.1.2.2.4.cmml">g</mi></mrow><mrow id="S2.E7.m1.1.1.1.1.1.2.3" xref="S2.E7.m1.1.1.1.1.1.2.3.cmml"><msub id="S2.E7.m1.1.1.1.1.1.2.3.2" xref="S2.E7.m1.1.1.1.1.1.2.3.2.cmml"><mi id="S2.E7.m1.1.1.1.1.1.2.3.2.2" xref="S2.E7.m1.1.1.1.1.1.2.3.2.2.cmml">x</mi><mn id="S2.E7.m1.1.1.1.1.1.2.3.2.3" xref="S2.E7.m1.1.1.1.1.1.2.3.2.3.cmml">1</mn></msub><mo id="S2.E7.m1.1.1.1.1.1.2.3.1" xref="S2.E7.m1.1.1.1.1.1.2.3.1.cmml">∈</mo><mi id="S2.E7.m1.1.1.1.1.1.2.3.3" xref="S2.E7.m1.1.1.1.1.1.2.3.3.cmml">O</mi></mrow></munder><mrow id="S2.E7.m1.1.1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.1.1.cmml"><munder id="S2.E7.m1.1.1.1.1.1.1.2" xref="S2.E7.m1.1.1.1.1.1.1.2.cmml"><mo id="S2.E7.m1.1.1.1.1.1.1.2.2" lspace="0.167em" movablelimits="false" rspace="0em" xref="S2.E7.m1.1.1.1.1.1.1.2.2.cmml">min</mo><mrow id="S2.E7.m1.1.1.1.1.1.1.2.3" xref="S2.E7.m1.1.1.1.1.1.1.2.3.cmml"><msub id="S2.E7.m1.1.1.1.1.1.1.2.3.2" xref="S2.E7.m1.1.1.1.1.1.1.2.3.2.cmml"><mi id="S2.E7.m1.1.1.1.1.1.1.2.3.2.2" xref="S2.E7.m1.1.1.1.1.1.1.2.3.2.2.cmml">x</mi><mn id="S2.E7.m1.1.1.1.1.1.1.2.3.2.3" xref="S2.E7.m1.1.1.1.1.1.1.2.3.2.3.cmml">2</mn></msub><mo id="S2.E7.m1.1.1.1.1.1.1.2.3.1" xref="S2.E7.m1.1.1.1.1.1.1.2.3.1.cmml">∈</mo><mi id="S2.E7.m1.1.1.1.1.1.1.2.3.3" xref="S2.E7.m1.1.1.1.1.1.1.2.3.3.cmml">O</mi></mrow></munder><mrow id="S2.E7.m1.1.1.1.1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.1.1.1.2.cmml"><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.2" xref="S2.E7.m1.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E7.m1.1.1.1.1.1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">R</mi><mrow id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">g</mi><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">⁢</mo><mi id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">t</mi></mrow></msub><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">x</mi><mn id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></msub></mrow><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mrow id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">g</mi><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.cmml"><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.cmml">(</mo><mrow id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.cmml"><mrow id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.cmml"><mi id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.2.cmml">R</mi><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml">⁢</mo><msub id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.cmml"><mi id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.2" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.2.cmml">x</mi><mn id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.3.cmml">2</mn></msub></mrow><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml">+</mo><mi id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.3.cmml">t</mi></mrow><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.3" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E7.m1.1.1.1.1.1.1.1.1.3" xref="S2.E7.m1.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow></mrow></mrow></mrow><mo id="S2.E7.m1.1.1.1.2" lspace="0em" xref="S2.E7.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E7.m1.1b"><apply id="S2.E7.m1.1.1.1.1.cmml" xref="S2.E7.m1.1.1.1"><eq id="S2.E7.m1.1.1.1.1.2.cmml" xref="S2.E7.m1.1.1.1.1.2"></eq><apply id="S2.E7.m1.1.1.1.1.3.cmml" xref="S2.E7.m1.1.1.1.1.3"><minus id="S2.E7.m1.1.1.1.1.3.1.cmml" xref="S2.E7.m1.1.1.1.1.3.1"></minus><apply id="S2.E7.m1.1.1.1.1.3.2.cmml" xref="S2.E7.m1.1.1.1.1.3.2"><times id="S2.E7.m1.1.1.1.1.3.2.1.cmml" xref="S2.E7.m1.1.1.1.1.3.2.1"></times><ci id="S2.E7.m1.1.1.1.1.3.2.2.cmml" xref="S2.E7.m1.1.1.1.1.3.2.2">𝐴</ci><ci id="S2.E7.m1.1.1.1.1.3.2.3.cmml" xref="S2.E7.m1.1.1.1.1.3.2.3">𝐷</ci><ci id="S2.E7.m1.1.1.1.1.3.2.4.cmml" xref="S2.E7.m1.1.1.1.1.3.2.4">𝐷</ci></apply><ci id="S2.E7.m1.1.1.1.1.3.3.cmml" xref="S2.E7.m1.1.1.1.1.3.3">𝑆</ci></apply><apply id="S2.E7.m1.1.1.1.1.1.cmml" xref="S2.E7.m1.1.1.1.1.1"><apply id="S2.E7.m1.1.1.1.1.1.2.cmml" xref="S2.E7.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E7.m1.1.1.1.1.1.2.1.cmml" xref="S2.E7.m1.1.1.1.1.1.2">subscript</csymbol><apply id="S2.E7.m1.1.1.1.1.1.2.2.cmml" xref="S2.E7.m1.1.1.1.1.1.2.2"><times id="S2.E7.m1.1.1.1.1.1.2.2.1.cmml" xref="S2.E7.m1.1.1.1.1.1.2.2.1"></times><ci id="S2.E7.m1.1.1.1.1.1.2.2.2.cmml" xref="S2.E7.m1.1.1.1.1.1.2.2.2">𝑎</ci><ci id="S2.E7.m1.1.1.1.1.1.2.2.3.cmml" xref="S2.E7.m1.1.1.1.1.1.2.2.3">𝑣</ci><ci id="S2.E7.m1.1.1.1.1.1.2.2.4.cmml" xref="S2.E7.m1.1.1.1.1.1.2.2.4">𝑔</ci></apply><apply id="S2.E7.m1.1.1.1.1.1.2.3.cmml" xref="S2.E7.m1.1.1.1.1.1.2.3"><in id="S2.E7.m1.1.1.1.1.1.2.3.1.cmml" xref="S2.E7.m1.1.1.1.1.1.2.3.1"></in><apply id="S2.E7.m1.1.1.1.1.1.2.3.2.cmml" xref="S2.E7.m1.1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S2.E7.m1.1.1.1.1.1.2.3.2.1.cmml" xref="S2.E7.m1.1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S2.E7.m1.1.1.1.1.1.2.3.2.2.cmml" xref="S2.E7.m1.1.1.1.1.1.2.3.2.2">𝑥</ci><cn id="S2.E7.m1.1.1.1.1.1.2.3.2.3.cmml" type="integer" xref="S2.E7.m1.1.1.1.1.1.2.3.2.3">1</cn></apply><ci id="S2.E7.m1.1.1.1.1.1.2.3.3.cmml" xref="S2.E7.m1.1.1.1.1.1.2.3.3">𝑂</ci></apply></apply><apply id="S2.E7.m1.1.1.1.1.1.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1"><apply id="S2.E7.m1.1.1.1.1.1.1.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E7.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.2">subscript</csymbol><min id="S2.E7.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.2.2"></min><apply id="S2.E7.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.2.3"><in id="S2.E7.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.2.3.1"></in><apply id="S2.E7.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S2.E7.m1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S2.E7.m1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.2.3.2.2">𝑥</ci><cn id="S2.E7.m1.1.1.1.1.1.1.2.3.2.3.cmml" type="integer" xref="S2.E7.m1.1.1.1.1.1.1.2.3.2.3">2</cn></apply><ci id="S2.E7.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.2.3.3">𝑂</ci></apply></apply><apply id="S2.E7.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E7.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1"><minus id="S2.E7.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.3"></minus><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1"><plus id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2"><times id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑅</ci><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3"><times id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1"></times><ci id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2">𝑔</ci><ci id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3">𝑡</ci></apply></apply><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝑥</ci><cn id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑡</ci><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3"><times id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2">𝑔</ci><ci id="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1"><plus id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.1"></plus><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2"><times id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.1"></times><ci id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.2">𝑅</ci><apply id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3"><csymbol cd="ambiguous" id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3">subscript</csymbol><ci id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.2.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.2">𝑥</ci><cn id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.3.cmml" type="integer" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.3.3">2</cn></apply></apply><ci id="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.3.cmml" xref="S2.E7.m1.1.1.1.1.1.1.1.1.1.2.1.1.3">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E7.m1.1c">ADD{\rm{-}}S=\mathop{avg}\limits_{{x_{1}}\in O}\mathop{\min}\limits_{{x_{2}}%
\in O}\left\|{\left({{R_{gt}}{x_{1}}+{t_{gt}}}\right)-\left({R{x_{2}}+t}\right%
)}\right\|.\vspace{-0.5em}</annotation><annotation encoding="application/x-llamapun" id="S2.E7.m1.1d">italic_A italic_D italic_D - italic_S = start_BIGOP italic_a italic_v italic_g end_BIGOP start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∈ italic_O end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ italic_O end_POSTSUBSCRIPT ∥ ( italic_R start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_t start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT ) - ( italic_R italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_t ) ∥ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.SSS2.p2.7">Meanwhile, the area under the ADD and the ADD-S curve (AUC) are often leveraged for evaluation. Specifically, if the ADD and ADD-S are smaller than a given threshold, the predicted pose will be considered correct. Moreover, there are many methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib70" title="">70</a>]</cite> that evaluate asymmetric and symmetric objects using ADD and ADD-S, respectively. This metric is termed ADD(S).</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p3">
<p class="ltx_p" id="S2.SS4.SSS2.p3.4">In addition, <math alttext="n^{\circ}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p3.1.m1.1"><semantics id="S2.SS4.SSS2.p3.1.m1.1a"><msup id="S2.SS4.SSS2.p3.1.m1.1.1" xref="S2.SS4.SSS2.p3.1.m1.1.1.cmml"><mi id="S2.SS4.SSS2.p3.1.m1.1.1.2" xref="S2.SS4.SSS2.p3.1.m1.1.1.2.cmml">n</mi><mo id="S2.SS4.SSS2.p3.1.m1.1.1.3" xref="S2.SS4.SSS2.p3.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p3.1.m1.1b"><apply id="S2.SS4.SSS2.p3.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p3.1.m1.1.1.1.cmml" xref="S2.SS4.SSS2.p3.1.m1.1.1">superscript</csymbol><ci id="S2.SS4.SSS2.p3.1.m1.1.1.2.cmml" xref="S2.SS4.SSS2.p3.1.m1.1.1.2">𝑛</ci><compose id="S2.SS4.SSS2.p3.1.m1.1.1.3.cmml" xref="S2.SS4.SSS2.p3.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p3.1.m1.1c">n^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p3.1.m1.1d">italic_n start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math><math alttext="m{\rm{cm}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p3.2.m2.1"><semantics id="S2.SS4.SSS2.p3.2.m2.1a"><mrow id="S2.SS4.SSS2.p3.2.m2.1.1" xref="S2.SS4.SSS2.p3.2.m2.1.1.cmml"><mi id="S2.SS4.SSS2.p3.2.m2.1.1.2" xref="S2.SS4.SSS2.p3.2.m2.1.1.2.cmml">m</mi><mo id="S2.SS4.SSS2.p3.2.m2.1.1.1" xref="S2.SS4.SSS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S2.SS4.SSS2.p3.2.m2.1.1.3" xref="S2.SS4.SSS2.p3.2.m2.1.1.3.cmml">cm</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p3.2.m2.1b"><apply id="S2.SS4.SSS2.p3.2.m2.1.1.cmml" xref="S2.SS4.SSS2.p3.2.m2.1.1"><times id="S2.SS4.SSS2.p3.2.m2.1.1.1.cmml" xref="S2.SS4.SSS2.p3.2.m2.1.1.1"></times><ci id="S2.SS4.SSS2.p3.2.m2.1.1.2.cmml" xref="S2.SS4.SSS2.p3.2.m2.1.1.2">𝑚</ci><ci id="S2.SS4.SSS2.p3.2.m2.1.1.3.cmml" xref="S2.SS4.SSS2.p3.2.m2.1.1.3">cm</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p3.2.m2.1c">m{\rm{cm}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p3.2.m2.1d">italic_m roman_cm</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib71" title="">71</a>]</cite> is also currently a prevalent evaluation metric (especially in category-level object pose estimation). It directly quantifies the errors in predicted 3D rotation and 3D translation. An object pose prediction is deemed correct if its rotation error is below threshold <math alttext="n^{\circ}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p3.3.m3.1"><semantics id="S2.SS4.SSS2.p3.3.m3.1a"><msup id="S2.SS4.SSS2.p3.3.m3.1.1" xref="S2.SS4.SSS2.p3.3.m3.1.1.cmml"><mi id="S2.SS4.SSS2.p3.3.m3.1.1.2" xref="S2.SS4.SSS2.p3.3.m3.1.1.2.cmml">n</mi><mo id="S2.SS4.SSS2.p3.3.m3.1.1.3" xref="S2.SS4.SSS2.p3.3.m3.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p3.3.m3.1b"><apply id="S2.SS4.SSS2.p3.3.m3.1.1.cmml" xref="S2.SS4.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p3.3.m3.1.1.1.cmml" xref="S2.SS4.SSS2.p3.3.m3.1.1">superscript</csymbol><ci id="S2.SS4.SSS2.p3.3.m3.1.1.2.cmml" xref="S2.SS4.SSS2.p3.3.m3.1.1.2">𝑛</ci><compose id="S2.SS4.SSS2.p3.3.m3.1.1.3.cmml" xref="S2.SS4.SSS2.p3.3.m3.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p3.3.m3.1c">n^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p3.3.m3.1d">italic_n start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math> and its translation error is below threshold <math alttext="m{\rm{cm}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p3.4.m4.1"><semantics id="S2.SS4.SSS2.p3.4.m4.1a"><mrow id="S2.SS4.SSS2.p3.4.m4.1.1" xref="S2.SS4.SSS2.p3.4.m4.1.1.cmml"><mi id="S2.SS4.SSS2.p3.4.m4.1.1.2" xref="S2.SS4.SSS2.p3.4.m4.1.1.2.cmml">m</mi><mo id="S2.SS4.SSS2.p3.4.m4.1.1.1" xref="S2.SS4.SSS2.p3.4.m4.1.1.1.cmml">⁢</mo><mi id="S2.SS4.SSS2.p3.4.m4.1.1.3" xref="S2.SS4.SSS2.p3.4.m4.1.1.3.cmml">cm</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p3.4.m4.1b"><apply id="S2.SS4.SSS2.p3.4.m4.1.1.cmml" xref="S2.SS4.SSS2.p3.4.m4.1.1"><times id="S2.SS4.SSS2.p3.4.m4.1.1.1.cmml" xref="S2.SS4.SSS2.p3.4.m4.1.1.1"></times><ci id="S2.SS4.SSS2.p3.4.m4.1.1.2.cmml" xref="S2.SS4.SSS2.p3.4.m4.1.1.2">𝑚</ci><ci id="S2.SS4.SSS2.p3.4.m4.1.1.3.cmml" xref="S2.SS4.SSS2.p3.4.m4.1.1.3">cm</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p3.4.m4.1c">m{\rm{cm}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p3.4.m4.1d">italic_m roman_cm</annotation></semantics></math>. It can be defined as an indicator function as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\vspace{-0.5em}{I_{{n^{\circ}}m{\rm{cm}}}}({e_{R}},{e_{t}})=\left\{{\begin{%
array}[]{*{20}{c}}{1,}\\
{0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20%
}{c}}{{e_{R}}&lt;{n^{\circ}}\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}{e_{t%
}}&lt;m{\rm{cm}}}\end{array}}\\
{{\rm{otherwise}}}\end{array}," class="ltx_Math" display="block" id="S2.E8.m1.3"><semantics id="S2.E8.m1.3a"><mrow id="S2.E8.m1.3.3.1" xref="S2.E8.m1.3.3.1.1.cmml"><mrow id="S2.E8.m1.3.3.1.1" xref="S2.E8.m1.3.3.1.1.cmml"><mrow id="S2.E8.m1.3.3.1.1.2" xref="S2.E8.m1.3.3.1.1.2.cmml"><msub id="S2.E8.m1.3.3.1.1.2.4" xref="S2.E8.m1.3.3.1.1.2.4.cmml"><mi id="S2.E8.m1.3.3.1.1.2.4.2" xref="S2.E8.m1.3.3.1.1.2.4.2.cmml">I</mi><mrow id="S2.E8.m1.3.3.1.1.2.4.3" xref="S2.E8.m1.3.3.1.1.2.4.3.cmml"><msup id="S2.E8.m1.3.3.1.1.2.4.3.2" xref="S2.E8.m1.3.3.1.1.2.4.3.2.cmml"><mi id="S2.E8.m1.3.3.1.1.2.4.3.2.2" xref="S2.E8.m1.3.3.1.1.2.4.3.2.2.cmml">n</mi><mo id="S2.E8.m1.3.3.1.1.2.4.3.2.3" xref="S2.E8.m1.3.3.1.1.2.4.3.2.3.cmml">∘</mo></msup><mo id="S2.E8.m1.3.3.1.1.2.4.3.1" xref="S2.E8.m1.3.3.1.1.2.4.3.1.cmml">⁢</mo><mi id="S2.E8.m1.3.3.1.1.2.4.3.3" xref="S2.E8.m1.3.3.1.1.2.4.3.3.cmml">m</mi><mo id="S2.E8.m1.3.3.1.1.2.4.3.1a" xref="S2.E8.m1.3.3.1.1.2.4.3.1.cmml">⁢</mo><mi id="S2.E8.m1.3.3.1.1.2.4.3.4" xref="S2.E8.m1.3.3.1.1.2.4.3.4.cmml">cm</mi></mrow></msub><mo id="S2.E8.m1.3.3.1.1.2.3" xref="S2.E8.m1.3.3.1.1.2.3.cmml">⁢</mo><mrow id="S2.E8.m1.3.3.1.1.2.2.2" xref="S2.E8.m1.3.3.1.1.2.2.3.cmml"><mo id="S2.E8.m1.3.3.1.1.2.2.2.3" stretchy="false" xref="S2.E8.m1.3.3.1.1.2.2.3.cmml">(</mo><msub id="S2.E8.m1.3.3.1.1.1.1.1.1" xref="S2.E8.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S2.E8.m1.3.3.1.1.1.1.1.1.2" xref="S2.E8.m1.3.3.1.1.1.1.1.1.2.cmml">e</mi><mi id="S2.E8.m1.3.3.1.1.1.1.1.1.3" xref="S2.E8.m1.3.3.1.1.1.1.1.1.3.cmml">R</mi></msub><mo id="S2.E8.m1.3.3.1.1.2.2.2.4" xref="S2.E8.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="S2.E8.m1.3.3.1.1.2.2.2.2" xref="S2.E8.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S2.E8.m1.3.3.1.1.2.2.2.2.2" xref="S2.E8.m1.3.3.1.1.2.2.2.2.2.cmml">e</mi><mi id="S2.E8.m1.3.3.1.1.2.2.2.2.3" xref="S2.E8.m1.3.3.1.1.2.2.2.2.3.cmml">t</mi></msub><mo id="S2.E8.m1.3.3.1.1.2.2.2.5" stretchy="false" xref="S2.E8.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E8.m1.3.3.1.1.3" xref="S2.E8.m1.3.3.1.1.3.cmml">=</mo><mrow id="S2.E8.m1.3.3.1.1.4" xref="S2.E8.m1.3.3.1.1.4.cmml"><mrow id="S2.E8.m1.3.3.1.1.4.2.2" xref="S2.E8.m1.3.3.1.1.4.2.1.cmml"><mo id="S2.E8.m1.3.3.1.1.4.2.2.1" xref="S2.E8.m1.3.3.1.1.4.2.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E8.m1.2.2" rowspacing="0pt" xref="S2.E8.m1.2.2.cmml"><mtr id="S2.E8.m1.2.2a" xref="S2.E8.m1.2.2.cmml"><mtd id="S2.E8.m1.2.2b" xref="S2.E8.m1.2.2.cmml"><mrow id="S2.E8.m1.1.1.1.1.1.3" xref="S2.E8.m1.2.2.cmml"><mn id="S2.E8.m1.1.1.1.1.1.1" xref="S2.E8.m1.1.1.1.1.1.1.cmml">1</mn><mo id="S2.E8.m1.1.1.1.1.1.3.1" xref="S2.E8.m1.2.2.cmml">,</mo></mrow></mtd><mtd id="S2.E8.m1.2.2c" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2d" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2e" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2f" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2g" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2h" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2i" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2j" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2k" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2l" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2m" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2n" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2o" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2p" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2q" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2r" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2s" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2t" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2u" xref="S2.E8.m1.2.2.cmml"></mtd></mtr><mtr id="S2.E8.m1.2.2v" xref="S2.E8.m1.2.2.cmml"><mtd id="S2.E8.m1.2.2w" xref="S2.E8.m1.2.2.cmml"><mrow id="S2.E8.m1.2.2.2.1.1.3" xref="S2.E8.m1.2.2.cmml"><mn id="S2.E8.m1.2.2.2.1.1.1" xref="S2.E8.m1.2.2.2.1.1.1.cmml">0</mn><mo id="S2.E8.m1.2.2.2.1.1.3.1" xref="S2.E8.m1.2.2.cmml">,</mo></mrow></mtd><mtd id="S2.E8.m1.2.2x" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2y" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2z" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2aa" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2ab" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2ac" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2ad" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2ae" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2af" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2ag" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2ah" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2ai" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2aj" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2ak" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2al" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2am" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2an" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2ao" xref="S2.E8.m1.2.2.cmml"></mtd><mtd id="S2.E8.m1.2.2ap" xref="S2.E8.m1.2.2.cmml"></mtd></mtr></mtable><mi id="S2.E8.m1.3.3.1.1.4.2.2.2" xref="S2.E8.m1.3.3.1.1.4.2.1.1.cmml"></mi></mrow><mo id="S2.E8.m1.3.3.1.1.4.1" lspace="0.167em" xref="S2.E8.m1.3.3.1.1.4.1.cmml">⁢</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E8.m1.3.3.1.1.4.3" rowspacing="0pt" xref="S2.E8.m1.3.3.1.1.4.3.cmml"><mtr id="S2.E8.m1.3.3.1.1.4.3a" xref="S2.E8.m1.3.3.1.1.4.3.cmml"><mtd id="S2.E8.m1.3.3.1.1.4.3b" xref="S2.E8.m1.3.3.1.1.4.3.cmml"><mrow id="S2.E8.m1.3.3.1.1.4.3.1.1.1" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.cmml"><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.2.cmml">if</mi><mo id="S2.E8.m1.3.3.1.1.4.3.1.1.1.1" lspace="0.167em" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.1.cmml">⁢</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"><mtr id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3a" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3b" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"><mrow id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.cmml"><msub id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.cmml"><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.2.cmml">e</mi><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.3.cmml">R</mi></msub><mo id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.3.cmml">&lt;</mo><mrow id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.cmml"><msup id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.cmml"><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.2.cmml">n</mi><mo id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.3.cmml">∘</mo></msup><mo id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.1" lspace="0.167em" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.1.cmml">⁢</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"><mtr id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3a" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3b" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.1.1.1" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.1.1.1.cmml">and</mi></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3c" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3d" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3e" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3f" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3g" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3h" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3i" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3j" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3k" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3l" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3m" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3n" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3o" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3p" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3q" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3r" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3s" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3t" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3u" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"></mtd></mtr></mtable><mo id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.1a" lspace="0.167em" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.1.cmml">⁢</mo><msub id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.cmml"><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.2.cmml">e</mi><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.3.cmml">t</mi></msub></mrow><mo id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.5" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.5.cmml">&lt;</mo><mrow id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.cmml"><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.2.cmml">m</mi><mo id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.1" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.1.cmml">⁢</mo><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.3.cmml">cm</mi></mrow></mrow></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3c" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3d" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3e" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3f" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3g" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3h" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3i" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3j" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3k" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3l" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3m" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3n" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3o" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3p" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3q" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3r" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3s" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3t" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3u" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"></mtd></mtr></mtable></mrow></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3c" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3d" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3e" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3f" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3g" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3h" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3i" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3j" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3k" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3l" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3m" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3n" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3o" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3p" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3q" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3r" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3s" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3t" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3u" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd></mtr><mtr id="S2.E8.m1.3.3.1.1.4.3v" xref="S2.E8.m1.3.3.1.1.4.3.cmml"><mtd id="S2.E8.m1.3.3.1.1.4.3w" xref="S2.E8.m1.3.3.1.1.4.3.cmml"><mi id="S2.E8.m1.3.3.1.1.4.3.2.1.1" xref="S2.E8.m1.3.3.1.1.4.3.2.1.1.cmml">otherwise</mi></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3x" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3y" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3z" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3aa" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3ab" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3ac" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3ad" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3ae" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3af" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3ag" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3ah" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3ai" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3aj" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3ak" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3al" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3am" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3an" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3ao" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd><mtd id="S2.E8.m1.3.3.1.1.4.3ap" xref="S2.E8.m1.3.3.1.1.4.3.cmml"></mtd></mtr></mtable></mrow></mrow><mo id="S2.E8.m1.3.3.1.2" lspace="0.167em" xref="S2.E8.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E8.m1.3b"><apply id="S2.E8.m1.3.3.1.1.cmml" xref="S2.E8.m1.3.3.1"><eq id="S2.E8.m1.3.3.1.1.3.cmml" xref="S2.E8.m1.3.3.1.1.3"></eq><apply id="S2.E8.m1.3.3.1.1.2.cmml" xref="S2.E8.m1.3.3.1.1.2"><times id="S2.E8.m1.3.3.1.1.2.3.cmml" xref="S2.E8.m1.3.3.1.1.2.3"></times><apply id="S2.E8.m1.3.3.1.1.2.4.cmml" xref="S2.E8.m1.3.3.1.1.2.4"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.2.4.1.cmml" xref="S2.E8.m1.3.3.1.1.2.4">subscript</csymbol><ci id="S2.E8.m1.3.3.1.1.2.4.2.cmml" xref="S2.E8.m1.3.3.1.1.2.4.2">𝐼</ci><apply id="S2.E8.m1.3.3.1.1.2.4.3.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3"><times id="S2.E8.m1.3.3.1.1.2.4.3.1.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.1"></times><apply id="S2.E8.m1.3.3.1.1.2.4.3.2.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.2"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.2.4.3.2.1.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.2">superscript</csymbol><ci id="S2.E8.m1.3.3.1.1.2.4.3.2.2.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.2.2">𝑛</ci><compose id="S2.E8.m1.3.3.1.1.2.4.3.2.3.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.2.3"></compose></apply><ci id="S2.E8.m1.3.3.1.1.2.4.3.3.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.3">𝑚</ci><ci id="S2.E8.m1.3.3.1.1.2.4.3.4.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.4">cm</ci></apply></apply><interval closure="open" id="S2.E8.m1.3.3.1.1.2.2.3.cmml" xref="S2.E8.m1.3.3.1.1.2.2.2"><apply id="S2.E8.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E8.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E8.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E8.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E8.m1.3.3.1.1.1.1.1.1.2">𝑒</ci><ci id="S2.E8.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E8.m1.3.3.1.1.1.1.1.1.3">𝑅</ci></apply><apply id="S2.E8.m1.3.3.1.1.2.2.2.2.cmml" xref="S2.E8.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S2.E8.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E8.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S2.E8.m1.3.3.1.1.2.2.2.2.2">𝑒</ci><ci id="S2.E8.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S2.E8.m1.3.3.1.1.2.2.2.2.3">𝑡</ci></apply></interval></apply><apply id="S2.E8.m1.3.3.1.1.4.cmml" xref="S2.E8.m1.3.3.1.1.4"><times id="S2.E8.m1.3.3.1.1.4.1.cmml" xref="S2.E8.m1.3.3.1.1.4.1"></times><apply id="S2.E8.m1.3.3.1.1.4.2.1.cmml" xref="S2.E8.m1.3.3.1.1.4.2.2"><csymbol cd="latexml" id="S2.E8.m1.3.3.1.1.4.2.1.1.cmml" xref="S2.E8.m1.3.3.1.1.4.2.2.1">cases</csymbol><matrix id="S2.E8.m1.2.2.cmml" xref="S2.E8.m1.2.2"><matrixrow id="S2.E8.m1.2.2a.cmml" xref="S2.E8.m1.2.2"><cn id="S2.E8.m1.1.1.1.1.1.1.cmml" type="integer" xref="S2.E8.m1.1.1.1.1.1.1">1</cn><cerror id="S2.E8.m1.2.2b.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2c.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2d.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2e.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2f.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2g.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2h.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2i.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2j.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2k.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2l.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2m.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2n.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2o.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2p.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2q.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2r.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2s.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2t.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2u.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2v.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2w.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2x.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2y.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2z.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2aa.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2ab.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ac.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2ad.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ae.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2af.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ag.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2ah.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ai.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2aj.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ak.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2al.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2am.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror></matrixrow><matrixrow id="S2.E8.m1.2.2an.cmml" xref="S2.E8.m1.2.2"><cn id="S2.E8.m1.2.2.2.1.1.1.cmml" type="integer" xref="S2.E8.m1.2.2.2.1.1.1">0</cn><cerror id="S2.E8.m1.2.2ao.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ap.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2aq.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ar.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2as.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2at.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2au.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2av.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2aw.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ax.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2ay.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2az.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2ba.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bb.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bc.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bd.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2be.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bf.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bg.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bh.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bi.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bj.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bk.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bl.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bm.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bn.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bo.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bp.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bq.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2br.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bs.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bt.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bu.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bv.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bw.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bx.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2by.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bz.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><matrix id="S2.E8.m1.3.3.1.1.4.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><matrixrow id="S2.E8.m1.3.3.1.1.4.3a.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><apply id="S2.E8.m1.3.3.1.1.4.3.1.1.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1"><times id="S2.E8.m1.3.3.1.1.4.3.1.1.1.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.1"></times><ci id="S2.E8.m1.3.3.1.1.4.3.1.1.1.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.2">if</ci><matrix id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><matrixrow id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3a.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><apply id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1"><and id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1a.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1"></and><apply id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1b.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1"><lt id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.3"></lt><apply id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2">subscript</csymbol><ci id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.2">𝑒</ci><ci id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.3">𝑅</ci></apply><apply id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4"><times id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.1"></times><apply id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2">superscript</csymbol><ci id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.2">𝑛</ci><compose id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.3"></compose></apply><matrix id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><matrixrow id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3a.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><ci id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.1.1.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.1.1.1">and</ci><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3b.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3c.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3d.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3e.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3f.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3g.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3h.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3i.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3j.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3k.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3l.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3m.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3n.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3o.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3p.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3q.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3r.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3s.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3t.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3u.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3v.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3w.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3x.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3y.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3z.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3aa.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ab.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ac.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ad.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ae.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3af.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ag.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ah.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ai.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3aj.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ak.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3al.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3am.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror></matrixrow></matrix><apply id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4">subscript</csymbol><ci id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.2">𝑒</ci><ci id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.3">𝑡</ci></apply></apply></apply><apply id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1c.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1"><lt id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.5.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.5"></lt><share href="https://arxiv.org/html/2405.07801v3#S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.cmml" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1d.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1"></share><apply id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6"><times id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.1"></times><ci id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.2">𝑚</ci><ci id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.3">cm</ci></apply></apply></apply><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3b.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3c.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3d.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3e.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3f.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3g.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3h.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3i.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3j.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3k.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3l.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3m.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3n.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3o.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3p.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3q.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3r.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3s.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3t.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3u.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3v.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3w.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3x.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3y.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3z.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3aa.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ab.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ac.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ad.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ae.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3af.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ag.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ah.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ai.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3aj.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ak.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3al.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3am.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><cerror id="S2.E8.m1.3.3.1.1.4.3b.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3c.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3d.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3e.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3f.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3g.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3h.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3i.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3j.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3k.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3l.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3m.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3n.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3o.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3p.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3q.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3r.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3s.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3t.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3u.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3v.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3w.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3x.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3y.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3z.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3aa.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3ab.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ac.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3ad.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ae.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3af.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ag.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3ah.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ai.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3aj.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ak.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3al.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3am.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror></matrixrow><matrixrow id="S2.E8.m1.3.3.1.1.4.3an.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><ci id="S2.E8.m1.3.3.1.1.4.3.2.1.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.2.1.1">otherwise</ci><cerror id="S2.E8.m1.3.3.1.1.4.3ao.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ap.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3aq.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ar.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3as.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3at.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3au.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3av.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3aw.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ax.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3ay.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3az.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3ba.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bb.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bc.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bd.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3be.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bf.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bg.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bh.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bi.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bj.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bk.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bl.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bm.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bn.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bo.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bp.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bq.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3br.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bs.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bt.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bu.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bv.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bw.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bx.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3by.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bz.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E8.m1.3c">\vspace{-0.5em}{I_{{n^{\circ}}m{\rm{cm}}}}({e_{R}},{e_{t}})=\left\{{\begin{%
array}[]{*{20}{c}}{1,}\\
{0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20%
}{c}}{{e_{R}}&lt;{n^{\circ}}\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}{e_{t%
}}&lt;m{\rm{cm}}}\end{array}}\\
{{\rm{otherwise}}}\end{array},</annotation><annotation encoding="application/x-llamapun" id="S2.E8.m1.3d">italic_I start_POSTSUBSCRIPT italic_n start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT italic_m roman_cm end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = { start_ARRAY start_ROW start_CELL 1 , end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL 0 , end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY start_ARRAY start_ROW start_CELL roman_if start_ARRAY start_ROW start_CELL italic_e start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT &lt; italic_n start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT start_ARRAY start_ROW start_CELL roman_and end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT &lt; italic_m roman_cm end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL roman_otherwise end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW end_ARRAY ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.SSS2.p3.6">where <math alttext="{{e_{R}}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p3.5.m1.1"><semantics id="S2.SS4.SSS2.p3.5.m1.1a"><msub id="S2.SS4.SSS2.p3.5.m1.1.1" xref="S2.SS4.SSS2.p3.5.m1.1.1.cmml"><mi id="S2.SS4.SSS2.p3.5.m1.1.1.2" xref="S2.SS4.SSS2.p3.5.m1.1.1.2.cmml">e</mi><mi id="S2.SS4.SSS2.p3.5.m1.1.1.3" xref="S2.SS4.SSS2.p3.5.m1.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p3.5.m1.1b"><apply id="S2.SS4.SSS2.p3.5.m1.1.1.cmml" xref="S2.SS4.SSS2.p3.5.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p3.5.m1.1.1.1.cmml" xref="S2.SS4.SSS2.p3.5.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p3.5.m1.1.1.2.cmml" xref="S2.SS4.SSS2.p3.5.m1.1.1.2">𝑒</ci><ci id="S2.SS4.SSS2.p3.5.m1.1.1.3.cmml" xref="S2.SS4.SSS2.p3.5.m1.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p3.5.m1.1c">{{e_{R}}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p3.5.m1.1d">italic_e start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="{{e_{t}}}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p3.6.m2.1"><semantics id="S2.SS4.SSS2.p3.6.m2.1a"><msub id="S2.SS4.SSS2.p3.6.m2.1.1" xref="S2.SS4.SSS2.p3.6.m2.1.1.cmml"><mi id="S2.SS4.SSS2.p3.6.m2.1.1.2" xref="S2.SS4.SSS2.p3.6.m2.1.1.2.cmml">e</mi><mi id="S2.SS4.SSS2.p3.6.m2.1.1.3" xref="S2.SS4.SSS2.p3.6.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p3.6.m2.1b"><apply id="S2.SS4.SSS2.p3.6.m2.1.1.cmml" xref="S2.SS4.SSS2.p3.6.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p3.6.m2.1.1.1.cmml" xref="S2.SS4.SSS2.p3.6.m2.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p3.6.m2.1.1.2.cmml" xref="S2.SS4.SSS2.p3.6.m2.1.1.2">𝑒</ci><ci id="S2.SS4.SSS2.p3.6.m2.1.1.3.cmml" xref="S2.SS4.SSS2.p3.6.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p3.6.m2.1c">{{e_{t}}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p3.6.m2.1d">italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> represent the rotation and translation errors between the estimated and ground-truth values, respectively.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p4">
<p class="ltx_p" id="S2.SS4.SSS2.p4.1">Furthermore, compared to directly comparing 6DoF pose in 3D space, the simplicity and practicality of the 2D Projection metric <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib38" title="">38</a>]</cite> make it suitable for evaluation as well, which quantifies the average distance between CAD model points when projected under the estimated object pose and the ground-truth pose. A pose is considered correct if the projected distances are less than 5 pixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.3 </span>9DoF Evaluation Metric</h4>
<div class="ltx_para" id="S2.SS4.SSS3.p1">
<p class="ltx_p" id="S2.SS4.SSS3.p1.1"><math alttext="Io{U_{3D}}" class="ltx_Math" display="inline" id="S2.SS4.SSS3.p1.1.m1.1"><semantics id="S2.SS4.SSS3.p1.1.m1.1a"><mrow id="S2.SS4.SSS3.p1.1.m1.1.1" xref="S2.SS4.SSS3.p1.1.m1.1.1.cmml"><mi id="S2.SS4.SSS3.p1.1.m1.1.1.2" xref="S2.SS4.SSS3.p1.1.m1.1.1.2.cmml">I</mi><mo id="S2.SS4.SSS3.p1.1.m1.1.1.1" xref="S2.SS4.SSS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.SS4.SSS3.p1.1.m1.1.1.3" xref="S2.SS4.SSS3.p1.1.m1.1.1.3.cmml">o</mi><mo id="S2.SS4.SSS3.p1.1.m1.1.1.1a" xref="S2.SS4.SSS3.p1.1.m1.1.1.1.cmml">⁢</mo><msub id="S2.SS4.SSS3.p1.1.m1.1.1.4" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.cmml"><mi id="S2.SS4.SSS3.p1.1.m1.1.1.4.2" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.2.cmml">U</mi><mrow id="S2.SS4.SSS3.p1.1.m1.1.1.4.3" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.3.cmml"><mn id="S2.SS4.SSS3.p1.1.m1.1.1.4.3.2" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.3.2.cmml">3</mn><mo id="S2.SS4.SSS3.p1.1.m1.1.1.4.3.1" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S2.SS4.SSS3.p1.1.m1.1.1.4.3.3" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.3.3.cmml">D</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.1.m1.1b"><apply id="S2.SS4.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1"><times id="S2.SS4.SSS3.p1.1.m1.1.1.1.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1.1"></times><ci id="S2.SS4.SSS3.p1.1.m1.1.1.2.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1.2">𝐼</ci><ci id="S2.SS4.SSS3.p1.1.m1.1.1.3.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1.3">𝑜</ci><apply id="S2.SS4.SSS3.p1.1.m1.1.1.4.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p1.1.m1.1.1.4.1.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1.4">subscript</csymbol><ci id="S2.SS4.SSS3.p1.1.m1.1.1.4.2.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.2">𝑈</ci><apply id="S2.SS4.SSS3.p1.1.m1.1.1.4.3.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.3"><times id="S2.SS4.SSS3.p1.1.m1.1.1.4.3.1.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.3.1"></times><cn id="S2.SS4.SSS3.p1.1.m1.1.1.4.3.2.cmml" type="integer" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.3.2">3</cn><ci id="S2.SS4.SSS3.p1.1.m1.1.1.4.3.3.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1.4.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.1.m1.1c">Io{U_{3D}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS3.p1.1.m1.1d">italic_I italic_o italic_U start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT</annotation></semantics></math> denotes the Intersection-over-Union (IoU)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib22" title="">22</a>]</cite> percentage between the ground-truth and predicted 3D bounding boxes, which can evaluate the 6DoF pose estimation as well as the 3DoF size estimation. It can be expressed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Io{U_{3D}}=\frac{{{P_{B}}\cap{G_{B}}}}{{{P_{B}}\cup{G_{B}}}},\vspace{-0.5em}" class="ltx_Math" display="block" id="S2.E9.m1.1"><semantics id="S2.E9.m1.1a"><mrow id="S2.E9.m1.1.1.1" xref="S2.E9.m1.1.1.1.1.cmml"><mrow id="S2.E9.m1.1.1.1.1" xref="S2.E9.m1.1.1.1.1.cmml"><mrow id="S2.E9.m1.1.1.1.1.2" xref="S2.E9.m1.1.1.1.1.2.cmml"><mi id="S2.E9.m1.1.1.1.1.2.2" xref="S2.E9.m1.1.1.1.1.2.2.cmml">I</mi><mo id="S2.E9.m1.1.1.1.1.2.1" xref="S2.E9.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.E9.m1.1.1.1.1.2.3" xref="S2.E9.m1.1.1.1.1.2.3.cmml">o</mi><mo id="S2.E9.m1.1.1.1.1.2.1a" xref="S2.E9.m1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S2.E9.m1.1.1.1.1.2.4" xref="S2.E9.m1.1.1.1.1.2.4.cmml"><mi id="S2.E9.m1.1.1.1.1.2.4.2" xref="S2.E9.m1.1.1.1.1.2.4.2.cmml">U</mi><mrow id="S2.E9.m1.1.1.1.1.2.4.3" xref="S2.E9.m1.1.1.1.1.2.4.3.cmml"><mn id="S2.E9.m1.1.1.1.1.2.4.3.2" xref="S2.E9.m1.1.1.1.1.2.4.3.2.cmml">3</mn><mo id="S2.E9.m1.1.1.1.1.2.4.3.1" xref="S2.E9.m1.1.1.1.1.2.4.3.1.cmml">⁢</mo><mi id="S2.E9.m1.1.1.1.1.2.4.3.3" xref="S2.E9.m1.1.1.1.1.2.4.3.3.cmml">D</mi></mrow></msub></mrow><mo id="S2.E9.m1.1.1.1.1.1" xref="S2.E9.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S2.E9.m1.1.1.1.1.3" xref="S2.E9.m1.1.1.1.1.3.cmml"><mrow id="S2.E9.m1.1.1.1.1.3.2" xref="S2.E9.m1.1.1.1.1.3.2.cmml"><msub id="S2.E9.m1.1.1.1.1.3.2.2" xref="S2.E9.m1.1.1.1.1.3.2.2.cmml"><mi id="S2.E9.m1.1.1.1.1.3.2.2.2" xref="S2.E9.m1.1.1.1.1.3.2.2.2.cmml">P</mi><mi id="S2.E9.m1.1.1.1.1.3.2.2.3" xref="S2.E9.m1.1.1.1.1.3.2.2.3.cmml">B</mi></msub><mo id="S2.E9.m1.1.1.1.1.3.2.1" xref="S2.E9.m1.1.1.1.1.3.2.1.cmml">∩</mo><msub id="S2.E9.m1.1.1.1.1.3.2.3" xref="S2.E9.m1.1.1.1.1.3.2.3.cmml"><mi id="S2.E9.m1.1.1.1.1.3.2.3.2" xref="S2.E9.m1.1.1.1.1.3.2.3.2.cmml">G</mi><mi id="S2.E9.m1.1.1.1.1.3.2.3.3" xref="S2.E9.m1.1.1.1.1.3.2.3.3.cmml">B</mi></msub></mrow><mrow id="S2.E9.m1.1.1.1.1.3.3" xref="S2.E9.m1.1.1.1.1.3.3.cmml"><msub id="S2.E9.m1.1.1.1.1.3.3.2" xref="S2.E9.m1.1.1.1.1.3.3.2.cmml"><mi id="S2.E9.m1.1.1.1.1.3.3.2.2" xref="S2.E9.m1.1.1.1.1.3.3.2.2.cmml">P</mi><mi id="S2.E9.m1.1.1.1.1.3.3.2.3" xref="S2.E9.m1.1.1.1.1.3.3.2.3.cmml">B</mi></msub><mo id="S2.E9.m1.1.1.1.1.3.3.1" xref="S2.E9.m1.1.1.1.1.3.3.1.cmml">∪</mo><msub id="S2.E9.m1.1.1.1.1.3.3.3" xref="S2.E9.m1.1.1.1.1.3.3.3.cmml"><mi id="S2.E9.m1.1.1.1.1.3.3.3.2" xref="S2.E9.m1.1.1.1.1.3.3.3.2.cmml">G</mi><mi id="S2.E9.m1.1.1.1.1.3.3.3.3" xref="S2.E9.m1.1.1.1.1.3.3.3.3.cmml">B</mi></msub></mrow></mfrac></mrow><mo id="S2.E9.m1.1.1.1.2" xref="S2.E9.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E9.m1.1b"><apply id="S2.E9.m1.1.1.1.1.cmml" xref="S2.E9.m1.1.1.1"><eq id="S2.E9.m1.1.1.1.1.1.cmml" xref="S2.E9.m1.1.1.1.1.1"></eq><apply id="S2.E9.m1.1.1.1.1.2.cmml" xref="S2.E9.m1.1.1.1.1.2"><times id="S2.E9.m1.1.1.1.1.2.1.cmml" xref="S2.E9.m1.1.1.1.1.2.1"></times><ci id="S2.E9.m1.1.1.1.1.2.2.cmml" xref="S2.E9.m1.1.1.1.1.2.2">𝐼</ci><ci id="S2.E9.m1.1.1.1.1.2.3.cmml" xref="S2.E9.m1.1.1.1.1.2.3">𝑜</ci><apply id="S2.E9.m1.1.1.1.1.2.4.cmml" xref="S2.E9.m1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S2.E9.m1.1.1.1.1.2.4.1.cmml" xref="S2.E9.m1.1.1.1.1.2.4">subscript</csymbol><ci id="S2.E9.m1.1.1.1.1.2.4.2.cmml" xref="S2.E9.m1.1.1.1.1.2.4.2">𝑈</ci><apply id="S2.E9.m1.1.1.1.1.2.4.3.cmml" xref="S2.E9.m1.1.1.1.1.2.4.3"><times id="S2.E9.m1.1.1.1.1.2.4.3.1.cmml" xref="S2.E9.m1.1.1.1.1.2.4.3.1"></times><cn id="S2.E9.m1.1.1.1.1.2.4.3.2.cmml" type="integer" xref="S2.E9.m1.1.1.1.1.2.4.3.2">3</cn><ci id="S2.E9.m1.1.1.1.1.2.4.3.3.cmml" xref="S2.E9.m1.1.1.1.1.2.4.3.3">𝐷</ci></apply></apply></apply><apply id="S2.E9.m1.1.1.1.1.3.cmml" xref="S2.E9.m1.1.1.1.1.3"><divide id="S2.E9.m1.1.1.1.1.3.1.cmml" xref="S2.E9.m1.1.1.1.1.3"></divide><apply id="S2.E9.m1.1.1.1.1.3.2.cmml" xref="S2.E9.m1.1.1.1.1.3.2"><intersect id="S2.E9.m1.1.1.1.1.3.2.1.cmml" xref="S2.E9.m1.1.1.1.1.3.2.1"></intersect><apply id="S2.E9.m1.1.1.1.1.3.2.2.cmml" xref="S2.E9.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S2.E9.m1.1.1.1.1.3.2.2.1.cmml" xref="S2.E9.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S2.E9.m1.1.1.1.1.3.2.2.2.cmml" xref="S2.E9.m1.1.1.1.1.3.2.2.2">𝑃</ci><ci id="S2.E9.m1.1.1.1.1.3.2.2.3.cmml" xref="S2.E9.m1.1.1.1.1.3.2.2.3">𝐵</ci></apply><apply id="S2.E9.m1.1.1.1.1.3.2.3.cmml" xref="S2.E9.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E9.m1.1.1.1.1.3.2.3.1.cmml" xref="S2.E9.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S2.E9.m1.1.1.1.1.3.2.3.2.cmml" xref="S2.E9.m1.1.1.1.1.3.2.3.2">𝐺</ci><ci id="S2.E9.m1.1.1.1.1.3.2.3.3.cmml" xref="S2.E9.m1.1.1.1.1.3.2.3.3">𝐵</ci></apply></apply><apply id="S2.E9.m1.1.1.1.1.3.3.cmml" xref="S2.E9.m1.1.1.1.1.3.3"><union id="S2.E9.m1.1.1.1.1.3.3.1.cmml" xref="S2.E9.m1.1.1.1.1.3.3.1"></union><apply id="S2.E9.m1.1.1.1.1.3.3.2.cmml" xref="S2.E9.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.E9.m1.1.1.1.1.3.3.2.1.cmml" xref="S2.E9.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S2.E9.m1.1.1.1.1.3.3.2.2.cmml" xref="S2.E9.m1.1.1.1.1.3.3.2.2">𝑃</ci><ci id="S2.E9.m1.1.1.1.1.3.3.2.3.cmml" xref="S2.E9.m1.1.1.1.1.3.3.2.3">𝐵</ci></apply><apply id="S2.E9.m1.1.1.1.1.3.3.3.cmml" xref="S2.E9.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E9.m1.1.1.1.1.3.3.3.1.cmml" xref="S2.E9.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.E9.m1.1.1.1.1.3.3.3.2.cmml" xref="S2.E9.m1.1.1.1.1.3.3.3.2">𝐺</ci><ci id="S2.E9.m1.1.1.1.1.3.3.3.3.cmml" xref="S2.E9.m1.1.1.1.1.3.3.3.3">𝐵</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E9.m1.1c">Io{U_{3D}}=\frac{{{P_{B}}\cap{G_{B}}}}{{{P_{B}}\cup{G_{B}}}},\vspace{-0.5em}</annotation><annotation encoding="application/x-llamapun" id="S2.E9.m1.1d">italic_I italic_o italic_U start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT = divide start_ARG italic_P start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ∩ italic_G start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT end_ARG start_ARG italic_P start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ∪ italic_G start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.SSS3.p1.6">where <math alttext="{{G_{B}}}" class="ltx_Math" display="inline" id="S2.SS4.SSS3.p1.2.m1.1"><semantics id="S2.SS4.SSS3.p1.2.m1.1a"><msub id="S2.SS4.SSS3.p1.2.m1.1.1" xref="S2.SS4.SSS3.p1.2.m1.1.1.cmml"><mi id="S2.SS4.SSS3.p1.2.m1.1.1.2" xref="S2.SS4.SSS3.p1.2.m1.1.1.2.cmml">G</mi><mi id="S2.SS4.SSS3.p1.2.m1.1.1.3" xref="S2.SS4.SSS3.p1.2.m1.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.2.m1.1b"><apply id="S2.SS4.SSS3.p1.2.m1.1.1.cmml" xref="S2.SS4.SSS3.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p1.2.m1.1.1.1.cmml" xref="S2.SS4.SSS3.p1.2.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p1.2.m1.1.1.2.cmml" xref="S2.SS4.SSS3.p1.2.m1.1.1.2">𝐺</ci><ci id="S2.SS4.SSS3.p1.2.m1.1.1.3.cmml" xref="S2.SS4.SSS3.p1.2.m1.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.2.m1.1c">{{G_{B}}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS3.p1.2.m1.1d">italic_G start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="{{P_{B}}}" class="ltx_Math" display="inline" id="S2.SS4.SSS3.p1.3.m2.1"><semantics id="S2.SS4.SSS3.p1.3.m2.1a"><msub id="S2.SS4.SSS3.p1.3.m2.1.1" xref="S2.SS4.SSS3.p1.3.m2.1.1.cmml"><mi id="S2.SS4.SSS3.p1.3.m2.1.1.2" xref="S2.SS4.SSS3.p1.3.m2.1.1.2.cmml">P</mi><mi id="S2.SS4.SSS3.p1.3.m2.1.1.3" xref="S2.SS4.SSS3.p1.3.m2.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.3.m2.1b"><apply id="S2.SS4.SSS3.p1.3.m2.1.1.cmml" xref="S2.SS4.SSS3.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p1.3.m2.1.1.1.cmml" xref="S2.SS4.SSS3.p1.3.m2.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p1.3.m2.1.1.2.cmml" xref="S2.SS4.SSS3.p1.3.m2.1.1.2">𝑃</ci><ci id="S2.SS4.SSS3.p1.3.m2.1.1.3.cmml" xref="S2.SS4.SSS3.p1.3.m2.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.3.m2.1c">{{P_{B}}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS3.p1.3.m2.1d">italic_P start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math> represent the ground-truth and the predicted 3D bounding boxes, respectively. The symbols <math alttext="\cap" class="ltx_Math" display="inline" id="S2.SS4.SSS3.p1.4.m3.1"><semantics id="S2.SS4.SSS3.p1.4.m3.1a"><mo id="S2.SS4.SSS3.p1.4.m3.1.1" xref="S2.SS4.SSS3.p1.4.m3.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.4.m3.1b"><intersect id="S2.SS4.SSS3.p1.4.m3.1.1.cmml" xref="S2.SS4.SSS3.p1.4.m3.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.4.m3.1c">\cap</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS3.p1.4.m3.1d">∩</annotation></semantics></math> and <math alttext="\cup" class="ltx_Math" display="inline" id="S2.SS4.SSS3.p1.5.m4.1"><semantics id="S2.SS4.SSS3.p1.5.m4.1a"><mo id="S2.SS4.SSS3.p1.5.m4.1.1" xref="S2.SS4.SSS3.p1.5.m4.1.1.cmml">∪</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.5.m4.1b"><union id="S2.SS4.SSS3.p1.5.m4.1.1.cmml" xref="S2.SS4.SSS3.p1.5.m4.1.1"></union></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.5.m4.1c">\cup</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS3.p1.5.m4.1d">∪</annotation></semantics></math> represent the intersection and union, respectively. The correctness of the predicted object pose is determined based on whether the <math alttext="Io{U_{3D}}" class="ltx_Math" display="inline" id="S2.SS4.SSS3.p1.6.m5.1"><semantics id="S2.SS4.SSS3.p1.6.m5.1a"><mrow id="S2.SS4.SSS3.p1.6.m5.1.1" xref="S2.SS4.SSS3.p1.6.m5.1.1.cmml"><mi id="S2.SS4.SSS3.p1.6.m5.1.1.2" xref="S2.SS4.SSS3.p1.6.m5.1.1.2.cmml">I</mi><mo id="S2.SS4.SSS3.p1.6.m5.1.1.1" xref="S2.SS4.SSS3.p1.6.m5.1.1.1.cmml">⁢</mo><mi id="S2.SS4.SSS3.p1.6.m5.1.1.3" xref="S2.SS4.SSS3.p1.6.m5.1.1.3.cmml">o</mi><mo id="S2.SS4.SSS3.p1.6.m5.1.1.1a" xref="S2.SS4.SSS3.p1.6.m5.1.1.1.cmml">⁢</mo><msub id="S2.SS4.SSS3.p1.6.m5.1.1.4" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.cmml"><mi id="S2.SS4.SSS3.p1.6.m5.1.1.4.2" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.2.cmml">U</mi><mrow id="S2.SS4.SSS3.p1.6.m5.1.1.4.3" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.3.cmml"><mn id="S2.SS4.SSS3.p1.6.m5.1.1.4.3.2" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.3.2.cmml">3</mn><mo id="S2.SS4.SSS3.p1.6.m5.1.1.4.3.1" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.3.1.cmml">⁢</mo><mi id="S2.SS4.SSS3.p1.6.m5.1.1.4.3.3" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.3.3.cmml">D</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.6.m5.1b"><apply id="S2.SS4.SSS3.p1.6.m5.1.1.cmml" xref="S2.SS4.SSS3.p1.6.m5.1.1"><times id="S2.SS4.SSS3.p1.6.m5.1.1.1.cmml" xref="S2.SS4.SSS3.p1.6.m5.1.1.1"></times><ci id="S2.SS4.SSS3.p1.6.m5.1.1.2.cmml" xref="S2.SS4.SSS3.p1.6.m5.1.1.2">𝐼</ci><ci id="S2.SS4.SSS3.p1.6.m5.1.1.3.cmml" xref="S2.SS4.SSS3.p1.6.m5.1.1.3">𝑜</ci><apply id="S2.SS4.SSS3.p1.6.m5.1.1.4.cmml" xref="S2.SS4.SSS3.p1.6.m5.1.1.4"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p1.6.m5.1.1.4.1.cmml" xref="S2.SS4.SSS3.p1.6.m5.1.1.4">subscript</csymbol><ci id="S2.SS4.SSS3.p1.6.m5.1.1.4.2.cmml" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.2">𝑈</ci><apply id="S2.SS4.SSS3.p1.6.m5.1.1.4.3.cmml" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.3"><times id="S2.SS4.SSS3.p1.6.m5.1.1.4.3.1.cmml" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.3.1"></times><cn id="S2.SS4.SSS3.p1.6.m5.1.1.4.3.2.cmml" type="integer" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.3.2">3</cn><ci id="S2.SS4.SSS3.p1.6.m5.1.1.4.3.3.cmml" xref="S2.SS4.SSS3.p1.6.m5.1.1.4.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.6.m5.1c">Io{U_{3D}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS3.p1.6.m5.1d">italic_I italic_o italic_U start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT</annotation></semantics></math> value exceeds a predefined threshold.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.4 </span>Other Metric</h4>
<div class="ltx_para" id="S2.SS4.SSS4.p1">
<p class="ltx_p" id="S2.SS4.SSS4.p1.3">Since some Normalized Object Coordinate Space (NOCS) shape alignment-based category-level methods reconstruct the 3D object shape before estimating the object pose, the Chamfer Distance (CD) metric<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib72" title="">72</a>]</cite>, which not only captures the global shape deviation but is also sensitive to local shape differences, is commonly leveraged to evaluate the NOCS shape reconstruction accuracy of these methods as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{D_{cd}}=\sum\limits_{x\in N}{\mathop{\min}\limits_{y\in{M_{gt}}}}\parallel x-%
y\parallel_{2}^{2}+\sum\limits_{y\in{M_{gt}}}{\mathop{\min}\limits_{x\in N}}%
\parallel x-y\parallel_{2}^{2},\vspace{-0.5em}" class="ltx_Math" display="block" id="S2.E10.m1.1"><semantics id="S2.E10.m1.1a"><mrow id="S2.E10.m1.1.1.1" xref="S2.E10.m1.1.1.1.1.cmml"><mrow id="S2.E10.m1.1.1.1.1" xref="S2.E10.m1.1.1.1.1.cmml"><msub id="S2.E10.m1.1.1.1.1.4" xref="S2.E10.m1.1.1.1.1.4.cmml"><mi id="S2.E10.m1.1.1.1.1.4.2" xref="S2.E10.m1.1.1.1.1.4.2.cmml">D</mi><mrow id="S2.E10.m1.1.1.1.1.4.3" xref="S2.E10.m1.1.1.1.1.4.3.cmml"><mi id="S2.E10.m1.1.1.1.1.4.3.2" xref="S2.E10.m1.1.1.1.1.4.3.2.cmml">c</mi><mo id="S2.E10.m1.1.1.1.1.4.3.1" xref="S2.E10.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S2.E10.m1.1.1.1.1.4.3.3" xref="S2.E10.m1.1.1.1.1.4.3.3.cmml">d</mi></mrow></msub><mo id="S2.E10.m1.1.1.1.1.3" rspace="0.111em" xref="S2.E10.m1.1.1.1.1.3.cmml">=</mo><mrow id="S2.E10.m1.1.1.1.1.2" xref="S2.E10.m1.1.1.1.1.2.cmml"><mrow id="S2.E10.m1.1.1.1.1.1.1" xref="S2.E10.m1.1.1.1.1.1.1.cmml"><munder id="S2.E10.m1.1.1.1.1.1.1.2" xref="S2.E10.m1.1.1.1.1.1.1.2.cmml"><mo id="S2.E10.m1.1.1.1.1.1.1.2.2" movablelimits="false" xref="S2.E10.m1.1.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S2.E10.m1.1.1.1.1.1.1.2.3" xref="S2.E10.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E10.m1.1.1.1.1.1.1.2.3.2" xref="S2.E10.m1.1.1.1.1.1.1.2.3.2.cmml">x</mi><mo id="S2.E10.m1.1.1.1.1.1.1.2.3.1" xref="S2.E10.m1.1.1.1.1.1.1.2.3.1.cmml">∈</mo><mi id="S2.E10.m1.1.1.1.1.1.1.2.3.3" xref="S2.E10.m1.1.1.1.1.1.1.2.3.3.cmml">N</mi></mrow></munder><mrow id="S2.E10.m1.1.1.1.1.1.1.1" xref="S2.E10.m1.1.1.1.1.1.1.1.cmml"><munder id="S2.E10.m1.1.1.1.1.1.1.1.2" xref="S2.E10.m1.1.1.1.1.1.1.1.2.cmml"><mo id="S2.E10.m1.1.1.1.1.1.1.1.2.2" lspace="0em" movablelimits="false" rspace="0em" xref="S2.E10.m1.1.1.1.1.1.1.1.2.2.cmml">min</mo><mrow id="S2.E10.m1.1.1.1.1.1.1.1.2.3" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E10.m1.1.1.1.1.1.1.1.2.3.2" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.2.cmml">y</mi><mo id="S2.E10.m1.1.1.1.1.1.1.1.2.3.1" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.1.cmml">∈</mo><msub id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.2" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.2.cmml">M</mi><mrow id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.cmml"><mi id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.2" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.2.cmml">g</mi><mo id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.1" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.1.cmml">⁢</mo><mi id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.3" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.3.cmml">t</mi></mrow></msub></mrow></munder><msubsup id="S2.E10.m1.1.1.1.1.1.1.1.1" xref="S2.E10.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mo id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">y</mi></mrow><mo id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.E10.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.3.cmml">2</mn><mn id="S2.E10.m1.1.1.1.1.1.1.1.1.3" xref="S2.E10.m1.1.1.1.1.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow><mo id="S2.E10.m1.1.1.1.1.2.3" rspace="0.055em" xref="S2.E10.m1.1.1.1.1.2.3.cmml">+</mo><mrow id="S2.E10.m1.1.1.1.1.2.2" xref="S2.E10.m1.1.1.1.1.2.2.cmml"><munder id="S2.E10.m1.1.1.1.1.2.2.2" xref="S2.E10.m1.1.1.1.1.2.2.2.cmml"><mo id="S2.E10.m1.1.1.1.1.2.2.2.2" movablelimits="false" xref="S2.E10.m1.1.1.1.1.2.2.2.2.cmml">∑</mo><mrow id="S2.E10.m1.1.1.1.1.2.2.2.3" xref="S2.E10.m1.1.1.1.1.2.2.2.3.cmml"><mi id="S2.E10.m1.1.1.1.1.2.2.2.3.2" xref="S2.E10.m1.1.1.1.1.2.2.2.3.2.cmml">y</mi><mo id="S2.E10.m1.1.1.1.1.2.2.2.3.1" xref="S2.E10.m1.1.1.1.1.2.2.2.3.1.cmml">∈</mo><msub id="S2.E10.m1.1.1.1.1.2.2.2.3.3" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.cmml"><mi id="S2.E10.m1.1.1.1.1.2.2.2.3.3.2" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.2.cmml">M</mi><mrow id="S2.E10.m1.1.1.1.1.2.2.2.3.3.3" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.cmml"><mi id="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.2" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.2.cmml">g</mi><mo id="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.1" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.1.cmml">⁢</mo><mi id="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.3" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.3.cmml">t</mi></mrow></msub></mrow></munder><mrow id="S2.E10.m1.1.1.1.1.2.2.1" xref="S2.E10.m1.1.1.1.1.2.2.1.cmml"><munder id="S2.E10.m1.1.1.1.1.2.2.1.2" xref="S2.E10.m1.1.1.1.1.2.2.1.2.cmml"><mo id="S2.E10.m1.1.1.1.1.2.2.1.2.2" lspace="0em" movablelimits="false" rspace="0em" xref="S2.E10.m1.1.1.1.1.2.2.1.2.2.cmml">min</mo><mrow id="S2.E10.m1.1.1.1.1.2.2.1.2.3" xref="S2.E10.m1.1.1.1.1.2.2.1.2.3.cmml"><mi id="S2.E10.m1.1.1.1.1.2.2.1.2.3.2" xref="S2.E10.m1.1.1.1.1.2.2.1.2.3.2.cmml">x</mi><mo id="S2.E10.m1.1.1.1.1.2.2.1.2.3.1" xref="S2.E10.m1.1.1.1.1.2.2.1.2.3.1.cmml">∈</mo><mi id="S2.E10.m1.1.1.1.1.2.2.1.2.3.3" xref="S2.E10.m1.1.1.1.1.2.2.1.2.3.3.cmml">N</mi></mrow></munder><msubsup id="S2.E10.m1.1.1.1.1.2.2.1.1" xref="S2.E10.m1.1.1.1.1.2.2.1.1.cmml"><mrow id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.2.cmml"><mo id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.2" stretchy="false" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.2" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.2.cmml">x</mi><mo id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.1" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.cmml">−</mo><mi id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.3" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.3.cmml">y</mi></mrow><mo id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.3" stretchy="false" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.E10.m1.1.1.1.1.2.2.1.1.1.3" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.3.cmml">2</mn><mn id="S2.E10.m1.1.1.1.1.2.2.1.1.3" xref="S2.E10.m1.1.1.1.1.2.2.1.1.3.cmml">2</mn></msubsup></mrow></mrow></mrow></mrow><mo id="S2.E10.m1.1.1.1.2" xref="S2.E10.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E10.m1.1b"><apply id="S2.E10.m1.1.1.1.1.cmml" xref="S2.E10.m1.1.1.1"><eq id="S2.E10.m1.1.1.1.1.3.cmml" xref="S2.E10.m1.1.1.1.1.3"></eq><apply id="S2.E10.m1.1.1.1.1.4.cmml" xref="S2.E10.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.4.1.cmml" xref="S2.E10.m1.1.1.1.1.4">subscript</csymbol><ci id="S2.E10.m1.1.1.1.1.4.2.cmml" xref="S2.E10.m1.1.1.1.1.4.2">𝐷</ci><apply id="S2.E10.m1.1.1.1.1.4.3.cmml" xref="S2.E10.m1.1.1.1.1.4.3"><times id="S2.E10.m1.1.1.1.1.4.3.1.cmml" xref="S2.E10.m1.1.1.1.1.4.3.1"></times><ci id="S2.E10.m1.1.1.1.1.4.3.2.cmml" xref="S2.E10.m1.1.1.1.1.4.3.2">𝑐</ci><ci id="S2.E10.m1.1.1.1.1.4.3.3.cmml" xref="S2.E10.m1.1.1.1.1.4.3.3">𝑑</ci></apply></apply><apply id="S2.E10.m1.1.1.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.2"><plus id="S2.E10.m1.1.1.1.1.2.3.cmml" xref="S2.E10.m1.1.1.1.1.2.3"></plus><apply id="S2.E10.m1.1.1.1.1.1.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1"><apply id="S2.E10.m1.1.1.1.1.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E10.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.2.2"></sum><apply id="S2.E10.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E10.m1.1.1.1.1.1.1.2.3"><in id="S2.E10.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.2.3.1"></in><ci id="S2.E10.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.2.3.2">𝑥</ci><ci id="S2.E10.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E10.m1.1.1.1.1.1.1.2.3.3">𝑁</ci></apply></apply><apply id="S2.E10.m1.1.1.1.1.1.1.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1"><apply id="S2.E10.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2">subscript</csymbol><min id="S2.E10.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.2"></min><apply id="S2.E10.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3"><in id="S2.E10.m1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.1"></in><ci id="S2.E10.m1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.2">𝑦</ci><apply id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3">subscript</csymbol><ci id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.2">𝑀</ci><apply id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3"><times id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.1"></times><ci id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.2">𝑔</ci><ci id="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.3.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.2.3.3.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S2.E10.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S2.E10.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑦</ci></apply></apply><cn id="S2.E10.m1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E10.m1.1.1.1.1.1.1.1.1.1.3">2</cn></apply><cn id="S2.E10.m1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E10.m1.1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply><apply id="S2.E10.m1.1.1.1.1.2.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2"><apply id="S2.E10.m1.1.1.1.1.2.2.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.2.2.2.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2">subscript</csymbol><sum id="S2.E10.m1.1.1.1.1.2.2.2.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.2"></sum><apply id="S2.E10.m1.1.1.1.1.2.2.2.3.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.3"><in id="S2.E10.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.3.1"></in><ci id="S2.E10.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.3.2">𝑦</ci><apply id="S2.E10.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3">subscript</csymbol><ci id="S2.E10.m1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.2">𝑀</ci><apply id="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.3"><times id="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.1"></times><ci id="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.2">𝑔</ci><ci id="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.3.cmml" xref="S2.E10.m1.1.1.1.1.2.2.2.3.3.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S2.E10.m1.1.1.1.1.2.2.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1"><apply id="S2.E10.m1.1.1.1.1.2.2.1.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.2"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.2.2.1.2.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.2">subscript</csymbol><min id="S2.E10.m1.1.1.1.1.2.2.1.2.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.2.2"></min><apply id="S2.E10.m1.1.1.1.1.2.2.1.2.3.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.2.3"><in id="S2.E10.m1.1.1.1.1.2.2.1.2.3.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.2.3.1"></in><ci id="S2.E10.m1.1.1.1.1.2.2.1.2.3.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.2.3.2">𝑥</ci><ci id="S2.E10.m1.1.1.1.1.2.2.1.2.3.3.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.2.3.3">𝑁</ci></apply></apply><apply id="S2.E10.m1.1.1.1.1.2.2.1.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.1"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.2.2.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.1">superscript</csymbol><apply id="S2.E10.m1.1.1.1.1.2.2.1.1.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.1"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.1.1.2.2.1.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.1">subscript</csymbol><apply id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.2.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.2">norm</csymbol><apply id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1"><minus id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.1"></minus><ci id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.1.1.1.3">𝑦</ci></apply></apply><cn id="S2.E10.m1.1.1.1.1.2.2.1.1.1.3.cmml" type="integer" xref="S2.E10.m1.1.1.1.1.2.2.1.1.1.3">2</cn></apply><cn id="S2.E10.m1.1.1.1.1.2.2.1.1.3.cmml" type="integer" xref="S2.E10.m1.1.1.1.1.2.2.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E10.m1.1c">{D_{cd}}=\sum\limits_{x\in N}{\mathop{\min}\limits_{y\in{M_{gt}}}}\parallel x-%
y\parallel_{2}^{2}+\sum\limits_{y\in{M_{gt}}}{\mathop{\min}\limits_{x\in N}}%
\parallel x-y\parallel_{2}^{2},\vspace{-0.5em}</annotation><annotation encoding="application/x-llamapun" id="S2.E10.m1.1d">italic_D start_POSTSUBSCRIPT italic_c italic_d end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_x ∈ italic_N end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_y ∈ italic_M start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ italic_x - italic_y ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ∑ start_POSTSUBSCRIPT italic_y ∈ italic_M start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_x ∈ italic_N end_POSTSUBSCRIPT ∥ italic_x - italic_y ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.SSS4.p1.2">where <math alttext="N" class="ltx_Math" display="inline" id="S2.SS4.SSS4.p1.1.m1.1"><semantics id="S2.SS4.SSS4.p1.1.m1.1a"><mi id="S2.SS4.SSS4.p1.1.m1.1.1" xref="S2.SS4.SSS4.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS4.p1.1.m1.1b"><ci id="S2.SS4.SSS4.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS4.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS4.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS4.p1.1.m1.1d">italic_N</annotation></semantics></math> and <math alttext="M_{gt}" class="ltx_Math" display="inline" id="S2.SS4.SSS4.p1.2.m2.1"><semantics id="S2.SS4.SSS4.p1.2.m2.1a"><msub id="S2.SS4.SSS4.p1.2.m2.1.1" xref="S2.SS4.SSS4.p1.2.m2.1.1.cmml"><mi id="S2.SS4.SSS4.p1.2.m2.1.1.2" xref="S2.SS4.SSS4.p1.2.m2.1.1.2.cmml">M</mi><mrow id="S2.SS4.SSS4.p1.2.m2.1.1.3" xref="S2.SS4.SSS4.p1.2.m2.1.1.3.cmml"><mi id="S2.SS4.SSS4.p1.2.m2.1.1.3.2" xref="S2.SS4.SSS4.p1.2.m2.1.1.3.2.cmml">g</mi><mo id="S2.SS4.SSS4.p1.2.m2.1.1.3.1" xref="S2.SS4.SSS4.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S2.SS4.SSS4.p1.2.m2.1.1.3.3" xref="S2.SS4.SSS4.p1.2.m2.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS4.p1.2.m2.1b"><apply id="S2.SS4.SSS4.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS4.p1.2.m2.1.1.1.cmml" xref="S2.SS4.SSS4.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS4.SSS4.p1.2.m2.1.1.2.cmml" xref="S2.SS4.SSS4.p1.2.m2.1.1.2">𝑀</ci><apply id="S2.SS4.SSS4.p1.2.m2.1.1.3.cmml" xref="S2.SS4.SSS4.p1.2.m2.1.1.3"><times id="S2.SS4.SSS4.p1.2.m2.1.1.3.1.cmml" xref="S2.SS4.SSS4.p1.2.m2.1.1.3.1"></times><ci id="S2.SS4.SSS4.p1.2.m2.1.1.3.2.cmml" xref="S2.SS4.SSS4.p1.2.m2.1.1.3.2">𝑔</ci><ci id="S2.SS4.SSS4.p1.2.m2.1.1.3.3.cmml" xref="S2.SS4.SSS4.p1.2.m2.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS4.p1.2.m2.1c">M_{gt}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS4.p1.2.m2.1d">italic_M start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> represent the reconstructed and ground-truth NOCS shape, respectively.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Instance-Level Object Pose Estimation</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Instance-level object pose estimation describes the task of estimating the pose of the objects that have been seen during the training of the model. We classify existing instance-level methods into four categories: correspondence-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS1" title="3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.1</span></a>), template-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS2" title="3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.2</span></a>), voting-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS3" title="3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.3</span></a>), and regression-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS4" title="3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.4</span></a>) methods.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span class="ltx_text ltx_font_italic" id="S3.SS1.1.1">Correspondence-Based Methods</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Correspondence-based object pose estimation refers to techniques that involve identifying correspondences between the input data and the given complete object CAD model. Correspondence-based methods can be divided into sparse and dense correspondences. Sparse correspondence-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS1.SSS1" title="3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>) involve detecting object keypoints in the input image or point cloud to establish 2D-3D or 3D-3D correspondences between the input data and the object CAD model, followed by the utilization of the Perspective-n-Point (PnP) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib73" title="">73</a>]</cite> or least square method to determine the object pose.
Dense correspondence-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS1.SSS2" title="3.1.2 Dense Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>) aim to establish dense 2D-3D or 3D-3D correspondences, ultimately leading to more accurate object pose estimation.
For the RGB image, they leverage every pixel or multiple patches to generate pixel-wise correspondences, while for the point cloud, they use the entire point cloud to find point-wise correspondences.
The illustration of these two types of methods is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.F4" title="Figure 4 ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4</span></a>. The attributes and performance of some representative methods are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.T1" title="TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Sparse Correspondence Methods</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">As a representative method, Rad <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib74" title="">74</a>]</cite> first used a segmentation method to detect the object of interest in an RGB image. Then, they predicted the 2D projections of the object’s 3D bounding box corners. Finally, they used the PnP algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib73" title="">73</a>]</cite> to estimate the object pose. Additionally, they employed a classifier to determine the pose range in real-time, addressing the issue of ambiguity in symmetric objects. Tekin <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib75" title="">75</a>]</cite> proposed a CNN network inspired by YOLO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib76" title="">76</a>]</cite> to integrate object detection and pose estimation, directly predicting the locations of the projected vertices of the 3D object bounding box.
Unlike <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib74" title="">74</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib75" title="">75</a>]</cite>, Pavlakos <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib77" title="">77</a>]</cite> predicted the 2D projections of predefined semantic keypoints.
Doosti <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib78" title="">78</a>]</cite> introduced a compact model comprising two adaptive graph convolutional neural networks (GCNNs)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib79" title="">79</a>]</cite>, collaborating to estimate object and hand poses.
To further enhance the robustness of object pose estimation, Song <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib80" title="">80</a>]</cite> employed a hybrid intermediate representation to convey geometric details in the input image, encompassing keypoints, edge vectors, and symmetry correspondences.
Liu <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib81" title="">81</a>]</cite> proposed a multi-directional feature pyramid network along with a method that calculates object pose estimation confidence by incorporating spatial and plane information. Hu et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib82" title="">82</a>]</cite> introduced a single-stage hierarchical end-to-end trainable network to address pose estimation challenges associated with scale variations in aerospace objects. In a recent development, Lian <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.7">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib83" title="">83</a>]</cite> increased the number of predefined 3D keypoints to enhance the establishment of correspondences. Moreover, they devised a hierarchical binary encoding approach for localizing keypoints, enabling gradual refinement of correspondences and transforming correspondence regression into a more efficient classification task.
To estimate transparent object pose, Chang <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.8">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib84" title="">84</a>]</cite> used a 3D bounding box prediction network and multi-view geometry techniques. The method first detects 2D projections of 3D bounding box vertices, and then reconstructs 3D points based on the multi-view detected 2D projections incorporating camera motion data. Additionally, they introduced a generalized pose definition to address pose ambiguity for symmetric objects.
To enhance the efficiency of pose estimation networks, Guo <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.9">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib85" title="">85</a>]</cite> integrated knowledge distillation into object pose estimation by distilling the teacher’s distribution of local predictions into the student network.
Liu <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.10">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib86" title="">86</a>]</cite> argued that differentiable PnP strategies conflict with the averaging nature of the PnP problem, resulting in gradients that may encourage the network to degrade the accuracy of individual correspondences. To mitigate this, they introduced a linear covariance loss, which can be used for both sparse and dense correspondence-based methods.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">To mitigate vulnerability caused by large occlusions, Crivellaro <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib87" title="">87</a>]</cite> used several control points to represent each object part. Then, they predicted the 2D projections of these control points to calculate the object pose.
Some researchers solved the occlusion problem by predicting keypoints using small patches.
Oberweger <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib88" title="">88</a>]</cite> processed each patch separately to generate heatmaps and then aggregated the results to achieve precise and reliable predictions. Additionally, they offered a straightforward but efficient strategy to resolve ambiguities between patches and heatmaps during training.
Hu <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib89" title="">89</a>]</cite> unveiled a segmentation-driven pose estimation framework in which every visible object part offers a local pose prediction through 2D keypoint locations.
Furthermore, Huang <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p2.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib90" title="">90</a>]</cite> conceptualized 2D keypoint locations as probabilistic distributions within the loss function and designed a confidence-based network.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">Reducing the reliance on annotated real-world data is also an important task.
Some methods exploit geometric consistency as additional information to alleviate the need for annotation.
Zhao <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib91" title="">91</a>]</cite> employed image pairs with object annotations and relative transformation between viewpoints to automatically identify objects’ 3D keypoints that are geometrically and visually consistent.
In addition, Yang <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib92" title="">92</a>]</cite> used a keypoint consistency regularization for dual-scale images with a labeled 2D bounding box.
Using semi-supervised learning, Liu <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p3.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib93" title="">93</a>]</cite> developed a unified framework for estimating 3D hand and object poses. They constructed a joint learning framework that conducts explicit contextual reasoning between hand and object representations. To generate pseudo labels in semi-supervised learning, they utilized the spatial-temporal consistency found in large-scale hand-object videos as a constraint.
Synthetic data is also a way to solve the annotation problem.
Georgakis <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p3.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib94" title="">94</a>]</cite> reduced the need for expensive 3DoF pose annotations by selecting keypoints and maintaining viewpoint and modality invariance in RGB images and CAD model renderings.
Sock <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p3.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib95" title="">95</a>]</cite> utilized self-supervision to minimize the gap between synthetic and real data and enforced photometric consistency across different object views to fine-tune the model.
Further, Zhang <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p3.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib96" title="">96</a>]</cite> utilized the invariance of geometry relations between keypoints across real and synthetic domains to accomplish domain adaptation.
Thalhammer <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p3.1.7">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib97" title="">97</a>]</cite> introduced a specialized feature pyramid network to compute multi-scale features, enabling the simultaneous generation of pose hypotheses across various feature map resolutions.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1">Overall, the sparse correspondence-based methods can estimate object pose efficiently. However, relying on only a few control points can lead to sub-optimal accuracy.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F4.1"><span class="ltx_text" id="S3.F4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="540" id="S3.F4.1.1.g1" src="extracted/5635000/Fig3.jpg" width="598"/></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of the correspondence-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS1" title="3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.1</span></a>), template-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS2" title="3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.2</span></a>), voting-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS3" title="3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.3</span></a>), and regression-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS4" title="3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.4</span></a>) instance-level methods. Correspondence-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS1" title="3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.1</span></a>) involve establishing correspondences between input data and a provided object CAD model. Template-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS2" title="3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.2</span></a>) involve identifying the most similar template from a set of templates labeled with ground-truth object poses. Voting-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS3" title="3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.3</span></a>) determine object pose through a pixel-level or point-level voting scheme. Regression-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS4" title="3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.4</span></a>) aim to obtain the object pose directly from the learned features.
</figcaption>
</figure>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Representative instance-level methods. For each method, we report its 10 properties: published year, training input, inference input, pose DoF (3DoF, 6DoF, and 9DoF), object property (rigid, articulated), task (estimation, tracking, and refinement), domain training paradigm (source domain, domain adaptation, and domain generalization), inference mode, application area, and its performance of key metrics on key datasets. Notably, for the input of training and inference, we only focus on the input of the pose estimation model, not the input of the front-end segmentation method (because it can be obtained through RGB as well as through depth or RGBD). D, S, C, T, V, P, and R denote object detection, instance segmentation, correspondence prediction, template matching, voting, pose solution/regression, and pose refinement, respectively. We report the average recall of ADD(S) within 10<math alttext="\%" class="ltx_Math" display="inline" id="S3.T1.3.m1.1"><semantics id="S3.T1.3.m1.1b"><mo id="S3.T1.3.m1.1.1" xref="S3.T1.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.m1.1c"><csymbol cd="latexml" id="S3.T1.3.m1.1.1.cmml" xref="S3.T1.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.m1.1d">\%</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.m1.1e">%</annotation></semantics></math> of the object diameter (termed ADD(S)-0.1d) of LM-O and LM datasets, and the AUC of ADD-S (<math alttext="&lt;" class="ltx_Math" display="inline" id="S3.T1.4.m2.1"><semantics id="S3.T1.4.m2.1b"><mo id="S3.T1.4.m2.1.1" xref="S3.T1.4.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.m2.1c"><lt id="S3.T1.4.m2.1.1.cmml" xref="S3.T1.4.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.m2.1d">&lt;</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.m2.1e">&lt;</annotation></semantics></math>0.1m) of YCB-V dataset (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2" title="2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">2</span></a>).</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.6" style="width:433.6pt;height:819.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-323.0pt,610.7pt) scale(0.401633629579567,0.401633629579567) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.6.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3" id="S3.T1.6.2.2.3">Methods</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.2.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.2.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.2.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.4.1.1.1">Published</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.2.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.4.1.2.1">Year</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.2.5">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.2.5.1">
<tr class="ltx_tr" id="S3.T1.6.2.2.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.5.1.1.1">Training</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.2.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.5.1.2.1">Input</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.2.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.2.6.1">
<tr class="ltx_tr" id="S3.T1.6.2.2.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.6.1.1.1">Inference</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.2.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.6.1.2.1">Input</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.2.7">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.2.7.1">
<tr class="ltx_tr" id="S3.T1.6.2.2.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.7.1.1.1">Pose</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.2.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.7.1.2.1">DoF</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.2.8">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.2.8.1">
<tr class="ltx_tr" id="S3.T1.6.2.2.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.8.1.1.1">Object</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.2.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.8.1.2.1">Property</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.2.9">Task</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.2.10">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.2.10.1">
<tr class="ltx_tr" id="S3.T1.6.2.2.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.10.1.1.1">Domain Training</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.2.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.10.1.2.1">Paradigm</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.2.11">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.2.11.1">
<tr class="ltx_tr" id="S3.T1.6.2.2.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.11.1.1.1">Inference</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.2.11.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.11.1.2.1">Mode</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.2.12">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.2.12.1">
<tr class="ltx_tr" id="S3.T1.6.2.2.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.12.1.1.1">Application</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.2.12.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.12.1.2.1">Area</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" colspan="2" id="S3.T1.5.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.5.1.1.1.1">
<tr class="ltx_tr" id="S3.T1.5.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.1.1.1.1.1.1">LM-O <math alttext="|" class="ltx_Math" display="inline" id="S3.T1.5.1.1.1.1.1.1.m1.1"><semantics id="S3.T1.5.1.1.1.1.1.1.m1.1a"><mo fence="false" id="S3.T1.5.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.5.1.1.1.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.1.1.1.1.1.1.m1.1b"><ci id="S3.T1.5.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.5.1.1.1.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.1.1.1.1.1.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.1.1.1.1.1.1.m1.1d">|</annotation></semantics></math> LM</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.1.1.1.1.2.1">ADD(S)-0.1d</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.2.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.2.2.1">
<tr class="ltx_tr" id="S3.T1.6.2.2.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.2.1.2.1">YCB-V</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.2.2.1.1.1">ADD-S (<math alttext="&lt;" class="ltx_Math" display="inline" id="S3.T1.6.2.2.2.1.1.1.m1.1"><semantics id="S3.T1.6.2.2.2.1.1.1.m1.1a"><mo id="S3.T1.6.2.2.2.1.1.1.m1.1.1" xref="S3.T1.6.2.2.2.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.2.2.2.1.1.1.m1.1b"><lt id="S3.T1.6.2.2.2.1.1.1.m1.1.1.cmml" xref="S3.T1.6.2.2.2.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.2.2.2.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.2.2.2.1.1.1.m1.1d">&lt;</annotation></semantics></math>0.1m)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.3.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.1" rowspan="14"><span class="ltx_text" id="S3.T1.6.2.3.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.3.1.1.1.1" style="width:8.9pt;height:139.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:139.8pt;transform:translate(-65.46pt,-64.49pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.3.1.1.1.1.1">Correspondence-Based Methods</span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.2" rowspan="7"><span class="ltx_text" id="S3.T1.6.2.3.1.2.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.3.1.2.1.1" style="width:8.9pt;height:98.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:98.3pt;transform:translate(-44.68pt,-43.71pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.3.1.2.1.1.1">Sparse correspondence</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.3">Rad <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.3.1.3.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib74" title="">74</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.4">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.5">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.3.1.5.1">
<tr class="ltx_tr" id="S3.T1.6.2.3.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.3.1.5.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.3.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.3.1.5.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.3.1.6.1">
<tr class="ltx_tr" id="S3.T1.6.2.3.1.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.3.1.6.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.3.1.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.3.1.6.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.7">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.8">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.9">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.10">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.11">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.3.1.11.1">
<tr class="ltx_tr" id="S3.T1.6.2.3.1.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.3.1.11.1.1.1">four-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.3.1.11.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.3.1.11.1.2.1">S+C+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.12">symmetrical objects</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.13">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.14">62.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.3.1.15">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.1">Tekin <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.4.2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib75" title="">75</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.2">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.4.2.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.4.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.4.2.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.4.2.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.4.2.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.4.2.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.4.2.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.4.2.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.4.2.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.4.2.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.4.2.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.4.2.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.4.2.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.4.2.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.4.2.9.1.2.1">C+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.12">56.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.4.2.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.1">Hu <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.5.3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib89" title="">89</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.2">2019</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.5.3.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.5.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.5.3.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.5.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.5.3.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.5.3.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.5.3.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.5.3.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.5.3.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.5.3.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.5.3.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.5.3.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.5.3.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.5.3.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.5.3.9.1.2.1">C+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.10">occlusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.11">27.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.5.3.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.1">Song <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.6.4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib80" title="">80</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.6.4.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.6.4.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.6.4.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.6.4.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.6.4.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.6.4.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.6.4.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.6.4.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.6.4.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.6.4.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.6.4.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.6.4.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.6.4.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.6.4.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.6.4.9.1.2.1">C+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.10">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.6.4.10.1">
<tr class="ltx_tr" id="S3.T1.6.2.6.4.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.6.4.10.1.1.1">symmetrical objects,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.6.4.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.6.4.10.1.2.1">occlusion</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.11">47.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.12">91.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.6.4.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.1">Hu <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.7.5.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib82" title="">82</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.7.5.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.7.5.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.7.5.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.7.5.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.7.5.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.7.5.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.7.5.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.7.5.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.7.5.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.7.5.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.7.5.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.7.5.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.7.5.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.7.5.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.7.5.9.1.2.1">C+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.10">large scale variations</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.11">48.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.7.5.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.1">Chang <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.8.6.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib84" title="">84</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.8.6.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.8.6.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.8.6.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.8.6.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.8.6.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.8.6.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.8.6.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.8.6.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.8.6.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.8.6.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.8.6.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.8.6.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.8.6.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.8.6.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.8.6.9.1.2.1">C+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.10">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.8.6.10.1">
<tr class="ltx_tr" id="S3.T1.6.2.8.6.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.8.6.10.1.1.1">transparent,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.8.6.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.8.6.10.1.2.1">symmetrical objects</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.8.6.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.1">Guo <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.9.7.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib85" title="">85</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.9.7.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.9.7.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.9.7.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.9.7.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.9.7.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.9.7.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.9.7.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.9.7.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.9.7.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.9.7.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.9.7.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.9.7.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.9.7.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.9.7.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.9.7.9.1.2.1">D+C+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.11">44.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.9.7.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.1" rowspan="7"><span class="ltx_text" id="S3.T1.6.2.10.8.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.10.8.1.1.1" style="width:8.9pt;height:95.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:95.9pt;transform:translate(-43.49pt,-42.51pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.10.8.1.1.1.1">Dense correspondence</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.2">Li <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.10.8.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.3">2019</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.10.8.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.10.8.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.10.8.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.10.8.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.10.8.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.5">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.10.8.5.1">
<tr class="ltx_tr" id="S3.T1.6.2.10.8.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.10.8.5.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.10.8.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.10.8.5.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.6">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.7">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.8">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.9">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.10">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.10.8.10.1">
<tr class="ltx_tr" id="S3.T1.6.2.10.8.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.10.8.10.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.10.8.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.10.8.10.1.2.1">D+C+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.11">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.13">89.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.10.8.14">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.1">Hodan <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.11.9.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib98" title="">98</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.11.9.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.11.9.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.11.9.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.11.9.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.11.9.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.11.9.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.11.9.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.11.9.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.11.9.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.11.9.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.11.9.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.11.9.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.11.9.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.11.9.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.11.9.9.1.2.1">C+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.10">symmetrical objects</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.11.9.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.1">Shugurov <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.12.10.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib99" title="">99</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.12.10.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.12.10.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.12.10.3.1.1.1">RGB/RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.12.10.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.12.10.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.12.10.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.12.10.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.12.10.4.1.1.1">RGB/RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.12.10.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.12.10.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.12.10.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.12.10.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.12.10.9.1.1.1">four-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.12.10.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.12.10.9.1.2.1">D+C+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.12">99.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.12.10.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.1">Chen <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.13.11.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib100" title="">100</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.13.11.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.13.11.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.13.11.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.13.11.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.13.11.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.13.11.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.13.11.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.13.11.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.13.11.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.13.11.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.13.11.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.13.11.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.13.11.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.13.11.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.13.11.9.1.2.1">D+C+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.12">95.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.13.11.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.1">Haugaard <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.14.12.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib101" title="">101</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.14.12.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.14.12.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.14.12.3.1.1.1">RGB/RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.14.12.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.14.12.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.14.12.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.14.12.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.14.12.4.1.1.1">RGB/RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.14.12.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.14.12.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.8">generalization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.14.12.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.14.12.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.14.12.9.1.1.1">four-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.14.12.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.14.12.9.1.2.1">D+C+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.14.12.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.1">Li <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.15.13.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib102" title="">102</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.15.13.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.15.13.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.15.13.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.15.13.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.15.13.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.15.13.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.15.13.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.15.13.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.15.13.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.15.13.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.15.13.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.15.13.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.15.13.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.15.13.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.15.13.9.1.2.1">D+C+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.11">51.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.12">97.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.15.13.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.16.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.1">Xu <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.16.14.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib103" title="">103</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.16.14.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.16.14.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.16.14.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.16.14.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.16.14.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.16.14.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.16.14.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.16.14.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.16.14.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.16.14.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.7">refinement</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.16.14.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.16.14.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.16.14.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.16.14.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.16.14.9.1.2.1">P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.10">occlusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.11">60.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.12">97.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.16.14.13">85.7</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.17.15">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.1" rowspan="7"><span class="ltx_text" id="S3.T1.6.2.17.15.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.17.15.1.1.1" style="width:8.9pt;height:111.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:111.4pt;transform:translate(-51.24pt,-50.26pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.17.15.1.1.1.1">Template-Based Methods</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.2" rowspan="4"><span class="ltx_text" id="S3.T1.6.2.17.15.2.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.17.15.2.1.1" style="width:6.9pt;height:49.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:49.8pt;transform:translate(-21.45pt,-21.45pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.17.15.2.1.1.1">RGB-based</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.3">Sundermeyer <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.17.15.3.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib104" title="">104</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.4">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.5">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.17.15.5.1">
<tr class="ltx_tr" id="S3.T1.6.2.17.15.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.17.15.5.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.17.15.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.17.15.5.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.17.15.6.1">
<tr class="ltx_tr" id="S3.T1.6.2.17.15.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.17.15.6.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.17.15.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.17.15.6.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.7">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.8">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.9">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.10">generalization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.11">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.17.15.11.1">
<tr class="ltx_tr" id="S3.T1.6.2.17.15.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.17.15.11.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.17.15.11.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.17.15.11.1.2.1">D+T+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.12">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.13">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.14">31.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.17.15.15">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.18.16">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.1">Papaioannidis <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.18.16.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib105" title="">105</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.18.16.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.18.16.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.18.16.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.18.16.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.18.16.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.18.16.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.18.16.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.18.16.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.18.16.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.18.16.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.5">3DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.18.16.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.18.16.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.18.16.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.18.16.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.18.16.9.1.2.1">D+T</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.18.16.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.19.17">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.1">Li <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.19.17.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib106" title="">106</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.19.17.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.19.17.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.19.17.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.19.17.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.19.17.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.19.17.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.19.17.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.19.17.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.19.17.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.19.17.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.19.17.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.19.17.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.19.17.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.19.17.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.19.17.9.1.2.1">D+T+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.12">88.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.19.17.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.20.18">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.1">Deng <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.20.18.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib107" title="">107</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.20.18.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.20.18.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.20.18.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.20.18.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.20.18.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.20.18.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.20.18.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.20.18.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.20.18.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.20.18.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.7">tracking</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.8">generalization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.20.18.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.20.18.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.20.18.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.20.18.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.20.18.9.1.2.1">D+T</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.10">symmetrical objects</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.20.18.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.21.19">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.1" rowspan="3"><span class="ltx_text" id="S3.T1.6.2.21.19.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.21.19.1.1.1" style="width:6.9pt;height:50.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:50.1pt;transform:translate(-21.6pt,-21.6pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.21.19.1.1.1.1">Point cloud</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.2">Li <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.21.19.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib70" title="">70</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.3">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.21.19.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.21.19.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.21.19.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.21.19.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.21.19.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.5">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.21.19.5.1">
<tr class="ltx_tr" id="S3.T1.6.2.21.19.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.21.19.5.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.21.19.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.21.19.5.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.6">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.7">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.8">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.9">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.10">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.21.19.10.1">
<tr class="ltx_tr" id="S3.T1.6.2.21.19.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.21.19.10.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.21.19.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.21.19.10.1.2.1">S+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.11">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.12">70.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.13">99.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.21.19.14">96.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.22.20">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.1">Jiang <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.22.20.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib108" title="">108</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.22.20.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.22.20.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.22.20.3.1.1.1">Depth,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.22.20.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.22.20.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.22.20.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.22.20.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.22.20.4.1.1.1">Depth,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.22.20.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.22.20.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.22.20.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.22.20.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.22.20.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.22.20.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.22.20.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.22.20.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.23.21">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.1">Dang <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.23.21.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib109" title="">109</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.23.21.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.23.21.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.23.21.3.1.1.1">Depth,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.23.21.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.23.21.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.23.21.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.23.21.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.23.21.4.1.1.1">Depth,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.23.21.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.23.21.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.23.21.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.23.21.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.23.21.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.23.21.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.23.21.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.11">52.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.12">69.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.23.21.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.24.22">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.1" rowspan="11"><span class="ltx_text" id="S3.T1.6.2.24.22.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.24.22.1.1.1" style="width:8.9pt;height:99.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:99.4pt;transform:translate(-45.26pt,-44.29pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.24.22.1.1.1.1">Voting-Based Methods</span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.2" rowspan="6"><span class="ltx_text" id="S3.T1.6.2.24.22.2.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.24.22.2.1.1" style="width:8.9pt;height:64.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:64.8pt;transform:translate(-27.93pt,-26.96pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.24.22.2.1.1.1">Indirect voting</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.3">Peng <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.24.22.3.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib17" title="">17</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.4">2019</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.5">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.6">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.7">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.8">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.9">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.10">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.11">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.24.22.11.1">
<tr class="ltx_tr" id="S3.T1.6.2.24.22.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.24.22.11.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.24.22.11.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.24.22.11.1.2.1">V+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.12">occlusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.13">40.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.14">86.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.24.22.15">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.25.23">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.1">He <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.25.23.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib18" title="">18</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.25.23.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.25.23.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.25.23.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.25.23.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.25.23.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.25.23.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.25.23.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.25.23.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.25.23.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.25.23.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.25.23.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.25.23.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.25.23.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.25.23.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.25.23.9.1.2.1">S+V+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.12">99.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.25.23.13">95.5</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.26.24">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.1">He <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.26.24.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib21" title="">21</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.26.24.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.26.24.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.26.24.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.26.24.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.26.24.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.26.24.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.26.24.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.26.24.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.26.24.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.26.24.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.26.24.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.26.24.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.26.24.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.26.24.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.26.24.9.1.2.1">S+V+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.11">66.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.12">99.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.26.24.13">96.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.27.25">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.1">Cao <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.27.25.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib110" title="">110</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.27.25.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.27.25.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.27.25.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.27.25.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.27.25.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.9">end to end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.11">58.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.27.25.13">90.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.28.26">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.1">Wu <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.28.26.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib111" title="">111</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.28.26.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.28.26.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.28.26.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.28.26.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.28.26.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.28.26.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.28.26.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.28.26.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.28.26.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.28.26.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.28.26.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.28.26.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.28.26.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.28.26.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.28.26.9.1.2.1">S+V+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.11">70.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.12">99.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.28.26.13">96.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.29.27">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.1">Zhou <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.29.27.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib112" title="">112</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.29.27.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.29.27.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.29.27.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.29.27.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.29.27.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.29.27.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.29.27.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.29.27.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.29.27.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.29.27.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.29.27.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.29.27.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.29.27.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.29.27.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.29.27.9.1.2.1">S+V+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.11">77.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.12">99.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.29.27.13">96.7</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.30.28">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.1" rowspan="5"><span class="ltx_text" id="S3.T1.6.2.30.28.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.30.28.1.1.1" style="width:8.8pt;height:57.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:57.7pt;transform:translate(-24.44pt,-23.47pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.30.28.1.1.1.1">Direct voting</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.2">Wang <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.30.28.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib16" title="">16</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.3">2019</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.5">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.6">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.7">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.8">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.9">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.10">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.30.28.10.1">
<tr class="ltx_tr" id="S3.T1.6.2.30.28.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.30.28.10.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.30.28.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.30.28.10.1.2.1">P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.11">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.13">94.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.30.28.14">93.1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.31.29">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.1">Tian <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.31.29.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib113" title="">113</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.31.29.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.31.29.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.31.29.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.31.29.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.31.29.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.31.29.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.31.29.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.31.29.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.31.29.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.31.29.9.1.2.1">S+V+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.12">92.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.31.29.13">91.8</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.32.30">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.1">Zhou <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.32.30.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib114" title="">114</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.32.30.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.32.30.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.32.30.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.32.30.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.32.30.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.32.30.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.32.30.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.32.30.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.32.30.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.32.30.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.11">65.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.12">99.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.32.30.13">95.8</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.33.31">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.1">Mo <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.33.31.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib115" title="">115</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.3">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.33.31.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.33.31.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.33.31.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.33.31.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.33.31.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.33.31.13">93.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.34.32">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.1">Hong <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.34.32.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib116" title="">116</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.3">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.34.32.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.34.32.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.34.32.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.34.32.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.34.32.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.11">71.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.12">96.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.34.32.13">92.7</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.35.33">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.1" rowspan="28"><span class="ltx_text" id="S3.T1.6.2.35.33.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.35.33.1.1.1" style="width:8.9pt;height:116.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:116.9pt;transform:translate(-54.01pt,-53.04pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.35.33.1.1.1.1">Regression-Based Methods</span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.2" rowspan="6"><span class="ltx_text" id="S3.T1.6.2.35.33.2.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.35.33.2.1.1" style="width:8.9pt;height:75.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:75.4pt;transform:translate(-33.24pt,-32.27pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.35.33.2.1.1.1">Geometry-guided</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.3">Chen <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.35.33.3.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib117" title="">117</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.4">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.5">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.6">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.7">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.8">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.9">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.10">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.11">end to end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.12">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.13">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.14">98.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.35.33.15">92.4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.36.34">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.1">Hu <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.36.34.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib118" title="">118</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.36.34.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.36.34.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.36.34.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.36.34.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.36.34.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.9">end to end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.11">43.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.36.34.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.37.35">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.1">Labbé <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.37.35.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib119" title="">119</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.37.35.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.37.35.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.37.35.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.37.35.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.37.35.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.37.35.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.37.35.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.37.35.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.37.35.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.37.35.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.37.35.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.37.35.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.37.35.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.37.35.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.37.35.9.1.2.1">D+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.37.35.13">93.4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.38.36">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.1">Wang <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.38.36.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib120" title="">120</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.38.36.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.38.36.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.38.36.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.38.36.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.38.36.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.38.36.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.38.36.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.38.36.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.38.36.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.38.36.9.1.2.1">D+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.11">62.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.38.36.13">91.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.39.37">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.1">Di <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.39.37.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib121" title="">121</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.39.37.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.39.37.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.39.37.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.39.37.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.39.37.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.39.37.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.39.37.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.39.37.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.39.37.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.39.37.9.1.2.1">D+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.10">occlusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.11">62.32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.12">96.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.39.37.13">90.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.40.38">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.1">Wang <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.40.38.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib122" title="">122</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.40.38.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.40.38.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.40.38.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.40.38.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.40.38.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.8">adaptation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.40.38.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.40.38.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.40.38.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.40.38.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.40.38.9.1.2.1">D+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.10">occlusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.11">59.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.12">85.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.40.38.13">90.5</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.41.39">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.1" rowspan="17"><span class="ltx_text" id="S3.T1.6.2.41.39.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6.2.41.39.1.1.1" style="width:8.8pt;height:73.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:73.4pt;transform:translate(-32.31pt,-31.33pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.6.2.41.39.1.1.1.1">Direct regression</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.2">Xiang <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.41.39.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib15" title="">15</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.3">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.5">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.6">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.7">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.8">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.9">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.10">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.41.39.10.1">
<tr class="ltx_tr" id="S3.T1.6.2.41.39.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.41.39.10.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.41.39.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.41.39.10.1.2.1">S+V+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.11">cluttered</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.12">24.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.13">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.41.39.14">75.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.42.40">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.1">Li <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.42.40.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib123" title="">123</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.2">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.3">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.42.40.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.42.40.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.42.40.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.42.40.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.42.40.9.1.2.1">P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.42.40.13">94.3</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.43.41">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.1">Li <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.43.41.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib124" title="">124</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.2">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.43.41.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.43.41.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.43.41.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.43.41.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.43.41.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.43.41.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.43.41.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.43.41.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.43.41.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.43.41.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.7">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.43.41.7.1">
<tr class="ltx_tr" id="S3.T1.6.2.43.41.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.43.41.7.1.1.1">refinement,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.43.41.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.43.41.7.1.2.1">tracking</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.9">end to end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.11">55.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.12">88.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.43.41.13">81.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.44.42">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.1">Manhardt <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.44.42.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib125" title="">125</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.2">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.44.42.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.44.42.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.44.42.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.44.42.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.44.42.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.44.42.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.44.42.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.44.42.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.44.42.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.44.42.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.7">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.44.42.7.1">
<tr class="ltx_tr" id="S3.T1.6.2.44.42.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.44.42.7.1.1.1">refinement,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.44.42.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.44.42.7.1.2.1">tracking</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.8">generaliztion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.9">end to end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.44.42.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.45.43">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.1">Manhardt <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.45.43.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib126" title="">126</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.2">2019</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.9">end to end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.10">symmetrical objects</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.45.43.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.46.44">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.1">Papaioannidis <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.46.44.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib127" title="">127</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.2">2019</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.5">3DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.46.44.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.46.44.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.46.44.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.46.44.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.46.44.9.1.2.1">D+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.46.44.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.47.45">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.1">Liu <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.47.45.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib128" title="">128</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.2">2019</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.5">3DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.47.45.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.47.45.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.47.45.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.47.45.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.47.45.9.1.2.1">D+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.10">texture-less</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.47.45.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.48.46">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.1">Wen <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.48.46.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib48" title="">48</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.48.46.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.48.46.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.48.46.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.48.46.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.48.46.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.7">tracking</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.8">generaliztion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.9">end to end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.48.46.13">93.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.49.47">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.1">Wang <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.49.47.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib68" title="">68</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.49.47.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.49.47.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.49.47.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.49.47.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.49.47.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.8">generaliztion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.9">end to end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.11">32.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.12">58.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.49.47.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.50.48">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.1">Jiang <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.50.48.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib69" title="">69</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.3">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.9">end to end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.11">30.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.12">97.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.50.48.13">95.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.51.49">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.1">Hai <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.51.49.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib129" title="">129</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.51.49.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.51.49.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.51.49.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.51.49.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.51.49.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.7">refinement</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.9">end to end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.11">66.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.12">99.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.2.51.49.13">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.52.50">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.1">Li <em class="ltx_emph ltx_font_italic" id="S3.T1.6.2.52.50.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib130" title="">130</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.52.50.3.1">
<tr class="ltx_tr" id="S3.T1.6.2.52.50.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.52.50.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.52.50.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.52.50.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.52.50.4.1">
<tr class="ltx_tr" id="S3.T1.6.2.52.50.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.52.50.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.52.50.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.52.50.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.7">refinement</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.9">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.2.52.50.9.1">
<tr class="ltx_tr" id="S3.T1.6.2.52.50.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.52.50.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.52.50.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.2.52.50.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.2.52.50.13">97.0</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Dense Correspondence Methods</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Dense correspondence-based methods utilize a significantly larger number of correspondences compared to sparse correspondence-based methods. This enables them to achieve higher accuracy and handle occlusions more effectively.
Li <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib19" title="">19</a>]</cite> argued for the differentiation between rotation and translation, proposing the coordinates-based disentangled pose network. This network separates pose estimation into distinct predictions for rotation and translation.
Zakharov <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib20" title="">20</a>]</cite> introduced the dense multi-class 2D-3D correspondence-based object pose detector and a tailored deep learning-based refinement process.
In addition, Cai <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib131" title="">131</a>]</cite> proposed a technique to automatically identify and match image landmarks consistently across different views, aiming to enhance the process of learning 2D-3D mapping.
Wang <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib132" title="">132</a>]</cite> developed a pose estimation pipeline guided by reconstruction, capitalizing on geometric consistency.
Further, Shugurov <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib99" title="">99</a>]</cite> built upon Zakharov <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib20" title="">20</a>]</cite> by developing a unified deep network capable of accommodating multiple image modalities (such as RGB and Depth) and integrating a differentiable rendering-based pose refinement method.
Su <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.7">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib133" title="">133</a>]</cite> introduced a discrete descriptor realized by hierarchical binary grouping, capable of densely representing the object surface. As a result, this method can predict fine-grained correspondences.
Chen <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.8">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib100" title="">100</a>]</cite> introduced a probabilistic PnP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib73" title="">73</a>]</cite> layer designed for general end-to-end pose estimation. This layer generates a pose distribution on the SE(3) manifold.
On the other hand, Xu <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.9">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib134" title="">134</a>]</cite> argued that encoding pose-sensitive local features and modeling the statistical distribution of inlier poses are crucial for accurate and robust 6DoF pose estimation. Inspired by PPF<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib11" title="">11</a>]</cite>, they exploited pose-sensitive information carried by each pair of oriented points and an ensemble of redundant pose predictions to achieve robust performance on severe inter-object occlusion and systematic noises in scene point clouds.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">Some methods recover object poses by establishing 3D-3D correspondences.
Huang <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib135" title="">135</a>]</cite> used an RGB image to predict 3D object coordinates in the camera frustum, thus establishing 3D-3D correspondences.
Further, Jiang <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib136" title="">136</a>]</cite> introduced a center-based decoupled framework, leveraging bird’s eye and front views for object center voting. They utilized feature similarity between the center-aligned object and the object CAD model to establish correspondences for Singular Value Decomposition (SVD)-based<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib137" title="">137</a>]</cite> rotation estimation.
More recently, Lin <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib138" title="">138</a>]</cite> utilized an RGBD image as input and employed point-to-surface matching to estimate the object surface correspondence. They establish 3D-3D correspondences by iteratively constricting the surface, transitioning it into a correspondence point while progressively eliminating outliers.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1">Some methods put more effort into handling challenging cases, such as symmetric objects<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib139" title="">139</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib98" title="">98</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib140" title="">140</a>]</cite> and texture-less objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib141" title="">141</a>]</cite>.
Park <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib139" title="">139</a>]</cite> utilized generative adversarial training to reconstruct occluded parts to alleviate the impact of occlusion. They handle symmetric objects by guiding predictions towards the nearest symmetric pose.
In addition, Hodan <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib98" title="">98</a>]</cite> modeled an object using compact surface fragments to handle symmetries in object modeling effectively. For each pixel, the network predicts: the likelihood of each object’s presence, the probability of the fragments conditional on the object’s presence, and the exact 3D translation of each fragment. Finally, the object pose is determined using a robust and efficient version of the PnP-RANSAC algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib73" title="">73</a>]</cite>.
Further, Wu <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p3.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib140" title="">140</a>]</cite> employed a geometric-aware dense matching network to acquire visible dense correspondences. Additionally, they utilized the distance consistency of these correspondences to mitigate ambiguity in symmetrical objects.
For texture-less objects, Wu <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p3.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib141" title="">141</a>]</cite> leveraged information from the object CAD model and established 2D-3D correspondences using a pseudo-Siamese neural network.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p4">
<p class="ltx_p" id="S3.SS1.SSS2.p4.1">With research development, domain adaptation, weak supervision, and self-supervision techniques have been introduced into pose estimation.
Li <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib142" title="">142</a>]</cite> noticed that images with varying levels of realism and semantics exhibit different transferability between synthetic and real domains. Consequently, they decomposed the input image into multi-level semantic representations and merged the strengths of these representations to mitigate the domain gap.
Further, Hu <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p4.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib143" title="">143</a>]</cite> introduced a method exclusively trained on synthetic images, which infers the necessary pose correction for refining rough poses.
Haugaard <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p4.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib101" title="">101</a>]</cite> utilized learned distributions to sample, score, and refine pose hypotheses. Correspondence distributions are learned using a contrastive loss. This method is unsupervised regarding visual ambiguities.
More recently, Li <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p4.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib102" title="">102</a>]</cite> introduced a weakly-supervised reconstruction-based pipeline. Initially, they reconstructed the objects from various viewpoints using an implicit neural representation. Subsequently, they trained a network to predict pixel-wise 2D-3D correspondences.
Hai <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p4.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib144" title="">144</a>]</cite> proposed a refinement strategy that uses the geometry constraint in synthetic-to-real image pairs captured from multiple viewpoints.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p5">
<p class="ltx_p" id="S3.SS1.SSS2.p5.1">There are also methods that focus on pose refinement.
Lipson <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p5.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib145" title="">145</a>]</cite> iteratively refined pose and correspondences in a tightly coupled manner. They incorporated a differentiable layer to refine the pose by solving the bidirectional depth-augmented PnP problem.
In addition, Xu <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p5.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib103" title="">103</a>]</cite> formulated object pose refinement as a non-linear least squares problem using the estimated correspondence field, <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p5.1.3">i.e.</em>, the correspondence between the RGB image and the rendered image using the initial pose. The non-linear least squares problem is then solved by a differentiable levenberg-marquardt algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib146" title="">146</a>]</cite>, enabling end-to-end training.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p6">
<p class="ltx_p" id="S3.SS1.SSS2.p6.1">In general, the aforementioned correspondence-based methods exhibit robustness to occlusion since they can utilize local correspondences to predict object pose. However, these methods may encounter challenges when handling objects that lack salient shape features or texture.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span class="ltx_text ltx_font_italic" id="S3.SS2.1.1">Template-Based Methods</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">By leveraging global information from the image, template-based methods can effectively address the challenges posed by texture-less objects. Template-based methods involve identifying the most similar template from a set of templates labeled with ground-truth object poses. They can be categorized into RGB-based template (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS2.SSS1" title="3.2.1 RGB-Based Template Methods ‣ 3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>) and point cloud-based template (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS2.SSS2" title="3.2.2 Point Cloud-Based Template Methods ‣ 3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>) methods. These two methods are illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.F4" title="Figure 4 ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4</span></a>. When the input is an RGB image, the templates comprise 2D projections extracted from object CAD models, with annotations of ground-truth poses. This process transforms object pose estimation into image retrieval. Conversely, when dealing with a point cloud, the template comprises the object CAD model with the canonical pose. Notably, we classify the methods that directly regress the relative pose between the object CAD model and the observed point cloud as template-based methods. This is because these methods can be interpreted as seeking the optimal relative pose that aligns the observed point cloud with the template. Consequently, the determined relative pose serves as the object pose.
The characteristics and performance of some representative methods are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.T1" title="TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>RGB-Based Template Methods</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">As a seminal contribution, Sundermeyer <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib104" title="">104</a>]</cite> achieved 3D rotation estimation through a variant of denoising autoencoder, which learns an implicit representation of object rotation. If depth is available, it can be used for pose refinement.
Liu <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib147" title="">147</a>]</cite> developed a CNN akin to an autoencoder to reconstruct arbitrary scenes featuring the target object and extract the object area.
In addition, Zhang <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib148" title="">148</a>]</cite> utilized an object detector and a keypoint extractor to simplify the template search process.
Papaioannidis <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib105" title="">105</a>]</cite> suggested that estimating object poses in synthetic images is more straightforward. Therefore, they employed a generative adversarial network to convert real images into synthetic ones while preserving the object pose. Li <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib106" title="">106</a>]</cite> utilized a new pose representation (<em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.6">i.e.</em>, 3D location field) to guide an auto-encoder to distill pose-related features, thereby enhancing the handling of pose ambiguity.
Stevšič <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.7">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib149" title="">149</a>]</cite> proposed a spatial attention mechanism to identify and utilize spatial details for pose refinement.
Different from the above methods, Deng <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.8">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib107" title="">107</a>]</cite> addressed the 6DoF object pose tracking problem within the Rao-Blackwellized particle filtering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib150" title="">150</a>]</cite> framework. They finely discretized the rotation space and trained an autoencoder network to build a codebook of feature embeddings for these discretized rotations. This method efficiently estimates the 3D translation along with the full distribution over the 3D rotation.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">RGB cameras are widely used as visual sensors, yet they struggle to capture sufficient information under poor lighting conditions. This results in poor pose estimation performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Point Cloud-Based Template Methods</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">With the popularity of consumer-grade 3D cameras, point cloud-based methods take full advantage of their ability to adapt to poor illumination and capture geometric information.
Li <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib70" title="">70</a>]</cite> adopted a feature disentanglement and alignment module to establish part-to-part correspondences between the partial point cloud and object CAD model, enhancing geometric constraint.
Jiang <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS2.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib108" title="">108</a>]</cite> proposed a point cloud registration framework based on the SE(3) diffusion model, which gradually perturbs the optimal rigid transformation of a pair of point clouds by continuously injecting perturbation transformation through the SE(3) forward diffusion process. Then, the SE(3) reverse denoising process is used to gradually denoise, making it closer to the optimal transformation for accurate pose estimation.
Dang <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS2.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib109" title="">109</a>]</cite> proposed two key contributions to enhance pose estimation performance on real-world data. First, they introduced a directly supervised loss function that bypasses the SVD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib137" title="">137</a>]</cite> operation, mitigating the sensitivity of SVD-based loss functions to the rotation range between the input partial point cloud and the object CAD model. Second, they devised a match normalization strategy to address disparities in feature distributions between the partial point cloud and the CAD model.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">In general, template-based methods leverage global information from the image, enabling them to effectively handle texture-less objects. However, achieving high pose estimation accuracy may lead to increased memory usage by the templates and a rapid rise in computational complexity. Additionally, they may also exhibit poor performance when confronted with occluded objects.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span class="ltx_text ltx_font_italic" id="S3.SS3.1.1">Voting-Based Methods</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Voting-based methods determine object pose through a pixel-level or point-level voting scheme, which can be categorized into two main types: indirect voting and direct voting. Indirect voting methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS3.SSS1" title="3.3.1 Indirect Voting Methods ‣ 3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>) estimate a set of pre-defined 2D keypoints from the RGB image through pixel-level voting, or a set of pre-defined 3D keypoints from the point cloud via point-level voting. Subsequently, the object pose is determined through 2D-3D or 3D-3D keypoint correspondences between the input image and the CAD model. Direct voting methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS3.SSS2" title="3.3.2 Direct Voting Methods ‣ 3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>) directly predict the pose and confidence at the pixel-level or point-level, then select the pose with the highest confidence as the object pose.
The illustration of these types of methods is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.F4" title="Figure 4 ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4</span></a>. The attributes and performance of some representative methods are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.T1" title="TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Indirect Voting Methods</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Some researchers predicted 2D keypoints and then derived the object pose through 2D-3D keypoints correspondence. Liu <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib151" title="">151</a>]</cite> introduced a continuous representation method called keypoint distance field (KDF), which extracts 2D keypoints by voting on each KDF.
Meanwhile, Cao <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib110" title="">110</a>]</cite> proposed a method called dynamic graph PnP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib73" title="">73</a>]</cite> to learn the object pose from 2D-3D correspondence, enabling end-to-end training.
Moreover, Liu <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib152" title="">152</a>]</cite> introduced a bidirectional depth residual fusion network to fuse RGBD information, thereby estimating 2D keypoints precisely.
Inspired by the diffusion model, Xu <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib153" title="">153</a>]</cite> proposed a diffusion-based framework to formulate 2D keypoint detection as a denoising process to establish more accurate 2D-3D correspondences.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">Unlike the aforementioned methods that predict 2D keypoints, He <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib18" title="">18</a>]</cite> proposed a depth hough voting network to predict 3D keypoints. Subsequently, they estimated the object pose through levenberg-marquardt algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib146" title="">146</a>]</cite>.
Furthermore, He <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib21" title="">21</a>]</cite> introduced a bidirectional fusion network to complement RGB and depth heterogeneous data, thereby better predicting the 3D keypoints.
To better capture features among object points in 3D space, Mei <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib154" title="">154</a>]</cite> utilized graph convolutional networks to facilitate feature exchange among points in 3D space, aiming to improve the accuracy of predicting 3D keypoints.
Wu <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p2.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib111" title="">111</a>]</cite> proposed a 3D keypoint voting scheme based on cross-spherical surfaces, allowing for generating smaller and more dispersed 3D keypoint sets, thus improving estimation efficiency.
To obtain more accurate 3D keypoints, Wang <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p2.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib155" title="">155</a>]</cite> presented an iterative 3D keypoint voting network to refine the initial localization of 3D keypoints.
Most recently, Zhou <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p2.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib112" title="">112</a>]</cite> introduced a novel weighted vector 3D keypoints voting algorithm, which adopts a non-iterative global optimization strategy to precisely localize 3D keypoints, while also achieving near real-time inference speed.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.1">In response to challenging scenarios such as cluttered or occlusion, Peng <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib17" title="">17</a>]</cite> introduced a pixel-wise voting network to regress pixel-level vectors pointing to 3D keypoints. These vectors create a flexible representation for locating occluded or truncated 3D keypoints. Since most industrial parts are parameterized, Zeng <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib156" title="">156</a>]</cite> defined 3D keypoints linked to parameters through driven parameters and symmetries. This approach effectively addresses the pose estimation of objects in stacking scenes.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p4">
<p class="ltx_p" id="S3.SS3.SSS1.p4.1">Rather than utilizing a single-view RGBD image as input, Duffhauss <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib157" title="">157</a>]</cite> employed multi-view RGBD images as input.
They extracted visual features from each RGB image, while geometric features were extracted from the object point cloud (generated by fusing all depth images). This multi-view RGBD feature fusion-based method can accurately predict object pose in cluttered scenes.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p5">
<p class="ltx_p" id="S3.SS3.SSS1.p5.1">Some researchers have proposed new training strategies to improve pose estimation performance. Yu <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p5.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib158" title="">158</a>]</cite> developed a differentiable proxy voting loss that simulates hypothesis selection during the voting process, enabling end-to-end training.
In addition, Lin <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p5.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib159" title="">159</a>]</cite> proposed a novel learning framework, which utilizes the accurate result of the RGBD-based pose refinement method to supervise the RGB-based pose estimator.
To bridge the domain gap between synthetic and real data, Ikeda <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p5.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib160" title="">160</a>]</cite> introduced a method to transfer object style transfer from synthetic to realistic without manual intervention.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p6">
<p class="ltx_p" id="S3.SS3.SSS1.p6.1">Overall, indirect voting-based methods provide an excellent solution for instance-level object pose estimation. However, the accuracy of pose estimation heavily relies on the quality of the keypoints, which can result in lower robustness.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Direct Voting Methods</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">The performance of the indirect voting methods heavily depends on the selection of keypoints. Consequently, direct voting methods have been proposed as an alternative solution. Tian <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib113" title="">113</a>]</cite> uniformly sampled rotation anchors in SO(3). Subsequently, they predicted constraint deviations for each anchor towards the target, using the uncertainty score to select the best prediction. Then, they detected the 3D translation by aggregating point-to-center vectors towards the object center to recover the 6DoF pose.
Wang <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib16" title="">16</a>]</cite> fused RGB and depth features on a per-pixel basis and utilized a pose predictor to generate 6DoF pose and confidence for each pixel. Subsequently, they selected the pose of the pixel with the highest confidence as the final pose.
Zhou <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib161" title="">161</a>]</cite> employed CNNs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib162" title="">162</a>]</cite> to extract RGB features, which are then integrated into the point cloud to obtain fused features. Unlike <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib16" title="">16</a>]</cite>, the fused features take the form of point sets rather than feature mappings.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">However, the aforementioned RGBD fusion methods merely concatenate RGB and depth features without delving into their intrinsic relationship. Therefore, Zhou <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib114" title="">114</a>]</cite> proposed a new multi-modal fusion graph convolutional network to enhance the fusion of RGB and depth images, capturing the inter-modality correlations through local information propagation. Liu <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib163" title="">163</a>]</cite> decoupled scale-related and scale-invariant information in the depth image to guide the network in perceiving the scene’s 3D structure and provide scene texture for the RGB image feature extraction. Unlike the aforementioned approaches that use still images, Mu <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib164" title="">164</a>]</cite> proposed a time fusion model integrating temporal motion information from RGBD images for 6DoF object pose estimation. This method effectively captures object motion and changes, thereby enhancing pose estimation accuracy and stability.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.1">Symmetric objects may have multiple true poses, leading to ambiguity in pose estimation. To address this issue, Mo<em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib115" title="">115</a>]</cite> designed a symmetric-invariant pose distance metric, which enables the network to estimate symmetric objects accurately. Cai <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib165" title="">165</a>]</cite> introduced a 3D rotation representation to learn the object implicit symmetry, eliminating the need for additional prior knowledge about object symmetry.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p4">
<p class="ltx_p" id="S3.SS3.SSS2.p4.1">To reduce dependency on annotated real data, Zeng <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib166" title="">166</a>]</cite> trained their model solely on synthetic dataset. Then, they utilized a sim-to-real learning network to improve their generalization ability. During pose estimation, they transformed scene points into centroid space and obtained object pose through clustering and voting.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p5">
<p class="ltx_p" id="S3.SS3.SSS2.p5.1">Overall, voting-based methods have demonstrated superior performance in pose estimation tasks. However, the voting process is time-consuming and increases computational complexity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib167" title="">167</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span><span class="ltx_text ltx_font_italic" id="S3.SS4.1.1">Regression-Based Methods</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Regression-based methods aim to directly obtain the object pose from the learned features. They can be divided into two main types: geometry-guided regression and direct regression.
Geometry-guided regression methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS4.SSS1" title="3.4.1 Geometry-Guided Regression Methods ‣ 3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.4.1</span></a>) leverage geometric information from RGBD images (such as object 3D structural features or 2D-3D geometric constraints) to assist in object pose estimation.
Direct regression methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.SS4.SSS2" title="3.4.2 Direct Regression Methods ‣ 3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>) directly regress the object pose, utilizing RGBD image information. The illustration of these two types of methods is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.F4" title="Figure 4 ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4</span></a>. The attributes and performance of some representative methods are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.T1" title="TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Geometry-Guided Regression Methods</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">Gao <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib168" title="">168</a>]</cite> employed decoupled networks for rotation and translation regression from the object point cloud.
Meanwhile, Chen <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib117" title="">117</a>]</cite> introduced a rotation residual estimator to estimate the residual between the predicted rotation and the ground truth, enhancing the accuracy of rotation prediction.
Lin <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib169" title="">169</a>]</cite> used a network to extract the geometric features of the object point cloud. Then, they enhanced the pairwise consistency of geometric features by applying spectral convolution on pairwise compatibility graphs.
Additionally, Shi <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib170" title="">170</a>]</cite> learned geometric and contextual features within point cloud blocks. Then, they trained a sub-block network to predict the pose of each point cloud block. Finally, the most reliable block pose is selected as the object pose.
To address the challenge of point cloud-based object pose tracking, Liu <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p1.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib171" title="">171</a>]</cite> proposed a shifted point convolution operation between the point clouds of adjacent frames to facilitate the local context interaction.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1">Approaches solely relying on object point cloud often overlook the object texture details. Therefore, Wen <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib172" title="">172</a>]</cite> and An <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib173" title="">173</a>]</cite> leveraged the complementary nature of RGB and depth information. They improved cross-modal fusion strategies by employing attention mechanisms to effectively align and integrate these two heterogeneous data sources, resulting in enhanced performance.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p3">
<p class="ltx_p" id="S3.SS4.SSS1.p3.1">In contrast to the aforementioned methods that directly derive geometric information from the depth image or the object CAD model, many researchers focused more on generating geometric constraints from the RGB image.
Hu <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib118" title="">118</a>]</cite> learned the 2D offset from the CAD model center to the 3D bounding box corners from the RGB image, and then directly regressed the object pose from the 2D-3D correspondence. Di <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib121" title="">121</a>]</cite> used a shared encoder and two independent decoders to generate 2D-3D correspondence and self-occlusion information, improving the robustness of object pose estimation under occlusion.
Further, Wang <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p3.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib120" title="">120</a>]</cite> proposed a Geometry-Guided Direct Regression Network (GDR-Net) to learn object pose from dense 2D-3D correspondence in an end-to-end manner.
Wang <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p3.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib122" title="">122</a>]</cite> introduced noise-augmented student training and differentiable rendering based on GDR-Net<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib120" title="">120</a>]</cite>, enabling robustness to occlusion scenes through self-supervised learning with multiple geometric constraints.
Zhang <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p3.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib174" title="">174</a>]</cite> proposed a transformer-based pose estimation approach that consists of a patch-aware feature fusion module and a transformer-based pose refinement module to address the limitation of CNN-based networks in capturing global dependencies.
Most recently, Feng <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p3.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib175" title="">175</a>]</cite> decoupled rotation into two sets of corresponding 3D normals. This decoupling strategy significantly improves the rotation accuracy.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p4">
<p class="ltx_p" id="S3.SS4.SSS1.p4.1">Given the labor-intensive nature of real-world data annotation, some methods leverage synthetic data training to generalize to the real world. Gao <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib176" title="">176</a>]</cite> constructed a lightweight synthetic point cloud generation pipeline and leveraged an enhanced point cloud-based autoencoder to learn latent object pose information to regress object pose. To improve generalization to real-world scenes, Zhou <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p4.1.2">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib177" title="">177</a>]</cite> utilized annotated synthetic data to supervise the network convergence. They proposed a self-supervised pipeline for unannotated real data by minimizing the distance between the CAD model transformed from the predicted pose and the input point cloud. Tan <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p4.1.3">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib178" title="">178</a>]</cite> proposed a self-supervised monocular object pose estimation network consisting of teacher and student modules. The teacher module is trained on synthetic data for initial object pose estimation, and the student model predicts camera pose from the unannotated real image. The student module acquires knowledge of object pose estimation from the teacher module by imposing geometric constraints derived from the camera pose.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p5">
<p class="ltx_p" id="S3.SS4.SSS1.p5.1">Geometry-guided regression methods typically require additional processing steps to extract and handle geometric information, which increases computational costs and complexity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Direct Regression Methods</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Direct regression methods aim to directly recover the object pose from the RGBD image without additional transformation steps, thus reducing complexity. These methods encompass various strategies, including coupled pose output, decoupled pose output, and 3D rotation (3DoF pose) output. Coupled pose involves predicting object rotation and translation together, while decoupled pose involves predicting them separately. Moreover, the 3DoF pose output focuses solely on predicting object rotation without considering translation. These strategies are discussed in detail below.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p2.1.1">Coupled Pose:</span> To overcome lighting variations in the environment, Rambach <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib179" title="">179</a>]</cite> used a pencil filter to normalize the input image into light-invariant representations, and then directly regressed the object coupled pose using a CNN network.
Additionally, Kleeberger <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib180" title="">180</a>]</cite> introduced a robust framework to handle occlusions between objects and estimate the multiple objects pose in the image. This framework is capable of running in real-time at 65 FPS.
Sarode <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p2.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib181" title="">181</a>]</cite> introduced a PointNet-based<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib182" title="">182</a>]</cite> framework to align point clouds for pose estimation, aiming to reduce sensitivity to pose misalignment.
Estimating object pose from a single RGB image introduces an inherent ambiguity problem. Manhardt <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p2.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib126" title="">126</a>]</cite> suggested explicitly addressing these ambiguities. They predicted multiple 6DoF poses for each object to estimate specific pose distributions caused by symmetry and repetitive textures. Inspired by the visible surface difference metric, Bengtson <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p2.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib183" title="">183</a>]</cite> relied on a differentiable renderer and the CAD model to generate multiple weighted poses, avoiding falling into local minima.
Moreover, Park <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p2.1.7">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib184" title="">184</a>]</cite> proposed a method for pose estimation based on the local grid in object space. The method locates the grid region of interest on a ray in camera space and transforms the grid into object space via the estimated pose. The transformed grid is a new standard for sampling mesh and estimating pose.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p3">
<p class="ltx_p" id="S3.SS4.SSS2.p3.1">For object pose tracking, Garon <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib185" title="">185</a>]</cite> proposed a real-time tracking method that learns transformation relationships from consecutive frames during training, and used FCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib186" title="">186</a>]</cite> to obtain the relative pose between two frames for training and inference.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p4">
<p class="ltx_p" id="S3.SS4.SSS2.p4.1">Coupled pose may lead to information coupling between rotation and translation, making it difficult to distinguish their relationship during the optimization process, thus affecting estimation accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p5">
<p class="ltx_p" id="S3.SS4.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p5.1.1">Decoupled Pose:</span> Decoupling the 6DoF object pose enables explicit modeling of the dependencies and independencies between object rotation and translation<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p6">
<p class="ltx_p" id="S3.SS4.SSS2.p6.1">In object pose estimation, Xiang <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p6.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib15" title="">15</a>]</cite> estimated the 3D translation by locating the object center in the image and predicting the distance from the object center to the camera. They further estimated the 3D rotation by regressing to a quaternion representation, and introduced a novel loss function to handle symmetric objects better. Meanwhile, Kehl <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p6.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib187" title="">187</a>]</cite> extended the SSD framework<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib188" title="">188</a>]</cite> to generate 2D bounding boxes, as well as confidence scores for each viewpoint and in-plane rotation. Then, they chose the 2D bounding box through non-maximum suppression, along with the highest confidence viewpoint and in-plane rotation to infer the 3D translation, resulting in the full 6DoF object pose.
Wu <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p6.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib189" title="">189</a>]</cite>, Do<em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p6.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib190" title="">190</a>]</cite> and Bukschat <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p6.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib167" title="">167</a>]</cite> used two parallel FCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib186" title="">186</a>]</cite> branches to regress the object rotation and translation independently. To eliminate the dependence on annotations of real data, Wang <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p6.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib68" title="">68</a>]</cite> used synthetic RGB data for fully supervised training, and then leveraged neural rendering for self-supervised learning on unannotated real RGBD data. Moreover, Jiang <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p6.1.7">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib69" title="">69</a>]</cite> fused RGBD, built-in 2D-pixel coordinate encoding, and depth normal vector features to better estimate the object rotation and translation.
Single-view methods suffer from ambiguity, therefore, Li <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p6.1.8">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib123" title="">123</a>]</cite> proposed a multi-view fusion framework to reduce the ambiguity inherent in single-view frameworks. Further, Labbé <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p6.1.9">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib119" title="">119</a>]</cite> proposed a unified approach for multi-view, multi-object pose estimation. Initially, they utilized a single-view, single-object pose estimation technique to derive pose hypotheses for individual objects. Then, they aligned these object pose hypotheses across multiple input images to collectively infer both the camera viewpoints and object pose within a unified scene. Most recently, Hsiao <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p6.1.10">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib191" title="">191</a>]</cite> introduced a score-based diffusion method to solve the pose ambiguity problem in RGB-based object pose estimation.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p7">
<p class="ltx_p" id="S3.SS4.SSS2.p7.1">For object pose tracking, Wen <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p7.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib48" title="">48</a>]</cite> proposed a data-driven optimization strategy to stabilize the 6DoF object pose tracking. Specifically, they predicted the 6DoF pose by predicting the relative pose between the adjacent frames. Liu <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p7.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib192" title="">192</a>]</cite> proposed a new subtraction feature fusion module based on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib48" title="">48</a>]</cite> to establish sufficient spatiotemporal information interaction between adjacent frames, improving the robustness of object pose tracking in complex scenes. Different from the method based on RGBD input, Ge <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p7.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib193" title="">193</a>]</cite> designed a novel deep neural network architecture that integrates visual and inertial features to predict the relative object pose between consecutive image frames.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p8">
<p class="ltx_p" id="S3.SS4.SSS2.p8.1">In object pose refinement, Li <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p8.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib124" title="">124</a>]</cite> iteratively refined pose through aligning RGB image with rendered image of object CAD model. Additionally, they predicted optical flow and foreground masks to stabilize the training procedure.
Manhardt <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p8.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib125" title="">125</a>]</cite> refined the 6DoF pose by aligning object contour between the RGB image and rendered contour. The rendered contour is obtained from the object CAD model using the initial pose. Hai <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p8.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib129" title="">129</a>]</cite> proposed a shape-constraint recursive matching framework to refine the initial pose. They first computed a pose-induced flow based on the initial and currently estimated pose, and then directly decoupled the 6DoF pose from the pose-induced flow.
To address the low running efficiency of the pose refinement methods, Iwase <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p8.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib194" title="">194</a>]</cite> introduced a deep texture rendering-based pose refinement method for fast feature extraction using an object CAD model with a learnable texture.
Most recently, Li <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p8.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib130" title="">130</a>]</cite> proposed a two-stage method. The first stage performs pose classification and renders the object CAD model in the classified poses. The second stage performs regression to predict fine-grained residual in the classified poses. This method improves robustness by guiding residual pose regression through pose classification.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p9">
<p class="ltx_p" id="S3.SS4.SSS2.p9.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p9.1.1">3DoF Pose:</span> Some researchers pursue more efficient and practical pose estimation by solely regressing the 3D rotation. Papaioannidis <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p9.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib127" title="">127</a>]</cite> proposed a novel quaternion-based multi-objective loss function, which integrates manifold learning and regression for learning 3DoF pose descriptors. They obtained the 3DoF pose through the regression of the learned descriptors. Liu <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p9.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib128" title="">128</a>]</cite> trained a triple network based on convolutional neural networks to extract discriminative features from binary images. They incorporated pose-guided methods and regression constraints into the constructed triple network to adapt the features for the regression task, enhancing robustness.
In addition, Josifovski <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p9.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib195" title="">195</a>]</cite> estimated the camera viewpoint related to the object coordinate system by constructing a viewpoint estimation model, thereby obtaining the 3DoF pose appearing in the bounding box.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p10">
<p class="ltx_p" id="S3.SS4.SSS2.p10.1">Overall, direct regression methods simplify the object pose estimation process and further enhance the performance of instance-level methods. However, instance-level methods can only estimate specific object instances in the training data, limiting their generalization to unseen objects. Additionally, most instance-level methods require accurate object CAD models, which is a challenge, especially for objects with complex shapes and textures.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Category-Level Object Pose Estimation</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Research on category-level methods has garnered significant attention due to their potential for generalizing to unseen objects within established categories <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib196" title="">196</a>]</cite>. In this section, we review category-level methods by dividing them into shape prior-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS1" title="4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.1</span></a>) and shape prior-free (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2" title="4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.2</span></a>) methods. The illustration of these two categories is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.F5" title="Figure 5 ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5</span></a>. The characteristics and performance of some representative SOTA methods are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.T2" title="TABLE II ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="278" id="S4.F5.g1" src="extracted/5635000/Fig4.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration of the shape prior-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS1" title="4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.1</span></a>) and shape prior-free (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2" title="4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.2</span></a>) category-level methods. The dashed arrows indicate offline training, which means that we need to train a model offline using the category-level model library to obtain shape priors. (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS1" title="4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.1</span></a>): Taking RGBD input as an example, NOCS shape alignment methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS1.SSS1" title="4.1.1 NOCS Shape Alignment Methods ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>) first learn a model to predict the NOCS shape/map of the object, and then align the object point cloud with the NOCS shape/map through a non-differentiable pose solution method such as the Umeyama algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib197" title="">197</a>]</cite> to solve the object pose. In contrast, direct regress pose methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS1.SSS2" title="4.1.2 Direct Regress Pose Methods ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>) directly regress the object pose from the extracted input features. On the other hand, the shape prior-free methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2" title="4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.2</span></a>) do not have the process of shape priors regression: Depth-guided geometry-aware methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2.SSS1" title="4.2.1 Depth-Guided Geometry-Aware Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>) focus on perceiving the global and local geometric information of the object and leverage these 3D geometric features to estimate the object pose. Conversely, RGBD-guided semantic and geometry fusion methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2.SSS2" title="4.2.2 RGBD-Guided Semantic and Geometry Fusion Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>) regress the object pose by fusing the 2D semantic and 3D geometric information of the object.
</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span class="ltx_text ltx_font_italic" id="S4.SS1.1.1">Shape Prior-Based Methods</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Shape prior-based methods first learn a neural network using CAD models of intra-class seen objects in offline mode to derive shape priors, and then utilize them as 3D geometry prior information to guide intra-class unseen object pose estimation. In this part, we divide the shape prior-based methods into two categories based on their approach to addressing object pose estimation. The first category is Normalized Object Coordinate Space (NOCS) shape alignment methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS1.SSS1" title="4.1.1 NOCS Shape Alignment Methods ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>). They first predict the NOCS shape/map, and then use an offline pose solution method (such as the Umeyama algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib197" title="">197</a>]</cite>) to align the object point cloud with the predicted NOCS shape/map to obtain the object pose. The other category is the pose regression methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS1.SSS2" title="4.1.2 Direct Regress Pose Methods ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>). They directly regress the object pose from the feature level, making the pose acquisition process differentiable. The illustration of these two categories is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.F5" title="Figure 5 ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Representative category-level methods. For each method, we report its 10 properties, which have the same meanings as described in Table. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.T1" title="TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">I</span></a>. D, S, N, K, and P denote object detection, instance segmentation, NOCS shape/map regression, keypoints detection, and pose solution/regression, respectively. Moreover, we report the <math alttext="5^{\circ}" class="ltx_Math" display="inline" id="S4.T2.3.m1.1"><semantics id="S4.T2.3.m1.1b"><msup id="S4.T2.3.m1.1.1" xref="S4.T2.3.m1.1.1.cmml"><mn id="S4.T2.3.m1.1.1.2" xref="S4.T2.3.m1.1.1.2.cmml">5</mn><mo id="S4.T2.3.m1.1.1.3" xref="S4.T2.3.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T2.3.m1.1c"><apply id="S4.T2.3.m1.1.1.cmml" xref="S4.T2.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.m1.1.1.1.cmml" xref="S4.T2.3.m1.1.1">superscript</csymbol><cn id="S4.T2.3.m1.1.1.2.cmml" type="integer" xref="S4.T2.3.m1.1.1.2">5</cn><compose id="S4.T2.3.m1.1.1.3.cmml" xref="S4.T2.3.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.m1.1d">5^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.m1.1e">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math><math alttext="5{\rm{cm}}" class="ltx_Math" display="inline" id="S4.T2.4.m2.1"><semantics id="S4.T2.4.m2.1b"><mrow id="S4.T2.4.m2.1.1" xref="S4.T2.4.m2.1.1.cmml"><mn id="S4.T2.4.m2.1.1.2" xref="S4.T2.4.m2.1.1.2.cmml">5</mn><mo id="S4.T2.4.m2.1.1.1" xref="S4.T2.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.T2.4.m2.1.1.3" mathvariant="normal" xref="S4.T2.4.m2.1.1.3.cmml">c</mi><mo id="S4.T2.4.m2.1.1.1b" xref="S4.T2.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.T2.4.m2.1.1.4" mathvariant="normal" xref="S4.T2.4.m2.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.m2.1c"><apply id="S4.T2.4.m2.1.1.cmml" xref="S4.T2.4.m2.1.1"><times id="S4.T2.4.m2.1.1.1.cmml" xref="S4.T2.4.m2.1.1.1"></times><cn id="S4.T2.4.m2.1.1.2.cmml" type="integer" xref="S4.T2.4.m2.1.1.2">5</cn><ci id="S4.T2.4.m2.1.1.3.cmml" xref="S4.T2.4.m2.1.1.3">c</ci><ci id="S4.T2.4.m2.1.1.4.cmml" xref="S4.T2.4.m2.1.1.4">m</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.m2.1d">5{\rm{cm}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.m2.1e">5 roman_c roman_m</annotation></semantics></math> metric of CAMERA25 and REAL275 datasets (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2" title="2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">2</span></a>).</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.9" style="width:433.6pt;height:660pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-245.9pt,374.1pt) scale(0.468523038369362,0.468523038369362) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.8.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3" id="S4.T2.8.4.4.5">Methods</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.4.4.6">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.8.4.4.6.1">
<tr class="ltx_tr" id="S4.T2.8.4.4.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.6.1.1.1">Published</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.4.4.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.6.1.2.1">Year</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.4.4.7">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.8.4.4.7.1">
<tr class="ltx_tr" id="S4.T2.8.4.4.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.7.1.1.1">Training</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.4.4.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.7.1.2.1">Input</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.4.4.8">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.8.4.4.8.1">
<tr class="ltx_tr" id="S4.T2.8.4.4.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.8.1.1.1">Inference</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.4.4.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.8.1.2.1">Input</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.4.4.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.8.4.4.9.1">
<tr class="ltx_tr" id="S4.T2.8.4.4.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.9.1.1.1">Pose</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.4.4.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.9.1.2.1">DoF</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.4.4.10">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.8.4.4.10.1">
<tr class="ltx_tr" id="S4.T2.8.4.4.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.10.1.1.1">Object</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.4.4.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.10.1.2.1">Property</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.4.4.11">Task</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.4.4.12">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.8.4.4.12.1">
<tr class="ltx_tr" id="S4.T2.8.4.4.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.12.1.1.1">Domain Training</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.4.4.12.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.12.1.2.1">Paradigm</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.4.4.13">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.8.4.4.13.1">
<tr class="ltx_tr" id="S4.T2.8.4.4.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.13.1.1.1">Inference</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.4.4.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.13.1.2.1">Mode</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.4.4.14">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.8.4.4.14.1">
<tr class="ltx_tr" id="S4.T2.8.4.4.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.14.1.1.1">Application</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.4.4.14.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.14.1.2.1">Area</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.2.2.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.6.2.2.2.2">
<tr class="ltx_tr" id="S4.T2.6.2.2.2.2.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.6.2.2.2.2.3.1">CAMERA25</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.2.2.2.2.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.6.2.2.2.2.2.2">
<math alttext="5^{\circ}" class="ltx_Math" display="inline" id="S4.T2.5.1.1.1.1.1.1.m1.1"><semantics id="S4.T2.5.1.1.1.1.1.1.m1.1a"><msup id="S4.T2.5.1.1.1.1.1.1.m1.1.1" xref="S4.T2.5.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T2.5.1.1.1.1.1.1.m1.1.1.2" xref="S4.T2.5.1.1.1.1.1.1.m1.1.1.2.cmml">5</mn><mo id="S4.T2.5.1.1.1.1.1.1.m1.1.1.3" xref="S4.T2.5.1.1.1.1.1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T2.5.1.1.1.1.1.1.m1.1b"><apply id="S4.T2.5.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.5.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.5.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.5.1.1.1.1.1.1.m1.1.1">superscript</csymbol><cn id="S4.T2.5.1.1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.5.1.1.1.1.1.1.m1.1.1.2">5</cn><compose id="S4.T2.5.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.5.1.1.1.1.1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.1.1.1.1.1.1.m1.1c">5^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.1.1.1.1.1.1.m1.1d">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math><math alttext="5{\rm{cm}}" class="ltx_Math" display="inline" id="S4.T2.6.2.2.2.2.2.2.m2.1"><semantics id="S4.T2.6.2.2.2.2.2.2.m2.1a"><mrow id="S4.T2.6.2.2.2.2.2.2.m2.1.1" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1.cmml"><mn id="S4.T2.6.2.2.2.2.2.2.m2.1.1.2" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1.2.cmml">5</mn><mo id="S4.T2.6.2.2.2.2.2.2.m2.1.1.1" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.T2.6.2.2.2.2.2.2.m2.1.1.3" mathvariant="normal" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1.3.cmml">c</mi><mo id="S4.T2.6.2.2.2.2.2.2.m2.1.1.1a" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.T2.6.2.2.2.2.2.2.m2.1.1.4" mathvariant="normal" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.2.2.2.2.2.2.m2.1b"><apply id="S4.T2.6.2.2.2.2.2.2.m2.1.1.cmml" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1"><times id="S4.T2.6.2.2.2.2.2.2.m2.1.1.1.cmml" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1.1"></times><cn id="S4.T2.6.2.2.2.2.2.2.m2.1.1.2.cmml" type="integer" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1.2">5</cn><ci id="S4.T2.6.2.2.2.2.2.2.m2.1.1.3.cmml" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1.3">c</ci><ci id="S4.T2.6.2.2.2.2.2.2.m2.1.1.4.cmml" xref="S4.T2.6.2.2.2.2.2.2.m2.1.1.4">m</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.2.2.2.2.2.2.m2.1c">5{\rm{cm}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.2.2.2.2.2.2.m2.1d">5 roman_c roman_m</annotation></semantics></math> (mAP)</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.4.4.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.8.4.4.4.2">
<tr class="ltx_tr" id="S4.T2.8.4.4.4.2.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.4.2.3.1">REAL275</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.4.4.4.2.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.8.4.4.4.2.2.2">
<math alttext="5^{\circ}" class="ltx_Math" display="inline" id="S4.T2.7.3.3.3.1.1.1.m1.1"><semantics id="S4.T2.7.3.3.3.1.1.1.m1.1a"><msup id="S4.T2.7.3.3.3.1.1.1.m1.1.1" xref="S4.T2.7.3.3.3.1.1.1.m1.1.1.cmml"><mn id="S4.T2.7.3.3.3.1.1.1.m1.1.1.2" xref="S4.T2.7.3.3.3.1.1.1.m1.1.1.2.cmml">5</mn><mo id="S4.T2.7.3.3.3.1.1.1.m1.1.1.3" xref="S4.T2.7.3.3.3.1.1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T2.7.3.3.3.1.1.1.m1.1b"><apply id="S4.T2.7.3.3.3.1.1.1.m1.1.1.cmml" xref="S4.T2.7.3.3.3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.7.3.3.3.1.1.1.m1.1.1.1.cmml" xref="S4.T2.7.3.3.3.1.1.1.m1.1.1">superscript</csymbol><cn id="S4.T2.7.3.3.3.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.7.3.3.3.1.1.1.m1.1.1.2">5</cn><compose id="S4.T2.7.3.3.3.1.1.1.m1.1.1.3.cmml" xref="S4.T2.7.3.3.3.1.1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.3.3.3.1.1.1.m1.1c">5^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.3.3.3.1.1.1.m1.1d">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math><math alttext="5{\rm{cm}}" class="ltx_Math" display="inline" id="S4.T2.8.4.4.4.2.2.2.m2.1"><semantics id="S4.T2.8.4.4.4.2.2.2.m2.1a"><mrow id="S4.T2.8.4.4.4.2.2.2.m2.1.1" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1.cmml"><mn id="S4.T2.8.4.4.4.2.2.2.m2.1.1.2" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1.2.cmml">5</mn><mo id="S4.T2.8.4.4.4.2.2.2.m2.1.1.1" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.T2.8.4.4.4.2.2.2.m2.1.1.3" mathvariant="normal" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1.3.cmml">c</mi><mo id="S4.T2.8.4.4.4.2.2.2.m2.1.1.1a" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.T2.8.4.4.4.2.2.2.m2.1.1.4" mathvariant="normal" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.4.4.4.2.2.2.m2.1b"><apply id="S4.T2.8.4.4.4.2.2.2.m2.1.1.cmml" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1"><times id="S4.T2.8.4.4.4.2.2.2.m2.1.1.1.cmml" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1.1"></times><cn id="S4.T2.8.4.4.4.2.2.2.m2.1.1.2.cmml" type="integer" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1.2">5</cn><ci id="S4.T2.8.4.4.4.2.2.2.m2.1.1.3.cmml" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1.3">c</ci><ci id="S4.T2.8.4.4.4.2.2.2.m2.1.1.4.cmml" xref="S4.T2.8.4.4.4.2.2.2.m2.1.1.4">m</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.4.4.4.2.2.2.m2.1c">5{\rm{cm}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.4.4.4.2.2.2.m2.1d">5 roman_c roman_m</annotation></semantics></math> (mAP)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.6.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.1" rowspan="14"><span class="ltx_text" id="S4.T2.9.5.6.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.9.5.6.1.1.1.1" style="width:8.9pt;height:122.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:122.7pt;transform:translate(-56.89pt,-55.92pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.9.5.6.1.1.1.1.1">Shape Prior-Based Methods</span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.2" rowspan="6"><span class="ltx_text" id="S4.T2.9.5.6.1.2.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.9.5.6.1.2.1.1" style="width:8.9pt;height:102.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:102.6pt;transform:translate(-46.83pt,-45.86pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.9.5.6.1.2.1.1.1">NOCS shape alignment</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.3">Tian <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.6.1.3.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib72" title="">72</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.4">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.6.1.5.1">
<tr class="ltx_tr" id="S4.T2.9.5.6.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.6.1.5.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.6.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.6.1.5.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.6">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.7">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.8">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.9">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.10">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.11">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.6.1.11.1">
<tr class="ltx_tr" id="S4.T2.9.5.6.1.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.6.1.11.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.6.1.11.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.6.1.11.1.2.1">S+N+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.12">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.13">59.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.6.1.14">21.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.7.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.1">Wang <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.7.2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib198" title="">198</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.7.2.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.7.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.7.2.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.7.2.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.7.2.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.7.2.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.7.2.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.7.2.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.7.2.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.7.2.9.1.2.1">S+N+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.11">76.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.7.2.12">34.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.8.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.1">Chen <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.8.3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib23" title="">23</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.8.3.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.8.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.8.3.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.8.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.8.3.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.8.3.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.8.3.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.8.3.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.8.3.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.8.3.9.1.2.1">S+N+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.11">74.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.8.3.12">39.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.9.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.1">Zou <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.9.4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib199" title="">199</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.9.4.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.9.4.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.9.4.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.9.4.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.9.4.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.9.4.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.9.4.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.9.4.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.9.4.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.9.4.9.1.2.1">S+N+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.11">76.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.9.4.12">41.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.10.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.1">Fan <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.10.5.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib200" title="">200</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.10.5.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.10.5.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.10.5.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.10.5.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.10.5.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.10.5.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.10.5.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.10.5.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.10.5.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.10.5.9.1.2.1">S+N+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.10.5.12">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.11.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.1">Wei <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.11.6.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib201" title="">201</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.11.6.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.11.6.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.11.6.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.11.6.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.11.6.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.11.6.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.11.6.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.11.6.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.11.6.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.11.6.9.1.2.1">S+N+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.11.6.12">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.12.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.1" rowspan="8"><span class="ltx_text" id="S4.T2.9.5.12.7.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.9.5.12.7.1.1.1" style="width:8.8pt;height:82.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:82.6pt;transform:translate(-36.92pt,-35.94pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.9.5.12.7.1.1.1.1">Direct regress pose</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.2">Irshad <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.12.7.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib202" title="">202</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.3">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.12.7.4.1">
<tr class="ltx_tr" id="S4.T2.9.5.12.7.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.12.7.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.12.7.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.12.7.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.5">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.6">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.7">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.8">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.9">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.10">end-to-end</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.11">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.12">66.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.12.7.13">29.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.13.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.1">Lin <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.13.8.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib203" title="">203</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.3">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.4">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.8">generalization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.13.8.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.13.8.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.13.8.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.13.8.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.13.8.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.11">70.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.13.8.12">42.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.14.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.1">Zhang <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.14.9.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib204" title="">204</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.14.9.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.14.9.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.14.9.3.1.1.1">Depth,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.14.9.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.14.9.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.4">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.14.9.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.14.9.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.14.9.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.14.9.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.14.9.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.11">75.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.14.9.12">44.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.15.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.1">Zhang <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.15.10.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib205" title="">205</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.15.10.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.15.10.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.15.10.3.1.1.1">Depth,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.15.10.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.15.10.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.4">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.15.10.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.15.10.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.15.10.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.15.10.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.15.10.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.11">79.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.15.10.12">48.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.16.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.1">Liu <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.16.11.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib206" title="">206</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.3">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.4">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.7">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.16.11.7.1">
<tr class="ltx_tr" id="S4.T2.9.5.16.11.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.16.11.7.1.1.1">refinement,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.16.11.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.16.11.7.1.2.1">tracking</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.16.11.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.16.11.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.16.11.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.16.11.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.16.11.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.11">80.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.16.11.12">54.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.17.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.1">Lin <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.17.12.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib24" title="">24</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.17.12.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.17.12.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.17.12.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.17.12.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.17.12.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.8">generalization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.17.12.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.17.12.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.17.12.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.17.12.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.17.12.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.17.12.12">45.0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.18.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.1">Ze <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.18.13.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib55" title="">55</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.18.13.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.18.13.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.18.13.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.18.13.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.18.13.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.8">adaptation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.18.13.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.18.13.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.18.13.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.18.13.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.18.13.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.18.13.12">33.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.19.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.1">Liu <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.19.14.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib207" title="">207</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.19.14.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.19.14.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.19.14.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.19.14.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.19.14.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.8">generalization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.19.14.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.19.14.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.19.14.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.19.14.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.19.14.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.19.14.12">50.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.20.15">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.1" rowspan="34"><span class="ltx_text" id="S4.T2.9.5.20.15.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.9.5.20.15.1.1.1" style="width:8.9pt;height:115.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:115.1pt;transform:translate(-53.13pt,-52.15pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.9.5.20.15.1.1.1.1">Shape Prior-Free Methods</span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.2" rowspan="7"><span class="ltx_text" id="S4.T2.9.5.20.15.2.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.9.5.20.15.2.1.1" style="width:8.9pt;height:131.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:131.3pt;transform:translate(-61.21pt,-60.24pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.9.5.20.15.2.1.1.1">Depth-guided geometry-aware</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.3">Li <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.20.15.3.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib208" title="">208</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.4">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.5">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.6">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.7">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.8">articulated</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.9">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.10">generalization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.11">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.20.15.11.1">
<tr class="ltx_tr" id="S4.T2.9.5.20.15.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.20.15.11.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.20.15.11.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.20.15.11.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.12">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.13">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.20.15.14">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.21.16">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.1">Chen <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.21.16.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib209" title="">209</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.3">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.4">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.21.16.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.21.16.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.21.16.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.21.16.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.21.16.9.1.2.1">D+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.21.16.12">28.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.22.17">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.1">Weng <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.22.17.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib210" title="">210</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.3">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.4">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.6">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.22.17.6.1">
<tr class="ltx_tr" id="S4.T2.9.5.22.17.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.22.17.6.1.1.1">rigid,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.22.17.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.22.17.6.1.2.1">articulated</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.7">tracking</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.22.17.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.22.17.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.22.17.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.22.17.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.22.17.9.1.2.1">N+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.22.17.12">62.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.23.18">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.1">Di <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.23.18.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib25" title="">25</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.3">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.4">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.23.18.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.23.18.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.23.18.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.23.18.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.23.18.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.11">79.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.23.18.12">42.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.24.19">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.1">You <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.24.19.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib211" title="">211</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.3">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.4">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.8">generalization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.24.19.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.24.19.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.24.19.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.24.19.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.24.19.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.24.19.12">16.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.25.20">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.1">Zheng <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.25.20.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib26" title="">26</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.3">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.4">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.25.20.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.25.20.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.25.20.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.25.20.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.25.20.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.11">80.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.25.20.12">55.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.26.21">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.1">Zhang <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.26.21.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib212" title="">212</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.3">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.4">Depth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.7">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.26.21.7.1">
<tr class="ltx_tr" id="S4.T2.9.5.26.21.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.26.21.7.1.1.1">estimation,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.26.21.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.26.21.7.1.2.1">tracking</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.26.21.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.26.21.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.26.21.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.26.21.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.26.21.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.26.21.12">60.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.9.5.5.1" rowspan="10"><span class="ltx_text" id="S4.T2.9.5.5.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.9.5.5.1.1.1" style="width:8.9pt;height:187.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:187.3pt;transform:translate(-89.2pt,-88.23pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.9.5.5.1.1.1.1">RGBD-guided semantic <math alttext="\&amp;" class="ltx_Math" display="inline" id="S4.T2.9.5.5.1.1.1.1.m1.1"><semantics id="S4.T2.9.5.5.1.1.1.1.m1.1a"><mo id="S4.T2.9.5.5.1.1.1.1.m1.1.1" xref="S4.T2.9.5.5.1.1.1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.5.5.1.1.1.1.m1.1b"><and id="S4.T2.9.5.5.1.1.1.1.m1.1.1.cmml" xref="S4.T2.9.5.5.1.1.1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.5.5.1.1.1.1.m1.1c">\&amp;</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.5.5.1.1.1.1.m1.1d">&amp;</annotation></semantics></math> geometry fusion</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.2">Wang <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.5.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib22" title="">22</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.3">2019</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.5">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.6">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.7">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.8">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.9">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.10">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.5.10.1">
<tr class="ltx_tr" id="S4.T2.9.5.5.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.5.10.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.5.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.5.10.1.2.1">(S+N)+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.11">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.12">40.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.5.13">10.0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.27.22">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.1">Wang <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.27.22.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib213" title="">213</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.3">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.7">tracking</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.27.22.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.27.22.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.27.22.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.27.22.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.27.22.9.1.2.1">K+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.27.22.12">33.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.28.23">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.1">Lin <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.28.23.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib214" title="">214</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.3">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.28.23.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.28.23.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.28.23.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.28.23.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.28.23.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.11">70.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.28.23.12">35.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.29.24">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.1">Wen <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.29.24.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib215" title="">215</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.3">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.7">tracking</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.29.24.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.29.24.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.29.24.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.29.24.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.29.24.9.1.2.1">S+K+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.29.24.12">87.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.30.25">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.1">Peng <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.30.25.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib216" title="">216</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.30.25.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.30.25.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.30.25.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.30.25.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.30.25.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.8">adaptation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.30.25.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.30.25.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.30.25.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.30.25.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.30.25.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.30.25.12">33.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.31.26">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.1">Lee <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.31.26.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib217" title="">217</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.31.26.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.31.26.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.31.26.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.31.26.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.31.26.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.8">adaptation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.31.26.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.31.26.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.31.26.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.31.26.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.31.26.9.1.2.1">S+N+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.31.26.12">34.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.32.27">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.1">Lee <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.32.27.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib218" title="">218</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.32.27.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.32.27.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.32.27.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.32.27.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.32.27.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.8">generalization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.32.27.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.32.27.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.32.27.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.32.27.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.32.27.9.1.2.1">S+N+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.32.27.12">35.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.33.28">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.1">Liu <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.33.28.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.3">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.33.28.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.33.28.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.33.28.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.33.28.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.33.28.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.11">79.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.33.28.12">53.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.34.29">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.1">Lin <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.34.29.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib219" title="">219</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.34.29.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.34.29.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.34.29.3.1.1.1">RGBD or</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.34.29.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.34.29.3.1.2.1">Depth</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.34.29.4.1">
<tr class="ltx_tr" id="S4.T2.9.5.34.29.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.34.29.4.1.1.1">RGBD or</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.34.29.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.34.29.4.1.2.1">Depth</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.34.29.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.34.29.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.34.29.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.34.29.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.34.29.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.11">81.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.34.29.12">57.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.35.30">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.1">Chen <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.35.30.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib220" title="">220</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.3">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.35.30.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.35.30.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.35.30.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.35.30.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.35.30.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.35.30.12">63.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.36.31">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.1" rowspan="3"><span class="ltx_text" id="S4.T2.9.5.36.31.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.9.5.36.31.1.1.1" style="width:6.9pt;height:29.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:29.5pt;transform:translate(-11.29pt,-11.29pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.9.5.36.31.1.1.1.1">Others</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.2">Lee <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.36.31.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib221" title="">221</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.3">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.36.31.4.1">
<tr class="ltx_tr" id="S4.T2.9.5.36.31.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.36.31.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.36.31.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.36.31.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.5">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.6">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.7">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.8">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.9">generalization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.10">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.36.31.10.1">
<tr class="ltx_tr" id="S4.T2.9.5.36.31.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.36.31.10.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.36.31.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.36.31.10.1.2.1">S+N+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.11">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.5.36.31.13">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.37.32">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.1">Lin <em class="ltx_emph ltx_font_italic" id="S4.T2.9.5.37.32.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib222" title="">222</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.37.32.3.1">
<tr class="ltx_tr" id="S4.T2.9.5.37.32.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.37.32.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.37.32.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.37.32.3.1.2.1">Text</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.37.32.4.1">
<tr class="ltx_tr" id="S4.T2.9.5.37.32.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.37.32.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.37.32.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.37.32.4.1.2.1">Text</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.5">9DoF</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.8">source</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.5.37.32.9.1">
<tr class="ltx_tr" id="S4.T2.9.5.37.32.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.37.32.9.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.37.32.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.9.5.37.32.9.1.2.1">S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.11">82.2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.9.5.37.32.12">58.3</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>NOCS Shape Alignment Methods</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">As a pioneering work, Tian <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib72" title="">72</a>]</cite> first extracted the shape prior in offline mode, which is used to represent the mean shape of a category of objects. For example, mugs are composed of a cylindrical cup body and an arc-shaped handle. Next, they introduced a shape prior deformation network for the intra-class unseen object to reconstruct its NOCS shape. Finally, the Umeyama algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib197" title="">197</a>]</cite> is employed to solve the object pose by aligning the NOCS shape and the object point cloud. Following Tian <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib72" title="">72</a>]</cite>, some methods aim to reconstruct the NOCS shape more accurately. Specifically, Wang <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib198" title="">198</a>]</cite> designed a recurrent reconstruction network to iteratively refine the reconstructed NOCS shape. Further, Chen <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib23" title="">23</a>]</cite> adjusted the shape prior dynamically by using the structure similarity between the RGBD image and the shape prior. Zou <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib199" title="">199</a>]</cite> proposed two multi-scale transformer-based networks (Pixelformer and Pointformer) for extracting RGB and point cloud features, and subsequently merging them for shape prior deformation. Different from the previous methods, Fan <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib223" title="">223</a>]</cite> introduced an adversarial canonical representation reconstruction framework, which includes a reconstructor and a discriminator of NOCS representation. Specifically, the reconstructor mainly consists of a pose-irrelevant module and a relational reconstruction module to reduce the sensitivity to rotation and translation, as well as to generate high-quality features, respectively. Then, the discriminator is used to guide the reconstructor to generate realistic NOCS representations. Nie <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.7">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib224" title="">224</a>]</cite> improved the accuracy of pose estimation via geometry-informed instance-specific priors and multi-stage shape reconstruction. More recently, Zhou <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.8">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib225" title="">225</a>]</cite> designed a two-stage pipeline consisting of deformation and registration to improve accuracy. Zou <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.9">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib226" title="">226</a>]</cite> introduced a graph-guided point transformer consisting of a graph-guided attention encoder and an iterative non-parametric decoder to further extract the point cloud feature. In addition, Li <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.10">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib227" title="">227</a>]</cite> leveraged discrepancies in instance-category structures alongside potential geometric-semantic associations to better investigate intra-class shape information. Yu <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p1.1.11">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib228" title="">228</a>]</cite> further divided the NOCS shape reconstruction process into three parts: coarse deformation, fine deformation, and recurrent refinement to enhance the accuracy of NOCS shape reconstruction.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">Given that the annotation of ground-truth object pose is time-consuming, He <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib229" title="">229</a>]</cite> explored a self-supervised method via enforcing the geometric consistency between point cloud and category prior mesh, avoiding using the real-world pose annotation. Further, Li <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib230" title="">230</a>]</cite> first extracted semantic primitives via a part segmentation network, and leveraged semantic primitives to compute SIM(3)-invariant shape descriptor to generate the optimized shape. Then, the Umeyama algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib197" title="">197</a>]</cite> is utilized to recover the object pose. Through this approach, they achieved domain generalization, bridging the gap between synthesis and real-world application.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">Depth images may be unavailable in some challenging scenes (<em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p3.1.1">e.g.</em>, under strong or low light conditions). Therefore, achieving monocular category-level object pose estimation is of great significance across various applications. Fan <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p3.1.2">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib200" title="">200</a>]</cite> directly predicted object-level depth and NOCS shape from a monocular RGB image by deforming the shape prior, and subsequently leveraged the Umeyama algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib197" title="">197</a>]</cite> to solve the object pose. Unlike <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib200" title="">200</a>]</cite>, Wei <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p3.1.3">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib201" title="">201</a>]</cite> estimated the 2.5D sketch and separated scale recovery using the shape prior. They then reconstructed the NOCS shape, employing the RANSAC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib73" title="">73</a>]</cite> algorithm to remove outliers, before utilizing the PnP algorithm for recovering the object pose. For transparent objects, Chen <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p3.1.4">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib231" title="">231</a>]</cite> proposed a new solution based on stereo vision, which defines a back-view NOCS map to tackle the problem of image content aliasing.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1">In general, although these NOCS shape alignment methods can recover the object pose, the alignment process is non-differentiable and is not integrated into the learning process. Thus, errors in predicting the NOCS shape/map have a significant impact on the accuracy of pose estimation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Direct Regress Pose Methods</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Due to the non-differentiable nature of the NOCS shape alignment process, several direct regression-based pose methods have been proposed recently to enable end-to-end training. Irshad <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib202" title="">202</a>]</cite> treated object instances as spatial centers and proposed an end-to-end method that combines object detection, reconstruction, and pose estimation. Wang <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib232" title="">232</a>]</cite> developed a deformable template field to decouple shape and pose deformation, improving the accuracy of shape reconstruction and pose estimation. On the other hand, Zhang <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib204" title="">204</a>]</cite> proposed a symmetry-aware shape prior deformation method, which integrates shape prior into a direct pose estimation network. Further, Zhang <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib205" title="">205</a>]</cite> introduced a geometry-guided residual object bounding box projection framework to address the challenge of insufficient pose-sensitive feature extraction. In order to obtain a more precise object pose, Liu <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib206" title="">206</a>]</cite> designed CATRE, a pose refinement method based on the alignment of the shape prior and the object point cloud to refine the object pose estimated by the above methods. Zheng <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib233" title="">233</a>]</cite> extended CATRE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib206" title="">206</a>]</cite> to address the geometric variation problem by integrating hybrid scope layers and learnable affine transformations.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">Due to the extensive manual effort required for annotating real-world training data, Lin <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib203" title="">203</a>]</cite> explored the shape alignment of each intra-class unseen instance against its corresponding category-level shape prior, implicitly representing its 3D rotation. This approach facilitates domain generalization from synthesis to real-world scenarios. Further, Ze <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib55" title="">55</a>]</cite> proposed a novel framework based on pose and shape differentiable rendering to achieve domain adaptation object pose estimation. In addition, they collected a large Wild6D dataset for category-level object pose estimation in the wild. Following Ze <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib55" title="">55</a>]</cite>, Zhang <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p2.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib234" title="">234</a>]</cite> introduced 2D-3D and 3D-2D geometry correspondences to enhance the ability of domain adaptation. Different from the previous approaches, Remus <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p2.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib235" title="">235</a>]</cite> leveraged instance-level methods for domain-generalized category-level object pose estimation via a single RGB image. Lin <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p2.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib24" title="">24</a>]</cite> proposed a deep prior deformation-based network and leveraged a parallel learning scheme to achieve domain generalization.
More recently, Liu <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p2.1.7">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib207" title="">207</a>]</cite> designed a multi-hypothesis consistency learning framework. This framework addresses the uncertainty problem and reduces the domain gap between synthetic and real-world datasets by employing multiple feature extraction and fusion techniques.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">Overall, while the shape prior-based methods mentioned above significantly improve pose estimation performance, obtaining the shape priors requires constructing category-level CAD model libraries and subsequently training a network, which is both cumbersome and time-consuming.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span class="ltx_text ltx_font_italic" id="S4.SS2.1.1">Shape Prior-Free Methods</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Shape prior-free methods do not rely on using shape priors and thus have better generalization capabilities. These methods can be divided into three main categories: depth-guided geometry-aware (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2.SSS1" title="4.2.1 Depth-Guided Geometry-Aware Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>), RGBD-guided semantic and geometry fusion (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2.SSS2" title="4.2.2 RGBD-Guided Semantic and Geometry Fusion Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>), and other (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.SS2.SSS3" title="4.2.3 Others ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4.2.3</span></a>) methods. The illustration of the first two categories is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4.F5" title="Figure 5 ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Depth-Guided Geometry-Aware Methods</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Thanks to the rapid development of 3D Graph Convolution (3DGC), Chen <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib209" title="">209</a>]</cite> leveraged 3DGC and introduced a fast shape-based method, which consists of an RGB-based network to achieve 2D object detection, a shape-based network for 3D segmentation and rotation regression, and a residual-based network for translation and size regression.
Inspired by Chen <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib209" title="">209</a>]</cite>, Liu <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib236" title="">236</a>]</cite> improved the network with structure encoder and reasoning attention.
Further, Di <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib25" title="">25</a>]</cite> proposed a geometry-guided point-wise voting method that exploits geometric insights to enhance the learning of pose-sensitive features. Specifically, they designed a symmetry-aware point cloud reconstruction network and introduced a point-wise bounding box voting mechanism during training to add additional geometric guidance.
Due to the translation and scale invariant properties of the 3DGC, these methods are limited in perceiving object translation and size information. Based on this, Zheng <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p1.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib26" title="">26</a>]</cite> further designed a hybrid scope feature extraction layer, which can simultaneously perceive global and local geometric structures and encode size and translation information.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">Besides the above 3DGC-based methods, Deng <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib237" title="">237</a>]</cite> combined a category-level auto-encoder with a particle filter framework to achieve object pose estimation and tracking.
Wang <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib238" title="">238</a>]</cite> leveraged learnable sparse queries as implicit prior to perform deformation and matching for pose estimation.
In addition, Wan <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib239" title="">239</a>]</cite> developed a semantically-aware object coordinate space to address the semantically incoherent problem of NOCS<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib22" title="">22</a>]</cite>.
More recently, Zhang <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p2.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib212" title="">212</a>]</cite> proposed a scored-based diffusion model to address the multi-hypothesis problem in symmetric objects and partial point clouds. They first leveraged the scored-based diffusion model to generate multiple pose candidates, and then utilized an energy-based diffusion model to remove abnormal poses.
On the other hand, Lin <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p2.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib240" title="">240</a>]</cite> first introduced an instance-adaptive keypoints detection method and then designed a geometric-aware global and local features aggregation network based on the detected keypoints for pose and size estimation. Li <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p2.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib241" title="">241</a>]</cite> leveraged category-level method to determine part object poses for assembling multi-part multi-joint 3D shape.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">To perform pose estimation on articulated objects, Li <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib208" title="">208</a>]</cite> inspired by Wang <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib22" title="">22</a>]</cite>, introduced a standard representation for different articulated objects within a category by designing an articulation-aware normalized coordinate space hierarchy, which simultaneously constructs a canonical object space and a set of canonical part spaces.
Weng <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p3.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib210" title="">210</a>]</cite> further proposed CAPTRA, a unified framework that enables 9DoF pose tracking of rigid and articulated objects simultaneously.
Due to the nearly unlimited freedom of garments and extreme self-occlusion, Chi <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p3.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib242" title="">242</a>]</cite> introduce GarmentNets, which conceptualizes deformable object pose estimation as a shape completion problem within a canonical space. More recently, Liu <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p3.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib243" title="">243</a>]</cite> developed a reinforcement learning-based pipeline to predict 9DoF articulated object pose via fitting joint states through reinforced agent training.
Further, Liu <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p3.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib244" title="">244</a>]</cite> learned part-level SE(3)-equivariant features via a pose-aware equivariant point convolution operator to address the issue of self-supervised articulated object pose estimation.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">To avoid using extensive real-world labeled data for training, Li <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib245" title="">245</a>]</cite> used SE(3) equivariant point cloud networks for self-supervised object pose estimation.
You <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p4.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib211" title="">211</a>]</cite> introduced a category-level point pair feature voting method to reduce the impact of synthetic to real-world domain gap, achieving generalizable object pose estimation in the wild.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1">In general, these methods fully extract the pose-related geometric features. However, the absence of semantic information limits their better performance. Appropriate fusion of semantic and geometric information can significantly improve the robustness of pose estimation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>RGBD-Guided Semantic and Geometry Fusion Methods</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">As a groundbreaking research, Wang <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib22" title="">22</a>]</cite> designed a normalized object coordinate space to provide a canonical representation for a category of objects. They first predicted the class label, mask, and NOCS map of the intra-class unseen object. Then, they utilized the Umeyama algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib197" title="">197</a>]</cite> to solve object pose by aligning the NOCS map with the object point cloud.
To handle various shape changes of intra-class objects, Chen <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib246" title="">246</a>]</cite> learned a canonical shape space as a unified representation.
On the other hand, Lin <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib247" title="">247</a>]</cite> explored the applicability of sparse steerable convolution (SSC) to object pose estimation and proposed an SSC-based pipeline.
Further, Lin <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib214" title="">214</a>]</cite> proposed a dual pose network, which consists of a shared pose encoder and two parallel explicit and implicit pose decoders. The implicit decoder can enforce predicted pose consistency when there are no CAD models during inference. Wang <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib248" title="">248</a>]</cite> designed an attention-guided network with relation-aware and structure-aware for RGB image and point cloud features fusion.
Very recently, Liu <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib27" title="">27</a>]</cite> explored the necessity of shape priors for shape reconstruction of intra-class unseen objects. They demonstrated that the deformation process is more important than the shape prior and proposed a prior-free implicit space transformation network.
Lin <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.7">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib219" title="">219</a>]</cite> addressed the poor rotation estimation accuracy by decoupling the rotation estimation into viewpoint and in-plane rotation. In addition, they also proposed a spherical feature pyramid network based on spatial spherical convolution to process spherical signals.
With the rapid development of the Large Vision Model (LVM), Chen <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.8">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib220" title="">220</a>]</cite> further leveraged the LVM DINOv2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib249" title="">249</a>]</cite> to extract the SE(3)-consistent semantic features and fused them with object-specific hierarchical geometric features to encapsulate category-level information for rotation estimation.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">Since the above methods still require a large amount of real-world annotated training data, their applicability in real-world scenes is limited. To this end, Peng <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib216" title="">216</a>]</cite> proposed a real-world self-supervised training framework based on deep implicit shape representation. They leveraged the deep signed distance function<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib250" title="">250</a>]</cite> as a 3D representation to achieve domain adaptation from synthesis to the real world. In addition, Lee <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib217" title="">217</a>]</cite> introduced a teacher-student self-supervised learning mechanism. They used supervised training in the source domain and self-supervised training in the target domain, effectively achieving domain adaptation.
Recently, Lee <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib218" title="">218</a>]</cite> further proposed a test-time adaptation framework for domain-generalized category-level object pose estimation. Specifically, they first trained the model using labeled synthetic data and then leveraged the pre-trained model for test-time adaptation in the real world during inference.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">To improve the running speed of the object pose estimation method, once the object pose of the first frame is acquired, continuous spatio-temporal information can be utilized to track the object pose. Wang <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib213" title="">213</a>]</cite> proposed an anchors-based object pose tracking method. They first detected the anchors of each frame as the keypoints, and then solved the relative object pose through the keypoints correspondence. Wen <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib215" title="">215</a>]</cite> first obtained continuous frame RGBD masks through the video segmentation network, transductive-VOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib251" title="">251</a>]</cite>, and then leveraged LF-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib252" title="">252</a>]</cite> for generalized keypoints detection. Next, they matched keypoints between consecutive frames and performed coarse registration to estimate the initial relative pose. Finally, a memory-augmented pose graph optimization method is proposed for continuous pose tracking.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1">Overall, these RGBD-guided semantic and geometry fusion methods achieve superior performance. However, if the input depth image contains errors, the accuracy of pose estimation can significantly decrease. Hence, ensuring robustness in pose estimation when dealing with erroneous or missing depth images is crucial.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Others</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Since most mobile devices are not equipped with depth cameras, Chen <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib253" title="">253</a>]</cite> incorporated a neural synthesis module with a gradient-based fitting procedure to simultaneously predict object shape and pose, achieving monocular object pose estimation. Lee <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib221" title="">221</a>]</cite> estimated the NOCS shape and the metric scale shape of the object, and performed a similarity transformation between them to solve the object pose and size. Further, Yen-Chen <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib254" title="">254</a>]</cite> inverted neural radiance fields for monocular category-level pose estimation. Different from the previous methods, Lin <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib255" title="">255</a>]</cite> proposed a keypoint-based single-stage pipeline via a single RGB image. Guo <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib256" title="">256</a>]</cite> redefined the monocular category-level object pose estimation problem from a long-horizon visual navigation perspective. On the other hand, Ma <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib257" title="">257</a>]</cite> enhanced the robustness of the monocular method in occlusion scenes through coarse-to-fine rendering of neural features. Given that transparent instances lack both color and depth information, Zhang <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.1.7">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib258" title="">258</a>]</cite> proposed to utilize depth completion and surface normal estimation to achieve category-level pose estimation for transparent instances.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">In order to improve the running efficiency of the monocular method, Lin <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib259" title="">259</a>]</cite> developed a keypoint-based monocular object pose tracking approach. This approach demonstrates the significance of integrating uncertainty estimation using a tracklet-conditioned deep network and probabilistic filtering. Following Lin <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib259" title="">259</a>]</cite>, Yu <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib260" title="">260</a>]</cite> further improved the pose tracking accuracy through a network that combines convolutions and transformers.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">To further improve the generalization of category-level methods, Goodwin <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib261" title="">261</a>]</cite> introduced a reference image-based zero-shot approach, which first extracts spatial feature descriptors and builds cyclical descriptor distances. Then, they established the top-k semantic correspondences for pose estimation. Zaccaria <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib262" title="">262</a>]</cite> proposed a self-supervised framework via optical flow consistency. Very recently, Cai <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p3.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib263" title="">263</a>]</cite> developed an open-vocabulary framework that aims to generalize to unseen categories using textual prompts in unseen scene images. Felice <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p3.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib264" title="">264</a>]</cite> explored zero-shot novel view synthesis based on a diffusion model for 3D object reconstruction, and recovered the object pose through correspondences. Lin <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p3.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib222" title="">222</a>]</cite> used a pre-trained vision-language model to make full use of rich semantic knowledge and align the representations of the three modalities (image, point cloud, and text) in the feature space through multi-modal contrastive learning.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.1">On the whole, these shape prior-free methods circumvent the reliance on shape priors and further improve the generalization ability of category-level object pose estimation methods. Nevertheless, these methods are limited to generalizing within intra-class unseen objects. For objects of different categories, the training data need to be collected and the models need to be retrained, which remains a significant limitation.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Unseen Object Pose Estimation</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Unseen object pose estimation methods can generalize to unseen objects without the need for retraining. Point Pair Features (PPF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib11" title="">11</a>]</cite> is a classical method for unseen object pose estimation that utilizes oriented point pair features to build global model description and a fast voting scheme to match locally. The final pose is solved by pose clustering and iterative closest point <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib137" title="">137</a>]</cite> refinement. However, PPF suffers from low accuracy and slow runtime, limiting its applicability. In contrast, deep learning-based methods leverage neural networks to learn more complex features from data without specifically designed feature engineering, thus enhancing accuracy and efficiency. In this section, we review the deep learning-based unseen object pose estimation methods and classify them into CAD model-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS1" title="5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.1</span></a>) and manual reference view-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS2" title="5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.2</span></a>) methods. The illustration of these two categories of methods is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.F6" title="Figure 6 ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span class="ltx_text ltx_font_italic" id="S5.SS1.1.1">CAD Model-Based Methods</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The CAD model-based methods involve utilizing the object CAD model as prior knowledge during the process of estimating the pose of an unseen object. These methods can be further categorized into feature matching-based and template matching-based methods. Feature matching-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS1.SSS1" title="5.1.1 Feature Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.1.1</span></a>) focus on designing a network to match features between the CAD model and the query image, establishing 2D-3D or 3D-3D correspondences, and solving the pose by the PnP algorithm or least squares method. Template matching-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS1.SSS2" title="5.1.2 Template Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.1.2</span></a>) utilize rendered templates from the CAD model for retrieval. The initial pose is acquired based on the most similar template, and further refinement is necessary using a refiner to obtain a more accurate pose. The illustration of these two categories of methods is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.F6" title="Figure 6 ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">6</span></a>. The characteristics and performance of some representative methods are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.T3" title="TABLE III ‣ 5.1.1 Feature Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="244" id="S5.F6.g1" src="extracted/5635000/Fig5.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Illustration of the CAD model-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS1" title="5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.1</span></a>) and manual reference view-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS2" title="5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.2</span></a>) methods for unseen object pose estimation. (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS1" title="5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.1</span></a>): The feature matching-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS1.SSS1" title="5.1.1 Feature Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.1.1</span></a>) focus on designing a network to match features between the CAD model and the query image, establishing correspondences (2D-3D or 3D-3D), and solving the pose using the PnP algorithm or least squares method. The template matching-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS1.SSS2" title="5.1.2 Template Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.1.2</span></a>) utilize rendered templates from the CAD model for retrieval. The initial pose is acquired based on the most similar template, and further refinement is necessary using a refiner to obtain a more accurate pose. (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS2" title="5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.2</span></a>): There are two types of feature matching-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS2.SSS1" title="5.2.1 Feature Matching-Based Methods ‣ 5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>). One involves extracting features from reference views and the query image, obtaining 3D-3D correspondences through a feature matching network. The other initially reconstructs the 3D object representation using the reference views and establishes the 2D-3D correspondences between the query image and the 3D representation. The object pose is solved using correspondence-based algorithms, like PnP or the least squares method. Template matching-based methods (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS2.SSS2" title="5.2.2 Template Matching-Based Methods ‣ 5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.2.2</span></a>) also have two types. One reconstructs the 3D object representation using the reference views and then renders multiple templates. The initial pose is acquired by retrieving the most similar template and then refining it to get the final pose. The other directly uses the reference views as the templates for template matching.
</figcaption>
</figure>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Feature Matching-Based Methods</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">As an early exploratory work, Pitteri <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS1.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib265" title="">265</a>]</cite> proposed a 3DoF pose estimation approach that approximates object’s geometry using only the corner points of the CAD model. Nonetheless, it only works effectively on objects having specific corners. Hence, Pitteri <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS1.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib266" title="">266</a>]</cite> further introduced an embedding that captures the local geometry of 3D points on the object surface. Matching these embeddings can create 2D-3D correspondences, and the pose is then determined using the PnP+RANSAC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib73" title="">73</a>]</cite> algorithm. However, these methods only estimate the 3DoF pose.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1">Gou <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS1.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib267" title="">267</a>]</cite> defined the challenge of estimating the 6DoF pose of unseen objects, offering a baseline solution through the identification of 3D correspondences between object and scene point clouds. Similarly, Hagelskjær <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS1.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib268" title="">268</a>]</cite> trained a network to match keypoints from the CAD model to the object point cloud. Yet, it focuses on bin picking with homogeneous bins, which only demonstrates that generalized pose estimation can achieve outstanding performance in restricted scenarios.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.p3.1">Inspired by point cloud registration methods on unseen objects, Zhao <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS1.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib269" title="">269</a>]</cite> proposed a geometry correspondence-based method using generic and object-agnostic geometry features to establish unambiguous and robust 3D-3D correspondences. Nevertheless, it still needs to get the class label and segmentation mask of unseen objects through other methods such as Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib270" title="">270</a>]</cite>. To this end, Chen <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS1.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib271" title="">271</a>]</cite> explored a framework named ZeroPose, which realizes joint instance segmentation and pose estimation of unseen objects. Specifically, they utilized the foundation model SAM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib272" title="">272</a>]</cite> to generate possible object proposals and adopted a template matching method to accomplish instance segmentation. After that, they developed a hierarchical geometric feature matching network based on GeoTransformer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib273" title="">273</a>]</cite> to establish correspondences. Following ZeroPose, Lin <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS1.p3.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib30" title="">30</a>]</cite> devised a novel matching score in terms of semantics, appearance, and geometry to obtain better segmentation. As for pose estimation, they proposed a two-stage partial-to-partial point matching model to construct dense 3D-3D correspondence effectively.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p4">
<p class="ltx_p" id="S5.SS1.SSS1.p4.1">Besides these methods employing geometry features, Caraffa <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS1.p4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib274" title="">274</a>]</cite> devised a method that fuses visual and geometric features extracted from different pre-trained models to enhance pose prediction stability and accuracy. It is the first technique to estimate the unseen object pose by utilizing the synergy between geometric and vision foundation models. Additionally, Huang <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS1.p4.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib275" title="">275</a>]</cite> proposed a method for object pose prediction from RGBD images by combining 2D texture and 3D geometric cues.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p5">
<p class="ltx_p" id="S5.SS1.SSS1.p5.1">To sum up, feature matching-based methods aim to extract generic object-agnostic features and achieve strong correspondences by matching these features. However, these methods require not only robust feature matching models but also tailored designs to enhance the representation of object features, presenting a significant challenge.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Representative CAD-based methods. Since the domain training paradigm of most unseen methods is domain generalization, we report 9 properties for each method, which have the same meanings as described in the caption of Table. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.T1" title="TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">I</span></a>. D, S, F, T, P, R, and V denote object detection, instance segmentation, feature matching to build correspondences, template matching to retrieve pose, pose solution/regression, pose refinement, and pose voting, respectively. We report the <em class="ltx_emph ltx_font_italic" id="S5.T3.10.1">BOP-M</em> across the LM-O and YCB-V datasets (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2" title="2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">2</span></a>) for various methods. Notably, these methods use Mask-RCNN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib270" title="">270</a>]</cite> (normal font), CNOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib276" title="">276</a>]</cite> or their own proposed methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib271" title="">271</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib30" title="">30</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib277" title="">277</a>]</cite> (<span class="ltx_text ltx_font_bold" id="S5.T3.11.2">bold</span>), and a combination of PPF and SIFT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib278" title="">278</a>]</cite> (<span class="ltx_text ltx_font_italic" id="S5.T3.12.3">italics</span>) for unseen object location, respectively. Moreover, Örnek <em class="ltx_emph ltx_font_italic" id="S5.T3.13.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib279" title="">279</a>]</cite> and Caraffa <em class="ltx_emph ltx_font_italic" id="S5.T3.14.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib274" title="">274</a>]</cite> don’t require any task-specific training, we use ”<math alttext="\times" class="ltx_Math" display="inline" id="S5.T3.2.m1.1"><semantics id="S5.T3.2.m1.1b"><mo id="S5.T3.2.m1.1.1" xref="S5.T3.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.m1.1c"><times id="S5.T3.2.m1.1.1.cmml" xref="S5.T3.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.m1.1e">×</annotation></semantics></math>” to denote it.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.4" style="width:433.6pt;height:357.6pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-210.4pt,173.3pt) scale(0.507453875893502,0.507453875893502) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.2.3.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3" id="S5.T3.4.2.3.1.1">Methods</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.3.1.2">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.3.1.2.1">
<tr class="ltx_tr" id="S5.T3.4.2.3.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.2.1.1.1">Published</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.3.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.2.1.2.1">Year</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.3.1.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.3.1.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.3.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.3.1.1.1">Training</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.3.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.3.1.2.1">Input</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.3.1.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.3.1.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.3.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.4.1.1.1">Inference</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.3.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.4.1.2.1">Input</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.3.1.5">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.3.1.5.1">
<tr class="ltx_tr" id="S5.T3.4.2.3.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.5.1.1.1">Pose</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.3.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.5.1.2.1">DoF</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.3.1.6">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.3.1.6.1">
<tr class="ltx_tr" id="S5.T3.4.2.3.1.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.6.1.1.1">Object</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.3.1.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.6.1.2.1">Property</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.3.1.7">Task</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.3.1.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.3.1.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.3.1.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.8.1.1.1">Inference</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.3.1.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.8.1.2.1">Mode</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.3.1.9">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.3.1.9.1">
<tr class="ltx_tr" id="S5.T3.4.2.3.1.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.9.1.1.1">Application</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.3.1.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.9.1.2.1">Area</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.3.1.10">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.3.1.10.1">
<tr class="ltx_tr" id="S5.T3.4.2.3.1.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.10.1.1.1">LM-O</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.3.1.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.10.1.2.1"><em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.3.1.10.1.2.1.1">BOP-M</em></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.3.1.11">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.3.1.11.1">
<tr class="ltx_tr" id="S5.T3.4.2.3.1.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.11.1.1.1">YCB-V</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.3.1.11.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.3.1.11.1.2.1"><em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.3.1.11.1.2.1.1">BOP-M</em></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.1" rowspan="32"><span class="ltx_text" id="S5.T3.4.2.4.2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.2.4.2.1.1.1" style="width:6.9pt;height:123.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:123.4pt;transform:translate(-58.25pt,-58.25pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T3.4.2.4.2.1.1.1.1">CAD Model-Based Methods</span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.2" rowspan="6"><span class="ltx_text" id="S5.T3.4.2.4.2.2.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.2.4.2.2.1.1" style="width:8.9pt;height:76.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:76.6pt;transform:translate(-33.83pt,-32.86pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T3.4.2.4.2.2.1.1.1">Feature matching</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.3">Pitteri <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.4.2.3.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib266" title="">266</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.4">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.5">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.4.2.5.1">
<tr class="ltx_tr" id="S5.T3.4.2.4.2.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.4.2.5.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.4.2.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.4.2.5.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.6">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.4.2.6.1">
<tr class="ltx_tr" id="S5.T3.4.2.4.2.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.4.2.6.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.4.2.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.4.2.6.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.7">3DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.8">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.9">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.10">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.4.2.10.1">
<tr class="ltx_tr" id="S5.T3.4.2.4.2.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.4.2.10.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.4.2.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.4.2.10.1.2.1">S+F+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.11">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.12">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.4.2.13">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.1">Zhao <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.5.3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib269" title="">269</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.5.3.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.5.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.5.3.3.1.1.1">Depth,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.5.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.5.3.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.5.3.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.5.3.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.5.3.4.1.1.1">Depth,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.5.3.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.5.3.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.5.3.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.5.3.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.5.3.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.5.3.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.5.3.8.1.2.1">S+F+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.10">65.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.5.3.11">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.1">Chen <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.6.4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib271" title="">271</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.6.4.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.6.4.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.6.4.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.6.4.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.6.4.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.6.4.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.6.4.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.6.4.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.6.4.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.6.4.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.6.4.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.6.4.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.6.4.8.1.1.1">four-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.6.4.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.6.4.8.1.2.1">S+F+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.10"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.6.4.10.1">49.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.6.4.11"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.6.4.11.1">57.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.1">Lin <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.7.5.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib30" title="">30</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.7.5.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.7.5.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.7.5.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.7.5.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.7.5.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.7.5.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.7.5.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.7.5.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.7.5.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.7.5.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.7.5.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.7.5.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.7.5.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.7.5.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.7.5.8.1.2.1">S+F+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.10"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.7.5.10.1">69.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.7.5.11"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.7.5.11.1">84.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.1">Huang <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.8.6.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib275" title="">275</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.8.6.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.8.6.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.8.6.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.8.6.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.8.6.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.8.6.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.8.6.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.8.6.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.8.6.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.8.6.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.8.6.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.8.6.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.8.6.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.8.6.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.8.6.8.1.2.1">S+F+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.10"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.8.6.10.1">56.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.8.6.11"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.8.6.11.1">60.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.2">Caraffa <em class="ltx_emph ltx_font_italic" id="S5.T3.3.1.1.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib274" title="">274</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.3">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T3.3.1.1.1.m1.1"><semantics id="S5.T3.3.1.1.1.m1.1a"><mo id="S5.T3.3.1.1.1.m1.1.1" xref="S5.T3.3.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.1.1.1.m1.1b"><times id="S5.T3.3.1.1.1.m1.1.1.cmml" xref="S5.T3.3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.1.1.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.3.1.1.4.1">
<tr class="ltx_tr" id="S5.T3.3.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.3.1.1.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.3.1.1.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.3.1.1.8.1">
<tr class="ltx_tr" id="S5.T3.3.1.1.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.3.1.1.8.1.1.1">four-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.1.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.3.1.1.8.1.2.1">S+F+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.10"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.10.1">69.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.1.11"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.11.1">85.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.9.7">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.1" rowspan="20"><span class="ltx_text" id="S5.T3.4.2.9.7.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.2.9.7.1.1.1" style="width:8.9pt;height:84.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:84.4pt;transform:translate(-37.78pt,-36.81pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T3.4.2.9.7.1.1.1.1">Template matching</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.2">Sundermeyer <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.9.7.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib280" title="">280</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.3">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.9.7.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.9.7.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.9.7.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.9.7.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.9.7.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.5">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.9.7.5.1">
<tr class="ltx_tr" id="S5.T3.4.2.9.7.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.9.7.5.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.9.7.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.9.7.5.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.6">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.7">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.8">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.9">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.9.7.9.1">
<tr class="ltx_tr" id="S5.T3.4.2.9.7.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.9.7.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.9.7.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.9.7.9.1.2.1">D/S+T+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.11">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.9.7.12">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.1">Okorn <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.10.8.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib278" title="">278</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.2">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.10.8.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.10.8.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.10.8.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.10.8.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.10.8.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.10.8.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.10.8.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.10.8.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.10.8.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.10.8.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.10.8.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.10.8.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.10.8.8.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.10.8.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.10.8.8.1.2.1">P+V</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.10"><span class="ltx_text ltx_font_italic" id="S5.T3.4.2.10.8.10.1">59.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.10.8.11"><span class="ltx_text ltx_font_italic" id="S5.T3.4.2.10.8.11.1">51.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.1">Shugurov <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.11.9.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib277" title="">277</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.11.9.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.11.9.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.11.9.3.1.1.1">RGB/RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.11.9.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.11.9.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.11.9.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.11.9.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.11.9.4.1.1.1">RGB/RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.11.9.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.11.9.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.11.9.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.11.9.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.11.9.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.11.9.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.11.9.8.1.2.1">S+T+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.10"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.11.9.10.1">46.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.11.9.11"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.11.9.11.1">54.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.1">Nguyen <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.12.10.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib281" title="">281</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.12.10.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.12.10.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.12.10.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.12.10.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.12.10.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.12.10.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.12.10.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.12.10.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.12.10.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.12.10.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.5">3DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.12.10.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.12.10.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.12.10.8.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.12.10.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.12.10.8.1.2.1">D+T</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.9">occlusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.10">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.12.10.11">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.1">Labbé <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.13.11.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib28" title="">28</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.13.11.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.13.11.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.13.11.3.1.1.1">RGB/RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.13.11.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.13.11.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.13.11.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.13.11.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.13.11.4.1.1.1">RGB/RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.13.11.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.13.11.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.7">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.13.11.7.1">
<tr class="ltx_tr" id="S5.T3.4.2.13.11.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.13.11.7.1.1.1">estimation,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.13.11.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.13.11.7.1.2.1">refinement</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.13.11.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.13.11.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.13.11.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.13.11.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.13.11.8.1.2.1">D+T+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.10">58.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.13.11.11">63.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.2">Örnek <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.2.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib279" title="">279</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.3">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T3.4.2.2.1.m1.1"><semantics id="S5.T3.4.2.2.1.m1.1a"><mo id="S5.T3.4.2.2.1.m1.1.1" xref="S5.T3.4.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.2.2.1.m1.1b"><times id="S5.T3.4.2.2.1.m1.1.1.cmml" xref="S5.T3.4.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.2.2.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.2.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.2.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.2.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.2.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.2.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.2.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.2.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.2.8.1.1.1">four-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.2.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.2.8.1.2.1">S+T+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.10"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.2.10.1">61.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.2.11"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.2.11.1">69.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.1">Nguyen <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.14.12.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib29" title="">29</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.14.12.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.14.12.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.14.12.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.14.12.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.14.12.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.14.12.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.14.12.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.14.12.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.14.12.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.14.12.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.14.12.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.14.12.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.14.12.8.1.1.1">four-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.14.12.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.14.12.8.1.2.1">D+T+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.10"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.14.12.10.1">63.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.14.12.11"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.14.12.11.1">65.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.1">Moon <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.15.13.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib282" title="">282</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.15.13.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.15.13.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.15.13.3.1.1.1">RGB/RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.15.13.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.15.13.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.15.13.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.15.13.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.15.13.4.1.1.1">RGB/RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.15.13.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.15.13.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.7">refinement</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.15.13.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.15.13.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.15.13.8.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.15.13.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.15.13.8.1.2.1">D+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.10"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.15.13.10.1">62.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.15.13.11"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.15.13.11.1">82.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.16.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.1">Wang <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.16.14.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib283" title="">283</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.16.14.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.16.14.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.16.14.3.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.16.14.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.16.14.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.16.14.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.16.14.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.16.14.4.1.1.1">RGB,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.16.14.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.16.14.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.5">3DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.16.14.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.16.14.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.16.14.8.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.16.14.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.16.14.8.1.2.1">D+T</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.10">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.2.16.14.11">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.17.15">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.1">Wen <em class="ltx_emph ltx_font_italic" id="S5.T3.4.2.17.15.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.17.15.3.1">
<tr class="ltx_tr" id="S5.T3.4.2.17.15.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.17.15.3.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.17.15.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.17.15.3.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.17.15.4.1">
<tr class="ltx_tr" id="S5.T3.4.2.17.15.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.17.15.4.1.1.1">RGBD,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.17.15.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.17.15.4.1.2.1">CAD Model</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.7">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.17.15.7.1">
<tr class="ltx_tr" id="S5.T3.4.2.17.15.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.17.15.7.1.1.1">estimation,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.17.15.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.17.15.7.1.2.1">tracking</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.2.17.15.8.1">
<tr class="ltx_tr" id="S5.T3.4.2.17.15.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.17.15.8.1.1.1">four-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.17.15.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.2.17.15.8.1.2.1">D+T+R+V</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.10">78.8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.17.15.11">88.0</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Template Matching-Based Methods</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">Template matching has been widely used in computer vision and stands as an effective solution for tackling the pose estimation challenges posed by unseen objects. Wohlhart <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib67" title="">67</a>]</cite> and Balntas <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib284" title="">284</a>]</cite> were pioneers in using deep pose descriptors for object matching and pose retrieval. However, their descriptors are tailored to specific orientations and categories, limiting their utility to objects with similar appearances. In contrast, Sundermeyer <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib280" title="">280</a>]</cite> proposed a single-encoder-multi-decoder network for jointly estimating the 3D rotation of multiple objects. This approach eliminates the need to segregate views of different objects in the latent space and enables the sharing of common features in the encoder. Yet, it still requires training multiple decoders. Wen <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib285" title="">285</a>]</cite> addressed this problem by decoupling object shape and pose in the latent representation, enabling auto-encoding without the necessity of multi-path decoders for different objects, thus enhancing scalability.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">Instead of training a network to learn features across objects, Okorn <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib278" title="">278</a>]</cite> first generated candidate poses by PPF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib11" title="">11</a>]</cite> and projected each into the scene. Later, they designed a scoring network to evaluate the hypothesis by comparing color and geometry differences between the projected object point cloud and RGBD image. Busam <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib286" title="">286</a>]</cite> reformulated 6DoF pose retrieval as an action decision process and determined the final pose by iteratively estimating probable movements. Cai <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib287" title="">287</a>]</cite> retrieved various candidate viewpoints from a target object viewpoint codebook, and then conducted in-plane 2D rotational regression on each retrieved viewpoint to obtain a set of 3D rotation estimates. These estimates were evaluated using a consistency score to generate the final rotation prediction. Meanwhile, Shugurov <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p2.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib277" title="">277</a>]</cite> matched the detected objects with the rendering database for initial viewpoint estimation. Then, they predicted the dense 2D-2D correspondences between the template and the image via feature matching. Pose estimation was eventually performed by using PnP+RANSAC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib73" title="">73</a>]</cite> or Kabsch<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib288" title="">288</a>]</cite>+RANSAC.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p3">
<p class="ltx_p" id="S5.SS1.SSS2.p3.1">Since estimating the full 6DoF pose of an unseen object is extremely challenging, some works focus on estimating the 3D rotation to simplify it. Different from the previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib284" title="">284</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib289" title="">289</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib104" title="">104</a>]</cite> that exploited a global image representation to measure image similarity, Nguyen <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib281" title="">281</a>]</cite> used CNN-extracted local features to compare the similarity between the input image and templates, showing better property and occlusion robustness over global representation. Another noteworthy approach is an image retrieval framework based on multi-scale local similarities developed by Zhao <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib290" title="">290</a>]</cite>. They extracted feature maps of various sizes from the input image and devised a similarity fusion module to robustly predict image similarity scores from multi-scale pairwise feature maps. Further, Thalhammer <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p3.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib291" title="">291</a>]</cite> and Ausserlechner <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p3.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib292" title="">292</a>]</cite> extended the scheme of Nguyen <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p3.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib281" title="">281</a>]</cite> and demonstrated that the pre-trained Vision-Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib293" title="">293</a>]</cite> outperforms task-specific fine-tuned CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib162" title="">162</a>]</cite> for template matching. However, these methods still have a noticeable performance gap between seen and unseen objects. To this end, Wang <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p3.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib283" title="">283</a>]</cite> introduced diffusion features that show great potential in modeling unseen objects. Furthermore, they designed three aggregation networks to efficiently capture and aggregate diffusion features at different granularities, thus improving its generalizability.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p4">
<p class="ltx_p" id="S5.SS1.SSS2.p4.1">In order to further improve the generalization and robustness of the 6DoF pose estimation, Labbé <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib28" title="">28</a>]</cite> used a render-and-compare approach and a coarse-to-fine strategy. Notably, they leveraged a large-scale 3D model dataset to generate a synthetic dataset containing 2 million images and over 20,000 models. It achieved strong generalization by training the network on this dataset. Compared to the non-differentiable rendering pipeline of Labbé <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p4.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib28" title="">28</a>]</cite>, Tremblay <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p4.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib294" title="">294</a>]</cite> utilized recent advancements in differentiable rendering to design a flexible refiner, allowing fine-tuning the setup without retraining. On the other hand, Moon <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p4.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib282" title="">282</a>]</cite> presented a shape-constraint recurrent flow framework, which predicts the optical flow between the template and query image and refines the pose iteratively. It took advantage of shape information directly to improve the accuracy and scalability. Recently, Wen <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p4.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib3" title="">3</a>]</cite> inherited the idea of Labbé <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p4.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib28" title="">28</a>]</cite> and developed a novel synthesis data generation pipeline using emerging large-scale 3D model databases, Large Language Models (LLMs), and diffusion models. It greatly expanded the amount and diversity of data, and ultimately achieved comparable results to instance-level methods in a render-and-compare manner.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p5">
<p class="ltx_p" id="S5.SS1.SSS2.p5.1">It is well known that template matching methods are sensitive to occlusions and require considerable time to match numerous templates. Therefore, Nguyen <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p5.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib29" title="">29</a>]</cite> achieved rapid and robust pose estimation by finding the suitable trade-off between the use of template matching and patch correspondences. In particular, the features of the query image and templates are extracted using the ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib293" title="">293</a>]</cite>, followed by fast template matching using a sub-linear nearest neighbor search. The most similar template provides two DoFs for azimuth and elevation, while the remaining four DoFs are obtained by constructing correspondences between the query image and this template. Örnek <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p5.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib279" title="">279</a>]</cite> utilized DINOv2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib249" title="">249</a>]</cite> to extract descriptors for the query image and templates. Moreover, they introduced a fast template retrieval method based on visual words constructed from DINOv2 patch descriptors, thereby decreasing the reliance on extensive data and enhancing matching speed compared to Labbé <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS2.p5.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib28" title="">28</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p6">
<p class="ltx_p" id="S5.SS1.SSS2.p6.1">In summary, template matching-based methods make full use of the advantages provided by a multitude of templates, enabling high accuracy and strong generalization. Nonetheless, they have limitations in terms of time consumption, sensitivity to occlusions, and challenges posed by complex backgrounds and lighting variations.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p7">
<p class="ltx_p" id="S5.SS1.SSS2.p7.1">Whether the above-mentioned feature matching-based or template matching-based methods, they both require a CAD model of the target object to provide prior information. In practice, accurate CAD models often require specialized hardware to build, which limits the practical application of these methods to a certain extent.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span class="ltx_text ltx_font_italic" id="S5.SS2.1.1">Manual Reference View-Based Methods</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Aside from these CAD model-based approaches, there are some manual reference view-based methods that do not require the unseen object CAD model as a prior condition but instead require providing some manual labeled reference views with the target object. Similar to CAD model-based methods, these methods are also categorized into two types: feature matching-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS2.SSS1" title="5.2.1 Feature Matching-Based Methods ‣ 5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>) and template matching-based (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.SS2.SSS2" title="5.2.2 Template Matching-Based Methods ‣ 5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5.2.2</span></a>) methods. These two categories of methods are illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.F6" title="Figure 6 ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">6</span></a>. The attributes and performance of some representative methods are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.T4" title="TABLE IV ‣ 5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Representative manual reference view-based methods. For each method, we report its 9 properties, which have the same meanings as described in the caption of Table. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3.T1" title="TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">I</span></a>. D, S, F, T, P, R, and V have the same meanings as Table. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5.T3" title="TABLE III ‣ 5.1.1 Feature Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">III</span></a>. We report the average recall of ADD(S) within 10<math alttext="\%" class="ltx_Math" display="inline" id="S5.T4.2.m1.1"><semantics id="S5.T4.2.m1.1b"><mo id="S5.T4.2.m1.1.1" xref="S5.T4.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.m1.1c"><csymbol cd="latexml" id="S5.T4.2.m1.1.1.cmml" xref="S5.T4.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.m1.1d">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.m1.1e">%</annotation></semantics></math> of the object diameter, termed as ADD(S)-0.1d (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S2" title="2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">2</span></a>). Notably, ”YOLOv5” and ”GT” denote the use of YOLOv5<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib295" title="">295</a>]</cite> and ground-truth bounding box/segmentation mask for object localization, respectively. For a fair comparison, we also report the number of used reference views. ”Full” represents all views.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.3" style="width:433.6pt;height:286.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-159.1pt,105.1pt) scale(0.576770566899733,0.576770566899733) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3" id="S5.T4.3.1.1.1.1">Methods</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.1.1.2.1">
<tr class="ltx_tr" id="S5.T4.3.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.2.1.1.1">Published</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.2.1.2.1">Year</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.1.1.3.1">
<tr class="ltx_tr" id="S5.T4.3.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.3.1.1.1">Training</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.3.1.2.1">Input</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.1.1.4.1">
<tr class="ltx_tr" id="S5.T4.3.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.4.1.1.1">Inference</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.4.1.2.1">Input</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.1.1.5.1">
<tr class="ltx_tr" id="S5.T4.3.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.5.1.1.1">Pose</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.5.1.2.1">DoF</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.1.1.6">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.1.1.6.1">
<tr class="ltx_tr" id="S5.T4.3.1.1.1.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.6.1.1.1">Object</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.1.1.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.6.1.2.1">Property</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.1.1.7">Task</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.1.1.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.1.1.8.1">
<tr class="ltx_tr" id="S5.T4.3.1.1.1.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.8.1.1.1">Inference</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.1.1.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.8.1.2.1">Mode</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.1.1.9">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.1.1.9.1">
<tr class="ltx_tr" id="S5.T4.3.1.1.1.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.9.1.1.1">Application</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.1.1.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.9.1.2.1">Area</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.1.1.10">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.1.1.10.1">
<tr class="ltx_tr" id="S5.T4.3.1.1.1.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.10.1.1.1">LM</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.1.1.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.1.1.10.1.2.1">ADD(S)-0.1d</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.1" rowspan="22"><span class="ltx_text" id="S5.T4.3.1.2.2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.3.1.2.2.1.1.1" style="width:6.9pt;height:174pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:174.0pt;transform:translate(-83.54pt,-83.54pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.3.1.2.2.1.1.1.1">Manual Reference View-Based Methods</span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.2" rowspan="5"><span class="ltx_text" id="S5.T4.3.1.2.2.2.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.3.1.2.2.2.1.1" style="width:8.9pt;height:76.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:76.6pt;transform:translate(-33.83pt,-32.86pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.3.1.2.2.2.1.1.1">Feature matching</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.3">He <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.2.2.3.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib296" title="">296</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.4">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.5">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.6">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.7">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.8">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.9">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.10">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.2.2.10.1">
<tr class="ltx_tr" id="S5.T4.3.1.2.2.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.2.2.10.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.2.2.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.2.2.10.1.2.1">D+F+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.11">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2.2.12">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.2.2.12.1">
<tr class="ltx_tr" id="S5.T4.3.1.2.2.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.2.2.12.1.1.1">83.4</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.2.2.12.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.2.2.12.1.2.1">(GT+16)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3.3.1">Sun <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.3.3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib66" title="">66</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3.3.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3.3.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3.3.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3.3.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3.3.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3.3.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3.3.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.3.3.8.1">
<tr class="ltx_tr" id="S5.T4.3.1.3.3.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.3.3.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.3.3.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.3.3.8.1.2.1">D+F+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3.3.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3.3.10">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.3.3.10.1">
<tr class="ltx_tr" id="S5.T4.3.1.3.3.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.3.3.10.1.1.1">63.6</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.3.3.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.3.3.10.1.2.1">(YOLOv5+Full)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4.4.1">He <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.4.4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib2" title="">2</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4.4.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4.4.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4.4.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4.4.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4.4.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4.4.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4.4.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.4.4.8.1">
<tr class="ltx_tr" id="S5.T4.3.1.4.4.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.4.4.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.4.4.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.4.4.8.1.2.1">D+F+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4.4.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4.4.10">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.4.4.10.1">
<tr class="ltx_tr" id="S5.T4.3.1.4.4.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.4.4.10.1.1.1">76.9</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.4.4.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.4.4.10.1.2.1">(YOLOv5+Full)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.5.5.1">Castro <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.5.5.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib297" title="">297</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.5.5.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.5.5.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.5.5.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.5.5.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.5.5.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.5.5.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.5.5.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.5.5.8.1">
<tr class="ltx_tr" id="S5.T4.3.1.5.5.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.5.5.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.5.5.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.5.5.8.1.2.1">D+F+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.5.5.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.5.5.10">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.5.5.10.1">
<tr class="ltx_tr" id="S5.T4.3.1.5.5.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.5.5.10.1.1.1">87.5</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.5.5.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.5.5.10.1.2.1">(GT+Full)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.6.6.1">Lee <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.6.6.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib298" title="">298</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.6.6.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.6.6.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.6.6.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.6.6.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.6.6.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.6.6.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.6.6.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.6.6.8.1">
<tr class="ltx_tr" id="S5.T4.3.1.6.6.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.6.6.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.6.6.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.6.6.8.1.2.1">D+F+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.6.6.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.6.6.10">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.6.6.10.1">
<tr class="ltx_tr" id="S5.T4.3.1.6.6.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.6.6.10.1.1.1">78.4</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.6.6.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.6.6.10.1.2.1">(YOLOv5+64)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.1" rowspan="12"><span class="ltx_text" id="S5.T4.3.1.7.7.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.3.1.7.7.1.1.1" style="width:8.9pt;height:84.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:84.4pt;transform:translate(-37.78pt,-36.81pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.3.1.7.7.1.1.1.1">Template matching</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.2">Park <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.7.7.2.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib65" title="">65</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.3">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.5">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.6">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.7">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.8">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.9">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.7.7.9.1">
<tr class="ltx_tr" id="S5.T4.3.1.7.7.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.7.7.9.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.7.7.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.7.7.9.1.2.1">S+T+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.10">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.7.7.11">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.7.7.11.1">
<tr class="ltx_tr" id="S5.T4.3.1.7.7.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.7.7.11.1.1.1">87.1</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.7.7.11.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.7.7.11.1.2.1">(GT+16)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.8.8.1">Nguyen <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.8.8.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib299" title="">299</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.8.8.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.8.8.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.8.8.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.8.8.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.8.8.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.8.8.7">tracking</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.8.8.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.8.8.8.1">
<tr class="ltx_tr" id="S5.T4.3.1.8.8.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.8.8.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.8.8.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.8.8.8.1.2.1">D+S+P</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.8.8.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.8.8.10">-</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.9.9.1">Liu <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.9.9.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib1" title="">1</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.9.9.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.9.9.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.9.9.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.9.9.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.9.9.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.9.9.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.9.9.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.9.9.8.1">
<tr class="ltx_tr" id="S5.T4.3.1.9.9.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.9.9.8.1.1.1">three-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.9.9.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.9.9.8.1.2.1">D+T+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.9.9.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.9.9.10">-</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.10.10.1">Gao <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.10.10.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib300" title="">300</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.10.10.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.10.10.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.10.10.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.10.10.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.10.10.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.10.10.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.10.10.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.10.10.8.1">
<tr class="ltx_tr" id="S5.T4.3.1.10.10.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.10.10.8.1.1.1">four-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.10.10.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.10.10.8.1.2.1">S+D+P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.10.10.9">occlusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.10.10.10">-</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.11.11.1">Cai <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.11.11.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib301" title="">301</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.11.11.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.11.11.3">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.11.11.4">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.11.11.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.11.11.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.11.11.7">estimation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.11.11.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.11.11.8.1">
<tr class="ltx_tr" id="S5.T4.3.1.11.11.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.11.11.8.1.1.1">two-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.11.11.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.11.11.8.1.2.1">P+R</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.11.11.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.11.11.10">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.11.11.10.1">
<tr class="ltx_tr" id="S5.T4.3.1.11.11.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.11.11.10.1.1.1">81.7</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.11.11.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.11.11.10.1.2.1">(Full)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.12.12.1">Wen <em class="ltx_emph ltx_font_italic" id="S5.T4.3.1.12.12.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.12.12.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.12.12.3">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.12.12.4">RGBD</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.12.12.5">6DoF</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.12.12.6">rigid</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.12.12.7">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.12.12.7.1">
<tr class="ltx_tr" id="S5.T4.3.1.12.12.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.12.12.7.1.1.1">estimation,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.12.12.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.12.12.7.1.2.1">tracking</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.12.12.8">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.12.12.8.1">
<tr class="ltx_tr" id="S5.T4.3.1.12.12.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.12.12.8.1.1.1">four-stage,</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.12.12.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.12.12.8.1.2.1">D+T+R+V</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.12.12.9">general</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.3.1.12.12.10">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.3.1.12.12.10.1">
<tr class="ltx_tr" id="S5.T4.3.1.12.12.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.12.12.10.1.1.1">99.9</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.12.12.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.1.12.12.10.1.2.1">(GT+16)</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Feature Matching-Based Methods</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">Different from CAD model-based feature matching methods, manual reference view-based feature matching methods primarily establish 3D-3D correspondences between the RGBD query image and RGBD reference images, or 2D-3D correspondences between the query image and sparse point cloud reconstructed by reference views. Subsequently, the object pose is solved according to the different correspondences. He <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS1.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib296" title="">296</a>]</cite> proposed the first few-shot 6DoF object pose estimation method, which can estimate the pose of an unseen object by a few support views without extra training. Specifically, they desinged a dense RGBD prototype matching framework based on transformers to fully explore the semantic and geometric relationship between the query image and reference views. Corsetti <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS1.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib302" title="">302</a>]</cite> used a textual prompt for object segmentation and reformulated the problem as a relative pose estimation between two scenes. The relative pose was obtained via point cloud registration.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1">Some methods took an alternative route from the perspective of matching after reconstruction. Wu <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS1.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib303" title="">303</a>]</cite> developed a global registration-based method that used reference and query images to reconstruct full-view and single-view models, and then searched for point matches between the two models. Sun <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS1.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib66" title="">66</a>]</cite> drew inspiration from visual localization and revised the pipeline to adapt it for pose estimation. More precisely, they reconstructed a Structure from Motion (SfM) model of the unseen object using RGB sequences from all reference viewpoints. Then, they matched 2D keypoints in the query image with the 3D points in the SfM model by a graph attention network. Nevertheless, it performed poorly on low-textured objects because of its reliance on repeatably detected keypoints. To deal with this problem, He <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS1.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib2" title="">2</a>]</cite> designed a new keypoint-free SfM method to reconstruct semi-dense point cloud models of low-textured objects based on the detector-free feature matching method LoFTR<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib304" title="">304</a>]</cite>. Castro <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS1.p2.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib297" title="">297</a>]</cite> pointed out that these pre-trained feature matching models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib305" title="">305</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib304" title="">304</a>]</cite> fail to capture the optimal descriptions for pose estimation. Based on this, they redesigned the training pipeline based on a three-view system for one-shot object-to-image matching.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p3">
<p class="ltx_p" id="S5.SS2.SSS1.p3.1">The aforementioned works still require dense support views (<em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS1.p3.1.1">i.e.</em>, <math alttext="\geq 32" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p3.1.m1.1"><semantics id="S5.SS2.SSS1.p3.1.m1.1a"><mrow id="S5.SS2.SSS1.p3.1.m1.1.1" xref="S5.SS2.SSS1.p3.1.m1.1.1.cmml"><mi id="S5.SS2.SSS1.p3.1.m1.1.1.2" xref="S5.SS2.SSS1.p3.1.m1.1.1.2.cmml"></mi><mo id="S5.SS2.SSS1.p3.1.m1.1.1.1" xref="S5.SS2.SSS1.p3.1.m1.1.1.1.cmml">≥</mo><mn id="S5.SS2.SSS1.p3.1.m1.1.1.3" xref="S5.SS2.SSS1.p3.1.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p3.1.m1.1b"><apply id="S5.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p3.1.m1.1.1"><geq id="S5.SS2.SSS1.p3.1.m1.1.1.1.cmml" xref="S5.SS2.SSS1.p3.1.m1.1.1.1"></geq><csymbol cd="latexml" id="S5.SS2.SSS1.p3.1.m1.1.1.2.cmml" xref="S5.SS2.SSS1.p3.1.m1.1.1.2">absent</csymbol><cn id="S5.SS2.SSS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S5.SS2.SSS1.p3.1.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p3.1.m1.1c">\geq 32</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p3.1.m1.1d">≥ 32</annotation></semantics></math> views). To address this problem, Fan <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS1.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib306" title="">306</a>]</cite> turned the 6DoF object pose estimation task into relative pose estimation between the retrieved object in the target view and the reference view. Given only one reference view, they achieved it by using the DINOv2 model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib249" title="">249</a>]</cite> for global matching and the LoFTR model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib304" title="">304</a>]</cite> for local matching. Note that this method cannot estimate absolute translation (or object scale), as this is an ill-posed problem when only considering two views. Beyond that, Lee <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS1.p3.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib298" title="">298</a>]</cite> applied a powerful pre-trained technique tailored for 3D vision<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib307" title="">307</a>]</cite> and demonstrated geometry-oriented visual pre-training can get better generalization capability with fewer reference views.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p4">
<p class="ltx_p" id="S5.SS2.SSS1.p4.1">Generally, due to the lack of prior geometric information from CAD models, manual reference view-based feature matching methods often require special designs to extract the geometric features of unseen objects. The number of reference views also constrains the actual application of such approaches to a certain extent.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Template Matching-Based Methods</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">Template matching-based methods mainly adopt the strategy of retrieval and refinement. There are two types: one reconstructs a 3D object representation using reference views, renders multiple templates based on this 3D representation, and employs a similarity network to compare the query image with each template for the initial pose. A refiner is then used to refine this initial pose for increased accuracy. The other directly uses reference views as templates, requiring plenty of views for retrieval and greater reliance on a refiner for accuracy. Park <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib65" title="">65</a>]</cite> introduced a novel framework for pose estimation of unseen objects without the CAD model. They reconstructed 3D object representations from a few reference views, followed by estimating translation using mask bounding boxes and corresponding depth values. The initial rotation was determined by sampling angles and refined using gradient updates via a render-and-compare approach. By training the network to render and reconstruct diverse 3D shapes, it achieved excellent generalization performance on unseen objects.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">Unlike Park <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib65" title="">65</a>]</cite> that used the strategy of render-and-compare after reconstruction, Liu <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib1" title="">1</a>]</cite> designed a pipeline for detection, retrieval, and refinement. They first designed a detector to identify object bounding boxes in the target view. Next, they compared the query and reference images at the pixel level to acquire the initial pose based on the similarity score. The pose was then refined using feature volume and multiple 3D convolution layers. However, object-centered reference images from cluttered scenes are constrained by actual segmentation or bounding box cropping, limiting its real-world applicability. To overcome this limitation, Gao <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib300" title="">300</a>]</cite> proposed adaptive segmentation modules to learn distinguishable representations of unseen objects, and Zhao <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p2.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib308" title="">308</a>]</cite> leveraged distributed reference kernels and translation estimator to achieve multi-scale correlation computation and object translation parameter prediction, thus robustly learning the prior translation of unseen objects.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p3">
<p class="ltx_p" id="S5.SS2.SSS2.p3.1">To further enhance the robustness of the translation estimation for object detection, Pan <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib309" title="">309</a>]</cite> modified the framework of Liu <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib1" title="">1</a>]</cite>. Precisely, they utilized pre-trained ViT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib293" title="">293</a>]</cite> to learn robust feature representations and adopted a top-K pose proposal scheme for pose initialization. Additionally, they applied a coarse-to-fine cascaded refinement process, incorporating feature pyramids and adaptive discrete pose hypotheses. Besides <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib309" title="">309</a>]</cite>, Cai <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p3.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib301" title="">301</a>]</cite> revisited the pipeline of Liu <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p3.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib1" title="">1</a>]</cite>. They proposed a generic joint segmentation method and an efficient 3D Gaussian Splatting-based refiner, improving the performance and robustness of object localization and pose estimation.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p4">
<p class="ltx_p" id="S5.SS2.SSS2.p4.1">In unseen object tracking, Nguyen <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p4.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib299" title="">299</a>]</cite> proposed the first method that extended to invisible categories without requiring 3D information and extra reference images, given the ground-truth object pose in the first frame. Their transformer-based architecture outputs continuous relative object poses between consecutive frames, combined with the initial object pose, to provide the object pose for each frame. Wen <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p4.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib310" title="">310</a>]</cite> used the collaborative design of concurrent tracking and neural object fields to perform 6DoF tracking from RGBD sequences. Key aspects of it include online pose graph optimization, concurrent neural object fields for 3D shape and appearance reconstruction, and a memory pool facilitating communication between the two processes.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p5">
<p class="ltx_p" id="S5.SS2.SSS2.p5.1">More recently, Nguyen <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p5.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib311" title="">311</a>]</cite> reconsidered template matching from the perspective of generating new views. Given a single reference view, they trained a model to directly predict the discriminative embeddings of the novel viewpoints of the object. In contrast, Wen <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p5.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib3" title="">3</a>]</cite> applied an object-centric neural field representation for object modeling and RGBD rendering.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p6">
<p class="ltx_p" id="S5.SS2.SSS2.p6.1">In summary, similar to template matching-based methods using CAD models, manual reference view-based methods also rely on massive templates. Moreover, due to limited reference views, these methods need to generate new templates or employ additional strategies to optimize the initial pose obtained through template matching.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Applications</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">With the advancement of object pose estimation technology, several applications leveraging this progress have been deployed. In this section, we elaborate on the development trends of these applications. Specifically, these applications include robotic manipulation (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS1" title="6.1 Robotic Manipulation ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">6.1</span></a>), Augmented Reality (AR)/Virtual Reality (VR) (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS2" title="6.2 Augmented Reality/Virtual Reality ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">6.2</span></a>), aerospace (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS3" title="6.3 Aerospace ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">6.3</span></a>), hand-object interaction (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS4" title="6.4 Hand-Object Interaction ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">6.4</span></a>), and autonomous driving (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.SS5" title="6.5 Autonomous Driving ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">6.5</span></a>). The chronological overview is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S6.F7" title="Figure 7 ‣ 6.1.1 Instance-Level Manipulation ‣ 6.1 Robotic Manipulation ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span><span class="ltx_text ltx_font_italic" id="S6.SS1.1.1">Robotic Manipulation</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We categorize the robotic manipulation application into instance-level, category-level, and unseen objects. This classification helps in better understanding the challenges and requirements across different levels.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>Instance-Level Manipulation</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">To tackle the challenge of annotating real data during training, many works utilize synthetic data for training as it is easy to acquire and annotate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib312" title="">312</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib313" title="">313</a>]</cite>. At the same time, synthetic data can simulate various scenes and environmental changes, thus helping to improve the adaptability of robotic manipulation. Li <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib314" title="">314</a>]</cite> used a large-scale synthetic dataset and a small-scale weakly labeled real-world dataset to reduce the difficulty of system deployment. Additionally, Chen <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib315" title="">315</a>]</cite> proposed an iterative self-training framework, using a teacher network trained on synthetic data to generate pseudo-labels for real data. Meanwhile, Fu <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib316" title="">316</a>]</cite> trained only on synthetic images based on physical rendering.
One of the critical challenges of synthetic data is bridging the gap with reality, which Tremblay <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib317" title="">317</a>]</cite> addressed by combining domain randomization with real data.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p2">
<p class="ltx_p" id="S6.SS1.SSS1.p2.1">Handling stacked occlusion scenes is another significant challenge, especially in industrial automation and logistics. In these scenarios, robots must accurately identify and localize objects stacked on each other, which requires an effective process of occluded objects and accurate pose estimation.
Dong <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib318" title="">318</a>]</cite> argued that the regression poses of points from the same object should tightly reside in the pose space. Therefore, these points can be clustered into different instances, and their corresponding object poses can be estimated simultaneously. This method can handle severe object occlusion.
Moreover, Zhuang <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib319" title="">319</a>]</cite> established an end-to-end pipeline to synchronously regress all potential object poses from an unsegmented point cloud. Most recently, Wada <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib320" title="">320</a>]</cite> proposed a system that fully utilized identified accurate object
CAD models and non-parametric reconstruction of unrecognized structures to estimate the occluded objects pose in real-time.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p3">
<p class="ltx_p" id="S6.SS1.SSS1.p3.1">Low-textured objects lack object surface texture information, making robotic manipulation challenging. Therefore, Zhang <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p3.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib321" title="">321</a>]</cite> proposed a pose estimation method for texture-less industrial parts. Poor surface texture and brightness make it challenging to compute discriminative local appearance descriptors. This method achieves more accurate results by optimizing the pose in the edge image. In addition, Chang <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p3.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib322" title="">322</a>]</cite> carried out transparent object grasping by estimating the object pose using a proposed model-free method that relies on multiview geometry.
In agricultural scenes, Kim <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p3.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib323" title="">323</a>]</cite> constructed an automated data collection scheme based on a 3D simulator environment to achieve three-level ripeness classification and pose estimation of target fruits.</p>
</div>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="260" id="S6.F7.g1" src="x4.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Chronological overview of some representative applications of object pose estimation methods. The black references, <span class="ltx_text" id="S6.F7.4.1" style="color:#FF0000;">red references</span>, and <span class="ltx_text" id="S6.F7.5.2" style="color:#FF8000;">orange references</span> represent the application of instance-level, category-level, and unseen methods, respectively. From this, we can also see the development trend, <em class="ltx_emph ltx_font_italic" id="S6.F7.6.3">i.e.</em>, from instance-level methods to category-level and unseen methods.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Category-Level Manipulation</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">To investigate the application of category-level object pose estimation for robotic manipulation, Liu <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS2.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib324" title="">324</a>]</cite> introduced a fine segmentation-guided category-level method with difference-aware shape deformation for robotic grasping.
Yu <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS2.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib325" title="">325</a>]</cite> proposed a shape prior-based approach and explored its application for robotic grasping.
Further, Liu <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS2.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib4" title="">4</a>]</cite> developed a robotic continuous grasping system with a pre-defined vector orientation-based grasping strategy, based on shape transformer-guided object pose estimation.
To improve efficiency and enable the pose estimation method to be applied to tasks with higher real-time requirements, Sun <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS2.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib326" title="">326</a>]</cite> utilized the inter-frame consistent keypoints to perform object pose tracking for aerial manipulation.
To further avoid manual data annotation in the real-world scene, Yu <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS2.p1.1.5">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib327" title="">327</a>]</cite> built a robotic grasping platform and designed a self-supervised-based method for category-level robotic grasping. More recently, Liu <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS2.p1.1.6">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib5" title="">5</a>]</cite> explored a contrastive learning-guided prior-free object pose estimation method for domain-generalized robotic picking.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3 </span>Unseen Object Manipulation</h4>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">Since unseen object pose estimation belongs to an emerging research, there is currently a lack of specialized designs for robotic. Here, we report several methods that validate the effectiveness of unseen object pose estimation through robotic manipulation.
Okorn <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS3.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib278" title="">278</a>]</cite> introduced a method for zero-shot object pose estimation in clutter. By scoring pose hypotheses and choosing the highest-scoring pose, they successfully grasped a novel drill object using a robotic arm.
Labbé <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS3.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib28" title="">28</a>]</cite> and Wen <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS3.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib3" title="">3</a>]</cite> adopted the render-and-compare strategy and trained the network on a large-scale synthetic dataset, resulting in an outstanding generalization. They further verified the effectiveness of their methods through robotic grasping experiments.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span><span class="ltx_text ltx_font_italic" id="S6.SS2.1.1">Augmented Reality/Virtual Reality</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Object pose estimation has various specific applications in AR and VR fields. In AR, accurate pose estimation allows for a precise overlay of virtual objects onto the real world. The key to VR technology lies in tracking the head-mounted display pose and controller in 3D space.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">Su <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib328" title="">328</a>]</cite> combined two CNN architectures<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib162" title="">162</a>]</cite> into a network, consisting of a state estimation branch and a pose estimation branch explicitly trained on synthetic images, to achieve AR assembly applications. Pandey <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib329" title="">329</a>]</cite> introduced a method for automatically annotating the handheld objects pose in camera space, addressing the efficient 6Dof pose tracking problem for handheld controllers from the perspective of egocentric cameras. Liu <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib1" title="">1</a>]</cite> presented a generalizable model-free 6DoF object pose estimator that has realized the complete object detection and pose estimation process. By simply capturing reference images of an unseen object and retrieving the poses of reference images, this method can predict the object pose on arbitrary query images and be easily applied to daily objects for AR/VR applications. He <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib2" title="">2</a>]</cite> adopted matching after the reconstruction strategy, which establishes the correspondences between the query image and the reconstructed point cloud from reference views. This method does not rely on keypoint matching and allows for AR applications even on low-texture objects. Wen <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.5">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib3" title="">3</a>]</cite> achieved strong generality by employing large-scale comprehensive training and innovative transformer-based architecture. This method has been successfully applied in various domains, including AR and robotic manipulation.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span><span class="ltx_text ltx_font_italic" id="S6.SS3.1.1">Aerospace</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Estimating the object pose in space presents unique challenges not commonly encountered in terrestrial environments. One of the most significant differences is the lack of atmospheric scattering, which complicates lighting conditions and makes objects invisible over long distances. In-orbit proximity operations for space rendezvous, docking, and debris removal require precise pose estimation under diverse lighting conditions and on high-texture backgrounds. Proença <em class="ltx_emph ltx_font_italic" id="S6.SS3.p1.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib330" title="">330</a>]</cite> proposed URSO, a simulator developed on Unreal Engine 4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib331" title="">331</a>]</cite> for generating annotated images of spacecraft orbiting Earth, which can be used as valuable data for aerospace application. Hu <em class="ltx_emph ltx_font_italic" id="S6.SS3.p1.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib82" title="">82</a>]</cite> proposed an encoder-decoder architecture that reliably handles large-scale changes under challenging conditions, enhancing robustness. Wang <em class="ltx_emph ltx_font_italic" id="S6.SS3.p1.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib332" title="">332</a>]</cite> introduced a counterfactual analysis framework to achieve robust pose estimation of spaceborne targets in complex backgrounds. Ulmer <em class="ltx_emph ltx_font_italic" id="S6.SS3.p1.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib333" title="">333</a>]</cite> generated multiple pose hypotheses for objects and introduced a pixel-level posterior formula to estimate the probability of each hypothesis. This approach can handle extreme visual conditions, including overexposure, high contrast, and low signal-to-noise ratio.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span><span class="ltx_text ltx_font_italic" id="S6.SS4.1.1">Hand-Object Interaction</span>
</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">When humans/robots interact with the physical world, they primarily do so through their hands. Therefore, accurately understanding how hands interact with objects is crucial.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">Hand-object interaction methods often rely on the object CAD model, and obtaining
the object CAD model from daily life scenes is challenging. Patten <em class="ltx_emph ltx_font_italic" id="S6.SS4.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib334" title="">334</a>]</cite> reconstructed high-quality object CAD model to mitigate the reliance on object CAD model in hand-object interaction.
To further enhance hand-object interaction, Lin <em class="ltx_emph ltx_font_italic" id="S6.SS4.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib6" title="">6</a>]</cite> utilized an effective attention model to improve the representation capability of hand and object features, thereby improving the accuracy of hand and object pose estimation. However, this method has limited utilization of the underlying geometric structures, leading to an increased reliance on visual features. Performance may degrade when objects lack visual features or when these features are occluded. Therefore, Rezazadeh <em class="ltx_emph ltx_font_italic" id="S6.SS4.p2.1.3">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib7" title="">7</a>]</cite> introduced a hierarchical graph neural network architecture combined with multimodal (visual and tactile) data to compensate for visual deficiencies and improve robustness. Moreover, Qi <em class="ltx_emph ltx_font_italic" id="S6.SS4.p2.1.4">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib335" title="">335</a>]</cite> introduced a hand-object pose estimation network guided by Signed Distance Fields (SDF), which jointly leverages the SDFs of both the hand and the object to provide a complete global implicit representation. This method aids in guiding the pose estimation of hands and objects in occlusion scenarios.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span><span class="ltx_text ltx_font_italic" id="S6.SS5.1.1">Autonomous Driving</span>
</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">Object pose estimation can be used to perceive surrounding objects such as vehicles, pedestrians, and obstacles, aiding the autonomous driving system in making timely decisions.</p>
</div>
<div class="ltx_para" id="S6.SS5.p2">
<p class="ltx_p" id="S6.SS5.p2.1">In order to address the pose estimation problem in autonomous driving, Hoque <em class="ltx_emph ltx_font_italic" id="S6.SS5.p2.1.1">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib336" title="">336</a>]</cite> proposed a 6DoF pose hypothesis based on a deep hybrid structure composed of CNNs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib162" title="">162</a>]</cite> and RNNs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib337" title="">337</a>]</cite>.
More recently, Sun <em class="ltx_emph ltx_font_italic" id="S6.SS5.p2.1.2">et al.</em><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#bib.bib338" title="">338</a>]</cite> designed an effective keypoint selection algorithm, which takes into account the shape information of panel objects within the scene of robot cabin inspection, addressing the challenge of 6DoF pose estimation of highly variable panel objects.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion and Future Direction</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this survey, we have provided a systematic overview of the latest deep learning-based object pose estimation methods, covering a comprehensive classification, a comparison of their strengths and weaknesses, and an exploration of their applications. Despite the great success, many challenges still exist, as discussed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S3" title="3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">3</span></a>, Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S4" title="4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">4</span></a>, and Sec. <a class="ltx_ref" href="https://arxiv.org/html/2405.07801v3#S5" title="5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"><span class="ltx_text ltx_ref_tag">5</span></a>. Based on these challenges, we further point out some promising future directions aimed at advancing research in object pose estimation.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">From the perspective of <span class="ltx_text ltx_font_bold" id="S7.p2.1.1">label-efficient learning</span>, prevailing methodologies predominantly rely on the utilization of real-world labeled datasets for training purposes. Nevertheless, the labor-intensive nature of manually collecting and annotating training data is widely acknowledged. Hence, we advocate for the exploration of label-efficient learning techniques for object pose estimation, which can be pursued through the following avenues:
<span class="ltx_text ltx_font_bold" id="S7.p2.1.2">1) LLMs/LVMs-guided weak/self-supervised learning methods.</span> With the rapid advancements in pre-trained LLMs/LVMs, their versatile application in various scenarios through an unsupervised manner has become feasible. Leveraging LLMs/LVMs as prior knowledge holds promise for exploring weak or self-supervised learning techniques in object pose estimation.
<span class="ltx_text ltx_font_bold" id="S7.p2.1.3">2) Synthesis to real-world domain adaptation and generalization methods.</span> Due to the high costs associated with acquiring real-world training data through manual efforts, synthetic data generation offers a cost-effective alternative. We believe that by exploring domain adaptation and generalization techniques from synthetic to real-world domains, we can mitigate domain gaps and achieve the capability to generalize synthetic data-trained models for real-world applications.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">In terms of <span class="ltx_text ltx_font_bold" id="S7.p3.1.1">applications</span>, facilitating the deployment of object pose estimation methods on mobile devices and robots is crucial. We argue that enhancing the deployability of existing methods can be achieved through the following approaches:
<span class="ltx_text ltx_font_bold" id="S7.p3.1.2">1) End-to-end methods integrating detection or segmentation.</span> Current SOTA approaches typically require initial object detection or segmentation using a pre-trained model before inputting the image into a pose estimation model (indirect pose estimation models even need to use non-differentiable PnP or Umeyama algorithms to solve pose), which complicates deployment. Future research can enhance the deployability on mobile devices and robots by exploring end-to-end object pose estimation methods that seamlessly integrate detection or segmentation.
<span class="ltx_text ltx_font_bold" id="S7.p3.1.3">2) Single RGB image-based methods.</span> Given that most mobile devices (such as smartphones and tablets) lack depth cameras, achieving high-precision estimation of unseen object poses using a single RGB image is crucial. Due to inherent geometric limitations in 2D images, future research can explore LVMs-based monocular depth estimation methods to enhance the accuracy of monocular object pose estimation by incorporating scene-level depth information.
<span class="ltx_text ltx_font_bold" id="S7.p3.1.4">3) Model lightweighting.</span> Existing SOTA models often have large parameter sizes and inefficient running performance, which presents challenges for deployment on mobile devices and robots with limited computational resources. Future work can explore effective lightweight methods, such as teacher-student models, to research reducing model parameter count (GPU memory) and improving model running efficiency.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">Existing methods are predominantly designed for common objects and scenes, rendering them ineffective for <span class="ltx_text ltx_font_bold" id="S7.p4.1.1">challenging objects and scenes</span>. We believe that the applicability can be enhanced through the following avenues:
<span class="ltx_text ltx_font_bold" id="S7.p4.1.2">1) Articulated object pose estimation.</span> Articulated objects (such as clothing and drawers) exhibit multiple DoF and significant self-occlusion compared to rigid objects, making pose estimation challenging. Achieving high-precision pose estimation for articulated objects is an important research problem that remains to be addressed in the future.
<span class="ltx_text ltx_font_bold" id="S7.p4.1.3">2) Transparent object pose estimation.</span> The simultaneous absence of texture, color, and depth information poses a significant challenge for estimating the pose of transparent objects. Future research endeavors could focus on enhancing the geometric information of transparent objects through depth augmentation or completion techniques, thereby improving the accuracy of pose estimation.
<span class="ltx_text ltx_font_bold" id="S7.p4.1.4">3) Robust methods for handling occlusion.</span> Occlusion is the most common challenge. Currently, there exists no object pose estimation method that can effectively handle severe occlusion. Severe occlusion leads to an incomplete representation of texture and geometric features in objects, introducing uncertainty into the pose estimation model. Hence, improving the model’s ability to perceive severe occlusion is crucial for enhancing its robustness.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1">From the aspect of <span class="ltx_text ltx_font_bold" id="S7.p5.1.1">problem formulation</span>, recent instance-level methods have achieved high precision but exhibit poor generalization. Category-level methods demonstrate good generalization for intra-class unseen objects but fail to generalize to unseen object categories. Unseen object pose estimation methods have the potential to generalize to any unseen object, yet they still rely on object CAD models or reference views. The following paths can be explored from the problem formulation to further enhance the generalization of object pose estimation:
<span class="ltx_text ltx_font_bold" id="S7.p5.1.2">1) Few-shot learning-based category-level methods for unseen categories.</span> Since category-level methods need to re-obtain a large amount of annotated training data for unseen object categories, their generalization is severely limited. Therefore, future research could focus on exploring how to leverage few-shot learning to enable the rapid generalization of category-level methods to unseen object categories.
<span class="ltx_text ltx_font_bold" id="S7.p5.1.3">2) CAD model-free and sparse manual reference view-based unseen object pose estimation.</span> While current unseen object pose estimation methods do not require retraining for unseen objects, they still rely on either the CAD models or extensive annotated reference views of unseen objects, both of which still require manual acquisition. To this end, exploring CAD model-free and sparse manual reference view-based unseen object pose estimation methods is crucial.
<span class="ltx_text ltx_font_bold" id="S7.p5.1.4">3) Open-vocabulary strong generalization methods.</span> Given the broad applicability of object pose estimation in human-machine interaction scenes, future research could leverage open vocabulary provided by humans as prompts to enhance generalization to unseen objects and scenes.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the National Natural Science Foundation of China under Grant U22A2059, Shenzhen Science and Technology Foundation under Grant 2021Szvup035, China Scholarship Council under Grant 202306130074 and Grant 202206130048, Natural Science Foundation of Hunan Province under Grant 2024JJ5098, and by the State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body Open Foundation. Ajmal Mian was supported by the Australian Research Council Future Fellowship Award funded by the Australian Government under Project FT210100268.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Liu and Y. Wen, “Gen6d: Generalizable model-free 6-dof object pose
estimation from rgb images,” in <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
X. He and J. Sun, “Onepose++: Keypoint-free one-shot object pose estimation
without cad models,” in <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">NeurIPS</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. Wen and W. Yang, “Foundationpose: Unified 6d pose estimation and tracking
of novel objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Liu and W. Sun, “Robotic continuous grasping system by shape
transformer-guided multi-object category-level 6d pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">IEEE TII</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Liu and W. Sun, “Domain-generalized robotic picking via contrastive
learning-based 6-d pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">IEEE TII</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Z. Lin and C. Ding, “Harmonious feature learning for interactive hand-object
pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">CVPR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Rezazadeh and S. Dikhale, “Hierarchical graph nural networks for
proprioceptive 6d pose estimation of in-hand objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">ICRA</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
D. G. Lowe, “Distinctive image features from scale-invariant keypoints,” <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">IJCV</span>, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. B. Rusu and N. Blodow, “Fast point feature histograms (fpfh) for 3d
registration,” in <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">ICRA</span>, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. B. Rusu and G. Bradski, “Fast 3d recognition and pose using the viewpoint
feature histogram,” in <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">IROS</span>, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
B. Drost and M. Ulrich, “Model globally, match locally: Efficient and robust
3d object recognition,” in <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">CVPR</span>, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
C. Choi and H. I. Christensen, “3d pose estimation of daily objects using an
rgb-d camera,” in <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">IROS</span>, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. Choi and A. J. Trevor, “Rgb-d edge detection and edge-based registration,”
in <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IROS</span>, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. Birdal and S. Ilic, “Point pair features based object detection and pose
estimation revisited,” in <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">3DV</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Xiang and T. Schmidt, “Posecnn: A convolutional neural network for 6d
object pose estimation in cluttered scenes,” <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint
arXiv:1711.00199</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Wang and D. Xu, “Densefusion: 6d object pose estimation by iterative dense
fusion,” in <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">CVPR</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. Peng and Y. Liu, “Pvnet: Pixel-wise voting network for 6dof pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">CVPR</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y. He and W. Sun, “Pvn3d: A deep point-wise 3d keypoints voting network for
6dof pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Z. Li and G. Wang, “Cdpn: Coordinates-based disentangled pose network for
real-time rgb-based 6-dof object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">ICCV</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Zakharov and I. Shugurov, “Dpod: 6d pose object detector and refiner,” in
<span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">ICCV</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. He and H. Huang, “Ffb6d: A full flow bidirectional fusion network for 6d
pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">CVPR</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Wang and S. Sridhar, “Normalized object coordinate space for category-level
6d object pose and size estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">CVPR</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
K. Chen and Q. Dou, “Sgpa: Structure-guided prior adaptation for
category-level 6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">ICCV</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Lin and Z. Wei, “Category-level 6d object pose and size estimation using
self-supervised deep prior deformation networks,” in <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Di and R. Zhang, “Gpv-pose: Category-level object pose estimation via
geometry-guided point-wise voting,” in <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
L. Zheng and C. Wang, “Hs-pose: Hybrid scope feature extraction for
category-level object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">CVPR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Liu and Y. Chen, “Ist-net: Prior-free category-level pose estimation with
implicit space transformation,” in <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Labbé and L. Manuelli, “Megapose: 6d pose estimation of novel objects
via render &amp; compare,” in <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">CoRL</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
V. N. Nguyen and T. Groueix, “Gigapose: Fast and robust novel object pose
estimation via one correspondence,” in <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Lin and L. Liu, “Sam-6d: Segment anything model meets zero-shot 6d object
pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Hoque and M. Y. Arafat, “A comprehensive review on 3d object detection and
6d pose estimation with deep learning,” <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">IEEE Access</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
G. Marullo and L. Tanzi, “6d object position estimation from 2d images: A
literature review,” <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Multimed. Tools Appl.</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Z. Fan and Y. Zhu, “Deep learning on monocular object pose detection and
tracking: A comprehensive overview,” <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">ACM Comput. Surv.</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
G. Du and K. Wang, “Vision-based robotic grasping from object localization,
object pose estimation to grasp estimation for parallel grippers: A review,”
<span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Artif Intell Rev.</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. Guan and Y. Hao, “A survey of 6dof object pose estimation methods for
different application scenarios,” <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Sensors</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T. Hodan and M. Sundermeyer, “Bop challenge 2023 on detection, segmentation
and pose estimation of seen and unseen rigid objects,” <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">arXiv preprint
arXiv:2403.09799</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
S. Hinterstoisser and V. Lepetit, “Model based training, detection and pose
estimation of texture-less 3d objects in heavily cluttered scenes,” in <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">ACCV</span>, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
E. Brachmann and F. Michel, “Uncertainty-driven 6d pose estimation of objects
and scenes from a single rgb image,” in <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">CVPR</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
E. Brachmann and A. Krull, “Learning 6d object pose estimation using 3d object
coordinates,” in <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">ECCV</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A. Tejani and D. Tang, “Latent-class hough forests for 3d object detection and
pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">ECCV</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
A. Doumanoglou and R. Kouskouridas, “Recovering 6d object pose and predicting
next-best-view in the crowd,” in <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">CVPR</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
C. Rennie and R. Shome, “A dataset for improved rgbd-based object detection
and pose estimation for warehouse pick-and-place,” <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">IEEE RAL</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
B. Calli and A. Singh, “The ycb object and model set: Towards common
benchmarks for manipulation research,” in <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">ICRA</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
T. Hodan and P. Haluza, “T-less: An rgb-d dataset for 6d pose estimation of
texture-less objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">WACV</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
B. Drost and M. Ulrich, “Introducing mvtec itodd - a dataset for 3d object
recognition in industry,” in <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">ICCVW</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
R. Kaskman and S. Zakharov, “Homebreweddb: Rgb-d dataset for 6d pose
estimation of 3d objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">ICCVW</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
S. Tyree and J. Tremblay, “6-dof pose estimation of household objects for
robotic manipulation: An accessible dataset and benchmark,” in <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">IROS</span>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
B. Wen and C. Mitash, “Se(3)-tracknet: Data-driven 6d pose tracking by
calibrating image residuals in synthetic domains,” in <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">IROS</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
X. Chen and H. Zhang, “Clearpose: Large-scale transparent object dataset and
benchmark,” in <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
L. Chen and H. Yang, “Mp6d: An rgb-d dataset for metal parts’ 6d pose
estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">IEEE RAL</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
A. X. Chang and T. Funkhouser, “Shapenet: An information-rich 3d model
repository,” <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:1512.03012</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
L. Manuelli and W. Gao, “kpam: Keypoint affordances for category-level robotic
manipulation,” in <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">ISRR</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
X. Liu and R. Jonschkowski, “Keypose: Multi-view 3d labeling and keypoint
estimation for transparent objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
A. Ahmadyan and L. Zhang, “Objectron: A large scale dataset of object-centric
videos in the wild with pose annotations,” in <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">CVPR</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Y. Ze and X. Wang, “Category-level 6d object pose estimation in the wild: A
semi-supervised learning approach and a new dataset,” in <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">NeurIPS</span>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
P. Wang and H. Jung, “Phocal: A multi-modal dataset for category-level object
pose estimation with photometrically challenging objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">CVPR</span>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
H. Jung and S.-C. Wu, “Housecat6d–a large-scale multi-modal category level 6d
object pose dataset with household objects in realistic scenarios,” in <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
F. Michel and A. Krull, “Pose estimation of kinematic chain instances via
object coordinate regression.,” in <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">BMVC</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
R. Martín-Martín and C. Eppner, “The rbo dataset of articulated
objects and interactions,” <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">IJRR</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Y. Liu and Y. Liu, “Hoi4d: A 4d egocentric dataset for category-level
human-object interaction,” in <span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
L. Liu and H. Xue, “Toward real-world category-level articulation pose
estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">IEEE TIP</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Z. Zhu and J. Wang, “Contactart: Learning 3d interaction priors for
category-level articulated object and hand poses estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">arXiv
preprint arXiv:2305.01618</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Y. Qin and H. Su, “From one hand to multiple hands: Imitation learning for
dexterous manipulation from single-camera teleoperation,” <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">IEEE RAL</span>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
K. Mo and S. Zhu, “Partnet: A large-scale benchmark for fine-grained and
hierarchical part-level 3d object understanding,” in <span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">CVPR</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
K. Park and A. Mousavian, “Latentfusion: End-to-end differentiable
reconstruction and rendering for unseen object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
J. Sun and Z. Wang, “Onepose: One-shot object pose estimation without cad
models,” in <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
P. Wohlhart and V. Lepetit, “Learning descriptors for object recognition and
3d pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">CVPR</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
G. Wang and F. Manhardt, “Self6d: Self-supervised monocular 6d object pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">ECCV</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
X. Jiang and D. Li, “Uni6d: A unified cnn framework without projection
breakdown for 6d pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
H. Li and J. Lin, “Dcl-net: Deep correspondence learning network for 6d pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
J. Shotton and B. Glocker, “Scene coordinate regression forests for camera
relocalization in rgb-d images,” in <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">CVPR</span>, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
M. Tian and M. H. Ang, “Shape prior deformation for categorical 6d object pose
and size estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">ECCV</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
M. A. Fischler and R. C. Bolles, “Random sample consensus,” <span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">COMMUN ACM</span>,
1981.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
M. Rad and V. Lepetit, “Bb8: A scalable, accurate, robust to partial occlusion
method for predicting the 3d poses of challenging objects without using
depth,” in <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">ICCV</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
B. Tekin and S. N. Sinha, “Real-time seamless single shot 6d object pose
prediction,” in <span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">CVPR</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
J. Redmon and S. Divvala, “You only look once: Unified, real-time object
detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">CVPR</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
G. Pavlakos and X. Zhou, “6-dof object pose from semantic keypoints,” in <span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">ICRA</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
B. Doosti and S. Naha, “Hope-net: A graph-based model for hand-object pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
T. N. Kipf and M. Welling, “Semi-supervised classification with graph
convolutional networks,” <span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:1609.02907</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
C. Song and J. Song, “Hybridpose: 6d object pose estimation under hybrid
representations,” in <span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
P. Liu and Q. Zhang, “Mfpn-6d : Real-time one-stage pose estimation of objects
on rgb images,” in <span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">ICRA</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Y. Hu and S. Speierer, “Wide-depth-range 6d object pose estimation in space,”
in <span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">CVPR</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
R. Lian and H. Ling, “Checkerpose: Progressive dense keypoint localization for
object pose estimation with graph neural network,” in <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
J. Chang and M. Kim, “Ghostpose: Multi-view pose estimation of transparent
objects for robot hand grasping,” in <span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">IROS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
S. Guo and Y. Hu, “Knowledge distillation for 6d pose estimation by aligning
distributions of local predictions,” in <span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">CVPR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
F. Liu and Y. Hu, “Linear-covariance loss for end-to-end learning of 6d pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
A. Crivellaro and M. Rad, “Robust 3d object tracking from monocular images
using stable parts,” <span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">IEEE TPAMI</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
M. Oberweger and M. Rad, “Making deep heatmaps robust to partial occlusions
for 3d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">ECCV</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Y. Hu and J. Hugonot, “Segmentation-driven 6d object pose estimation,” in
<span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">CVPR</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
W.-L. Huang and C.-Y. Hung, “Confidence-based 6d object pose estimation,”
<span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">IEEE TMM</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
W. Zhao and S. Zhang, “Learning deep network for detecting 3d object keypoints
and 6d poses,” in <span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Z. Yang and X. Yu, “Dsc-posenet: Learning 6dof object pose estimation via
dual-scale consistency,” in <span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">CVPR</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
S. Liu and H. Jiang, “Semi-supervised 3d hand-object poses estimation with
interactions in time,” in <span class="ltx_text ltx_font_italic" id="bib.bib93.1.1">CVPR</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
G. Georgakis and S. Karanam, “Learning local rgb-to-cad correspondences for
object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">ICCV</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
J. Sock and G. Garcia-Hernando, “Introducing pose consistency and
warp-alignment for self-supervised 6d object pose estimation in color
images,” in <span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">3DV</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
S. Zhang and W. Zhao, “Keypoint-graph-driven learning framework for object
pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">CVPR</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
S. Thalhammer and M. Leitner, “Pyrapose: Feature pyramids for fast and
accurate object pose estimation under domain shift,” in <span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">ICRA</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
T. Hodan and D. Barath, “Epos: Estimating 6d pose of objects with
symmetries,” in <span class="ltx_text ltx_font_italic" id="bib.bib98.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
I. Shugurov and S. Zakharov, “Dpodv2: Dense correspondence-based 6 dof pose
estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib99.1.1">IEEE TPAMI</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
H. Chen and P. Wang, “Epro-pnp: Generalized end-to-end probabilistic
perspective-n-points for monocular object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">CVPR</span>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
R. L. Haugaard and A. G. Buch, “Surfemb: Dense and continuous correspondence
distributions for object pose estimation with learnt surface embeddings,” in
<span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
F. Li and S. R. Vutukur, “Nerf-pose: A first-reconstruct-then-regress approach
for weakly-supervised 6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Y. Xu and K.-Y. Lin, “Rnnpose: 6-dof object pose estimation via recurrent
correspondence field estimation and pose optimization,” <span class="ltx_text ltx_font_italic" id="bib.bib103.1.1">IEEE TPAMI</span>,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
M. Sundermeyer and Z.-C. Marton, “Implicit 3d orientation learning for 6d
object detection from rgb images,” in <span class="ltx_text ltx_font_italic" id="bib.bib104.1.1">ECCV</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
C. Papaioannidis and V. Mygdalis, “Domain-translated 3d object pose
estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib105.1.1">IEEE TIP</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Z. Li and X. Ji, “Pose-guided auto-encoder and feature-based refinement for
6-dof object pose regression,” in <span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">ICRA</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
X. Deng and A. Mousavian, “Poserbpf: A rao–blackwellized particle filter for
6-d object pose tracking,” <span class="ltx_text ltx_font_italic" id="bib.bib107.1.1">IEEE TRO</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
H. Jiang and M. Salzmann, “Se(3) diffusion model-based point cloud
registration for robust 6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib108.1.1">NeurIPS</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Z. Dang and L. Wang, “Match normalization: Learning-based point cloud
registration for 6d object pose estimation in the real world,” <span class="ltx_text ltx_font_italic" id="bib.bib109.1.1">IEEE
TPAMI</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
T. Cao and F. Luo, “Dgecn: A depth-guided edge convolutional network for
end-to-end 6d pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib110.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Y. Wu and M. Zand, “Vote from the center: 6 dof pose estimation in rgb-d
images by radial keypoint voting,” in <span class="ltx_text ltx_font_italic" id="bib.bib111.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
J. Zhou and K. Chen, “Deep fusion transformer network with weighted
vector-wise keypoints voting for robust 6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib112.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
M. Tian and L. Pan, “Robust 6d object pose estimation by learning rgb-d
features,” in <span class="ltx_text ltx_font_italic" id="bib.bib113.1.1">ICRA</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
G. Zhou and H. Wang, “Pr-gcn: A deep graph convolutional network with point
refinement for 6d pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib114.1.1">ICCV</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
N. Mo and W. Gan, “Es6d: A computation efficient and symmetry-aware 6d pose
regression framework,” in <span class="ltx_text ltx_font_italic" id="bib.bib115.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
J.-X. Hong and H.-B. Zhang, “A transformer-based multi-modal fusion network
for 6d pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib116.1.1">Information Fusion</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
W. Chen and X. Jia, “G2l-net: Global to local network for real-time 6d pose
estimation with embedding vector features,” in <span class="ltx_text ltx_font_italic" id="bib.bib117.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Y. Hu and P. Fua, “Single-stage 6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib118.1.1">CVPR</span>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Y. Labbé and J. Carpentier, “Cosypose: Consistent multi-view multi-object
6d pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib119.1.1">ECCV</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
G. Wang and F. Manhardt, “Gdr-net: Geometry-guided direct regression network
for monocular 6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib120.1.1">CVPR</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Y. Di and F. Manhardt, “So-pose: Exploiting self-occlusion for direct 6d pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib121.1.1">ICCV</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
G. Wang and F. Manhardt, “Occlusion-aware self-supervised monocular 6d object
pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib122.1.1">IEEE TPAMI</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
C. Li and J. Bai, “A unified framework for multi-view multi-class object pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib123.1.1">ECCV</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Y. Li and G. Wang, “Deepim: Deep iterative matching for 6d pose estimation,”
in <span class="ltx_text ltx_font_italic" id="bib.bib124.1.1">ECCV</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
F. Manhardt and W. Kehl, “Deep model-based 6d pose refinement in rgb,” in
<span class="ltx_text ltx_font_italic" id="bib.bib125.1.1">ECCV</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
F. Manhardt and D. M. Arroyo, “Explaining the ambiguity of object detection
and 6d pose from visual data,” in <span class="ltx_text ltx_font_italic" id="bib.bib126.1.1">ICCV</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
C. Papaioannidis and I. Pitas, “3d object pose estimation using
multi-objective quaternion learning,” <span class="ltx_text ltx_font_italic" id="bib.bib127.1.1">IEEE TCSVT</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Y. Liu and L. Zhou, “Regression-based three-dimensional pose estimation for
texture-less objects,” <span class="ltx_text ltx_font_italic" id="bib.bib128.1.1">IEEE TMM</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Y. Hai and R. Song, “Shape-constraint recurrent flow for 6d object pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib129.1.1">CVPR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Y. Li and Y. Mao, “Mrc-net: 6-dof pose estimation with multiscale residual
correlation,” in <span class="ltx_text ltx_font_italic" id="bib.bib130.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
M. Cai and I. Reid, “Reconstruct locally, localize globally: A model free
method for object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib131.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
D. Wang and G. Zhou, “Geopose: Dense reconstruction guided 6d object pose
estimation with geometric consistency,” <span class="ltx_text ltx_font_italic" id="bib.bib132.1.1">IEEE TMM</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Y. Su and M. Saleh, “Zebrapose: Coarse to fine surface encoding for 6dof
object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib133.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Z. Xu and Y. Zhang, “Bico-net: Regress globally, match locally for robust 6d
pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib134.1.1">IJCAI</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
L. Huang and T. Hodan, “Neural correspondence field for object pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib135.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
H. Jiang and Z. Dang, “Center-based decoupled point cloud registration for 6d
object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib136.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
P. Besl and N. D. McKay, “A method for registration of 3-d shapes,” <span class="ltx_text ltx_font_italic" id="bib.bib137.1.1">IEEE
TPAMI</span>, 1992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Y. Lin and Y. Su, “Hipose: Hierarchical binary surface encoding and
correspondence pruning for rgb-d 6dof object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib138.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
K. Park and T. Patten, “Pix2pose: Pixel-wise coordinate regression of objects
for 6d pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib139.1.1">ICCV</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
C. Wu and L. Chen, “Geometric-aware dense matching network for 6d pose
estimation of objects from rgb-d images,” <span class="ltx_text ltx_font_italic" id="bib.bib140.1.1">PR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
C. Wu and L. Chen, “Pseudo-siamese graph matching network for textureless
objects’ 6-d pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib141.1.1">IEEE TIE</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Z. Li and Y. Hu, “Sd-pose: Semantic decomposition for cross-domain 6d object
pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib142.1.1">AAAI</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Y. Hu and P. Fua, “Perspective flow aggregation for data-limited 6d object
pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib143.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Y. Hai and R. Song, “Pseudo flow consistency for self-supervised 6d object
pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib144.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
L. Lipson and Z. Teed, “Coupled iterative refinement for 6d multi-object pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib145.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
J. J. Moré, “The levenberg-marquardt algorithm: Implementation and
theory,” in <span class="ltx_text ltx_font_italic" id="bib.bib146.1.1">Numerical Analysis</span>, 1978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
X. Liu and J. Zhang, “6dof pose estimation with object cutout based on a deep
autoencoder,” in <span class="ltx_text ltx_font_italic" id="bib.bib147.1.1">ISMAR-Adjunct</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
Y. Zhang and C. Zhang, “6d object pose estimation algorithm using
preprocessing of segmentation and keypoint extraction,” in <span class="ltx_text ltx_font_italic" id="bib.bib148.1.1">I2MTC</span>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
S. Stevšič and O. Hilliges, “Spatial attention improves iterative
6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib149.1.1">3DV</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
K. Murphy and S. Russell, <span class="ltx_text ltx_font_italic" id="bib.bib150.1.1">Sequential Monte Carlo Methods in Practice</span>,
ch. Rao-blackwellised particle filtering for dynamic bayesian networks.

</span>
<span class="ltx_bibblock">Springer, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
X. Liu and S. Iwase, “Kdfnet: Learning keypoint distance field for 6d object
pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib151.1.1">IROS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
P. Liu and Q. Zhang, “Bdr6d: Bidirectional deep residual fusion network for 6d
pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib152.1.1">IEEE TASE</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
L. Xu and H. Qu, “6d-diff: A keypoint diffusion framework for 6d object pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib153.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
J. Mei and X. Jiang, “Spatial feature mapping for 6dof object pose
estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib154.1.1">PR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
F. Wang and X. Zhang, “Kvnet: An iterative 3d keypoints voting network for
real-time 6-dof object pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib155.1.1">Neurocomputing</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
L. Zeng and W. J. Lv, “Parametricnet: 6dof pose estimation network for
parametric shapes in stacked scenarios,” in <span class="ltx_text ltx_font_italic" id="bib.bib156.1.1">ICRA</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
F. Duffhauss and T. Demmler, “Mv6d: Multi-view 6d pose estimation on rgb-d
frames using a deep point-wise voting network,” in <span class="ltx_text ltx_font_italic" id="bib.bib157.1.1">IROS</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
X. Yu and Z. Zhuang, “6dof object pose estimation via differentiable proxy
voting loss,” <span class="ltx_text ltx_font_italic" id="bib.bib158.1.1">arXiv preprint arXiv:2002.03923</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
H. Lin and S. Peng, “Learning to estimate object poses without real image
annotations.,” in <span class="ltx_text ltx_font_italic" id="bib.bib159.1.1">IJCAI</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
T. Ikeda and S. Tanishige, “Sim2real instance-level style transfer for 6d pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib160.1.1">IROS</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
G. Zhou and Y. Yan, “A novel depth and color feature fusion framework for 6d
object pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib161.1.1">IEEE TMM</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Y. LeCun and B. Boser, “Backpropagation applied to handwritten zip code
recognition,” <span class="ltx_text ltx_font_italic" id="bib.bib162.1.1">Neural Comput</span>, 1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
X. Liu and X. Yuan, “A depth adaptive feature extraction and dense prediction
network for 6-d pose estimation in robotic grasping,” <span class="ltx_text ltx_font_italic" id="bib.bib163.1.1">IEEE TII</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
F. Mu and R. Huang, “Temporalfusion: Temporal motion reasoning with
multi-frame fusion for 6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib164.1.1">IROS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
D. Cai and J. Heikkilä, “Sc6d: Symmetry-agnostic and correspondence-free
6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib165.1.1">3DV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
L. Zeng and W. J. Lv, “Ppr-net++: Accurate 6-d pose estimation in stacked
scenarios,” <span class="ltx_text ltx_font_italic" id="bib.bib166.1.1">IEEE TASE</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Y. Bukschat and M. Vetter, “Efficientpose: An efficient, accurate and scalable
end-to-end 6d multi object pose estimation approach,” <span class="ltx_text ltx_font_italic" id="bib.bib167.1.1">arXiv preprint
arXiv:2011.04307</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
G. Gao and M. Lauri, “6d object pose regression via supervised learning on
point clouds,” in <span class="ltx_text ltx_font_italic" id="bib.bib168.1.1">ICRA</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
M. Lin and V. Murali, “6d object pose estimation with pairwise compatible
geometric features,” in <span class="ltx_text ltx_font_italic" id="bib.bib169.1.1">ICRA</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Y. Shi and J. Huang, “Stablepose: Learning 6d object poses from geometrically
stable patches,” in <span class="ltx_text ltx_font_italic" id="bib.bib170.1.1">CVPR</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
Z. Liu and Q. Wang, “Pa-pose: Partial point cloud fusion based on reliable
alignment for 6d pose tracking,” <span class="ltx_text ltx_font_italic" id="bib.bib171.1.1">PR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Y. Wen and Y. Fang, “Gccn: Geometric constraint co-attention network for 6d
object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib172.1.1">ACM MM</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Y. An and D. Yang, “Hft6d: Multimodal 6d object pose estimation based on
hierarchical feature transformer,” <span class="ltx_text ltx_font_italic" id="bib.bib173.1.1">Measurement</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
Z. Zhang and W. Chen, “Trans6d: Transformer-based 6d object pose estimation
and refinement,” in <span class="ltx_text ltx_font_italic" id="bib.bib174.1.1">ECCVW</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
G. Feng and T.-B. Xu, “Nvr-net: Normal vector guided regression network for
disentangled 6d pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib175.1.1">IEEE TCSVT</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
G. Gao and M. Lauri, “Cloudaae: Learning 6d object pose regression with
on-line data synthesis on point clouds,” in <span class="ltx_text ltx_font_italic" id="bib.bib176.1.1">ICRA</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
G. Zhou and D. Wang, “Semi-supervised 6d object pose estimation without using
real annotations,” <span class="ltx_text ltx_font_italic" id="bib.bib177.1.1">IEEE TCSVT</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
T. Tan and Q. Dong, “Smoc-net: Leveraging camera pose for self-supervised
monocular object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib178.1.1">CVPR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
J. Rambach and C. Deng, “Learning 6dof object poses from synthetic single
channel images,” in <span class="ltx_text ltx_font_italic" id="bib.bib179.1.1">ISMAR-Adjunct</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
K. Kleeberger and M. F. Huber, “Single shot 6d object pose estimation,” in
<span class="ltx_text ltx_font_italic" id="bib.bib180.1.1">ICRA</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
V. Sarode and X. Li, “Pcrnet: Point cloud registration network using pointnet
encoding,” <span class="ltx_text ltx_font_italic" id="bib.bib181.1.1">arXiv preprint arXiv:1908.07906</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
C. R. Qi and H. Su, “Pointnet: Deep learning on point sets for 3d
classification and segmentation,” in <span class="ltx_text ltx_font_italic" id="bib.bib182.1.1">CVPR</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
S. H. Bengtson and H. Åström, “Pose estimation from rgb images of
highly symmetric objects using a novel multi-pose loss and differential
rendering,” in <span class="ltx_text ltx_font_italic" id="bib.bib183.1.1">IROS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
J. Park and N. Cho, “Dprost: Dynamic projective spatial transformer network
for 6d pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib184.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
M. Garon and J.-F. Lalonde, “Deep 6-dof tracking,” <span class="ltx_text ltx_font_italic" id="bib.bib185.1.1">IEEE TVCG</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
J. Long and E. Shelhamer, “Fully convolutional networks for semantic
segmentation,” in <span class="ltx_text ltx_font_italic" id="bib.bib186.1.1">CVPR</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
W. Kehl and F. Manhardt, “Ssd-6d: Making rgb-based 3d detection and 6d pose
estimation great again,” in <span class="ltx_text ltx_font_italic" id="bib.bib187.1.1">ICCV</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in <span class="ltx_text ltx_font_italic" id="bib.bib188.1.1">ECCV</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
J. Wu and B. Zhou, “Real-time object pose estimation with pose interpreter
networks,” in <span class="ltx_text ltx_font_italic" id="bib.bib189.1.1">IROS</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
T.-T. Do and M. Cai, “Deep-6dpose: Recovering 6d object pose from a single rgb
image,” <span class="ltx_text ltx_font_italic" id="bib.bib190.1.1">arXiv preprint arXiv:1802.10367</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
T.-C. Hsiao and H.-W. Chen, “Confronting ambiguity in 6d object pose
estimation via score-based diffusion on se(3),” in <span class="ltx_text ltx_font_italic" id="bib.bib191.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
J. Liu and W. Sun, “Hff6d: Hierarchical feature fusion network for robust 6d
object pose tracking,” <span class="ltx_text ltx_font_italic" id="bib.bib192.1.1">IEEE TCSVT</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
R. Ge and G. Loianno, “Vipose: Real-time visual-inertial 6d object pose
tracking,” in <span class="ltx_text ltx_font_italic" id="bib.bib193.1.1">IROS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
S. Iwase and X. Liu, “Repose: Fast 6d object pose refinement via deep texture
rendering,” in <span class="ltx_text ltx_font_italic" id="bib.bib194.1.1">ICCV</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
J. Josifovski and M. Kerzel, “Object detection and pose estimation based on
convolutional neural networks trained with synthetic data,” in <span class="ltx_text ltx_font_italic" id="bib.bib195.1.1">IROS</span>,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
C. Sahin and T.-K. Kim, “Category-level 6d object pose recovery in depth
images,” in <span class="ltx_text ltx_font_italic" id="bib.bib196.1.1">ECCVW</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
S. Umeyama, “Least-squares estimation of transformation parameters between two
point patterns,” <span class="ltx_text ltx_font_italic" id="bib.bib197.1.1">IEEE TPAMI</span>, 1991.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
J. Wang and K. Chen, “Category-level 6d object pose estimation via cascaded
relation and recurrent reconstruction networks,” in <span class="ltx_text ltx_font_italic" id="bib.bib198.1.1">IROS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
L. Zou and Z. Huang, “6d-vit: Category-level 6d object pose estimation via
transformer-based instance representation learning,” <span class="ltx_text ltx_font_italic" id="bib.bib199.1.1">IEEE TIP</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
Z. Fan and Z. Song, “Object level depth reconstruction for category level 6d
object pose estimation from monocular rgb image,” in <span class="ltx_text ltx_font_italic" id="bib.bib200.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
J. Wei and X. Song, “Rgb-based category-level object pose estimation via
decoupled metric scale recovery,” <span class="ltx_text ltx_font_italic" id="bib.bib201.1.1">arXiv preprint arXiv:2309.10255</span>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
M. Z. Irshad and T. Kollar, “Centersnap: Single-shot multi-object 3d shape
reconstruction and categorical 6d pose and size estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib202.1.1">ICRA</span>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
H. Lin and Z. Liu, “Sar-net: Shape alignment and recovery network for
category-level 6d object pose and size estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib203.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
R. Zhang and Y. Di, “Ssp-pose: Symmetry-aware shape prior deformation for
direct category-level object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib204.1.1">IROS</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
R. Zhang and Y. Di, “Rbp-pose: Residual bounding box projection for
category-level pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib205.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
X. Liu and G. Wang, “Catre: Iterative point clouds alignment for
category-level object pose refinement,” in <span class="ltx_text ltx_font_italic" id="bib.bib206.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
J. Liu and W. Sun, “Mh6d: Multi-hypothesis consistency learning for
category-level 6-d object pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib207.1.1">IEEE TNNLS</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
X. Li and H. Wang, “Category-level articulated object pose estimation,” in
<span class="ltx_text ltx_font_italic" id="bib.bib208.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
W. Chen and X. Jia, “Fs-net: Fast shape-based network for category-level 6d
object pose estimation with decoupled rotation mechanism,” in <span class="ltx_text ltx_font_italic" id="bib.bib209.1.1">CVPR</span>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
Y. Weng and H. Wang, “Captra: Category-level pose tracking for rigid and
articulated objects from point clouds,” in <span class="ltx_text ltx_font_italic" id="bib.bib210.1.1">ICCV</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
Y. You and R. Shi, “Cppf: Towards robust category-level 9d pose estimation in
the wild,” in <span class="ltx_text ltx_font_italic" id="bib.bib211.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
J. Zhang and M. Wu, “Generative category-level object pose estimation via
diffusion models,” in <span class="ltx_text ltx_font_italic" id="bib.bib212.1.1">NeurIPS</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
C. Wang and Martín-Martín, “6-pack: Category-level 6d pose tracker
with anchor-based keypoints,” in <span class="ltx_text ltx_font_italic" id="bib.bib213.1.1">ICRA</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
J. Lin and Z. Wei, “Dualposenet: Category-level 6d object pose and size
estimation using dual pose network with refined learning of pose
consistency,” in <span class="ltx_text ltx_font_italic" id="bib.bib214.1.1">ICCV</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
B. Wen and K. Bekris, “Bundletrack: 6d pose tracking for novel objects without
instance or category-level 3d models,” in <span class="ltx_text ltx_font_italic" id="bib.bib215.1.1">IROS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
W. Peng and J. Yan, “Self-supervised category-level 6d object pose estimation
with deep implicit shape representation,” in <span class="ltx_text ltx_font_italic" id="bib.bib216.1.1">AAAI</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
T. Lee and B.-U. Lee, “Uda-cope: Unsupervised domain adaptation for
category-level object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib217.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
T. Lee and J. Tremblay, “Tta-cope: Test-time adaptation for category-level
object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib218.1.1">CVPR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
J. Lin and Z. Wei, “Vi-net: Boosting category-level 6d object pose estimation
via learning decoupled rotations on the spherical representations,” in <span class="ltx_text ltx_font_italic" id="bib.bib219.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
Y. Chen and Y. Di, “Secondpose: Se (3)-consistent dual-stream feature fusion
for category-level pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib220.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
T. Lee and B.-U. Lee, “Category-level metric scale object shape and pose
estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib221.1.1">IEEE RAL</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
X. Lin and M. Zhu, “Clipose: Category-level object pose estimation with
pre-trained vision-language knowledge,” <span class="ltx_text ltx_font_italic" id="bib.bib222.1.1">arXiv preprint
arXiv:2402.15726</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
Z. Fan and Z. Song, “Acr-pose: Adversarial canonical representation
reconstruction network for category level 6d object pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib223.1.1">arXiv preprint arXiv:2111.10524</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
T. Nie and J. Ma, “Category-level 6d pose estimation using geometry-guided
instance-aware prior and multi-stage reconstruction,” <span class="ltx_text ltx_font_italic" id="bib.bib224.1.1">IEEE RAL</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
L. Zhou and Z. Liu, “Dr-pose: A two-stage deformation-and-registration
pipeline for category-level 6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib225.1.1">IROS</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
L. Zou and Z. Huang, “Gpt-cope: A graph-guided point transformer for
category-level object pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib226.1.1">IEEE TCSVT</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
G. Li and D. Zhu, “Sd-pose: Structural discrepancy aware category-level 6d
object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib227.1.1">WACV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
S. Yu and D.-H. Zhai, “Catformer: Category-level 6d object pose estimation
with transformer,” in <span class="ltx_text ltx_font_italic" id="bib.bib228.1.1">AAAI</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
Y. He and H. Fan, “Towards self-supervised category-level object pose and size
estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib229.1.1">arXiv preprint arXiv:2203.02884</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
G. Li and Y. Li, “Generative category-level shape and pose estimation with
semantic primitives,” in <span class="ltx_text ltx_font_italic" id="bib.bib230.1.1">CoRL</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
K. Chen and S. James, “Stereopose: Category-level 6d transparent object pose
estimation from stereo images via back-view nocs,” in <span class="ltx_text ltx_font_italic" id="bib.bib231.1.1">ICRA</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
H. Wang and Z. Fan, “Dtf-net: Category-level pose estimation and shape
reconstruction via deformable template field,” in <span class="ltx_text ltx_font_italic" id="bib.bib232.1.1">ACM MM</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
L. Zheng and T. H. E. Tse, “Georef: Geometric alignment across shape variation
for category-level object pose refinement,” in <span class="ltx_text ltx_font_italic" id="bib.bib233.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
K. Zhang and Y. Fu, “Self-supervised geometric correspondence for
category-level 6d object pose estimation in the wild,” in <span class="ltx_text ltx_font_italic" id="bib.bib234.1.1">ICLR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
A. Remus and S. D’Avella, “I2c-net: Using instance-level neural networks for
monocular category-level 6d pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib235.1.1">IEEE RAL</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
J. Liu and Z. Cao, “Category-level 6d object pose estimation with structure
encoder and reasoning attention,” <span class="ltx_text ltx_font_italic" id="bib.bib236.1.1">IEEE TCSVT</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_tag_bibitem">[237]</span>
<span class="ltx_bibblock">
X. Deng and J. Geng, “icaps: Iterative category-level object pose and shape
estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib237.1.1">IEEE RAL</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_tag_bibitem">[238]</span>
<span class="ltx_bibblock">
R. Wang and X. Wang, “Query6dof: Learning sparse queries as implicit shape
prior for category-level 6dof pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib238.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_tag_bibitem">[239]</span>
<span class="ltx_bibblock">
B. Wan and Y. Shi, “Socs: Semantically-aware object coordinate space for
category-level 6d object pose estimation under large shape variations,” in
<span class="ltx_text ltx_font_italic" id="bib.bib239.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_tag_bibitem">[240]</span>
<span class="ltx_bibblock">
X. Lin and W. Yang, “Instance-adaptive and geometric-aware keypoint learning
for category-level 6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib240.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_tag_bibitem">[241]</span>
<span class="ltx_bibblock">
Y. Li and K. Mo, “Category-level multi-part multi-joint 3d shape assembly,”
in <span class="ltx_text ltx_font_italic" id="bib.bib241.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_tag_bibitem">[242]</span>
<span class="ltx_bibblock">
C. Chi and S. Song, “Garmentnets: Category-level pose estimation for garments
via canonical space shape completion,” in <span class="ltx_text ltx_font_italic" id="bib.bib242.1.1">ICCV</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_tag_bibitem">[243]</span>
<span class="ltx_bibblock">
L. Liu and J. Du, “Category-level articulated object 9d pose estimation via
reinforcement learning,” in <span class="ltx_text ltx_font_italic" id="bib.bib243.1.1">ACM MM</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_tag_bibitem">[244]</span>
<span class="ltx_bibblock">
X. Liu and J. Zhang, “Self-supervised category-level articulated object pose
estimation with part-level se (3) equivariance,” <span class="ltx_text ltx_font_italic" id="bib.bib244.1.1">ICLR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_tag_bibitem">[245]</span>
<span class="ltx_bibblock">
X. Li and Y. Weng, “Leveraging se (3) equivariance for self-supervised
category-level object pose estimation from point clouds,” in <span class="ltx_text ltx_font_italic" id="bib.bib245.1.1">NeurIPS</span>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_tag_bibitem">[246]</span>
<span class="ltx_bibblock">
D. Chen and J. Li, “Learning canonical shape space for category-level 6d
object pose and size estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib246.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_tag_bibitem">[247]</span>
<span class="ltx_bibblock">
J. Lin and H. Li, “Sparse steerable convolutions: An efficient learning of se
(3)-equivariant features for estimation and tracking of object poses in 3d
space,” in <span class="ltx_text ltx_font_italic" id="bib.bib247.1.1">NeurIPS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_tag_bibitem">[248]</span>
<span class="ltx_bibblock">
H. Wang and W. Li, “Attention-guided rgb-d fusion network for category-level
6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib248.1.1">IROS</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_tag_bibitem">[249]</span>
<span class="ltx_bibblock">
M. Oquab and T. Darcet, “Dinov2: Learning robust visual features without
supervision,” <span class="ltx_text ltx_font_italic" id="bib.bib249.1.1">arXiv preprint arXiv:2304.07193</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_tag_bibitem">[250]</span>
<span class="ltx_bibblock">
J. J. Park and P. Florence, “Deepsdf: Learning continuous signed distance
functions for shape representation,” in <span class="ltx_text ltx_font_italic" id="bib.bib250.1.1">CVPR</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib251">
<span class="ltx_tag ltx_tag_bibitem">[251]</span>
<span class="ltx_bibblock">
Y. Zhang and Z. Wu, “A transductive approach for video object segmentation,”
in <span class="ltx_text ltx_font_italic" id="bib.bib251.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib252">
<span class="ltx_tag ltx_tag_bibitem">[252]</span>
<span class="ltx_bibblock">
Y. Ono and E. Trulls, “Lf-net: Learning local features from images,” in <span class="ltx_text ltx_font_italic" id="bib.bib252.1.1">NeurIPS</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib253">
<span class="ltx_tag ltx_tag_bibitem">[253]</span>
<span class="ltx_bibblock">
X. Chen and Z. Dong, “Category level object pose estimation via neural
analysis-by-synthesis,” in <span class="ltx_text ltx_font_italic" id="bib.bib253.1.1">ECCV</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib254">
<span class="ltx_tag ltx_tag_bibitem">[254]</span>
<span class="ltx_bibblock">
L. Yen-Chen and P. Florence, “inerf: Inverting neural radiance fields for pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib254.1.1">IROS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib255">
<span class="ltx_tag ltx_tag_bibitem">[255]</span>
<span class="ltx_bibblock">
Y. Lin and J. Tremblay, “Single-stage keypoint-based category-level object
pose estimation from an rgb image,” in <span class="ltx_text ltx_font_italic" id="bib.bib255.1.1">ICRA</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib256">
<span class="ltx_tag ltx_tag_bibitem">[256]</span>
<span class="ltx_bibblock">
J. Guo and F. Zhong, “A visual navigation perspective for category-level
object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib256.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib257">
<span class="ltx_tag ltx_tag_bibitem">[257]</span>
<span class="ltx_bibblock">
W. Ma and A. Wang, “Robust category-level 6d pose estimation with
coarse-to-fine rendering of neural features,” in <span class="ltx_text ltx_font_italic" id="bib.bib257.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib258">
<span class="ltx_tag ltx_tag_bibitem">[258]</span>
<span class="ltx_bibblock">
H. Zhang and A. Opipari, “Transnet: Category-level transparent object pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib258.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib259">
<span class="ltx_tag ltx_tag_bibitem">[259]</span>
<span class="ltx_bibblock">
Y. Lin and J. Tremblay, “Keypoint-based category-level object pose tracking
from an rgb sequence with uncertainty estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib259.1.1">ICRA</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib260">
<span class="ltx_tag ltx_tag_bibitem">[260]</span>
<span class="ltx_bibblock">
S. Yu and D.-H. Zhai, “Cattrack: Single-stage category-level 6d object pose
tracking via convolution and vision transformer,” <span class="ltx_text ltx_font_italic" id="bib.bib260.1.1">IEEE TMM</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib261">
<span class="ltx_tag ltx_tag_bibitem">[261]</span>
<span class="ltx_bibblock">
W. Goodwin and S. Vaze, “Zero-shot category-level object pose estimation,” in
<span class="ltx_text ltx_font_italic" id="bib.bib261.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib262">
<span class="ltx_tag ltx_tag_bibitem">[262]</span>
<span class="ltx_bibblock">
M. Zaccaria and F. Manhardt, “Self-supervised category-level 6d object pose
estimation with optical flow consistency,” <span class="ltx_text ltx_font_italic" id="bib.bib262.1.1">IEEE RAL</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib263">
<span class="ltx_tag ltx_tag_bibitem">[263]</span>
<span class="ltx_bibblock">
J. Cai and Y. He, “Ov9d: Open-vocabulary category-level 9d object pose and
size estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib263.1.1">arXiv preprint arXiv:2403.12396</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib264">
<span class="ltx_tag ltx_tag_bibitem">[264]</span>
<span class="ltx_bibblock">
F. Di Felice and A. Remus, “Zero123-6d: Zero-shot novel view synthesis for rgb
category-level 6d pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib264.1.1">arXiv preprint arXiv:2403.14279</span>,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib265">
<span class="ltx_tag ltx_tag_bibitem">[265]</span>
<span class="ltx_bibblock">
G. Pitteri and S. Ilic, “Cornet: Generic 3d corners for 6d pose estimation of
new objects without retraining,” in <span class="ltx_text ltx_font_italic" id="bib.bib265.1.1">ICCVW</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib266">
<span class="ltx_tag ltx_tag_bibitem">[266]</span>
<span class="ltx_bibblock">
G. Pitteri and A. Bugeau, “3d object detection and pose estimation of unseen
objects in color images with local surface embeddings,” in <span class="ltx_text ltx_font_italic" id="bib.bib266.1.1">ACCV</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib267">
<span class="ltx_tag ltx_tag_bibitem">[267]</span>
<span class="ltx_bibblock">
M. Gou and H. Pan, “Unseen object 6d pose estimation: A benchmark and
baselines,” <span class="ltx_text ltx_font_italic" id="bib.bib267.1.1">arXiv preprint arXiv:2206.11808</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib268">
<span class="ltx_tag ltx_tag_bibitem">[268]</span>
<span class="ltx_bibblock">
F. Hagelskjær and R. L. Haugaard, “Keymatchnet: Zero-shot pose estimation
in 3d point clouds by generalized keypoint matching,” <span class="ltx_text ltx_font_italic" id="bib.bib268.1.1">arXiv preprint
arXiv:2303.16102</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib269">
<span class="ltx_tag ltx_tag_bibitem">[269]</span>
<span class="ltx_bibblock">
H. Zhao and S. Wei, “Learning symmetry-aware geometry correspondences for 6d
object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib269.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib270">
<span class="ltx_tag ltx_tag_bibitem">[270]</span>
<span class="ltx_bibblock">
K. He and G. Gkioxari, “Mask r-cnn,” in <span class="ltx_text ltx_font_italic" id="bib.bib270.1.1">ICCV</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib271">
<span class="ltx_tag ltx_tag_bibitem">[271]</span>
<span class="ltx_bibblock">
J. Chen and M. Sun, “Zeropose: Cad-model-based zero-shot pose estimation,”
<span class="ltx_text ltx_font_italic" id="bib.bib271.1.1">arXiv preprint arXiv:2305.17934</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib272">
<span class="ltx_tag ltx_tag_bibitem">[272]</span>
<span class="ltx_bibblock">
A. Kirillov and E. Mintun, “Segment anything,” in <span class="ltx_text ltx_font_italic" id="bib.bib272.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib273">
<span class="ltx_tag ltx_tag_bibitem">[273]</span>
<span class="ltx_bibblock">
Z. Qin and H. Yu, “Geometric transformer for fast and robust point cloud
registration,” in <span class="ltx_text ltx_font_italic" id="bib.bib273.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib274">
<span class="ltx_tag ltx_tag_bibitem">[274]</span>
<span class="ltx_bibblock">
A. Caraffa and D. Boscaini, “Freeze: Training-free zero-shot 6d pose
estimation with geometric and vision foundation models,” <span class="ltx_text ltx_font_italic" id="bib.bib274.1.1">arXiv preprint
arXiv:2312.00947</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib275">
<span class="ltx_tag ltx_tag_bibitem">[275]</span>
<span class="ltx_bibblock">
J. Huang and H. Yu, “Matchu: Matching unseen objects for 6d pose estimation
from rgb-d images,” in <span class="ltx_text ltx_font_italic" id="bib.bib275.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib276">
<span class="ltx_tag ltx_tag_bibitem">[276]</span>
<span class="ltx_bibblock">
V. N. Nguyen and T. Groueix, “Cnos: A strong baseline for cad-based novel
object segmentation,” in <span class="ltx_text ltx_font_italic" id="bib.bib276.1.1">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib277">
<span class="ltx_tag ltx_tag_bibitem">[277]</span>
<span class="ltx_bibblock">
I. Shugurov and F. Li, “Osop: A multi-stage one shot object pose estimation
framework,” in <span class="ltx_text ltx_font_italic" id="bib.bib277.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib278">
<span class="ltx_tag ltx_tag_bibitem">[278]</span>
<span class="ltx_bibblock">
B. Okorn and Q. Gu, “Zephyr: Zero-shot pose hypothesis rating,” in <span class="ltx_text ltx_font_italic" id="bib.bib278.1.1">ICRA</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib279">
<span class="ltx_tag ltx_tag_bibitem">[279]</span>
<span class="ltx_bibblock">
E. P. Örnek and Y. Labbé, “Foundpose: Unseen object pose estimation
with foundation features,” <span class="ltx_text ltx_font_italic" id="bib.bib279.1.1">arXiv preprint arXiv:2311.18809</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib280">
<span class="ltx_tag ltx_tag_bibitem">[280]</span>
<span class="ltx_bibblock">
M. Sundermeyer and M. Durner, “Multi-path learning for object pose estimation
across domains,” in <span class="ltx_text ltx_font_italic" id="bib.bib280.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib281">
<span class="ltx_tag ltx_tag_bibitem">[281]</span>
<span class="ltx_bibblock">
V. N. Nguyen and Y. Hu, “Templates for 3d object pose estimation revisited:
Generalization to new objects and robustness to occlusions,” in <span class="ltx_text ltx_font_italic" id="bib.bib281.1.1">CVPR</span>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib282">
<span class="ltx_tag ltx_tag_bibitem">[282]</span>
<span class="ltx_bibblock">
S. Moon and H. Son, “Genflow: Generalizable recurrent flow for 6d pose
refinement of novel objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib282.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib283">
<span class="ltx_tag ltx_tag_bibitem">[283]</span>
<span class="ltx_bibblock">
T. Wang and G. Hu, “Object pose estimation via the aggregation of diffusion
features,” in <span class="ltx_text ltx_font_italic" id="bib.bib283.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib284">
<span class="ltx_tag ltx_tag_bibitem">[284]</span>
<span class="ltx_bibblock">
V. Balntas and A. Doumanoglou, “Pose guided rgbd feature learning for 3d
object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib284.1.1">ICCV</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib285">
<span class="ltx_tag ltx_tag_bibitem">[285]</span>
<span class="ltx_bibblock">
Y. Wen and X. Li, “Disp6d: Disentangled implicit shape and pose learning for
scalable 6d pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib285.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib286">
<span class="ltx_tag ltx_tag_bibitem">[286]</span>
<span class="ltx_bibblock">
B. Busam and H. J. Jung, “I like to move it: 6d pose estimation as an action
decision process,” <span class="ltx_text ltx_font_italic" id="bib.bib286.1.1">arXiv preprint arXiv:2009.12678</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib287">
<span class="ltx_tag ltx_tag_bibitem">[287]</span>
<span class="ltx_bibblock">
D. Cai and J. Heikkilä, “Ove6d: Object viewpoint encoding for depth-based
6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib287.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib288">
<span class="ltx_tag ltx_tag_bibitem">[288]</span>
<span class="ltx_bibblock">
W. Kabsch, “A solution for the best rotation to relate two sets of vectors,”
<span class="ltx_text ltx_font_italic" id="bib.bib288.1.1">Acta Crystallographica Section A: Crystal Physics, Diffraction,
Theoretical and General Crystallography</span>, 1976.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib289">
<span class="ltx_tag ltx_tag_bibitem">[289]</span>
<span class="ltx_bibblock">
E. Corona and K. Kundu, “Pose estimation for objects with rotational
symmetry,” in <span class="ltx_text ltx_font_italic" id="bib.bib289.1.1">IROS</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib290">
<span class="ltx_tag ltx_tag_bibitem">[290]</span>
<span class="ltx_bibblock">
C. Zhao and Y. Hu, “Fusing local similarities for retrieval-based 3d
orientation estimation of unseen objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib290.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib291">
<span class="ltx_tag ltx_tag_bibitem">[291]</span>
<span class="ltx_bibblock">
S. Thalhammer and J.-B. Weibel, “Self-supervised vision transformers for 3d
pose estimation of novel objects,” <span class="ltx_text ltx_font_italic" id="bib.bib291.1.1">Image Vision Comput</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib292">
<span class="ltx_tag ltx_tag_bibitem">[292]</span>
<span class="ltx_bibblock">
P. Ausserlechner and D. Haberger, “Zs6d: Zero-shot 6d object pose estimation
using vision transformers,” <span class="ltx_text ltx_font_italic" id="bib.bib292.1.1">arXiv preprint arXiv:2309.11986</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib293">
<span class="ltx_tag ltx_tag_bibitem">[293]</span>
<span class="ltx_bibblock">
A. Dosovitskiy and L. Beyer, “An image is worth 16x16 words: Transformers for
image recognition at scale,” in <span class="ltx_text ltx_font_italic" id="bib.bib293.1.1">CoLR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib294">
<span class="ltx_tag ltx_tag_bibitem">[294]</span>
<span class="ltx_bibblock">
J. Tremblay and B. Wen, “Diff-dope: Differentiable deep object pose
estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib294.1.1">arXiv preprint arXiv:2310.00463</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib295">
<span class="ltx_tag ltx_tag_bibitem">[295]</span>
<span class="ltx_bibblock">
Ultralytics, “GitHub - ultralytics/yolov5.”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ultralytics/yolov5" title="">https://github.com/ultralytics/yolov5</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib296">
<span class="ltx_tag ltx_tag_bibitem">[296]</span>
<span class="ltx_bibblock">
Y. He and Y. Wang, “Fs6d: Few-shot 6d pose estimation of novel objects,” in
<span class="ltx_text ltx_font_italic" id="bib.bib296.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib297">
<span class="ltx_tag ltx_tag_bibitem">[297]</span>
<span class="ltx_bibblock">
P. Castro and T.-K. Kim, “Posematcher: One-shot 6d object pose estimation by
deep feature matching,” in <span class="ltx_text ltx_font_italic" id="bib.bib297.1.1">ICCVW</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib298">
<span class="ltx_tag ltx_tag_bibitem">[298]</span>
<span class="ltx_bibblock">
J. Lee and Y. Cabon, “Mfos: Model-free &amp; one-shot object pose estimation,”
in <span class="ltx_text ltx_font_italic" id="bib.bib298.1.1">AAAI</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib299">
<span class="ltx_tag ltx_tag_bibitem">[299]</span>
<span class="ltx_bibblock">
Y. Du and Y. Xiao, “Pizza: A powerful image-only zero-shot zero-cad approach
to 6 dof tracking,” in <span class="ltx_text ltx_font_italic" id="bib.bib299.1.1">3DV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib300">
<span class="ltx_tag ltx_tag_bibitem">[300]</span>
<span class="ltx_bibblock">
N. Gao and V. A. Ngo, “Sa6d: Self-adaptive few-shot 6d pose estimator for
novel and occluded objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib300.1.1">CoRL</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib301">
<span class="ltx_tag ltx_tag_bibitem">[301]</span>
<span class="ltx_bibblock">
D. Cai and J. Heikkilä, “Gs-pose: Cascaded framework for generalizable
segmentation-based 6d object pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib301.1.1">arXiv preprint
arXiv:2403.10683</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib302">
<span class="ltx_tag ltx_tag_bibitem">[302]</span>
<span class="ltx_bibblock">
C. Jaime and B. Davide, “Open-vocabulary object 6d pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib302.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib303">
<span class="ltx_tag ltx_tag_bibitem">[303]</span>
<span class="ltx_bibblock">
J. Wu and Y. Wang, “Unseen object pose estimation via registration,” in <span class="ltx_text ltx_font_italic" id="bib.bib303.1.1">RCAR</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib304">
<span class="ltx_tag ltx_tag_bibitem">[304]</span>
<span class="ltx_bibblock">
J. Sun and Z. Shen, “Loftr: Detector-free local feature matching with
transformers,” in <span class="ltx_text ltx_font_italic" id="bib.bib304.1.1">CVPR</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib305">
<span class="ltx_tag ltx_tag_bibitem">[305]</span>
<span class="ltx_bibblock">
P.-E. Sarlin and D. DeTone, “Superglue: Learning feature matching with graph
neural networks,” in <span class="ltx_text ltx_font_italic" id="bib.bib305.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib306">
<span class="ltx_tag ltx_tag_bibitem">[306]</span>
<span class="ltx_bibblock">
Z. Fan and P. Pan, “Pope: 6-dof promptable pose estimation of any object, in
any scene, with one reference,” <span class="ltx_text ltx_font_italic" id="bib.bib306.1.1">arXiv preprint arXiv:2305.15727</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib307">
<span class="ltx_tag ltx_tag_bibitem">[307]</span>
<span class="ltx_bibblock">
P. Weinzaepfel and V. Leroy, “Croco: Self-supervised pre-training for 3d
vision tasks by cross-view completion,” in <span class="ltx_text ltx_font_italic" id="bib.bib307.1.1">NeurIPS</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib308">
<span class="ltx_tag ltx_tag_bibitem">[308]</span>
<span class="ltx_bibblock">
C. Zhao and Y. Hu, “Locposenet: Robust location prior for unseen object pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib308.1.1">3DV</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib309">
<span class="ltx_tag ltx_tag_bibitem">[309]</span>
<span class="ltx_bibblock">
P. Pan and Z. Fan, “Learning to estimate 6dof pose from limited data: A
few-shot, generalizable approach using rgb images,” in <span class="ltx_text ltx_font_italic" id="bib.bib309.1.1">3DV</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib310">
<span class="ltx_tag ltx_tag_bibitem">[310]</span>
<span class="ltx_bibblock">
B. Wen and J. Tremblay, “Bundlesdf: Neural 6-dof tracking and 3d
reconstruction of unknown objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib310.1.1">CVPR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib311">
<span class="ltx_tag ltx_tag_bibitem">[311]</span>
<span class="ltx_bibblock">
V. N. Nguyen and T. Groueix, “Nope: Novel object pose estimation from a single
image,” in <span class="ltx_text ltx_font_italic" id="bib.bib311.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib312">
<span class="ltx_tag ltx_tag_bibitem">[312]</span>
<span class="ltx_bibblock">
K. Park and T. Patten, “Neural object learning for 6d pose estimation using a
few cluttered images,” in <span class="ltx_text ltx_font_italic" id="bib.bib312.1.1">ECCV</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib313">
<span class="ltx_tag ltx_tag_bibitem">[313]</span>
<span class="ltx_bibblock">
H. Chen and F. Manhardt, “Texpose: Neural texture learning for self-supervised
6d object pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib313.1.1">CVPR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib314">
<span class="ltx_tag ltx_tag_bibitem">[314]</span>
<span class="ltx_bibblock">
Y. Li and J. Sun, “Weakly supervised 6d pose estimation for robotic
grasping,” in <span class="ltx_text ltx_font_italic" id="bib.bib314.1.1">SIGGRAPH</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib315">
<span class="ltx_tag ltx_tag_bibitem">[315]</span>
<span class="ltx_bibblock">
K. Chen and R. Cao, “Sim-to-real 6d object pose estimation via iterative
self-training for robotic bin picking,” in <span class="ltx_text ltx_font_italic" id="bib.bib315.1.1">ECCV</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib316">
<span class="ltx_tag ltx_tag_bibitem">[316]</span>
<span class="ltx_bibblock">
B. Fu and S. K. Leong, “6d robotic assembly based on rgb-only object pose
estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib316.1.1">IROS</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib317">
<span class="ltx_tag ltx_tag_bibitem">[317]</span>
<span class="ltx_bibblock">
J. Tremblay and T. To, “Deep object pose estimation for semantic robotic
grasping of household objects,” <span class="ltx_text ltx_font_italic" id="bib.bib317.1.1">arXiv preprint arXiv:1809.10790</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib318">
<span class="ltx_tag ltx_tag_bibitem">[318]</span>
<span class="ltx_bibblock">
Z. Dong and S. Liu, “Ppr-net: Point-wise pose regression network for instance
segmentation and 6dd pose estimation in bin-picking scenarios,” in <span class="ltx_text ltx_font_italic" id="bib.bib318.1.1">IROS</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib319">
<span class="ltx_tag ltx_tag_bibitem">[319]</span>
<span class="ltx_bibblock">
C. Zhuang and H. Wang, “Attentionvote: A coarse-to-fine voting network of
anchor-free 6d pose estimation on point cloud for robotic bin-picking
application,” <span class="ltx_text ltx_font_italic" id="bib.bib319.1.1">Robot Cim-int Manuf</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib320">
<span class="ltx_tag ltx_tag_bibitem">[320]</span>
<span class="ltx_bibblock">
K. Wada and E. Sucar, “Morefusion: Multi-object reasoning for 6d pose
estimation from volumetric fusion,” in <span class="ltx_text ltx_font_italic" id="bib.bib320.1.1">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib321">
<span class="ltx_tag ltx_tag_bibitem">[321]</span>
<span class="ltx_bibblock">
H. Zhang and Q. Cao, “Detect in rgb, optimize in edge: Accurate 6d pose
estimation for texture-less industrial parts,” in <span class="ltx_text ltx_font_italic" id="bib.bib321.1.1">ICRA</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib322">
<span class="ltx_tag ltx_tag_bibitem">[322]</span>
<span class="ltx_bibblock">
J. Chang and M. Kim, “Ghostpose: Multi-view pose estimation of transparent
objects for robot hand grasping,” in <span class="ltx_text ltx_font_italic" id="bib.bib322.1.1">IROS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib323">
<span class="ltx_tag ltx_tag_bibitem">[323]</span>
<span class="ltx_bibblock">
J. Kim and H. Pyo, “Tomato harvesting robotic system based on deep-tomatos:
Deep learning network using transformation loss for 6d pose estimation of
maturity classified tomatoes with side-stem,” <span class="ltx_text ltx_font_italic" id="bib.bib323.1.1">Comput Electron Agric</span>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib324">
<span class="ltx_tag ltx_tag_bibitem">[324]</span>
<span class="ltx_bibblock">
C. Liu and W. Sun, “Fine segmentation and difference-aware shape adjustment
for category-level 6dof object pose estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib324.1.1">Appl Intell</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib325">
<span class="ltx_tag ltx_tag_bibitem">[325]</span>
<span class="ltx_bibblock">
S. Yu and D.-H. Zhai, “Category-level 6-d object pose estimation with shape
deformation for robotic grasp detection,” <span class="ltx_text ltx_font_italic" id="bib.bib325.1.1">IEEE TNNLS</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib326">
<span class="ltx_tag ltx_tag_bibitem">[326]</span>
<span class="ltx_bibblock">
J. Sun and Y. Wang, “Ick-track: A category-level 6-dof pose tracker using
inter-frame consistent keypoints for aerial manipulation,” in <span class="ltx_text ltx_font_italic" id="bib.bib326.1.1">IROS</span>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib327">
<span class="ltx_tag ltx_tag_bibitem">[327]</span>
<span class="ltx_bibblock">
S. Yu and D.-H. Zhai, “Robotic grasp detection based on category-level object
pose estimation with self-supervised learning,” <span class="ltx_text ltx_font_italic" id="bib.bib327.1.1">IEEE TMEC</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib328">
<span class="ltx_tag ltx_tag_bibitem">[328]</span>
<span class="ltx_bibblock">
Y. Su and J. Rambach, “Deep multi-state object pose estimation for augmented
reality assembly,” in <span class="ltx_text ltx_font_italic" id="bib.bib328.1.1">ISMAR-Adjunct</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib329">
<span class="ltx_tag ltx_tag_bibitem">[329]</span>
<span class="ltx_bibblock">
R. Pandey and P. Pidlypenskyi, “Efficient 6-dof tracking of handheld objects
from an egocentric viewpoint,” in <span class="ltx_text ltx_font_italic" id="bib.bib329.1.1">ECCV</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib330">
<span class="ltx_tag ltx_tag_bibitem">[330]</span>
<span class="ltx_bibblock">
P. F. Proença and Y. Gao, “Deep learning for spacecraft pose estimation
from photorealistic rendering,” in <span class="ltx_text ltx_font_italic" id="bib.bib330.1.1">ICRA</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib331">
<span class="ltx_tag ltx_tag_bibitem">[331]</span>
<span class="ltx_bibblock">
U. E. 4, “Unreal engine 4.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.unrealengine.com" title="">https://www.unrealengine.com</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib332">
<span class="ltx_tag ltx_tag_bibitem">[332]</span>
<span class="ltx_bibblock">
S. Wang and S. Wang, “Ca-spacenet: Counterfactual analysis for 6d pose
estimation in space,” in <span class="ltx_text ltx_font_italic" id="bib.bib332.1.1">IROS</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib333">
<span class="ltx_tag ltx_tag_bibitem">[333]</span>
<span class="ltx_bibblock">
M. Ulmer and M. Durner, “6d object pose estimation from approximate 3d models
for orbital robotics,” in <span class="ltx_text ltx_font_italic" id="bib.bib333.1.1">IROS</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib334">
<span class="ltx_tag ltx_tag_bibitem">[334]</span>
<span class="ltx_bibblock">
T. Patten and K. Park, “Object learning for 6d pose estimation and grasping
from rgb-d videos of in-hand manipulation,” in <span class="ltx_text ltx_font_italic" id="bib.bib334.1.1">IROS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib335">
<span class="ltx_tag ltx_tag_bibitem">[335]</span>
<span class="ltx_bibblock">
H. Qi and C. Zhao, “Hoisdf: Constraining 3d hand-object pose estimation with
global signed distance fields,” in <span class="ltx_text ltx_font_italic" id="bib.bib335.1.1">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib336">
<span class="ltx_tag ltx_tag_bibitem">[336]</span>
<span class="ltx_bibblock">
S. Hoque and S. Xu, “Deep learning for 6d pose estimation of objects-a case
study for autonomous driving,” <span class="ltx_text ltx_font_italic" id="bib.bib336.1.1">Expert Syst Appl</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib337">
<span class="ltx_tag ltx_tag_bibitem">[337]</span>
<span class="ltx_bibblock">
J. L. Elman, “Finding structure in time,” <span class="ltx_text ltx_font_italic" id="bib.bib337.1.1">Cogn Sci</span>, 1990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib338">
<span class="ltx_tag ltx_tag_bibitem">[338]</span>
<span class="ltx_bibblock">
H. Sun and P. Ni, “Panelpose: A 6d pose estimation of highly-variable panel
object for robotic robust cockpit panel inspection,” in <span class="ltx_text ltx_font_italic" id="bib.bib338.1.1">IROS</span>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 31 15:08:14 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
