<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data</title>
<!--Generated on Thu Jun 20 15:29:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.14412v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S1" title="In Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S2" title="In Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>3DDogs-Lab</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S3" title="In Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments and Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S4" title="In Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S5" title="In Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>3DDogs-Lab</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S5.SS1" title="In 5 3DDogs-Lab â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Generation of 3DDogs-Wild</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6" title="In Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experiments and Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.SS1" title="In 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Pose estimation models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.SS2" title="In 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>3DDogs-Wild vs 3DDogs-Lab</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.SS3" title="In 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Generalisation to other species</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Moira Shooter
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1" style="font-size:90%;">m.shooter@surrey.ac.uk</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Charles Malleson
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1" style="font-size:90%;">charles.malleson@surrey.ac.uk</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adrian Hilton
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.1.id1" style="font-size:90%;">a.hilton@surrey.ac.uk</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey,
<br class="ltx_break"/>Guildford UK
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">We introduce a new benchmark analysis focusing on 3D canine pose estimation from monocular in-the-wild images. A multi-modal dataset â€œ3DDogs-Labâ€ was captured indoors, featuring various dog breeds trotting on a walkway. It includes data from optical marker-based mocap systems, RGBD cameras, IMUs, and a pressure mat. While providing high-quality motion data, the presence of optical markers and limited background diversity make the captured video less representative of real-world conditions. To address this, we created â€œ3DDogs-Wildâ€, a naturalised version of the dataset where the optical markers are in-painted and the subjects are placed in diverse environments, enhancing its utility for training RGB image-based pose detectors. We show that using the 3DDogs-Wild to train the models leads to improved performance when evaluating on in-the-wild data. Additionally, we provide a thorough analysis using various pose estimation models, revealing their respective strengths and weaknesses. We believe that our findings, coupled with the datasets provided, offer valuable insights for advancing 3D animal pose estimation.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="615" id="S1.F1.g1" src="extracted/5681311/QualityBenchmark.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Qualitative results of different pose estimation methods trained on the 3DDogs-Wild dataset.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Traditionally, multiple sensor systems such as motion capture (mocap) are used to analyse the gait of animals. While these systems offer accurate kinematic and kinetic information, they are invasive and setting up these systems requires time and specialised expertise. In response to this, there is a growing interest in monocular and markerless approaches using machine learning techniques offering simple and non-invasive systems. While it is possible to use the RGB image data captured by mocap for training pose estimation models, these models are prone to fail to generalise when applied to images captured in natural environments. This is due to the domain gap as there are significant differences in conditions between the controlled environment of motion capture and the varied conditions found in natural settings.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The majority of existing 3D datasets were acquired in controlled environments using multi-view systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>. However, there has been an effort in producing 3D in-the-wild datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> where the 3D ground truth was generated by triangulating the 2D joint coordinates from multiple views. Despite this, the reliability of the 3D ground truth is directly dependent on the quality of the 2D manual annotations made by humans.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recognising the limitations associated with producing 3D datasets, previous research proposed the creation of synthetic datasets using real-time game engines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> or employing parametric models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. However, a major challenge associated with using synthetic training data is the domain gap between real and synthetic data.
To address this, previous research proposed the generation of near-realistic data using style-transfer methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> or by modifying the Grand Theft Auto game to simulate real-world data combined by leveraging existing foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Capturing 3D datasets of animals presents greater challenges than working with humans, primarily due to the unpredictability of animal behaviour. Consequently, thereâ€™s been a growing trend towards releasing 2D animal pose datasets, as they are simpler to develop. These datasets were designed with the aim of both pose estimation and tracking, as well as understanding animal behaviour <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Due to these advances and lack of 3D animal pose datasets, previous works related to 3D animal pose relied heavily on 2D ground truth and priors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. While these show promising results, due to the sole reliance on 2D signals, there remains a potential for depth ambiguity. Additionally, the acquisition of datasets including prior information such as ground contact <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> presents a challenge due to the time and cost associated with labelling. Moreover, given the scarcity of 3D datasets, when evaluating the 3D predictions, most of these methods project the 3D pose onto the image plane. This approach does not provide an accurate assessment of the predicted 3D pose.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">In this paper</span>, we present a benchmark analysis for monocular 3D dog pose estimation. To leverage the motion capture datasetâ€™s 3D ground truth and for machine learning models to generalise across different image conditions, an in-the-wild version of the original dataset was created by removing the optical markers and replacing the default backgrounds. Both datasets will be made available for research.
We focus on the generalisation capabilities of the models using both the mocap and in-the-wild version datasets. Specifically, we explore two key questions: (i) whether the pose estimation models exhibit better performance when trained on the in-the-wild version of the data compared to the original, indoor dataset containing optical markers and no diversity of background. (ii) whether the top performing pose estimation models can generalise to other animal species using the Animal3D dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>.

</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>3DDogs-Lab</h2>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="257" id="S2.F2.g1" src="extracted/5681311/ogvsinthewild.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Samples of 3DDogs-Lab showcasing the original motion capture vs in-the-wild version sequences.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Data capture of <span class="ltx_text ltx_font_italic" id="S2.p1.1.1.1">3DDogs-Lab</span></span>.
The aim of creating the dataset was to detect lameness in dogs by using different types of sensors/capture systems including pressure mat, optical markers, IMUs and RGBD cameras. The capture included 64 dogs, each performing three trials of walking and three of trotting. In light of technical challenges with the capture hardware, it was not possible to obtain valid recordings for all participants. As a result, there is a reduction in the number of usable subjects and trials available for analysis. The final dataset contained a total of 37 subjects and 143 valid recordings. In each trial, the dog moves along a walkway from left to right and then right to left. The capture configuration comprised 8 RGBD cameras along with optical marker and IMU tracking systems. Since all cameras were located on one side of the walkway, only one side of the dog is visible in each pass. Henceforth, the original dataset is denoted as the <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S2.p1.1.2">3DDogs-Lab</span> dataset.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Generation of <span class="ltx_text ltx_font_italic" id="S2.p2.1.1.1">3DDogs-Wild</span></span>. To produce an in-the-wild version of the 3DDogs-Lab dataset, we removed the optical markers from the frames and produced various backgrounds for each sequence (<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S2.F2" title="In 2 3DDogs-Lab â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>). We produced an in-the-wild version for one camera view. As the dog is only in view of each camera for part of the sequence, we only generate the in-the-wild output for frames which the dog is in the cameraâ€™s field of view. This involved reprojecting the 3D coordinates of optical markers into the image plane. During this step, we also created masks specifically for frames where the optical markers were located within the cameraâ€™s image plane. These masks were later used in the inpainting process of the optical markers, ensuring a uniform fur appearance without optical markers across frames. By leveraging the capabilities of ProPainter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> we achieved near-consistent inpainting results for each recorded sequence.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">To integrate the subjects with various backgrounds, we produced binary masks by combining the binary masks from both off-the-shelf segmentation models such as SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> and YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. The binary masks were merged using operations such as addition and subtraction to leverage the benefits of both. Additionally, we manually verified each frame for quality assurance. The application of binary masks for subject-background composition resulted in a distinct â€œcopy-pasteâ€ artifact, characterised by overly sharp edges of the subject. We refined the integration of the subject into the background by producing alpha mattes from the binary masks. This process involved creating trimaps, which are segmentation maps delineating foreground, background, and the transitional area, by applying erosion and dilation operations on the binary masks. To produce the final alpha mattes, the RGB image along with the trimap, was fed into the VitMatte image matting model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.8">To generate backgrounds, we leveraged the Stable Diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>. We originally provided a text prompt, binary mask, and the RGB image as input. However, this method resulted in artifacts, such as additional dog limbs, which were unintended (see Fig. S<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S5.F4" title="Figure 4 â€£ 5.1 Generation of 3DDogs-Wild â€£ 5 3DDogs-Lab â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">4</span></a> in supplementary material (Supp.)). While this approach may suffice for a single image, its lack of background consistency rendered it unsuitable for a sequence of frames. Therefore, we decided to generate one static image for each sequence using only a text prompt, such as <math alttext="t_{0}=" class="ltx_Math" display="inline" id="S2.p4.1.m1.1"><semantics id="S2.p4.1.m1.1a"><mrow id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml"><msub id="S2.p4.1.m1.1.1.2" xref="S2.p4.1.m1.1.1.2.cmml"><mi id="S2.p4.1.m1.1.1.2.2" xref="S2.p4.1.m1.1.1.2.2.cmml">t</mi><mn id="S2.p4.1.m1.1.1.2.3" xref="S2.p4.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="S2.p4.1.m1.1.1.1" xref="S2.p4.1.m1.1.1.1.cmml">=</mo><mi id="S2.p4.1.m1.1.1.3" xref="S2.p4.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><apply id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1"><eq id="S2.p4.1.m1.1.1.1.cmml" xref="S2.p4.1.m1.1.1.1"></eq><apply id="S2.p4.1.m1.1.1.2.cmml" xref="S2.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.p4.1.m1.1.1.2.1.cmml" xref="S2.p4.1.m1.1.1.2">subscript</csymbol><ci id="S2.p4.1.m1.1.1.2.2.cmml" xref="S2.p4.1.m1.1.1.2.2">ğ‘¡</ci><cn id="S2.p4.1.m1.1.1.2.3.cmml" type="integer" xref="S2.p4.1.m1.1.1.2.3">0</cn></apply><csymbol cd="latexml" id="S2.p4.1.m1.1.1.3.cmml" xref="S2.p4.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">t_{0}=</annotation><annotation encoding="application/x-llamapun" id="S2.p4.1.m1.1d">italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT =</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.p4.8.1">â€œA clean and empty realistic colored photograph of </span>{<span class="ltx_text ltx_font_italic" id="S2.p4.8.2">s</span>}<span class="ltx_text ltx_font_italic" id="S2.p4.8.3"> environment with floor.â€</span>. The variable <math alttext="s" class="ltx_Math" display="inline" id="S2.p4.2.m2.1"><semantics id="S2.p4.2.m2.1a"><mi id="S2.p4.2.m2.1.1" xref="S2.p4.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.p4.2.m2.1b"><ci id="S2.p4.2.m2.1.1.cmml" xref="S2.p4.2.m2.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.p4.2.m2.1d">italic_s</annotation></semantics></math> is set to <math alttext="s=outdoor" class="ltx_Math" display="inline" id="S2.p4.3.m3.1"><semantics id="S2.p4.3.m3.1a"><mrow id="S2.p4.3.m3.1.1" xref="S2.p4.3.m3.1.1.cmml"><mi id="S2.p4.3.m3.1.1.2" xref="S2.p4.3.m3.1.1.2.cmml">s</mi><mo id="S2.p4.3.m3.1.1.1" xref="S2.p4.3.m3.1.1.1.cmml">=</mo><mrow id="S2.p4.3.m3.1.1.3" xref="S2.p4.3.m3.1.1.3.cmml"><mi id="S2.p4.3.m3.1.1.3.2" xref="S2.p4.3.m3.1.1.3.2.cmml">o</mi><mo id="S2.p4.3.m3.1.1.3.1" xref="S2.p4.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.3.m3.1.1.3.3" xref="S2.p4.3.m3.1.1.3.3.cmml">u</mi><mo id="S2.p4.3.m3.1.1.3.1a" xref="S2.p4.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.3.m3.1.1.3.4" xref="S2.p4.3.m3.1.1.3.4.cmml">t</mi><mo id="S2.p4.3.m3.1.1.3.1b" xref="S2.p4.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.3.m3.1.1.3.5" xref="S2.p4.3.m3.1.1.3.5.cmml">d</mi><mo id="S2.p4.3.m3.1.1.3.1c" xref="S2.p4.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.3.m3.1.1.3.6" xref="S2.p4.3.m3.1.1.3.6.cmml">o</mi><mo id="S2.p4.3.m3.1.1.3.1d" xref="S2.p4.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.3.m3.1.1.3.7" xref="S2.p4.3.m3.1.1.3.7.cmml">o</mi><mo id="S2.p4.3.m3.1.1.3.1e" xref="S2.p4.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.3.m3.1.1.3.8" xref="S2.p4.3.m3.1.1.3.8.cmml">r</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.3.m3.1b"><apply id="S2.p4.3.m3.1.1.cmml" xref="S2.p4.3.m3.1.1"><eq id="S2.p4.3.m3.1.1.1.cmml" xref="S2.p4.3.m3.1.1.1"></eq><ci id="S2.p4.3.m3.1.1.2.cmml" xref="S2.p4.3.m3.1.1.2">ğ‘ </ci><apply id="S2.p4.3.m3.1.1.3.cmml" xref="S2.p4.3.m3.1.1.3"><times id="S2.p4.3.m3.1.1.3.1.cmml" xref="S2.p4.3.m3.1.1.3.1"></times><ci id="S2.p4.3.m3.1.1.3.2.cmml" xref="S2.p4.3.m3.1.1.3.2">ğ‘œ</ci><ci id="S2.p4.3.m3.1.1.3.3.cmml" xref="S2.p4.3.m3.1.1.3.3">ğ‘¢</ci><ci id="S2.p4.3.m3.1.1.3.4.cmml" xref="S2.p4.3.m3.1.1.3.4">ğ‘¡</ci><ci id="S2.p4.3.m3.1.1.3.5.cmml" xref="S2.p4.3.m3.1.1.3.5">ğ‘‘</ci><ci id="S2.p4.3.m3.1.1.3.6.cmml" xref="S2.p4.3.m3.1.1.3.6">ğ‘œ</ci><ci id="S2.p4.3.m3.1.1.3.7.cmml" xref="S2.p4.3.m3.1.1.3.7">ğ‘œ</ci><ci id="S2.p4.3.m3.1.1.3.8.cmml" xref="S2.p4.3.m3.1.1.3.8">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.3.m3.1c">s=outdoor</annotation><annotation encoding="application/x-llamapun" id="S2.p4.3.m3.1d">italic_s = italic_o italic_u italic_t italic_d italic_o italic_o italic_r</annotation></semantics></math> or <math alttext="s=indoor" class="ltx_Math" display="inline" id="S2.p4.4.m4.1"><semantics id="S2.p4.4.m4.1a"><mrow id="S2.p4.4.m4.1.1" xref="S2.p4.4.m4.1.1.cmml"><mi id="S2.p4.4.m4.1.1.2" xref="S2.p4.4.m4.1.1.2.cmml">s</mi><mo id="S2.p4.4.m4.1.1.1" xref="S2.p4.4.m4.1.1.1.cmml">=</mo><mrow id="S2.p4.4.m4.1.1.3" xref="S2.p4.4.m4.1.1.3.cmml"><mi id="S2.p4.4.m4.1.1.3.2" xref="S2.p4.4.m4.1.1.3.2.cmml">i</mi><mo id="S2.p4.4.m4.1.1.3.1" xref="S2.p4.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.4.m4.1.1.3.3" xref="S2.p4.4.m4.1.1.3.3.cmml">n</mi><mo id="S2.p4.4.m4.1.1.3.1a" xref="S2.p4.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.4.m4.1.1.3.4" xref="S2.p4.4.m4.1.1.3.4.cmml">d</mi><mo id="S2.p4.4.m4.1.1.3.1b" xref="S2.p4.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.4.m4.1.1.3.5" xref="S2.p4.4.m4.1.1.3.5.cmml">o</mi><mo id="S2.p4.4.m4.1.1.3.1c" xref="S2.p4.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.4.m4.1.1.3.6" xref="S2.p4.4.m4.1.1.3.6.cmml">o</mi><mo id="S2.p4.4.m4.1.1.3.1d" xref="S2.p4.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.4.m4.1.1.3.7" xref="S2.p4.4.m4.1.1.3.7.cmml">r</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.4.m4.1b"><apply id="S2.p4.4.m4.1.1.cmml" xref="S2.p4.4.m4.1.1"><eq id="S2.p4.4.m4.1.1.1.cmml" xref="S2.p4.4.m4.1.1.1"></eq><ci id="S2.p4.4.m4.1.1.2.cmml" xref="S2.p4.4.m4.1.1.2">ğ‘ </ci><apply id="S2.p4.4.m4.1.1.3.cmml" xref="S2.p4.4.m4.1.1.3"><times id="S2.p4.4.m4.1.1.3.1.cmml" xref="S2.p4.4.m4.1.1.3.1"></times><ci id="S2.p4.4.m4.1.1.3.2.cmml" xref="S2.p4.4.m4.1.1.3.2">ğ‘–</ci><ci id="S2.p4.4.m4.1.1.3.3.cmml" xref="S2.p4.4.m4.1.1.3.3">ğ‘›</ci><ci id="S2.p4.4.m4.1.1.3.4.cmml" xref="S2.p4.4.m4.1.1.3.4">ğ‘‘</ci><ci id="S2.p4.4.m4.1.1.3.5.cmml" xref="S2.p4.4.m4.1.1.3.5">ğ‘œ</ci><ci id="S2.p4.4.m4.1.1.3.6.cmml" xref="S2.p4.4.m4.1.1.3.6">ğ‘œ</ci><ci id="S2.p4.4.m4.1.1.3.7.cmml" xref="S2.p4.4.m4.1.1.3.7">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.4.m4.1c">s=indoor</annotation><annotation encoding="application/x-llamapun" id="S2.p4.4.m4.1d">italic_s = italic_i italic_n italic_d italic_o italic_o italic_r</annotation></semantics></math>. To enhance the diversity of generated backgrounds, when the variable <math alttext="s=outdoor" class="ltx_Math" display="inline" id="S2.p4.5.m5.1"><semantics id="S2.p4.5.m5.1a"><mrow id="S2.p4.5.m5.1.1" xref="S2.p4.5.m5.1.1.cmml"><mi id="S2.p4.5.m5.1.1.2" xref="S2.p4.5.m5.1.1.2.cmml">s</mi><mo id="S2.p4.5.m5.1.1.1" xref="S2.p4.5.m5.1.1.1.cmml">=</mo><mrow id="S2.p4.5.m5.1.1.3" xref="S2.p4.5.m5.1.1.3.cmml"><mi id="S2.p4.5.m5.1.1.3.2" xref="S2.p4.5.m5.1.1.3.2.cmml">o</mi><mo id="S2.p4.5.m5.1.1.3.1" xref="S2.p4.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.5.m5.1.1.3.3" xref="S2.p4.5.m5.1.1.3.3.cmml">u</mi><mo id="S2.p4.5.m5.1.1.3.1a" xref="S2.p4.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.5.m5.1.1.3.4" xref="S2.p4.5.m5.1.1.3.4.cmml">t</mi><mo id="S2.p4.5.m5.1.1.3.1b" xref="S2.p4.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.5.m5.1.1.3.5" xref="S2.p4.5.m5.1.1.3.5.cmml">d</mi><mo id="S2.p4.5.m5.1.1.3.1c" xref="S2.p4.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.5.m5.1.1.3.6" xref="S2.p4.5.m5.1.1.3.6.cmml">o</mi><mo id="S2.p4.5.m5.1.1.3.1d" xref="S2.p4.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.5.m5.1.1.3.7" xref="S2.p4.5.m5.1.1.3.7.cmml">o</mi><mo id="S2.p4.5.m5.1.1.3.1e" xref="S2.p4.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S2.p4.5.m5.1.1.3.8" xref="S2.p4.5.m5.1.1.3.8.cmml">r</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.5.m5.1b"><apply id="S2.p4.5.m5.1.1.cmml" xref="S2.p4.5.m5.1.1"><eq id="S2.p4.5.m5.1.1.1.cmml" xref="S2.p4.5.m5.1.1.1"></eq><ci id="S2.p4.5.m5.1.1.2.cmml" xref="S2.p4.5.m5.1.1.2">ğ‘ </ci><apply id="S2.p4.5.m5.1.1.3.cmml" xref="S2.p4.5.m5.1.1.3"><times id="S2.p4.5.m5.1.1.3.1.cmml" xref="S2.p4.5.m5.1.1.3.1"></times><ci id="S2.p4.5.m5.1.1.3.2.cmml" xref="S2.p4.5.m5.1.1.3.2">ğ‘œ</ci><ci id="S2.p4.5.m5.1.1.3.3.cmml" xref="S2.p4.5.m5.1.1.3.3">ğ‘¢</ci><ci id="S2.p4.5.m5.1.1.3.4.cmml" xref="S2.p4.5.m5.1.1.3.4">ğ‘¡</ci><ci id="S2.p4.5.m5.1.1.3.5.cmml" xref="S2.p4.5.m5.1.1.3.5">ğ‘‘</ci><ci id="S2.p4.5.m5.1.1.3.6.cmml" xref="S2.p4.5.m5.1.1.3.6">ğ‘œ</ci><ci id="S2.p4.5.m5.1.1.3.7.cmml" xref="S2.p4.5.m5.1.1.3.7">ğ‘œ</ci><ci id="S2.p4.5.m5.1.1.3.8.cmml" xref="S2.p4.5.m5.1.1.3.8">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.5.m5.1c">s=outdoor</annotation><annotation encoding="application/x-llamapun" id="S2.p4.5.m5.1d">italic_s = italic_o italic_u italic_t italic_d italic_o italic_o italic_r</annotation></semantics></math>, we expanded the text prompt to include the type of floor, <math alttext="t=t_{0}+" class="ltx_Math" display="inline" id="S2.p4.6.m6.1"><semantics id="S2.p4.6.m6.1a"><mrow id="S2.p4.6.m6.1.1" xref="S2.p4.6.m6.1.1.cmml"><mi id="S2.p4.6.m6.1.1.2" xref="S2.p4.6.m6.1.1.2.cmml">t</mi><mo id="S2.p4.6.m6.1.1.1" xref="S2.p4.6.m6.1.1.1.cmml">=</mo><mrow id="S2.p4.6.m6.1.1.3" xref="S2.p4.6.m6.1.1.3.cmml"><msub id="S2.p4.6.m6.1.1.3.2" xref="S2.p4.6.m6.1.1.3.2.cmml"><mi id="S2.p4.6.m6.1.1.3.2.2" xref="S2.p4.6.m6.1.1.3.2.2.cmml">t</mi><mn id="S2.p4.6.m6.1.1.3.2.3" xref="S2.p4.6.m6.1.1.3.2.3.cmml">0</mn></msub><mo id="S2.p4.6.m6.1.1.3.3" xref="S2.p4.6.m6.1.1.3.3.cmml">+</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.6.m6.1b"><apply id="S2.p4.6.m6.1.1.cmml" xref="S2.p4.6.m6.1.1"><eq id="S2.p4.6.m6.1.1.1.cmml" xref="S2.p4.6.m6.1.1.1"></eq><ci id="S2.p4.6.m6.1.1.2.cmml" xref="S2.p4.6.m6.1.1.2">ğ‘¡</ci><apply id="S2.p4.6.m6.1.1.3.cmml" xref="S2.p4.6.m6.1.1.3"><csymbol cd="latexml" id="S2.p4.6.m6.1.1.3.1.cmml" xref="S2.p4.6.m6.1.1.3">limit-from</csymbol><apply id="S2.p4.6.m6.1.1.3.2.cmml" xref="S2.p4.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S2.p4.6.m6.1.1.3.2.1.cmml" xref="S2.p4.6.m6.1.1.3.2">subscript</csymbol><ci id="S2.p4.6.m6.1.1.3.2.2.cmml" xref="S2.p4.6.m6.1.1.3.2.2">ğ‘¡</ci><cn id="S2.p4.6.m6.1.1.3.2.3.cmml" type="integer" xref="S2.p4.6.m6.1.1.3.2.3">0</cn></apply><plus id="S2.p4.6.m6.1.1.3.3.cmml" xref="S2.p4.6.m6.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.6.m6.1c">t=t_{0}+</annotation><annotation encoding="application/x-llamapun" id="S2.p4.6.m6.1d">italic_t = italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT +</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S2.p4.8.4">â€œwith floor made of </span>{<span class="ltx_text ltx_font_italic" id="S2.p4.8.5">e</span>}<span class="ltx_text ltx_font_italic" id="S2.p4.8.6">â€</span>. The variable <math alttext="e" class="ltx_Math" display="inline" id="S2.p4.7.m7.1"><semantics id="S2.p4.7.m7.1a"><mi id="S2.p4.7.m7.1.1" xref="S2.p4.7.m7.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S2.p4.7.m7.1b"><ci id="S2.p4.7.m7.1.1.cmml" xref="S2.p4.7.m7.1.1">ğ‘’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.7.m7.1c">e</annotation><annotation encoding="application/x-llamapun" id="S2.p4.7.m7.1d">italic_e</annotation></semantics></math> was set to <math alttext="e=" class="ltx_Math" display="inline" id="S2.p4.8.m8.1"><semantics id="S2.p4.8.m8.1a"><mrow id="S2.p4.8.m8.1.1" xref="S2.p4.8.m8.1.1.cmml"><mi id="S2.p4.8.m8.1.1.2" xref="S2.p4.8.m8.1.1.2.cmml">e</mi><mo id="S2.p4.8.m8.1.1.1" xref="S2.p4.8.m8.1.1.1.cmml">=</mo><mi id="S2.p4.8.m8.1.1.3" xref="S2.p4.8.m8.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.8.m8.1b"><apply id="S2.p4.8.m8.1.1.cmml" xref="S2.p4.8.m8.1.1"><eq id="S2.p4.8.m8.1.1.1.cmml" xref="S2.p4.8.m8.1.1.1"></eq><ci id="S2.p4.8.m8.1.1.2.cmml" xref="S2.p4.8.m8.1.1.2">ğ‘’</ci><csymbol cd="latexml" id="S2.p4.8.m8.1.1.3.cmml" xref="S2.p4.8.m8.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.8.m8.1c">e=</annotation><annotation encoding="application/x-llamapun" id="S2.p4.8.m8.1d">italic_e =</annotation></semantics></math>{<span class="ltx_text ltx_font_italic" id="S2.p4.8.7">sand, grass, rocks, pavement, tiles, or snow</span>}. A total of 37 subjects, 286 sequences, and 12,940 frames were generated for the final in-the-wild-version of the dataset. Henceforth, the â€œnaturalisedâ€ version of the dataset is denoted as the <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S2.p4.8.8">3DDogs-Wild</span> dataset.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and Results</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.p1.1.1">Evaluation metrics</span>. We used the Percentage of Correct Keypoints (PCK) and Mean Per Joint Position Error (MPJPE) metrics for evaluating the pose estimation methods. The PCK determines a keypoint to be correctly localised if the predicted position of the keypoint falls within a specified threshold of the ground truth keypoint. Meanwhile, the MPJPE metric calculates the average distance between the predicted and ground truth keypoints. The distances between predicted and ground truth keypoints were normalised relative to the length of the bounding box diagonal for both 2D and 3D PCK. The thresholds for both 2D and 3D PCKs were set to 0.15. While normalisation for 2D MPJPE was based on the bounding box diagonal length, the 3D MPJPE was not normalised and reported in millimeters. Furthermore, we included supplementary 3D metrics, PA-MPJPE and PA-PCK, indicating the metrics after the alignment between predicted 3D pose and 3D ground truth through Procrustes alignment (PA).</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="437" id="S3.F3.g1" src="extracted/5681311/Animals3D_benchmark3.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Qualitative results on samples of the Animals3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> test set from D-Pose (DINOv2-S) trained on only the 3DDogs-Wild dataset. The pose is viewed from different angles.</span></figcaption>
</figure>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.6.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.6.7.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S3.T1.6.7.1.2">3D</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S3.T1.6.7.1.3">2D</th>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6">
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S3.T1.6.6.7"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.2">PA-MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.m1.1a"><mo id="S3.T1.2.2.2.m1.1.1" stretchy="false" xref="S3.T1.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.3.3.3">PCK@0.15 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.3.3.3.m1.1"><semantics id="S3.T1.3.3.3.m1.1a"><mo id="S3.T1.3.3.3.m1.1.1" stretchy="false" xref="S3.T1.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.4.4.4">PA-PCK@0.15 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.4.4.4.m1.1"><semantics id="S3.T1.4.4.4.m1.1a"><mo id="S3.T1.4.4.4.m1.1.1" stretchy="false" xref="S3.T1.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.m1.1b"><ci id="S3.T1.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.5.5.5">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.5.5.5.m1.1"><semantics id="S3.T1.5.5.5.m1.1a"><mo id="S3.T1.5.5.5.m1.1.1" stretchy="false" xref="S3.T1.5.5.5.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.m1.1b"><ci id="S3.T1.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.6.6.6">PCK@0.15 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.6.6.6.m1.1"><semantics id="S3.T1.6.6.6.m1.1a"><mo id="S3.T1.6.6.6.m1.1.1" stretchy="false" xref="S3.T1.6.6.6.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.m1.1b"><ci id="S3.T1.6.6.6.m1.1.1.cmml" xref="S3.T1.6.6.6.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.m1.1d">â†‘</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.6.8.1">
<td class="ltx_td ltx_align_left" id="S3.T1.6.8.1.1">8-StackedHourglass</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.8.1.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.8.1.2.1">20.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.8.1.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.8.1.3.1">14.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.8.1.4"><span class="ltx_text ltx_ulem_uuline" id="S3.T1.6.8.1.4.1">68.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.8.1.5"><span class="ltx_text ltx_ulem_uuline" id="S3.T1.6.8.1.5.1">86.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.8.1.6">3.67</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.8.1.7">88.16</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.9.2">
<td class="ltx_td ltx_align_left" id="S3.T1.6.9.2.1">DLC2018 (ResNet50)</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.9.2.2">29.79</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.9.2.3">23.79</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.9.2.4">62.35</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.9.2.5">76.65</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.9.2.6"><span class="ltx_text ltx_ulem_uuline" id="S3.T1.6.9.2.6.1">2.12</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.9.2.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.9.2.7.1">91.91</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.10.3">
<td class="ltx_td ltx_align_left" id="S3.T1.6.10.3.1">D-Pose (ResNet50)</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.10.3.2"><span class="ltx_text ltx_ulem_uuline" id="S3.T1.6.10.3.2.1">22.65</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.10.3.3"><span class="ltx_text ltx_ulem_uuline" id="S3.T1.6.10.3.3.1">16.90</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.10.3.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.10.3.4.1">75.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.10.3.5">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.10.3.5.1">91.3</span>1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.10.3.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.10.3.6.1">2.07</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.10.3.7"><span class="ltx_text ltx_ulem_uuline" id="S3.T1.6.10.3.7.1">90.73</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.11.4">
<td class="ltx_td ltx_align_left" id="S3.T1.6.11.4.1">D-Pose (DINOv2-S)</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.11.4.2"><span class="ltx_text ltx_font_bold" id="S3.T1.6.11.4.2.1">16.81</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.11.4.3"><span class="ltx_text ltx_font_bold" id="S3.T1.6.11.4.3.1">11.36</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.11.4.4"><span class="ltx_text ltx_font_bold" id="S3.T1.6.11.4.4.1">86.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.11.4.5">98.04</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.11.4.6"><span class="ltx_text ltx_font_bold" id="S3.T1.6.11.4.6.1">1.56</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.11.4.7"><span class="ltx_text ltx_font_bold" id="S3.T1.6.11.4.7.1">98.63</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.12.5">
<td class="ltx_td ltx_align_left" id="S3.T1.6.12.5.1">D-Pose (DINOv2-B)</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.12.5.2">16.38</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.12.5.3">10.43</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.12.5.4">86.09</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.12.5.5">98.78</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.12.5.6">1.37</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.12.5.7">98.87</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.13.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.6.13.6.1">8-StackedHourglass-lab</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.13.6.2">39.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.13.6.3">24.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.13.6.4">32.18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.13.6.5">61.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.13.6.6">10.96</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.13.6.7">45.50</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.14.7">
<td class="ltx_td ltx_align_left" id="S3.T1.6.14.7.1">DLC2018 (ResNet50)-lab</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.14.7.2">34.83</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.14.7.3">28.06</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.14.7.4">51.48</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.14.7.5">66.67</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.14.7.6">3.27</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.14.7.7">87.64</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.15.8">
<td class="ltx_td ltx_align_left" id="S3.T1.6.15.8.1">D-Pose (ResNet50)-lab</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.15.8.2">34.78</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.15.8.3">23.03</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.15.8.4">52.53</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.15.8.5">77.09</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.15.8.6">2.98</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.15.8.7">87.33</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.16.9">
<td class="ltx_td ltx_align_left" id="S3.T1.6.16.9.1">D-Pose (DINOv2-S)-lab</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.16.9.2">19.41</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.16.9.3">11.71</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.16.9.4">80.81</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.16.9.5"><span class="ltx_text ltx_font_bold" id="S3.T1.6.16.9.5.1">98.12</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.16.9.6">1.72</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.16.9.7">98.39</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.17.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.6.17.10.1">D-Pose (DINOv2-B)-lab</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.17.10.2">17.83</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.17.10.3">10.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.17.10.4">84.65</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.6.17.10.5">98.63</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.17.10.6">1.62</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.6.17.10.7">98.55</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.13.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.14.2" style="font-size:90%;">Quantitative results from different pose estimation networks trained on either the 3DDogs-Wild or 3DDogs-Lab datasets (<span class="ltx_text ltx_font_italic" id="S3.T1.14.2.1">-lab</span>) and <span class="ltx_text ltx_font_bold" id="S3.T1.14.2.2">evaluated on the 3DDogs-Wild</span> test set. The top-3 best performing networks are highlighted: <span class="ltx_text ltx_font_bold" id="S3.T1.14.2.3">first</span>, <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.14.2.4">second</span>, <span class="ltx_text ltx_ulem_uuline" id="S3.T1.14.2.5">third</span>, excluding the D-Pose (DINOv2-B) model for fair comparison.</span></figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.6.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T2.6.7.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S3.T2.6.7.1.2">3D</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S3.T2.6.7.1.3">2D</th>
</tr>
<tr class="ltx_tr" id="S3.T2.6.6">
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S3.T2.6.6.7"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.1.1">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.m1.1.1" stretchy="false" xref="S3.T2.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.2.2.2">PA-MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.2.2.2.m1.1"><semantics id="S3.T2.2.2.2.m1.1a"><mo id="S3.T2.2.2.2.m1.1.1" stretchy="false" xref="S3.T2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.m1.1b"><ci id="S3.T2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.2.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.3.3.3">PCK@0.15 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.3.3.3.m1.1"><semantics id="S3.T2.3.3.3.m1.1a"><mo id="S3.T2.3.3.3.m1.1.1" stretchy="false" xref="S3.T2.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.m1.1b"><ci id="S3.T2.3.3.3.m1.1.1.cmml" xref="S3.T2.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.3.3.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.4.4.4">PA-PCK@0.15 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.4.4.4.m1.1"><semantics id="S3.T2.4.4.4.m1.1a"><mo id="S3.T2.4.4.4.m1.1.1" stretchy="false" xref="S3.T2.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.4.m1.1b"><ci id="S3.T2.4.4.4.m1.1.1.cmml" xref="S3.T2.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.4.4.4.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.5.5.5">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.5.5.5.m1.1"><semantics id="S3.T2.5.5.5.m1.1a"><mo id="S3.T2.5.5.5.m1.1.1" stretchy="false" xref="S3.T2.5.5.5.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.5.m1.1b"><ci id="S3.T2.5.5.5.m1.1.1.cmml" xref="S3.T2.5.5.5.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.5.5.5.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.6.6.6">PCK@0.15 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.6.6.6.m1.1"><semantics id="S3.T2.6.6.6.m1.1a"><mo id="S3.T2.6.6.6.m1.1.1" stretchy="false" xref="S3.T2.6.6.6.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.6.m1.1b"><ci id="S3.T2.6.6.6.m1.1.1.cmml" xref="S3.T2.6.6.6.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.6.6.6.m1.1d">â†‘</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.6.8.1">
<td class="ltx_td ltx_align_left" id="S3.T2.6.8.1.1">8-StackedHourglass</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.6.8.1.2">26.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.6.8.1.3">17.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.6.8.1.4">54.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.6.8.1.5">79.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.6.8.1.6">6.05</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T2.6.8.1.7">70.68</td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.9.2">
<td class="ltx_td ltx_align_left" id="S3.T2.6.9.2.1">DLC2018 (ResNet50)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.9.2.2">30.65</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.9.2.3">24.74</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.9.2.4">60.62</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.6.9.2.5">75.72</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.9.2.6">2.73</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.6.9.2.7">89.50</td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.10.3">
<td class="ltx_td ltx_align_left" id="S3.T2.6.10.3.1">D-Pose (ResNet50)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.10.3.2">24.98</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.10.3.3">18.67</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.10.3.4">72.62</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.6.10.3.5">88.30</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.10.3.6">2.50</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.6.10.3.7">89.13</td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.11.4">
<td class="ltx_td ltx_align_left" id="S3.T2.6.11.4.1">D-Pose (DINOv2-S)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.11.4.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.6.11.4.2.1">17.47</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.11.4.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.6.11.4.3.1">11.97</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.11.4.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.6.11.4.4.1">84.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.6.11.4.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.6.11.4.5.1">97.66</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.11.4.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.6.11.4.6.1">1.77</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.6.11.4.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.6.11.4.7.1">97.10</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.12.5">
<td class="ltx_td ltx_align_left" id="S3.T2.6.12.5.1">D-Pose (DINOv2-B)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.12.5.2">16.20</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.12.5.3">10.74</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.12.5.4">85.67</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.6.12.5.5">98.25</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.12.5.6">1.56</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.6.12.5.7">97.95</td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.13.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.6.13.6.1">8-StackedHourglass-lab</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.6.13.6.2">20.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.6.13.6.3">14.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.6.13.6.4">68.08</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.6.13.6.5">85.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.6.13.6.6">3.37</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T2.6.13.6.7">90.98</td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.14.7">
<td class="ltx_td ltx_align_left" id="S3.T2.6.14.7.1">D-Pose (ResNet50)-lab</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.14.7.2"><span class="ltx_text ltx_ulem_uuline" id="S3.T2.6.14.7.2.1">19.15</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.14.7.3"><span class="ltx_text ltx_ulem_uuline" id="S3.T2.6.14.7.3.1">13.41</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.14.7.4"><span class="ltx_text ltx_ulem_uuline" id="S3.T2.6.14.7.4.1">82.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.6.14.7.5"><span class="ltx_text ltx_ulem_uuline" id="S3.T2.6.14.7.5.1">96.00</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.14.7.6"><span class="ltx_text ltx_ulem_uuline" id="S3.T2.6.14.7.6.1">1.97</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.6.14.7.7">90.80</td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.15.8">
<td class="ltx_td ltx_align_left" id="S3.T2.6.15.8.1">DLC2018 (ResNet50)-lab</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.15.8.2">24.80</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.15.8.3">19.43</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.15.8.4">73.28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.6.15.8.5">86.77</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.15.8.6">2.08</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.6.15.8.7"><span class="ltx_text ltx_ulem_uuline" id="S3.T2.6.15.8.7.1">92.14</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.16.9">
<td class="ltx_td ltx_align_left" id="S3.T2.6.16.9.1">D-Pose (DINOv2-S)-lab</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.16.9.2"><span class="ltx_text ltx_font_bold" id="S3.T2.6.16.9.2.1">15.90</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.16.9.3"><span class="ltx_text ltx_font_bold" id="S3.T2.6.16.9.3.1">10.51</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.16.9.4"><span class="ltx_text ltx_font_bold" id="S3.T2.6.16.9.4.1">87.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.6.16.9.5"><span class="ltx_text ltx_font_bold" id="S3.T2.6.16.9.5.1">98.37</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.16.9.6"><span class="ltx_text ltx_font_bold" id="S3.T2.6.16.9.6.1">1.40</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.6.16.9.7"><span class="ltx_text ltx_font_bold" id="S3.T2.6.16.9.7.1">98.83</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.17.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.6.17.10.1">D-Pose (DINOv2-B)-lab</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.6.17.10.2">15.87</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.6.17.10.3">10.57</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.6.17.10.4">86.43</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.6.17.10.5">98.64</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.6.17.10.6">1.30</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T2.6.17.10.7">99.08</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.13.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.14.2" style="font-size:90%;">Quantitative results from different pose estimation networks trained on either the 3DDogs-Wild or 3DDogs-Lab datasets (<span class="ltx_text ltx_font_italic" id="S3.T2.14.2.1">-lab</span>) and <span class="ltx_text ltx_font_bold" id="S3.T2.14.2.2">evaluated on the 3DDogs-Lab</span> test set. The top-3 best performing networks are highlighted: <span class="ltx_text ltx_font_bold" id="S3.T2.14.2.3">first</span>, <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.14.2.4">second</span>, <span class="ltx_text ltx_ulem_uuline" id="S3.T2.14.2.5">third</span>, excluding the D-Pose (DINOv2-B) model for fair comparison.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.7"><span class="ltx_text ltx_font_bold" id="S3.p2.7.1">Pose estimation models</span>. We use 3 different pose estimation networks: an 8-stacked hourglass network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, D-Pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> with varying backbones, and Deeplabcut (DLC2018) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. All networks including a ResNet backbone were fully fine-tuned. However, only the last three layers of the DINOv2 in the D-Pose model underwent fine-tuning. We identified an implementation error in the D-Pose architecture related to the axis permutation. We addressed this issue and document our results using the corrected D-Pose model. Additionally, we modified the resolution of the heatmaps to <math alttext="64\times 64" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mn id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">64</mn><mo id="S3.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><times id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1"></times><cn id="S3.p2.1.m1.1.1.2.cmml" type="integer" xref="S3.p2.1.m1.1.1.2">64</cn><cn id="S3.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.p2.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">64\times 64</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">64 Ã— 64</annotation></semantics></math>. Originally designed for 2D pose estimation, both the stacked hourglass network and DLC2018 were adapted for 3D pose estimation by extending their methods to output <math alttext="K\times 3" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mrow id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">K</mi><mo id="S3.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><times id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1"></times><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">ğ¾</ci><cn id="S3.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.p2.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">K\times 3</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_K Ã— 3</annotation></semantics></math> instead of <math alttext="K" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">italic_K</annotation></semantics></math> heatmaps, where <math alttext="K=29" class="ltx_Math" display="inline" id="S3.p2.4.m4.1"><semantics id="S3.p2.4.m4.1a"><mrow id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">K</mi><mo id="S3.p2.4.m4.1.1.1" xref="S3.p2.4.m4.1.1.1.cmml">=</mo><mn id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">29</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><eq id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1"></eq><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">ğ¾</ci><cn id="S3.p2.4.m4.1.1.3.cmml" type="integer" xref="S3.p2.4.m4.1.1.3">29</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">K=29</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.1d">italic_K = 29</annotation></semantics></math> denotes the number of keypoints. The heatmaps represents the probability of the 3D coordinates in <math alttext="XY" class="ltx_Math" display="inline" id="S3.p2.5.m5.1"><semantics id="S3.p2.5.m5.1a"><mrow id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">X</mi><mo id="S3.p2.5.m5.1.1.1" xref="S3.p2.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><times id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1"></times><ci id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">ğ‘‹</ci><ci id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">ğ‘Œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">XY</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m5.1d">italic_X italic_Y</annotation></semantics></math>, <math alttext="ZY" class="ltx_Math" display="inline" id="S3.p2.6.m6.1"><semantics id="S3.p2.6.m6.1a"><mrow id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mi id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2.cmml">Z</mi><mo id="S3.p2.6.m6.1.1.1" xref="S3.p2.6.m6.1.1.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><times id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1.1"></times><ci id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2">ğ‘</ci><ci id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3">ğ‘Œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">ZY</annotation><annotation encoding="application/x-llamapun" id="S3.p2.6.m6.1d">italic_Z italic_Y</annotation></semantics></math>, and <math alttext="XZ" class="ltx_Math" display="inline" id="S3.p2.7.m7.1"><semantics id="S3.p2.7.m7.1a"><mrow id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mi id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">X</mi><mo id="S3.p2.7.m7.1.1.1" xref="S3.p2.7.m7.1.1.1.cmml">â¢</mo><mi id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">Z</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><times id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1.1"></times><ci id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">ğ‘‹</ci><ci id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">XZ</annotation><annotation encoding="application/x-llamapun" id="S3.p2.7.m7.1d">italic_X italic_Z</annotation></semantics></math> planes. The final pose was extracted following the approach outlined in the work of Shooter e al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. Throughout the experiments, we upheld consistency by applying identical hyperparameters and augmentation strategies (see Sec. S<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.SS1" title="6.1 Pose estimation models â€£ 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">6.1</span></a> in Supp.).</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.p3.1.1">Datasets.</span> The training split, based on breed type, allocated 20 individuals for training, 9 for validation, and 8 for testing, meaning each split contains different individuals. This resulted in 148 sequences for training, 76 for validation, and 62 for testing.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">Challenges</span>. (i) <span class="ltx_text ltx_font_italic" id="S3.p4.1.2">3DDogs-Wild vs 3DDogs-Lab</span>. We evaluate whether the pose estimation models exhibit better performance when trained on 3DDogs-Wild compared to 3DDogs-Lab. The evaluation of this task is performed on sequences of dogs that were not seen whilst training from the 3DDogs-Wild dataset. Models trained on 3DDogs-Lab are designated withâ€œ-labâ€ appended to their names. <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S3.T1" title="In 3 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> shows that models trained with the 3DDogs-Wild dataset demonstrate enhanced performance compared to those trained with 3DDogs-Lab. One might argue this is due to the domain gap. However, when evaluating these models on cross-domain test sets, we observe a narrowing of the performance gap. Models trained on 3DDogs-Wild exhibit more robust generalisation to the 3DDogs-Lab dataset than vice versa (<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S3.T1" title="In 3 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> vs <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S3.T2" title="In 3 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>). For example, when comparing the MPJPE performance of both the 8-StackedHourglass networks evaluated on the 3DDogs-Wild test set in <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S3.T1" title="In 3 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>, the difference in performance is equal to 19.23%. However, when comparing the same models and metric evaluated on 3DDogs-Lab test set in <a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S3.T2" title="In 3 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> the difference in performance is equal to 5.56%. This confirms our initial hypothesis that models exhibit superior generalisation capabilities to in-the-wild data and demonstrate enhanced performance on out-of-domain datasets when trained on 3DDogs-Wild data compared to mocap data (3DDogs-Lab).</p>
</div>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.6.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T3.6.7.1.1"><span class="ltx_text" id="S3.T3.6.7.1.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S3.T3.6.7.1.2"><span class="ltx_text" id="S3.T3.6.7.1.2.1" style="font-size:90%;">3D</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S3.T3.6.7.1.3"><span class="ltx_text" id="S3.T3.6.7.1.3.1" style="font-size:90%;">2D</span></th>
</tr>
<tr class="ltx_tr" id="S3.T3.6.6">
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S3.T3.6.6.7"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.1.1.1">
<span class="ltx_text" id="S3.T3.1.1.1.1" style="font-size:90%;">MPJPE </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.m1.1a"><mo id="S3.T3.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T3.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.2.2.2">
<span class="ltx_text" id="S3.T3.2.2.2.1" style="font-size:90%;">PA-MPJPE </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.2.2.2.m1.1"><semantics id="S3.T3.2.2.2.m1.1a"><mo id="S3.T3.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T3.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.m1.1b"><ci id="S3.T3.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.3.3.3">
<span class="ltx_text" id="S3.T3.3.3.3.1" style="font-size:90%;">PCK@0.15 </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.3.3.3.m1.1"><semantics id="S3.T3.3.3.3.m1.1a"><mo id="S3.T3.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T3.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.m1.1b"><ci id="S3.T3.3.3.3.m1.1.1.cmml" xref="S3.T3.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.3.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.4.4.4">
<span class="ltx_text" id="S3.T3.4.4.4.1" style="font-size:90%;">PA-PCK@0.15 </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.4.4.4.m1.1"><semantics id="S3.T3.4.4.4.m1.1a"><mo id="S3.T3.4.4.4.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T3.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.m1.1b"><ci id="S3.T3.4.4.4.m1.1.1.cmml" xref="S3.T3.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.4.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.5.5">
<span class="ltx_text" id="S3.T3.5.5.5.1" style="font-size:90%;">MPJPE </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.5.5.5.m1.1"><semantics id="S3.T3.5.5.5.m1.1a"><mo id="S3.T3.5.5.5.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T3.5.5.5.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.m1.1b"><ci id="S3.T3.5.5.5.m1.1.1.cmml" xref="S3.T3.5.5.5.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.5.5.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.6.6.6">
<span class="ltx_text" id="S3.T3.6.6.6.1" style="font-size:90%;">PCK@0.15 </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.6.6.6.m1.1"><semantics id="S3.T3.6.6.6.m1.1a"><mo id="S3.T3.6.6.6.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T3.6.6.6.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.6.m1.1b"><ci id="S3.T3.6.6.6.m1.1.1.cmml" xref="S3.T3.6.6.6.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.6.6.m1.1d">â†‘</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.6.8.1">
<td class="ltx_td ltx_align_left" id="S3.T3.6.8.1.1"><span class="ltx_text" id="S3.T3.6.8.1.1.1" style="font-size:90%;">8-StackedHourglass-anim</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.8.1.2"><span class="ltx_text ltx_ulem_uuline" id="S3.T3.6.8.1.2.1" style="font-size:90%;">153.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.8.1.3"><span class="ltx_text" id="S3.T3.6.8.1.3.1" style="font-size:90%;">129.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.8.1.4"><span class="ltx_text" id="S3.T3.6.8.1.4.1" style="font-size:90%;">28.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.6.8.1.5"><span class="ltx_text" id="S3.T3.6.8.1.5.1" style="font-size:90%;">27.29</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.8.1.6"><span class="ltx_text" id="S3.T3.6.8.1.6.1" style="font-size:90%;">20.41</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.6.8.1.7"><span class="ltx_text" id="S3.T3.6.8.1.7.1" style="font-size:90%;">27.12</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.9.2">
<td class="ltx_td ltx_align_left" id="S3.T3.6.9.2.1"><span class="ltx_text" id="S3.T3.6.9.2.1.1" style="font-size:90%;">D-Pose (ResNet50)-anim</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.9.2.2"><span class="ltx_text" id="S3.T3.6.9.2.2.1" style="font-size:90%;">160.60</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.9.2.3"><span class="ltx_text" id="S3.T3.6.9.2.3.1" style="font-size:90%;">123.84</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.9.2.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.6.9.2.4.1" style="font-size:90%;">30.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.6.9.2.5"><span class="ltx_text" id="S3.T3.6.9.2.5.1" style="font-size:90%;">29.88</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.9.2.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.6.9.2.6.1" style="font-size:90%;">10.31</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.6.9.2.7"><span class="ltx_text" id="S3.T3.6.9.2.7.1" style="font-size:90%;">61.00</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.10.3">
<td class="ltx_td ltx_align_left" id="S3.T3.6.10.3.1"><span class="ltx_text" id="S3.T3.6.10.3.1.1" style="font-size:90%;">D-Pose (DINOv2-S)-anim</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.10.3.2"><span class="ltx_text" id="S3.T3.6.10.3.2.1" style="font-size:90%;">169.85</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.10.3.3"><span class="ltx_text ltx_font_bold" id="S3.T3.6.10.3.3.1" style="font-size:90%;">108.79</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.10.3.4"><span class="ltx_text" id="S3.T3.6.10.3.4.1" style="font-size:90%;">27.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.6.10.3.5"><span class="ltx_text ltx_font_bold" id="S3.T3.6.10.3.5.1" style="font-size:90%;">37.31</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.10.3.6"><span class="ltx_text ltx_font_bold" id="S3.T3.6.10.3.6.1" style="font-size:90%;">5.98</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.6.10.3.7"><span class="ltx_text ltx_font_bold" id="S3.T3.6.10.3.7.1" style="font-size:90%;">88.33</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.11.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.6.11.4.1"><span class="ltx_text" id="S3.T3.6.11.4.1.1" style="font-size:90%;">8-StackedHourglass*</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.11.4.2"><span class="ltx_text" id="S3.T3.6.11.4.2.1" style="font-size:90%;">159.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.11.4.3"><span class="ltx_text" id="S3.T3.6.11.4.3.1" style="font-size:90%;">128.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.11.4.4"><span class="ltx_text" id="S3.T3.6.11.4.4.1" style="font-size:90%;">24.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.6.11.4.5"><span class="ltx_text" id="S3.T3.6.11.4.5.1" style="font-size:90%;">27.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.11.4.6"><span class="ltx_text" id="S3.T3.6.11.4.6.1" style="font-size:90%;">26.46</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.6.11.4.7"><span class="ltx_text" id="S3.T3.6.11.4.7.1" style="font-size:90%;">24.46</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.12.5">
<td class="ltx_td ltx_align_left" id="S3.T3.6.12.5.1"><span class="ltx_text" id="S3.T3.6.12.5.1.1" style="font-size:90%;">D-Pose (ResNet50)*</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.12.5.2"><span class="ltx_text" id="S3.T3.6.12.5.2.1" style="font-size:90%;">159.74</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.12.5.3"><span class="ltx_text" id="S3.T3.6.12.5.3.1" style="font-size:90%;">129.71</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.12.5.4"><span class="ltx_text" id="S3.T3.6.12.5.4.1" style="font-size:90%;">25.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.6.12.5.5"><span class="ltx_text" id="S3.T3.6.12.5.5.1" style="font-size:90%;">25.98</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.12.5.6"><span class="ltx_text" id="S3.T3.6.12.5.6.1" style="font-size:90%;">15.04</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.6.12.5.7"><span class="ltx_text" id="S3.T3.6.12.5.7.1" style="font-size:90%;">46.15</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.13.6">
<td class="ltx_td ltx_align_left" id="S3.T3.6.13.6.1"><span class="ltx_text" id="S3.T3.6.13.6.1.1" style="font-size:90%;">D-Pose (DINOv2-S)*</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.13.6.2"><span class="ltx_text ltx_font_bold" id="S3.T3.6.13.6.2.1" style="font-size:90%;">149.54</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.13.6.3"><span class="ltx_text ltx_ulem_uuline" id="S3.T3.6.13.6.3.1" style="font-size:90%;">119.52</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.13.6.4"><span class="ltx_text ltx_font_bold" id="S3.T3.6.13.6.4.1" style="font-size:90%;">30.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.6.13.6.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.6.13.6.5.1" style="font-size:90%;">33.15</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.13.6.6"><span class="ltx_text ltx_ulem_uuline" id="S3.T3.6.13.6.6.1" style="font-size:90%;">10.46</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.6.13.6.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.6.13.6.7.1" style="font-size:90%;">68.39</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.14.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.6.14.7.1"><span class="ltx_text" id="S3.T3.6.14.7.1.1" style="font-size:90%;">8-StackedHourglass-lab*</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.14.7.2"><span class="ltx_text" id="S3.T3.6.14.7.2.1" style="font-size:90%;">163.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.14.7.3"><span class="ltx_text" id="S3.T3.6.14.7.3.1" style="font-size:90%;">128.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.14.7.4"><span class="ltx_text" id="S3.T3.6.14.7.4.1" style="font-size:90%;">19.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.6.14.7.5"><span class="ltx_text" id="S3.T3.6.14.7.5.1" style="font-size:90%;">27.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.14.7.6"><span class="ltx_text" id="S3.T3.6.14.7.6.1" style="font-size:90%;">30.87</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.6.14.7.7"><span class="ltx_text" id="S3.T3.6.14.7.7.1" style="font-size:90%;">12.58</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.15.8">
<td class="ltx_td ltx_align_left" id="S3.T3.6.15.8.1"><span class="ltx_text" id="S3.T3.6.15.8.1.1" style="font-size:90%;">D-Pose (ResNet50)-lab*</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.15.8.2"><span class="ltx_text" id="S3.T3.6.15.8.2.1" style="font-size:90%;">161.52</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.15.8.3"><span class="ltx_text" id="S3.T3.6.15.8.3.1" style="font-size:90%;">129.76</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.15.8.4"><span class="ltx_text" id="S3.T3.6.15.8.4.1" style="font-size:90%;">23.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.6.15.8.5"><span class="ltx_text" id="S3.T3.6.15.8.5.1" style="font-size:90%;">26.52</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.15.8.6"><span class="ltx_text" id="S3.T3.6.15.8.6.1" style="font-size:90%;">17.96</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.6.15.8.7"><span class="ltx_text" id="S3.T3.6.15.8.7.1" style="font-size:90%;">36.69</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.16.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T3.6.16.9.1"><span class="ltx_text" id="S3.T3.6.16.9.1.1" style="font-size:90%;">D-Pose (DINOv2-S)-lab*</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.16.9.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.6.16.9.2.1" style="font-size:90%;">151.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.16.9.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.6.16.9.3.1" style="font-size:90%;">118.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.16.9.4"><span class="ltx_text ltx_ulem_uuline" id="S3.T3.6.16.9.4.1" style="font-size:90%;">29.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T3.6.16.9.5"><span class="ltx_text ltx_ulem_uuline" id="S3.T3.6.16.9.5.1" style="font-size:90%;">33.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.16.9.6"><span class="ltx_text" id="S3.T3.6.16.9.6.1" style="font-size:90%;">11.32</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T3.6.16.9.7"><span class="ltx_text ltx_ulem_uuline" id="S3.T3.6.16.9.7.1" style="font-size:90%;">64.13</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Quantitative results <span class="ltx_text ltx_font_bold" id="S3.T3.25.1">evaluated on the Animals3</span>D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> test set using top performing models trained on Animals3D (<span class="ltx_text ltx_font_italic" id="S3.T3.26.2">-anim</span>), 3DDogs-Wild and 3DDogs-Lab datasets (<span class="ltx_text ltx_font_italic" id="S3.T3.27.3">-lab</span>) for both 3D and 2D. D-Pose (DINOv2-B) is excluded for fair comparison. The top-3 best performing networks are highlighted: <span class="ltx_text ltx_font_bold" id="S3.T3.28.4">first</span>, <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.29.5">second</span>, <span class="ltx_text ltx_ulem_uuline" id="S3.T3.30.6">third</span>. The asterisk (*) denotes that the model was trained on a subset of the 3DDogs dataset, proportionate to the Animals3D dataset.</figcaption>
</figure>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">To compare the models fairly, the D-Pose (DINOv2-B) model was excluded due to its high parameter count (see Tab. S<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.T4" title="Table 4 â€£ 6.2 3DDogs-Wild vs 3DDogs-Lab â€£ 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">4</span></a> in Supp.). After assessing the models with the 3DDogs-Lab test set, D-Pose (DINOv2-S) with lower parameter count emerged as the top performer. In terms of 3D pose evaluation, both the 8-StackedHourglass and D-Pose (ResNet50) models are competing for the second position, while in the 2D pose assessment, the DLC2018 (ResNet50) and D-Pose (ResNet50) models are competing for the next-best position. Overall, we argue that D-Pose (ResNet50) is the second best performing model due to its consistent performance across both 3D and 2D evaluations, as well as its strong performance on the cross-domain test set, where the domain gap is narrower. When comparing the â€œrawâ€ and PA-metrics, D-Pose (DINOv2) models demonstrate strong performance even without requiring alignment. These results, underlines the significance of selecting and leveraging effective pre-trained backbones to attain high performance in animal pose estimation tasks. Furthermore, when comparing the D-Pose models with varying sizes of DINOv2 backbones, as expected, there is an improvement in performance as the number of parameters increases.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.2">(ii) <span class="ltx_text ltx_font_italic" id="S3.p6.2.1">Generalisation to other species</span>. We evaluate whether the top-3 best performing pose estimation models from the previous challenge can generalise to other animal species using the Animal3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> dataset. We first perform a baseline with the models trained on the Animals3D dataset and then evaluate the models that were trained either on the 3DDogs-Wild or the 3DDogs-Lab dataset. To train the models with the Animals3D dataset, the dataset was split with an 80:20 training/testing ratio. Given that the Animals3D dataset comprises <math alttext="\sim" class="ltx_Math" display="inline" id="S3.p6.1.m1.1"><semantics id="S3.p6.1.m1.1a"><mo id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><csymbol cd="latexml" id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.1d">âˆ¼</annotation></semantics></math>3K data points, which is smaller in comparison to the <math alttext="\sim" class="ltx_Math" display="inline" id="S3.p6.2.m2.1"><semantics id="S3.p6.2.m2.1a"><mo id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><csymbol cd="latexml" id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.p6.2.m2.1d">âˆ¼</annotation></semantics></math>12K data points in the 3DDogs dataset, we present results from models trained on a subset of the 3DDogs dataset equal in size to the Animals3D dataset to have a fair comparison.</p>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1">The D-Pose (DINOv2-S)-anim model trained on the Animals3D dataset demonstrates superior performance in 2D, outperforming other models trained on the same dataset. While other models trained on Animals3D exhibit stronger performance in raw metrics for 3D, D-Pose (DINOv2-S)â€“anim outperforms them after applying Procrustes alignment. In terms of overall performance, D-Pose (DINOv2-S) trained on the 3DDogs-Wild dataset stands out as the best model, particularly when assessing raw metrics against all other models. The reason why D-Pose (DINOv2-S) trained on the 3DDogs-Wild dataset does not achieve comparable 2D performance to D-Pose (DINOv2-S)-anim trained on the Animals3D dataset is due to the variation in viewpoints present in the Animals3D dataset, whereas the 3DDogs dataset features samples with consistent viewpoints. Additionally, it can be concluded that when looking at the raw metrics (prior to Procrustes alignment), all the models trained on the 3DDogs-Wild dataset generalise better to out-of-domain data than models trained on mocap data (3DDogs-lab) for both 3D and 2D.
</p>
</div>
<div class="ltx_para" id="S3.p8">
<p class="ltx_p" id="S3.p8.1">The following section discusses the models trained on the 3DDogs-Wild dataset. Both the 8-StackedHourglass and D-Pose (ResNet50) models fail to generalise to cross-domain data. The D-Pose model exhibits superior performance due to the generalisation capabilities of DINOv2, however, its performance on the Animals3D test set falls short for 3D and remains acceptable for 2D (<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S3.T3" title="In 3 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>). However, qualitative evaluation shows that D-Pose predicts plausible 3D poses (<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S3.F3" title="In 3 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>). This is due to 3 reasons:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The differing keypoint semantics between the 3DDogs-Lab and Animals3D datasets (see Fig. S<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.F5" title="Figure 5 â€£ 6.3 Generalisation to other species â€£ 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">5</span></a> in Supp.)</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">The Animals3D dataset features images of animals facing the camera which poses a challenge, as the model was exclusively trained on side-view dog images</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">The Animals3D dataset was produced by fitting the animal parametric model, SMAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, to monocular images using only 2D signals. Despite this process, there remains a potential for depth ambiguity, indicating that the resulting 3D representations may not fully capture the true ground truth</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We present a new benchmark and two datasets for 3D dog pose estimation from monocular views, designed to facilitate fair comparisons of pose estimation methods across various datasets. 3DDogs-Wild offers both in-the-wild and 3D gold standard ground truth, sourced from the 3DDogs-Lab dataset. We offer valuable insights into the capabilities and limitations of existing pose estimation techniques, by addressing key questions regarding model performance with in-the-wild data compared to motion capture data, as well as generalisation across different animal species. Our results highlight the superiority of D-Pose over alternative models, attributed to its backbone and we show that models trained with 3DDogs-Wild have better generalisation and performance compared to those trained on 3DDogs-Lab due to the increased diversity in the dataset.</p>
</div>
<section class="ltx_subsection" id="S4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Acknowledgement</h3>
<div class="ltx_para" id="S4.SSx1.p1">
<p class="ltx_p" id="S4.SSx1.p1.1">This research was partially supported by EPSRC Audio-Visual Media Platform Grant EP/P022529/1 and by the Leverhulme Trust Early Career Fellowship scheme.


<span class="ltx_text" id="S4.SSx1.p1.1.1" style="font-size:90%;"></span></p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">An etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Liang An, Jilong Ren, Tao Yu, Tang Hai, Yichang Jia, and Yebin Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Three-dimensional surface motion capture of multiple freely moving pigs using MAMMAL.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.9.1" style="font-size:90%;">Nat Commun</em><span class="ltx_text" id="bib.bib1.10.2" style="font-size:90%;">, 14(1):7727, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Bala etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
PraneetÂ C. Bala, BenjaminÂ R. Eisenreich, Seng BumÂ Michael Yoo, BenjaminÂ Y. Hayden, HyunÂ Soo Park, and Jan Zimmermann.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Automated markerless pose estimation in freely moving macaques with openmonkeystudio.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.9.1" style="font-size:90%;">Nature Communications</em><span class="ltx_text" id="bib.bib2.10.2" style="font-size:90%;">, 11(1):4560, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Biggs etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Benjamin Biggs, Ollie Boyne, James Charles, Andrew Fitzgibbon, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Who left the dogs out: 3D animal reconstruction with expectation maximization in the loop.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib3.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Deane etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Jake Deane, SinÃ©ad Kearney, KwangÂ In Kim, and Darren Cosker.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">Rgbt-dog: A parametric model and pose prior for canine body analysis data creation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em><span class="ltx_text" id="bib.bib4.11.3" style="font-size:90%;">, pages 6056â€“6066, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Hu etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Shaoxiang Hu, Zhiwu Liao, Rong Hou, and Peng Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Giant panda video image sequence and application in 3d reconstruction.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.9.1" style="font-size:90%;">Frontiers in Physics</em><span class="ltx_text" id="bib.bib5.10.2" style="font-size:90%;">, 10, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.4.4.1" style="font-size:90%;">Jiang and Ostadabbas [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.6.1" style="font-size:90%;">
Le Jiang and Sarah Ostadabbas.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">Spac-net: Synthetic pose-aware animal controlnet for enhanced pose estimation, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Jocher etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Glenn Jocher, Ayush Chaurasia, and Jing Qiu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Ultralytics YOLO, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Joska etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Daniel Joska, Liam Clark, Naoya Muramatsu, Ricardo Jericevich, Fred Nicolls, Alexander Mathis, MackenzieÂ W. Mathis, and Amir Patel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Acinoset: A 3d pose estimation dataset and baseline models for cheetahs in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">2021 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, page 13901â€“13908. IEEE Press, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Kearney etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Sinead Kearney, Wenbin Li, Martin Parsons, KwangÂ In Kim, and Darren Cosker.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">Rgbd-dog: Predicting canine pose from rgbd sensors.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib9.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Kirillov etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, AlexanderÂ C. Berg, Wan-Yen Lo, Piotr DollÃ¡r, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.9.1" style="font-size:90%;">arXiv:2304.02643</em><span class="ltx_text" id="bib.bib10.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Liu etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Dan Liu, Jin Hou, Shaoli Huang, Jing Liu, Yuxin He, Bochuan Zheng, Jifeng Ning, and Jingdong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Lote-animal: A long time-span dataset for endangered animal behavior understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib11.11.3" style="font-size:90%;">, pages 20064â€“20075, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">Mathis etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Alexander Mathis, Pranav Mamidanna, KevinÂ M. Cury, Taiga Abe, VenkateshÂ N. Murthy, MackenzieÂ Weygandt Mathis, and Matthias Bethge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">Deeplabcut: markerless pose estimation of user-defined body parts with deep learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.9.1" style="font-size:90%;">Nature Neuroscience</em><span class="ltx_text" id="bib.bib12.10.2" style="font-size:90%;">, 21(9):1281â€“1289, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Mu etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Jiteng Mu, Weichao Qiu, GregoryÂ D. Hager, and AlanÂ L. Yuille.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">Learning from synthetic animals.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.2" style="font-size:90%;">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib13.11.3" style="font-size:90%;">, pages 12383â€“12392, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Newell etÂ al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Alejandro Newell, Kaiyu Yang, and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Stacked hourglass networks for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.10.2" style="font-size:90%;">Computer Vision â€“ ECCV 2016</em><span class="ltx_text" id="bib.bib14.11.3" style="font-size:90%;">, pages 483â€“499, Cham, 2016. Springer International Publishing.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Ng etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
XunÂ Long Ng, KianÂ Eng Ong, Qichen Zheng, Yun Ni, SiÂ Yong Yeo, and Jun Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Animal kingdom: A large and diverse dataset for animal behavior understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">, pages 19001â€“19012, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Rombach etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib16.11.3" style="font-size:90%;">, pages 10684â€“10695, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Rueegg etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Nadine Rueegg, Silvia Zuffi, Konrad Schindler, and MichaelÂ J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">BARC: Learning to regress 3D dog shape from images by exploiting breed information.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.10.2" style="font-size:90%;">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib17.11.3" style="font-size:90%;">, pages 3876â€“3884, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">RÃ¼egg etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Nadine RÃ¼egg, Shashank Tripathi, Konrad Schindler, MichaelÂ J. Black, and Silvia Zuffi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">BITE: Beyond priors for improved three-D dog pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.10.2" style="font-size:90%;">IEEE/CVF Conf.Â on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib18.11.3" style="font-size:90%;">, pages 8867â€“8876, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Shooter etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Moira Shooter, Charles Malleson, and Adrian Hilton.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">Sydog-video: A synthetic dog video dataset for temporal pose estimation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.9.1" style="font-size:90%;">International Journal of Computer Vision</em><span class="ltx_text" id="bib.bib19.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Shooter etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Moira Shooter, Charles Malleson, and Adrian Hilton.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Digidogs: Single-view 3d pose estimation of dogs using synthetic training data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops</em><span class="ltx_text" id="bib.bib20.11.3" style="font-size:90%;">, pages 80â€“89, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Xu etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang, Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen Cui, Alan Yuille, and Adam Kortylewski.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">Animal3d: A comprehensive dataset of 3d animal pose and shape.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib21.11.3" style="font-size:90%;">, pages 9099â€“9109, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Yang etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Yuxiang Yang, Junjie Yang, Yufei Xu, Jing Zhang, Long Lan, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Apt-36k: A large-scale benchmark for animal pose estimation and tracking.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib22.10.2" style="font-size:90%;">, 35:17301â€“17313, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Yao etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Jingfeng Yao, Xinggang Wang, Shusheng Yang, and Baoyuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Vitmatte: Boosting image matting with pre-trained plain vision transformers.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.9.1" style="font-size:90%;">Information Fusion</em><span class="ltx_text" id="bib.bib23.10.2" style="font-size:90%;">, 103:102091, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Yu etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Hang Yu, Yufei Xu, Jing Zhang, Wei Zhao, Ziyu Guan, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Ap-10k: A benchmark for animal pose estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.10.2" style="font-size:90%;">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em><span class="ltx_text" id="bib.bib24.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Yu etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">Inpaint anything: Segment anything meets image inpainting.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.06790</em><span class="ltx_text" id="bib.bib25.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Zhou etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Shangchen Zhou, Chongyi Li, KelvinÂ C.K Chan, and ChenÂ Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">ProPainter: Improving propagation and transformer for video inpainting.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib26.10.2" style="font-size:90%;">Proceedings of IEEE International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib26.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Zuffi etÂ al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Silvia Zuffi, Angjoo Kanazawa, DavidÂ W. Jacobs, and MichaelÂ J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">3d menagerie: Modeling the 3d shape and pose of animals.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib27.10.2" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib27.11.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Zuffi etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and MichaelÂ J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">Three-d safari: Learning to estimate zebra pose, shape, and texture from images â€in the wildâ€.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.10.2" style="font-size:90%;">International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib28.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\thetitle</span>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.2"><span class="ltx_text" id="p1.2.1" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"/></span></p>
</div>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">5 </span>3DDogs-Lab</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Generation of 3DDogs-Wild</h3>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="104" id="S5.F4.g1" src="extracted/5681311/bg_artefact.png" width="598"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.4.1.1" style="font-size:63%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.5.2" style="font-size:63%;">Background generation artifacts such as extra limb hallucinations and inconsistencies across frames.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments and Results</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Pose estimation models</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1"><span class="ltx_text" id="S6.SS1.p1.1.1" style="font-size:144%;">The models were optimised using an Adam optimizer with a batch size of 8. Initially, the learning rate was established at 3e-5, which was subsequently reduced by a factor of 1/30 after 7 epochs. Although the maximum number of epochs was set to 300, we implemented early stopping for enhanced training effectiveness, halting when the validation loss ceased to decrease for 5 consecutive epochs. During training, data augmentation was performed by randomly applying color jitter, gaussian blur, and grayscale transformations with a probability of 0.3 for each operation.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>3DDogs-Wild vs 3DDogs-Lab</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><span class="ltx_text" id="S6.SS2.p1.1.1" style="font-size:144%;">In </span><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S3" style="font-size:144%;" title="3 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S6.SS2.p1.1.2" style="font-size:144%;"> we compared various pose estimation models, however, often we exclude the D-Pose (DINOv2-B) model due to high parameter count, as we expected it to yield superior performance compared to models with fewer parameters. </span><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.T4" style="font-size:144%;" title="In 6.2 3DDogs-Wild vs 3DDogs-Lab â€£ 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S6.SS2.p1.1.3" style="font-size:144%;"> shows the pose estimation models including the number of parameters.</span></p>
</div>
<figure class="ltx_table" id="S6.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T4.2.1.1.1"><span class="ltx_text" id="S6.T4.2.1.1.1.1" style="font-size:144%;">Method</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.2"><span class="ltx_text" id="S6.T4.2.1.1.2.1" style="font-size:144%;">parameters (M)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.2.2.1.1"><span class="ltx_text" id="S6.T4.2.2.1.1.1" style="font-size:144%;">8-StackedHourglass</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T4.2.2.1.2"><span class="ltx_text" id="S6.T4.2.2.1.2.1" style="font-size:144%;">25.9</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.2.3.2.1"><span class="ltx_text" id="S6.T4.2.3.2.1.1" style="font-size:144%;">DLC2018 (ResNet50)</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T4.2.3.2.2"><span class="ltx_text" id="S6.T4.2.3.2.2.1" style="font-size:144%;">26.7</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.2.4.3.1"><span class="ltx_text" id="S6.T4.2.4.3.1.1" style="font-size:144%;">D-Pose (ResNet50)</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T4.2.4.3.2"><span class="ltx_text" id="S6.T4.2.4.3.2.1" style="font-size:144%;">46.7</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.2.5.4.1"><span class="ltx_text" id="S6.T4.2.5.4.1.1" style="font-size:144%;">D-Pose (DINOv2-S)</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T4.2.5.4.2"><span class="ltx_text" id="S6.T4.2.5.4.2.1" style="font-size:144%;">22.9</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T4.2.6.5.1"><span class="ltx_text" id="S6.T4.2.6.5.1.1" style="font-size:144%;">D-Pose (DINOv2-B)</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S6.T4.2.6.5.2"><span class="ltx_text" id="S6.T4.2.6.5.2.1" style="font-size:144%;">88.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T4.5.1.1" style="font-size:63%;">Table 4</span>: </span><span class="ltx_text" id="S6.T4.6.2" style="font-size:63%;">Number of parameters for each pose estimation model.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text" id="S6.SS2.p2.1.1" style="font-size:144%;">Additionally, we present the mean inference time using 170 samples (</span><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.T5" style="font-size:144%;" title="In 6.2 3DDogs-Wild vs 3DDogs-Lab â€£ 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text" id="S6.SS2.p2.1.2" style="font-size:144%;">). The inference time was measured on hardware with a single GPU, an NVIDIA GeForce RTX 3060 with 8GB of memory. While the D-Pose (DINOv2-S) model is marginally slower than the D-Pose (ResNet50), its significantly improved performance justifies this trade-off. For a well-balanced compromise between accuracy and speed, the D-Pose (ResNet50) or D-Pose (DINOv2-S) models should be considered. However, if utmost accuracy is the priority, one should be mindful of the longer inference time associated with D-Pose (DINOv2-B).</span></p>
</div>
<figure class="ltx_table" id="S6.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T5.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T5.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T5.2.1.1.1"><span class="ltx_text" id="S6.T5.2.1.1.1.1" style="font-size:144%;">Method</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.2.1.1.2"><span class="ltx_text" id="S6.T5.2.1.1.2.1" style="font-size:144%;">Mean duration (ms)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T5.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T5.2.2.1.1"><span class="ltx_text" id="S6.T5.2.2.1.1.1" style="font-size:144%;">8-StackedHourglass</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T5.2.2.1.2"><span class="ltx_text" id="S6.T5.2.2.1.2.1" style="font-size:144%;">66.05</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.2.3.2.1"><span class="ltx_text" id="S6.T5.2.3.2.1.1" style="font-size:144%;">DLC2018 (ResNet50)</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T5.2.3.2.2"><span class="ltx_text" id="S6.T5.2.3.2.2.1" style="font-size:144%;">17.39</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.2.4.3.1"><span class="ltx_text" id="S6.T5.2.4.3.1.1" style="font-size:144%;">D-Pose (ResNet50)</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T5.2.4.3.2"><span class="ltx_text" id="S6.T5.2.4.3.2.1" style="font-size:144%;">24.28</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.2.5.4.1"><span class="ltx_text" id="S6.T5.2.5.4.1.1" style="font-size:144%;">D-Pose (DINOv2-S)</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T5.2.5.4.2"><span class="ltx_text" id="S6.T5.2.5.4.2.1" style="font-size:144%;">27.91</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T5.2.6.5.1"><span class="ltx_text" id="S6.T5.2.6.5.1.1" style="font-size:144%;">D-Pose (DINOv2-B)</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S6.T5.2.6.5.2"><span class="ltx_text" id="S6.T5.2.6.5.2.1" style="font-size:144%;">59.92</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T5.5.1.1" style="font-size:63%;">Table 5</span>: </span><span class="ltx_text" id="S6.T5.6.2" style="font-size:63%;">Mean inference time for each model using 170 samples.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><span class="ltx_text" id="S6.SS2.p3.1.1" style="font-size:144%;">We show additional qualitative results in </span><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.F6" style="font-size:144%;" title="In 6.3 Generalisation to other species â€£ 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text" id="S6.SS2.p3.1.2" style="font-size:144%;"> from samples of the 3DDogs-Wild test set. Technical challenges during capturing the dataset, such as subjects shaking off the optical markers, lead to incomplete 3D poses in our results.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Generalisation to other species</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1"><span class="ltx_text" id="S6.SS3.p1.1.1" style="font-size:144%;">In </span><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.F5" style="font-size:144%;" title="In 6.3 Generalisation to other species â€£ 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text" id="S6.SS3.p1.1.2" style="font-size:144%;"> we show the definitions of the keypoints in the 3DDogs-Lab dataset overlayed with the Animals3Dâ€™s keypoint definitions that are semantically different from the 3DDogs-Lab dataset.
To demonstrate the differences between keypoint semantics we visualise it using red arrows.</span></p>
</div>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="360" id="S6.F5.g1" src="extracted/5681311/keypoint_differences.png" width="598"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F5.4.1.1" style="font-size:63%;">Figure 5</span>: </span><span class="ltx_text" id="S6.F5.5.2" style="font-size:63%;">Demonstrating the keypoint differences between the 3DDogs and Animals3D datasets. Additionally, showcasing samples of D-Pose (DINOv2-S) modelâ€™s predictions and ground truth of the Animals3D test set. </span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1"><span class="ltx_text" id="S6.SS3.p2.1.1" style="font-size:144%;">In </span><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.F7" style="font-size:144%;" title="In 6.3 Generalisation to other species â€£ 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">7</span></a><span class="ltx_text" id="S6.SS3.p2.1.2" style="font-size:144%;"> we show qualitative results for samples of the Animals3D test set comparing the top-3 performing networks e.g. D-Pose (DINOv2-S), D-Pose (ResNet50) and 8-StackedHourglass. We provide further qualitative results from the 2-top performing networks in </span><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S6.F8" style="font-size:144%;" title="In 6.3 Generalisation to other species â€£ 6 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" id="S6.SS3.p2.1.3" style="font-size:144%;">, showcasing random images or videos sourced from the internet. For the videos, please refer to the supplementary material folder. We estimate the pose on a frame-basis. We omitted results from the stacked hourglass network, as indicated in </span><a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#S3.T3" style="font-size:144%;" title="In 3 Experiments and Results â€£ Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S6.SS3.p2.1.4" style="font-size:144%;">, due to its inability to generalise to cross-domain data. In the visualisations, the right side of the skeleton is coloured in red, while the left side is coloured in blue. Upon global evaluation both D-Pose (DINOv2-S) and D-Pose (ResNet50) demonstrate the ability to generate plausible 3D poses across various animal species. However, a clear distinction emerges when the subject appearance differs significantly from that of a dog, as seen with the samples of horses. In such cases, while D-Pose (DINOv2-S) maintains accuracy, D-Pose (ResNet50) fails to yield convincing results. This discrepancy is further highlighted in the supplementary videos accompanying the paper.</span></p>
</div>
<figure class="ltx_figure" id="S6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S6.F6.g1" src="extracted/5681311/QualitativeResultsITW.png" width="598"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F6.4.1.1" style="font-size:63%;">Figure 6</span>: </span><span class="ltx_text" id="S6.F6.5.2" style="font-size:63%;">Further qualitative results on samples of the 3DDogs-Wild test set from different pose estimation models.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="201" id="S6.F7.g1" src="extracted/5681311/Animals3D_benchmark.png" width="598"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F7.7.1.1" style="font-size:63%;">Figure 7</span>: </span><span class="ltx_text" id="S6.F7.8.2" style="font-size:63%;">Qualitative results evaluated on samples of the Animals3D dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14412v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> from the top-performing networks trained solely on the 3DDogs-Wild dataset.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="326" id="S6.F8.g1" src="extracted/5681311/Animals3D_qual.png" width="598"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F8.4.1.1" style="font-size:63%;">Figure 8</span>: </span><span class="ltx_text" id="S6.F8.5.2" style="font-size:63%;">Qualitative results on random internet images from the 2-top best performing models.</span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jun 20 15:29:24 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
