<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting</title>
<!--Generated on Thu Mar 14 14:28:34 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Multiperson 3D Pose Estimation,  Real Time System,  Omnidirectional Camera,  Radar Sensing
" lang="en" name="keywords"/>
<base href="/html/2403.09437v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S1" title="I Introduction â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S2" title="II Methodology â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S2.SS1" title="II-A Camera and Radar Calibration â€£ II Methodology â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Camera and Radar Calibration</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S2.SS2" title="II-B Data Fetching, 2D Keypoint Localisation and Person Matching â€£ II Methodology â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Data Fetching, 2D Keypoint Localisation and Person Matching</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S2.SS3" title="II-C Unsupervised 2D-3D Lifting and 3D placement â€£ II Methodology â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Unsupervised 2D-3D Lifting and 3D placement</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S3" title="III Results â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S3.SS1" title="III-A Results of matching people in the image and radar â€£ III Results â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Results of matching people in the image and radar</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S3.SS2" title="III-B 2D-3D Lifting and Occlusion Handling Results â€£ III Results â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">2D-3D Lifting and Occlusion Handling Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S3.SS3" title="III-C Radar Localisation Results â€£ III Results â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Radar Localisation Results</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S4" title="IV Conclusion â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY-NC-SA 4.0</div><div id="watermark-tr">arXiv:2403.09437v1 [cs.CV] 14 Mar 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup class="ltx_sup" id="id2.1.id1">Â§</sup> Pawel Knap
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.2.id1">Vision Learning and Control, (ECS)</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id4.3.id2">University of Southampton
<br class="ltx_break"/></span>Southampton, UK 
<br class="ltx_break"/>pmk1g20@soton.ac.uk
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup class="ltx_sup" id="id5.1.id1">Â§</sup> Peter Hardy
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id6.2.id1">Vision Learning and Control, (ECS)</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id7.3.id2">University of Southampton
<br class="ltx_break"/></span>Southampton, UK 
<br class="ltx_break"/>p.t.d.hardy@soton.ac.uk
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">2<sup class="ltx_sup" id="id8.1.id1">nd</sup> Alberto Tamajo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id9.2.id1">Vision Learning and Control, (ECS)</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id10.3.id2">University of Southampton
<br class="ltx_break"/></span>Southampton, UK 
<br class="ltx_break"/>at2n19@soton.ac.uk
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">3<sup class="ltx_sup" id="id11.1.id1">rd</sup> Hwasup Lim
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id12.2.id1">Korean Institute of Science and Technology
<br class="ltx_break"/></span>Seoul, South Korea 
<br class="ltx_break"/>hslim@kist.re.kr
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">4<sup class="ltx_sup" id="id13.1.id1">th</sup> Hansung Kim
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id14.2.id1">Vision Learning and Control, (ECS)</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id15.3.id2">University of Southampton
<br class="ltx_break"/></span>Southampton, UK 
<br class="ltx_break"/>h.kim@soton.ac.uk
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">Current human pose estimation systems focus on retrieving an accurate 3D global estimate of a single person. Therefore, this paper presents one of the first 3D multi-person human pose estimation systems that is able to work in real-time and is also able to handle basic forms of occlusion. First, we adjust an off-the-shelf 2D detector and an unsupervised 2D-3D lifting model for use with a 360<math alttext="{}^{\circ}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><compose id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{\circ}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT âˆ˜ end_FLOATSUPERSCRIPT</annotation></semantics></math> panoramic camera and mmWave radar sensors. We then introduce several contributions, including camera and radar calibrations, and the improved matching of people within the image and radar space. The system addresses both the depth and scale ambiguity problems by employing a lightweight 2D-3D pose lifting algorithm that is able to work in real-time while exhibiting accurate performance in both indoor and outdoor environments which offers both an affordable and scalable solution. Notably, our systemâ€™s time complexity remains nearly constant irrespective of the number of detected individuals, achieving a frame rate of approximately 7-8 fps on a laptop with a commercial-grade GPU.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Multiperson 3D Pose Estimation, Real Time System, Omnidirectional Camera, Radar Sensing

</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">Â§</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">Â§</sup><span class="ltx_note_type">footnotetext: </span>Both authors contributed equally to this research. 
<br class="ltx_break"/>This work was supported by the Korea Institute of Science and Technology (KIST) Institutional Program (Project No. 2E32303)</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="260" id="S1.F1.g1" src="extracted/5470926/setup.jpg" width="204"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Our experimental setup consists of a laptop with RTX 3060, the Ricoh Theta V omindirectional camera and three TI AWR1843BOOST mmWava radars.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">3D human pose estimation (HPE) from a single camera is an important task, with various applications such as security, 3D animation and physical therapy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib3" title="">3</a>]</cite>.Nevertheless, obtaining precise 3D global coordinates from a single viewpoint remains challenging due to inherent perspective ambiguities. Previous strategies have often resorted to merging RGB cameras with laser or infrared (IR) depth sensors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib5" title="">5</a>]</cite>. While effective, these methods are marred by cost issues associated with laser-based sensors and sub-optimal performance of IR-based sensors in outdoor environments due to sunlight, thus limiting their widespread adoption. To address this, radar-based methods have emerged as a cost-effective solution for both indoor and outdoor scenarios. To this end we present one of a new approach that utilises mmWave radars and omnidirectional cameras, allowing us to accurately reconstruct multiple people within a <math alttext="360^{\circ}" class="ltx_Math" display="inline" id="S1.p1.1.m1.1"><semantics id="S1.p1.1.m1.1a"><msup id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml"><mn id="S1.p1.1.m1.1.1.2" xref="S1.p1.1.m1.1.1.2.cmml">360</mn><mo id="S1.p1.1.m1.1.1.3" xref="S1.p1.1.m1.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><apply id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p1.1.m1.1.1.1.cmml" xref="S1.p1.1.m1.1.1">superscript</csymbol><cn id="S1.p1.1.m1.1.1.2.cmml" type="integer" xref="S1.p1.1.m1.1.1.2">360</cn><compose id="S1.p1.1.m1.1.1.3.cmml" xref="S1.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">360^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S1.p1.1.m1.1d">360 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math> scene. To detect and lift the pose to 3D its own local coordinate space we utilise two off-the-shelf methods. The first is the 2D detector OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib6" title="">6</a>]</cite> which we use to obtain the 2D keypoint locations of people within the image space. The second is our preliminary 2D-3D lifting network LInKs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib7" title="">7</a>]</cite>, an unsupervised 2D-3D lifting approach that is also able to handle basic forms of occlusion. This is especially important due to occlusion being highly prominent in omnidirectional video, be that self-occlusion or occlusion from an object. We further refine our methodology based on prior work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib8" title="">8</a>]</cite>, introducing an alternative approach to match individuals in both the image and radar domains. This enhancement not only elevates matching accuracy by 4.63% but also reduces the absolute error in placing poses within the 3D coordinate system. Consequently, this paper represents a significant evolution from our previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib8" title="">8</a>]</cite>, encompassing essential advancements, including radar and camera calibration and an improved matching algorithm. These updates translate into substantial improvements in precision, accuracy, and our ability to effectively handle occlusions. The experimental setup of our approach is visually depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S1.F1" title="Figure 1 â€£ I Introduction â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The core of our method resolved around transforming the 2D keypoints detected into the image space into 3D keypoints that are within our global coordinate system. An overview of each stage can be seen in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S2.F2" title="Figure 2 â€£ II Methodology â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="186" id="S2.F2.g1" src="extracted/5470926/system_pipeline_4.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of our approach. We use the video from an omnidirectional camera to obtain 2D body keypoints in the image space. Simultaneously we use 3 radar sensors to localise each person in our global 3D coordinate system. We then match these detected 2D poses to our radarâ€™s depth estimate. Next, these 2D poses are lifted into 3D and to finalise we transform their predicted 3D coordinates to be within our global coordinate system.</figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Camera and Radar Calibration</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">To accurately localise the people within our global coordinate system we calibrated both the camera and radars. The camera was calibrated using the method introduced in Zang <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib9" title="">9</a>]</cite>. To calibrate our radars, for each of our radars <math alttext="(x,z)" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.2"><semantics id="S2.SS1.p1.1.m1.2a"><mrow id="S2.SS1.p1.1.m1.2.3.2" xref="S2.SS1.p1.1.m1.2.3.1.cmml"><mo id="S2.SS1.p1.1.m1.2.3.2.1" stretchy="false" xref="S2.SS1.p1.1.m1.2.3.1.cmml">(</mo><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">x</mi><mo id="S2.SS1.p1.1.m1.2.3.2.2" xref="S2.SS1.p1.1.m1.2.3.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">z</mi><mo id="S2.SS1.p1.1.m1.2.3.2.3" stretchy="false" xref="S2.SS1.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.2b"><interval closure="open" id="S2.SS1.p1.1.m1.2.3.1.cmml" xref="S2.SS1.p1.1.m1.2.3.2"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘¥</ci><ci id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">ğ‘§</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.2c">(x,z)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.2d">( italic_x , italic_z )</annotation></semantics></math> direction, an affine transformation was obtained using the Levenbergâ€“Marquardt (LM) algorithm. To perform this multiple radar readings were collected by placing an individual at different known radar coordinates space 50cm apart. These readings were recorded for several seconds and the average value was then compared to the correct location to obtain our affine transformation.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Data Fetching, 2D Keypoint Localisation and Person Matching</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.5">The first stage of our proposed system involved acquiring data from each of the sensors, specifically an image frame from the camera and localisation data from each of the radars. As synchronisation is crucial to ensure consistency between the camera and radars, we separate the data obtained from each sensor into different threads. The camera data is obtained in the main thread, whereas the radar data is obtained via separate threads. The camera thread then signals the radar threads to add their data to a shared queue, ensuring synchronisation. To obtain the 2D keypoints, <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">bold_x</annotation></semantics></math>, of people in our image we used OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib6" title="">6</a>]</cite>, a popular 2D pose detector that is capable of detecting multiple people in real-time. To associate the 2D keypoints of people in the image space with their corresponding radar data, we employed a binary search tree method with a threshold value. The matching technique relied on the disparity between the average image <math alttext="x" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">italic_x</annotation></semantics></math> coordinate of a person detected by OpenPose, denoted as <math alttext="\bar{x}=\frac{\mathbf{x}}{N}" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><mrow id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mover accent="true" id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2.2" xref="S2.SS2.p1.3.m3.1.1.2.2.cmml">x</mi><mo id="S2.SS2.p1.3.m3.1.1.2.1" xref="S2.SS2.p1.3.m3.1.1.2.1.cmml">Â¯</mo></mover><mo id="S2.SS2.p1.3.m3.1.1.1" xref="S2.SS2.p1.3.m3.1.1.1.cmml">=</mo><mfrac id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml"><mi id="S2.SS2.p1.3.m3.1.1.3.2" xref="S2.SS2.p1.3.m3.1.1.3.2.cmml">ğ±</mi><mi id="S2.SS2.p1.3.m3.1.1.3.3" xref="S2.SS2.p1.3.m3.1.1.3.3.cmml">N</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><eq id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1.1"></eq><apply id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2"><ci id="S2.SS2.p1.3.m3.1.1.2.1.cmml" xref="S2.SS2.p1.3.m3.1.1.2.1">Â¯</ci><ci id="S2.SS2.p1.3.m3.1.1.2.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2.2">ğ‘¥</ci></apply><apply id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3"><divide id="S2.SS2.p1.3.m3.1.1.3.1.cmml" xref="S2.SS2.p1.3.m3.1.1.3"></divide><ci id="S2.SS2.p1.3.m3.1.1.3.2.cmml" xref="S2.SS2.p1.3.m3.1.1.3.2">ğ±</ci><ci id="S2.SS2.p1.3.m3.1.1.3.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\bar{x}=\frac{\mathbf{x}}{N}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">overÂ¯ start_ARG italic_x end_ARG = divide start_ARG bold_x end_ARG start_ARG italic_N end_ARG</annotation></semantics></math> where <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><mi id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">italic_N</annotation></semantics></math> is the number of keypoints detected (15 in our study), and the radars coordinates transformed into the image coordinate space through a learned transform. This transform is described in the pseudo-inverse section of Oh <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.5.1">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib10" title="">10</a>]</cite>. Lastly, in our simultaneous stage of radar localisation, we used the people counting algorithm introduced by Garcia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib11" title="">11</a>]</cite> to acquire the <math alttext="(x,z)" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m5.2"><semantics id="S2.SS2.p1.5.m5.2a"><mrow id="S2.SS2.p1.5.m5.2.3.2" xref="S2.SS2.p1.5.m5.2.3.1.cmml"><mo id="S2.SS2.p1.5.m5.2.3.2.1" stretchy="false" xref="S2.SS2.p1.5.m5.2.3.1.cmml">(</mo><mi id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml">x</mi><mo id="S2.SS2.p1.5.m5.2.3.2.2" xref="S2.SS2.p1.5.m5.2.3.1.cmml">,</mo><mi id="S2.SS2.p1.5.m5.2.2" xref="S2.SS2.p1.5.m5.2.2.cmml">z</mi><mo id="S2.SS2.p1.5.m5.2.3.2.3" stretchy="false" xref="S2.SS2.p1.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.2b"><interval closure="open" id="S2.SS2.p1.5.m5.2.3.1.cmml" xref="S2.SS2.p1.5.m5.2.3.2"><ci id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">ğ‘¥</ci><ci id="S2.SS2.p1.5.m5.2.2.cmml" xref="S2.SS2.p1.5.m5.2.2">ğ‘§</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.2c">(x,z)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.5.m5.2d">( italic_x , italic_z )</annotation></semantics></math> coordinates of the people within our scene. These were then transformed into the common coordinate system to calculate the distance of each person from the camera.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Unsupervised 2D-3D Lifting and 3D placement</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.3">To lift the detected 2D pose to 3D we employed a recent 2D-3D lifting network known as LInKs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib7" title="">7</a>]</cite>. We chose this due to its accuracy when generalising to unseen poses, as well as its ability to handle the most common forms of pose occlusion. Similar to other unsupervised 2D-3D lifting networks, the LInKs algorithm does not predict the absolute depth of each keypoint, but instead the depth off-set (<math alttext="\hat{d}" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mover accent="true" id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">d</mi><mo id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><ci id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1">^</ci><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\hat{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">over^ start_ARG italic_d end_ARG</annotation></semantics></math>) of each keypoint relative to a root joint (typically the pelvis), when the pose is assumed to be <math alttext="c" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.2.m2.1d">italic_c</annotation></semantics></math> units from the camera. The final 3D location of a specific keypoint, <math alttext="\mathbf{x}_{i}" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m3.1"><semantics id="S2.SS3.p1.3.m3.1a"><msub id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml"><mi id="S2.SS3.p1.3.m3.1.1.2" xref="S2.SS3.p1.3.m3.1.1.2.cmml">ğ±</mi><mi id="S2.SS3.p1.3.m3.1.1.3" xref="S2.SS3.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><apply id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.3.m3.1.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS3.p1.3.m3.1.1.2.cmml" xref="S2.SS3.p1.3.m3.1.1.2">ğ±</ci><ci id="S2.SS3.p1.3.m3.1.1.3.cmml" xref="S2.SS3.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">\mathbf{x}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.3.m3.1d">bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, was then obtained via perspective projection:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\mathbf{x}_{i}&amp;=(x_{i}\hat{z}_{i},y_{i}\hat{z}_{i},\hat{z}_{i}),%
\\
\mathbf{where}\quad\hat{z}_{i}&amp;=\max(1,\hat{d}_{i}+c).\end{split}" class="ltx_Math" display="block" id="S2.E1.m1.36"><semantics id="S2.E1.m1.36a"><mtable columnspacing="0pt" displaystyle="true" id="S2.E1.m1.36.36.4" rowspacing="0pt"><mtr id="S2.E1.m1.36.36.4a"><mtd class="ltx_align_right" columnalign="right" id="S2.E1.m1.36.36.4b"><msub id="S2.E1.m1.2.2.2.2.2"><mi id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml">ğ±</mi><mi id="S2.E1.m1.2.2.2.2.2.2.1" xref="S2.E1.m1.2.2.2.2.2.2.1.cmml">i</mi></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.36.36.4c"><mrow id="S2.E1.m1.34.34.2.33.19.17.17"><mrow id="S2.E1.m1.34.34.2.33.19.17.17.1"><mi id="S2.E1.m1.34.34.2.33.19.17.17.1.4"></mi><mo id="S2.E1.m1.3.3.3.3.1.1" xref="S2.E1.m1.3.3.3.3.1.1.cmml">=</mo><mrow id="S2.E1.m1.34.34.2.33.19.17.17.1.3.3"><mo id="S2.E1.m1.4.4.4.4.2.2" stretchy="false">(</mo><mrow id="S2.E1.m1.34.34.2.33.19.17.17.1.1.1.1"><msub id="S2.E1.m1.34.34.2.33.19.17.17.1.1.1.1.2"><mi id="S2.E1.m1.5.5.5.5.3.3" xref="S2.E1.m1.5.5.5.5.3.3.cmml">x</mi><mi id="S2.E1.m1.6.6.6.6.4.4.1" xref="S2.E1.m1.6.6.6.6.4.4.1.cmml">i</mi></msub><mo id="S2.E1.m1.34.34.2.33.19.17.17.1.1.1.1.1">â¢</mo><msub id="S2.E1.m1.34.34.2.33.19.17.17.1.1.1.1.3"><mover accent="true" id="S2.E1.m1.7.7.7.7.5.5" xref="S2.E1.m1.7.7.7.7.5.5.cmml"><mi id="S2.E1.m1.7.7.7.7.5.5.2" xref="S2.E1.m1.7.7.7.7.5.5.2.cmml">z</mi><mo id="S2.E1.m1.7.7.7.7.5.5.1" xref="S2.E1.m1.7.7.7.7.5.5.1.cmml">^</mo></mover><mi id="S2.E1.m1.8.8.8.8.6.6.1" xref="S2.E1.m1.8.8.8.8.6.6.1.cmml">i</mi></msub></mrow><mo id="S2.E1.m1.9.9.9.9.7.7">,</mo><mrow id="S2.E1.m1.34.34.2.33.19.17.17.1.2.2.2"><msub id="S2.E1.m1.34.34.2.33.19.17.17.1.2.2.2.2"><mi id="S2.E1.m1.10.10.10.10.8.8" xref="S2.E1.m1.10.10.10.10.8.8.cmml">y</mi><mi id="S2.E1.m1.11.11.11.11.9.9.1" xref="S2.E1.m1.11.11.11.11.9.9.1.cmml">i</mi></msub><mo id="S2.E1.m1.34.34.2.33.19.17.17.1.2.2.2.1">â¢</mo><msub id="S2.E1.m1.34.34.2.33.19.17.17.1.2.2.2.3"><mover accent="true" id="S2.E1.m1.12.12.12.12.10.10" xref="S2.E1.m1.12.12.12.12.10.10.cmml"><mi id="S2.E1.m1.12.12.12.12.10.10.2" xref="S2.E1.m1.12.12.12.12.10.10.2.cmml">z</mi><mo id="S2.E1.m1.12.12.12.12.10.10.1" xref="S2.E1.m1.12.12.12.12.10.10.1.cmml">^</mo></mover><mi id="S2.E1.m1.13.13.13.13.11.11.1" xref="S2.E1.m1.13.13.13.13.11.11.1.cmml">i</mi></msub></mrow><mo id="S2.E1.m1.14.14.14.14.12.12">,</mo><msub id="S2.E1.m1.34.34.2.33.19.17.17.1.3.3.3"><mover accent="true" id="S2.E1.m1.15.15.15.15.13.13" xref="S2.E1.m1.15.15.15.15.13.13.cmml"><mi id="S2.E1.m1.15.15.15.15.13.13.2" xref="S2.E1.m1.15.15.15.15.13.13.2.cmml">z</mi><mo id="S2.E1.m1.15.15.15.15.13.13.1" xref="S2.E1.m1.15.15.15.15.13.13.1.cmml">^</mo></mover><mi id="S2.E1.m1.16.16.16.16.14.14.1" xref="S2.E1.m1.16.16.16.16.14.14.1.cmml">i</mi></msub><mo id="S2.E1.m1.17.17.17.17.15.15" stretchy="false">)</mo></mrow></mrow><mo id="S2.E1.m1.18.18.18.18.16.16">,</mo></mrow></mtd></mtr><mtr id="S2.E1.m1.36.36.4d"><mtd class="ltx_align_right" columnalign="right" id="S2.E1.m1.36.36.4e"><mrow id="S2.E1.m1.35.35.3.34.15.4.4"><mi id="S2.E1.m1.19.19.19.1.1.1" xref="S2.E1.m1.19.19.19.1.1.1.cmml">ğ°ğ¡ğğ«ğ</mi><mspace id="S2.E1.m1.35.35.3.34.15.4.4.2" width="1em"></mspace><msub id="S2.E1.m1.35.35.3.34.15.4.4.1"><mover accent="true" id="S2.E1.m1.20.20.20.2.2.2" xref="S2.E1.m1.20.20.20.2.2.2.cmml"><mi id="S2.E1.m1.20.20.20.2.2.2.2" xref="S2.E1.m1.20.20.20.2.2.2.2.cmml">z</mi><mo id="S2.E1.m1.20.20.20.2.2.2.1" xref="S2.E1.m1.20.20.20.2.2.2.1.cmml">^</mo></mover><mi id="S2.E1.m1.21.21.21.3.3.3.1" xref="S2.E1.m1.21.21.21.3.3.3.1.cmml">i</mi></msub></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.36.36.4f"><mrow id="S2.E1.m1.36.36.4.35.16.12.12"><mrow id="S2.E1.m1.36.36.4.35.16.12.12.1"><mi id="S2.E1.m1.36.36.4.35.16.12.12.1.2"></mi><mo id="S2.E1.m1.22.22.22.4.1.1" xref="S2.E1.m1.22.22.22.4.1.1.cmml">=</mo><mrow id="S2.E1.m1.36.36.4.35.16.12.12.1.1.1"><mi id="S2.E1.m1.23.23.23.5.2.2" xref="S2.E1.m1.23.23.23.5.2.2.cmml">max</mi><mo id="S2.E1.m1.36.36.4.35.16.12.12.1.1.1a">â¡</mo><mrow id="S2.E1.m1.36.36.4.35.16.12.12.1.1.1.1"><mo id="S2.E1.m1.24.24.24.6.3.3" stretchy="false">(</mo><mn id="S2.E1.m1.25.25.25.7.4.4" xref="S2.E1.m1.25.25.25.7.4.4.cmml">1</mn><mo id="S2.E1.m1.26.26.26.8.5.5">,</mo><mrow id="S2.E1.m1.36.36.4.35.16.12.12.1.1.1.1.1"><msub id="S2.E1.m1.36.36.4.35.16.12.12.1.1.1.1.1.1"><mover accent="true" id="S2.E1.m1.27.27.27.9.6.6" xref="S2.E1.m1.27.27.27.9.6.6.cmml"><mi id="S2.E1.m1.27.27.27.9.6.6.2" xref="S2.E1.m1.27.27.27.9.6.6.2.cmml">d</mi><mo id="S2.E1.m1.27.27.27.9.6.6.1" xref="S2.E1.m1.27.27.27.9.6.6.1.cmml">^</mo></mover><mi id="S2.E1.m1.28.28.28.10.7.7.1" xref="S2.E1.m1.28.28.28.10.7.7.1.cmml">i</mi></msub><mo id="S2.E1.m1.29.29.29.11.8.8" xref="S2.E1.m1.29.29.29.11.8.8.cmml">+</mo><mi id="S2.E1.m1.30.30.30.12.9.9" xref="S2.E1.m1.30.30.30.12.9.9.cmml">c</mi></mrow><mo id="S2.E1.m1.31.31.31.13.10.10" stretchy="false">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.32.32.32.14.11.11" lspace="0em">.</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E1.m1.36b"><apply id="S2.E1.m1.33.33.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.33.33.1.1.1.3a.cmml">formulae-sequence</csymbol><apply id="S2.E1.m1.33.33.1.1.1.1.1.cmml"><eq id="S2.E1.m1.3.3.3.3.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1"></eq><apply id="S2.E1.m1.33.33.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.33.33.1.1.1.1.1.3.1.cmml">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1">ğ±</ci><ci id="S2.E1.m1.2.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.2.2.1">ğ‘–</ci></apply><list id="S2.E1.m1.33.33.1.1.1.1.1.1.2.cmml"><vector id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.4.cmml"><apply id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.1.1.cmml"><times id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.1.1.1.cmml"></times><apply id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.1.1.2.1.cmml">subscript</csymbol><ci id="S2.E1.m1.5.5.5.5.3.3.cmml" xref="S2.E1.m1.5.5.5.5.3.3">ğ‘¥</ci><ci id="S2.E1.m1.6.6.6.6.4.4.1.cmml" xref="S2.E1.m1.6.6.6.6.4.4.1">ğ‘–</ci></apply><apply id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.1.1.3.1.cmml">subscript</csymbol><apply id="S2.E1.m1.7.7.7.7.5.5.cmml" xref="S2.E1.m1.7.7.7.7.5.5"><ci id="S2.E1.m1.7.7.7.7.5.5.1.cmml" xref="S2.E1.m1.7.7.7.7.5.5.1">^</ci><ci id="S2.E1.m1.7.7.7.7.5.5.2.cmml" xref="S2.E1.m1.7.7.7.7.5.5.2">ğ‘§</ci></apply><ci id="S2.E1.m1.8.8.8.8.6.6.1.cmml" xref="S2.E1.m1.8.8.8.8.6.6.1">ğ‘–</ci></apply></apply><apply id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.2.2.cmml"><times id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.2.2.1.cmml"></times><apply id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.2.2.2.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.2.2.2.1.cmml">subscript</csymbol><ci id="S2.E1.m1.10.10.10.10.8.8.cmml" xref="S2.E1.m1.10.10.10.10.8.8">ğ‘¦</ci><ci id="S2.E1.m1.11.11.11.11.9.9.1.cmml" xref="S2.E1.m1.11.11.11.11.9.9.1">ğ‘–</ci></apply><apply id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.2.2.3.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.2.2.3.1.cmml">subscript</csymbol><apply id="S2.E1.m1.12.12.12.12.10.10.cmml" xref="S2.E1.m1.12.12.12.12.10.10"><ci id="S2.E1.m1.12.12.12.12.10.10.1.cmml" xref="S2.E1.m1.12.12.12.12.10.10.1">^</ci><ci id="S2.E1.m1.12.12.12.12.10.10.2.cmml" xref="S2.E1.m1.12.12.12.12.10.10.2">ğ‘§</ci></apply><ci id="S2.E1.m1.13.13.13.13.11.11.1.cmml" xref="S2.E1.m1.13.13.13.13.11.11.1">ğ‘–</ci></apply></apply><apply id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.3.3.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.33.33.1.1.1.1.1.1.1.1.3.3.1.cmml">subscript</csymbol><apply id="S2.E1.m1.15.15.15.15.13.13.cmml" xref="S2.E1.m1.15.15.15.15.13.13"><ci id="S2.E1.m1.15.15.15.15.13.13.1.cmml" xref="S2.E1.m1.15.15.15.15.13.13.1">^</ci><ci id="S2.E1.m1.15.15.15.15.13.13.2.cmml" xref="S2.E1.m1.15.15.15.15.13.13.2">ğ‘§</ci></apply><ci id="S2.E1.m1.16.16.16.16.14.14.1.cmml" xref="S2.E1.m1.16.16.16.16.14.14.1">ğ‘–</ci></apply></vector><ci id="S2.E1.m1.19.19.19.1.1.1.cmml" xref="S2.E1.m1.19.19.19.1.1.1">ğ°ğ¡ğğ«ğ</ci></list></apply><apply id="S2.E1.m1.33.33.1.1.1.2.2.cmml"><eq id="S2.E1.m1.22.22.22.4.1.1.cmml" xref="S2.E1.m1.22.22.22.4.1.1"></eq><apply id="S2.E1.m1.33.33.1.1.1.2.2.3.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.33.33.1.1.1.2.2.3.1.cmml">subscript</csymbol><apply id="S2.E1.m1.20.20.20.2.2.2.cmml" xref="S2.E1.m1.20.20.20.2.2.2"><ci id="S2.E1.m1.20.20.20.2.2.2.1.cmml" xref="S2.E1.m1.20.20.20.2.2.2.1">^</ci><ci id="S2.E1.m1.20.20.20.2.2.2.2.cmml" xref="S2.E1.m1.20.20.20.2.2.2.2">ğ‘§</ci></apply><ci id="S2.E1.m1.21.21.21.3.3.3.1.cmml" xref="S2.E1.m1.21.21.21.3.3.3.1">ğ‘–</ci></apply><apply id="S2.E1.m1.33.33.1.1.1.2.2.1.2.cmml"><max id="S2.E1.m1.23.23.23.5.2.2.cmml" xref="S2.E1.m1.23.23.23.5.2.2"></max><cn id="S2.E1.m1.25.25.25.7.4.4.cmml" type="integer" xref="S2.E1.m1.25.25.25.7.4.4">1</cn><apply id="S2.E1.m1.33.33.1.1.1.2.2.1.1.1.1.cmml"><plus id="S2.E1.m1.29.29.29.11.8.8.cmml" xref="S2.E1.m1.29.29.29.11.8.8"></plus><apply id="S2.E1.m1.33.33.1.1.1.2.2.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.33.33.1.1.1.2.2.1.1.1.1.2.1.cmml">subscript</csymbol><apply id="S2.E1.m1.27.27.27.9.6.6.cmml" xref="S2.E1.m1.27.27.27.9.6.6"><ci id="S2.E1.m1.27.27.27.9.6.6.1.cmml" xref="S2.E1.m1.27.27.27.9.6.6.1">^</ci><ci id="S2.E1.m1.27.27.27.9.6.6.2.cmml" xref="S2.E1.m1.27.27.27.9.6.6.2">ğ‘‘</ci></apply><ci id="S2.E1.m1.28.28.28.10.7.7.1.cmml" xref="S2.E1.m1.28.28.28.10.7.7.1">ğ‘–</ci></apply><ci id="S2.E1.m1.30.30.30.12.9.9.cmml" xref="S2.E1.m1.30.30.30.12.9.9">ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.36c">\begin{split}\mathbf{x}_{i}&amp;=(x_{i}\hat{z}_{i},y_{i}\hat{z}_{i},\hat{z}_{i}),%
\\
\mathbf{where}\quad\hat{z}_{i}&amp;=\max(1,\hat{d}_{i}+c).\end{split}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.36d">start_ROW start_CELL bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL = ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , end_CELL end_ROW start_ROW start_CELL bold_where over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL = roman_max ( 1 , over^ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_c ) . end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS3.p1.11">where <math alttext="d_{i}" class="ltx_Math" display="inline" id="S2.SS3.p1.4.m1.1"><semantics id="S2.SS3.p1.4.m1.1a"><msub id="S2.SS3.p1.4.m1.1.1" xref="S2.SS3.p1.4.m1.1.1.cmml"><mi id="S2.SS3.p1.4.m1.1.1.2" xref="S2.SS3.p1.4.m1.1.1.2.cmml">d</mi><mi id="S2.SS3.p1.4.m1.1.1.3" xref="S2.SS3.p1.4.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m1.1b"><apply id="S2.SS3.p1.4.m1.1.1.cmml" xref="S2.SS3.p1.4.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.4.m1.1.1.1.cmml" xref="S2.SS3.p1.4.m1.1.1">subscript</csymbol><ci id="S2.SS3.p1.4.m1.1.1.2.cmml" xref="S2.SS3.p1.4.m1.1.1.2">ğ‘‘</ci><ci id="S2.SS3.p1.4.m1.1.1.3.cmml" xref="S2.SS3.p1.4.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m1.1c">d_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.4.m1.1d">italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> was our modelsâ€™ depth-offset prediction for keypoint <math alttext="i" class="ltx_Math" display="inline" id="S2.SS3.p1.5.m2.1"><semantics id="S2.SS3.p1.5.m2.1a"><mi id="S2.SS3.p1.5.m2.1.1" xref="S2.SS3.p1.5.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.5.m2.1b"><ci id="S2.SS3.p1.5.m2.1.1.cmml" xref="S2.SS3.p1.5.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.5.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.5.m2.1d">italic_i</annotation></semantics></math>. As LInKs was originally trained on Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib12" title="">12</a>]</cite>, which uses different keypoints than those detected by OpenPose, we retrained it on the OpenPose keypoints present in the Human3.6M dataset. Once we had lifted our 2D pose to 3D, we now had to transform it from its local coordinate system, where the root joint was at position (0,0,<math alttext="c" class="ltx_Math" display="inline" id="S2.SS3.p1.6.m3.1"><semantics id="S2.SS3.p1.6.m3.1a"><mi id="S2.SS3.p1.6.m3.1.1" xref="S2.SS3.p1.6.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.6.m3.1b"><ci id="S2.SS3.p1.6.m3.1.1.cmml" xref="S2.SS3.p1.6.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.6.m3.1c">c</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.6.m3.1d">italic_c</annotation></semantics></math>), to our global coordinate system. To do this we subtracted <math alttext="c" class="ltx_Math" display="inline" id="S2.SS3.p1.7.m4.1"><semantics id="S2.SS3.p1.7.m4.1a"><mi id="S2.SS3.p1.7.m4.1.1" xref="S2.SS3.p1.7.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.7.m4.1b"><ci id="S2.SS3.p1.7.m4.1.1.cmml" xref="S2.SS3.p1.7.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.7.m4.1c">c</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.7.m4.1d">italic_c</annotation></semantics></math> from the pose and added the <math alttext="x" class="ltx_Math" display="inline" id="S2.SS3.p1.8.m5.1"><semantics id="S2.SS3.p1.8.m5.1a"><mi id="S2.SS3.p1.8.m5.1.1" xref="S2.SS3.p1.8.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.8.m5.1b"><ci id="S2.SS3.p1.8.m5.1.1.cmml" xref="S2.SS3.p1.8.m5.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.8.m5.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.8.m5.1d">italic_x</annotation></semantics></math> and <math alttext="z" class="ltx_Math" display="inline" id="S2.SS3.p1.9.m6.1"><semantics id="S2.SS3.p1.9.m6.1a"><mi id="S2.SS3.p1.9.m6.1.1" xref="S2.SS3.p1.9.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.9.m6.1b"><ci id="S2.SS3.p1.9.m6.1.1.cmml" xref="S2.SS3.p1.9.m6.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.9.m6.1c">z</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.9.m6.1d">italic_z</annotation></semantics></math> coordinates from our radar sensors. Additionally, to maintain ground-plane contact the <math alttext="y" class="ltx_Math" display="inline" id="S2.SS3.p1.10.m7.1"><semantics id="S2.SS3.p1.10.m7.1a"><mi id="S2.SS3.p1.10.m7.1.1" xref="S2.SS3.p1.10.m7.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.10.m7.1b"><ci id="S2.SS3.p1.10.m7.1.1.cmml" xref="S2.SS3.p1.10.m7.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.10.m7.1c">y</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.10.m7.1d">italic_y</annotation></semantics></math> coordinates of the pose were updated by subtracting the <math alttext="y" class="ltx_Math" display="inline" id="S2.SS3.p1.11.m8.1"><semantics id="S2.SS3.p1.11.m8.1a"><mi id="S2.SS3.p1.11.m8.1.1" xref="S2.SS3.p1.11.m8.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.11.m8.1b"><ci id="S2.SS3.p1.11.m8.1.1.cmml" xref="S2.SS3.p1.11.m8.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.11.m8.1c">y</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.11.m8.1d">italic_y</annotation></semantics></math> coordinate of the lowest ankle.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="194" id="S2.F3.g1" src="extracted/5470926/poses_2_v2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Qualtative results of our approach. The above images show the input frame to our model with poses captured by OpenPose. The bottom images show the corresponding reconstructed 3D poses in our global 3D coordinate system. All pictures are partially cropped around the top and bottom. </figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Results</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.2">Here we present the quantitative of our improved people matching algorithm, 2D-3D human pose lifting model and the radar localisation error for the <math alttext="x" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_x</annotation></semantics></math> and <math alttext="z" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_z</annotation></semantics></math> coordinate. The qualitative results of our approach can be seen in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S2.F3" title="Figure 3 â€£ II-C Unsupervised 2D-3D Lifting and 3D placement â€£ II Methodology â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_tag">3</span></a>. In our experiment we used a commercial RTX 3060, Ricoh Theta V omnidirectional camera and three TI AWR1843BOOST mmWave radars. However, it is worth noting that it will work with any GPU, mmWave radars or omnidirectional camera as long as the latter outputs video frames in the equirectangular format.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Results of matching people in the image and radar</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.2">In our preliminary work, we matched people by using the angle between people detected in the radar and camera relative to the cameraâ€™s <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_x</annotation></semantics></math> coordinate. In our approach, we implemented an improved method that solely focuses on the <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_x</annotation></semantics></math> coordinate in the camera space which illustrated a significant improvement. To demonstrate this we calculated the matching error as a % which represents the absolute difference between the radar and camera values of an individual, divided by the camera values as seen in Table. Our results for this can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S3.T1" title="TABLE I â€£ III-A Results of matching people in the image and radar â€£ III Results â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_tag">I</span></a>
</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Showing the matching error of our preliminary work and our improved approach. The value is a % obtained by computing the absolute difference between the corresponding radar and camera values</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.9" style="width:433.6pt;height:69.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(49.2pt,-7.9pt) scale(1.29331255068478,1.29331255068478) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.9.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.3.3.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T1.3.3.3.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1">Radar 1 <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.2.2">Radar 2 <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.2.m1.1a"><mo id="S3.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S3.T1.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.3.3.3.3">Radar 3 <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.m1.1"><semantics id="S3.T1.3.3.3.3.m1.1a"><mo id="S3.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S3.T1.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.6.6.6.4">Preliminary Work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib8" title="">8</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.4.4.1">23.89% <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.4.4.4.1.m1.1"><semantics id="S3.T1.4.4.4.1.m1.1a"><mo id="S3.T1.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.1b"><csymbol cd="latexml" id="S3.T1.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.1.m1.1d">Â±</annotation></semantics></math> 6.57%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.5.5.2">33.57% <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.5.5.5.2.m1.1"><semantics id="S3.T1.5.5.5.2.m1.1a"><mo id="S3.T1.5.5.5.2.m1.1.1" xref="S3.T1.5.5.5.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.2.m1.1b"><csymbol cd="latexml" id="S3.T1.5.5.5.2.m1.1.1.cmml" xref="S3.T1.5.5.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.2.m1.1d">Â±</annotation></semantics></math> 50.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.3">66.89% <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.6.6.6.3.m1.1"><semantics id="S3.T1.6.6.6.3.m1.1a"><mo id="S3.T1.6.6.6.3.m1.1.1" xref="S3.T1.6.6.6.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.3.m1.1b"><csymbol cd="latexml" id="S3.T1.6.6.6.3.m1.1.1.cmml" xref="S3.T1.6.6.6.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.3.m1.1d">Â±</annotation></semantics></math> 263.89</td>
</tr>
<tr class="ltx_tr" id="S3.T1.9.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.9.9.9.4">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.7.7.7.1">2.52% <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.7.7.7.1.m1.1"><semantics id="S3.T1.7.7.7.1.m1.1a"><mo id="S3.T1.7.7.7.1.m1.1.1" xref="S3.T1.7.7.7.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S3.T1.7.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.7.1.m1.1d">Â±</annotation></semantics></math> 2.51</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.8.8.8.2">9.44% <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.8.8.8.2.m1.1"><semantics id="S3.T1.8.8.8.2.m1.1a"><mo id="S3.T1.8.8.8.2.m1.1.1" xref="S3.T1.8.8.8.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.2.m1.1b"><csymbol cd="latexml" id="S3.T1.8.8.8.2.m1.1.1.cmml" xref="S3.T1.8.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.8.2.m1.1d">Â±</annotation></semantics></math> 13.27</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.3">1.94% <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.9.9.9.3.m1.1"><semantics id="S3.T1.9.9.9.3.m1.1a"><mo id="S3.T1.9.9.9.3.m1.1.1" xref="S3.T1.9.9.9.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.3.m1.1b"><csymbol cd="latexml" id="S3.T1.9.9.9.3.m1.1.1.cmml" xref="S3.T1.9.9.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.9.9.3.m1.1d">Â±</annotation></semantics></math> 1.52</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">2D-3D Lifting and Occlusion Handling Results</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As previously mentioned we used the LInKs lifting network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib7" title="">7</a>]</cite> for 2D-3D pose lifting. We trained LInKs unsupervised on Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib12" title="">12</a>]</cite> with identical training and model parameters to its original publication, with the only modification being adapting it to use the keypoints detected via OpenPose. We report the mean per joint position error which is the euclidean distance in millimetres between the ground truth keypoints and those within our reconstructed pose. For this we report both the error once our pose has been scaled to the ground truth (N-MPJPE) and once our pose was rigidly aligned to the ground truth. We also show the results in various occlusion scenarios. These can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S3.T2" title="TABLE II â€£ III-B 2D-3D Lifting and Occlusion Handling Results â€£ III Results â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_tag">II</span></a>. As shown we achieved similar results in N-MPJPE under scenarios of no occlusion with a slightly higher PA-MPJPE. We attribute this to the OpenPose keypoints not using the spine and head-top keypoint which are relatively easy to estimate the depth-offset of from the pelvis, being that they are typically directly above in the majority of standing scenarios. This is why we see a similar N-MPJPE as our scaled poses have a very similar accuracy however a slightly higher PA-MPJPE as two relatively easy keypoints are not included in the alignment.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Showing the 2D-3D lifting results of using the LInKs model trained on the OpenPose keypoints of Human3.6M</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:433.6pt;height:294.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(70.9pt,-48.1pt) scale(1.48584498841842,1.48584498841842) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.1.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.1.1.1.1.2">Occlusion</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.1.3">PA-MPJPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.1.4">N-MPJPE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.1.2.1.1">LInKs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib7" title="">7</a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.1.2.1.2">None</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.1.3">33.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.1.4">61.6</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.3.2.1">Ours (Recreation)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.3.2.2">None</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.2.3">37.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.2.4">61.7</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.4.3.1">Ours (Recreation)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.4.3.2">Left Arm</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.3.3">52.1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.3.4">78.1</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.5.4.1">Ours (Recreation)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.5.4.2">Left Leg</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.4.3">46.0</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.4.4">73.2</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.6.5.1">Ours (Recreation)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.6.5.2">Right Arm</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.5.3">49.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.5.4">75.7</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.7.6.1">Ours (Recreation)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.7.6.2">Right Leg</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.7.6.3">44.5</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.7.6.4">71.6</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.8.7.1">Ours (Recreation)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.8.7.2">Left Arm &amp; Leg</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.7.3">62.0</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.7.4">86.0</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.9.8.1">Ours (Recreation)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.9.8.2">Right Arm &amp; Leg</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.8.3">60.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.8.4">83.7</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.10.9.1">Ours (Recreation)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.10.9.2">Both Legs</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.10.9.3">69.3</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.10.9.4">99.8</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.1.1.11.10.1">Ours (Recreation)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.1.1.11.10.2">Torso</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.11.10.3">88.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.11.10.4">122.0</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="193" id="S3.F4.g1" src="extracted/5470926/radar_errors_x_1.png" width="264"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="190" id="S3.F4.g2" src="extracted/5470926/radar_errors_z_1.png" width="264"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Showing the localisation error in metres at various points around our setup. The errors were evaluated in each radarâ€™s <math alttext="\hat{x}" class="ltx_Math" display="inline" id="S3.F4.4.m1.1"><semantics id="S3.F4.4.m1.1b"><mover accent="true" id="S3.F4.4.m1.1.1" xref="S3.F4.4.m1.1.1.cmml"><mi id="S3.F4.4.m1.1.1.2" xref="S3.F4.4.m1.1.1.2.cmml">x</mi><mo id="S3.F4.4.m1.1.1.1" xref="S3.F4.4.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.F4.4.m1.1c"><apply id="S3.F4.4.m1.1.1.cmml" xref="S3.F4.4.m1.1.1"><ci id="S3.F4.4.m1.1.1.1.cmml" xref="S3.F4.4.m1.1.1.1">^</ci><ci id="S3.F4.4.m1.1.1.2.cmml" xref="S3.F4.4.m1.1.1.2">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.4.m1.1d">\hat{x}</annotation><annotation encoding="application/x-llamapun" id="S3.F4.4.m1.1e">over^ start_ARG italic_x end_ARG</annotation></semantics></math> (left) and <math alttext="\hat{z}" class="ltx_Math" display="inline" id="S3.F4.5.m2.1"><semantics id="S3.F4.5.m2.1b"><mover accent="true" id="S3.F4.5.m2.1.1" xref="S3.F4.5.m2.1.1.cmml"><mi id="S3.F4.5.m2.1.1.2" xref="S3.F4.5.m2.1.1.2.cmml">z</mi><mo id="S3.F4.5.m2.1.1.1" xref="S3.F4.5.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.F4.5.m2.1c"><apply id="S3.F4.5.m2.1.1.cmml" xref="S3.F4.5.m2.1.1"><ci id="S3.F4.5.m2.1.1.1.cmml" xref="S3.F4.5.m2.1.1.1">^</ci><ci id="S3.F4.5.m2.1.1.2.cmml" xref="S3.F4.5.m2.1.1.2">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.5.m2.1d">\hat{z}</annotation><annotation encoding="application/x-llamapun" id="S3.F4.5.m2.1e">over^ start_ARG italic_z end_ARG</annotation></semantics></math> (right) directions. The figures represent these errors in the <math alttext="(\mathbf{\hat{X}},\mathbf{\hat{Z}})" class="ltx_Math" display="inline" id="S3.F4.6.m3.2"><semantics id="S3.F4.6.m3.2b"><mrow id="S3.F4.6.m3.2.3.2" xref="S3.F4.6.m3.2.3.1.cmml"><mo id="S3.F4.6.m3.2.3.2.1" stretchy="false" xref="S3.F4.6.m3.2.3.1.cmml">(</mo><mover accent="true" id="S3.F4.6.m3.1.1" xref="S3.F4.6.m3.1.1.cmml"><mi id="S3.F4.6.m3.1.1.2" xref="S3.F4.6.m3.1.1.2.cmml">ğ—</mi><mo id="S3.F4.6.m3.1.1.1" xref="S3.F4.6.m3.1.1.1.cmml">^</mo></mover><mo id="S3.F4.6.m3.2.3.2.2" xref="S3.F4.6.m3.2.3.1.cmml">,</mo><mover accent="true" id="S3.F4.6.m3.2.2" xref="S3.F4.6.m3.2.2.cmml"><mi id="S3.F4.6.m3.2.2.2" xref="S3.F4.6.m3.2.2.2.cmml">ğ™</mi><mo id="S3.F4.6.m3.2.2.1" xref="S3.F4.6.m3.2.2.1.cmml">^</mo></mover><mo id="S3.F4.6.m3.2.3.2.3" stretchy="false" xref="S3.F4.6.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.6.m3.2c"><interval closure="open" id="S3.F4.6.m3.2.3.1.cmml" xref="S3.F4.6.m3.2.3.2"><apply id="S3.F4.6.m3.1.1.cmml" xref="S3.F4.6.m3.1.1"><ci id="S3.F4.6.m3.1.1.1.cmml" xref="S3.F4.6.m3.1.1.1">^</ci><ci id="S3.F4.6.m3.1.1.2.cmml" xref="S3.F4.6.m3.1.1.2">ğ—</ci></apply><apply id="S3.F4.6.m3.2.2.cmml" xref="S3.F4.6.m3.2.2"><ci id="S3.F4.6.m3.2.2.1.cmml" xref="S3.F4.6.m3.2.2.1">^</ci><ci id="S3.F4.6.m3.2.2.2.cmml" xref="S3.F4.6.m3.2.2.2">ğ™</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.6.m3.2d">(\mathbf{\hat{X}},\mathbf{\hat{Z}})</annotation><annotation encoding="application/x-llamapun" id="S3.F4.6.m3.2e">( over^ start_ARG bold_X end_ARG , over^ start_ARG bold_Z end_ARG )</annotation></semantics></math> 2D global coordinate system. The red dot marks the system location.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Radar Localisation Results</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.7">To demonstrate the improvement in radar localisation due to our affine transformation we present the average absolute error in centimetres in the <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_x</annotation></semantics></math> and <math alttext="z" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_z</annotation></semantics></math> direction for people within our scene. These results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S3.T3" title="TABLE III â€£ III-C Radar Localisation Results â€£ III Results â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_tag">III</span></a>, additionally Fig. <a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#S3.F4" title="Figure 4 â€£ III-B 2D-3D Lifting and Occlusion Handling Results â€£ III Results â€£ Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting"><span class="ltx_text ltx_ref_tag">4</span></a> visualises the localisation errors of objects from the radars in metres at various points around the system. As shown in our results our affine transformation has led to a reduction in the error along the <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">italic_x</annotation></semantics></math> and <math alttext="z" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_z</annotation></semantics></math> direction for nearly all radars while performing similarly for the <math alttext="z" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">italic_z</annotation></semantics></math> direction for radar 1. Despite this, we note that there are still some errors present, especially when the subject is positioned at a <math alttext="60^{\circ}" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><msup id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><mn id="S3.SS3.p1.6.m6.1.1.2" xref="S3.SS3.p1.6.m6.1.1.2.cmml">60</mn><mo id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">superscript</csymbol><cn id="S3.SS3.p1.6.m6.1.1.2.cmml" type="integer" xref="S3.SS3.p1.6.m6.1.1.2">60</cn><compose id="S3.SS3.p1.6.m6.1.1.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">60^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">60 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math> angle from the centre of the radar despite the radars <math alttext="120^{\circ}" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m7.1"><semantics id="S3.SS3.p1.7.m7.1a"><msup id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml"><mn id="S3.SS3.p1.7.m7.1.1.2" xref="S3.SS3.p1.7.m7.1.1.2.cmml">120</mn><mo id="S3.SS3.p1.7.m7.1.1.3" xref="S3.SS3.p1.7.m7.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><apply id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">superscript</csymbol><cn id="S3.SS3.p1.7.m7.1.1.2.cmml" type="integer" xref="S3.SS3.p1.7.m7.1.1.2">120</cn><compose id="S3.SS3.p1.7.m7.1.1.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">120^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m7.1d">120 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math> coverage. In addition, we noticed void spaces exist in these areas where none of our radars were able to detect our subjects. One possible remediation for this would be the inclusion of additional radars directed at these areas of high error.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Table showing the mean absolute error in centimetres in the x and z direction for each of our radars in our preliminary work and our new approach with affine transformation.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.6" style="width:260.2pt;height:177.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(37.6pt,-25.6pt) scale(1.40613529302229,1.40613529302229) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.6.6.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.6.6.7.1.1">Radar</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.6.6.7.1.2">Direction</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.6.6.7.1.3">Preliminary <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib8" title="">8</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.6.6.7.1.4">Ours</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.2" rowspan="2"><span class="ltx_text" id="S3.T3.1.1.1.2.1">1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1"><math alttext="x" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.m1.1a"><mi id="S3.T3.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.m1.1d">italic_x</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.3">20.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.4">16.45</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.2.2.1"><math alttext="z" class="ltx_Math" display="inline" id="S3.T3.2.2.2.1.m1.1"><semantics id="S3.T3.2.2.2.1.m1.1a"><mi id="S3.T3.2.2.2.1.m1.1.1" xref="S3.T3.2.2.2.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.1.m1.1b"><ci id="S3.T3.2.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.2.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.1.m1.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.1.m1.1d">italic_z</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.2">11.41</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.3">11.45</td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.3.3.3.2" rowspan="2"><span class="ltx_text" id="S3.T3.3.3.3.2.1">2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.3.3.3.1"><math alttext="x" class="ltx_Math" display="inline" id="S3.T3.3.3.3.1.m1.1"><semantics id="S3.T3.3.3.3.1.m1.1a"><mi id="S3.T3.3.3.3.1.m1.1.1" xref="S3.T3.3.3.3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.1.m1.1b"><ci id="S3.T3.3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.3.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.3.1.m1.1d">italic_x</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.3.3.3.3">26.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.3.3.3.4">24.86</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.4.4.4.1"><math alttext="z" class="ltx_Math" display="inline" id="S3.T3.4.4.4.1.m1.1"><semantics id="S3.T3.4.4.4.1.m1.1a"><mi id="S3.T3.4.4.4.1.m1.1.1" xref="S3.T3.4.4.4.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.1.m1.1b"><ci id="S3.T3.4.4.4.1.m1.1.1.cmml" xref="S3.T3.4.4.4.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.1.m1.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.4.1.m1.1d">italic_z</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.2">15.39</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.3">10.77</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T3.5.5.5.2" rowspan="2"><span class="ltx_text" id="S3.T3.5.5.5.2.1">3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.5.5.1"><math alttext="x" class="ltx_Math" display="inline" id="S3.T3.5.5.5.1.m1.1"><semantics id="S3.T3.5.5.5.1.m1.1a"><mi id="S3.T3.5.5.5.1.m1.1.1" xref="S3.T3.5.5.5.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.1.m1.1b"><ci id="S3.T3.5.5.5.1.m1.1.1.cmml" xref="S3.T3.5.5.5.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.5.5.1.m1.1d">italic_x</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.5.3">16.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.5.4">15.94</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T3.6.6.6.1"><math alttext="z" class="ltx_Math" display="inline" id="S3.T3.6.6.6.1.m1.1"><semantics id="S3.T3.6.6.6.1.m1.1a"><mi id="S3.T3.6.6.6.1.m1.1.1" xref="S3.T3.6.6.6.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.6.1.m1.1b"><ci id="S3.T3.6.6.6.1.m1.1.1.cmml" xref="S3.T3.6.6.6.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.6.1.m1.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.6.6.1.m1.1d">italic_z</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.6.6.2">13.83</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.6.6.3">13.46</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In conclusion, our real-time 3D multi-person detection system significantly improves our preliminary work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09437v1#bib.bib8" title="">8</a>]</cite>, offering simplicity, robustness, and scalability. Challenges remain in system speed, range, and occlusion handling. Future research will focus on improving occlusion handling, optimizing algorithm speed, and expanding the systemâ€™s range. The inclusion of additional radar units and higher-resolution cameras can eliminate limitations related to detection gaps and bent knees. Our contributions enhance technology accessibility and robustness for computer vision applications, making it an affordable industry solution.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K.Â Ludwig, S.Â Scherer, M.Â Einfalt, and R.Â Lienhart, â€œSelf-supervised learning for human pose estimation in sports,â€ in <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">2021 IEEE International Conference On Multimedia &amp; Expo Workshops (ICMEW)</span>, pp.Â 1â€“6, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
L.Â Kumarapu and P.Â Mukherjee, â€œAnimePose: Multi-person 3D pose estimation and animation,â€ <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Pattern Recognition Letters</span>, vol.Â 147, pp.Â 16â€“24, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M.Â Martin, S.Â Stuehmer, M.Â Voit, and R.Â Stiefelhagen, â€œReal time driver body pose estimation for novel assistance systems,â€ in <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">2017 IEEE 20th International Conference On Intelligent Transportation Systems (ITSC)</span>, pp.Â 1â€“7, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M.Â Furst, S.Â Gupta, R.Â Schuster, O.Â Wasenmuller, and D.Â Stricker, â€œHPERL: 3D Human Pose Estimation from RGB and LiDAR,â€ 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C.Â Keskin and etÂ al., â€œReal time hand pose estimation using depth sensors,â€ in <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Consumer Depth Cameras for Computer Vision</span>, pp.Â 119â€“137, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Z.Â Cao, G.Â HidalgoÂ Martinez, T.Â Simon, S.-E. Wei, and Y.Â Sheikh, â€œOpenpose: Realtime multi-person 2d pose estimation using part affinity fields,â€ <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, pp.Â 1â€“1, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P.Â Hardy and H.Â Kim, â€œLinks â€lifting independent keypointsâ€ â€“ partial pose lifting for occlusion handling with improved accuracy in 2d-3d human pose estimation,â€ 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.Â Aarti, T.Â Alberto, K.Â Isaac, S.Â Emil, F.Â Timothy, L.Â Hwasup, and K.Â Hansung, â€œReal-time 3d multi-person pose estimation using an omnidirectional camera and mmwave radars,â€ in <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proc. ICEET</span>, October 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z.Â Zhang, â€œA flexible new technique for camera calibration,â€ <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE Transactions on pattern analysis and machine intelligence</span>, vol.Â 22, no.Â 11, pp.Â 1330â€“1334, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J.Â Oh, K.-S. Kim, M.Â Park, and S.Â Kim, â€œA comparative study on camera-radar calibration methods,â€ in <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)</span>, pp.Â 1057â€“1062, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K.Â Garcia, â€œBringing intelligent autonomy to fine motion detection and people counting with ti mmwave sensors,â€ 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
C.Â Ionescu, D.Â Papava, V.Â Olaru, and C.Â Sminchisescu, â€œHuman3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments,â€ <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, vol.Â 36, pp.Â 1325â€“1339, jul 2014.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Mar 14 14:28:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
