<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation</title>
<!--Generated on Tue Dec 12 03:55:36 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Visual Activity Detection,  Large Language Models,  Human Activity Recognition" lang="en" name="keywords"/>
<base href="/html/2312.06965v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S1" title="1. Introduction ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS1" title="1.1. Challenges in Vision-Based HAR ‣ 1. Introduction ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Challenges in Vision-Based HAR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS2" title="1.2. A Novel Approach - GroupFormer ‣ 1. Introduction ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>A Novel Approach - GroupFormer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S2" title="2. Framework ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3. Proof of Concept: Natural Language Generation for Accurate Human Activity Recognition ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proof of Concept: Natural Language Generation for Accurate Human Activity Recognition</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1. Dataset and Action Selection ‣ 3. Proof of Concept: Natural Language Generation for Accurate Human Activity Recognition ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Dataset and Action Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="3.2. Training Methodology ‣ 3. Proof of Concept: Natural Language Generation for Accurate Human Activity Recognition ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="3.3. Inference and Evaluation ‣ 3. Proof of Concept: Natural Language Generation for Accurate Human Activity Recognition ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Inference and Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S4" title="4. Implications for Future Research ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Implications for Future Research</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="5. Potential Applications ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Potential Applications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S6" title="6. Future Work ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S7" title="7. Conclusion ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY-SA 4.0</div><div id="watermark-tr">arXiv:2312.06965v1 [cs.HC] 12 Dec 2023</div></div>
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nikhil Kashyap
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:nkash@uw.edu">nkash@uw.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of Washington</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Seattle</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Washington</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">United States</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Manas Satish Bedmutha
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mbedmutha@ucsd.edu">mbedmutha@ucsd.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-3427-2226" title="ORCID identifier">0000-0003-3427-2226</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">University of California San Diego</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">La Jolla</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">United States</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Prerit Chaudhary, Brian Wood, Wanda Pratt, Janice Sabin, Andrea Hartzler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:prerit16,%20bwood2,%20wpratt,%20sabinja,%20andreah@uw.edu">prerit16, bwood2, wpratt, sabinja, andreah@uw.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">University of Washington</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">Seattle</span><span class="ltx_text ltx_affiliation_state" id="id11.3.id3">Washington</span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4">United States</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nadir Weibel
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:weibel@ucsd.edu">weibel@ucsd.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-3457-4227" title="ORCID identifier">0000-0002-3457-4227</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">University of California San Diego</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">La Jolla</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">United States</span>
</span></span></span>
</div>
<div class="ltx_dates">(2023)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id17.id1">Vision-based human activity recognition (HAR) has made substantial progress in recognizing predefined gestures but lacks adaptability for emerging activities. This paper introduces a paradigm shift by harnessing generative modeling and large language models (LLMs) to enhance vision-based HAR. We propose utilizing LLMs to generate descriptive textual representations of activities using pose keypoints as an intermediate representation. Incorporating pose keypoints adds contextual depth to the recognition process, allowing for sequences of vectors resembling text chunks, compatible with LLMs. This innovative fusion of computer vision and natural language processing holds significant potential for revolutionizing activity recognition. A proof of concept study on a Kinetics700 dataset subset validates the approach’s efficacy, highlighting improved accuracy and interpretability. Future implications encompass enhanced accuracy, novel research avenues, model generalization, and ethical considerations for transparency. This framework has real-world applications, including personalized gym workout feedback and nuanced sports training insights. By connecting visual cues to interpretable textual descriptions, the proposed framework advances HAR accuracy and applicability, shaping the landscape of pervasive computing and activity recognition research. As this approach evolves, it promises a more insightful understanding of human activities across diverse contexts, marking a significant step towards a better world.</p>
</div>
<div class="ltx_keywords">Visual Activity Detection, Large Language Models, Human Activity Recognition
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2023</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Generative AI for Pervasive Computing Symposium at the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing; October 8–12, 2023; Cancun, Quintana Roo, Mexico</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Ubiquitous and mobile computing systems and tools</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="68" id="S1.F1.g1" src="extracted/5289171/framework.png" width="598"/></div>
<div class="ltx_flex_cell">
<p class="ltx_p ltx_align_center" id="S1.F1.2">[Overall Framework]Proposed framework where each frame from a video feed can be used to first estimate the pose keypoints of the subjects, then preprocessed to use them with large language models</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Proposed framework: Each frame from a video feed can be used to first estimate the pose keypoints of the subjects, then preprocessed to use them with large language models</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the realm of pervasive computing, Human Activity Recognition (HAR) is an established approach to attempt deciphering human actions and behaviors in various environments. However, despite numerous successful use-cases <cite class="ltx_cite ltx_citemacro_citep">(Dang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib5" title="">2020</a>)</cite>, human activity recognition is still an evolving field and countless types of actions that we undergo on a daily basis still can’t be recognized. As datasets corresponding to different activities keep growing <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib17" title="">2017</a>)</cite>, most current approaches develop models or pipelines catered to recognizing specific activities in those datasets. With recent advances in representational learning and generative modeling, we can leverage highly powerful large neural networks to adapt to changing needs of data, in terms of both the input applied and the outputs generated. In this work we explore the use of large language models as a solution to extend human activity recognition beyond vision-based approaches.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1. </span>Challenges in Vision-Based HAR</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Vision-based HAR methods have garnered significant attention due to their potential for capturing intricate nuances of human actions, making them particularly valuable in applications where precision is paramount. For instance, these methods excel at discerning subtle hand movements in sign language interpretation, accurately tracking facial expressions in emotion recognition, and precisely capturing the body mechanics in sports actions like tennis serves or golf swings. However, these methods encounter several challenges that hinder their accuracy and practicality, ultimately limiting their effectiveness in real-world scenarios.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">Traditional vision-based approaches often rely on complex models that necessitate substantial amounts of training data and meticulous parameter tuning. This dependency on large datasets introduces potential biases and restricts the applicability of the model to specific conditions. Additionally, the intricate architecture of these models can lead to computationally intensive operations, rendering them less feasible for deployment on resource-constrained devices or in real-time applications. Furthermore, the need for manual parameter adjustment makes the models less adaptable to new scenarios or activities without laborious reconfiguration.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">In this work, we focus on addressing the issue of model generalization in vision-based HAR. Specifically, we tackle the challenge of adapting recognition models to real-world scenarios that exhibit variations in lighting conditions, camera viewpoints, and user demographics. This is a crucial aspect of our endeavor to enhance the accuracy and practicality of HAR methods, making them more robust and versatile in the face of dynamic and diverse environments.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2. </span>A Novel Approach - GroupFormer</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">Recently, Li et. al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib11" title="">2021</a>)</cite> proposed the use of language models to solve other vision based tasks through the GroupFormer framework. It presents a unique perspective by leveraging both visual and human pose estimation features to enhance activity recognition. The model employs a pre-trained inflated 3D convolutional deep neural network (I3D-CNN) to extract visual features of individuals. These visual features are combined with pose estimation features from AlphaPose <cite class="ltx_cite ltx_citemacro_citep">(Fang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib6" title="">2022</a>)</cite>, resulting in a comprehensive set of player features. These player features serve as inputs to a stack of clustered spatial-temporal transformers, refining the representation of human activities.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.1">GroupFormer, while promising, carries certain limitations. Specifically, the method’s computational demands are relatively high compared to other attention mechanisms. This stems from the necessity for GroupFormer to learn distinct attention maps for individual feature groups, potentially impacting its scalability in resource-constrained environments.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Framework</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this paper, we propose a novel approach to HAR that involves treating activity recognition as a Natural Language Generation (NLG) challenge. Our framework (Fig. <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Towards Enhanced Human Activity Recognition through Natural Language Generation and Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>) revolves around transforming video data into human pose estimation, which is subsequently fed into a Language Generation Model (LLM). This LLM leverages the acquired pose keypoints to generate descriptive textual representations of observed activities.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Instead of transforming images directly into embeddings, an intermediate level of pose keypoints helps us add context towards the activities in the scene. This also helps convert a sequence of images into a sequence of vectors that can be viewed as chunks of text by the language models.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">By encapsulating activity recognition as a natural language generation task, we aim to create a bridge between the visual cues and interpretable textual descriptions, potentially enhancing both accuracy and human interpretability.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Proof of Concept: Natural Language Generation for Accurate Human Activity Recognition</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To validate the effectiveness of our proposed framework, we conducted a proof of concept study using a subset of the Kinetics700 dataset by DeepMind <cite class="ltx_cite ltx_citemacro_citep">(Carreira et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib3" title="">2022</a>)</cite>. This dataset is widely recognized for its diverse collection of human actions in video format, making it an ideal benchmark for testing our novel approach. Our experiments aim to showcase how our NLG-based framework enhances the accuracy of Human Activity Recognition (HAR) while offering insights into its potential benefits for future researchers.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Dataset and Action Selection</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Within Kinectics700 subset, we focused on 2865 video files containing various actions exhibiting a wide range of motion patterns and complexities, making them suitable for testing the robustness of our framework. Our dataset contains 673 actions, for a total of 392,265 keypoints.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Training Methodology</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To train our NLG-based model, we adopted the training methodology proposed in the Comet Atomic research paper <cite class="ltx_cite ltx_citemacro_citep">(Hwang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib9" title="">2021</a>)</cite>. In this approach, GPT2-XL <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib12" title="">2019</a>)</cite>, a language model, is fine-tuned using a structured tuple format.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For our HAR application, we adapted this methodology by encoding each video sequence of pose keypoints along with an associated activity label in the format: <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1">&lt;frame0_keypoints&gt; 
<br class="ltx_break"/>&lt;frame1_keypoints&gt; ... &lt;frame9_keypoints&gt; [GEN] &lt;HAR label&gt; [SEP]</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.1">&lt;GEN&gt;</span> token serves as the delimiter for the model to generate the textual representation of the activity label. We utilized the AlphaPose pose estimation model to extract 17 COCO format keypoints from each video frame, forming the input for our GPT2-XL model.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Inference and Evaluation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">During the inference stage, given the sequence of pose keypoints from the first to the ninth frame, our trained GPT2-XL model generates the predicted activity label using the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.1.1">[GEN]</span> token. This approach leverages the spatial information captured by pose keypoints to generate a textual description that corresponds to the recognized human activity.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To evaluate the performance of our approach, we employed standard Top-1 accuracy. By comparing our NLG-based approach with traditional methods of HAR that directly analyze raw video frames, we aimed to demonstrate the efficacy of our framework in improving accuracy and interpretability.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Model Accuracy</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Top-1 Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.2.1.1">InternVideo <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib15" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.2">0.84</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.1.3.2.1">Our model</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.3.2.2">0.52</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Implications for Future Research</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our proof of concept study underscores the promise of our NLG-based framework in HAR. By expanding our model’s training dataset, we have the potential to attain elevated levels of accuracy. By conceptualizing HAR through the lens of an NLG challenge, we not only strive for improved accuracy in activity recognition, but also offer a more lucid and understandable portrayal of human actions. This approach holds substantial potential despite the current accuracy levels, and its viability is grounded in its unique perspective on the problem. This approach has several implications for future researchers:
</p>
</div>
<div class="ltx_para" id="S4.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Enhanced Accuracy and Interpretability: Our approach enhances activity recognition accuracy by leveraging both spatial and contextual information. Researchers can benefit from more reliable and interpretable results, enabling better decision-making in real-world applications.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">New Research Avenues: The NLG paradigm opens up new research avenues at the intersection of computer vision and natural language processing. Future researchers can explore ways to incorporate semantic understanding and context into activity recognition models.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Model Generalization: Our approach demonstrates the potential for models trained on a subset of a larger dataset to generalize well to broader datasets. This insight could guide future research in data-efficient model training.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">Ethical Considerations: The interpretability of textual descriptions generated by our model can aid in addressing ethical concerns associated with black-box deep learning models, promoting transparency and accountability.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">By leveraging pose keypoints and GPT2-XL-based language generation, we offer a novel perspective on activity recognition that has the potential to reshape how future researchers approach this field. Our proof of concept study showcases the feasibility and benefits of reimagining HAR as an NLG problem.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Potential Applications</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The proposed framework unlocks substantial potential across various real-world applications, offering a versatile solution for tasks that rely on Human Activity Recognition (HAR). This framework’s applicability extends to tasks utilizing HAR as a foundational element, leveraging the power of large language models to enhance camera-based activities.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">A compelling use case is in the domain of exercise support, where vision-based activity recognition systems have previously been employed for repetition counting, <cite class="ltx_cite ltx_citemacro_citep">(Khurana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib10" title="">2018</a>; Ferreira et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib7" title="">2021</a>; Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib4" title="">2023</a>)</cite>. In this scenario, our framework could take on a pivotal role by facilitating personalization and providing nuanced insights into athletes’ movements and techniques. This personalized feedback fosters targeted improvements and empowers individuals to optimize their workout routines effectively.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Another example could focus on dance movements, that are characterized by their ever-changing nature. These movements stand to benefit significantly from our framework. Specifically, models capable of generalizing to novel gestures can play a crucial role in assessing, replicating, or refining choreographic steps <cite class="ltx_cite ltx_citemacro_citep">(Samanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib13" title="">2012</a>)</cite>. This can empower choreographers and dancers to fine-tune their performances, leading to more captivating and precisely executed routines.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">I many settings, to accomplish succesfull HAR, approaches have been rooted in sensor-based methodologies. Our approach offers vision-based methods a deeper contextual understanding of human activities. This innovation could reduce the reliance on multimodal sensor setups for identifying microgestures <cite class="ltx_cite ltx_citemacro_citep">(Sharma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib14" title="">2019</a>)</cite>. Moreover, it could extend to more intricate tasks such as cooking actions, where the interplay of visual cues and textual descriptions can enhance the recognition and understanding of cooking processes <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib16" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">In essence, the proposed framework transcends traditional boundaries, positioning itself as a catalyst for innovation across a spectrum of applications. By seamlessly integrating vision-based HAR with the capabilities of large language models, it is possible to facilitates enhanced personalization, deeper insights, and improved contextual understanding, thereby revolutionizing how activities are recognized, analyzed, and enhanced. By establishing a novel connection between visual cues and textual descriptions, our approach aims to push the boundaries of HAR accuracy and applicability, unlocking new horizons for pervasive computing and activity recognition research.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">While our approach has shown promising results, there remain several avenues for future research and development:

<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S6.p1.1.1">1. Semantic Understanding:</span> Enhancing the semantic understanding of the generated textual descriptions could lead to more contextually relevant and accurate activity labels. Investigating techniques to incorporate semantic knowledge into the NLG process is an interesting direction.

<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S6.p1.1.2">2. Temporal Modeling:</span> Exploring advanced temporal modeling techniques to capture longer-term dependencies within activity sequences could improve recognition accuracy, especially for activities with extended motion patterns.

<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S6.p1.1.3">3. Multi-modal Fusion:</span> Integrating other modalities such as audio or depth information alongside pose keypoints could provide a richer representation of human activities, potentially leading to more accurate recognition.

<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S6.p1.1.4">4. Real-time Applications:</span> Adapting the NLG-based HAR framework for real-time applications would be valuable. Developing strategies to maintain accuracy while achieving low-latency inference is a challenging yet important area of research.

<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S6.p1.1.5">5. Multi-person Activity Recognition:</span> Extending the framework to recognize activities involving multiple individuals could open up new possibilities, as many real-world scenarios involve interactions between multiple people.

<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S6.p1.1.6">6. Data Augmentation Techniques:</span> Exploring effective data augmentation techniques specific to pose keypoints could help improve model robustness and generalization.

<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S6.p1.1.7">7. Ethical Considerations:</span> Addressing ethical concerns related to privacy and bias in textual descriptions generated by the NLG model is a critical aspect that needs careful attention.

<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S6.p1.1.8">8. Transfer Learning and Scalability:</span> Investigating transfer learning techniques and model compression methods to make the framework more accessible and scalable for researchers with varying computational resources.

<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S6.p1.1.9">9. Benchmarking and Comparative Studies:</span> Conducting benchmarking and comparative studies against state-of-the-art HAR methods, both in terms of accuracy and computational efficiency, would provide a clearer understanding of the strengths and limitations of the NLG-based approach.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this research paper, we presented a novel framework for Human Activity Recognition (HAR) that capitalizes on the power of Natural Language Generation (NLG). By treating HAR as an NLG task, we harnessed the strengths of pose estimation models and large language models to generate textual descriptions of human activities based on pose keypoints. Our proof of concept study demonstrated the viability of this approach, showcasing its potential to enhance accuracy and interpretability in activity recognition. We have taken a significant step towards bridging the gap between visual data and textual descriptions, opening new avenues for research in the field of HAR. While our current approach may not possess semantic and temporal understanding, its applicability extends to identifying non-verbal cues through human keypoints. For instance, real-time feedback of non-verbal cues in clinical communication <cite class="ltx_cite ltx_citemacro_citep">(Hartzler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib8" title="">2014</a>; Bedmutha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="#bib.bib2" title="">2023</a>)</cite> is a potential application where our approach could contribute.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We thank our collaborators on the UnBIASED project, funding (NIH #1R01LM013301) for feedback and support.

</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bedmutha et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Manas Bedmutha, Emily
Bascom, Kimberly Sladek, Alexandra
Andreiu, Brian Wood, Wanda Pratt,
Janice Sabin, Andrea Hartzler, and
Nadir Weibel. 2023.

</span>
<span class="ltx_bibblock">Extracting Meaningful Social Signals Associated
with Bias from Patient-Provider Interactions to Improve Patient Care. In
<em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">2023 Annual Research Meeting</em>. AcademyHealth.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Joao Carreira, Eric
Noland, Chloe Hillier, and Andrew
Zisserman. 2022.

</span>
<span class="ltx_bibblock">A Short Note on the Kinetics-700 Human Action
Dataset.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1907.06987 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sheng-Hsien Cheng,
Muhammad Atif Sarwar, Yousef-Awwad
Daraghmi, Tsì-Uí İk, and
Yih-Lang Li. 2023.

</span>
<span class="ltx_bibblock">Periodic physical activity information
segmentation, counting and recognition from video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">IEEE Access</em> 11
(2023), 23019–23031.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dang et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
L Minh Dang, Kyungbok
Min, Hanxiang Wang, Md Jalil Piran,
Cheol Hee Lee, and Hyeonjoon Moon.
2020.

</span>
<span class="ltx_bibblock">Sensor-based and vision-based human activity
recognition: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Pattern Recognition</em> 108
(2020), 107561.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hao-Shu Fang, Jiefeng Li,
Hongyang Tang, Chao Xu,
Haoyi Zhu, Yuliang Xiu,
Yong-Lu Li, and Cewu Lu.
2022.

</span>
<span class="ltx_bibblock">AlphaPose: Whole-Body Regional Multi-Person Pose
Estimation and Tracking in Real-Time.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2211.03375 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferreira et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Bruno Ferreira, Pedro M
Ferreira, Gil Pinheiro, Nelson
Figueiredo, Filipe Carvalho, Paulo
Menezes, and Jorge Batista.
2021.

</span>
<span class="ltx_bibblock">Deep learning approaches for workout repetition
counting and validation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Pattern Recognition Letters</em>
151 (2021), 259–266.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartzler et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
AL Hartzler, RA Patel,
M Czerwinski, W Pratt, A
Roseway, N Chandrasekaran, and A
Back. 2014.

</span>
<span class="ltx_bibblock">Real-time feedback on nonverbal clinical
communication.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Methods of information in medicine</em>
53, 05 (2014),
389–405.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jena D. Hwang, Chandra
Bhagavatula, Ronan Le Bras, Jeff Da,
Keisuke Sakaguchi, Antoine Bosselut,
and Yejin Choi. 2021.

</span>
<span class="ltx_bibblock">COMET-ATOMIC 2020: On Symbolic and Neural
Commonsense Knowledge Graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">AAAI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khurana et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Rushil Khurana, Karan
Ahuja, Zac Yu, Jennifer Mankoff,
Chris Harrison, and Mayank Goel.
2018.

</span>
<span class="ltx_bibblock">GymCam: Detecting, recognizing and tracking
simultaneous exercises in unconstrained scenes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies</em> 2,
4 (2018), 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shuaicheng Li, Qianggang
Cao, Lingbo Liu, Kunlin Yang,
Shinan Liu, Jun Hou, and
Shuai Yi. 2021.

</span>
<span class="ltx_bibblock">Groupformer: Group activity recognition with
clustered spatial-temporal transformer. In
<em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the IEEE/CVF International
Conference on Computer Vision</em>. 13668–13677.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu,
Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever.
2019.

</span>
<span class="ltx_bibblock">Language Models are Unsupervised Multitask
Learners.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:160025533" title="">https://api.semanticscholar.org/CorpusID:160025533</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samanta et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Soumitra Samanta, Pulak
Purkait, and Bhabatosh Chanda.
2012.

</span>
<span class="ltx_bibblock">Indian classical dance classification by learning
dance pose bases. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">2012 IEEE Workshop on the
Applications of Computer Vision (WACV)</em>. IEEE, 265–270.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Adwait Sharma, Joan Sol
Roo, and Jürgen Steimle.
2019.

</span>
<span class="ltx_bibblock">Grasping microgestures: Eliciting single-hand
microgestures for handheld objects. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings
of the 2019 CHI Conference on Human Factors in Computing Systems</em>.
1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yi Wang, Kunchang Li,
Yizhuo Li, Yinan He,
Bingkun Huang, Zhiyu Zhao,
Hongjie Zhang, Jilan Xu,
Yi Liu, Zun Wang, Sen
Xing, Guo Chen, Junting Pan,
Jiashuo Yu, Yali Wang,
Limin Wang, and Yu Qiao.
2022.

</span>
<span class="ltx_bibblock">InternVideo: General Video Foundation Models via
Generative and Discriminative Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2212.03191 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xuhai Xu, Jiahao Li,
Tianyi Yuan, Liang He,
Xin Liu, Yukang Yan,
Yuntao Wang, Yuanchun Shi,
Jennifer Mankoff, and Anind K Dey.
2021.

</span>
<span class="ltx_bibblock">Hulamove: Using commodity imu for waist
interaction. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the 2021 CHI
Conference on Human Factors in Computing Systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Shugang Zhang, Zhiqiang
Wei, Jie Nie, Lei Huang,
Shuang Wang, Zhen Li, et al<span class="ltx_text" id="bib.bib17.3.1">.</span>
2017.

</span>
<span class="ltx_bibblock">A review on human activity recognition using
vision-based method.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.4.1">Journal of healthcare engineering</em>
2017 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Dec 12 03:55:36 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
