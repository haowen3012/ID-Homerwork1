<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey</title>
<!--Generated on Wed Jul  3 02:07:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.18844v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S1" title="In Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S1.SS1" title="In 1 Introduction ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S1.SS2" title="In 1 Introduction ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Scope of this survey</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S2" title="In Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S2.SS1" title="In 2 Background ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Sensors used for 3D HPE and HMR</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S2.SS1.SSS1" title="In 2.1 Sensors used for 3D HPE and HMR ‚Ä£ 2 Background ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Active sensors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S2.SS1.SSS2" title="In 2.1 Sensors used for 3D HPE and HMR ‚Ä£ 2 Background ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Passive sensors</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S2.SS2" title="In 2 Background ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Representation for human body</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S3" title="In Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Overview of Deep Learning for 3D HPE and HMR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4" title="In Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>3D Human Pose Estimation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.SS1" title="In 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Single person 3D pose estimation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.SS1.SSS1" title="In 4.1 Single person 3D pose estimation ‚Ä£ 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Single person 3D pose estimation in images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.SS1.SSS2" title="In 4.1 Single person 3D pose estimation ‚Ä£ 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Single person 3D pose estimation in videos</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.SS2" title="In 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Multi-person 3D pose estimation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.SS2.SSS1" title="In 4.2 Multi-person 3D pose estimation ‚Ä£ 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Top-down methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.SS2.SSS2" title="In 4.2 Multi-person 3D pose estimation ‚Ä£ 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Bottom-up methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.SS3" title="In 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Summary of 3D pose estimation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5" title="In Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>3D Human Mesh Recovery</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5.SS1" title="In 5 3D Human Mesh Recovery ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Template-based human mesh recovery</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5.SS1.SSS1" title="In 5.1 Template-based human mesh recovery ‚Ä£ 5 3D Human Mesh Recovery ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Naked human body recovery</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5.SS1.SSS2" title="In 5.1 Template-based human mesh recovery ‚Ä£ 5 3D Human Mesh Recovery ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Detailed human body recovery</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5.SS2" title="In 5 3D Human Mesh Recovery ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Template-free human body recovery</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5.SS3" title="In 5 3D Human Mesh Recovery ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Summary of human mesh recovery</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S6" title="In Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S6.SS1" title="In 6 Evaluation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Evaluation metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S6.SS2" title="In 6 Evaluation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S7" title="In Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Applications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S8" title="In Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Challenges and Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Liu
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Changzhen Qiu
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhiyong Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">School of Electronics and Communication Engineering, Sun Yat-sen University, Shenzhen, Guangdong, China
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">3D human pose estimation and mesh recovery have attracted widespread research interest in many areas, such as computer vision, autonomous driving, and robotics.
Deep learning on 3D human pose estimation and mesh recovery has recently thrived, with numerous methods proposed to address different problems in this area.
In this paper, to stimulate future research, we present a comprehensive review of recent progress over the past five years in deep learning methods for this area by delving into over 200 references.
To the best of our knowledge, this survey is arguably the first to comprehensively cover deep learning methods for 3D human pose estimation, including both single-person and multi-person approaches, as well as human mesh recovery, encompassing methods based on explicit models and implicit representations.
We also present comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.
A regularly updated project page can be found at <a class="ltx_ref ltx_href" href="https://github.com/liuyangme/SOTA-3DHPE-HMR" title="">https://github.com/liuyangme/SOTA-3DHPE-HMR</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Human pose estimation, 3D human pose, human mesh recovery, human reconstruction, deep learning, literature survey

</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id1"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">journal: </span>Neurocomputing</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Motivation</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Humans are the foremost actors in various social activities, and it is imperative to equip AI with an understanding of humans for contributing to society. Consequently, there are many human-centric tasks positioning the epicenter of research. Among them, 3D <span class="ltx_text ltx_font_bold" id="S1.SS1.p1.1.1">Human Pose Estimation (HPE)</span> and <span class="ltx_text ltx_font_bold" id="S1.SS1.p1.1.2">Human Mesh Recovery (HMR)</span> represent crucial tasks in the field of computer vision to interpret the status and behavior of humans in complex real-world environments.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">3D human pose estimation can accurately predict human body keypoint coordinates in three-dimensional space. This approach provides more comprehensive and accurate spatial information when compared to its 2D counterpart, thereby facilitating a better understanding of complex human behaviors in higher-level computer vision applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>. Human mesh recovery reconstructs a three-dimensional digital model of the body, which captures details such as shape <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, gestures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, clothing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, and facial expressions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, thus offering direct insights into human interactions with the physical world. 3D pose estimation and mesh recovery have a broad range of applications, such as security and surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, human-computer interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>, autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, and virtual reality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">With advanced deep learning technology, 3D human pose estimation and mesh recovery have garnered increasing attention in recent years. 3D pose estimation has evolved from concentrating on single individuals to encompassing multiple persons with more varied data inputs. In human mesh recovery, advancements have been made in terms of data inputs and in capturing more intricate details. The augmentation in explicit model parameters allows for a more nuanced representation of the human body, and the expansion in parameter types facilitates the portrayal of finer surfaces. With the progress of implicit rendering and its incorporation into human mesh recovery, more flexible body representations are achieved. However, both 3D pose estimation and mesh recovery face significant challenges, such as multi-person scenarios, self-occlusion issues, and the detailed reconstruction of bodies. Thus, a systematic and comprehensive review of recent advancements in 3D human pose estimation and mesh recovery is essential.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Scope of this survey</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">Over the past five years, numerous reviews have been on 3D human pose estimation and mesh recovery. The reviews <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> primarily concentrate on 2D and 3D pose estimation, yet they devote less attention to HMR-related literature. The survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> provides a thorough review exclusively dedicated to mesh recovery, focusing on methods based on explicit models. The survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> is distinctly centered on mesh recovery using implicit rendering techniques; however, it falls short in offering an exhaustive overview of the latest implicit rendering methods and systems for 3D pose estimation and mesh recovery. These reviews only marginally explore 3D pose estimation and mesh recovery technologies but scarcely delve into their applications in other computer vision tasks and future challenges.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.1"><em class="ltx_emph ltx_font_italic" id="S1.SS2.p2.1.1">This review primarily concentrates on deep learning approaches to 3D human pose estimation and human mesh recovery. 3D pose estimation, both in single-person and multi-person scenarios, is considered. In human mesh recovery, it methodically reviews techniques grounded in both explicit and implicit models. As illustrated in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S1.F1" title="Figure 1 ‚Ä£ 1.2 Scope of this survey ‚Ä£ 1 Introduction ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>, our survey comprehensively includes the most recent state-of-the-art publications (2019-2023) from mainstream computer vision conferences and journals.</em></p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="S1.F1.g1" src="x1.png" width="381"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Recent research of deep learning for 3D HPE and HMR.</figcaption>
</figure>
<div class="ltx_para" id="S1.SS2.p3">
<p class="ltx_p" id="S1.SS2.p3.1">The main contributions of this work compared to the existing literature can be summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.SS2.p4">
<p class="ltx_p" id="S1.SS2.p4.1">(1) To the best of our knowledge, this survey is arguably the first to comprehensively cover deep learning methods for 3D human pose estimation, including both single-person and multi-person approaches, as well as human mesh recovery, encompassing methods based on explicit models and implicit representations.</p>
</div>
<div class="ltx_para" id="S1.SS2.p5">
<p class="ltx_p" id="S1.SS2.p5.1">(2) Unlike existing reviews, we have not overlooked the role of implicit representations in the methods, particularly with the recent rapid advances in implicit rendering. This approach can produce detailed outputs, including clothed human figures with expressions, movements, and other intricacies essential for achieving photorealism.</p>
</div>
<div class="ltx_para" id="S1.SS2.p6">
<p class="ltx_p" id="S1.SS2.p6.1">(3) This paper comprehensively reviews the most recent developments in deep learning for 3D pose estimation and mesh recovery, providing readers with a detailed overview of cutting-edge methodologies. Additionally, it explores how these advancements contribute to various other computer vision tasks and delves into the challenges within this domain.</p>
</div>
<div class="ltx_para" id="S1.SS2.p7">
<p class="ltx_p" id="S1.SS2.p7.1">The structure of this paper is as follows: Section <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S2" title="2 Background ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a> introduces the sensor type and representation for the human body that have been widely used. Section <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S3" title="3 Overview of Deep Learning for 3D HPE and HMR ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">3</span></a> presents the overview of deep learning for 3D human pose estimation and mesh recovery. Section <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4" title="4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">4</span></a> surveys existing single person and multi-person 3D pose estimation methods. Section <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5" title="5 3D Human Mesh Recovery ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a> reviews the methods for human mesh recovery, including template-based and template-free methods. Section <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S6" title="6 Evaluation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">6</span></a> introduces the evaluation metrics and datasets for the respective tasks. Moreover, Section <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S7" title="7 Applications ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">7</span></a> discusses the applications, along with their impact on other computer vision tasks. Finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S8" title="8 Challenges and Conclusion ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">8</span></a> concludes the paper. Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S1.F2" title="Figure 2 ‚Ä£ 1.2 Scope of this survey ‚Ä£ 1 Introduction ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a> shows the taxonomy of deep learning methods for 3D HPE and HMR. Additionally, we host a regularly updated project page, which can be accessed at: <a class="ltx_ref ltx_href" href="https://github.com/liuyangme/SOTA-3DHPE-HMR" title="">https://github.com/liuyangme/SOTA-3DHPE-HMR</a>.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="S1.F2.g1" src="x2.png" width="653"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A taxonomy of deep learning methods for 3D HPE and HMR.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Sensors used for 3D HPE and HMR</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">There are a variety of sensors that can be used for 3D human pose estimation and mesh recovery, which are mainly categorized as active sensors and passive sensors.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Active sensors</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Active sensors operate by emitting a set of signals and measuring by detecting their reflections, such as Motion Capture (MoCap) systems with reflective markers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>, tactile sensing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, Time of Flight (ToF) cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, and Radio Frequency (RF) technologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>. MoCap and tactile devices are only suitable for cooperative targets. Active cameras generally cannot be used outdoors, and using multiple active devices simultaneously may lead to mutual interference. Thus, these devices have limited application scenarios.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Passive sensors</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Passive sensors do not actively emit any signals during the measurement; instead, they rely on signals from the objects or natural sources, including Inertial Measurement Units (IMUs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> and image sensors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>. Among these, RGB image sensors are particularly notable for being simple, user-friendly, adaptable to various environments, and capable of capturing high-resolution color images. Additionally, multiple RGB image sensors can be combined to form multi-view systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1">In this survey, considering the widespread applicability of RGB image sensors and the length limitations of the article, we focus on 3D human pose estimation and mesh recovery using RGB image sensors.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Representation for human body</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">3D pose estimators and mesh reconstructors can generate corresponding outcomes from the sensor input described previously. The 3D human pose estimation output comprises 3D coordinates detailing positions and joint orientations of the human body, representing the spatial location of each joint (e.g., head, neck, shoulders, elbows, knees), and the skeleton maps the interconnections between joints. Typically, these coordinates are expressed in a global coordinate system or relative to the camera‚Äôs coordinate system. A keypoints tree structure is commonly employed to illustrate the human pose, where the tree nodes represent the joints and the edges denote the connections between joints.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.2">On the other hand, the skeleton-based human pose representation method does not provide detailed information about the body‚Äôs surface. The output of human mesh recovery is generally a 3D body model, which encompasses a comprehensive depiction of the body‚Äôs shape and surface details and offers a more enriched representation. Statistical-based models are widely used in human mesh representations, such as SCAPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> and SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>. SMPL is a learnable skin-vertex model representing the human body as a 3D mesh with a topological structure. The pose and shape of the body are described by pose parameters <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.1d">italic_Œ∏</annotation></semantics></math> and shape parameters <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS2.p2.2.m2.1"><semantics id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">Œ≤</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">ùõΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.2.m2.1d">italic_Œ≤</annotation></semantics></math>. The pose parameters control the joint angles and global posture, and the shape parameters determine the body‚Äôs shape. There are several models based on SMPL to expand the representational capabilities, such as the MANO model (SMPL+H) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> with hand representation and the FLAME model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> with facial representation. SMPL-X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> is a comprehensive model that simultaneously captures the human body, face, and hands by incorporating the FLAME head model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> and the MANO hand model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>. H4D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> can represent the dynamic human shape and pose by building upon the prior knowledge from the SMPL model and extending its capabilities by incorporating a temporal dimension.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In addition to the explicit model representations mentioned above, recent years have seen the development of methods based on implicit models, which offer a more flexible representation of the human body through non-parametric models. The voxel-based model is suitable for volume rendering and physical simulation. However, its resolution is limited by the size of the voxels, which may result in larger storage requirements. Varol et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> proposed an end-to-end network architecture, BodyNet, capable of predicting the volumetric shape of the human body from a single image. Implicit reconstruction transforms inputs into implicit function representations, facilitating the reconstruction of high-quality three-dimensional mesh models from irregular and noisy data. To address the ill-posed problem of reconstructing a 3D mesh from images, Onizuka et al. proposed the TetraTSDF (Tetrahedral Truncated Signed Distance Function) model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>. TetraTSDF is a tetrahedral representation model capable of accurately recovering intricate human body shapes, even when the subject is wearing loose clothing. Zheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> designed the Parametric Model-Conditioned Implicit Representation (PaMIR), which utilizes the 2D image feature map and corresponding SMPL feature volume to generate an implicit surface representation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Overview of Deep Learning for 3D HPE and HMR</h2>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="S3.F3.g1" src="x3.png" width="382"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A basic framework of deep learning for 3D HPE and HMR.</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section provides an overview of the deep learning framework for 3D human pose estimation and mesh recovery. As shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S3.F3" title="Figure 3 ‚Ä£ 3 Overview of Deep Learning for 3D HPE and HMR ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">3</span></a>, there are four components in deep learning-based systems. First, data are collected from various sensors, including RGB images, depth images, RF signals, IMUs, and so on. Second, the input data are processed through a deep learning model, which typically consists of an encoder and a decoder. The encoder extracts representational features from the input data, such as those obtained using architectures like ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> or HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>. The decoder, which could be based on frameworks like MLP (Multi-Layer Perceptron) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> or Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>, then outputs the body‚Äôs 3D pose and reconstruction model. This structure enables the model to efficiently process and translate complex input data into detailed and accurate 3D representations of human pose and mesh. Thirdly, various learning methods can be selected during the model learning process. In addition to fully supervised learning, to alleviate data dependency approaches such as weakly supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, unsupervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>, and few-shot learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> are employed. To reduce the model size, techniques such as knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">53</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>, model pruning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>, and parameter quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite> can be applied. Furthermore, methodologies such as meta-learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> and reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> can also be incorporated, allowing the model to adapt to different scenarios and data constraints. Finally, the deep learning model outputs the results of 3D human pose estimation and mesh recovery. These results can be represented in various forms, including keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite>, mesh <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite>, and voxels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">62</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>. These representations contribute to a comprehensive understanding of the human body in three dimensions. In the following sections, we will delve into the specifics of 3D human pose estimation and mesh recovery, categorizing and elaborating on each aspect.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>3D Human Pose Estimation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">3D human pose estimation can provide a more accurate pose by predicting the depth information of body keypoints, but it is much more challenging than 2D pose estimation. 3D pose estimation can be classified into single-person and multi-person estimation, according to the number of targets. Single-person 3D pose estimation has seen rapid progress due to the fast development of deep learning technology, but it still faces many challenges, such as efficiency and the invisibility of certain body parts. Multi-person estimation in crowded scenes is even more challenging because of the interaction and occlusion between bodies and scenes. In this section, we will detail the research progress in this field, categorizing it into two types. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.T1" title="Table 1 ‚Ä£ 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes all the representative methods.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overview of 3D human pose estimation.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.52" style="width:433.6pt;height:502.5pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-212.2pt,245.7pt) scale(0.505367236817744,0.505367236817744) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.52.52">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.52.52.53.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3" id="S4.T1.52.52.53.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.52.52.53.1.1.1">Motivations</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.52.52.53.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.52.52.53.1.2.1">Methods</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2" rowspan="39" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Single Person</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3" rowspan="17" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">in Images</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">Solving Depth Ambiguity</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">‚àô</annotation></semantics></math> Optical-aware: VI-HC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>, Ray3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.2.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.2.2.2.1.m1.1"><semantics id="S4.T1.2.2.2.1.m1.1a"><mo id="S4.T1.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.1.m1.1d">‚àô</annotation></semantics></math> Appropriate feature representation: HEMlets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.3.3.2" rowspan="4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.3.2.1">Solving Body Structure Understanding</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.3.3.3.1.m1.1"><semantics id="S4.T1.3.3.3.1.m1.1a"><mo id="S4.T1.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.m1.1b"><ci id="S4.T1.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.1.m1.1d">‚àô</annotation></semantics></math> Joint aware: JRAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.4.4.4.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.4.4.4.1.m1.1"><semantics id="S4.T1.4.4.4.1.m1.1a"><mo id="S4.T1.4.4.4.1.m1.1.1" xref="S4.T1.4.4.4.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.1.m1.1b"><ci id="S4.T1.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.4.1.m1.1d">‚àô</annotation></semantics></math> Limb aware: Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite>, Deep grammar network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.5.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.5.5.5.1.m1.1"><semantics id="S4.T1.5.5.5.1.m1.1a"><mo id="S4.T1.5.5.5.1.m1.1.1" xref="S4.T1.5.5.5.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.1.m1.1b"><ci id="S4.T1.5.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.5.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.5.1.m1.1d">‚àô</annotation></semantics></math> Orientation keypoints: Fisch et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.6.6.6.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.6.6.6.1.m1.1"><semantics id="S4.T1.6.6.6.1.m1.1a"><mo id="S4.T1.6.6.6.1.m1.1.1" xref="S4.T1.6.6.6.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.1.m1.1b"><ci id="S4.T1.6.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.6.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.6.1.m1.1d">‚àô</annotation></semantics></math> Graph-based: Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite>, LCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite>, Modulated-GCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite>, Skeletal-GNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite>, HopFIR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a>]</cite>, RS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.7.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.7.7.7.2" rowspan="7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.7.7.7.2.1">Solving Occlusion Problems</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.7.7.7.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.7.7.7.1.m1.1"><semantics id="S4.T1.7.7.7.1.m1.1a"><mo id="S4.T1.7.7.7.1.m1.1.1" xref="S4.T1.7.7.7.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.1.m1.1b"><ci id="S4.T1.7.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.7.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.7.7.1.m1.1d">‚àô</annotation></semantics></math> Learnable-triangulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.8.8.8.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.8.8.8.1.m1.1"><semantics id="S4.T1.8.8.8.1.m1.1a"><mo id="S4.T1.8.8.8.1.m1.1.1" xref="S4.T1.8.8.8.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.1.m1.1b"><ci id="S4.T1.8.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.8.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.8.8.1.m1.1d">‚àô</annotation></semantics></math> RPSM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.9.9.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.9.9.9.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.9.9.9.1.m1.1"><semantics id="S4.T1.9.9.9.1.m1.1a"><mo id="S4.T1.9.9.9.1.m1.1.1" xref="S4.T1.9.9.9.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.1.m1.1b"><ci id="S4.T1.9.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.9.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.9.9.1.m1.1d">‚àô</annotation></semantics></math> Lightweight multi-view <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.10.10.10.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.10.10.10.1.m1.1"><semantics id="S4.T1.10.10.10.1.m1.1a"><mo id="S4.T1.10.10.10.1.m1.1.1" xref="S4.T1.10.10.10.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.1.m1.1b"><ci id="S4.T1.10.10.10.1.m1.1.1.cmml" xref="S4.T1.10.10.10.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.10.10.1.m1.1d">‚àô</annotation></semantics></math> AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.11.11.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.11.11.11.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.11.11.11.1.m1.1"><semantics id="S4.T1.11.11.11.1.m1.1a"><mo id="S4.T1.11.11.11.1.m1.1.1" xref="S4.T1.11.11.11.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.11.1.m1.1b"><ci id="S4.T1.11.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.11.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.11.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.11.11.1.m1.1d">‚àô</annotation></semantics></math> Bartol et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.12.12.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.12.12.12.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.12.12.12.1.m1.1"><semantics id="S4.T1.12.12.12.1.m1.1a"><mo id="S4.T1.12.12.12.1.m1.1.1" xref="S4.T1.12.12.12.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.12.1.m1.1b"><ci id="S4.T1.12.12.12.1.m1.1.1.cmml" xref="S4.T1.12.12.12.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.12.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.12.12.1.m1.1d">‚àô</annotation></semantics></math> 3D pose consensus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.13.13.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.13.13.13.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.13.13.13.1.m1.1"><semantics id="S4.T1.13.13.13.1.m1.1a"><mo id="S4.T1.13.13.13.1.m1.1.1" xref="S4.T1.13.13.13.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.13.1.m1.1b"><ci id="S4.T1.13.13.13.1.m1.1.1.cmml" xref="S4.T1.13.13.13.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.13.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.13.13.13.1.m1.1d">‚àô</annotation></semantics></math> Probabilistic triangulation module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.14.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.14.14.14.2" rowspan="4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.14.14.14.2.1">Solving Data Lacking</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.14.14.14.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.14.14.14.1.m1.1"><semantics id="S4.T1.14.14.14.1.m1.1a"><mo id="S4.T1.14.14.14.1.m1.1.1" xref="S4.T1.14.14.14.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.14.1.m1.1b"><ci id="S4.T1.14.14.14.1.m1.1.1.cmml" xref="S4.T1.14.14.14.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.14.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.14.14.14.1.m1.1d">‚àô</annotation></semantics></math> Unsupervised learning: Kudo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">82</span></a>]</cite>, ElePose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.15.15.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.15.15.15.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.15.15.15.1.m1.1"><semantics id="S4.T1.15.15.15.1.m1.1a"><mo id="S4.T1.15.15.15.1.m1.1.1" xref="S4.T1.15.15.15.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.15.1.m1.1b"><ci id="S4.T1.15.15.15.1.m1.1.1.cmml" xref="S4.T1.15.15.15.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.15.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.15.15.15.1.m1.1d">‚àô</annotation></semantics></math> Self-supervised learning: EpipolarPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib84" title=""><span class="ltx_text" style="font-size:90%;">84</span></a>]</cite>, Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">85</span></a>]</cite>, MRP-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib86" title=""><span class="ltx_text" style="font-size:90%;">86</span></a>]</cite>, PoseTriplet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.16.16.16">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.16.16.16.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.16.16.16.1.m1.1"><semantics id="S4.T1.16.16.16.1.m1.1a"><mo id="S4.T1.16.16.16.1.m1.1.1" xref="S4.T1.16.16.16.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.16.1.m1.1b"><ci id="S4.T1.16.16.16.1.m1.1.1.cmml" xref="S4.T1.16.16.16.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.16.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.16.16.16.1.m1.1d">‚àô</annotation></semantics></math> Weakly-supervised learning: Hua et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib87" title=""><span class="ltx_text" style="font-size:90%;">87</span></a>]</cite>, CameraPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.17.17.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.17.17.17.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.17.17.17.1.m1.1"><semantics id="S4.T1.17.17.17.1.m1.1a"><mo id="S4.T1.17.17.17.1.m1.1.1" xref="S4.T1.17.17.17.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.17.17.17.1.m1.1b"><ci id="S4.T1.17.17.17.1.m1.1.1.cmml" xref="S4.T1.17.17.17.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.17.17.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.17.17.17.1.m1.1d">‚àô</annotation></semantics></math> Transfer learning: Adaptpose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib88" title=""><span class="ltx_text" style="font-size:90%;">88</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.18.18.18">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.18.18.18.2" rowspan="22" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.18.18.18.2.1">in Videos</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.18.18.18.3" rowspan="8" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.18.18.18.3.1">Solving Single-frame Limitation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.18.18.18.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.18.18.18.1.m1.1"><semantics id="S4.T1.18.18.18.1.m1.1a"><mo id="S4.T1.18.18.18.1.m1.1.1" xref="S4.T1.18.18.18.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.18.18.18.1.m1.1b"><ci id="S4.T1.18.18.18.1.m1.1.1.cmml" xref="S4.T1.18.18.18.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.18.18.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.18.18.18.1.m1.1d">‚àô</annotation></semantics></math> VideoPose3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib89" title=""><span class="ltx_text" style="font-size:90%;">89</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.19.19">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.19.19.19.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.19.19.19.1.m1.1"><semantics id="S4.T1.19.19.19.1.m1.1a"><mo id="S4.T1.19.19.19.1.m1.1.1" xref="S4.T1.19.19.19.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.19.19.19.1.m1.1b"><ci id="S4.T1.19.19.19.1.m1.1.1.cmml" xref="S4.T1.19.19.19.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.19.19.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.19.19.19.1.m1.1d">‚àô</annotation></semantics></math> PoseFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.20.20.20">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.20.20.20.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.20.20.20.1.m1.1"><semantics id="S4.T1.20.20.20.1.m1.1a"><mo id="S4.T1.20.20.20.1.m1.1.1" xref="S4.T1.20.20.20.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.20.20.20.1.m1.1b"><ci id="S4.T1.20.20.20.1.m1.1.1.cmml" xref="S4.T1.20.20.20.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.20.20.20.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.20.20.20.1.m1.1d">‚àô</annotation></semantics></math> UniPose+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib91" title=""><span class="ltx_text" style="font-size:90%;">91</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.21.21.21">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.21.21.21.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.21.21.21.1.m1.1"><semantics id="S4.T1.21.21.21.1.m1.1a"><mo id="S4.T1.21.21.21.1.m1.1.1" xref="S4.T1.21.21.21.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.21.21.21.1.m1.1b"><ci id="S4.T1.21.21.21.1.m1.1.1.cmml" xref="S4.T1.21.21.21.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.21.21.21.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.21.21.21.1.m1.1d">‚àô</annotation></semantics></math> MHFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib92" title=""><span class="ltx_text" style="font-size:90%;">92</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.22.22.22">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.22.22.22.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.22.22.22.1.m1.1"><semantics id="S4.T1.22.22.22.1.m1.1a"><mo id="S4.T1.22.22.22.1.m1.1.1" xref="S4.T1.22.22.22.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.22.22.22.1.m1.1b"><ci id="S4.T1.22.22.22.1.m1.1.1.cmml" xref="S4.T1.22.22.22.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.22.22.22.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.22.22.22.1.m1.1d">‚àô</annotation></semantics></math> MixSTE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib93" title=""><span class="ltx_text" style="font-size:90%;">93</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.23.23.23">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.23.23.23.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.23.23.23.1.m1.1"><semantics id="S4.T1.23.23.23.1.m1.1a"><mo id="S4.T1.23.23.23.1.m1.1.1" xref="S4.T1.23.23.23.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.23.23.23.1.m1.1b"><ci id="S4.T1.23.23.23.1.m1.1.1.cmml" xref="S4.T1.23.23.23.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.23.23.23.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.23.23.23.1.m1.1d">‚àô</annotation></semantics></math> Honari et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib94" title=""><span class="ltx_text" style="font-size:90%;">94</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.24.24.24">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.24.24.24.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.24.24.24.1.m1.1"><semantics id="S4.T1.24.24.24.1.m1.1a"><mo id="S4.T1.24.24.24.1.m1.1.1" xref="S4.T1.24.24.24.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.24.24.24.1.m1.1b"><ci id="S4.T1.24.24.24.1.m1.1.1.cmml" xref="S4.T1.24.24.24.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.24.24.24.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.24.24.24.1.m1.1d">‚àô</annotation></semantics></math> HSTFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib95" title=""><span class="ltx_text" style="font-size:90%;">95</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.25.25.25">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.25.25.25.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.25.25.25.1.m1.1"><semantics id="S4.T1.25.25.25.1.m1.1a"><mo id="S4.T1.25.25.25.1.m1.1.1" xref="S4.T1.25.25.25.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.25.25.25.1.m1.1b"><ci id="S4.T1.25.25.25.1.m1.1.1.cmml" xref="S4.T1.25.25.25.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.25.25.25.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.25.25.25.1.m1.1d">‚àô</annotation></semantics></math> STCFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib96" title=""><span class="ltx_text" style="font-size:90%;">96</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.26.26.26">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.26.26.26.2" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.26.26.26.2.1">Solving Real-time Problems</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.26.26.26.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.26.26.26.1.m1.1"><semantics id="S4.T1.26.26.26.1.m1.1a"><mo id="S4.T1.26.26.26.1.m1.1.1" xref="S4.T1.26.26.26.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.26.26.26.1.m1.1b"><ci id="S4.T1.26.26.26.1.m1.1.1.cmml" xref="S4.T1.26.26.26.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.26.26.26.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.26.26.26.1.m1.1d">‚àô</annotation></semantics></math> Temporally sparse sampling: Einfalt et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.27.27.27">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.27.27.27.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.27.27.27.1.m1.1"><semantics id="S4.T1.27.27.27.1.m1.1a"><mo id="S4.T1.27.27.27.1.m1.1.1" xref="S4.T1.27.27.27.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.27.27.27.1.m1.1b"><ci id="S4.T1.27.27.27.1.m1.1.1.cmml" xref="S4.T1.27.27.27.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.27.27.27.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.27.27.27.1.m1.1d">‚àô</annotation></semantics></math> Spatio-temporal sparse sampling: MixSynthFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib97" title=""><span class="ltx_text" style="font-size:90%;">97</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.28.28.28">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.28.28.28.2" rowspan="4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.28.28.28.2.1">Solving Body Structure Understanding</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.28.28.28.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.28.28.28.1.m1.1"><semantics id="S4.T1.28.28.28.1.m1.1a"><mo id="S4.T1.28.28.28.1.m1.1.1" xref="S4.T1.28.28.28.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.28.28.28.1.m1.1b"><ci id="S4.T1.28.28.28.1.m1.1.1.cmml" xref="S4.T1.28.28.28.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.28.28.28.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.28.28.28.1.m1.1d">‚àô</annotation></semantics></math> Motion loss: Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib98" title=""><span class="ltx_text" style="font-size:90%;">98</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.29.29.29">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.29.29.29.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.29.29.29.1.m1.1"><semantics id="S4.T1.29.29.29.1.m1.1a"><mo id="S4.T1.29.29.29.1.m1.1.1" xref="S4.T1.29.29.29.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.29.29.29.1.m1.1b"><ci id="S4.T1.29.29.29.1.m1.1.1.cmml" xref="S4.T1.29.29.29.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.29.29.29.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.29.29.29.1.m1.1d">‚àô</annotation></semantics></math> Human-joint affinity: DG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib99" title=""><span class="ltx_text" style="font-size:90%;">99</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.30.30.30">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.30.30.30.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.30.30.30.1.m1.1"><semantics id="S4.T1.30.30.30.1.m1.1a"><mo id="S4.T1.30.30.30.1.m1.1.1" xref="S4.T1.30.30.30.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.30.30.30.1.m1.1b"><ci id="S4.T1.30.30.30.1.m1.1.1.cmml" xref="S4.T1.30.30.30.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.30.30.30.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.30.30.30.1.m1.1d">‚àô</annotation></semantics></math> Anatomy-aware: Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib100" title=""><span class="ltx_text" style="font-size:90%;">100</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.31.31.31">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.31.31.31.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.31.31.31.1.m1.1"><semantics id="S4.T1.31.31.31.1.m1.1a"><mo id="S4.T1.31.31.31.1.m1.1.1" xref="S4.T1.31.31.31.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.31.31.31.1.m1.1b"><ci id="S4.T1.31.31.31.1.m1.1.1.cmml" xref="S4.T1.31.31.31.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.31.31.31.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.31.31.31.1.m1.1d">‚àô</annotation></semantics></math> Part aware attention: Xue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib101" title=""><span class="ltx_text" style="font-size:90%;">101</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.32.32.32">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.32.32.32.2" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.32.32.32.2.1">Solving Occlusion Problems</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.32.32.32.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.32.32.32.1.m1.1"><semantics id="S4.T1.32.32.32.1.m1.1a"><mo id="S4.T1.32.32.32.1.m1.1.1" xref="S4.T1.32.32.32.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.32.32.32.1.m1.1b"><ci id="S4.T1.32.32.32.1.m1.1.1.cmml" xref="S4.T1.32.32.32.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.32.32.32.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.32.32.32.1.m1.1d">‚àô</annotation></semantics></math> Optical-flow consistency constraint: Cheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib102" title=""><span class="ltx_text" style="font-size:90%;">102</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.33.33.33">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.33.33.33.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.33.33.33.1.m1.1"><semantics id="S4.T1.33.33.33.1.m1.1a"><mo id="S4.T1.33.33.33.1.m1.1.1" xref="S4.T1.33.33.33.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.33.33.33.1.m1.1b"><ci id="S4.T1.33.33.33.1.m1.1.1.cmml" xref="S4.T1.33.33.33.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.33.33.33.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.33.33.33.1.m1.1d">‚àô</annotation></semantics></math> Multi-view: MTF-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.34.34.34">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.34.34.34.2" rowspan="6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.34.34.34.2.1">Solving Data Lacking</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.34.34.34.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.34.34.34.1.m1.1"><semantics id="S4.T1.34.34.34.1.m1.1a"><mo id="S4.T1.34.34.34.1.m1.1.1" xref="S4.T1.34.34.34.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.34.34.34.1.m1.1b"><ci id="S4.T1.34.34.34.1.m1.1.1.cmml" xref="S4.T1.34.34.34.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.34.34.34.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.34.34.34.1.m1.1d">‚àô</annotation></semantics></math> Unsupervised learning: Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib103" title=""><span class="ltx_text" style="font-size:90%;">103</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.35.35.35">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.35.35.35.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.35.35.35.1.m1.1"><semantics id="S4.T1.35.35.35.1.m1.1a"><mo id="S4.T1.35.35.35.1.m1.1.1" xref="S4.T1.35.35.35.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.35.35.35.1.m1.1b"><ci id="S4.T1.35.35.35.1.m1.1.1.cmml" xref="S4.T1.35.35.35.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.35.35.35.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.35.35.35.1.m1.1d">‚àô</annotation></semantics></math> Weakly-supervised learning: Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib104" title=""><span class="ltx_text" style="font-size:90%;">104</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.36.36.36">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.36.36.36.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.36.36.36.1.m1.1"><semantics id="S4.T1.36.36.36.1.m1.1a"><mo id="S4.T1.36.36.36.1.m1.1.1" xref="S4.T1.36.36.36.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.36.36.36.1.m1.1b"><ci id="S4.T1.36.36.36.1.m1.1.1.cmml" xref="S4.T1.36.36.36.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.36.36.36.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.36.36.36.1.m1.1d">‚àô</annotation></semantics></math> Semi-supervised learning: MCSS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib105" title=""><span class="ltx_text" style="font-size:90%;">105</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.37.37.37">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.37.37.37.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.37.37.37.1.m1.1"><semantics id="S4.T1.37.37.37.1.m1.1a"><mo id="S4.T1.37.37.37.1.m1.1.1" xref="S4.T1.37.37.37.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.37.37.37.1.m1.1b"><ci id="S4.T1.37.37.37.1.m1.1.1.cmml" xref="S4.T1.37.37.37.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.37.37.37.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.37.37.37.1.m1.1d">‚àô</annotation></semantics></math> Self-supervised learning: Kundu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib106" title=""><span class="ltx_text" style="font-size:90%;">106</span></a>]</cite>, P-STMO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib107" title=""><span class="ltx_text" style="font-size:90%;">107</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.38.38.38">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.38.38.38.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.38.38.38.1.m1.1"><semantics id="S4.T1.38.38.38.1.m1.1a"><mo id="S4.T1.38.38.38.1.m1.1.1" xref="S4.T1.38.38.38.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.38.38.38.1.m1.1b"><ci id="S4.T1.38.38.38.1.m1.1.1.cmml" xref="S4.T1.38.38.38.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.38.38.38.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.38.38.38.1.m1.1d">‚àô</annotation></semantics></math> Meta-learning: Cho et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.39.39.39">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.39.39.39.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.39.39.39.1.m1.1"><semantics id="S4.T1.39.39.39.1.m1.1a"><mo id="S4.T1.39.39.39.1.m1.1.1" xref="S4.T1.39.39.39.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.39.39.39.1.m1.1b"><ci id="S4.T1.39.39.39.1.m1.1.1.cmml" xref="S4.T1.39.39.39.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.39.39.39.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.39.39.39.1.m1.1d">‚àô</annotation></semantics></math> Data augmentation: PoseAug <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib108" title=""><span class="ltx_text" style="font-size:90%;">108</span></a>]</cite>, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib109" title=""><span class="ltx_text" style="font-size:90%;">109</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.40.40.40">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.40.40.40.2" rowspan="13" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.40.40.40.2.1">Multi-person</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.40.40.40.3" rowspan="6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.40.40.40.3.1">Top-down</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.40.40.40.4" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.40.40.40.4.1">Solving Real-time Problems</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.40.40.40.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.40.40.40.1.m1.1"><semantics id="S4.T1.40.40.40.1.m1.1a"><mo id="S4.T1.40.40.40.1.m1.1.1" xref="S4.T1.40.40.40.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.40.40.40.1.m1.1b"><ci id="S4.T1.40.40.40.1.m1.1.1.cmml" xref="S4.T1.40.40.40.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.40.40.40.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.40.40.40.1.m1.1d">‚àô</annotation></semantics></math> Multi-view: Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib110" title=""><span class="ltx_text" style="font-size:90%;">110</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.41.41.41">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.41.41.41.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.41.41.41.1.m1.1"><semantics id="S4.T1.41.41.41.1.m1.1a"><mo id="S4.T1.41.41.41.1.m1.1.1" xref="S4.T1.41.41.41.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.41.41.41.1.m1.1b"><ci id="S4.T1.41.41.41.1.m1.1.1.cmml" xref="S4.T1.41.41.41.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.41.41.41.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.41.41.41.1.m1.1d">‚àô</annotation></semantics></math> Whole body: AlphaPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib111" title=""><span class="ltx_text" style="font-size:90%;">111</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.42.42.42">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.42.42.42.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.42.42.42.2.1">Solving Representation Limitation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.42.42.42.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.42.42.42.1.m1.1"><semantics id="S4.T1.42.42.42.1.m1.1a"><mo id="S4.T1.42.42.42.1.m1.1.1" xref="S4.T1.42.42.42.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.42.42.42.1.m1.1b"><ci id="S4.T1.42.42.42.1.m1.1.1.cmml" xref="S4.T1.42.42.42.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.42.42.42.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.42.42.42.1.m1.1d">‚àô</annotation></semantics></math> VoxelTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.43.43.43">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.43.43.43.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.43.43.43.2.1">Solving Occlusion Problems</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.43.43.43.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.43.43.43.1.m1.1"><semantics id="S4.T1.43.43.43.1.m1.1a"><mo id="S4.T1.43.43.43.1.m1.1.1" xref="S4.T1.43.43.43.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.43.43.43.1.m1.1b"><ci id="S4.T1.43.43.43.1.m1.1.1.cmml" xref="S4.T1.43.43.43.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.43.43.43.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.43.43.43.1.m1.1d">‚àô</annotation></semantics></math> Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib112" title=""><span class="ltx_text" style="font-size:90%;">112</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.44.44.44">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.44.44.44.2" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.44.44.44.2.1">Solving Data Lacking</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.44.44.44.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.44.44.44.1.m1.1"><semantics id="S4.T1.44.44.44.1.m1.1a"><mo id="S4.T1.44.44.44.1.m1.1.1" xref="S4.T1.44.44.44.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.44.44.44.1.m1.1b"><ci id="S4.T1.44.44.44.1.m1.1.1.cmml" xref="S4.T1.44.44.44.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.44.44.44.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.44.44.44.1.m1.1d">‚àô</annotation></semantics></math> Single-shot: PandaNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.45.45.45">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.45.45.45.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.45.45.45.1.m1.1"><semantics id="S4.T1.45.45.45.1.m1.1a"><mo id="S4.T1.45.45.45.1.m1.1.1" xref="S4.T1.45.45.45.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.45.45.45.1.m1.1b"><ci id="S4.T1.45.45.45.1.m1.1.1.cmml" xref="S4.T1.45.45.45.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.45.45.45.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.45.45.45.1.m1.1d">‚àô</annotation></semantics></math> Optical-aware: Moon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib113" title=""><span class="ltx_text" style="font-size:90%;">113</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.46.46.46">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.46.46.46.2" rowspan="5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.46.46.46.2.1">Bottom-up</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.46.46.46.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.46.46.46.3.1">Solving Real-time Problems</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.46.46.46.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.46.46.46.1.m1.1"><semantics id="S4.T1.46.46.46.1.m1.1a"><mo id="S4.T1.46.46.46.1.m1.1.1" xref="S4.T1.46.46.46.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.46.46.46.1.m1.1b"><ci id="S4.T1.46.46.46.1.m1.1.1.cmml" xref="S4.T1.46.46.46.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.46.46.46.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.46.46.46.1.m1.1d">‚àô</annotation></semantics></math> Fabbri et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib114" title=""><span class="ltx_text" style="font-size:90%;">114</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.47.47.47">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.47.47.47.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.47.47.47.2.1">Solving Supervisory Limitation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.47.47.47.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.47.47.47.1.m1.1"><semantics id="S4.T1.47.47.47.1.m1.1a"><mo id="S4.T1.47.47.47.1.m1.1.1" xref="S4.T1.47.47.47.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.47.47.47.1.m1.1b"><ci id="S4.T1.47.47.47.1.m1.1.1.cmml" xref="S4.T1.47.47.47.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.47.47.47.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.47.47.47.1.m1.1d">‚àô</annotation></semantics></math> HMOR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib115" title=""><span class="ltx_text" style="font-size:90%;">115</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.48.48.48">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.48.48.48.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.48.48.48.2.1">Solving Data Lacking</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.48.48.48.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.48.48.48.1.m1.1"><semantics id="S4.T1.48.48.48.1.m1.1a"><mo id="S4.T1.48.48.48.1.m1.1.1" xref="S4.T1.48.48.48.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.48.48.48.1.m1.1b"><ci id="S4.T1.48.48.48.1.m1.1.1.cmml" xref="S4.T1.48.48.48.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.48.48.48.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.48.48.48.1.m1.1d">‚àô</annotation></semantics></math> Single-shot: SMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib116" title=""><span class="ltx_text" style="font-size:90%;">116</span></a>]</cite>, Benzine et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib117" title=""><span class="ltx_text" style="font-size:90%;">117</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.49.49.49">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.49.49.49.2" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.49.49.49.2.1">Solving Occlusion Problems</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.49.49.49.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.49.49.49.1.m1.1"><semantics id="S4.T1.49.49.49.1.m1.1a"><mo id="S4.T1.49.49.49.1.m1.1.1" xref="S4.T1.49.49.49.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.49.49.49.1.m1.1b"><ci id="S4.T1.49.49.49.1.m1.1.1.cmml" xref="S4.T1.49.49.49.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.49.49.49.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.49.49.49.1.m1.1d">‚àô</annotation></semantics></math> Mehta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib118" title=""><span class="ltx_text" style="font-size:90%;">118</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.50.50.50">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.50.50.50.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.50.50.50.1.m1.1"><semantics id="S4.T1.50.50.50.1.m1.1a"><mo id="S4.T1.50.50.50.1.m1.1.1" xref="S4.T1.50.50.50.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.50.50.50.1.m1.1b"><ci id="S4.T1.50.50.50.1.m1.1.1.cmml" xref="S4.T1.50.50.50.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.50.50.50.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.50.50.50.1.m1.1d">‚àô</annotation></semantics></math> LCR-Net++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib119" title=""><span class="ltx_text" style="font-size:90%;">119</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.51.51.51">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.51.51.51.2" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.51.51.51.2.1">Others</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.51.51.51.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.51.51.51.3.1">Single Stage</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.51.51.51.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.51.51.51.1.m1.1"><semantics id="S4.T1.51.51.51.1.m1.1a"><mo id="S4.T1.51.51.51.1.m1.1.1" xref="S4.T1.51.51.51.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.51.51.51.1.m1.1b"><ci id="S4.T1.51.51.51.1.m1.1.1.cmml" xref="S4.T1.51.51.51.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.51.51.51.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.51.51.51.1.m1.1d">‚àô</annotation></semantics></math> Jin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib120" title=""><span class="ltx_text" style="font-size:90%;">120</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.52.52.52">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.52.52.52.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.52.52.52.2.1">Top-down + Bottom-up</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.52.52.52.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S4.T1.52.52.52.1.m1.1"><semantics id="S4.T1.52.52.52.1.m1.1a"><mo id="S4.T1.52.52.52.1.m1.1.1" xref="S4.T1.52.52.52.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.T1.52.52.52.1.m1.1b"><ci id="S4.T1.52.52.52.1.m1.1.1.cmml" xref="S4.T1.52.52.52.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.52.52.52.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.T1.52.52.52.1.m1.1d">‚àô</annotation></semantics></math> Cheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib121" title=""><span class="ltx_text" style="font-size:90%;">121</span></a>]</cite>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Single person 3D pose estimation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">As illustrated in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.F4" title="Figure 4 ‚Ä£ 4.1 Single person 3D pose estimation ‚Ä£ 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">4</span></a>, single person 3D pose estimation is mainly classified into the direct estimation method and the 2D to 3D lifting method. The direct method estimates the 3D human pose directly from the input using a predictor, and the 2D to 3D lifting method, which estimates the 3D pose from the results of 2D estimation, involves first detecting the coordinates of human keypoints in 2D space, and then lifting these 2D keypoints onto 3D space coordinates.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="95" id="S4.F4.sf1.g1" src="x4.png" width="305"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S4.F4.sf1.3.2" style="font-size:80%;">The direct estimation method</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="91" id="S4.F4.sf2.g1" src="x5.png" width="436"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S4.F4.sf2.3.2" style="font-size:80%;">The 2D to 3D lifting method</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Typical single person 3D human pose estimation. (a) The direct estimation method; (b) The 2D to 3D lifting method.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Single person 3D pose estimation in images</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.1">Solving Depth Ambiguity.</span>
As shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.F5.sf1" title="In Figure 5 ‚Ä£ 4.1.1 Single person 3D pose estimation in images ‚Ä£ 4.1 Single person 3D pose estimation ‚Ä£ 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">5(a)</span></a>, different 3D pose coordinates projecting to 2D images may give the same results, leading to an ill-posed problem. This problem can be solved by using the propagation properties of light and camera imaging principle. To address the ill-posed problem, Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite> introduced a view-invariant framework to moderate the effects of viewpoint diversity. A View-Invariant Hierarchical Correction (VI-HC) network predicts the 3D pose refinement with view-consistent constraints in the proposed framework. Additionally, a view-invariant discriminative network actualizes high-level constraints after the base network generates an initial estimation. Ray-based 3D (Ray3D) absolute estimation method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> can convert the input from pixel space to 3D normalized rays. Another helpful approach for this problem is learning a more appropriate feature representation. Part-centric HEatMap triplets (HEMlets) framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite> utilizes three joint-heatmaps to represent the end-joints relative depth information, which bridges the gap between the 2D location and the 3D human pose.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="126" id="S4.F5.sf1.g1" src="x6.png" width="201"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S4.F5.sf1.3.2" style="font-size:80%;">Depth ambiguity</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="126" id="S4.F5.sf2.g1" src="x7.png" width="201"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S4.F5.sf2.3.2" style="font-size:80%;">Graph-based representation</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="127" id="S4.F5.sf3.g1" src="x8.png" width="207"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf3.2.1.1" style="font-size:80%;">(c)</span> </span><span class="ltx_text" id="S4.F5.sf3.3.2" style="font-size:80%;">Transfer learning</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>(a) Depth ambiguity; (b) Graph-based representation for human body; (c) Transfer learning.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1">Solving Body Structure Understanding.</span>
Unlike other computer vision tasks, the body‚Äôs unique structures can provide constraints or prior information to improve pose estimation performance. In the Joint Relationship Aware Network (JRAN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite>, a dual attention module was designed to generate both whole and local feature attention block weights. The limb poses aware network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> leverages kinematic constraint and trajectory information to prevent errors from accumulating along the human body structure. Pose grammar in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite> learns a 2D-3D mapping function, enabling the input 2D pose to be transformed into 3D space, and integrates three aspects of human structure (kinematics, symmetry, motor coordination) into a Bi-directional Recurrent Neural Network (RNN). The orientation keypoints-based method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite> uses virtual markers to generate sufficient information for accurately inferring rotations through simple post-processing.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">The Graph Neural Network (GNN) is a convolutional network defined on the graph data structure. It is difficult for typical neural networks to handle graph-structured data, such as body structure information. However, this challenge can be more easily addressed if graph-based learning methods are used, as shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.F5.sf2" title="In Figure 5 ‚Ä£ 4.1.1 Single person 3D pose estimation in images ‚Ä£ 4.1 Single person 3D pose estimation ‚Ä£ 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">5(b)</span></a>. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite> proposed a feature boosting network in which features are learned by the convolutional layers and are boosted with Graphical ConvLSTM to perceive the graphical long short-term dependency among different body parts. The Locally Connected Network (LCN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite> leverages the allocation of dedicated filters rather than sharing them for different joints. Modulated Graph Convolutional Network (Modulated-GCN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite> includes weight and affinity modulation to learn modulation vectors for different body joints and to adjust the graph structure, respectively. Zeng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite> designed a skeletal GNN learning framework to address the depth ambiguity and self-occlusion problems in 3D human pose estimation. In this framework, the proposed hop-aware hierarchical channel-squeezing fusion layer is designed to extract relevant information from neighboring nodes. Higher-order Regular Splitting graph Network (RS-Net) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite> captures long-range dependencies between body joints using multi-hop neighborhoods. It learns distinct modulation vectors for different body joints and adds modulation matrices to the corresponding skeletal adjacency matrices. Zhai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a>]</cite> employed intra-group joint refinement utilizing attention mechanisms to discover potential joint synergies to explore the potential synergies between joints.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p4.1.1">Solving Occlusion Problems.</span>
Partial occlusion of the human body, including self-occlusion and other occlusions (such as object occlusion and multi-person occlusion), is typical in various scenes. Occlusion may interfere with pose estimation, potentially resulting in the prediction of an error pose. Solving the occlusion problem through a multi-view approach is effective, where an occluded pose not visible in one view is likely to be visible in other views in a multi-view system. Thus, a multi-view based approach can provide more reliable results through cross-view frame inference. In practice, Iskakov et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite> combined algebraic and volumetric triangulation for 3D pose estimation from multi-view 2D images. The former method is based on a differentiable algebraic triangulation with confidence weights, while the latter method utilizes volumetric aggregation from intermediate 2D backbone feature maps. Multi-view geometric priors have been incorporated in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>, which combines two steps: first, predicting the 2D poses in multi-view RGB images through cross-view fusion, and second, utilizing the proposed Recursive Pictorial Structure Model (RPSM) to recover the 3D poses from the previously predicted multi-view 2D poses. To improve the real-time performance of multi-view 3D pose estimation, Remelli et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite> designed a lightweight framework with a differentiable Direct Linear Transform (DLT) layer. Adaptive multi-view fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>]</cite> enhances the features in occluded views by determining the correspondence between points in occluded and visible views. To address the problem of generalizability for multi-view estimation, Bartol et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite> proposed a stochastic learning framework for pose triangulation. Luvizon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite> effectively integrated 2D annotated data and 3D poses to design a consensus-aware method, which optimizes multi-view poses from uncalibrated images by different coherent estimations up to a scale factor from the intrinsic parameters. Probabilistic triangulation module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> embedded in a 3D pose estimation framework can extend multi-view methods to uncalibrated scenes. It models camera poses using probability distributions and iteratively updates them from 2D features, replacing the traditional update through camera poses and eliminating the dependency on camera calibration.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p5">
<p class="ltx_p" id="S4.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p5.1.1">Solving Data Lacking.</span>
In recent years, the development of diverse and efficient feature representations, along with end-to-end training modes, has significantly enhanced the accuracy of deep learning models in 3D human pose estimation. However, a significant limitation of these models is their reliance on fully supervised training, necessitating vast amounts of expensive and labor-intensive labeled 3D data, predominantly from indoor scenes. Addressing this challenge requires exploring alternative training strategies beyond fully supervised learning, such as unsupervised, semi-supervised, and few-shot learning approaches.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p6">
<p class="ltx_p" id="S4.SS1.SSS1.p6.1">Unsupervised learning typically focuses on learning features rather than specific tasks from unlabeled training data, and it finds relationships between samples by mining the intrinsic features of the data. The first work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite> can predict 3D human pose without any 3D dataset by adversarial learning, which is based on the generative adversarial networks via unsupervised training. The approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">82</span></a>]</cite> utilizes geometric self-supervision and randomly reprojects 2D pose camera viewpoints from the recovered 3D skeleton during training, thereby forming a self-consistency loss in the lift-reproject-lift process. Elepose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a>]</cite> utilizes random projections, estimating likelihood using normalizing flows on 2D poses in a linear subspace. Furthermore, Elepose also learns the distribution of camera angles to reduce the dependence on camera rotation priors in the training dataset.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p7">
<p class="ltx_p" id="S4.SS1.SSS1.p7.1">Self-supervised learning is a branch of unsupervised learning where the model can learn by itself from unlabeled training data and acquire the representation model on unlabeled data through pretext tasks. During on-the-ground execution, EpipolarPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib84" title=""><span class="ltx_text" style="font-size:90%;">84</span></a>]</cite> trains the 3D pose estimator without any 3D ground-truth data or camera extrinsic parameters, predicting 2D poses from multi-view images and obtaining a 3D pose and camera geometry via epipolar geometry. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">85</span></a>]</cite> designed a simple yet effective self-supervised correction mechanism for 3D human pose estimation by learning all intrinsic structures of the human pose, which is divided into two parts: the 2D-to-3D pose transformation and 3D-to-2D pose projection. MRP-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib86" title=""><span class="ltx_text" style="font-size:90%;">86</span></a>]</cite> reformulated the self-supervised 3D human pose estimation as an unsupervised domain adaptation problem, which includes model-free joint localization and model-based parametric regression. To reduce dependence on the consistency loss that guides learning, unlike previous works, the method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> generates 2D-3D pose pairs for augmenting supervision via the proposed self-enhancing dual-loop learning framework.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p8">
<p class="ltx_p" id="S4.SS1.SSS1.p8.1">A weakly-supervised model, characterized by its reliance on only weak labels, often faces the challenge of performing complex tasks with limited or imprecise guidance. Despite the lack of detailed annotations typically required in fully supervised scenarios, these models derive meaningful insights, utilizing vague or less informative labels to infer intricate patterns and relationships within the data. This ability to operate with minimal supervision makes them particularly useful when acquiring comprehensive labeled data is impractical or costly. In practical applications, Hua et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib87" title=""><span class="ltx_text" style="font-size:90%;">87</span></a>]</cite> proposed a weakly-supervised method that initially lifts the 2D keypoints into coarse 3D poses across two views using triangulation and then refines the 3D pose by employing spatial configurations and cross-view correlations. Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> designed a weakly-supervised framework that utilizes projection relationships to estimate 3D poses solely from 2D pose annotations under the condition of known camera parameters.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p9">
<p class="ltx_p" id="S4.SS1.SSS1.p9.1">Transfer learning enables the transfer of model parameters from the source domain to the target domain, allowing us to share learned model parameters with a new model, as shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.F5.sf3" title="In Figure 5 ‚Ä£ 4.1.1 Single person 3D pose estimation in images ‚Ä£ 4.1 Single person 3D pose estimation ‚Ä£ 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">5(c)</span></a>. This approach speeds up and optimizes efficiency, eliminating the need to learn from scratch, as is common with most networks. For example, Adaptpose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib88" title=""><span class="ltx_text" style="font-size:90%;">88</span></a>]</cite> is an end-to-end cross-dataset adaptation 3D human pose estimation predictor that is implemented using transfer learning for limited data applications.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Single person 3D pose estimation in videos</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">With hardware development, image data are often acquired and processed in videos. The video frame data can provide more continuous information than a single-frame image, predicting human pose estimation in sequence through the spatio-temporal domain. In addition, optical flow and scene flow information can be extracted from videos to predict 3D human pose from data of multimodal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib122" title=""><span class="ltx_text" style="font-size:90%;">122</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p2.1.1">Solving Single-frame Limitation.</span>
Using continuous image frames from video can capture dynamic changes and temporal information. By analyzing this information, deep learning models can extract motion features and spatio-temporal relationships, thereby effectively improving the effectiveness of 3D human pose estimation. Pavllo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib89" title=""><span class="ltx_text" style="font-size:90%;">89</span></a>]</cite> proposed a 3D human pose estimation method for video that extracts temporal cues with dilated convolutions over 2D keypoint trajectories, and then they designed a semi-supervised method with back-projection to improve performance. PoseFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite> as a spatial-temporal transformer-based framework predicts 3D body pose by analyzing human joint relations within each frame and their temporal correlations across frames. Based on previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib123" title=""><span class="ltx_text" style="font-size:90%;">123</span></a>]</cite>, unipose+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib91" title=""><span class="ltx_text" style="font-size:90%;">91</span></a>]</cite> leverages multi-scale feature representations to enhance the feature extractors in the framework‚Äôs backbone. This framework utilizes contextual information across scales and joint localization with Gaussian heatmap modulation to improve decoder estimation accuracy.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">Furthermore, Multi-Hypothesis transFormer (MHFormer) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib92" title=""><span class="ltx_text" style="font-size:90%;">92</span></a>]</cite> learns spatio-temporal representations with multiple plausible pose hypotheses in a three-stage framework. In Mixed Spatio-Temporal Encoder (MixSTE) framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib93" title=""><span class="ltx_text" style="font-size:90%;">93</span></a>]</cite>, the temporal transformer block learns the temporal motion of each joint and their inter-joint spatial correlation. Honari et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib94" title=""><span class="ltx_text" style="font-size:90%;">94</span></a>]</cite> proposed an unsupervised feature extraction method using Contrastive Self-Supervised (CSS) learning to extract temporal information from videos. Extending this line of research, Hierarchical Spatial-Temporaltrans Formers (HSTFormer) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib95" title=""><span class="ltx_text" style="font-size:90%;">95</span></a>]</cite> utilizes spatial-temporal correlations of joints at different levels simultaneously, marking a first in studying hierarchical transformer encoders with multi-level fusion. Recently, Spatio-Temporal Criss-cross (STC) attention block <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib96" title=""><span class="ltx_text" style="font-size:90%;">96</span></a>]</cite> decomposes correlation learning into space and time to reduce the computational cost of the joint-to-joint affinity matrix.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p4.1.1">Solving Real-time Problems.</span>
However, while various methods improve 3D pose estimation performance, they also introduce new problems and challenges in video-based estimation. For example, the continuous video frames add a substantial computational cost, which poses a challenge to the processing efficiency of the system and makes real-time processing more difficult. To reduce the total computational complexity, Einfalt et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> designed a transformer-based scheme that uplifts dense 3D poses from temporally sparse 2D pose sequences by temporal upsampling within Transformer blocks. Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib97" title=""><span class="ltx_text" style="font-size:90%;">97</span></a>]</cite> reduced the complexity of attention calculation in Transformers through spatio-temporal sparse sampling, enabling the estimation of 3D human poses in video sequences on computationally constrained platforms.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p5">
<p class="ltx_p" id="S4.SS1.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p5.1.1">Solving Body Structure Understanding.</span>
At the same time, the continuous motion information provided by video has led researchers to combine videos with human kinematics. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib98" title=""><span class="ltx_text" style="font-size:90%;">98</span></a>]</cite> designed a motion loss that computes the difference between the motion patterns of the predicted and ground truth keypoint trajectories, along with motion encoding, a simple yet effective representation of keypoint motion for 3D human pose estimation in videos. Dynamical Graph Network (DG-Net) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib99" title=""><span class="ltx_text" style="font-size:90%;">99</span></a>]</cite> dynamically determines the human-joint affinity and adaptively predicts human pose via spatial-temporal joint relations in videos. To address the shortcoming of directly regressing the 3D joint locations, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib100" title=""><span class="ltx_text" style="font-size:90%;">100</span></a>]</cite> proposed an anatomy-aware estimation framework. This human skeleton anatomy-based framework includes a bone direction prediction network and a bone length prediction network, and it effectively utilizes bone features to bridge the gap between 2D keypoints and 3D joints. Xue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib101" title=""><span class="ltx_text" style="font-size:90%;">101</span></a>]</cite> designed a part-aware temporal attention module capable of extracting each part‚Äôs temporal dependency separately.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p6">
<p class="ltx_p" id="S4.SS1.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p6.1.1">Solving Occlusion Problems.</span>
To address occlusion problems, video-based 3D human pose estimation enables the use of a multi-view approach and leverages the continuity of inputs between video frames to predict occluded body parts. Cheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib102" title=""><span class="ltx_text" style="font-size:90%;">102</span></a>]</cite> introduced an occlusion-aware model that utilizes estimated 2D confidence heatmaps of keypoints and an optical-flow consistency constraint to generate a more complete 3D pose in occlusion scenes. Additionally, Multi-view and Temporal Fusing Transformer (MTF-Transformer) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> fuses multi-view sequences in uncalibrated scenes for 3D human pose estimation in videos. To reduce dependency on camera calibration, the framework infers the relationship between pairs of views with a relative attention mechanism.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p7">
<p class="ltx_p" id="S4.SS1.SSS2.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p7.1.1">Solving Data Lacking.</span>
Human actions are inherently continuous, making video-based pose estimation critical for enhanced understanding. Compared with 3D annotated still images, high-quality, annotated 3D videos are relatively scarce. Nevertheless, the Internet abounds with a vast repository of unlabeled video data, the utilization of which for learning purposes holds considerable importance. Thus, video-based 3D human pose estimation to alleviate data dependency is essential. In practice, Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib103" title=""><span class="ltx_text" style="font-size:90%;">103</span></a>]</cite> divided the unsupervised 3D pose estimation process into two sub-tasks: a scale estimation module and a pose lifting module. The scale estimation module optimizes the 2D input pose, while the pose lifting module maps the optimized 2D pose to its 3D counterpart.
In weakly-supervised methods, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib104" title=""><span class="ltx_text" style="font-size:90%;">104</span></a>]</cite> proposed a weakly-supervised method, employing a view synthesis framework and a geometry-aware representation. In this framework, the method leverages 2D keypoints for supervision and learns a shared 3D representation between viewpoints by synthesizing the human pose from one viewpoint to another. Semi-supervised learning utilizes only partially labeled data, consisting of a large amount of unlabeled data and a small amount of labeled data. In this context, Mitra et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib105" title=""><span class="ltx_text" style="font-size:90%;">105</span></a>]</cite> present a multi-view consistent semi-supervised method, which can regress 3D human pose from unannotated, uncalibrated, but synchronized multi-view videos. In self-supervised methods, Kundu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib106" title=""><span class="ltx_text" style="font-size:90%;">106</span></a>]</cite> employed prior knowledge of the body skeleton and disentangles the inherent factors of variation through part-guided human image synthesis.
Similarly, Shan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib107" title=""><span class="ltx_text" style="font-size:90%;">107</span></a>]</cite> randomly mask the body joints in spatial and temporal domains to better capture spatial and temporal dependencies. Meta-learning (a.k.a. ‚Äùlearning to learn‚Äù) is a process where algorithms optimize their learning strategy based on experience. It involves training a model on various learning tasks, enabling it to learn new tasks more efficiently using fewer data samples. Cho et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> present an optimization-based meta-learning algorithm for 3D human pose estimation. This algorithm can adapt to arbitrary camera distortion by generating synthetic distorted data from undistorted 2D keypoints during model training. Apart from changing the learning supervision methods, data augmentation is also a practical approach to increasing the amount of data and enhancing the model‚Äôs generalization performance. PoseAug <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib108" title=""><span class="ltx_text" style="font-size:90%;">108</span></a>]</cite> adjusts geometric factors such as posture, body shape, and viewpoint for model learning under the premise of the discriminative module controlling the feasibility of augmented poses. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib109" title=""><span class="ltx_text" style="font-size:90%;">109</span></a>]</cite> generates more diverse and challenging poses online.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Multi-person 3D pose estimation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">As depicted in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S4.F6" title="Figure 6 ‚Ä£ 4.2 Multi-person 3D pose estimation ‚Ä£ 4 3D Human Pose Estimation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">6</span></a>, multi-person 3D pose estimation can be divided into two main categories: the bottom-up method (estimation + association) and the top-down method (detection + estimation). With a two-stage pipeline, the top-down method first detects every person in the input images and then extracts each person‚Äôs keypoints in the previously detected bounding box. The bottom-up method initially detects all keypoints in one stage and subsequently associates them with their respective persons. For instance, Cheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib121" title=""><span class="ltx_text" style="font-size:90%;">121</span></a>]</cite> integrated both top-down and bottom-up approaches in multi-person pose estimation, complementing each other‚Äôs shortcomings. The top-down method shows robustness against potential erroneous bounding boxes, while the bottom-up network is more robust in handling scale variations. Finally, the 3D poses estimated from both top-down and bottom-up networks are fed into an integrated network to obtain the final 3D pose. Some single-stage methods and approaches combine the two methods as mentioned above. Unlike mainstream two-stage solutions for multi-person 3D human pose estimation, Jin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib120" title=""><span class="ltx_text" style="font-size:90%;">120</span></a>]</cite> introduced a single-stage method. This method expresses the 2D pose in the image plane and the depth information of a 3D body instance via the proposed decoupled representation and predicts the scale information of instances by extracting 2D pose features and enabling depth regression.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="168" id="S4.F6.g1" src="x9.png" width="490"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Typical multi-person 3D pose estimation.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Top-down methods</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">In top-down methods, AlphaPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib111" title=""><span class="ltx_text" style="font-size:90%;">111</span></a>]</cite> predicts whole-body multi-person 3D human pose including face, body, hand, and foot. In the proposed framework, the symmetric integral keypoint regression module achieves fast and fine localization, the parametric pose non-maximum suppression module helps to eliminate redundant human detections, and the pose aware identity embedding module enables joint pose estimation and tracking. To achieve better real-time performance in multi-person 3D pose estimation, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib110" title=""><span class="ltx_text" style="font-size:90%;">110</span></a>]</cite> proposed a multi-view temporal consistency method in real-time from videos, which matches the 2D inputs with 3D poses directly in three-dimensional space and achieves over 150 FPS (frames per second) on a 12-camera setup and 34 FPS on a 28-camera setup. Additionally, choosing appropriate representations can effectively improve multi-person pose estimation results. For instance, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> employed a voxel representation to predict multi-person 3D pose and tracking, where the proposed voxel representation can determine whether each voxel contains a particular body joint. In multi-person scenarios, occlusion issues may be more severe and complex than in single-person scenarios. To address this challenge, Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib112" title=""><span class="ltx_text" style="font-size:90%;">112</span></a>]</cite> utilized graph neural networks to boost information passing efficiency for multi-person, multi-view 3D human pose estimation. The multi-view matching graph module associates coarse cross-view poses within the networks, and the center refinement graph module further refines the results. Single-shot learning trains the model using significantly less data than is typically required for supervised learning, aiming to improve effectiveness. PandaNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> as an anchor-based model introduces a pose-aware anchor selection strategy to discard ambiguous anchors. Moon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib113" title=""><span class="ltx_text" style="font-size:90%;">113</span></a>]</cite> proposed a top-down, camera distance-aware method for multi-person 3D pose estimation without relying on ground-truth information.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Bottom-up methods</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">MubyNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib124" title=""><span class="ltx_text" style="font-size:90%;">124</span></a>]</cite> is a typical bottom-up, multi-task method for multi-person 3D pose estimation, which allows for training all component parameters. In the research on the lightweight top-down framework, Fabbri et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib114" title=""><span class="ltx_text" style="font-size:90%;">114</span></a>]</cite> utilized high-resolution volumetric heatmaps to improve the estimation performance and designed a volumetric heatmap autoencoder that can compress the size of the representation. Designing better supervisory methods could also be effective, Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib115" title=""><span class="ltx_text" style="font-size:90%;">115</span></a>]</cite> introduced a novel supervisory approach named Hierarchical Multi-person Ordinal Relations (HMOR) using a monocular camera and designed a comprehensive top-level model to learn these ordinal relations, enhancing the accuracy of human depth estimation through a coarse-to-fine architecture. In the top-down estimation on single-shot learning, Mehta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib118" title=""><span class="ltx_text" style="font-size:90%;">118</span></a>]</cite> inferred whole body pose under strong partial occlusions through occlusion-robust pose-maps in their proposed single-shot framework. Zhen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib116" title=""><span class="ltx_text" style="font-size:90%;">116</span></a>]</cite> designed a single-shot, bottom-up framework that estimates the absolute positions of multiple people by leveraging depth-related cues across the entire image. After their previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>, Benzine et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib117" title=""><span class="ltx_text" style="font-size:90%;">117</span></a>]</cite> proposed a single-shot 3D human pose estimation method that predicts multi-person 3D poses without the need for bounding boxes. This method extends the Stacked Hourglass Network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib125" title=""><span class="ltx_text" style="font-size:90%;">125</span></a>]</cite> to handle multi-person situations. To address the issue of occlusion, LCR-Net++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib119" title=""><span class="ltx_text" style="font-size:90%;">119</span></a>]</cite> integrates adjacent pose hypotheses to predict the multi-person 2D and 3D poses without approximating initial human localization when a person is partially occluded or truncated by the image boundaries.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Summary of 3D pose estimation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Research in single-person 3D pose estimation predominantly focuses on some of the aforementioned critical issues. While there are commendable studies in this field, the challenges are yet to be fully resolved, necessitating more comprehensive research. In contrast to image-based methods, utilizing video-based methods is unequivocally seen as the trajectory of future advancements. The methodology adopted in multi-person 3D pose estimation must be tailored to the specific context. The current prevalent techniques can be categorized into two main approaches, each with inherent limitations. Top-down methods depend heavily on human detection and are susceptible to detection inaccuracies, often leading to unreliable pose estimations in environments with multiple people. Conversely, bottom-up methods, which operate independently of human detection and thus are immune to errors, face challenges in accurately processing all individuals in a scene concurrently, particularly affecting the detection of smaller-scale figures. In line with other domains in computer vision, the one-stage, end-to-end methods represent this field‚Äôs future direction.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>3D Human Mesh Recovery</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Human mesh recovery can be divided into two categories based on their representation models: template-based (parametric) methods and template-free (non-parametric) methods, as shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5.F7" title="Figure 7 ‚Ä£ 5 3D Human Mesh Recovery ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">7</span></a>. Template-based human mesh recovery reconstructs predefined models (such as SCAPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>) by estimating the model‚Äôs parameters. In contrast, template-free human mesh recovery predicts the 3D body directly from input data without reliance on predefined models. The parametric approaches can be more robust than non-parametric methods with the templates‚Äô prior knowledge, but their flexibility and detail are inherently limited even when extended with more parameters like SMPL+H <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> and SMPL+X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite>. Human mesh recovery is a crucial technique for digitizing the human body, posing significant challenges in computer vision and computer graphics due to the complex nature of geometric textures and color variations. Moreover, human mesh recovery faces challenges similar to 3D pose estimation, including environmental interference, multi-person scenarios, and occlusion issues. In this section, we will introduce the dominant methods based on these categories and challenges, with a comprehensive summary provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5.T2" title="Table 2 ‚Ä£ 5 3D Human Mesh Recovery ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="66" id="S5.F7.sf1.g1" src="x10.png" width="435"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S5.F7.sf1.3.2" style="font-size:80%;">Template based human mesh recovery</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="86" id="S5.F7.sf2.g1" src="x11.png" width="435"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S5.F7.sf2.3.2" style="font-size:80%;">Template-free human mesh recovery</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Typical human mesh recovery. (a) Template based human mesh recovery method; (b) Template-free human mesh recovery method.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Overview of human mesh recovery.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.69" style="width:368.6pt;height:554.6pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-253.8pt,381.6pt) scale(0.420658830478126,0.420658830478126) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T2.69.69">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.69.69.70.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3" id="S5.T2.69.69.70.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.69.69.70.1.1.1">Main ideas</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.69.69.70.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.69.69.70.1.2.1">Methods</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.1.1.2" rowspan="56" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1">Template-based</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.1.1.3" rowspan="40" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1">Naked</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.1.1.4" rowspan="4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.4.1">Multimodal Methods</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.m1.1d">‚àô</annotation></semantics></math> Hybrid annotations: Rong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib126" title=""><span class="ltx_text" style="font-size:90%;">126</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.2.2.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.2.2.2.1.m1.1"><semantics id="S5.T2.2.2.2.1.m1.1a"><mo id="S5.T2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.1.m1.1d">‚àô</annotation></semantics></math> Optical flow: DTS-VIBE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib127" title=""><span class="ltx_text" style="font-size:90%;">127</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.3.3.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.3.3.3.1.m1.1"><semantics id="S5.T2.3.3.3.1.m1.1a"><mo id="S5.T2.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.1.m1.1b"><ci id="S5.T2.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.1.m1.1d">‚àô</annotation></semantics></math> Silhouettes: LASOR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib128" title=""><span class="ltx_text" style="font-size:90%;">128</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.4.4.4.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.4.4.4.1.m1.1"><semantics id="S5.T2.4.4.4.1.m1.1a"><mo id="S5.T2.4.4.4.1.m1.1.1" xref="S5.T2.4.4.4.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.1.m1.1b"><ci id="S5.T2.4.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.4.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.1.m1.1d">‚àô</annotation></semantics></math> Cropped image and bounding box: CLIFF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib129" title=""><span class="ltx_text" style="font-size:90%;">129</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.5.5.5.2" rowspan="5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.5.2.1">Utilizing Attention Mechanism</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.5.5.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.5.5.5.1.m1.1"><semantics id="S5.T2.5.5.5.1.m1.1a"><mo id="S5.T2.5.5.5.1.m1.1.1" xref="S5.T2.5.5.5.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.1.m1.1b"><ci id="S5.T2.5.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.5.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.5.1.m1.1d">‚àô</annotation></semantics></math> Part-driven attention: PARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib130" title=""><span class="ltx_text" style="font-size:90%;">130</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.6.6.6.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.6.6.6.1.m1.1"><semantics id="S5.T2.6.6.6.1.m1.1a"><mo id="S5.T2.6.6.6.1.m1.1.1" xref="S5.T2.6.6.6.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.1.m1.1b"><ci id="S5.T2.6.6.6.1.m1.1.1.cmml" xref="S5.T2.6.6.6.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.6.6.1.m1.1d">‚àô</annotation></semantics></math> Graph attention: Mesh Graphormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib131" title=""><span class="ltx_text" style="font-size:90%;">131</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.7.7.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.7.7.7.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.7.7.7.1.m1.1"><semantics id="S5.T2.7.7.7.1.m1.1a"><mo id="S5.T2.7.7.7.1.m1.1.1" xref="S5.T2.7.7.7.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.7.1.m1.1b"><ci id="S5.T2.7.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.7.7.7.1.m1.1d">‚àô</annotation></semantics></math> Spatio-temporal attention: MPS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib132" title=""><span class="ltx_text" style="font-size:90%;">132</span></a>]</cite>, PSVT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib133" title=""><span class="ltx_text" style="font-size:90%;">133</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.8.8.8.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.8.8.8.1.m1.1"><semantics id="S5.T2.8.8.8.1.m1.1a"><mo id="S5.T2.8.8.8.1.m1.1.1" xref="S5.T2.8.8.8.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.8.1.m1.1b"><ci id="S5.T2.8.8.8.1.m1.1.1.cmml" xref="S5.T2.8.8.8.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.8.8.8.1.m1.1d">‚àô</annotation></semantics></math> Efficient architecture: FastMETRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib134" title=""><span class="ltx_text" style="font-size:90%;">134</span></a>]</cite>, Xue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib135" title=""><span class="ltx_text" style="font-size:90%;">135</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.9.9.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.9.9.9.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.9.9.9.1.m1.1"><semantics id="S5.T2.9.9.9.1.m1.1a"><mo id="S5.T2.9.9.9.1.m1.1.1" xref="S5.T2.9.9.9.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.9.9.9.1.m1.1b"><ci id="S5.T2.9.9.9.1.m1.1.1.cmml" xref="S5.T2.9.9.9.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.9.9.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.9.9.9.1.m1.1d">‚àô</annotation></semantics></math> End-to-end structure: METRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib136" title=""><span class="ltx_text" style="font-size:90%;">136</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.10.10.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.10.10.10.2" rowspan="6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.10.10.10.2.1">Exploiting Temporal Information</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.10.10.10.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.10.10.10.1.m1.1"><semantics id="S5.T2.10.10.10.1.m1.1a"><mo id="S5.T2.10.10.10.1.m1.1.1" xref="S5.T2.10.10.10.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.10.10.10.1.m1.1b"><ci id="S5.T2.10.10.10.1.m1.1.1.cmml" xref="S5.T2.10.10.10.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.10.10.10.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.10.10.10.1.m1.1d">‚àô</annotation></semantics></math> Temporally encoding features: Kanazawa et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib137" title=""><span class="ltx_text" style="font-size:90%;">137</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.11.11.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.11.11.11.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.11.11.11.1.m1.1"><semantics id="S5.T2.11.11.11.1.m1.1a"><mo id="S5.T2.11.11.11.1.m1.1.1" xref="S5.T2.11.11.11.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.11.11.11.1.m1.1b"><ci id="S5.T2.11.11.11.1.m1.1.1.cmml" xref="S5.T2.11.11.11.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.11.11.11.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.11.11.11.1.m1.1d">‚àô</annotation></semantics></math> Self-attention temporal: VIBE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib138" title=""><span class="ltx_text" style="font-size:90%;">138</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.12.12.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.12.12.12.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.12.12.12.1.m1.1"><semantics id="S5.T2.12.12.12.1.m1.1a"><mo id="S5.T2.12.12.12.1.m1.1.1" xref="S5.T2.12.12.12.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.12.12.12.1.m1.1b"><ci id="S5.T2.12.12.12.1.m1.1.1.cmml" xref="S5.T2.12.12.12.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.12.12.12.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.12.12.12.1.m1.1d">‚àô</annotation></semantics></math> Temporally consistent: TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib139" title=""><span class="ltx_text" style="font-size:90%;">139</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.13.13.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.13.13.13.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.13.13.13.1.m1.1"><semantics id="S5.T2.13.13.13.1.m1.1a"><mo id="S5.T2.13.13.13.1.m1.1.1" xref="S5.T2.13.13.13.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.13.13.13.1.m1.1b"><ci id="S5.T2.13.13.13.1.m1.1.1.cmml" xref="S5.T2.13.13.13.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.13.13.13.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.13.13.13.1.m1.1d">‚àô</annotation></semantics></math> Multi-level spatial-temporal attention: MAED <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib140" title=""><span class="ltx_text" style="font-size:90%;">140</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.14.14.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.14.14.14.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.14.14.14.1.m1.1"><semantics id="S5.T2.14.14.14.1.m1.1a"><mo id="S5.T2.14.14.14.1.m1.1.1" xref="S5.T2.14.14.14.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.14.14.14.1.m1.1b"><ci id="S5.T2.14.14.14.1.m1.1.1.cmml" xref="S5.T2.14.14.14.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.14.14.14.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.14.14.14.1.m1.1d">‚àô</annotation></semantics></math> Temporally embedded live stream: TePose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib141" title=""><span class="ltx_text" style="font-size:90%;">141</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.15.15.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.15.15.15.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.15.15.15.1.m1.1"><semantics id="S5.T2.15.15.15.1.m1.1a"><mo id="S5.T2.15.15.15.1.m1.1.1" xref="S5.T2.15.15.15.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.15.15.15.1.m1.1b"><ci id="S5.T2.15.15.15.1.m1.1.1.cmml" xref="S5.T2.15.15.15.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.15.15.15.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.15.15.15.1.m1.1d">‚àô</annotation></semantics></math> Short-term and long-term temporal correlations: GLoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib142" title=""><span class="ltx_text" style="font-size:90%;">142</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.16.16.16">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.16.16.16.2" rowspan="4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.16.16.16.2.1">Multi-view Methods</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.16.16.16.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.16.16.16.1.m1.1"><semantics id="S5.T2.16.16.16.1.m1.1a"><mo id="S5.T2.16.16.16.1.m1.1.1" xref="S5.T2.16.16.16.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.16.16.16.1.m1.1b"><ci id="S5.T2.16.16.16.1.m1.1.1.cmml" xref="S5.T2.16.16.16.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.16.16.16.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.16.16.16.1.m1.1d">‚àô</annotation></semantics></math> Confidence-aware majority voting mechanism: Dong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib143" title=""><span class="ltx_text" style="font-size:90%;">143</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.17.17.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.17.17.17.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.17.17.17.1.m1.1"><semantics id="S5.T2.17.17.17.1.m1.1a"><mo id="S5.T2.17.17.17.1.m1.1.1" xref="S5.T2.17.17.17.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.17.17.17.1.m1.1b"><ci id="S5.T2.17.17.17.1.m1.1.1.cmml" xref="S5.T2.17.17.17.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.17.17.17.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.17.17.17.1.m1.1d">‚àô</annotation></semantics></math> Probabilistic-based multi-view: Sengupta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib144" title=""><span class="ltx_text" style="font-size:90%;">144</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.18.18.18">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.18.18.18.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.18.18.18.1.m1.1"><semantics id="S5.T2.18.18.18.1.m1.1a"><mo id="S5.T2.18.18.18.1.m1.1.1" xref="S5.T2.18.18.18.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.18.18.18.1.m1.1b"><ci id="S5.T2.18.18.18.1.m1.1.1.cmml" xref="S5.T2.18.18.18.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.18.18.18.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.18.18.18.1.m1.1d">‚àô</annotation></semantics></math> Dynamic physics-geometry consistency: Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.19.19.19">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.19.19.19.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.19.19.19.1.m1.1"><semantics id="S5.T2.19.19.19.1.m1.1a"><mo id="S5.T2.19.19.19.1.m1.1.1" xref="S5.T2.19.19.19.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.19.19.19.1.m1.1b"><ci id="S5.T2.19.19.19.1.m1.1.1.cmml" xref="S5.T2.19.19.19.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.19.19.19.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.19.19.19.1.m1.1d">‚àô</annotation></semantics></math> Cross-view fusion: Zhuo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib145" title=""><span class="ltx_text" style="font-size:90%;">145</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.20.20.20">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.20.20.20.2" rowspan="4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.20.20.20.2.1">Boosting Efficiency</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.20.20.20.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.20.20.20.1.m1.1"><semantics id="S5.T2.20.20.20.1.m1.1a"><mo id="S5.T2.20.20.20.1.m1.1.1" xref="S5.T2.20.20.20.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.20.20.20.1.m1.1b"><ci id="S5.T2.20.20.20.1.m1.1.1.cmml" xref="S5.T2.20.20.20.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.20.20.20.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.20.20.20.1.m1.1d">‚àô</annotation></semantics></math> Sparse constrained formulation: SCOPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib146" title=""><span class="ltx_text" style="font-size:90%;">146</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.21.21.21">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.21.21.21.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.21.21.21.1.m1.1"><semantics id="S5.T2.21.21.21.1.m1.1a"><mo id="S5.T2.21.21.21.1.m1.1.1" xref="S5.T2.21.21.21.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.21.21.21.1.m1.1b"><ci id="S5.T2.21.21.21.1.m1.1.1.cmml" xref="S5.T2.21.21.21.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.21.21.21.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.21.21.21.1.m1.1d">‚àô</annotation></semantics></math> Single-stage model: BMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib147" title=""><span class="ltx_text" style="font-size:90%;">147</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.22.22.22">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.22.22.22.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.22.22.22.1.m1.1"><semantics id="S5.T2.22.22.22.1.m1.1a"><mo id="S5.T2.22.22.22.1.m1.1.1" xref="S5.T2.22.22.22.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.22.22.22.1.m1.1b"><ci id="S5.T2.22.22.22.1.m1.1.1.cmml" xref="S5.T2.22.22.22.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.22.22.22.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.22.22.22.1.m1.1d">‚àô</annotation></semantics></math> Process heatmap inputs: HeatER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib148" title=""><span class="ltx_text" style="font-size:90%;">148</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.23.23.23">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.23.23.23.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.23.23.23.1.m1.1"><semantics id="S5.T2.23.23.23.1.m1.1a"><mo id="S5.T2.23.23.23.1.m1.1.1" xref="S5.T2.23.23.23.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.23.23.23.1.m1.1b"><ci id="S5.T2.23.23.23.1.m1.1.1.cmml" xref="S5.T2.23.23.23.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.23.23.23.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.23.23.23.1.m1.1d">‚àô</annotation></semantics></math> Removing redundant tokens: TORE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib149" title=""><span class="ltx_text" style="font-size:90%;">149</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.24.24.24">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.24.24.24.2" rowspan="4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.24.24.24.2.1">Developing Various Representations</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.24.24.24.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.24.24.24.1.m1.1"><semantics id="S5.T2.24.24.24.1.m1.1a"><mo id="S5.T2.24.24.24.1.m1.1.1" xref="S5.T2.24.24.24.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.24.24.24.1.m1.1b"><ci id="S5.T2.24.24.24.1.m1.1.1.cmml" xref="S5.T2.24.24.24.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.24.24.24.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.24.24.24.1.m1.1d">‚àô</annotation></semantics></math> Texture map: TexturePose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib150" title=""><span class="ltx_text" style="font-size:90%;">150</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.25.25.25">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.25.25.25.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.25.25.25.1.m1.1"><semantics id="S5.T2.25.25.25.1.m1.1a"><mo id="S5.T2.25.25.25.1.m1.1.1" xref="S5.T2.25.25.25.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.25.25.25.1.m1.1b"><ci id="S5.T2.25.25.25.1.m1.1.1.cmml" xref="S5.T2.25.25.25.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.25.25.25.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.25.25.25.1.m1.1d">‚àô</annotation></semantics></math> UV map: Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib151" title=""><span class="ltx_text" style="font-size:90%;">151</span></a>]</cite>, DecoMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib152" title=""><span class="ltx_text" style="font-size:90%;">152</span></a>]</cite>, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib153" title=""><span class="ltx_text" style="font-size:90%;">153</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.26.26.26">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.26.26.26.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.26.26.26.1.m1.1"><semantics id="S5.T2.26.26.26.1.m1.1a"><mo id="S5.T2.26.26.26.1.m1.1.1" xref="S5.T2.26.26.26.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.26.26.26.1.m1.1b"><ci id="S5.T2.26.26.26.1.m1.1.1.cmml" xref="S5.T2.26.26.26.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.26.26.26.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.26.26.26.1.m1.1d">‚àô</annotation></semantics></math> Heat map: Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib154" title=""><span class="ltx_text" style="font-size:90%;">154</span></a>]</cite>, 3DCrowdNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib155" title=""><span class="ltx_text" style="font-size:90%;">155</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.27.27.27">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.27.27.27.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.27.27.27.1.m1.1"><semantics id="S5.T2.27.27.27.1.m1.1a"><mo id="S5.T2.27.27.27.1.m1.1.1" xref="S5.T2.27.27.27.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.27.27.27.1.m1.1b"><ci id="S5.T2.27.27.27.1.m1.1.1.cmml" xref="S5.T2.27.27.27.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.27.27.27.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.27.27.27.1.m1.1d">‚àô</annotation></semantics></math> Uniform representation: DSTformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib156" title=""><span class="ltx_text" style="font-size:90%;">156</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.28.28.28">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.28.28.28.2" rowspan="6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.28.28.28.2.1">Utilizing Structural Information</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.28.28.28.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.28.28.28.1.m1.1"><semantics id="S5.T2.28.28.28.1.m1.1a"><mo id="S5.T2.28.28.28.1.m1.1.1" xref="S5.T2.28.28.28.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.28.28.28.1.m1.1b"><ci id="S5.T2.28.28.28.1.m1.1.1.cmml" xref="S5.T2.28.28.28.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.28.28.28.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.28.28.28.1.m1.1d">‚àô</annotation></semantics></math> Part-based: holopose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib157" title=""><span class="ltx_text" style="font-size:90%;">157</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.29.29.29">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.29.29.29.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.29.29.29.1.m1.1"><semantics id="S5.T2.29.29.29.1.m1.1a"><mo id="S5.T2.29.29.29.1.m1.1.1" xref="S5.T2.29.29.29.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.29.29.29.1.m1.1b"><ci id="S5.T2.29.29.29.1.m1.1.1.cmml" xref="S5.T2.29.29.29.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.29.29.29.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.29.29.29.1.m1.1d">‚àô</annotation></semantics></math> Skeleton disentangling: Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib158" title=""><span class="ltx_text" style="font-size:90%;">158</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.30.30.30">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.30.30.30.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.30.30.30.1.m1.1"><semantics id="S5.T2.30.30.30.1.m1.1a"><mo id="S5.T2.30.30.30.1.m1.1.1" xref="S5.T2.30.30.30.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.30.30.30.1.m1.1b"><ci id="S5.T2.30.30.30.1.m1.1.1.cmml" xref="S5.T2.30.30.30.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.30.30.30.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.30.30.30.1.m1.1d">‚àô</annotation></semantics></math> Hybrid inverse kinematics: HybrIK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib159" title=""><span class="ltx_text" style="font-size:90%;">159</span></a>]</cite>, NIKI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib160" title=""><span class="ltx_text" style="font-size:90%;">160</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.31.31.31">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.31.31.31.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.31.31.31.1.m1.1"><semantics id="S5.T2.31.31.31.1.m1.1a"><mo id="S5.T2.31.31.31.1.m1.1.1" xref="S5.T2.31.31.31.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.31.31.31.1.m1.1b"><ci id="S5.T2.31.31.31.1.m1.1.1.cmml" xref="S5.T2.31.31.31.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.31.31.31.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.31.31.31.1.m1.1d">‚àô</annotation></semantics></math> Uncertainty-aware: Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib161" title=""><span class="ltx_text" style="font-size:90%;">161</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.32.32.32">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.32.32.32.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.32.32.32.1.m1.1"><semantics id="S5.T2.32.32.32.1.m1.1a"><mo id="S5.T2.32.32.32.1.m1.1.1" xref="S5.T2.32.32.32.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.32.32.32.1.m1.1b"><ci id="S5.T2.32.32.32.1.m1.1.1.cmml" xref="S5.T2.32.32.32.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.32.32.32.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.32.32.32.1.m1.1d">‚àô</annotation></semantics></math> Kinematic tree structure: Sengupta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib162" title=""><span class="ltx_text" style="font-size:90%;">162</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.33.33.33">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.33.33.33.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.33.33.33.1.m1.1"><semantics id="S5.T2.33.33.33.1.m1.1a"><mo id="S5.T2.33.33.33.1.m1.1.1" xref="S5.T2.33.33.33.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.33.33.33.1.m1.1b"><ci id="S5.T2.33.33.33.1.m1.1.1.cmml" xref="S5.T2.33.33.33.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.33.33.33.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.33.33.33.1.m1.1d">‚àô</annotation></semantics></math> Kinematic chains: SGRE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib163" title=""><span class="ltx_text" style="font-size:90%;">163</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.34.34.34">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.34.34.34.2" rowspan="7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.34.34.34.2.1">Choosing Appropriate Learning Strategies</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.34.34.34.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.34.34.34.1.m1.1"><semantics id="S5.T2.34.34.34.1.m1.1a"><mo id="S5.T2.34.34.34.1.m1.1.1" xref="S5.T2.34.34.34.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.34.34.34.1.m1.1b"><ci id="S5.T2.34.34.34.1.m1.1.1.cmml" xref="S5.T2.34.34.34.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.34.34.34.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.34.34.34.1.m1.1d">‚àô</annotation></semantics></math> Self-improving: SPIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib164" title=""><span class="ltx_text" style="font-size:90%;">164</span></a>]</cite>, ReFit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib165" title=""><span class="ltx_text" style="font-size:90%;">165</span></a>]</cite>, You et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.35.35.35">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.35.35.35.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.35.35.35.1.m1.1"><semantics id="S5.T2.35.35.35.1.m1.1a"><mo id="S5.T2.35.35.35.1.m1.1.1" xref="S5.T2.35.35.35.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.35.35.35.1.m1.1b"><ci id="S5.T2.35.35.35.1.m1.1.1.cmml" xref="S5.T2.35.35.35.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.35.35.35.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.35.35.35.1.m1.1d">‚àô</annotation></semantics></math> Novel losses: Zanfir et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib166" title=""><span class="ltx_text" style="font-size:90%;">166</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.36.36.36">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.36.36.36.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.36.36.36.1.m1.1"><semantics id="S5.T2.36.36.36.1.m1.1a"><mo id="S5.T2.36.36.36.1.m1.1.1" xref="S5.T2.36.36.36.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.36.36.36.1.m1.1b"><ci id="S5.T2.36.36.36.1.m1.1.1.cmml" xref="S5.T2.36.36.36.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.36.36.36.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.36.36.36.1.m1.1d">‚àô</annotation></semantics></math> Unsupervised learning: Madadi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib167" title=""><span class="ltx_text" style="font-size:90%;">167</span></a>]</cite>, Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.37.37.37">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.37.37.37.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.37.37.37.1.m1.1"><semantics id="S5.T2.37.37.37.1.m1.1a"><mo id="S5.T2.37.37.37.1.m1.1.1" xref="S5.T2.37.37.37.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.37.37.37.1.m1.1b"><ci id="S5.T2.37.37.37.1.m1.1.1.cmml" xref="S5.T2.37.37.37.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.37.37.37.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.37.37.37.1.m1.1d">‚àô</annotation></semantics></math> Bilevel online adaptation: Guan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib168" title=""><span class="ltx_text" style="font-size:90%;">168</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.38.38.38">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.38.38.38.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.38.38.38.1.m1.1"><semantics id="S5.T2.38.38.38.1.m1.1a"><mo id="S5.T2.38.38.38.1.m1.1.1" xref="S5.T2.38.38.38.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.38.38.38.1.m1.1b"><ci id="S5.T2.38.38.38.1.m1.1.1.cmml" xref="S5.T2.38.38.38.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.38.38.38.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.38.38.38.1.m1.1d">‚àô</annotation></semantics></math> Single-shot: Pose2UV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib169" title=""><span class="ltx_text" style="font-size:90%;">169</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.39.39.39">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.39.39.39.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.39.39.39.1.m1.1"><semantics id="S5.T2.39.39.39.1.m1.1a"><mo id="S5.T2.39.39.39.1.m1.1.1" xref="S5.T2.39.39.39.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.39.39.39.1.m1.1b"><ci id="S5.T2.39.39.39.1.m1.1.1.cmml" xref="S5.T2.39.39.39.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.39.39.39.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.39.39.39.1.m1.1d">‚àô</annotation></semantics></math> Contrastive learning: JOTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib170" title=""><span class="ltx_text" style="font-size:90%;">170</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.40.40.40">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.40.40.40.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.40.40.40.1.m1.1"><semantics id="S5.T2.40.40.40.1.m1.1a"><mo id="S5.T2.40.40.40.1.m1.1.1" xref="S5.T2.40.40.40.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.40.40.40.1.m1.1b"><ci id="S5.T2.40.40.40.1.m1.1.1.cmml" xref="S5.T2.40.40.40.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.40.40.40.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.40.40.40.1.m1.1d">‚àô</annotation></semantics></math> Domain adaptation: Nam et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib171" title=""><span class="ltx_text" style="font-size:90%;">171</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.41.41.41">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.41.41.41.2" rowspan="16" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.41.41.41.2.1">Detailed</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.41.41.41.3" rowspan="5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.41.41.41.3.1">With Clothes</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.41.41.41.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.41.41.41.1.m1.1"><semantics id="S5.T2.41.41.41.1.m1.1a"><mo id="S5.T2.41.41.41.1.m1.1.1" xref="S5.T2.41.41.41.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.41.41.41.1.m1.1b"><ci id="S5.T2.41.41.41.1.m1.1.1.cmml" xref="S5.T2.41.41.41.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.41.41.41.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.41.41.41.1.m1.1d">‚àô</annotation></semantics></math> Alldieck et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib172" title=""><span class="ltx_text" style="font-size:90%;">172</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.42.42.42">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.42.42.42.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.42.42.42.1.m1.1"><semantics id="S5.T2.42.42.42.1.m1.1a"><mo id="S5.T2.42.42.42.1.m1.1.1" xref="S5.T2.42.42.42.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.42.42.42.1.m1.1b"><ci id="S5.T2.42.42.42.1.m1.1.1.cmml" xref="S5.T2.42.42.42.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.42.42.42.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.42.42.42.1.m1.1d">‚àô</annotation></semantics></math> Multi-Garment Network (MGN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib173" title=""><span class="ltx_text" style="font-size:90%;">173</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.43.43.43">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.43.43.43.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.43.43.43.1.m1.1"><semantics id="S5.T2.43.43.43.1.m1.1a"><mo id="S5.T2.43.43.43.1.m1.1.1" xref="S5.T2.43.43.43.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.43.43.43.1.m1.1b"><ci id="S5.T2.43.43.43.1.m1.1.1.cmml" xref="S5.T2.43.43.43.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.43.43.43.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.43.43.43.1.m1.1d">‚àô</annotation></semantics></math> Texture map: Tex2Shape <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib174" title=""><span class="ltx_text" style="font-size:90%;">174</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.44.44.44">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.44.44.44.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.44.44.44.1.m1.1"><semantics id="S5.T2.44.44.44.1.m1.1a"><mo id="S5.T2.44.44.44.1.m1.1.1" xref="S5.T2.44.44.44.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.44.44.44.1.m1.1b"><ci id="S5.T2.44.44.44.1.m1.1.1.cmml" xref="S5.T2.44.44.44.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.44.44.44.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.44.44.44.1.m1.1d">‚àô</annotation></semantics></math> Layered garment representation: BCNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib175" title=""><span class="ltx_text" style="font-size:90%;">175</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.45.45.45">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.45.45.45.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.45.45.45.1.m1.1"><semantics id="S5.T2.45.45.45.1.m1.1a"><mo id="S5.T2.45.45.45.1.m1.1.1" xref="S5.T2.45.45.45.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.45.45.45.1.m1.1b"><ci id="S5.T2.45.45.45.1.m1.1.1.cmml" xref="S5.T2.45.45.45.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.45.45.45.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.45.45.45.1.m1.1d">‚àô</annotation></semantics></math> Temporal span: H4D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.46.46.46">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.46.46.46.2" rowspan="3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.46.46.46.2.1">With Hands</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.46.46.46.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.46.46.46.1.m1.1"><semantics id="S5.T2.46.46.46.1.m1.1a"><mo id="S5.T2.46.46.46.1.m1.1.1" xref="S5.T2.46.46.46.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.46.46.46.1.m1.1b"><ci id="S5.T2.46.46.46.1.m1.1.1.cmml" xref="S5.T2.46.46.46.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.46.46.46.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.46.46.46.1.m1.1d">‚àô</annotation></semantics></math> Linguistic priors: SGNify <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib176" title=""><span class="ltx_text" style="font-size:90%;">176</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.47.47.47">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.47.47.47.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.47.47.47.1.m1.1"><semantics id="S5.T2.47.47.47.1.m1.1a"><mo id="S5.T2.47.47.47.1.m1.1.1" xref="S5.T2.47.47.47.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.47.47.47.1.m1.1b"><ci id="S5.T2.47.47.47.1.m1.1.1.cmml" xref="S5.T2.47.47.47.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.47.47.47.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.47.47.47.1.m1.1d">‚àô</annotation></semantics></math> Two-hands interaction: <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib177" title=""><span class="ltx_text" style="font-size:90%;">177</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.48.48.48">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.48.48.48.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.48.48.48.1.m1.1"><semantics id="S5.T2.48.48.48.1.m1.1a"><mo id="S5.T2.48.48.48.1.m1.1.1" xref="S5.T2.48.48.48.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.48.48.48.1.m1.1b"><ci id="S5.T2.48.48.48.1.m1.1.1.cmml" xref="S5.T2.48.48.48.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.48.48.48.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.48.48.48.1.m1.1d">‚àô</annotation></semantics></math> Hand-object interaction: <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib178" title=""><span class="ltx_text" style="font-size:90%;">178</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.49.49.49">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.49.49.49.2" rowspan="8" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.49.49.49.2.1">Whole Body</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.49.49.49.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.49.49.49.1.m1.1"><semantics id="S5.T2.49.49.49.1.m1.1a"><mo id="S5.T2.49.49.49.1.m1.1.1" xref="S5.T2.49.49.49.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.49.49.49.1.m1.1b"><ci id="S5.T2.49.49.49.1.m1.1.1.cmml" xref="S5.T2.49.49.49.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.49.49.49.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.49.49.49.1.m1.1d">‚àô</annotation></semantics></math> PROX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib179" title=""><span class="ltx_text" style="font-size:90%;">179</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.50.50.50">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.50.50.50.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.50.50.50.1.m1.1"><semantics id="S5.T2.50.50.50.1.m1.1a"><mo id="S5.T2.50.50.50.1.m1.1.1" xref="S5.T2.50.50.50.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.50.50.50.1.m1.1b"><ci id="S5.T2.50.50.50.1.m1.1.1.cmml" xref="S5.T2.50.50.50.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.50.50.50.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.50.50.50.1.m1.1d">‚àô</annotation></semantics></math> ExPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib180" title=""><span class="ltx_text" style="font-size:90%;">180</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.51.51.51">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.51.51.51.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.51.51.51.1.m1.1"><semantics id="S5.T2.51.51.51.1.m1.1a"><mo id="S5.T2.51.51.51.1.m1.1.1" xref="S5.T2.51.51.51.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.51.51.51.1.m1.1b"><ci id="S5.T2.51.51.51.1.m1.1.1.cmml" xref="S5.T2.51.51.51.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.51.51.51.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.51.51.51.1.m1.1d">‚àô</annotation></semantics></math> FrankMocap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib181" title=""><span class="ltx_text" style="font-size:90%;">181</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.52.52.52">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.52.52.52.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.52.52.52.1.m1.1"><semantics id="S5.T2.52.52.52.1.m1.1a"><mo id="S5.T2.52.52.52.1.m1.1.1" xref="S5.T2.52.52.52.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.52.52.52.1.m1.1b"><ci id="S5.T2.52.52.52.1.m1.1.1.cmml" xref="S5.T2.52.52.52.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.52.52.52.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.52.52.52.1.m1.1d">‚àô</annotation></semantics></math> PIXIE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib182" title=""><span class="ltx_text" style="font-size:90%;">182</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.53.53.53">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.53.53.53.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.53.53.53.1.m1.1"><semantics id="S5.T2.53.53.53.1.m1.1a"><mo id="S5.T2.53.53.53.1.m1.1.1" xref="S5.T2.53.53.53.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.53.53.53.1.m1.1b"><ci id="S5.T2.53.53.53.1.m1.1.1.cmml" xref="S5.T2.53.53.53.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.53.53.53.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.53.53.53.1.m1.1d">‚àô</annotation></semantics></math> Moon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib183" title=""><span class="ltx_text" style="font-size:90%;">183</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.54.54.54">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.54.54.54.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.54.54.54.1.m1.1"><semantics id="S5.T2.54.54.54.1.m1.1a"><mo id="S5.T2.54.54.54.1.m1.1.1" xref="S5.T2.54.54.54.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.54.54.54.1.m1.1b"><ci id="S5.T2.54.54.54.1.m1.1.1.cmml" xref="S5.T2.54.54.54.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.54.54.54.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.54.54.54.1.m1.1d">‚àô</annotation></semantics></math> PyMAF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib184" title=""><span class="ltx_text" style="font-size:90%;">184</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.55.55.55">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.55.55.55.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.55.55.55.1.m1.1"><semantics id="S5.T2.55.55.55.1.m1.1a"><mo id="S5.T2.55.55.55.1.m1.1.1" xref="S5.T2.55.55.55.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.55.55.55.1.m1.1b"><ci id="S5.T2.55.55.55.1.m1.1.1.cmml" xref="S5.T2.55.55.55.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.55.55.55.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.55.55.55.1.m1.1d">‚àô</annotation></semantics></math> OSX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib185" title=""><span class="ltx_text" style="font-size:90%;">185</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.56.56.56">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.56.56.56.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.56.56.56.1.m1.1"><semantics id="S5.T2.56.56.56.1.m1.1a"><mo id="S5.T2.56.56.56.1.m1.1.1" xref="S5.T2.56.56.56.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.56.56.56.1.m1.1b"><ci id="S5.T2.56.56.56.1.m1.1.1.cmml" xref="S5.T2.56.56.56.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.56.56.56.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.56.56.56.1.m1.1d">‚àô</annotation></semantics></math> HybrIK-X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib186" title=""><span class="ltx_text" style="font-size:90%;">186</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.57.57.57">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.57.57.57.2" rowspan="13" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.57.57.57.2.1">Template-free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T2.57.57.57.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.57.57.57.3.1">Regression-based</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.57.57.57.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.57.57.57.1.m1.1"><semantics id="S5.T2.57.57.57.1.m1.1a"><mo id="S5.T2.57.57.57.1.m1.1.1" xref="S5.T2.57.57.57.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.57.57.57.1.m1.1b"><ci id="S5.T2.57.57.57.1.m1.1.1.cmml" xref="S5.T2.57.57.57.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.57.57.57.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.57.57.57.1.m1.1d">‚àô</annotation></semantics></math> FACSIMILE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib187" title=""><span class="ltx_text" style="font-size:90%;">187</span></a>]</cite>, PeeledHuman <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib188" title=""><span class="ltx_text" style="font-size:90%;">188</span></a>]</cite>, GTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib189" title=""><span class="ltx_text" style="font-size:90%;">189</span></a>]</cite>, NSF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib190" title=""><span class="ltx_text" style="font-size:90%;">190</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.58.58.58">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T2.58.58.58.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.58.58.58.2.1">Optimization-based Differentiable</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.58.58.58.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.58.58.58.1.m1.1"><semantics id="S5.T2.58.58.58.1.m1.1a"><mo id="S5.T2.58.58.58.1.m1.1.1" xref="S5.T2.58.58.58.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.58.58.58.1.m1.1b"><ci id="S5.T2.58.58.58.1.m1.1.1.cmml" xref="S5.T2.58.58.58.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.58.58.58.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.58.58.58.1.m1.1d">‚àô</annotation></semantics></math> DiffPhy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib191" title=""><span class="ltx_text" style="font-size:90%;">191</span></a>]</cite>, AG3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib192" title=""><span class="ltx_text" style="font-size:90%;">192</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.59.59.59">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T2.59.59.59.2" rowspan="4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.59.59.59.2.1">Implicit Representations</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.59.59.59.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.59.59.59.1.m1.1"><semantics id="S5.T2.59.59.59.1.m1.1a"><mo id="S5.T2.59.59.59.1.m1.1.1" xref="S5.T2.59.59.59.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.59.59.59.1.m1.1b"><ci id="S5.T2.59.59.59.1.m1.1.1.cmml" xref="S5.T2.59.59.59.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.59.59.59.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.59.59.59.1.m1.1d">‚àô</annotation></semantics></math> PIFu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib193" title=""><span class="ltx_text" style="font-size:90%;">193</span></a>]</cite>, PIFuHD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib194" title=""><span class="ltx_text" style="font-size:90%;">194</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.60.60.60">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.60.60.60.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.60.60.60.1.m1.1"><semantics id="S5.T2.60.60.60.1.m1.1a"><mo id="S5.T2.60.60.60.1.m1.1.1" xref="S5.T2.60.60.60.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.60.60.60.1.m1.1b"><ci id="S5.T2.60.60.60.1.m1.1.1.cmml" xref="S5.T2.60.60.60.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.60.60.60.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.60.60.60.1.m1.1d">‚àô</annotation></semantics></math> Canonical space: ARCH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib195" title=""><span class="ltx_text" style="font-size:90%;">195</span></a>]</cite>, ARCH++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib196" title=""><span class="ltx_text" style="font-size:90%;">196</span></a>]</cite>, CAR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib197" title=""><span class="ltx_text" style="font-size:90%;">197</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.61.61.61">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.61.61.61.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.61.61.61.1.m1.1"><semantics id="S5.T2.61.61.61.1.m1.1a"><mo id="S5.T2.61.61.61.1.m1.1.1" xref="S5.T2.61.61.61.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.61.61.61.1.m1.1b"><ci id="S5.T2.61.61.61.1.m1.1.1.cmml" xref="S5.T2.61.61.61.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.61.61.61.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.61.61.61.1.m1.1d">‚àô</annotation></semantics></math> Geometric priors: GeoPIFu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib198" title=""><span class="ltx_text" style="font-size:90%;">198</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.62.62.62">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.62.62.62.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.62.62.62.1.m1.1"><semantics id="S5.T2.62.62.62.1.m1.1a"><mo id="S5.T2.62.62.62.1.m1.1.1" xref="S5.T2.62.62.62.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.62.62.62.1.m1.1b"><ci id="S5.T2.62.62.62.1.m1.1.1.cmml" xref="S5.T2.62.62.62.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.62.62.62.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.62.62.62.1.m1.1d">‚àô</annotation></semantics></math> Novel representations: Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib199" title=""><span class="ltx_text" style="font-size:90%;">199</span></a>]</cite>, 3DNBF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib200" title=""><span class="ltx_text" style="font-size:90%;">200</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.63.63.63">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T2.63.63.63.2" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.63.63.63.2.1">Neural Radiance Fields</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.63.63.63.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.63.63.63.1.m1.1"><semantics id="S5.T2.63.63.63.1.m1.1a"><mo id="S5.T2.63.63.63.1.m1.1.1" xref="S5.T2.63.63.63.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.63.63.63.1.m1.1b"><ci id="S5.T2.63.63.63.1.m1.1.1.cmml" xref="S5.T2.63.63.63.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.63.63.63.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.63.63.63.1.m1.1d">‚àô</annotation></semantics></math> Volume deformation scheme <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib201" title=""><span class="ltx_text" style="font-size:90%;">201</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.64.64.64">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.64.64.64.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.64.64.64.1.m1.1"><semantics id="S5.T2.64.64.64.1.m1.1a"><mo id="S5.T2.64.64.64.1.m1.1.1" xref="S5.T2.64.64.64.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.64.64.64.1.m1.1b"><ci id="S5.T2.64.64.64.1.m1.1.1.cmml" xref="S5.T2.64.64.64.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.64.64.64.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.64.64.64.1.m1.1d">‚àô</annotation></semantics></math> ActorsNeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.65.65.65">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T2.65.65.65.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.65.65.65.2.1">Diffusion Models</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.65.65.65.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.65.65.65.1.m1.1"><semantics id="S5.T2.65.65.65.1.m1.1a"><mo id="S5.T2.65.65.65.1.m1.1.1" xref="S5.T2.65.65.65.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.65.65.65.1.m1.1b"><ci id="S5.T2.65.65.65.1.m1.1.1.cmml" xref="S5.T2.65.65.65.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.65.65.65.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.65.65.65.1.m1.1d">‚àô</annotation></semantics></math> HMDiff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib202" title=""><span class="ltx_text" style="font-size:90%;">202</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.66.66.66">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T2.66.66.66.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.66.66.66.2.1">Implicit + Explicit</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.66.66.66.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.66.66.66.1.m1.1"><semantics id="S5.T2.66.66.66.1.m1.1a"><mo id="S5.T2.66.66.66.1.m1.1.1" xref="S5.T2.66.66.66.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.66.66.66.1.m1.1b"><ci id="S5.T2.66.66.66.1.m1.1.1.cmml" xref="S5.T2.66.66.66.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.66.66.66.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.66.66.66.1.m1.1d">‚àô</annotation></semantics></math> HMD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib203" title=""><span class="ltx_text" style="font-size:90%;">203</span></a>]</cite>, IP-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib204" title=""><span class="ltx_text" style="font-size:90%;">204</span></a>]</cite>, PaMIR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>, Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib205" title=""><span class="ltx_text" style="font-size:90%;">205</span></a>]</cite>, ICON <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib206" title=""><span class="ltx_text" style="font-size:90%;">206</span></a>]</cite>, ECON <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib207" title=""><span class="ltx_text" style="font-size:90%;">207</span></a>]</cite>, DELTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, GETAvatar <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib208" title=""><span class="ltx_text" style="font-size:90%;">208</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.67.67.67">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T2.67.67.67.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.67.67.67.2.1">Diffusion + Explicit</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.67.67.67.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.67.67.67.1.m1.1"><semantics id="S5.T2.67.67.67.1.m1.1a"><mo id="S5.T2.67.67.67.1.m1.1.1" xref="S5.T2.67.67.67.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.67.67.67.1.m1.1b"><ci id="S5.T2.67.67.67.1.m1.1.1.cmml" xref="S5.T2.67.67.67.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.67.67.67.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.67.67.67.1.m1.1d">‚àô</annotation></semantics></math> DINAR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib209" title=""><span class="ltx_text" style="font-size:90%;">209</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.68.68.68">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T2.68.68.68.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.68.68.68.2.1">NeRF + Explicit</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.68.68.68.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.68.68.68.1.m1.1"><semantics id="S5.T2.68.68.68.1.m1.1a"><mo id="S5.T2.68.68.68.1.m1.1.1" xref="S5.T2.68.68.68.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.68.68.68.1.m1.1b"><ci id="S5.T2.68.68.68.1.m1.1.1.cmml" xref="S5.T2.68.68.68.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.68.68.68.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.68.68.68.1.m1.1d">‚àô</annotation></semantics></math> TransHuman <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib210" title=""><span class="ltx_text" style="font-size:90%;">210</span></a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.69.69.69">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="2" id="S5.T2.69.69.69.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.69.69.69.2.1">Gaussian Splatting + Explicit</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.69.69.69.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.T2.69.69.69.1.m1.1"><semantics id="S5.T2.69.69.69.1.m1.1a"><mo id="S5.T2.69.69.69.1.m1.1.1" xref="S5.T2.69.69.69.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S5.T2.69.69.69.1.m1.1b"><ci id="S5.T2.69.69.69.1.m1.1.1.cmml" xref="S5.T2.69.69.69.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.69.69.69.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.T2.69.69.69.1.m1.1d">‚àô</annotation></semantics></math> Animatable 3D Gaussian <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib211" title=""><span class="ltx_text" style="font-size:90%;">211</span></a>]</cite>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Template-based human mesh recovery</h3>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Naked human body recovery</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p1.1.1">Multimodal Methods.</span>
Multimodality in human mesh recovery harnesses the potential by combining various modalities of data, such as RGB images, depth information, and optical flow. Integrating data in different modalities can significantly enhance the robustness and precision of mesh recovery. Rong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib126" title=""><span class="ltx_text" style="font-size:90%;">126</span></a>]</cite> proposed a hybrid annotation method with a hybrid training strategy for human mesh recovery to reduce annotation costs, which effectively utilizes various types of heterogeneous annotations, including 3D and 2D annotations, body part segmentation, and dense correspondence. Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation (DTS-VIBE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib127" title=""><span class="ltx_text" style="font-size:90%;">127</span></a>]</cite> method redefines the task as a multimodal problem, merging RGB data with optical flow to achieve a more reliable estimation and address temporal inconsistencies from RGB videos. LASOR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib128" title=""><span class="ltx_text" style="font-size:90%;">128</span></a>]</cite> estimates 3D pose and shape by synthesizing occlusion-aware silhouettes and 2D keypoints data in scenes with inter-person occlusion. CLIFF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib129" title=""><span class="ltx_text" style="font-size:90%;">129</span></a>]</cite> utilizes cropped images and bounding box information from the pre-training phase as inputs to enhance the accuracy of global rotation estimation in the camera coordinate system.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p2.1.1">Utilizing Attention Mechanism.</span>
Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib212" title=""><span class="ltx_text" style="font-size:90%;">212</span></a>]</cite> as a self-attention model has demonstrated remarkable success in Natural Language Processing (NLP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib213" title=""><span class="ltx_text" style="font-size:90%;">213</span></a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib214" title=""><span class="ltx_text" style="font-size:90%;">214</span></a>]</cite>. Following this, the Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> has mirrored these successes in the field of computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib215" title=""><span class="ltx_text" style="font-size:90%;">215</span></a>]</cite>, and numerous Transformer-based methods are now being employed in mesh recovery. The attention mechanism serves to amplify the importance of certain parts in the neural network. To address the partial occlusion issues, Part Attention REgressor (PARE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib130" title=""><span class="ltx_text" style="font-size:90%;">130</span></a>]</cite> leverages the relationships between body parts derived from segmentation masks, prompting the network to enhance predictions for occluded body parts. Mesh Graphormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib131" title=""><span class="ltx_text" style="font-size:90%;">131</span></a>]</cite> utilizes a GCNN-reinforced transformer for estimating 3D mesh vertices and body joints, while a GCNN is employed to infer interactions among neighboring vertices based on pre-existing mesh topology. The architecture effectively merges graph-based networking with the attention mechanism of transformers, enabling the modeling of local and global interactions. MPS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib132" title=""><span class="ltx_text" style="font-size:90%;">132</span></a>]</cite> employs motion continuity attention to capture temporal coherence. This approach then leverages a hierarchical attentive feature mechanism to combine features from temporally adjacent representations more effectively. FastMETRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib134" title=""><span class="ltx_text" style="font-size:90%;">134</span></a>]</cite> leverages self-attention mechanisms for non-adjacent vertices based on the topology of the body‚Äôs triangular mesh to minimize memory overhead and accelerate inference speed. Xue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib135" title=""><span class="ltx_text" style="font-size:90%;">135</span></a>]</cite> proposed a learnable sampling module for human mesh recovery to reduce inherent depth ambiguity. This module aggregates global information by generating joint adaptive tokens, utilizing non-local information within the input image. METRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib136" title=""><span class="ltx_text" style="font-size:90%;">136</span></a>]</cite> is a mesh transformer for end-to-end human mesh recovery, in which the encoder captures interactions between vertices and joints, and the decoder outputs 3D joint coordinates and the mesh structure. Qiu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib133" title=""><span class="ltx_text" style="font-size:90%;">133</span></a>]</cite> proposed an end-to-end Transformer-based method for multi-person human mesh recovery in videos. This approach utilizes a spatio-temporal encoder to extract global features, followed by a spatio-temporal pose and shape decoder to predict human pose and mesh.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p3.1.1">Exploiting Temporal Information.</span>
With the advancement of video technology, video-based human mesh recovery methods that extract temporal information from adjacent frames have shown increased potential. To learn the 3D dynamics of the human body more accurately and effectively, Kanazawa et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib137" title=""><span class="ltx_text" style="font-size:90%;">137</span></a>]</cite> proposed a framework that produces smooth 3D meshes from videos. This framework also predicts past and future 3D motions by temporally encoding image features. Video Inference for Body pose and shape Estimation (VIBE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib138" title=""><span class="ltx_text" style="font-size:90%;">138</span></a>]</cite> estimates kinematically plausible motion sequences through self-attention mechanism-based temporal network and adversarial training without requiring any ground-truth 3D labels. To diminish the dependency on static features in the current frame, addressing a limitation of previous temporal-based methods, Choi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib139" title=""><span class="ltx_text" style="font-size:90%;">139</span></a>]</cite> developed the Temporally Consistent Mesh Recovery (TCMR) system. The system utilizes temporal information to ensure consistency and effectively recovers smooth 3D human motion by incorporating data from past and future frames. Multi-level Attention Encoder-Decoder (MAED) network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib140" title=""><span class="ltx_text" style="font-size:90%;">140</span></a>]</cite> captures relationships at multiple levels, including the spatial-temporal and human joint levels, through its multi-level attention mechanism. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib141" title=""><span class="ltx_text" style="font-size:90%;">141</span></a>]</cite> introduced the Temporally embedded 3D human body Pose and shape estimation (TePose) method, specifically tailored for live stream videos. They designed a motion discriminator for adversarial training, utilizing datasets without any 3D labels, through a multi-scale spatio-temporal graph convolutional network. Additionally, they employed a sequential data loading strategy to accommodate the unique start-to-end data processing requirements of live streaming. To effectively balance the learning of short-term and long-term temporal correlations, Global-to-Local Transformer (GLoT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib142" title=""><span class="ltx_text" style="font-size:90%;">142</span></a>]</cite> structurally decouples the modeling of long-term and short-term correlations.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p4">
<p class="ltx_p" id="S5.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p4.1.1">Multi-view Methods.</span>
Dong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib143" title=""><span class="ltx_text" style="font-size:90%;">143</span></a>]</cite> designed a practical multi-view framework that combines 2D observations from multi-view images into a unified 3D representation for individual instances using a confidence-aware majority voting mechanism. Sengupta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib144" title=""><span class="ltx_text" style="font-size:90%;">144</span></a>]</cite> introduced a probabilistic-based multi-view method without constraints such as specific target poses, viewpoints, or background conditions across image sequences. Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> proposed a dynamic physics-geometry consistency approach for multi-person multi-view mesh recovery. This method integrates motion priors, extrinsic camera parameters, and human mesh data to mitigate the impact of noisy human semantic data. Zhuo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib145" title=""><span class="ltx_text" style="font-size:90%;">145</span></a>]</cite> proposed a cross-view fusion method that predicts foot posture by achieving a more refined 3D intermediate representation and alleviating inconsistencies across different views.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p5">
<p class="ltx_p" id="S5.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p5.1.1">Boosting Efficiency.</span>
Maintaining excellent performance with a lightweight model and low computational cost is essential and challenging, especially in applications such as wearable devices, power-limited systems, and edge computing. SCOPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib146" title=""><span class="ltx_text" style="font-size:90%;">146</span></a>]</cite> efficiently computes the Gauss-Newton direction using 2D and 3D keypoints for human mesh recovery. The method capitalizes on inherent sparsity and employs a sparse constrained formulation, achieving real-time performance at over 30 FPS. To implement a single-stage multi-person human mesh recovery model, Body Meshes as Points (BMP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib147" title=""><span class="ltx_text" style="font-size:90%;">147</span></a>]</cite> represents multiple people as points and associates each with a single body mesh to significantly improve efficiency and performance. HeatER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib148" title=""><span class="ltx_text" style="font-size:90%;">148</span></a>]</cite> processes heatmap inputs directly to reduce memory and computational costs. Dou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib149" title=""><span class="ltx_text" style="font-size:90%;">149</span></a>]</cite> designed an efficient transformer for human mesh recovery to reduce model complexity and computational cost by removing redundant tokens.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p6">
<p class="ltx_p" id="S5.SS1.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p6.1.1">Developing Various Representations.</span>
Stable and effective feature representation is crucial for enhancing the capabilities of deep learning algorithms in human mesh recovery, as it enables the efficient extraction of meaningful patterns from complex input data. Based on the assumption that image texture remains constant across frames, TexturePose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib150" title=""><span class="ltx_text" style="font-size:90%;">150</span></a>]</cite> leverages the appearance constancy of the body across different frames. It measures the texture map for each frame using a texture consistency loss. DenseRaC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib216" title=""><span class="ltx_text" style="font-size:90%;">216</span></a>]</cite> generates a pixel-to-surface correspondence map to optimize the estimation of parameterized human pose and shape. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib151" title=""><span class="ltx_text" style="font-size:90%;">151</span></a>]</cite> established a connection between 2D pixels and 3D vertices by using dense correspondences of body parts, effectively addressing related issues. Their proposed DaNet model concentrates on learning the 2D-to-3D mapping, while the PartDrop strategy ensures that the model focuses more on complementary body parts and adjacent positional features. To address the lack of dense correspondence between image features and the 3D mesh, DecoMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib152" title=""><span class="ltx_text" style="font-size:90%;">152</span></a>]</cite> recovers the human mesh by establishing a pixel-to-surface dense correspondence map in the UV space. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib153" title=""><span class="ltx_text" style="font-size:90%;">153</span></a>]</cite> designed a two-branch network that utilizes a partial UV map to represent the human body when occluded by objects, effectively converting this map into an estimation of the 3D human shape. Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib154" title=""><span class="ltx_text" style="font-size:90%;">154</span></a>]</cite> proposed a body-center-guided representation method that predicts both the body center heatmap and the mesh parameter map. This approach describes the 3D body mesh at the pixel level, enabling one-stage multi-person 3D mesh regression. 3DCrowdNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib155" title=""><span class="ltx_text" style="font-size:90%;">155</span></a>]</cite> employs a joint-based regressor to isolate target features from others for recovering multi-person meshes in crowded scenes, which utilizes a crowded scene-robust image feature heatmap instead of the full feature map within a bounding box. Dual-stream Spatio-temporal Transformer (DSTformer) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib156" title=""><span class="ltx_text" style="font-size:90%;">156</span></a>]</cite> extracts long-range spatio-temporal relationships among skeletal joints to effectively capture unified human motion representations from large-scale and heterogeneous video data for human-centric tasks, such as human mesh recovery.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p7">
<p class="ltx_p" id="S5.SS1.SSS1.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p7.1.1">Utilizing Structural Information.</span>
The structural information of the body acts as unique prior knowledge in human mesh recovery, enhancing the understanding of body relations. Furthermore, introducing additional physical constraints, which describe the interrelationships between various body structures, can significantly improve the performance. Holopose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib157" title=""><span class="ltx_text" style="font-size:90%;">157</span></a>]</cite> employs a part-based multi-task regression network for 2D, 3D, and dense pose to estimate 3D human surfaces. Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib158" title=""><span class="ltx_text" style="font-size:90%;">158</span></a>]</cite> introduced an end-to-end method for human mesh recovery from single images and monocular videos. This method employs skeleton disentangling to reduce the complexity of decoupling and incorporates temporal coherence, efficiently capturing both short and long-term temporal cues. Hybrid Inverse Kinematics (HybrIK) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib159" title=""><span class="ltx_text" style="font-size:90%;">159</span></a>]</cite> calculates the swing rotation from 3D joints and employs a network to predict the twist rotation through the twist-and-swing decomposition. The method can tackle non-linearity challenges and misalignment between images and models in mesh estimation from 3D poses. In their later work, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib160" title=""><span class="ltx_text" style="font-size:90%;">160</span></a>]</cite> designed NIKI, a model capable of learning from both the forward and inverse processes using invertible networks. To address the non-linear mapping and drifting joint position issues, Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib161" title=""><span class="ltx_text" style="font-size:90%;">161</span></a>]</cite> introduced an uncertainty-aware method for human mesh recovery, leveraging information from 2D poses to address the inherent ambiguities in 2D. Sengupta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib162" title=""><span class="ltx_text" style="font-size:90%;">162</span></a>]</cite> presented a probabilistic approach to circumvent the ill-posed problem, which integrates the kinematic tree structure of the human body with a Gaussian distribution over SMPL parameters. This method then predicts the hierarchical matrix-fisher distribution of 3D joint rotation matrices. SGRE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib163" title=""><span class="ltx_text" style="font-size:90%;">163</span></a>]</cite> estimates the global rotation matrix of joints directly to avoid error accumulation along the kinematic chains in human mesh recovery.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p8">
<p class="ltx_p" id="S5.SS1.SSS1.p8.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p8.1.1">Choosing Appropriate Learning Strategies.</span>
SPIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib164" title=""><span class="ltx_text" style="font-size:90%;">164</span></a>]</cite> incorporates an initial estimate optimization routine into the training loop by the self-improving neural network, which can fit the body mesh estimate to 2D joints. Zanfir et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> developed a method integrating kinematic latent normalizing flow representations and dynamical models with structured, differentiable, semantic body part alignment loss functions aimed at enhancing semi-supervised and self-supervised 3D human pose and shape estimation. Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib166" title=""><span class="ltx_text" style="font-size:90%;">166</span></a>]</cite> introduced two novel loss functions for multi-person mesh recovery from single images: a distance field-based collision loss penalizing interpenetration between constructed figures and a depth ordering-aware loss addressing occlusions and promoting accurate depth ordering of targets. Madadi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib167" title=""><span class="ltx_text" style="font-size:90%;">167</span></a>]</cite> presented an unsupervised denoising autoencoder network to recover invisible landmarks using sparse motion capture data effectively. To tackle the challenges of pose failure and shape ambiguity in the unsupervised human mesh recovery task, Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> devised a strategy that decouples the task into unsupervised 3D pose estimation and leverages kinematic prior knowledge. Bilevel Online Adaptation (BOA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib217" title=""><span class="ltx_text" style="font-size:90%;">217</span></a>]</cite> employs bilevel optimization to reconcile conflicts between 2D and temporal constraints in out-of-domain streaming videos human mesh recovery. In their later work, Dynamic Bilevel Online Adaptation (DBOA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib168" title=""><span class="ltx_text" style="font-size:90%;">168</span></a>]</cite> integrates temporal constraints to compensate for the absence of 3D annotations. Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib169" title=""><span class="ltx_text" style="font-size:90%;">169</span></a>]</cite> developed Pose2UV, a single-shot human mesh recovery method capable of extracting target features under occlusions using a deep UV prior. JOTR<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib170" title=""><span class="ltx_text" style="font-size:90%;">170</span></a>]</cite> fuses 2D and 3D features and incorporates supervision for the 3D feature through a Transformer-based contrastive learning framework. ReFit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib165" title=""><span class="ltx_text" style="font-size:90%;">165</span></a>]</cite> reprojects keypoints and refines the human model via a feedback-update loop mechanism. You et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> introduced a co-evolution method for human mesh recovery that utilizes 3D pose as an intermediary. This method divides the process into two distinct stages: initially, it estimates the 3D human pose from video, and subsequently, it regresses mesh vertices based on the estimated 3D pose, combined with temporal image features. To bridge the gap between training and test data, CycleAdapt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib171" title=""><span class="ltx_text" style="font-size:90%;">171</span></a>]</cite> proposed a domain adaptation method including a mesh reconstruction network and a motion denoising network enabling more effective adaptation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Detailed human body recovery</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">Model-based human mesh recovery has moderately satisfactory results but still lacks detailed body representations; expansions on the naked parametric model now allow for parameterized depictions of various body parts and details, including clothing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib175" title=""><span class="ltx_text" style="font-size:90%;">175</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>, hands <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, face <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, and the entire body <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>, as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S2.SS2" title="2.2 Representation for human body ‚Ä£ 2 Background ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p2.1.1">With Clothes.</span> Alldieck et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib172" title=""><span class="ltx_text" style="font-size:90%;">172</span></a>]</cite> estimated the parameters of the SMPL model, including clothing and hair, from 1 to 8 frames of a monocular video. Bhatnagar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib173" title=""><span class="ltx_text" style="font-size:90%;">173</span></a>]</cite> developed the Multi-Garment Network (MGN) to reconstruct body shape and clothing layers on top of the SMPL model. Tex2Shape <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib174" title=""><span class="ltx_text" style="font-size:90%;">174</span></a>]</cite> converts the human body mesh regression problem into an image-to-image estimation task. Specifically, it predicts a partial texture map of the visible region and then reconstructs the body shape, adding details to visible and occluded parts. BCNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib175" title=""><span class="ltx_text" style="font-size:90%;">175</span></a>]</cite> features a layered garment representation atop the SMPL model and innovatively decouples the skinning weight of the garment from the body mesh. Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> introduced a method that employs a temporal span, SMPL parameters of shape and initial pose, and latent codes encoding motion and auxiliary information. This approach facilitates the recovery of detailed body shapes, including visible and occluded parts, by utilizing a texture map of the visible region.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p3">
<p class="ltx_p" id="S5.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p3.1.1">With Hands.</span>
Forte et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib176" title=""><span class="ltx_text" style="font-size:90%;">176</span></a>]</cite> proposed SGNify, a model that captures hand pose, facial expression, and body movement from sign language videos. It employs linguistic priors and constraints on 3D hand pose to effectively address the ambiguities in isolated signs. Additionally, the relationship between Two-Hands <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib177" title=""><span class="ltx_text" style="font-size:90%;">177</span></a>]</cite>, and Hand-Object <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib178" title=""><span class="ltx_text" style="font-size:90%;">178</span></a>]</cite> effectively reconstructs the hand‚Äôs details.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p4">
<p class="ltx_p" id="S5.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p4.1.1">Whole Body.</span>
To address the inconsistency between 3D human mesh and 3D scenes, Hassan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib179" title=""><span class="ltx_text" style="font-size:90%;">179</span></a>]</cite> introduced a human whole body and scenes recovery method named Proximal Relationships with Object eXclusion (PROX). EXpressive POse and Shape rEgression (ExPose) framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib180" title=""><span class="ltx_text" style="font-size:90%;">180</span></a>]</cite> employs a body-driven attention mechanism and adopts a regression approach for holistic expressive body reconstruction to mitigate local optima issues in optimization-based methods. FrankMocap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib181" title=""><span class="ltx_text" style="font-size:90%;">181</span></a>]</cite> operates by independently running 3D mesh recovery regression for face, hands, and body and subsequently combining the outputs through an integration module. PIXIE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib182" title=""><span class="ltx_text" style="font-size:90%;">182</span></a>]</cite> integrates independent estimates from the body, face, and hands using the shared shape space of SMPL-X across all body parts. Moon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib183" title=""><span class="ltx_text" style="font-size:90%;">183</span></a>]</cite> developed an end-to-end framework for whole-body human mesh recovery named Hand4Whole, which employs joint features for 3D joint rotations to enhance the accuracy of 3D hand predictions. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib184" title=""><span class="ltx_text" style="font-size:90%;">184</span></a>]</cite> enhanced the PyMAF framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib218" title=""><span class="ltx_text" style="font-size:90%;">218</span></a>]</cite>, developing PyMAF-X for the detailed reconstruction of full-body models. This advancement aims to resolve the misalignment issues in regression-based, one-stage human mesh recovery methods by employing a feature pyramid approach and refining the mesh-image alignment parameters. OSX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib185" title=""><span class="ltx_text" style="font-size:90%;">185</span></a>]</cite> employs a simple yet effective component-aware transformer that includes a global body encoder and a local face/hand decoder instead of separate networks for each part. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib186" title=""><span class="ltx_text" style="font-size:90%;">186</span></a>]</cite> extended the HybrIK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib159" title=""><span class="ltx_text" style="font-size:90%;">159</span></a>]</cite> framework and proposed HybrIK-X, a one-stage model based on a hybrid analytical-neural inverse kinematics framework to recover comprehensive whole-body meshes with details.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Template-free human body recovery</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Template-free methods for human mesh recovery, such as neural network regression models, optimization models based on differentiable rendering, implicit representation models, Neural Radiance Fields (NeRF), and Gaussian Splatting, demonstrate enhanced flexibility over template-based approaches, enabling the depiction of richer details.</p>
</div>
<figure class="ltx_figure" id="S5.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="131" id="S5.F8.sf1.g1" src="x12.png" width="164"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S5.F8.sf1.3.2" style="font-size:80%;">NeRF</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="86" id="S5.F8.sf2.g1" src="x13.png" width="435"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S5.F8.sf2.3.2" style="font-size:80%;">3D Guassian Splatting</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>(a) NeRF; (b) 3D Guassian Splatting.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Regression-based Methods.</span>
Regression-based human mesh recovery bypasses the limitations and biases inherent in template-based models and directly outputs the template-free 3D models of the human body. This approach allows for a more dynamic and flexible generation of models, which can capture a wider variety of human body shapes and postures. FACSIMILE (FAX) utilizes an image-translation network to recover geometry at the original image resolution directly, bypassing the need for indirect output through representations. Jinka et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib188" title=""><span class="ltx_text" style="font-size:90%;">188</span></a>]</cite> proposed a robust shape representation specifically for scenes with self-occlusions, where the method encodes the human body using peeled RGB and depth maps, significantly enhancing accuracy and efficiency during both training and inference. Neural Surface Fields (NSF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib190" title=""><span class="ltx_text" style="font-size:90%;">190</span></a>]</cite> models a continuous and flexible displacement field on the base surface for 3D clothed human mesh recovery from monocular depth. This approach adapts to base surfaces with varying resolutions and topologies without retraining during inference. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib189" title=""><span class="ltx_text" style="font-size:90%;">189</span></a>]</cite> introduced the Global-correlated 3D-decoupling Transformer for clothed Avatar reconstruction (GTA), a model that employs an encoder to capture globally-correlated image features and a decoder to decouple tri-plane features using cross-attention.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Optimization-based Methods.</span>
Optimization-based differentiable rendering integrates the rendering process into the optimization method by minimizing the rendering error. AG3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib192" title=""><span class="ltx_text" style="font-size:90%;">192</span></a>]</cite> captures the body‚Äôs and loose clothing‚Äôs shape and deformation by adopting a holistic 3D generator and integrating geometric cues in the form of predicted 2D normal maps. G√§rtner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib191" title=""><span class="ltx_text" style="font-size:90%;">191</span></a>]</cite> designed DiffPhy, a differentiable physics-based model that incorporates a physically plausible body representation with anatomical joint limits, significantly enhancing the performance and robustness of human body recovery.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Implicit Representation Methods.</span>
Implicit representations do not directly represent an object‚Äôs geometric data, such as vertices or meshes, but rather define whether a point in space belongs to the object through a function. The advantage of implicit representations lies in their ability to compactly and flexibly represent complex shapes, including those with intricate topologies or discontinuities. Thus, they have achieved impressive outcomes in human body reconstruction. Saito et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib193" title=""><span class="ltx_text" style="font-size:90%;">193</span></a>]</cite> proposed the Pixel-aligned Implicit Function (PIFu), an implicit representation that aligns pixels of 2D images with a 3D object, and designed an end-to-end deep learning framework for inferring both 3D surface and texture from a single image. PIFu was the first to apply implicit representation in human mesh recovery, enabling the reconstruction of geometric details of the human body in clothing. Subsequently, PIFuHD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib194" title=""><span class="ltx_text" style="font-size:90%;">194</span></a>]</cite> extended the work of PIFu to 4K resolution images, further enhancing detail. Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib195" title=""><span class="ltx_text" style="font-size:90%;">195</span></a>]</cite> designed the Animatable Reconstruction of Clothed Humans (ARCH) method by creating a semantic space and a semantic deformation field. He et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib196" title=""><span class="ltx_text" style="font-size:90%;">196</span></a>]</cite> introduced ARCH++, a co-supervising framework with cross-space consistency, which jointly estimates occupancy in both the posed and canonical spaces. They transform the human mesh recovery problem with implicit representation from pose space to canonical space for processing. However, this approach‚Äôs limitation is that it heavily relies on accurate pose estimation, and the clothing expression based on skinning weights still lacks sufficient naturalness in detail. GeoPIFu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib198" title=""><span class="ltx_text" style="font-size:90%;">198</span></a>]</cite> learns latent voxel features using a structure-aware 3D U-Net incorporating geometric priors. To tackle the ill-posed problem in representation learning when views are incredibly sparse, Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib199" title=""><span class="ltx_text" style="font-size:90%;">199</span></a>]</cite> developed a novel human body representation. This representation assumes that the neural representations learned at different frames share latent codes anchored to a deformable mesh. Clothed Avatar Reconstruction (CAR) method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib197" title=""><span class="ltx_text" style="font-size:90%;">197</span></a>]</cite> employs a learning-based implicit model to initially form the general shape in canonical space, followed by refining surface details through predicting non-rigid optimization deformation in the posed space. 3D-aware Neural Body Fitting (3DNBF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib200" title=""><span class="ltx_text" style="font-size:90%;">200</span></a>]</cite> addresses the challenges of occlusion and 2D-3D ambiguity by a volumetric human representation using Gaussian ellipsoidal kernels.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p5.1.1">Neural Radiance Fields.</span>
As illustrated in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5.F8.sf1" title="In Figure 8 ‚Ä£ 5.2 Template-free human body recovery ‚Ä£ 5 3D Human Mesh Recovery ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">8(a)</span></a>, Neural Radiance Fields <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib219" title=""><span class="ltx_text" style="font-size:90%;">219</span></a>]</cite> is based on the principle of implicit representation, using neural networks to learn the continuous volumetric density and color distribution of a scene, allowing for generating a high-quality 3D model from arbitrary viewpoints. Gao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib201" title=""><span class="ltx_text" style="font-size:90%;">201</span></a>]</cite> utilized a specialized representation that combines a canonical Neural Radiance Field (NeRF) with a volume deformation scheme, enabling the recovery of novel views and poses for a person not seen during training. Mu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> introduce ActorsNeRF, a NeRF-based human representation for human mesh recovery from a few monocular images by encoding the category-level prior through parameter sharing with a 2-level canonical space.</p>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p6.1.1">Diffusion models.</span>
Diffusion models are based on a series of diffusion processes, transforming original data by adding random noise and then gradually removing this noise to generate new data through a reverse process. Human Mesh Diffusion (HMDiff) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib202" title=""><span class="ltx_text" style="font-size:90%;">202</span></a>]</cite> treats mesh recovery as a reverse diffusion process, incorporates input-specific distribution information into the diffusion process, and introduces prior knowledge to simplify the task.</p>
</div>
<div class="ltx_para" id="S5.SS2.p7">
<p class="ltx_p" id="S5.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p7.1.1">Implicit Representation with Explicit Parameter.</span>
Methods based on implicit representations can recover free-form geometric shapes, but they may have excessive dependence on accurate poses or generate unsatisfactory shapes for novel poses or clothing. To increase robustness in these scenarios, existing work employs explicit body models to constrain mesh reconstruction in implicit methods. Researchers desire an approach that combines implicit and explicit methods based on a coarse body mesh‚Äôs geometry to refine and produce detailed human models meticulously. Hierarchical Mesh Deformation (HMD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib203" title=""><span class="ltx_text" style="font-size:90%;">203</span></a>]</cite> uses constraints from body joints, silhouettes, and per-pixel shading information to combine the robustness of a parametric model with the flexibility of free-form 3D deformation. Bhatnagar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib204" title=""><span class="ltx_text" style="font-size:90%;">204</span></a>]</cite> introduced an Implicit Part Network (IP-Net) to predict the detailed human surface, including the 3D surface of the dressed person, the inner body surface, and the parametric body model. Parametric Model-Conditioned Implicit Representation (PaMIR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> combines a parametric body model with a free-form deep implicit function to enhance generalization through regularization. Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib205" title=""><span class="ltx_text" style="font-size:90%;">205</span></a>]</cite> utilized a ‚Äôproject-predict-deform‚Äô strategy that refines the SMPL model generated by human recovery methods using supervision from joints, silhouettes, and shading information. ICON <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib206" title=""><span class="ltx_text" style="font-size:90%;">206</span></a>]</cite> generates a stable, coarse human mesh using the SMPL model, then renders the front and back body normals, which provide rich texture details and combine with the original image. Finally, the body‚Äôs normal clothed normal, and Signed Distance Function (SDF) information are fed into an implicit MLP to obtain the final result. Subsequently, Xiu et al. proposed ECON <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib207" title=""><span class="ltx_text" style="font-size:90%;">207</span></a>]</cite>, which integrates normal estimation, normal integration, and shape completion by using SMPL-X depth as a soft geometric constraint within the optimization equation of Normal Integration. It endeavors to maintain coherence with nearby surfaces during the integration of normals. Feng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> presented Disentangled Avatars (DELTA), a model representing humans with hybrid explicit-implicit 3D representations. GETAvatar <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib208" title=""><span class="ltx_text" style="font-size:90%;">208</span></a>]</cite> directly produces explicit textured 3D human meshes. GETAvatar creates an articulated 3D human representation with explicit surface modeling, enriches it with realistic surface details derived from 2D normal maps of 3D scan data, and utilizes a rasterization-based renderer for surface rendering. Diffusion Inpainting of Neural Avatars (DINAR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib209" title=""><span class="ltx_text" style="font-size:90%;">209</span></a>]</cite> combines neural textures with the SMPL-X body model and employs a latent diffusion model to recover textures of both seen and unseen regions, subsequently integrating these onto the base SMPL-X mesh. TransHuman <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib210" title=""><span class="ltx_text" style="font-size:90%;">210</span></a>]</cite> employs transformers to project the SMPL model into a canonical space and associates each output token with a deformable radiance field. This field encodes the query point in the observation space and is further utilized to integrate fine-grained information from reference images. Gaussian Splatting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib220" title=""><span class="ltx_text" style="font-size:90%;">220</span></a>]</cite> is a technique for processing and rendering point clouds, using a Gaussian function to create an influence range for each point, resulting in a smoother and more natural representation in two-dimensional images, as shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S5.F8.sf2" title="In Figure 8 ‚Ä£ 5.2 Template-free human body recovery ‚Ä£ 5 3D Human Mesh Recovery ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">8(b)</span></a>. It has achieved good results in SLAM (Simultaneous Localization and Mapping) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib221" title=""><span class="ltx_text" style="font-size:90%;">221</span></a>]</cite>, generative human modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib222" title=""><span class="ltx_text" style="font-size:90%;">222</span></a>]</cite>, dynamic scene reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib223" title=""><span class="ltx_text" style="font-size:90%;">223</span></a>]</cite>, and multimodal generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib224" title=""><span class="ltx_text" style="font-size:90%;">224</span></a>]</cite>. Reconstruction based on Gaussian Splatting requires an initial point cloud input, which poses a challenge for human mesh recovery from image inputs. However, explicit models can provide initial points, serving as a stepping stone to make Gaussian splatting human mesh recovery from RGB inputs feasible. Animatable 3D Gaussian <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib211" title=""><span class="ltx_text" style="font-size:90%;">211</span></a>]</cite> learns human avatars from input images and poses by extending 3D Gaussian to dynamic human scenes. In the proposed framework, they model a set of skinned 3D Gaussian and a corresponding skeleton in canonical space and deform 3D Gaussian to posed space according to the input poses.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Summary of human mesh recovery</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">This chapter provides a comprehensive review of human mesh recovery utilizing both explicit and implicit models. Explicit models excel in robustly reconstructing human mesh but often fall short in capturing intricate details, prompting a range of extensions for greater detail accuracy. In contrast, implicit-based human mesh recovery tends to lack stability, which is known for its flexibility and adaptability. Consequently, several studies have explored integrating implicit models with explicit counterparts to synergize their respective strengths. The trade-off between flexibility and robustness in human mesh recovery represents a pivotal and enduring area of research.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluation</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Evaluation metrics</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">There are many evaluation metrics that can be used fairly to measure the performance of deep models in human pose estimation, and some of the key evaluation metrics are provided below.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.3"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.3.1">Mean Per Joint Position Error (MPJPE)</span>
is widely used to evaluate the accuracy performance of 3D human pose estimation by calculating the L2 distance between the predicted joint coordinates and their ground truth counterparts. Denote the estimated coordinates of the j-th joint as <math alttext="p_{j}^{*}" class="ltx_Math" display="inline" id="S6.SS1.p2.1.m1.1"><semantics id="S6.SS1.p2.1.m1.1a"><msubsup id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml"><mi id="S6.SS1.p2.1.m1.1.1.2.2" xref="S6.SS1.p2.1.m1.1.1.2.2.cmml">p</mi><mi id="S6.SS1.p2.1.m1.1.1.2.3" xref="S6.SS1.p2.1.m1.1.1.2.3.cmml">j</mi><mo id="S6.SS1.p2.1.m1.1.1.3" xref="S6.SS1.p2.1.m1.1.1.3.cmml">‚àó</mo></msubsup><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><apply id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p2.1.m1.1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">superscript</csymbol><apply id="S6.SS1.p2.1.m1.1.1.2.cmml" xref="S6.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p2.1.m1.1.1.2.1.cmml" xref="S6.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S6.SS1.p2.1.m1.1.1.2.2.cmml" xref="S6.SS1.p2.1.m1.1.1.2.2">ùëù</ci><ci id="S6.SS1.p2.1.m1.1.1.2.3.cmml" xref="S6.SS1.p2.1.m1.1.1.2.3">ùëó</ci></apply><times id="S6.SS1.p2.1.m1.1.1.3.cmml" xref="S6.SS1.p2.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">p_{j}^{*}</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT</annotation></semantics></math> and the ground truth coordinates as <math alttext="p_{j}" class="ltx_Math" display="inline" id="S6.SS1.p2.2.m2.1"><semantics id="S6.SS1.p2.2.m2.1a"><msub id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml"><mi id="S6.SS1.p2.2.m2.1.1.2" xref="S6.SS1.p2.2.m2.1.1.2.cmml">p</mi><mi id="S6.SS1.p2.2.m2.1.1.3" xref="S6.SS1.p2.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><apply id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS1.p2.2.m2.1.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S6.SS1.p2.2.m2.1.1.2.cmml" xref="S6.SS1.p2.2.m2.1.1.2">ùëù</ci><ci id="S6.SS1.p2.2.m2.1.1.3.cmml" xref="S6.SS1.p2.2.m2.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">p_{j}</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.2.m2.1d">italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>. The MPJPE of <math alttext="j" class="ltx_Math" display="inline" id="S6.SS1.p2.3.m3.1"><semantics id="S6.SS1.p2.3.m3.1a"><mi id="S6.SS1.p2.3.m3.1.1" xref="S6.SS1.p2.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.3.m3.1b"><ci id="S6.SS1.p2.3.m3.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.3.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.3.m3.1d">italic_j</annotation></semantics></math>-th joint in the skeleton can be computed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="MPJPE=\frac{1}{N}\sum_{j=1}^{N}{\left\|p_{j}-p_{j}^{*}\right\|_{2}}" class="ltx_Math" display="block" id="S6.E1.m1.1"><semantics id="S6.E1.m1.1a"><mrow id="S6.E1.m1.1.1" xref="S6.E1.m1.1.1.cmml"><mrow id="S6.E1.m1.1.1.3" xref="S6.E1.m1.1.1.3.cmml"><mi id="S6.E1.m1.1.1.3.2" xref="S6.E1.m1.1.1.3.2.cmml">M</mi><mo id="S6.E1.m1.1.1.3.1" xref="S6.E1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.E1.m1.1.1.3.3" xref="S6.E1.m1.1.1.3.3.cmml">P</mi><mo id="S6.E1.m1.1.1.3.1a" xref="S6.E1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.E1.m1.1.1.3.4" xref="S6.E1.m1.1.1.3.4.cmml">J</mi><mo id="S6.E1.m1.1.1.3.1b" xref="S6.E1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.E1.m1.1.1.3.5" xref="S6.E1.m1.1.1.3.5.cmml">P</mi><mo id="S6.E1.m1.1.1.3.1c" xref="S6.E1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.E1.m1.1.1.3.6" xref="S6.E1.m1.1.1.3.6.cmml">E</mi></mrow><mo id="S6.E1.m1.1.1.2" xref="S6.E1.m1.1.1.2.cmml">=</mo><mrow id="S6.E1.m1.1.1.1" xref="S6.E1.m1.1.1.1.cmml"><mfrac id="S6.E1.m1.1.1.1.3" xref="S6.E1.m1.1.1.1.3.cmml"><mn id="S6.E1.m1.1.1.1.3.2" xref="S6.E1.m1.1.1.1.3.2.cmml">1</mn><mi id="S6.E1.m1.1.1.1.3.3" xref="S6.E1.m1.1.1.1.3.3.cmml">N</mi></mfrac><mo id="S6.E1.m1.1.1.1.2" xref="S6.E1.m1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S6.E1.m1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.cmml"><munderover id="S6.E1.m1.1.1.1.1.2" xref="S6.E1.m1.1.1.1.1.2.cmml"><mo id="S6.E1.m1.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S6.E1.m1.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S6.E1.m1.1.1.1.1.2.2.3" xref="S6.E1.m1.1.1.1.1.2.2.3.cmml"><mi id="S6.E1.m1.1.1.1.1.2.2.3.2" xref="S6.E1.m1.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S6.E1.m1.1.1.1.1.2.2.3.1" xref="S6.E1.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S6.E1.m1.1.1.1.1.2.2.3.3" xref="S6.E1.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S6.E1.m1.1.1.1.1.2.3" xref="S6.E1.m1.1.1.1.1.2.3.cmml">N</mi></munderover><msub id="S6.E1.m1.1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.1.cmml"><mrow id="S6.E1.m1.1.1.1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.1.1.2.cmml"><mo id="S6.E1.m1.1.1.1.1.1.1.1.2" xref="S6.E1.m1.1.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S6.E1.m1.1.1.1.1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S6.E1.m1.1.1.1.1.1.1.1.1.2" xref="S6.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S6.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S6.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml">p</mi><mi id="S6.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S6.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S6.E1.m1.1.1.1.1.1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msubsup id="S6.E1.m1.1.1.1.1.1.1.1.1.3" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S6.E1.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">p</mi><mi id="S6.E1.m1.1.1.1.1.1.1.1.1.3.2.3" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">j</mi><mo id="S6.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml">‚àó</mo></msubsup></mrow><mo id="S6.E1.m1.1.1.1.1.1.1.1.3" xref="S6.E1.m1.1.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S6.E1.m1.1.1.1.1.1.3" xref="S6.E1.m1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E1.m1.1b"><apply id="S6.E1.m1.1.1.cmml" xref="S6.E1.m1.1.1"><eq id="S6.E1.m1.1.1.2.cmml" xref="S6.E1.m1.1.1.2"></eq><apply id="S6.E1.m1.1.1.3.cmml" xref="S6.E1.m1.1.1.3"><times id="S6.E1.m1.1.1.3.1.cmml" xref="S6.E1.m1.1.1.3.1"></times><ci id="S6.E1.m1.1.1.3.2.cmml" xref="S6.E1.m1.1.1.3.2">ùëÄ</ci><ci id="S6.E1.m1.1.1.3.3.cmml" xref="S6.E1.m1.1.1.3.3">ùëÉ</ci><ci id="S6.E1.m1.1.1.3.4.cmml" xref="S6.E1.m1.1.1.3.4">ùêΩ</ci><ci id="S6.E1.m1.1.1.3.5.cmml" xref="S6.E1.m1.1.1.3.5">ùëÉ</ci><ci id="S6.E1.m1.1.1.3.6.cmml" xref="S6.E1.m1.1.1.3.6">ùê∏</ci></apply><apply id="S6.E1.m1.1.1.1.cmml" xref="S6.E1.m1.1.1.1"><times id="S6.E1.m1.1.1.1.2.cmml" xref="S6.E1.m1.1.1.1.2"></times><apply id="S6.E1.m1.1.1.1.3.cmml" xref="S6.E1.m1.1.1.1.3"><divide id="S6.E1.m1.1.1.1.3.1.cmml" xref="S6.E1.m1.1.1.1.3"></divide><cn id="S6.E1.m1.1.1.1.3.2.cmml" type="integer" xref="S6.E1.m1.1.1.1.3.2">1</cn><ci id="S6.E1.m1.1.1.1.3.3.cmml" xref="S6.E1.m1.1.1.1.3.3">ùëÅ</ci></apply><apply id="S6.E1.m1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1.1"><apply id="S6.E1.m1.1.1.1.1.2.cmml" xref="S6.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E1.m1.1.1.1.1.2.1.cmml" xref="S6.E1.m1.1.1.1.1.2">superscript</csymbol><apply id="S6.E1.m1.1.1.1.1.2.2.cmml" xref="S6.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E1.m1.1.1.1.1.2.2.1.cmml" xref="S6.E1.m1.1.1.1.1.2">subscript</csymbol><sum id="S6.E1.m1.1.1.1.1.2.2.2.cmml" xref="S6.E1.m1.1.1.1.1.2.2.2"></sum><apply id="S6.E1.m1.1.1.1.1.2.2.3.cmml" xref="S6.E1.m1.1.1.1.1.2.2.3"><eq id="S6.E1.m1.1.1.1.1.2.2.3.1.cmml" xref="S6.E1.m1.1.1.1.1.2.2.3.1"></eq><ci id="S6.E1.m1.1.1.1.1.2.2.3.2.cmml" xref="S6.E1.m1.1.1.1.1.2.2.3.2">ùëó</ci><cn id="S6.E1.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S6.E1.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S6.E1.m1.1.1.1.1.2.3.cmml" xref="S6.E1.m1.1.1.1.1.2.3">ùëÅ</ci></apply><apply id="S6.E1.m1.1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.E1.m1.1.1.1.1.1.2.cmml" xref="S6.E1.m1.1.1.1.1.1">subscript</csymbol><apply id="S6.E1.m1.1.1.1.1.1.1.2.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S6.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S6.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1"><minus id="S6.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S6.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S6.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.2.2">ùëù</ci><ci id="S6.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.2.3">ùëó</ci></apply><apply id="S6.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S6.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E1.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S6.E1.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3.2.2">ùëù</ci><ci id="S6.E1.m1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3.2.3">ùëó</ci></apply><times id="S6.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S6.E1.m1.1.1.1.1.1.1.1.1.3.3"></times></apply></apply></apply><cn id="S6.E1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S6.E1.m1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E1.m1.1c">MPJPE=\frac{1}{N}\sum_{j=1}^{N}{\left\|p_{j}-p_{j}^{*}\right\|_{2}}</annotation><annotation encoding="application/x-llamapun" id="S6.E1.m1.1d">italic_M italic_P italic_J italic_P italic_E = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚à• italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S6.SS1.p2.4">where the skeleton comprises <math alttext="N" class="ltx_Math" display="inline" id="S6.SS1.p2.4.m1.1"><semantics id="S6.SS1.p2.4.m1.1a"><mi id="S6.SS1.p2.4.m1.1.1" xref="S6.SS1.p2.4.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.4.m1.1b"><ci id="S6.SS1.p2.4.m1.1.1.cmml" xref="S6.SS1.p2.4.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.4.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.4.m1.1d">italic_N</annotation></semantics></math> joints, and unlike previously in 2D, where the error is quantified in pixels, the joint coordinates in 3D are measured and reported in millimeters (mm).</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.3"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.3.1">Mean Per Joint Angle Error (MPJAE)</span>
quantifies the angular discrepancy between the estimated and groundtruth joints. The function <math alttext="r_{j}^{*}" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1.1"><semantics id="S6.SS1.p3.1.m1.1a"><msubsup id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml"><mi id="S6.SS1.p3.1.m1.1.1.2.2" xref="S6.SS1.p3.1.m1.1.1.2.2.cmml">r</mi><mi id="S6.SS1.p3.1.m1.1.1.2.3" xref="S6.SS1.p3.1.m1.1.1.2.3.cmml">j</mi><mo id="S6.SS1.p3.1.m1.1.1.3" xref="S6.SS1.p3.1.m1.1.1.3.cmml">‚àó</mo></msubsup><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><apply id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p3.1.m1.1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1">superscript</csymbol><apply id="S6.SS1.p3.1.m1.1.1.2.cmml" xref="S6.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p3.1.m1.1.1.2.1.cmml" xref="S6.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S6.SS1.p3.1.m1.1.1.2.2.cmml" xref="S6.SS1.p3.1.m1.1.1.2.2">ùëü</ci><ci id="S6.SS1.p3.1.m1.1.1.2.3.cmml" xref="S6.SS1.p3.1.m1.1.1.2.3">ùëó</ci></apply><times id="S6.SS1.p3.1.m1.1.1.3.cmml" xref="S6.SS1.p3.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">r_{j}^{*}</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT</annotation></semantics></math> returns the estimated angle of <math alttext="j" class="ltx_Math" display="inline" id="S6.SS1.p3.2.m2.1"><semantics id="S6.SS1.p3.2.m2.1a"><mi id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><ci id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">j</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.2.m2.1d">italic_j</annotation></semantics></math>-th joint, and function <math alttext="r_{j}" class="ltx_Math" display="inline" id="S6.SS1.p3.3.m3.1"><semantics id="S6.SS1.p3.3.m3.1a"><msub id="S6.SS1.p3.3.m3.1.1" xref="S6.SS1.p3.3.m3.1.1.cmml"><mi id="S6.SS1.p3.3.m3.1.1.2" xref="S6.SS1.p3.3.m3.1.1.2.cmml">r</mi><mi id="S6.SS1.p3.3.m3.1.1.3" xref="S6.SS1.p3.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.3.m3.1b"><apply id="S6.SS1.p3.3.m3.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS1.p3.3.m3.1.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S6.SS1.p3.3.m3.1.1.2.cmml" xref="S6.SS1.p3.3.m3.1.1.2">ùëü</ci><ci id="S6.SS1.p3.3.m3.1.1.3.cmml" xref="S6.SS1.p3.3.m3.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.1c">r_{j}</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.3.m3.1d">italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> returns the ground truth angle. The MPJAE is computed in three dimensions as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="MPJAE=\frac{1}{3N}\sum_{j=1}^{3N}{\left|\left(r_{j}-r_{j}^{*}\right)\mathrm{%
mod}\pm 180\right|}" class="ltx_Math" display="block" id="S6.E2.m1.1"><semantics id="S6.E2.m1.1a"><mrow id="S6.E2.m1.1.1" xref="S6.E2.m1.1.1.cmml"><mrow id="S6.E2.m1.1.1.3" xref="S6.E2.m1.1.1.3.cmml"><mi id="S6.E2.m1.1.1.3.2" xref="S6.E2.m1.1.1.3.2.cmml">M</mi><mo id="S6.E2.m1.1.1.3.1" xref="S6.E2.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.E2.m1.1.1.3.3" xref="S6.E2.m1.1.1.3.3.cmml">P</mi><mo id="S6.E2.m1.1.1.3.1a" xref="S6.E2.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.E2.m1.1.1.3.4" xref="S6.E2.m1.1.1.3.4.cmml">J</mi><mo id="S6.E2.m1.1.1.3.1b" xref="S6.E2.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.E2.m1.1.1.3.5" xref="S6.E2.m1.1.1.3.5.cmml">A</mi><mo id="S6.E2.m1.1.1.3.1c" xref="S6.E2.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.E2.m1.1.1.3.6" xref="S6.E2.m1.1.1.3.6.cmml">E</mi></mrow><mo id="S6.E2.m1.1.1.2" xref="S6.E2.m1.1.1.2.cmml">=</mo><mrow id="S6.E2.m1.1.1.1" xref="S6.E2.m1.1.1.1.cmml"><mfrac id="S6.E2.m1.1.1.1.3" xref="S6.E2.m1.1.1.1.3.cmml"><mn id="S6.E2.m1.1.1.1.3.2" xref="S6.E2.m1.1.1.1.3.2.cmml">1</mn><mrow id="S6.E2.m1.1.1.1.3.3" xref="S6.E2.m1.1.1.1.3.3.cmml"><mn id="S6.E2.m1.1.1.1.3.3.2" xref="S6.E2.m1.1.1.1.3.3.2.cmml">3</mn><mo id="S6.E2.m1.1.1.1.3.3.1" xref="S6.E2.m1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S6.E2.m1.1.1.1.3.3.3" xref="S6.E2.m1.1.1.1.3.3.3.cmml">N</mi></mrow></mfrac><mo id="S6.E2.m1.1.1.1.2" xref="S6.E2.m1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S6.E2.m1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.cmml"><munderover id="S6.E2.m1.1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.2.cmml"><mo id="S6.E2.m1.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S6.E2.m1.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S6.E2.m1.1.1.1.1.2.2.3" xref="S6.E2.m1.1.1.1.1.2.2.3.cmml"><mi id="S6.E2.m1.1.1.1.1.2.2.3.2" xref="S6.E2.m1.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S6.E2.m1.1.1.1.1.2.2.3.1" xref="S6.E2.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S6.E2.m1.1.1.1.1.2.2.3.3" xref="S6.E2.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S6.E2.m1.1.1.1.1.2.3" xref="S6.E2.m1.1.1.1.1.2.3.cmml"><mn id="S6.E2.m1.1.1.1.1.2.3.2" xref="S6.E2.m1.1.1.1.1.2.3.2.cmml">3</mn><mo id="S6.E2.m1.1.1.1.1.2.3.1" xref="S6.E2.m1.1.1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S6.E2.m1.1.1.1.1.2.3.3" xref="S6.E2.m1.1.1.1.1.2.3.3.cmml">N</mi></mrow></munderover><mrow id="S6.E2.m1.1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.2.cmml"><mo id="S6.E2.m1.1.1.1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S6.E2.m1.1.1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.1.1.cmml"><mrow id="S6.E2.m1.1.1.1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">r</mi><mi id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msubsup id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">r</mi><mi id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">j</mi><mo id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">‚àó</mo></msubsup></mrow><mo id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S6.E2.m1.1.1.1.1.1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.1.1.1.1.2.cmml">‚Å¢</mo><mi id="S6.E2.m1.1.1.1.1.1.1.1.1.3" xref="S6.E2.m1.1.1.1.1.1.1.1.1.3.cmml">mod</mi></mrow><mo id="S6.E2.m1.1.1.1.1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.1.1.1.2.cmml">¬±</mo><mn id="S6.E2.m1.1.1.1.1.1.1.1.3" xref="S6.E2.m1.1.1.1.1.1.1.1.3.cmml">180</mn></mrow><mo id="S6.E2.m1.1.1.1.1.1.1.3" xref="S6.E2.m1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E2.m1.1b"><apply id="S6.E2.m1.1.1.cmml" xref="S6.E2.m1.1.1"><eq id="S6.E2.m1.1.1.2.cmml" xref="S6.E2.m1.1.1.2"></eq><apply id="S6.E2.m1.1.1.3.cmml" xref="S6.E2.m1.1.1.3"><times id="S6.E2.m1.1.1.3.1.cmml" xref="S6.E2.m1.1.1.3.1"></times><ci id="S6.E2.m1.1.1.3.2.cmml" xref="S6.E2.m1.1.1.3.2">ùëÄ</ci><ci id="S6.E2.m1.1.1.3.3.cmml" xref="S6.E2.m1.1.1.3.3">ùëÉ</ci><ci id="S6.E2.m1.1.1.3.4.cmml" xref="S6.E2.m1.1.1.3.4">ùêΩ</ci><ci id="S6.E2.m1.1.1.3.5.cmml" xref="S6.E2.m1.1.1.3.5">ùê¥</ci><ci id="S6.E2.m1.1.1.3.6.cmml" xref="S6.E2.m1.1.1.3.6">ùê∏</ci></apply><apply id="S6.E2.m1.1.1.1.cmml" xref="S6.E2.m1.1.1.1"><times id="S6.E2.m1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.2"></times><apply id="S6.E2.m1.1.1.1.3.cmml" xref="S6.E2.m1.1.1.1.3"><divide id="S6.E2.m1.1.1.1.3.1.cmml" xref="S6.E2.m1.1.1.1.3"></divide><cn id="S6.E2.m1.1.1.1.3.2.cmml" type="integer" xref="S6.E2.m1.1.1.1.3.2">1</cn><apply id="S6.E2.m1.1.1.1.3.3.cmml" xref="S6.E2.m1.1.1.1.3.3"><times id="S6.E2.m1.1.1.1.3.3.1.cmml" xref="S6.E2.m1.1.1.1.3.3.1"></times><cn id="S6.E2.m1.1.1.1.3.3.2.cmml" type="integer" xref="S6.E2.m1.1.1.1.3.3.2">3</cn><ci id="S6.E2.m1.1.1.1.3.3.3.cmml" xref="S6.E2.m1.1.1.1.3.3.3">ùëÅ</ci></apply></apply><apply id="S6.E2.m1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1"><apply id="S6.E2.m1.1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.1.1.2.1.cmml" xref="S6.E2.m1.1.1.1.1.2">superscript</csymbol><apply id="S6.E2.m1.1.1.1.1.2.2.cmml" xref="S6.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.1.1.2.2.1.cmml" xref="S6.E2.m1.1.1.1.1.2">subscript</csymbol><sum id="S6.E2.m1.1.1.1.1.2.2.2.cmml" xref="S6.E2.m1.1.1.1.1.2.2.2"></sum><apply id="S6.E2.m1.1.1.1.1.2.2.3.cmml" xref="S6.E2.m1.1.1.1.1.2.2.3"><eq id="S6.E2.m1.1.1.1.1.2.2.3.1.cmml" xref="S6.E2.m1.1.1.1.1.2.2.3.1"></eq><ci id="S6.E2.m1.1.1.1.1.2.2.3.2.cmml" xref="S6.E2.m1.1.1.1.1.2.2.3.2">ùëó</ci><cn id="S6.E2.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S6.E2.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S6.E2.m1.1.1.1.1.2.3.cmml" xref="S6.E2.m1.1.1.1.1.2.3"><times id="S6.E2.m1.1.1.1.1.2.3.1.cmml" xref="S6.E2.m1.1.1.1.1.2.3.1"></times><cn id="S6.E2.m1.1.1.1.1.2.3.2.cmml" type="integer" xref="S6.E2.m1.1.1.1.1.2.3.2">3</cn><ci id="S6.E2.m1.1.1.1.1.2.3.3.cmml" xref="S6.E2.m1.1.1.1.1.2.3.3">ùëÅ</ci></apply></apply><apply id="S6.E2.m1.1.1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.1.1.1"><abs id="S6.E2.m1.1.1.1.1.1.2.1.cmml" xref="S6.E2.m1.1.1.1.1.1.1.2"></abs><apply id="S6.E2.m1.1.1.1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S6.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.2">plus-or-minus</csymbol><apply id="S6.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1"><times id="S6.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.2"></times><apply id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2">ùëü</ci><ci id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3">ùëó</ci></apply><apply id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2">ùëü</ci><ci id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3">ùëó</ci></apply><times id="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3"></times></apply></apply><ci id="S6.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E2.m1.1.1.1.1.1.1.1.1.3">mod</ci></apply><cn id="S6.E2.m1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S6.E2.m1.1.1.1.1.1.1.1.3">180</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E2.m1.1c">MPJAE=\frac{1}{3N}\sum_{j=1}^{3N}{\left|\left(r_{j}-r_{j}^{*}\right)\mathrm{%
mod}\pm 180\right|}</annotation><annotation encoding="application/x-llamapun" id="S6.E2.m1.1d">italic_M italic_P italic_J italic_A italic_E = divide start_ARG 1 end_ARG start_ARG 3 italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 italic_N end_POSTSUPERSCRIPT | ( italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ) roman_mod ¬± 180 |</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p4.1.1">Mean Per Joint Localization Error (MPJLE)</span>
is a more perceptive and robust evaluation metric than MPJPE and MPJAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib225" title=""><span class="ltx_text" style="font-size:90%;">225</span></a>]</cite>. It allows for an adjustable tolerance level through a perceptual threshold <math alttext="t" class="ltx_Math" display="inline" id="S6.SS1.p4.1.m1.1"><semantics id="S6.SS1.p4.1.m1.1a"><mi id="S6.SS1.p4.1.m1.1.1" xref="S6.SS1.p4.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.1.m1.1b"><ci id="S6.SS1.p4.1.m1.1.1.cmml" xref="S6.SS1.p4.1.m1.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p4.1.m1.1d">italic_t</annotation></semantics></math> :</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="MPJLE=\frac{1}{N}\sum_{j=1}^{N}{\mathds{1}_{\left\|l_{j}-l_{j}^{*}\right\|_{2}%
\geq t}}" class="ltx_Math" display="block" id="S6.E3.m1.1"><semantics id="S6.E3.m1.1a"><mrow id="S6.E3.m1.1.2" xref="S6.E3.m1.1.2.cmml"><mrow id="S6.E3.m1.1.2.2" xref="S6.E3.m1.1.2.2.cmml"><mi id="S6.E3.m1.1.2.2.2" xref="S6.E3.m1.1.2.2.2.cmml">M</mi><mo id="S6.E3.m1.1.2.2.1" xref="S6.E3.m1.1.2.2.1.cmml">‚Å¢</mo><mi id="S6.E3.m1.1.2.2.3" xref="S6.E3.m1.1.2.2.3.cmml">P</mi><mo id="S6.E3.m1.1.2.2.1a" xref="S6.E3.m1.1.2.2.1.cmml">‚Å¢</mo><mi id="S6.E3.m1.1.2.2.4" xref="S6.E3.m1.1.2.2.4.cmml">J</mi><mo id="S6.E3.m1.1.2.2.1b" xref="S6.E3.m1.1.2.2.1.cmml">‚Å¢</mo><mi id="S6.E3.m1.1.2.2.5" xref="S6.E3.m1.1.2.2.5.cmml">L</mi><mo id="S6.E3.m1.1.2.2.1c" xref="S6.E3.m1.1.2.2.1.cmml">‚Å¢</mo><mi id="S6.E3.m1.1.2.2.6" xref="S6.E3.m1.1.2.2.6.cmml">E</mi></mrow><mo id="S6.E3.m1.1.2.1" xref="S6.E3.m1.1.2.1.cmml">=</mo><mrow id="S6.E3.m1.1.2.3" xref="S6.E3.m1.1.2.3.cmml"><mfrac id="S6.E3.m1.1.2.3.2" xref="S6.E3.m1.1.2.3.2.cmml"><mn id="S6.E3.m1.1.2.3.2.2" xref="S6.E3.m1.1.2.3.2.2.cmml">1</mn><mi id="S6.E3.m1.1.2.3.2.3" xref="S6.E3.m1.1.2.3.2.3.cmml">N</mi></mfrac><mo id="S6.E3.m1.1.2.3.1" xref="S6.E3.m1.1.2.3.1.cmml">‚Å¢</mo><mrow id="S6.E3.m1.1.2.3.3" xref="S6.E3.m1.1.2.3.3.cmml"><munderover id="S6.E3.m1.1.2.3.3.1" xref="S6.E3.m1.1.2.3.3.1.cmml"><mo id="S6.E3.m1.1.2.3.3.1.2.2" movablelimits="false" xref="S6.E3.m1.1.2.3.3.1.2.2.cmml">‚àë</mo><mrow id="S6.E3.m1.1.2.3.3.1.2.3" xref="S6.E3.m1.1.2.3.3.1.2.3.cmml"><mi id="S6.E3.m1.1.2.3.3.1.2.3.2" xref="S6.E3.m1.1.2.3.3.1.2.3.2.cmml">j</mi><mo id="S6.E3.m1.1.2.3.3.1.2.3.1" xref="S6.E3.m1.1.2.3.3.1.2.3.1.cmml">=</mo><mn id="S6.E3.m1.1.2.3.3.1.2.3.3" xref="S6.E3.m1.1.2.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S6.E3.m1.1.2.3.3.1.3" xref="S6.E3.m1.1.2.3.3.1.3.cmml">N</mi></munderover><msub id="S6.E3.m1.1.2.3.3.2" xref="S6.E3.m1.1.2.3.3.2.cmml"><mn id="S6.E3.m1.1.2.3.3.2.2" xref="S6.E3.m1.1.2.3.3.2.2.cmml">ùüô</mn><mrow id="S6.E3.m1.1.1.1" xref="S6.E3.m1.1.1.1.cmml"><msub id="S6.E3.m1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.cmml"><mrow id="S6.E3.m1.1.1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.1.2.cmml"><mo id="S6.E3.m1.1.1.1.1.1.1.2" xref="S6.E3.m1.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S6.E3.m1.1.1.1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.1.1.1.cmml"><msub id="S6.E3.m1.1.1.1.1.1.1.1.2" xref="S6.E3.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S6.E3.m1.1.1.1.1.1.1.1.2.2" xref="S6.E3.m1.1.1.1.1.1.1.1.2.2.cmml">l</mi><mi id="S6.E3.m1.1.1.1.1.1.1.1.2.3" xref="S6.E3.m1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S6.E3.m1.1.1.1.1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msubsup id="S6.E3.m1.1.1.1.1.1.1.1.3" xref="S6.E3.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S6.E3.m1.1.1.1.1.1.1.1.3.2.2" xref="S6.E3.m1.1.1.1.1.1.1.1.3.2.2.cmml">l</mi><mi id="S6.E3.m1.1.1.1.1.1.1.1.3.2.3" xref="S6.E3.m1.1.1.1.1.1.1.1.3.2.3.cmml">j</mi><mo id="S6.E3.m1.1.1.1.1.1.1.1.3.3" xref="S6.E3.m1.1.1.1.1.1.1.1.3.3.cmml">‚àó</mo></msubsup></mrow><mo id="S6.E3.m1.1.1.1.1.1.1.3" xref="S6.E3.m1.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S6.E3.m1.1.1.1.1.3" xref="S6.E3.m1.1.1.1.1.3.cmml">2</mn></msub><mo id="S6.E3.m1.1.1.1.2" xref="S6.E3.m1.1.1.1.2.cmml">‚â•</mo><mi id="S6.E3.m1.1.1.1.3" xref="S6.E3.m1.1.1.1.3.cmml">t</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E3.m1.1b"><apply id="S6.E3.m1.1.2.cmml" xref="S6.E3.m1.1.2"><eq id="S6.E3.m1.1.2.1.cmml" xref="S6.E3.m1.1.2.1"></eq><apply id="S6.E3.m1.1.2.2.cmml" xref="S6.E3.m1.1.2.2"><times id="S6.E3.m1.1.2.2.1.cmml" xref="S6.E3.m1.1.2.2.1"></times><ci id="S6.E3.m1.1.2.2.2.cmml" xref="S6.E3.m1.1.2.2.2">ùëÄ</ci><ci id="S6.E3.m1.1.2.2.3.cmml" xref="S6.E3.m1.1.2.2.3">ùëÉ</ci><ci id="S6.E3.m1.1.2.2.4.cmml" xref="S6.E3.m1.1.2.2.4">ùêΩ</ci><ci id="S6.E3.m1.1.2.2.5.cmml" xref="S6.E3.m1.1.2.2.5">ùêø</ci><ci id="S6.E3.m1.1.2.2.6.cmml" xref="S6.E3.m1.1.2.2.6">ùê∏</ci></apply><apply id="S6.E3.m1.1.2.3.cmml" xref="S6.E3.m1.1.2.3"><times id="S6.E3.m1.1.2.3.1.cmml" xref="S6.E3.m1.1.2.3.1"></times><apply id="S6.E3.m1.1.2.3.2.cmml" xref="S6.E3.m1.1.2.3.2"><divide id="S6.E3.m1.1.2.3.2.1.cmml" xref="S6.E3.m1.1.2.3.2"></divide><cn id="S6.E3.m1.1.2.3.2.2.cmml" type="integer" xref="S6.E3.m1.1.2.3.2.2">1</cn><ci id="S6.E3.m1.1.2.3.2.3.cmml" xref="S6.E3.m1.1.2.3.2.3">ùëÅ</ci></apply><apply id="S6.E3.m1.1.2.3.3.cmml" xref="S6.E3.m1.1.2.3.3"><apply id="S6.E3.m1.1.2.3.3.1.cmml" xref="S6.E3.m1.1.2.3.3.1"><csymbol cd="ambiguous" id="S6.E3.m1.1.2.3.3.1.1.cmml" xref="S6.E3.m1.1.2.3.3.1">superscript</csymbol><apply id="S6.E3.m1.1.2.3.3.1.2.cmml" xref="S6.E3.m1.1.2.3.3.1"><csymbol cd="ambiguous" id="S6.E3.m1.1.2.3.3.1.2.1.cmml" xref="S6.E3.m1.1.2.3.3.1">subscript</csymbol><sum id="S6.E3.m1.1.2.3.3.1.2.2.cmml" xref="S6.E3.m1.1.2.3.3.1.2.2"></sum><apply id="S6.E3.m1.1.2.3.3.1.2.3.cmml" xref="S6.E3.m1.1.2.3.3.1.2.3"><eq id="S6.E3.m1.1.2.3.3.1.2.3.1.cmml" xref="S6.E3.m1.1.2.3.3.1.2.3.1"></eq><ci id="S6.E3.m1.1.2.3.3.1.2.3.2.cmml" xref="S6.E3.m1.1.2.3.3.1.2.3.2">ùëó</ci><cn id="S6.E3.m1.1.2.3.3.1.2.3.3.cmml" type="integer" xref="S6.E3.m1.1.2.3.3.1.2.3.3">1</cn></apply></apply><ci id="S6.E3.m1.1.2.3.3.1.3.cmml" xref="S6.E3.m1.1.2.3.3.1.3">ùëÅ</ci></apply><apply id="S6.E3.m1.1.2.3.3.2.cmml" xref="S6.E3.m1.1.2.3.3.2"><csymbol cd="ambiguous" id="S6.E3.m1.1.2.3.3.2.1.cmml" xref="S6.E3.m1.1.2.3.3.2">subscript</csymbol><cn id="S6.E3.m1.1.2.3.3.2.2.cmml" type="integer" xref="S6.E3.m1.1.2.3.3.2.2">1</cn><apply id="S6.E3.m1.1.1.1.cmml" xref="S6.E3.m1.1.1.1"><geq id="S6.E3.m1.1.1.1.2.cmml" xref="S6.E3.m1.1.1.1.2"></geq><apply id="S6.E3.m1.1.1.1.1.cmml" xref="S6.E3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.1.1.2.cmml" xref="S6.E3.m1.1.1.1.1">subscript</csymbol><apply id="S6.E3.m1.1.1.1.1.1.2.cmml" xref="S6.E3.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S6.E3.m1.1.1.1.1.1.2.1.cmml" xref="S6.E3.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S6.E3.m1.1.1.1.1.1.1.1.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1"><minus id="S6.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.1"></minus><apply id="S6.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S6.E3.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.2.2">ùëô</ci><ci id="S6.E3.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.2.3">ùëó</ci></apply><apply id="S6.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S6.E3.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S6.E3.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.3.2.2">ùëô</ci><ci id="S6.E3.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.3.2.3">ùëó</ci></apply><times id="S6.E3.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.3.3"></times></apply></apply></apply><cn id="S6.E3.m1.1.1.1.1.3.cmml" type="integer" xref="S6.E3.m1.1.1.1.1.3">2</cn></apply><ci id="S6.E3.m1.1.1.1.3.cmml" xref="S6.E3.m1.1.1.1.3">ùë°</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E3.m1.1c">MPJLE=\frac{1}{N}\sum_{j=1}^{N}{\mathds{1}_{\left\|l_{j}-l_{j}^{*}\right\|_{2}%
\geq t}}</annotation><annotation encoding="application/x-llamapun" id="S6.E3.m1.1d">italic_M italic_P italic_J italic_L italic_E = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT blackboard_1 start_POSTSUBSCRIPT ‚à• italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚â• italic_t end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S6.SS1.p4.2">where <math alttext="l(\cdot)" class="ltx_Math" display="inline" id="S6.SS1.p4.2.m1.1"><semantics id="S6.SS1.p4.2.m1.1a"><mrow id="S6.SS1.p4.2.m1.1.2" xref="S6.SS1.p4.2.m1.1.2.cmml"><mi id="S6.SS1.p4.2.m1.1.2.2" xref="S6.SS1.p4.2.m1.1.2.2.cmml">l</mi><mo id="S6.SS1.p4.2.m1.1.2.1" xref="S6.SS1.p4.2.m1.1.2.1.cmml">‚Å¢</mo><mrow id="S6.SS1.p4.2.m1.1.2.3.2" xref="S6.SS1.p4.2.m1.1.2.cmml"><mo id="S6.SS1.p4.2.m1.1.2.3.2.1" stretchy="false" xref="S6.SS1.p4.2.m1.1.2.cmml">(</mo><mo id="S6.SS1.p4.2.m1.1.1" lspace="0em" rspace="0em" xref="S6.SS1.p4.2.m1.1.1.cmml">‚ãÖ</mo><mo id="S6.SS1.p4.2.m1.1.2.3.2.2" stretchy="false" xref="S6.SS1.p4.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.2.m1.1b"><apply id="S6.SS1.p4.2.m1.1.2.cmml" xref="S6.SS1.p4.2.m1.1.2"><times id="S6.SS1.p4.2.m1.1.2.1.cmml" xref="S6.SS1.p4.2.m1.1.2.1"></times><ci id="S6.SS1.p4.2.m1.1.2.2.cmml" xref="S6.SS1.p4.2.m1.1.2.2">ùëô</ci><ci id="S6.SS1.p4.2.m1.1.1.cmml" xref="S6.SS1.p4.2.m1.1.1">‚ãÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.2.m1.1c">l(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p4.2.m1.1d">italic_l ( ‚ãÖ )</annotation></semantics></math> indicates the joint localization.</p>
</div>
<div class="ltx_para" id="S6.SS1.p5">
<p class="ltx_p" id="S6.SS1.p5.1">There are various modifications of MPJPE, MPJAE, and MPJLE, including <span class="ltx_text ltx_font_bold" id="S6.SS1.p5.1.1">Procrustes-aligned (PA-) </span>MPJPE, MPJAE, MPJLE and <span class="ltx_text ltx_font_bold" id="S6.SS1.p5.1.2">Normalized (N-)</span> MPJPE, MPJAE, MPJLE. The former refers to procrustes-aligned metrics, while the latter denotes normalized metrics. Additionally, some 2D metrics are adaptable to 3D, such as the <span class="ltx_text ltx_font_bold" id="S6.SS1.p5.1.3">3D Percentage of Correct Keypoints (3D PCK)</span> and the <span class="ltx_text ltx_font_bold" id="S6.SS1.p5.1.4">3D Area Under the Curve (3D AUC)</span>. The PCK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib226" title=""><span class="ltx_text" style="font-size:90%;">226</span></a>]</cite> measures the distance between predicted and groundtruth keypoints, considering keypoints correct if this distance is less than a predefined threshold. The AUC signifies the aggregate area beneath the PCK threshold curve as the threshold varies.</p>
</div>
<div class="ltx_para" id="S6.SS1.p6">
<p class="ltx_p" id="S6.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p6.1.1">Mean Per Vertex Position Error (MPVPE)</span>
is a metric used to evaluate the human mesh reconstruction by computing the L2 distance between the predicted and ground truth mesh points. In some published articles, the MPVPE is also called Vertex-to-Vertex (V2V) error and Per-Vertex Error (PVE).</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Datasets</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Datasets are essential in developing deep learning-based human pose estimation and mesh recovery. Researchers have developed many datasets to train models and facilitate fair comparisons among different methods. In this section, we introduce the details of related datasets from recent years. Summary of these datasets, including information on 3D pose and 3D mesh, are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S6.T3" title="Table 3 ‚Ä£ 6.2 Datasets ‚Ä£ 6 Evaluation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The overview of the mainstream datasets.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.1" style="width:346.9pt;height:137pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-146.8pt,57.8pt) scale(0.541555948011419,0.541555948011419) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.2.1">Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.3.1">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.4.1">Total frames</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.5.1">Feature</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.6.1">Download link</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib225" title=""><span class="ltx_text" style="font-size:90%;">225</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">3D/Mesh</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">Video</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.6M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">multi-view</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.1.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="http://vision.imar.ro/human3.6m/description.php" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.3.2">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.2.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.3.2.1.1" style="background-color:#D9D9D9;">3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib227" title=""><span class="ltx_text" style="font-size:90%;">227</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.2.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.3.2.2.1" style="background-color:#D9D9D9;">3D/Mesh</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.2.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.3.2.3.1" style="background-color:#D9D9D9;">Video</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.2.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.3.2.4.1" style="background-color:#D9D9D9;">51K</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.2.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.3.2.5.1" style="background-color:#D9D9D9;">multi-person</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.2.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://virtualhumans.mpi-inf.mpg.de/3DPW/" style="background-color:#D9D9D9;" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.4.3">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib228" title=""><span class="ltx_text" style="font-size:90%;">228</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2D/3D</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">Video</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">2K</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.3.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">in-wild</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.3.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://vcai.mpi-inf.mpg.de/3dhp-dataset/" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.5.4">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.4.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.5.4.1.1" style="background-color:#D9D9D9;">HumanEva <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib229" title=""><span class="ltx_text" style="font-size:90%;">229</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.4.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.5.4.2.1" style="background-color:#D9D9D9;">3D</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.4.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.5.4.3.1" style="background-color:#D9D9D9;">Video</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.4.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.5.4.4.1" style="background-color:#D9D9D9;">40K</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.4.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.5.4.5.1" style="background-color:#D9D9D9;">multi-view</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.4.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="http://humaneva.is.tue.mpg.de/" style="background-color:#D9D9D9;" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.6.5">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">CMU-Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib230" title=""><span class="ltx_text" style="font-size:90%;">230</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.5.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">3D</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.5.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">Video</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.5.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.5M</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.5.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">multi-view/multi-person</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.5.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://domedb.perception.cs.cmu.edu/" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.7.6">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.6.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.7.6.1.1" style="background-color:#D9D9D9;">MuCo-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib118" title=""><span class="ltx_text" style="font-size:90%;">118</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.6.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.7.6.2.1" style="background-color:#D9D9D9;">3D</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.6.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.7.6.3.1" style="background-color:#D9D9D9;">Image</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.6.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.7.6.4.1" style="background-color:#D9D9D9;">8K</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.6.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.7.6.5.1" style="background-color:#D9D9D9;">multi-person/occluded scence</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.6.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://vcai.mpi-inf.mpg.de/projects/SingleShotMultiPerson/" style="background-color:#D9D9D9;" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.8.7">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.7.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib231" title=""><span class="ltx_text" style="font-size:90%;">231</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.7.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2D/3D/Mesh</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.7.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">Video</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.7.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">6.0M</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.7.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">synthetic model</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.7.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://www.di.ens.fr/willow/research/surreal/data/" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.9.8">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.9.8.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.9.8.1.1" style="background-color:#D9D9D9;">3DOH50K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib153" title=""><span class="ltx_text" style="font-size:90%;">153</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.9.8.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.9.8.2.1" style="background-color:#D9D9D9;">2D/3D/Mesh</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.9.8.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.9.8.3.1" style="background-color:#D9D9D9;">Image</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.9.8.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.9.8.4.1" style="background-color:#D9D9D9;">51K</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.9.8.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.9.8.5.1" style="background-color:#D9D9D9;">object-occluded</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.9.8.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://www.yangangwang.com/#me" style="background-color:#D9D9D9;" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.10.9">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.9.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">3DCP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib232" title=""><span class="ltx_text" style="font-size:90%;">232</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.9.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">Mesh</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.9.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">Mesh</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.9.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">190</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.9.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">contact</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.9.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://tuch.is.tue.mpg.de/" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.11.10">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.10.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.11.10.1.1" style="background-color:#D9D9D9;">AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib233" title=""><span class="ltx_text" style="font-size:90%;">233</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.10.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.11.10.2.1" style="background-color:#D9D9D9;">Mesh</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.10.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.11.10.3.1" style="background-color:#D9D9D9;">Motion</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.10.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.11.10.4.1" style="background-color:#D9D9D9;">11K</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.10.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.11.10.5.1" style="background-color:#D9D9D9;">soft-tissue dynamics</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.10.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://amass.is.tue.mpg.de/" style="background-color:#D9D9D9;" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.12.11">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.12.11.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">DensePose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib234" title=""><span class="ltx_text" style="font-size:90%;">234</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.12.11.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">Mesh</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.12.11.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">Image</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.12.11.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">50K</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.12.11.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">multi-person</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.12.11.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="http://densepose.org/" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.13.12">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.12.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.13.12.1.1" style="background-color:#D9D9D9;">UP-3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib235" title=""><span class="ltx_text" style="font-size:90%;">235</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.12.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.13.12.2.1" style="background-color:#D9D9D9;">3D/Mesh</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.12.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.13.12.3.1" style="background-color:#D9D9D9;">Image</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.12.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.13.12.4.1" style="background-color:#D9D9D9;">8K</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.12.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T3.1.1.13.12.5.1" style="background-color:#D9D9D9;">sport scence</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.12.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://files.is.tuebingen.mpg.de/classner/up/" style="background-color:#D9D9D9;" title="">Website</a></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.14.13.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">THuman2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib236" title=""><span class="ltx_text" style="font-size:90%;">236</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.14.13.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">Mesh</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.14.13.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">Image</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.14.13.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">7K</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.14.13.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">textured surface</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.14.13.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/ytrock/THuman2.0-Dataset" title="">Website</a></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.1.1">Human3.6M dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib225" title=""><span class="ltx_text" style="font-size:90%;">225</span></a>]</cite> is the most widely used dataset in the evaluation of 3D human pose estimation. It encompasses a vast collection of 3.6 million poses captured using RGB and ToF cameras from diverse viewpoints within a real-world setting. Additionally, this dataset incorporates high-resolution 3D scanner data of body meshes, playing a pivotal role in the progression of human sensing systems. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S6.T4" title="Table 4 ‚Ä£ 6.2 Datasets ‚Ä£ 6 Evaluation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">4</span></a> exhibits the performance of state-of-the-art 3D human pose estimation methods on the Human3.6M dataset.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparisons of 3D pose estimation methods on Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib225" title=""><span class="ltx_text" style="font-size:90%;">225</span></a>]</cite>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T4.2" style="width:346.9pt;height:217.8pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-142.7pt,89.4pt) scale(0.548670009442933,0.548670009442933) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T4.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T4.2.2.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.2.3.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T4.2.2.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.2.4.1">Year</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T4.2.2.2.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.2.5.1">Publication</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T4.2.2.2.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.2.6.1">Highlight</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T4.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.1">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T4.1.1.1.1.1.m1.1"><semantics id="S6.T4.1.1.1.1.1.m1.1a"><mo id="S6.T4.1.1.1.1.1.m1.1.1" stretchy="false" xref="S6.T4.1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.1.m1.1b"><ci id="S6.T4.1.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T4.1.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T4.2.2.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.2.2.1">PMPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T4.2.2.2.2.1.m1.1"><semantics id="S6.T4.2.2.2.2.1.m1.1a"><mo id="S6.T4.2.2.2.2.1.m1.1.1" stretchy="false" xref="S6.T4.2.2.2.2.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.2.1.m1.1b"><ci id="S6.T4.2.2.2.2.1.m1.1.1.cmml" xref="S6.T4.2.2.2.2.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T4.2.2.2.2.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T4.2.2.2.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.2.7.1">Code</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.2.2.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.3.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Graformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib237" title=""><span class="ltx_text" style="font-size:90%;">237</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.3.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2022</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.3.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.3.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">graph-based transformer</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.3.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">35.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.3.1.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.3.1.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/Graformer/GraFormer" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.4.2">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.4.2.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.4.2.1.1" style="background-color:#D9D9D9;">GLA-GCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib238" title=""><span class="ltx_text" style="font-size:90%;">238</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.4.2.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.4.2.2.1" style="background-color:#D9D9D9;">2023</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.4.2.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.4.2.3.1" style="background-color:#D9D9D9;">ICCV‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.4.2.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.4.2.4.1" style="background-color:#D9D9D9;">adaptive GCN</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.4.2.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.4.2.5.1" style="background-color:#D9D9D9;">34.4</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.4.2.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.4.2.6.1" style="background-color:#D9D9D9;">37.8</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.4.2.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/bruceyo/GLA-GCN" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.5.3">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.5.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">PoseDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.5.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2023</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.5.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">arXiv‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.5.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">domain adaptation</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.5.3.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.4</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.5.3.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">34.2</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.5.3.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/rese1f/PoseDA" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.6.4">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.6.4.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.6.4.1.1" style="background-color:#D9D9D9;">GFPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib239" title=""><span class="ltx_text" style="font-size:90%;">239</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.6.4.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.6.4.2.1" style="background-color:#D9D9D9;">2023</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.6.4.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.6.4.3.1" style="background-color:#D9D9D9;">CVPR‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.6.4.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.6.4.4.1" style="background-color:#D9D9D9;">gradient fields</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.6.4.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.6.4.5.1" style="background-color:#D9D9D9;">35.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.6.4.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.6.4.6.1" style="background-color:#D9D9D9;">30.5</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.6.4.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://sites.google.com/view/gfpose/" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.7.5">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.7.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">TP-LSTMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib240" title=""><span class="ltx_text" style="font-size:90%;">240</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.7.5.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2022</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.7.5.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">TPAMI‚Äô22</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.7.5.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">pose similarity metric</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.7.5.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">40.5</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.7.5.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">31.8</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.7.5.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.8.6">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.8.6.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.8.6.1.1" style="background-color:#D9D9D9;">FTCM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib122" title=""><span class="ltx_text" style="font-size:90%;">122</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.8.6.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.8.6.2.1" style="background-color:#D9D9D9;">2023</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.8.6.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.8.6.3.1" style="background-color:#D9D9D9;">TCSVT‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.8.6.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.8.6.4.1" style="background-color:#D9D9D9;">frequency-temporal collaborative</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.8.6.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.8.6.5.1" style="background-color:#D9D9D9;">28.1</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.8.6.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.8.6.6.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.8.6.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/zhenhuat/FTCM" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.9.7">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.9.7.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">VideoPose3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib89" title=""><span class="ltx_text" style="font-size:90%;">89</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.9.7.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2019</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.9.7.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô19</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.9.7.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">semi-supervised</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.9.7.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">46.8</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.9.7.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.5</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.9.7.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/facebookresearch/VideoPose3D" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.10.8">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.10.8.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.10.8.1.1" style="background-color:#D9D9D9;">PoseFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.10.8.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.10.8.2.1" style="background-color:#D9D9D9;">2021</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.10.8.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.10.8.3.1" style="background-color:#D9D9D9;">ICCV‚Äô21</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.10.8.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.10.8.4.1" style="background-color:#D9D9D9;">spatio-temporal transformer</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.10.8.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.10.8.5.1" style="background-color:#D9D9D9;">44.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.10.8.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.10.8.6.1" style="background-color:#D9D9D9;">34.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.10.8.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/zczcwh/PoseFormer" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.11.9">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.11.9.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">STCFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib96" title=""><span class="ltx_text" style="font-size:90%;">96</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.11.9.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2023</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.11.9.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.11.9.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">spatio-temporal transformer</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.11.9.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">40.5</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.11.9.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">31.8</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.11.9.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/zhenhuat/STCFormer" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.12.10">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.12.10.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.12.10.1.1" style="background-color:#D9D9D9;">3Dpose_ssl <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">85</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.12.10.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.12.10.2.1" style="background-color:#D9D9D9;">2020</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.12.10.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.12.10.3.1" style="background-color:#D9D9D9;">TPAMI‚Äô20</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.12.10.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.12.10.4.1" style="background-color:#D9D9D9;">self-supervised</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.12.10.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.12.10.5.1" style="background-color:#D9D9D9;">63.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.12.10.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.12.10.6.1" style="background-color:#D9D9D9;">63.7</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.12.10.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/chanyn/3Dpose_ssl" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.13.11">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.13.11.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">MTF-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.13.11.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2022</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.13.11.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">TPAMI‚Äô22</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.13.11.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">multi-view temporal fusion</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.13.11.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">26.2</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.13.11.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.13.11.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/lelexx/MTF-Transformer" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.14.12">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.14.12.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.14.12.1.1" style="background-color:#D9D9D9;">AdaptPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib88" title=""><span class="ltx_text" style="font-size:90%;">88</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.14.12.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.14.12.2.1" style="background-color:#D9D9D9;">2022</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.14.12.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.14.12.3.1" style="background-color:#D9D9D9;">CVPR‚Äô22</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.14.12.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.14.12.4.1" style="background-color:#D9D9D9;">cross datasets</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.14.12.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.14.12.5.1" style="background-color:#D9D9D9;">42.5</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.14.12.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.14.12.6.1" style="background-color:#D9D9D9;">34.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.14.12.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/mgholamikn/AdaptPose" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.15.13">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.15.13.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">3D-HPE-PAA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib101" title=""><span class="ltx_text" style="font-size:90%;">101</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.15.13.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2022</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.15.13.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">TIP‚Äô22</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.15.13.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">part aware attention</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.15.13.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">43.1</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.15.13.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">33.7</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.15.13.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/thuxyz19/3D-HPE-PAA" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.16.14">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.16.14.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.16.14.1.1" style="background-color:#D9D9D9;">DeciWatch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib241" title=""><span class="ltx_text" style="font-size:90%;">241</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.16.14.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.16.14.2.1" style="background-color:#D9D9D9;">2022</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.16.14.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.16.14.3.1" style="background-color:#D9D9D9;">ECCV‚Äô22</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.16.14.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.16.14.4.1" style="background-color:#D9D9D9;">efficient framework</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.16.14.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.16.14.5.1" style="background-color:#D9D9D9;">52.8</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.16.14.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.16.14.6.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.16.14.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/cure-lab/DeciWatch" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.17.15">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.17.15.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Diffpose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib242" title=""><span class="ltx_text" style="font-size:90%;">242</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.17.15.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2023</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.17.15.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.17.15.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">pose refine</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.17.15.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.9</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.17.15.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">28.7</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.17.15.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://gongjia0208.github.io/Diffpose/" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.18.16">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.18.16.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.18.16.1.1" style="background-color:#D9D9D9;">Elepose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.18.16.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.18.16.2.1" style="background-color:#D9D9D9;">2022</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.18.16.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.18.16.3.1" style="background-color:#D9D9D9;">CVPR‚Äô22</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.18.16.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.18.16.4.1" style="background-color:#D9D9D9;">unsupervised</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.18.16.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.18.16.5.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.18.16.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.18.16.6.1" style="background-color:#D9D9D9;">36.7</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.18.16.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/bastianwandt/ElePose" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.19.17">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.19.17.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Uplift and Upsample <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.19.17.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2023</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.19.17.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.19.17.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">efficient transformers</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.19.17.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.1</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.19.17.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">37.6</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.19.17.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/goldbricklemon/uplift-upsample-3dhpe" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.20.18">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.20.18.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.20.18.1.1" style="background-color:#D9D9D9;">RS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.20.18.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.20.18.2.1" style="background-color:#D9D9D9;">2023</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.20.18.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.20.18.3.1" style="background-color:#D9D9D9;">TIP‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.20.18.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.20.18.4.1" style="background-color:#D9D9D9;">regular splitting graph network</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.20.18.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.20.18.5.1" style="background-color:#D9D9D9;">48.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.20.18.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.20.18.6.1" style="background-color:#D9D9D9;">38.9</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.20.18.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/nies14/RS-Net" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.21.19">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.21.19.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">HSTFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib95" title=""><span class="ltx_text" style="font-size:90%;">95</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.21.19.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2023</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.21.19.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">arXiv‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.21.19.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">spatial-temporal transformers</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.21.19.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">42.7</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.21.19.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">33.7</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.21.19.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/qianxiaoye825/HSTFormer" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.22.20">
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.22.20.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.22.20.1.1" style="background-color:#D9D9D9;">PoseFormerV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.22.20.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.22.20.2.1" style="background-color:#D9D9D9;">2023</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.22.20.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.22.20.3.1" style="background-color:#D9D9D9;">CVPR‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.22.20.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.22.20.4.1" style="background-color:#D9D9D9;">frequency domain</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.22.20.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.22.20.5.1" style="background-color:#D9D9D9;">45.2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.22.20.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T4.2.2.22.20.6.1" style="background-color:#D9D9D9;">35.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.22.20.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/QitaoZhao/PoseFormerV2" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.23.21">
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T4.2.2.23.21.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">DiffPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib243" title=""><span class="ltx_text" style="font-size:90%;">243</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T4.2.2.23.21.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2023</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T4.2.2.23.21.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">ICCV‚Äô23</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T4.2.2.23.21.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">diffusion models</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T4.2.2.23.21.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">42.9</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T4.2.2.23.21.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">30.8</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T4.2.2.23.21.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/bastianwandt/DiffPose/" title="">Code</a></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p3.1.1">MPI-INF-3DHP dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib228" title=""><span class="ltx_text" style="font-size:90%;">228</span></a>]</cite> offers over 2K videos featuring joint annotations of 13 keypoints in outdoor scenes, apt for 2D and 3D human pose estimation. The ground truth was obtained using a multi-camera arrangement and a marker-less MoCap system, representing a shift from traditional marker-based MoCap systems that involve real individuals. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S6.T5" title="Table 5 ‚Ä£ 6.2 Datasets ‚Ä£ 6 Evaluation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a> showcases the performance of state-of-the-art methods on the 3DHP dataset.</p>
</div>
<figure class="ltx_table" id="S6.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparisons of 3D pose estimation methods on MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib228" title=""><span class="ltx_text" style="font-size:90%;">228</span></a>]</cite>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T5.3" style="width:346.9pt;height:125.1pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-152.3pt,54.7pt) scale(0.532481089222556,0.532481089222556) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T5.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T5.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.3.3.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.3.3.3.4.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.3.3.3.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.3.3.3.5.1">Year</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.3.3.3.6" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.3.3.3.6.1">Publication</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.3.3.3.7" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.3.3.3.7.1">Highlight</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.1">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T5.1.1.1.1.1.m1.1"><semantics id="S6.T5.1.1.1.1.1.m1.1a"><mo id="S6.T5.1.1.1.1.1.m1.1.1" stretchy="false" xref="S6.T5.1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.1.m1.1b"><ci id="S6.T5.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.1.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.2.2.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.2.2.2.2.1">PCK<math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T5.2.2.2.2.1.m1.1"><semantics id="S6.T5.2.2.2.2.1.m1.1a"><mo id="S6.T5.2.2.2.2.1.m1.1.1" stretchy="false" xref="S6.T5.2.2.2.2.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.2.2.1.m1.1b"><ci id="S6.T5.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.2.2.2.2.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.2.2.2.2.1.m1.1d">‚Üë</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.3.3.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.3.3.3.3.1">AUC<math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T5.3.3.3.3.1.m1.1"><semantics id="S6.T5.3.3.3.3.1.m1.1a"><mo id="S6.T5.3.3.3.3.1.m1.1.1" stretchy="false" xref="S6.T5.3.3.3.3.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S6.T5.3.3.3.3.1.m1.1b"><ci id="S6.T5.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.3.3.3.3.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.3.3.3.3.1.m1.1d">‚Üë</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.3.3.3.8" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.3.3.3.8.1">Code</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T5.3.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.3.4.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">HSTFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib95" title=""><span class="ltx_text" style="font-size:90%;">95</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.3.4.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.3.4.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">arXiv‚Äô23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.3.4.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">spatial-temporal transformers</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.3.4.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">28.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.3.4.1.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">98.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.3.4.1.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">78.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.3.4.1.8" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/qianxiaoye825/HSTFormer" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.5.2">
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.5.2.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.5.2.1.1" style="background-color:#D9D9D9;">PoseFormerV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.5.2.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.5.2.2.1" style="background-color:#D9D9D9;">2023</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.5.2.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.5.2.3.1" style="background-color:#D9D9D9;">CVPR‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.5.2.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.5.2.4.1" style="background-color:#D9D9D9;">frequency domain</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.5.2.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.5.2.5.1" style="background-color:#D9D9D9;">27.8</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.5.2.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.5.2.6.1" style="background-color:#D9D9D9;">97.9</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.5.2.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.5.2.7.1" style="background-color:#D9D9D9;">78.8</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.5.2.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/QitaoZhao/PoseFormerV2" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.6.3">
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.6.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Uplift and Upsample <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.6.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2023</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.6.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.6.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">efficient transformers</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.6.3.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">46.9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.6.3.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">95.4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.6.3.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">67.6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.6.3.8" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/goldbricklemon/uplift-upsample-3dhpe" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.7.4">
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.7.4.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.7.4.1.1" style="background-color:#D9D9D9;">RS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.7.4.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.7.4.2.1" style="background-color:#D9D9D9;">2023</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.7.4.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.7.4.3.1" style="background-color:#D9D9D9;">TIP‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.7.4.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.7.4.4.1" style="background-color:#D9D9D9;">regular splitting graph network</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.7.4.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.7.4.5.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.7.4.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.7.4.6.1" style="background-color:#D9D9D9;">85.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.7.4.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.7.4.7.1" style="background-color:#D9D9D9;">53.2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.7.4.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/nies14/RS-Net" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.8.5">
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.8.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Diffpose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib242" title=""><span class="ltx_text" style="font-size:90%;">242</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.8.5.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2023</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.8.5.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.8.5.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">pose refine</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.8.5.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">29.1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.8.5.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">98.0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.8.5.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">75.9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.8.5.8" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://gongjia0208.github.io/Diffpose/" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.9.6">
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.9.6.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.9.6.1.1" style="background-color:#D9D9D9;">FTCM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib122" title=""><span class="ltx_text" style="font-size:90%;">122</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.9.6.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.9.6.2.1" style="background-color:#D9D9D9;">2023</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.9.6.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.9.6.3.1" style="background-color:#D9D9D9;">TCSVT‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.9.6.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.9.6.4.1" style="background-color:#D9D9D9;">frequency-temporal collaborative</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.9.6.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.9.6.5.1" style="background-color:#D9D9D9;">31.2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.9.6.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.9.6.6.1" style="background-color:#D9D9D9;">97.9</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.9.6.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.9.6.7.1" style="background-color:#D9D9D9;">79.8</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.9.6.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/zhenhuat/FTCM" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.10.7">
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.10.7.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">STCFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib96" title=""><span class="ltx_text" style="font-size:90%;">96</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.10.7.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2023</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.10.7.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.10.7.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">spatio-temporal transformer</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.10.7.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">23.1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.10.7.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">98.7</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.10.7.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">83.9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.10.7.8" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/zhenhuat/STCFormer" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.11.8">
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.11.8.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.11.8.1.1" style="background-color:#D9D9D9;">PoseDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.11.8.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.11.8.2.1" style="background-color:#D9D9D9;">2023</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.11.8.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.11.8.3.1" style="background-color:#D9D9D9;">arXiv‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.11.8.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.11.8.4.1" style="background-color:#D9D9D9;">domain adaptation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.11.8.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.11.8.5.1" style="background-color:#D9D9D9;">61.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.11.8.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.11.8.6.1" style="background-color:#D9D9D9;">92.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.11.8.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.11.8.7.1" style="background-color:#D9D9D9;">62.5</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.11.8.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/rese1f/PoseDA" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.12.9">
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.12.9.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">TP-LSTMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib240" title=""><span class="ltx_text" style="font-size:90%;">240</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.12.9.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2022</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.12.9.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">TPAMI‚Äô22</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.12.9.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">pose similarity metric</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.12.9.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.12.9.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">82.6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.12.9.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">81.3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.12.9.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.13.10">
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.13.10.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.13.10.1.1" style="background-color:#D9D9D9;">AdaptPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib88" title=""><span class="ltx_text" style="font-size:90%;">88</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.13.10.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.13.10.2.1" style="background-color:#D9D9D9;">2022</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.13.10.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.13.10.3.1" style="background-color:#D9D9D9;">CVPR‚Äô22</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.13.10.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.13.10.4.1" style="background-color:#D9D9D9;">cross datasets</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.13.10.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.13.10.5.1" style="background-color:#D9D9D9;">77.2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.13.10.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.13.10.6.1" style="background-color:#D9D9D9;">88.4</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.13.10.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.13.10.7.1" style="background-color:#D9D9D9;">54.2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.13.10.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/mgholamikn/AdaptPose" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.14.11">
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.14.11.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">3D-HPE-PAA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib101" title=""><span class="ltx_text" style="font-size:90%;">101</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.14.11.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">2022</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.14.11.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">TIP‚Äô22</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.14.11.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">part aware attention</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.14.11.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">69.4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.14.11.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">90.3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.14.11.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">57.8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.3.14.11.8" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/thuxyz19/3D-HPE-PAA" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.3.15.12">
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.3.3.15.12.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.15.12.1.1" style="background-color:#D9D9D9;">Elepose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.3.3.15.12.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.15.12.2.1" style="background-color:#D9D9D9;">2022</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.3.3.15.12.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.15.12.3.1" style="background-color:#D9D9D9;">CVPR‚Äô22</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.3.3.15.12.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.15.12.4.1" style="background-color:#D9D9D9;">unsupervised</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.3.3.15.12.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.15.12.5.1" style="background-color:#D9D9D9;">54.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.3.3.15.12.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.15.12.6.1" style="background-color:#D9D9D9;">86.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.3.3.15.12.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T5.3.3.15.12.7.1" style="background-color:#D9D9D9;">50.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.3.3.15.12.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/bastianwandt/ElePose" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p4.1.1">3DPW dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib227" title=""><span class="ltx_text" style="font-size:90%;">227</span></a>]</cite> captures 51,000 sequences of single-view video, complemented by IMUs data. These videos were recorded using a handheld camera, with the IMU data facilitating the association of 2D poses with their 3D counterparts. 3DPW stands out as one of the most formidable datasets, establishing itself as a benchmark for 3D pose estimation in multi-person, in-wild scenarios of recent times. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#S6.T6" title="Table 6 ‚Ä£ 6.2 Datasets ‚Ä£ 6 Evaluation ‚Ä£ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"><span class="ltx_text ltx_ref_tag">6</span></a> shows the performance of state-of-the-art human mesh recovery methods on the Human3.6M and 3DPW datasets.</p>
</div>
<figure class="ltx_table" id="S6.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparisons of human mesh recovery methods on Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib225" title=""><span class="ltx_text" style="font-size:90%;">225</span></a>]</cite> and 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib227" title=""><span class="ltx_text" style="font-size:90%;">227</span></a>]</cite>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T6.5" style="width:433.6pt;height:285.3pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-194.3pt,127.6pt) scale(0.527374686686095,0.527374686686095) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T6.5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T6.5.5.6.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T6.5.5.6.1.1" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.5.5.6.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T6.5.5.6.1.2" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.5.5.6.1.2.1">Publication</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T6.5.5.6.1.3" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.5.5.6.1.3.1">Highlight</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S6.T6.5.5.6.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.5.5.6.1.4.1">Human3.6M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S6.T6.5.5.6.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.5.5.6.1.5.1">3DPW</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T6.5.5.6.1.6" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.5.5.6.1.6.1">Code</span></th>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T6.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.1.1.1">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T6.1.1.1.1.1.m1.1"><semantics id="S6.T6.1.1.1.1.1.m1.1a"><mo id="S6.T6.1.1.1.1.1.m1.1.1" stretchy="false" xref="S6.T6.1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S6.T6.1.1.1.1.1.m1.1b"><ci id="S6.T6.1.1.1.1.1.m1.1.1.cmml" xref="S6.T6.1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T6.1.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T6.2.2.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.2.2.2.2.1">PA-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T6.2.2.2.2.1.m1.1"><semantics id="S6.T6.2.2.2.2.1.m1.1a"><mo id="S6.T6.2.2.2.2.1.m1.1.1" stretchy="false" xref="S6.T6.2.2.2.2.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S6.T6.2.2.2.2.1.m1.1b"><ci id="S6.T6.2.2.2.2.1.m1.1.1.cmml" xref="S6.T6.2.2.2.2.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T6.2.2.2.2.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T6.3.3.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.3.3.3.3.1">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T6.3.3.3.3.1.m1.1"><semantics id="S6.T6.3.3.3.3.1.m1.1a"><mo id="S6.T6.3.3.3.3.1.m1.1.1" stretchy="false" xref="S6.T6.3.3.3.3.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S6.T6.3.3.3.3.1.m1.1b"><ci id="S6.T6.3.3.3.3.1.m1.1.1.cmml" xref="S6.T6.3.3.3.3.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.3.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T6.3.3.3.3.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T6.4.4.4.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.4.4.4.4.1">PA-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T6.4.4.4.4.1.m1.1"><semantics id="S6.T6.4.4.4.4.1.m1.1a"><mo id="S6.T6.4.4.4.4.1.m1.1.1" stretchy="false" xref="S6.T6.4.4.4.4.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S6.T6.4.4.4.4.1.m1.1b"><ci id="S6.T6.4.4.4.4.1.m1.1.1.cmml" xref="S6.T6.4.4.4.4.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.4.4.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T6.4.4.4.4.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T6.5.5.5.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.5.5.5.5.1">PVE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T6.5.5.5.5.1.m1.1"><semantics id="S6.T6.5.5.5.5.1.m1.1a"><mo id="S6.T6.5.5.5.5.1.m1.1.1" stretchy="false" xref="S6.T6.5.5.5.5.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S6.T6.5.5.5.5.1.m1.1b"><ci id="S6.T6.5.5.5.5.1.m1.1.1.cmml" xref="S6.T6.5.5.5.5.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.5.5.5.5.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T6.5.5.5.5.1.m1.1d">‚Üì</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T6.5.5.7.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.5.5.7.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">VirtualMarker <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib244" title=""><span class="ltx_text" style="font-size:90%;">244</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.5.5.7.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.5.5.7.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">novel intermediate representation</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.5.5.7.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">47.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.5.5.7.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">32.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.5.5.7.1.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">67.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.5.5.7.1.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">41.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.5.5.7.1.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">77.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.5.5.7.1.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/ShirleyMaxx/VirtualMarker" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.8.2">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.8.2.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.8.2.1.1" style="background-color:#D9D9D9;">NIKI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib160" title=""><span class="ltx_text" style="font-size:90%;">160</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.8.2.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.8.2.2.1" style="background-color:#D9D9D9;">CVPR‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.8.2.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.8.2.3.1" style="background-color:#D9D9D9;">inverse kinematics</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.8.2.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.8.2.4.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.8.2.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.8.2.5.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.8.2.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.8.2.6.1" style="background-color:#D9D9D9;">71.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.8.2.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.8.2.7.1" style="background-color:#D9D9D9;">40.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.8.2.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.8.2.8.1" style="background-color:#D9D9D9;">86.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.8.2.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/Jeff-sjtu/NIKI" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.9.3">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.9.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">TORE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib149" title=""><span class="ltx_text" style="font-size:90%;">149</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.9.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">ICCV‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.9.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">efficient transformer</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.9.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">59.6</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.9.3.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.9.3.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">72.3</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.9.3.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">44.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.9.3.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">88.2</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.9.3.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://frank-zy-dou.github.io/projects/Tore/index.html" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.10.4">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.10.4.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.10.4.1.1" style="background-color:#D9D9D9;">JOTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib170" title=""><span class="ltx_text" style="font-size:90%;">170</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.10.4.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.10.4.2.1" style="background-color:#D9D9D9;">ICCV‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.10.4.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.10.4.3.1" style="background-color:#D9D9D9;">contrastive learning</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.10.4.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.10.4.4.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.10.4.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.10.4.5.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.10.4.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.10.4.6.1" style="background-color:#D9D9D9;">76.4</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.10.4.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.10.4.7.1" style="background-color:#D9D9D9;">48.7</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.10.4.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.10.4.8.1" style="background-color:#D9D9D9;">92.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.10.4.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/xljh0520/JOTR" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.11.5">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.11.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">HMDiff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib202" title=""><span class="ltx_text" style="font-size:90%;">202</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.11.5.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">ICCV‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.11.5.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">reverse diffusion processing</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.11.5.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.3</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.11.5.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">32.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.11.5.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">72.7</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.11.5.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">44.5</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.11.5.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">82.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.11.5.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://gongjia0208.github.io/HMDiff/" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.12.6">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.12.6.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.12.6.1.1" style="background-color:#D9D9D9;">ReFit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib165" title=""><span class="ltx_text" style="font-size:90%;">165</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.12.6.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.12.6.2.1" style="background-color:#D9D9D9;">ICCV‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.12.6.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.12.6.3.1" style="background-color:#D9D9D9;">recurrent fitting network</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.12.6.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.12.6.4.1" style="background-color:#D9D9D9;">48.4</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.12.6.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.12.6.5.1" style="background-color:#D9D9D9;">32.2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.12.6.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.12.6.6.1" style="background-color:#D9D9D9;">65.8</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.12.6.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.12.6.7.1" style="background-color:#D9D9D9;">41.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.12.6.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.12.6.8.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.12.6.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/yufu-wang/ReFit" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.13.7">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.13.7.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">PyMAF-X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib184" title=""><span class="ltx_text" style="font-size:90%;">184</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.13.7.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">TPAMI‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.13.7.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">regression-based one-stage whole body</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.13.7.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.13.7.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.13.7.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">74.2</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.13.7.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">45.3</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.13.7.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">87.0</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.13.7.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://www.liuyebin.com/pymaf-x/" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.14.8">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.14.8.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.14.8.1.1" style="background-color:#D9D9D9;">PointHMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib245" title=""><span class="ltx_text" style="font-size:90%;">245</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.14.8.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.14.8.2.1" style="background-color:#D9D9D9;">CVPR‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.14.8.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.14.8.3.1" style="background-color:#D9D9D9;">vertex-relevant feature extraction</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.14.8.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.14.8.4.1" style="background-color:#D9D9D9;">48.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.14.8.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.14.8.5.1" style="background-color:#D9D9D9;">32.9</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.14.8.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.14.8.6.1" style="background-color:#D9D9D9;">73.9</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.14.8.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.14.8.7.1" style="background-color:#D9D9D9;">44.9</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.14.8.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.14.8.8.1" style="background-color:#D9D9D9;">85.5</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.14.8.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.14.8.9.1" style="background-color:#D9D9D9;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.15.9">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.15.9.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">PLIKS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib246" title=""><span class="ltx_text" style="font-size:90%;">246</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.15.9.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.15.9.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">inverse kinematics</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.15.9.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">47.0</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.15.9.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">34.5</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.15.9.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">60.5</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.15.9.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.5</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.15.9.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">73.3</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.15.9.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/karShetty/PLIKS" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.16.10">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.16.10.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.16.10.1.1" style="background-color:#D9D9D9;">ProPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib247" title=""><span class="ltx_text" style="font-size:90%;">247</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.16.10.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.16.10.2.1" style="background-color:#D9D9D9;">CVPR‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.16.10.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.16.10.3.1" style="background-color:#D9D9D9;">learning analytical posterior probability</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.16.10.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.16.10.4.1" style="background-color:#D9D9D9;">45.7</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.16.10.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.16.10.5.1" style="background-color:#D9D9D9;">29.1</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.16.10.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.16.10.6.1" style="background-color:#D9D9D9;">68.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.16.10.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.16.10.7.1" style="background-color:#D9D9D9;">40.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.16.10.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.16.10.8.1" style="background-color:#D9D9D9;">79.4</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.16.10.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/NetEase-GameAI/ProPose" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.17.11">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.17.11.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">POTTER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib248" title=""><span class="ltx_text" style="font-size:90%;">248</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.17.11.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.17.11.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">pooling attention transformer</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.17.11.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.5</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.17.11.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">35.1</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.17.11.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">75.0</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.17.11.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">44.8</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.17.11.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">87.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.17.11.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/zczcwh/POTTER" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.18.12">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.18.12.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.18.12.1.1" style="background-color:#D9D9D9;">PoseExaminer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib249" title=""><span class="ltx_text" style="font-size:90%;">249</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.18.12.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.18.12.2.1" style="background-color:#D9D9D9;">ICCV‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.18.12.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.18.12.3.1" style="background-color:#D9D9D9;">automated testing of out-of-distribution</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.18.12.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.18.12.4.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.18.12.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.18.12.5.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.18.12.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.18.12.6.1" style="background-color:#D9D9D9;">74.5</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.18.12.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.18.12.7.1" style="background-color:#D9D9D9;">46.5</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.18.12.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.18.12.8.1" style="background-color:#D9D9D9;">88.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.18.12.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/qihao067/PoseExaminer" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.19.13">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.19.13.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">MotionBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib156" title=""><span class="ltx_text" style="font-size:90%;">156</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.19.13.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">ICCV‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.19.13.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">pretrained human representations</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.19.13.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">43.1</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.19.13.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">27.8</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.19.13.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.8</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.19.13.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">40.6</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.19.13.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">79.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.19.13.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://motionbert.github.io/" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.20.14">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.20.14.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.20.14.1.1" style="background-color:#D9D9D9;">3DNBF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib200" title=""><span class="ltx_text" style="font-size:90%;">200</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.20.14.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.20.14.2.1" style="background-color:#D9D9D9;">ICCV‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.20.14.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.20.14.3.1" style="background-color:#D9D9D9;">analysis-by-synthesis approach</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.20.14.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.20.14.4.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.20.14.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.20.14.5.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.20.14.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.20.14.6.1" style="background-color:#D9D9D9;">88.8</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.20.14.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.20.14.7.1" style="background-color:#D9D9D9;">53.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.20.14.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.20.14.8.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.20.14.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/edz-o/3DNBF" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.21.15">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.21.15.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">FastMETRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib134" title=""><span class="ltx_text" style="font-size:90%;">134</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.21.15.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">ECCV‚Äô22</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.21.15.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">efficient architecture</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.21.15.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">52.2</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.21.15.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">33.7</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.21.15.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">73.5</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.21.15.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">44.6</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.21.15.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">84.1</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.21.15.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/postech-ami/FastMETRO" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.22.16">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.22.16.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.22.16.1.1" style="background-color:#D9D9D9;">CLIFF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib129" title=""><span class="ltx_text" style="font-size:90%;">129</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.22.16.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.22.16.2.1" style="background-color:#D9D9D9;">ECCV‚Äô22</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.22.16.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.22.16.3.1" style="background-color:#D9D9D9;">multi-modality inputs</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.22.16.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.22.16.4.1" style="background-color:#D9D9D9;">47.1</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.22.16.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.22.16.5.1" style="background-color:#D9D9D9;">32.7</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.22.16.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.22.16.6.1" style="background-color:#D9D9D9;">69.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.22.16.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.22.16.7.1" style="background-color:#D9D9D9;">43.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.22.16.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.22.16.8.1" style="background-color:#D9D9D9;">81.2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.22.16.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/huawei-noah/noah-research/tree/master/CLIFF" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.23.17">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.23.17.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">PARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib130" title=""><span class="ltx_text" style="font-size:90%;">130</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.23.17.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">ICCV‚Äô21</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.23.17.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">part-driven attention</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.23.17.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.23.17.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.23.17.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">74.5</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.23.17.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">46.5</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.23.17.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">88.6</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.23.17.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://pare.is.tue.mpg.de/" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.24.18">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.24.18.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.24.18.1.1" style="background-color:#D9D9D9;">Graphormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib131" title=""><span class="ltx_text" style="font-size:90%;">131</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.24.18.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.24.18.2.1" style="background-color:#D9D9D9;">ICCV‚Äô21</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.24.18.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.24.18.3.1" style="background-color:#D9D9D9;">GCNN-reinforced transformer</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.24.18.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.24.18.4.1" style="background-color:#D9D9D9;">51.2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.24.18.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.24.18.5.1" style="background-color:#D9D9D9;">34.5</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.24.18.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.24.18.6.1" style="background-color:#D9D9D9;">74.7</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.24.18.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.24.18.7.1" style="background-color:#D9D9D9;">45.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.24.18.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.24.18.8.1" style="background-color:#D9D9D9;">87.7</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.24.18.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/microsoft/MeshGraphormer" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.25.19">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.25.19.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">PSVT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib133" title=""><span class="ltx_text" style="font-size:90%;">133</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.25.19.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.25.19.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">spatio-temporal encoder</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.25.19.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.25.19.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.25.19.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">73.1</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.25.19.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">43.5</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.25.19.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">84.0</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.25.19.9" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.26.20">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.26.20.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.26.20.1.1" style="background-color:#D9D9D9;">GLoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib142" title=""><span class="ltx_text" style="font-size:90%;">142</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.26.20.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.26.20.2.1" style="background-color:#D9D9D9;">CVPR‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.26.20.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.26.20.3.1" style="background-color:#D9D9D9;">short-term and long-term temporal correlations</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.26.20.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.26.20.4.1" style="background-color:#D9D9D9;">67.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.26.20.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.26.20.5.1" style="background-color:#D9D9D9;">46.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.26.20.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.26.20.6.1" style="background-color:#D9D9D9;">80.7</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.26.20.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.26.20.7.1" style="background-color:#D9D9D9;">50.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.26.20.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.26.20.8.1" style="background-color:#D9D9D9;">96.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.26.20.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/sxl142/GLoT" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.27.21">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.27.21.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">MPS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib132" title=""><span class="ltx_text" style="font-size:90%;">132</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.27.21.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.27.21.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">temporally adjacent representations</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.27.21.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">69.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.27.21.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">47.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.27.21.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">91.6</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.27.21.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">54.0</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.27.21.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">109.6</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.27.21.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://mps-net.github.io/MPS-Net/" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.28.22">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.28.22.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.28.22.1.1" style="background-color:#D9D9D9;">MAED <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib140" title=""><span class="ltx_text" style="font-size:90%;">140</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.28.22.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.28.22.2.1" style="background-color:#D9D9D9;">ICCV‚Äô21</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.28.22.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.28.22.3.1" style="background-color:#D9D9D9;">multi-level attention</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.28.22.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.28.22.4.1" style="background-color:#D9D9D9;">56.4</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.28.22.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.28.22.5.1" style="background-color:#D9D9D9;">38.7</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.28.22.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.28.22.6.1" style="background-color:#D9D9D9;">79.1</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.28.22.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.28.22.7.1" style="background-color:#D9D9D9;">45.7</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.28.22.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.28.22.8.1" style="background-color:#D9D9D9;">92.6</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.28.22.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/ziniuwan/maed" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.29.23">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.29.23.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib161" title=""><span class="ltx_text" style="font-size:90%;">161</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.29.23.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">ICCV‚Äô21</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.29.23.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">uncertainty-aware</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.29.23.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">58.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.29.23.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.29.23.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">92.8</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.29.23.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">52.2</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.29.23.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">106.1</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.29.23.9" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.30.24">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.30.24.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.30.24.1.1" style="background-color:#D9D9D9;">TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib139" title=""><span class="ltx_text" style="font-size:90%;">139</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.30.24.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.30.24.2.1" style="background-color:#D9D9D9;">CVPR‚Äô21</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.30.24.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.30.24.3.1" style="background-color:#D9D9D9;">temporal consistency</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.30.24.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.30.24.4.1" style="background-color:#D9D9D9;">62.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.30.24.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.30.24.5.1" style="background-color:#D9D9D9;">41.1</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.30.24.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.30.24.6.1" style="background-color:#D9D9D9;">95.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.30.24.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.30.24.7.1" style="background-color:#D9D9D9;">55.8</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.30.24.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.30.24.8.1" style="background-color:#D9D9D9;">111.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.30.24.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.30.24.9.1" style="background-color:#D9D9D9;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.31.25">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.31.25.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">VIBE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib138" title=""><span class="ltx_text" style="font-size:90%;">138</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.31.25.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">CVPR‚Äô20</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.31.25.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">self-attention temporal network</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.31.25.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.6</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.31.25.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">41.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.31.25.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">82.9</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.31.25.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">51.9</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.31.25.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">99.1</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.31.25.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/mkocabas/VIBE" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.32.26">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.32.26.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.32.26.1.1" style="background-color:#D9D9D9;">ImpHMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib250" title=""><span class="ltx_text" style="font-size:90%;">250</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.32.26.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.32.26.2.1" style="background-color:#D9D9D9;">CVPR‚Äô23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.32.26.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.32.26.3.1" style="background-color:#D9D9D9;">implicitly imagine person in 3D space</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.32.26.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.32.26.4.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.32.26.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.32.26.5.1" style="background-color:#D9D9D9;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.32.26.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.32.26.6.1" style="background-color:#D9D9D9;">74.3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.32.26.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.32.26.7.1" style="background-color:#D9D9D9;">45.4</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.32.26.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.32.26.8.1" style="background-color:#D9D9D9;">87.1</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.32.26.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.32.26.9.1" style="background-color:#D9D9D9;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.33.27">
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.33.27.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">SGRE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib163" title=""><span class="ltx_text" style="font-size:90%;">163</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.33.27.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">ICCV‚Äô23</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.33.27.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">sequentially global rotation estimation</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.33.27.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.33.27.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.33.27.6" style="padding-top:1.5pt;padding-bottom:1.5pt;">78.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.33.27.7" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.6</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.33.27.8" style="padding-top:1.5pt;padding-bottom:1.5pt;">93.3</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.33.27.9" style="padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/kennethwdk/SGRE" title="">Code</a></td>
</tr>
<tr class="ltx_tr" id="S6.T6.5.5.34.28">
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.5.5.34.28.1" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.34.28.1.1" style="background-color:#D9D9D9;">PMCE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.5.5.34.28.2" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.34.28.2.1" style="background-color:#D9D9D9;">ICCV‚Äô23</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.5.5.34.28.3" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.34.28.3.1" style="background-color:#D9D9D9;">pose and mesh co-evolution network</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.5.5.34.28.4" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.34.28.4.1" style="background-color:#D9D9D9;">53.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.5.5.34.28.5" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.34.28.5.1" style="background-color:#D9D9D9;">37.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.5.5.34.28.6" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.34.28.6.1" style="background-color:#D9D9D9;">69.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.5.5.34.28.7" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.34.28.7.1" style="background-color:#D9D9D9;">46.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.5.5.34.28.8" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S6.T6.5.5.34.28.8.1" style="background-color:#D9D9D9;">84.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.5.5.34.28.9" style="background-color:#D9D9D9;padding-top:1.5pt;padding-bottom:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/kasvii/PMCE" style="background-color:#D9D9D9;" title="">Code</a></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S6.SS2.p5">
<p class="ltx_p" id="S6.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p5.1.1">HumanEva dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib229" title=""><span class="ltx_text" style="font-size:90%;">229</span></a>]</cite> is a multi-view 3D human pose estimation dataset comprising two versions: HumanEva-I and HumanEva-II. In HumanEva-I, the dataset includes around 40,000 multi-view video frames captured from seven cameras positioned at the front, left, and right (RGB) and four corners (Mono). Additionally, HumanEva-II features approximately 2,460 frames, recorded with four cameras at each corner.</p>
</div>
<div class="ltx_para" id="S6.SS2.p6">
<p class="ltx_p" id="S6.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p6.1.1">CMU-Panoptic dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib230" title=""><span class="ltx_text" style="font-size:90%;">230</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib251" title=""><span class="ltx_text" style="font-size:90%;">251</span></a>]</cite> includes 65 frame sequences, approximately 5.5 hours of footage, and features 1.5 million 3D annotated poses. Recorded via a massively multi-view system equipped with 511 calibrated cameras and 10 RGB-D sensors featuring hardware-based synchronization, this dataset is crucial for developing weakly supervised methods through multi-view geometry. These methods address the occlusion problems commonly encountered in traditional computer vision techniques.</p>
</div>
<div class="ltx_para" id="S6.SS2.p7">
<p class="ltx_p" id="S6.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p7.1.1">Multiperson Composited 3D Human Pose (MuCo-3DHP) dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib118" title=""><span class="ltx_text" style="font-size:90%;">118</span></a>]</cite> serves as a large-scale, multi-person occluded training set for 3D human pose estimation. Frames in the MuCo-3DHP are generated from the MPI-INF-3DHP dataset through a compositing and augmentation scheme.</p>
</div>
<div class="ltx_para" id="S6.SS2.p8">
<p class="ltx_p" id="S6.SS2.p8.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p8.1.1">SURREAL dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib231" title=""><span class="ltx_text" style="font-size:90%;">231</span></a>]</cite> is a large synthetic human body dataset containing 6 million RGB video frames. It provides a range of accurate annotations, including depth, body parts, optical flow, 2D/3D poses, and surfaces. In the SURREAL dataset, images exhibit variations in texture, view, and pose, and the body models are based on the SMPL parameters, a widely-recognized mesh representation standard.</p>
</div>
<div class="ltx_para" id="S6.SS2.p9">
<p class="ltx_p" id="S6.SS2.p9.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p9.1.1">3DOH50K dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib153" title=""><span class="ltx_text" style="font-size:90%;">153</span></a>]</cite> offers a collection of 51,600 images obtained from six distinct viewpoints in real-world settings, predominantly featuring object occlusions. Each image is annotated with ground truth 2D and 3D poses, SMPL parameters, and a segmentation mask. Utilized for training human estimation and reconstruction models, the 3DOH50K dataset facilitates exceptional performance in occlusion scenarios.</p>
</div>
<div class="ltx_para" id="S6.SS2.p10">
<p class="ltx_p" id="S6.SS2.p10.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p10.1.1">3DCP dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib232" title=""><span class="ltx_text" style="font-size:90%;">232</span></a>]</cite> represents a 3D human mesh dataset, derived from AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib233" title=""><span class="ltx_text" style="font-size:90%;">233</span></a>]</cite>. It includes 190 self-contact meshes spanning six human subjects (three males and three females), each modeled with an SMPL-X parameterized template.</p>
</div>
<div class="ltx_para" id="S6.SS2.p11">
<p class="ltx_p" id="S6.SS2.p11.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p11.1.1">AMASS dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib233" title=""><span class="ltx_text" style="font-size:90%;">233</span></a>]</cite> constitutes a comprehensive and diverse human motion dataset, encompassing over 11,000 motions from 300 subjects, totaling more than 40 hours. The motion data, accompanied by SMPL parameters for skeleton and mesh representation, is derived from a marker-based MoCap system utilizing 15 optical markers.</p>
</div>
<div class="ltx_para" id="S6.SS2.p12">
<p class="ltx_p" id="S6.SS2.p12.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p12.1.1">DensePose dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib234" title=""><span class="ltx_text" style="font-size:90%;">234</span></a>]</cite> features 50,000 manually annotated real images, comprising 5 million image-to-surface correspondence pairs extracted from the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib252" title=""><span class="ltx_text" style="font-size:90%;">252</span></a>]</cite> dataset. This dataset proves instrumental for training in dense human pose estimation, as well as in detection and segmentation tasks.</p>
</div>
<div class="ltx_para" id="S6.SS2.p13">
<p class="ltx_p" id="S6.SS2.p13.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p13.1.1">UP-3D dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib235" title=""><span class="ltx_text" style="font-size:90%;">235</span></a>]</cite> is a dedicated 3D human pose and shape estimation dataset featuring extensive annotations in sports scenarios. The UP-3D comprises approximately 8,000 images from the LSP and MPII datasets. Additionally, each image in UP-3D is accompanied by a metadata file indicating the quality (medium or high) of the 3D fit.</p>
</div>
<div class="ltx_para" id="S6.SS2.p14">
<p class="ltx_p" id="S6.SS2.p14.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p14.1.1">THuman dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib236" title=""><span class="ltx_text" style="font-size:90%;">236</span></a>]</cite> constitutes a 3D real-world human mesh dataset. It includes 7,000 RGBD images, each featuring a textured surface mesh obtained using a Kinect camera. Including surface mesh with detailed texture and the aligned SMPL model is anticipated to significantly enhance and stimulate future research in human mesh reconstruction.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Applications</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this section, we review related works about human pose estimation and mesh recovery for a few popular applications.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1">Motion Retargeting.</span>
Human motion retargeting can transfer human actions onto actors. Using pose estimation eliminates the need for motion capture systems and achieves image-to-image translation. Therefore, 3D human pose estimation is crucial for retargeting. Recently, there has been a significant amount of retargeting work based on 3D human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib253" title=""><span class="ltx_text" style="font-size:90%;">253</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib254" title=""><span class="ltx_text" style="font-size:90%;">254</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib255" title=""><span class="ltx_text" style="font-size:90%;">255</span></a>]</cite>. End-to-end methods and related datasets are also designed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib256" title=""><span class="ltx_text" style="font-size:90%;">256</span></a>]</cite>. Additionally, unsupervised methods in In-the-Wild scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib257" title=""><span class="ltx_text" style="font-size:90%;">257</span></a>]</cite> have been developed, achieved through canonicalization operations and derived regularizations. Beyond bodily retargeting, facial retargeting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib258" title=""><span class="ltx_text" style="font-size:90%;">258</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib259" title=""><span class="ltx_text" style="font-size:90%;">259</span></a>]</cite> has also gained prominence, wherein the intricacies of facial expressions offer a refined portrayal of the actor‚Äôs emotional and psychological states.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><span class="ltx_text ltx_font_bold" id="S7.p3.1.1">Human Avatars.</span>
Human avatars are human-like virtual entities created through digital technology, which can interact and express themselves on various digital media platforms. Human pose estimation and mesh recovery enable digital avatars to simulate human movements and behaviors more realistically and vividly, allowing developers to create virtual characters with behaviors highly similar to real individuals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib209" title=""><span class="ltx_text" style="font-size:90%;">209</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib260" title=""><span class="ltx_text" style="font-size:90%;">260</span></a>]</cite>. For instance, Luo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib261" title=""><span class="ltx_text" style="font-size:90%;">261</span></a>]</cite> proposed a real-time multi-person avatar controller using noisy poses from video-based pose estimators and language-based motion generators.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1"><span class="ltx_text ltx_font_bold" id="S7.p4.1.1">Action Recognition.</span>
Action recognition employs algorithms to identify and analyze human movements from images or videos. The results derived from 3D human pose estimation and reconstruction play a pivotal role in deciphering the dynamics of human motion within a three-dimensional context, thereby transforming these movements into actionable behavioral insights <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib262" title=""><span class="ltx_text" style="font-size:90%;">262</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib263" title=""><span class="ltx_text" style="font-size:90%;">263</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib264" title=""><span class="ltx_text" style="font-size:90%;">264</span></a>]</cite>. Moreover, these advancements can significantly augment the efficiency of action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib265" title=""><span class="ltx_text" style="font-size:90%;">265</span></a>]</cite>. It is also possible to address pose estimation and action recognition within the same framework through multi-task learning and sharing features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib266" title=""><span class="ltx_text" style="font-size:90%;">266</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1"><span class="ltx_text ltx_font_bold" id="S7.p5.1.1">Security Monitoring.</span>
In video surveillance systems at public places or critical facilities, pedestrian tracking and re-identification are key tasks. Combined with human pose estimation, Human tracking is utilized for the surveillance and analysis of pedestrian flow, tracking specific targets, and tracking and analyzing human behavior in space. This approach is highly beneficial for tracking humans in complex scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib267" title=""><span class="ltx_text" style="font-size:90%;">267</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib268" title=""><span class="ltx_text" style="font-size:90%;">268</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib269" title=""><span class="ltx_text" style="font-size:90%;">269</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib270" title=""><span class="ltx_text" style="font-size:90%;">270</span></a>]</cite>. Considering the constrained viewpoints of individual cameras in such systems, Re-identification enables the recognition and tracking of the same person as they move across different camera views. Human pose estimation significantly contributes to the enhancement of this re-identification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S7.p6">
<p class="ltx_p" id="S7.p6.1"><span class="ltx_text ltx_font_bold" id="S7.p6.1.1">SLAM.</span>
SLAM precisely estimates its location by gathering sensor data from its environment, employing technologies such as cameras and LiDAR. It simultaneously constructs or refines an environmental map, facilitating self-localization and map creation in unfamiliar territories. Diverging from the conventional SLAM systems that predominantly concentrate on objects, recent advancements have shifted focus towards incorporating humans within the environmental context. Notably, Dai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib271" title=""><span class="ltx_text" style="font-size:90%;">271</span></a>]</cite> have illustrated the significance of 3D human trajectory reconstruction in indoor settings, enhancing indoor navigation capabilities. Furthermore, Kocabas et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib272" title=""><span class="ltx_text" style="font-size:90%;">272</span></a>]</cite> have innovatively integrated human motion priors into SLAM, effectively merging human pose estimation with scene analysis.</p>
</div>
<div class="ltx_para" id="S7.p7">
<p class="ltx_p" id="S7.p7.1"><span class="ltx_text ltx_font_bold" id="S7.p7.1.1">Autonomous Driving.</span>
In robotic navigation and autonomous vehicle applications, estimating human pose enables these systems to comprehend human behavior and intentions better. This understanding facilitates more intelligent decision-making and interaction. Zheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> propose a multi-modal approach employing 2D labels on RGB images as weak supervision for 3D human pose estimation in the context of autonomous vehicles. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> have developed a comprehensive framework to learn physically plausible human dynamics from real driving scenarios, effectively bridging the gap between actual and simulated human behavior in safety-critical applications.</p>
</div>
<div class="ltx_para" id="S7.p8">
<p class="ltx_p" id="S7.p8.1"><span class="ltx_text ltx_font_bold" id="S7.p8.1.1">Human‚ÄìComputer Interaction.</span>
Human-computer interaction entails the bidirectional communication between humans and computers, underscored by the critical need for computers to interpret human poses accurately. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> advanced this field by proposing asymmetric relation-aware representation learning for head pose estimation in industrial human‚Äìcomputer interaction, which utilizes an effective Lorentz distribution learning scheme. In Augmented Reality (AR) systems, the accuracy of human pose estimation significantly elevates the interaction quality between virtual objects and the real environment, fostering more natural interactions between humans and virtual entities. In this context, Weng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> developed a novel method and application for animating a human subject from a single photo within AR.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Challenges and Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this survey, we have presented a contemporary overview of recent deep learning-based 3D human pose estimation and mesh recovery methods. A comprehensive taxonomy and performance comparison of these methods has been covered. We further point out a few promising research directions, hoping to promote advances in this field.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1"><span class="ltx_text ltx_font_bold" id="S8.p2.1.1">Speed.</span>
Speed is an essential aspect to consider in the practical deployment of algorithms. While most current research papers report achieving real-time performance on GPUs, a wide array of applications necessitates real-time and efficient processing on edge computing platforms, notably on ARM processors within smartphones. The disparity in performance between ARM processors and GPUs is pronounced, underscoring the immense value of optimizing for speed. Although some video-based visual processing tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib273" title=""><span class="ltx_text" style="font-size:90%;">273</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib274" title=""><span class="ltx_text" style="font-size:90%;">274</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib275" title=""><span class="ltx_text" style="font-size:90%;">275</span></a>]</cite> have already acknowledged the importance of processing speed for real-time application scenarios, further research is still needed in the context of 3D cases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1"><span class="ltx_text ltx_font_bold" id="S8.p3.1.1">Crowding and Occlusion Challenges.</span>
In open-world scenarios, the phenomena of crowding and occlusion are prevalent, and they represent long-standing challenges in the field of object detection. Currently, top-down methods depend on object detection, rendering these issues inescapable. Although bottom-up strategies may circumvent object detection, they confront formidable challenges regarding key point assembly. Therefore, some researchers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib121" title=""><span class="ltx_text" style="font-size:90%;">121</span></a>]</cite> attempt to integrate both methods together. Moreover, incorporating temporal consistency constraints (such as optical flow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib102" title=""><span class="ltx_text" style="font-size:90%;">102</span></a>]</cite>) during mining can also alleviate occlusion issues.</p>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.1"><span class="ltx_text ltx_font_bold" id="S8.p4.1.1">Large Models.</span>
The efficacy of large models in language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib276" title=""><span class="ltx_text" style="font-size:90%;">276</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib277" title=""><span class="ltx_text" style="font-size:90%;">277</span></a>]</cite> and foundational computer vision tasks, such as segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib278" title=""><span class="ltx_text" style="font-size:90%;">278</span></a>]</cite> and tracking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib279" title=""><span class="ltx_text" style="font-size:90%;">279</span></a>]</cite>, has been universally acknowledged and is quite remarkable. Additionally, while there is burgeoning research in human-centric computer vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib280" title=""><span class="ltx_text" style="font-size:90%;">280</span></a>]</cite>, the area of 3D tasks still demands further investigation. Furthermore, the development of large models not only constitutes a significant area of research but also the exploration of their effective utilization presents substantial challenges and practical importance. Exploratory efforts to fuse pose estimation with large language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib281" title=""><span class="ltx_text" style="font-size:90%;">281</span></a>]</cite> and the combination of large visual models with pose estimation are also meaningful.</p>
</div>
<div class="ltx_para" id="S8.p5">
<p class="ltx_p" id="S8.p5.1"><span class="ltx_text ltx_font_bold" id="S8.p5.1.1">More Detailed Reconstruction.</span>
At present, explicit model-based methodologies such as SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> and SMPL-X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite>, fall short of meeting the demands for detail that people expect. On the other hand, implicit representation approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib193" title=""><span class="ltx_text" style="font-size:90%;">193</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib195" title=""><span class="ltx_text" style="font-size:90%;">195</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib198" title=""><span class="ltx_text" style="font-size:90%;">198</span></a>]</cite>, as well as rendering techniques such as NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib201" title=""><span class="ltx_text" style="font-size:90%;">201</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> and Gaussian Splatting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib211" title=""><span class="ltx_text" style="font-size:90%;">211</span></a>]</cite>, are capable of capturing fine details but lack sufficient robustness in pose estimation. Bridging the gap between robust pose estimation and surface details remains a formidable challenge that necessitates collaborative efforts from computer vision and computer graphics researchers.</p>
</div>
<div class="ltx_para" id="S8.p6">
<p class="ltx_p" id="S8.p6.1"><span class="ltx_text ltx_font_bold" id="S8.p6.1.1">Reconstructing with the Environment.</span>
Humans engage with various objects and environments within the real world in the daily life. The ability to reconstruct these interactions in 3D is paramount for comprehending and simulating their dynamics. This aspect holds particular significance in the realm of artificial intelligence applied to human avatars. In the reconstruction of object interactions, challenges such as pose initialization and sparse observation of objects in Structure from Motion are encountered <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. Moreover, Yi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib282" title=""><span class="ltx_text" style="font-size:90%;">282</span></a>]</cite> indicate a reciprocal enhancement between environmental and human reconstruction, resulting in enhanced effectiveness across both domains.</p>
</div>
<div class="ltx_para" id="S8.p7">
<p class="ltx_p" id="S8.p7.1"><span class="ltx_text ltx_font_bold" id="S8.p7.1.1">Controllability and Animatability.</span>
Controllability refers to the precise control over the postures, movements, and expressions of human avatars, enabling them to exhibit various desired behaviors in virtual environments. Animation, on the other hand, denotes the ability of avatars to vividly and smoothly demonstrate various movements and expressions, making them appear more realistic and natural. Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.18844v2#bib.bib283" title=""><span class="ltx_text" style="font-size:90%;">283</span></a>]</cite> employed an explicit deformation field to deform the neural implicit field, thus enabling the animation of human characters. They mapped the target human body mesh to the template human body mesh, represented by a parameterized human body model, thereby simplifying the animation and reshaping of the generated avatars through the control of pose and shape parameters. By enhancing the controllability and animatability of human body mesh reconstruction, developers can create digital characters that are more realistic, vivid, and expressive. This, in turn, enhances their interactivity and appeal in the virtual world.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.2.2.1" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.4.1" style="font-size:90%;">
H.¬†Duan, Y.¬†Zhao, K.¬†Chen, D.¬†Lin, B.¬†Dai, Revisiting skeleton-based action
recognition, in: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2022, pp. 2969‚Äì2978.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.2.2.1" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.4.1" style="font-size:90%;">
Y.¬†Zhang, C.¬†Wang, X.¬†Wang, W.¬†Liu, W.¬†Zeng, Voxeltrack: Multi-person 3d human
pose estimation and tracking in the wild, IEEE Transactions on Pattern
Analysis and Machine Intelligence 45¬†(2) (2022) 2613‚Äì2626.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.2.2.1" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.4.1" style="font-size:90%;">
Y.¬†Zhu, H.¬†Shuai, G.¬†Liu, Q.¬†Liu, Multilevel spatial‚Äìtemporal excited graph
network for skeleton-based action recognition, IEEE Transactions on Image
Processing 32 (2022) 496‚Äì508.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.2.2.1" style="font-size:90%;">[4]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.4.1" style="font-size:90%;">
J.¬†Yang, Z.¬†Zhang, S.¬†Xiao, S.¬†Ma, Y.¬†Li, W.¬†Lu, X.¬†Gao, Efficient data-driven
behavior identification based on vision transformers for human activity
understanding, Neurocomputing 530 (2023) 104‚Äì115.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.2.2.1" style="font-size:90%;">[5]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.4.1" style="font-size:90%;">
Y.¬†You, H.¬†Liu, T.¬†Wang, W.¬†Li, R.¬†Ding, X.¬†Li, Co-evolution of pose and mesh
for 3d human body estimation from video, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2023, pp. 14963‚Äì14973.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.2.2.1" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.4.1" style="font-size:90%;">
S.¬†Tripathi, L.¬†M√ºller, C.-H.¬†P. Huang, O.¬†Taheri, M.¬†J. Black, D.¬†Tzionas,
3d human pose estimation via intuitive physics, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
4713‚Äì4725.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.2.2.1" style="font-size:90%;">[7]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.4.1" style="font-size:90%;">
Z.¬†Fan, M.¬†Parelli, M.¬†E. Kadoglou, M.¬†Kocabas, X.¬†Chen, M.¬†J. Black,
O.¬†Hilliges, Hold: Category-agnostic 3d reconstruction of interacting hands
and objects from video, arXiv preprint arXiv:2311.18448 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.2.2.1" style="font-size:90%;">[8]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.4.1" style="font-size:90%;">
L.¬†Dai, L.¬†Ma, S.¬†Qian, H.¬†Liu, Z.¬†Liu, H.¬†Xiong, Cloth2body: Generating 3d
human body mesh from 2d clothing, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2023, pp. 15007‚Äì15017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.2.2.1" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.4.1" style="font-size:90%;">
S.¬†Tang, G.¬†Wang, Q.¬†Ran, L.¬†Li, L.¬†Shen, P.¬†Tan, High-resolution volumetric
reconstruction for clothed humans, ACM Transactions on Graphics 42¬†(5) (2023)
1‚Äì15.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.2.2.1" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.4.1" style="font-size:90%;">
Y.¬†Feng, W.¬†Liu, T.¬†Bolkart, J.¬†Yang, M.¬†Pollefeys, M.¬†J. Black, Learning
disentangled avatars with hybrid 3d representations, arXiv preprint
arXiv:2309.06441 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.2.2.1" style="font-size:90%;">[11]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.4.1" style="font-size:90%;">
P.¬†Wang, Z.¬†Zhao, F.¬†Su, X.¬†Zu, N.¬†V. Boulgouris, Horeid: deep high-order
mapping enhances pose alignment for person re-identification, IEEE
Transactions on Image Processing 30 (2021) 2908‚Äì2922.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.2.2.1" style="font-size:90%;">[12]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.4.1" style="font-size:90%;">
H.¬†Liu, T.¬†Liu, Z.¬†Zhang, A.¬†K. Sangaiah, B.¬†Yang, Y.¬†Li, Arhpe: Asymmetric
relation-aware representation learning for head pose estimation in industrial
human‚Äìcomputer interaction, IEEE Transactions on Industrial Informatics
18¬†(10) (2022) 7107‚Äì7117.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.2.2.1" style="font-size:90%;">[13]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.4.1" style="font-size:90%;">
J.¬†Zou, Simplified neural architecture for efficient human motion prediction in
human-robot interaction, Neurocomputing 588 (2024) 127683.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.2.2.1" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.4.1" style="font-size:90%;">
J.¬†Zheng, X.¬†Shi, A.¬†Gorban, J.¬†Mao, Y.¬†Song, C.¬†R. Qi, T.¬†Liu, V.¬†Chari,
A.¬†Cornman, Y.¬†Zhou, et¬†al., Multi-modal 3d human pose estimation with 2d
weak supervision in autonomous driving, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2022, pp. 4478‚Äì4487.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.2.2.1" style="font-size:90%;">[15]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.4.1" style="font-size:90%;">
J.¬†Wang, Y.¬†Yuan, Z.¬†Luo, K.¬†Xie, D.¬†Lin, U.¬†Iqbal, S.¬†Fidler, S.¬†Khamis,
Learning human dynamics in autonomous driving scenarios, in: Proceedings of
the IEEE/CVF International Conference on Computer Vision, 2023, pp.
20796‚Äì20806.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.2.2.1" style="font-size:90%;">[16]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.4.1" style="font-size:90%;">
C.-Y. Weng, B.¬†Curless, I.¬†Kemelmacher-Shlizerman, Photo wake-up: 3d character
animation from a single photo, in: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, 2019, pp. 5908‚Äì5917.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.2.2.1" style="font-size:90%;">[17]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.4.1" style="font-size:90%;">
W.¬†Liu, Q.¬†Bao, Y.¬†Sun, T.¬†Mei, Recent advances of monocular 2d and 3d human
pose estimation: A deep learning perspective, ACM Computing Surveys 55¬†(4)
(2022) 1‚Äì41.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.2.2.1" style="font-size:90%;">[18]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.4.1" style="font-size:90%;">
C.¬†Zheng, W.¬†Wu, C.¬†Chen, T.¬†Yang, S.¬†Zhu, J.¬†Shen, N.¬†Kehtarnavaz, M.¬†Shah,
Deep learning-based human pose estimation: A survey, ACM Computing Surveys
56¬†(1) (2023) 1‚Äì37.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.2.2.1" style="font-size:90%;">[19]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.4.1" style="font-size:90%;">
Y.¬†Tian, H.¬†Zhang, Y.¬†Liu, L.¬†Wang, Recovering 3d human mesh from monocular
images: A survey, IEEE transactions on pattern analysis and machine
intelligence (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.2.2.1" style="font-size:90%;">[20]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.4.1" style="font-size:90%;">
L.¬†Chen, S.¬†Peng, X.¬†Zhou, Towards efficient and photorealistic 3d human
reconstruction: a brief survey, Visual Informatics 5¬†(4) (2021) 11‚Äì19.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.2.2.1" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.4.1" style="font-size:90%;">
M.¬†Einfalt, K.¬†Ludwig, R.¬†Lienhart, Uplift and upsample: Efficient 3d human
pose estimation with uplifting transformers, in: Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision, 2023, pp. 2903‚Äì2913.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.2.2.1" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.4.1" style="font-size:90%;">
Y.¬†Luo, Y.¬†Li, M.¬†Foshey, W.¬†Shou, P.¬†Sharma, T.¬†Palacios, A.¬†Torralba,
W.¬†Matusik, Intelligent carpet: Inferring 3d human pose from tactile signals,
in: Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, 2021, pp. 11255‚Äì11265.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.2.2.1" style="font-size:90%;">[23]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.4.1" style="font-size:90%;">
A.¬†Ruget, M.¬†Tyler, G.¬†Mora¬†Mart√≠n, S.¬†Scholes, F.¬†Zhu, I.¬†Gyongy,
B.¬†Hearn, S.¬†McLaughlin, A.¬†Halimi, J.¬†Leach, Pixels2pose: Super-resolution
time-of-flight imaging for 3d pose estimation, Science Advances 8¬†(48) (2022)
eade0123.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.2.2.1" style="font-size:90%;">[24]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.4.1" style="font-size:90%;">
R.¬†Pandey, A.¬†Tkach, S.¬†Yang, P.¬†Pidlypenskyi, J.¬†Taylor, R.¬†Martin-Brualla,
A.¬†Tagliasacchi, G.¬†Papandreou, P.¬†Davidson, C.¬†Keskin, et¬†al., Volumetric
capture of humans with a single rgbd camera via semi-parametric learning, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2019, pp. 9709‚Äì9718.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.2.2.1" style="font-size:90%;">[25]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.4.1" style="font-size:90%;">
Y.¬†Ren, Z.¬†Wang, Y.¬†Wang, S.¬†Tan, Y.¬†Chen, J.¬†Yang, Gopose: 3d human pose
estimation using wifi, Proceedings of the ACM on Interactive, Mobile,
Wearable and Ubiquitous Technologies 6¬†(2) (2022) 1‚Äì25.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.2.2.1" style="font-size:90%;">[26]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.4.1" style="font-size:90%;">
T.¬†Li, L.¬†Fan, Y.¬†Yuan, D.¬†Katabi, Unsupervised learning for human sensing
using radio signals, in: Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision, 2022, pp. 3288‚Äì3297.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.2.2.1" style="font-size:90%;">[27]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.4.1" style="font-size:90%;">
J.¬†L. Ponton, H.¬†Yun, A.¬†Aristidou, C.¬†Andujar, N.¬†Pelechano, Sparseposer:
Real-time full-body motion reconstruction from sparse data, ACM Transactions
on Graphics 43¬†(1) (2023) 1‚Äì14.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.2.2.1" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.4.1" style="font-size:90%;">
F.¬†Huang, A.¬†Zeng, M.¬†Liu, Q.¬†Lai, Q.¬†Xu, Deepfuse: An imu-aware network for
real-time 3d human pose estimation from multi-view image, in: Proceedings of
the IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp.
429‚Äì438.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.2.2.1" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.4.1" style="font-size:90%;">
S.¬†Zou, X.¬†Zuo, S.¬†Wang, Y.¬†Qian, C.¬†Guo, L.¬†Cheng, Human pose and shape
estimation from single polarization images, IEEE Transactions on Multimedia
(2022).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.2.2.1" style="font-size:90%;">[30]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.4.1" style="font-size:90%;">
L.¬†Xu, W.¬†Xu, V.¬†Golyanik, M.¬†Habermann, L.¬†Fang, C.¬†Theobalt, Eventcap:
Monocular 3d capture of high-speed human motions using an event camera, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2020, pp. 4968‚Äì4978.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.2.2.1" style="font-size:90%;">[31]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.4.1" style="font-size:90%;">
B.¬†Jiang, L.¬†Hu, S.¬†Xia, Probabilistic triangulation for uncalibrated
multi-view 3d human pose estimation, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2023, pp. 14850‚Äì14860.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.2.2.1" style="font-size:90%;">[32]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.4.1" style="font-size:90%;">
H.¬†Shuai, L.¬†Wu, Q.¬†Liu, Adaptive multi-view and temporal fusing transformer
for 3d human pose estimation, IEEE Transactions on Pattern Analysis and
Machine Intelligence 45¬†(4) (2022) 4122‚Äì4135.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.2.2.1" style="font-size:90%;">[33]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.4.1" style="font-size:90%;">
B.¬†Huang, Y.¬†Shu, T.¬†Zhang, Y.¬†Wang, Dynamic multi-person mesh recovery from
uncalibrated multi-view cameras, in: 2021 International Conference on 3D
Vision (3DV), IEEE, 2021, pp. 710‚Äì720.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.2.2.1" style="font-size:90%;">[34]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.4.1" style="font-size:90%;">
D.¬†Anguelov, P.¬†Srinivasan, D.¬†Koller, S.¬†Thrun, J.¬†Rodgers, J.¬†Davis, Scape:
shape completion and animation of people, in: ACM SIGGRAPH 2005 Papers, 2005,
pp. 408‚Äì416.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.2.2.1" style="font-size:90%;">[35]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.4.1" style="font-size:90%;">
M.¬†Loper, N.¬†Mahmood, J.¬†Romero, G.¬†Pons-Moll, M.¬†J. Black, Smpl: A skinned
multi-person linear model, ACM transactions on graphics (TOG) 34¬†(6) (2015)
1‚Äì16.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.2.2.1" style="font-size:90%;">[36]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.4.1" style="font-size:90%;">
J.¬†Romero, D.¬†Tzionas, M.¬†J. Black, Embodied hands: Modeling and capturing
hands and bodies together, arXiv preprint arXiv:2201.02610 (2022).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.2.2.1" style="font-size:90%;">[37]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.4.1" style="font-size:90%;">
T.¬†Li, T.¬†Bolkart, M.¬†J. Black, H.¬†Li, J.¬†Romero, Learning a model of facial
shape and expression from 4d scans., ACM Trans. Graph. 36¬†(6) (2017) 194‚Äì1.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.2.2.1" style="font-size:90%;">[38]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.4.1" style="font-size:90%;">
G.¬†Pavlakos, V.¬†Choutas, N.¬†Ghorbani, T.¬†Bolkart, A.¬†A. Osman, D.¬†Tzionas,
M.¬†J. Black, Expressive body capture: 3d hands, face, and body from a single
image, in: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, 2019, pp. 10975‚Äì10985.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.2.2.1" style="font-size:90%;">[39]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.4.1" style="font-size:90%;">
B.¬†Jiang, Y.¬†Zhang, X.¬†Wei, X.¬†Xue, Y.¬†Fu, H4d: Human 4d modeling by learning
neural compositional representation, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2022, pp.
19355‚Äì19365.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.2.2.1" style="font-size:90%;">[40]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.4.1" style="font-size:90%;">
G.¬†Varol, D.¬†Ceylan, B.¬†Russell, J.¬†Yang, E.¬†Yumer, I.¬†Laptev, C.¬†Schmid,
Bodynet: Volumetric inference of 3d human body shapes, in: Proceedings of the
European conference on computer vision (ECCV), 2018, pp. 20‚Äì36.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.2.2.1" style="font-size:90%;">[41]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.4.1" style="font-size:90%;">
H.¬†Onizuka, Z.¬†Hayirci, D.¬†Thomas, A.¬†Sugimoto, H.¬†Uchiyama, R.-i. Taniguchi,
Tetratsdf: 3d human reconstruction from a single image with a tetrahedral
outer shell, in: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2020, pp. 6011‚Äì6020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.2.2.1" style="font-size:90%;">[42]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.4.1" style="font-size:90%;">
Z.¬†Zheng, T.¬†Yu, Y.¬†Liu, Q.¬†Dai, Pamir: Parametric model-conditioned implicit
representation for image-based human reconstruction, IEEE transactions on
pattern analysis and machine intelligence 44¬†(6) (2021) 3170‚Äì3184.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.2.2.1" style="font-size:90%;">[43]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.4.1" style="font-size:90%;">
K.¬†He, X.¬†Zhang, S.¬†Ren, J.¬†Sun, Deep residual learning for image recognition,
in: Proceedings of the IEEE conference on computer vision and pattern
recognition, 2016, pp. 770‚Äì778.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.2.2.1" style="font-size:90%;">[44]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.4.1" style="font-size:90%;">
J.¬†Wang, K.¬†Sun, T.¬†Cheng, B.¬†Jiang, C.¬†Deng, Y.¬†Zhao, D.¬†Liu, Y.¬†Mu, M.¬†Tan,
X.¬†Wang, et¬†al., Deep high-resolution representation learning for visual
recognition, IEEE transactions on pattern analysis and machine intelligence
43¬†(10) (2020) 3349‚Äì3364.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.2.2.1" style="font-size:90%;">[45]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.4.1" style="font-size:90%;">
I.¬†O. Tolstikhin, N.¬†Houlsby, A.¬†Kolesnikov, L.¬†Beyer, X.¬†Zhai, T.¬†Unterthiner,
J.¬†Yung, A.¬†Steiner, D.¬†Keysers, J.¬†Uszkoreit, et¬†al., Mlp-mixer: An all-mlp
architecture for vision, Advances in neural information processing systems 34
(2021) 24261‚Äì24272.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.2.2.1" style="font-size:90%;">[46]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.4.1" style="font-size:90%;">
A.¬†Dosovitskiy, L.¬†Beyer, A.¬†Kolesnikov, D.¬†Weissenborn, X.¬†Zhai,
T.¬†Unterthiner, M.¬†Dehghani, M.¬†Minderer, G.¬†Heigold, S.¬†Gelly, et¬†al., An
image is worth 16x16 words: Transformers for image recognition at scale,
arXiv preprint arXiv:2010.11929 (2020).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.2.2.1" style="font-size:90%;">[47]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.4.1" style="font-size:90%;">
C.-Y. Yang, J.¬†Luo, L.¬†Xia, Y.¬†Sun, N.¬†Qiao, K.¬†Zhang, Z.¬†Jiang, J.-N. Hwang,
C.-H. Kuo, Camerapose: Weakly-supervised monocular 3d human pose estimation
by leveraging in-the-wild 2d annotations, in: Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision, 2023, pp. 2924‚Äì2933.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.2.2.1" style="font-size:90%;">[48]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.4.1" style="font-size:90%;">
A.¬†Zanfir, E.¬†G. Bazavan, H.¬†Xu, W.¬†T. Freeman, R.¬†Sukthankar, C.¬†Sminchisescu,
Weakly supervised 3d human pose and shape reconstruction with normalizing
flows, in: Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK,
August 23‚Äì28, 2020, Proceedings, Part VI 16, Springer, 2020, pp. 465‚Äì481.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.2.2.1" style="font-size:90%;">[49]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.4.1" style="font-size:90%;">
W.¬†Chai, Z.¬†Jiang, J.-N. Hwang, G.¬†Wang, Global adaptation meets local
generalization: Unsupervised domain adaptation for 3d human pose estimation,
arXiv preprint arXiv:2303.16456 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.2.2.1" style="font-size:90%;">[50]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.4.1" style="font-size:90%;">
Z.¬†Yu, J.¬†Wang, J.¬†Xu, B.¬†Ni, C.¬†Zhao, M.¬†Wang, W.¬†Zhang, Skeleton2mesh:
Kinematics prior injected unsupervised human mesh recovery, in: Proceedings
of the IEEE/CVF International Conference on Computer Vision, 2021, pp.
8619‚Äì8629.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.2.2.1" style="font-size:90%;">[51]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.4.1" style="font-size:90%;">
J.¬†Mu, S.¬†Sang, N.¬†Vasconcelos, X.¬†Wang, Actorsnerf: Animatable few-shot human
rendering with generalizable nerfs, arXiv preprint arXiv:2304.14401 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.2.2.1" style="font-size:90%;">[52]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.4.1" style="font-size:90%;">
A.¬†Benzine, F.¬†Chabot, B.¬†Luvison, Q.¬†C. Pham, C.¬†Achard, Pandanet:
Anchor-based single-shot multi-person 3d pose estimation, in: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.
6856‚Äì6865.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.2.2.1" style="font-size:90%;">[53]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.4.1" style="font-size:90%;">
Z.¬†Yang, A.¬†Zeng, C.¬†Yuan, Y.¬†Li, Effective whole-body pose estimation with
two-stages distillation, in: Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2023, pp. 4210‚Äì4220.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.2.2.1" style="font-size:90%;">[54]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.4.1" style="font-size:90%;">
S.¬†Tripathi, S.¬†Ranade, A.¬†Tyagi, A.¬†Agrawal, Posenet3d: Learning temporally
consistent 3d human pose via knowledge distillation, in: 2020 International
Conference on 3D Vision (3DV), IEEE, 2020, pp. 311‚Äì321.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.2.2.1" style="font-size:90%;">[55]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.4.1" style="font-size:90%;">
H.¬†Liu, C.¬†Ren, An effective 3d human pose estimation method based on dilated
convolutions for videos, in: 2019 IEEE International Conference on Robotics
and Biomimetics (ROBIO), IEEE, 2019, pp. 2327‚Äì2331.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib56.2.2.1" style="font-size:90%;">[56]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.4.1" style="font-size:90%;">
S.¬†Choi, S.¬†Choi, C.¬†Kim, Mobilehumanpose: Toward real-time 3d human pose
estimation in mobile devices, in: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, 2021, pp. 2328‚Äì2338.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib57.2.2.1" style="font-size:90%;">[57]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.4.1" style="font-size:90%;">
H.¬†Cho, Y.¬†Cho, J.¬†Yu, J.¬†Kim, Camera distortion-aware 3d human pose estimation
in video with optimization-based meta-learning, in: Proceedings of the
IEEE/CVF International Conference on Computer Vision, 2021, pp. 11169‚Äì11178.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib58.2.2.1" style="font-size:90%;">[58]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.4.1" style="font-size:90%;">
K.¬†Gong, B.¬†Li, J.¬†Zhang, T.¬†Wang, J.¬†Huang, M.¬†B. Mi, J.¬†Feng, X.¬†Wang,
Posetriplet: Co-evolving 3d human pose estimation, imitation, and
hallucination under self-supervision, in: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2022, pp.
11017‚Äì11027.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib59.2.2.1" style="font-size:90%;">[59]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.4.1" style="font-size:90%;">
M.¬†T. Hassan, A.¬†B. Hamza, Regular splitting graph network for 3d human pose
estimation, IEEE Transactions on Image Processing (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib60.2.2.1" style="font-size:90%;">[60]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.4.1" style="font-size:90%;">
Q.¬†Zhao, C.¬†Zheng, M.¬†Liu, P.¬†Wang, C.¬†Chen, Poseformerv2: Exploring frequency
domain for efficient and robust 3d human pose estimation, in: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
8877‚Äì8886.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib61.2.2.1" style="font-size:90%;">[61]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.4.1" style="font-size:90%;">
Z.¬†Cai, W.¬†Yin, A.¬†Zeng, C.¬†Wei, Q.¬†Sun, Y.¬†Wang, H.¬†E. Pang, H.¬†Mei, M.¬†Zhang,
L.¬†Zhang, et¬†al., Smpler-x: Scaling up expressive human pose and shape
estimation, arXiv preprint arXiv:2309.17448 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib62.2.2.1" style="font-size:90%;">[62]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.4.1" style="font-size:90%;">
R.¬†Li, Y.¬†Xiu, S.¬†Saito, Z.¬†Huang, K.¬†Olszewski, H.¬†Li, Monocular real-time
volumetric performance capture, in: Computer Vision‚ÄìECCV 2020: 16th European
Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part XXIII 16,
Springer, 2020, pp. 49‚Äì67.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib63.2.2.1" style="font-size:90%;">[63]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.4.1" style="font-size:90%;">
G.¬†Wei, C.¬†Lan, W.¬†Zeng, Z.¬†Chen, View invariant 3d human pose estimation, IEEE
Transactions on Circuits and Systems for Video Technology 30¬†(12) (2019)
4601‚Äì4610.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib64.2.2.1" style="font-size:90%;">[64]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.4.1" style="font-size:90%;">
Y.¬†Zhan, F.¬†Li, R.¬†Weng, W.¬†Choi, Ray3d: ray-based 3d human pose estimation for
monocular absolute 3d localization, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2022, pp.
13116‚Äì13125.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib65.2.2.1" style="font-size:90%;">[65]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.4.1" style="font-size:90%;">
K.¬†Zhou, X.¬†Han, N.¬†Jiang, K.¬†Jia, J.¬†Lu, Hemlets posh: learning part-centric
heatmap triplets for 3d human pose and shape estimation, IEEE Transactions on
Pattern Analysis and Machine Intelligence 44¬†(6) (2021) 3000‚Äì3014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib66.2.2.1" style="font-size:90%;">[66]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.4.1" style="font-size:90%;">
X.¬†Zheng, X.¬†Chen, X.¬†Lu, A joint relationship aware neural network for
single-image 3d human pose estimation, IEEE Transactions on Image Processing
29 (2020) 4747‚Äì4758.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib67.2.2.1" style="font-size:90%;">[67]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.4.1" style="font-size:90%;">
L.¬†Wu, Z.¬†Yu, Y.¬†Liu, Q.¬†Liu, Limb pose aware networks for monocular 3d pose
estimation, IEEE Transactions on Image Processing 31 (2021) 906‚Äì917.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib68.2.2.1" style="font-size:90%;">[68]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.4.1" style="font-size:90%;">
Y.¬†Xu, W.¬†Wang, T.¬†Liu, X.¬†Liu, J.¬†Xie, S.-C. Zhu, Monocular 3d pose estimation
via pose grammar and data augmentation, IEEE Transactions on Pattern Analysis
and Machine Intelligence 44¬†(10) (2021) 6327‚Äì6344.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib69.2.2.1" style="font-size:90%;">[69]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.4.1" style="font-size:90%;">
M.¬†Fisch, R.¬†Clark, Orientation keypoints for 6d human pose estimation, IEEE
Transactions on Pattern Analysis and Machine Intelligence 44¬†(12) (2021)
10145‚Äì10158.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib70.2.2.1" style="font-size:90%;">[70]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.4.1" style="font-size:90%;">
J.¬†Liu, H.¬†Ding, A.¬†Shahroudy, L.-Y. Duan, X.¬†Jiang, G.¬†Wang, A.¬†C. Kot,
Feature boosting network for 3d pose estimation, IEEE transactions on pattern
analysis and machine intelligence 42¬†(2) (2019) 494‚Äì501.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib71.2.2.1" style="font-size:90%;">[71]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.4.1" style="font-size:90%;">
H.¬†Ci, X.¬†Ma, C.¬†Wang, Y.¬†Wang, Locally connected network for monocular 3d
human pose estimation, IEEE Transactions on Pattern Analysis and Machine
Intelligence 44¬†(3) (2020) 1429‚Äì1442.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib72.2.2.1" style="font-size:90%;">[72]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.4.1" style="font-size:90%;">
Z.¬†Zou, W.¬†Tang, Modulated graph convolutional network for 3d human pose
estimation, in: Proceedings of the IEEE/CVF International Conference on
Computer Vision, 2021, pp. 11477‚Äì11487.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib73.2.2.1" style="font-size:90%;">[73]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.4.1" style="font-size:90%;">
A.¬†Zeng, X.¬†Sun, L.¬†Yang, N.¬†Zhao, M.¬†Liu, Q.¬†Xu, Learning skeletal graph
neural networks for hard 3d pose estimation, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2021, pp. 11436‚Äì11445.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib74.2.2.1" style="font-size:90%;">[74]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.4.1" style="font-size:90%;">
K.¬†Zhai, Q.¬†Nie, B.¬†Ouyang, X.¬†Li, S.¬†Yang, Hopfir: Hop-wise graphformer with
intragroup joint refinement for 3d human pose estimation, arXiv preprint
arXiv:2302.14581 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib75.2.2.1" style="font-size:90%;">[75]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.4.1" style="font-size:90%;">
K.¬†Iskakov, E.¬†Burkov, V.¬†Lempitsky, Y.¬†Malkov, Learnable triangulation of
human pose, in: Proceedings of the IEEE/CVF international conference on
computer vision, 2019, pp. 7718‚Äì7727.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib76.2.2.1" style="font-size:90%;">[76]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.4.1" style="font-size:90%;">
H.¬†Qiu, C.¬†Wang, J.¬†Wang, N.¬†Wang, W.¬†Zeng, Cross view fusion for 3d human pose
estimation, in: Proceedings of the IEEE/CVF international conference on
computer vision, 2019, pp. 4342‚Äì4351.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib77.2.2.1" style="font-size:90%;">[77]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.4.1" style="font-size:90%;">
E.¬†Remelli, S.¬†Han, S.¬†Honari, P.¬†Fua, R.¬†Wang, Lightweight multi-view 3d pose
estimation through camera-disentangled representation, in: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, 2020, pp.
6040‚Äì6049.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib78.2.2.1" style="font-size:90%;">[78]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.4.1" style="font-size:90%;">
Z.¬†Zhang, C.¬†Wang, W.¬†Qiu, W.¬†Qin, W.¬†Zeng, Adafuse: Adaptive multiview fusion
for accurate human pose estimation in the wild, International Journal of
Computer Vision 129 (2021) 703‚Äì718.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib79.2.2.1" style="font-size:90%;">[79]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.4.1" style="font-size:90%;">
K.¬†Bartol, D.¬†Bojaniƒá, T.¬†Petkoviƒá, T.¬†Pribaniƒá, Generalizable
human pose triangulation, in: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2022, pp. 11028‚Äì11037.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib80.2.2.1" style="font-size:90%;">[80]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.4.1" style="font-size:90%;">
D.¬†C. Luvizon, D.¬†Picard, H.¬†Tabia, Consensus-based optimization for 3d human
pose estimation in camera coordinates, International Journal of Computer
Vision 130¬†(3) (2022) 869‚Äì882.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib81.2.2.1" style="font-size:90%;">[81]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.4.1" style="font-size:90%;">
Y.¬†Kudo, K.¬†Ogaki, Y.¬†Matsui, Y.¬†Odagiri, Unsupervised adversarial learning of
3d human pose from 2d joint locations, arXiv preprint arXiv:1803.08244
(2018).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib82.2.2.1" style="font-size:90%;">[82]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.4.1" style="font-size:90%;">
C.-H. Chen, A.¬†Tyagi, A.¬†Agrawal, D.¬†Drover, R.¬†Mv, S.¬†Stojanov, J.¬†M. Rehg,
Unsupervised 3d pose estimation with geometric self-supervision, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2019, pp. 5714‚Äì5724.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib83.2.2.1" style="font-size:90%;">[83]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.4.1" style="font-size:90%;">
B.¬†Wandt, J.¬†J. Little, H.¬†Rhodin, Elepose: Unsupervised 3d human pose
estimation by predicting camera elevation and learning normalizing flows on
2d poses, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2022, pp. 6635‚Äì6645.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib84.2.2.1" style="font-size:90%;">[84]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.4.1" style="font-size:90%;">
M.¬†Kocabas, S.¬†Karagoz, E.¬†Akbas, Self-supervised learning of 3d human pose
using multi-view geometry, in: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, 2019, pp. 1077‚Äì1086.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib85.2.2.1" style="font-size:90%;">[85]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.4.1" style="font-size:90%;">
K.¬†Wang, L.¬†Lin, C.¬†Jiang, C.¬†Qian, P.¬†Wei, 3d human pose machines with
self-supervised learning, IEEE transactions on pattern analysis and machine
intelligence 42¬†(5) (2019) 1069‚Äì1082.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib86.2.2.1" style="font-size:90%;">[86]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.4.1" style="font-size:90%;">
J.¬†N. Kundu, S.¬†Seth, P.¬†YM, V.¬†Jampani, A.¬†Chakraborty, R.¬†V. Babu,
Uncertainty-aware adaptation for self-supervised 3d human pose estimation,
in: Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, 2022, pp. 20448‚Äì20459.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib87.2.2.1" style="font-size:90%;">[87]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.4.1" style="font-size:90%;">
G.¬†Hua, H.¬†Liu, W.¬†Li, Q.¬†Zhang, R.¬†Ding, X.¬†Xu, Weakly-supervised 3d human
pose estimation with cross-view u-shaped graph convolutional network, IEEE
Transactions on Multimedia (2022).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib88.2.2.1" style="font-size:90%;">[88]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.4.1" style="font-size:90%;">
M.¬†Gholami, B.¬†Wandt, H.¬†Rhodin, R.¬†Ward, Z.¬†J. Wang, Adaptpose: Cross-dataset
adaptation for 3d human pose estimation by learnable motion generation, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 13075‚Äì13085.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib89.2.2.1" style="font-size:90%;">[89]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.4.1" style="font-size:90%;">
D.¬†Pavllo, C.¬†Feichtenhofer, D.¬†Grangier, M.¬†Auli, 3d human pose estimation in
video with temporal convolutions and semi-supervised training, in:
Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, 2019, pp. 7753‚Äì7762.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib90.2.2.1" style="font-size:90%;">[90]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.4.1" style="font-size:90%;">
C.¬†Zheng, S.¬†Zhu, M.¬†Mendieta, T.¬†Yang, C.¬†Chen, Z.¬†Ding, 3d human pose
estimation with spatial and temporal transformers, in: Proceedings of the
IEEE/CVF International Conference on Computer Vision, 2021, pp. 11656‚Äì11665.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib91.2.2.1" style="font-size:90%;">[91]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.4.1" style="font-size:90%;">
B.¬†Artacho, A.¬†Savakis, Unipose+: A unified framework for 2d and 3d human pose
estimation in images and videos, IEEE Transactions on Pattern Analysis and
Machine Intelligence 44¬†(12) (2021) 9641‚Äì9653.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib92.2.2.1" style="font-size:90%;">[92]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.4.1" style="font-size:90%;">
W.¬†Li, H.¬†Liu, H.¬†Tang, P.¬†Wang, L.¬†Van¬†Gool, Mhformer: Multi-hypothesis
transformer for 3d human pose estimation, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2022, pp.
13147‚Äì13156.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib93.2.2.1" style="font-size:90%;">[93]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.4.1" style="font-size:90%;">
J.¬†Zhang, Z.¬†Tu, J.¬†Yang, Y.¬†Chen, J.¬†Yuan, Mixste: Seq2seq mixed
spatio-temporal encoder for 3d human pose estimation in video, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 13232‚Äì13242.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib94.2.2.1" style="font-size:90%;">[94]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.4.1" style="font-size:90%;">
S.¬†Honari, V.¬†Constantin, H.¬†Rhodin, M.¬†Salzmann, P.¬†Fua, Temporal
representation learning on monocular videos for 3d human pose estimation,
IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib95.2.2.1" style="font-size:90%;">[95]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.4.1" style="font-size:90%;">
X.¬†Qian, Y.¬†Tang, N.¬†Zhang, M.¬†Han, J.¬†Xiao, M.-C. Huang, R.-S. Lin, Hstformer:
Hierarchical spatial-temporal transformers for 3d human pose estimation,
arXiv preprint arXiv:2301.07322 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib96.2.2.1" style="font-size:90%;">[96]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.4.1" style="font-size:90%;">
Z.¬†Tang, Z.¬†Qiu, Y.¬†Hao, R.¬†Hong, T.¬†Yao, 3d human pose estimation with
spatio-temporal criss-cross attention, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2023, pp. 4790‚Äì4799.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib97.2.2.1" style="font-size:90%;">[97]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.4.1" style="font-size:90%;">
Y.¬†Sun, A.¬†W. Dougherty, Z.¬†Zhang, Y.¬†K. Choi, C.¬†Wu, Mixsynthformer: A
transformer encoder-like structure with mixed synthetic self-attention for
efficient human pose estimation, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2023, pp. 14884‚Äì14893.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib98.2.2.1" style="font-size:90%;">[98]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.4.1" style="font-size:90%;">
J.¬†Wang, S.¬†Yan, Y.¬†Xiong, D.¬†Lin, Motion guided 3d pose estimation from
videos, in: Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow,
UK, August 23‚Äì28, 2020, Proceedings, Part XIII 16, Springer, 2020, pp.
764‚Äì780.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib99.2.2.1" style="font-size:90%;">[99]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib99.4.1" style="font-size:90%;">
J.¬†Zhang, Y.¬†Wang, Z.¬†Zhou, T.¬†Luan, Z.¬†Wang, Y.¬†Qiao, Learning dynamical
human-joint affinity for 3d pose estimation in videos, IEEE Transactions on
Image Processing 30 (2021) 7914‚Äì7925.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib100.2.2.1" style="font-size:90%;">[100]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib100.4.1" style="font-size:90%;">
T.¬†Chen, C.¬†Fang, X.¬†Shen, Y.¬†Zhu, Z.¬†Chen, J.¬†Luo, Anatomy-aware 3d human pose
estimation with bone-based pose decomposition, IEEE Transactions on Circuits
and Systems for Video Technology 32¬†(1) (2021) 198‚Äì209.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib101.2.2.1" style="font-size:90%;">[101]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib101.4.1" style="font-size:90%;">
Y.¬†Xue, J.¬†Chen, X.¬†Gu, H.¬†Ma, H.¬†Ma, Boosting monocular 3d human pose
estimation with part aware attention, IEEE Transactions on Image Processing
31 (2022) 4278‚Äì4291.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib102.2.2.1" style="font-size:90%;">[102]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib102.4.1" style="font-size:90%;">
Y.¬†Cheng, B.¬†Yang, B.¬†Wang, W.¬†Yan, R.¬†T. Tan, Occlusion-aware networks for 3d
human pose estimation in video, in: Proceedings of the IEEE/CVF international
conference on computer vision, 2019, pp. 723‚Äì732.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib103.2.2.1" style="font-size:90%;">[103]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib103.4.1" style="font-size:90%;">
Z.¬†Yu, B.¬†Ni, J.¬†Xu, J.¬†Wang, C.¬†Zhao, W.¬†Zhang, Towards alleviating the
modeling ambiguity of unsupervised monocular 3d human pose estimation, in:
Proceedings of the IEEE/CVF International Conference on Computer Vision,
2021, pp. 8651‚Äì8660.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib104.2.2.1" style="font-size:90%;">[104]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib104.4.1" style="font-size:90%;">
X.¬†Chen, K.-Y. Lin, W.¬†Liu, C.¬†Qian, L.¬†Lin, Weakly-supervised discovery of
geometry-aware representation for 3d human pose estimation, in: Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition, 2019,
pp. 10895‚Äì10904.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib105.2.2.1" style="font-size:90%;">[105]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib105.4.1" style="font-size:90%;">
R.¬†Mitra, N.¬†B. Gundavarapu, A.¬†Sharma, A.¬†Jain, Multiview-consistent
semi-supervised learning for 3d human pose estimation, in: Proceedings of the
ieee/cvf conference on computer vision and pattern recognition, 2020, pp.
6907‚Äì6916.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib106.2.2.1" style="font-size:90%;">[106]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib106.4.1" style="font-size:90%;">
J.¬†N. Kundu, S.¬†Seth, V.¬†Jampani, M.¬†Rakesh, R.¬†V. Babu, A.¬†Chakraborty,
Self-supervised 3d human pose estimation via part guided novel image
synthesis, in: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, 2020, pp. 6152‚Äì6162.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib107.2.2.1" style="font-size:90%;">[107]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib107.4.1" style="font-size:90%;">
W.¬†Shan, Z.¬†Liu, X.¬†Zhang, S.¬†Wang, S.¬†Ma, W.¬†Gao, P-stmo: Pre-trained spatial
temporal many-to-one model for 3d human pose estimation, in: Computer
Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October
23‚Äì27, 2022, Proceedings, Part V, Springer, 2022, pp. 461‚Äì478.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib108.2.2.1" style="font-size:90%;">[108]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib108.4.1" style="font-size:90%;">
K.¬†Gong, J.¬†Zhang, J.¬†Feng, Poseaug: A differentiable pose augmentation
framework for 3d human pose estimation, in: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2021, pp. 8575‚Äì8584.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib109.2.2.1" style="font-size:90%;">[109]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib109.4.1" style="font-size:90%;">
J.¬†Zhang, K.¬†Gong, X.¬†Wang, J.¬†Feng, Learning to augment poses for 3d human
pose estimation in images and videos, IEEE Transactions on Pattern Analysis
and Machine Intelligence (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib110.2.2.1" style="font-size:90%;">[110]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib110.4.1" style="font-size:90%;">
L.¬†Chen, H.¬†Ai, R.¬†Chen, Z.¬†Zhuang, S.¬†Liu, Cross-view tracking for multi-human
3d pose estimation at over 100 fps, in: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2020, pp. 3279‚Äì3288.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib111.2.2.1" style="font-size:90%;">[111]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib111.4.1" style="font-size:90%;">
H.-S. Fang, J.¬†Li, H.¬†Tang, C.¬†Xu, H.¬†Zhu, Y.¬†Xiu, Y.-L. Li, C.¬†Lu, Alphapose:
Whole-body regional multi-person pose estimation and tracking in real-time,
IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib112.2.2.1" style="font-size:90%;">[112]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib112.4.1" style="font-size:90%;">
S.¬†Wu, S.¬†Jin, W.¬†Liu, L.¬†Bai, C.¬†Qian, D.¬†Liu, W.¬†Ouyang, Graph-based 3d
multi-person pose estimation using multi-view images, in: Proceedings of the
IEEE/CVF international conference on computer vision, 2021, pp. 11148‚Äì11157.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib113.2.2.1" style="font-size:90%;">[113]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib113.4.1" style="font-size:90%;">
G.¬†Moon, J.¬†Y. Chang, K.¬†M. Lee, Camera distance-aware top-down approach for 3d
multi-person pose estimation from a single rgb image, in: Proceedings of the
IEEE/CVF international conference on computer vision, 2019, pp. 10133‚Äì10142.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib114.2.2.1" style="font-size:90%;">[114]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib114.4.1" style="font-size:90%;">
M.¬†Fabbri, F.¬†Lanzi, S.¬†Calderara, S.¬†Alletto, R.¬†Cucchiara, Compressed
volumetric heatmaps for multi-person 3d pose estimation, in: Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp.
7204‚Äì7213.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib115.2.2.1" style="font-size:90%;">[115]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib115.4.1" style="font-size:90%;">
C.¬†Wang, J.¬†Li, W.¬†Liu, C.¬†Qian, C.¬†Lu, Hmor: Hierarchical multi-person ordinal
relations for monocular multi-person 3d pose estimation, in: Computer
Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28,
2020, Proceedings, Part III 16, Springer, 2020, pp. 242‚Äì259.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib116.2.2.1" style="font-size:90%;">[116]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib116.4.1" style="font-size:90%;">
J.¬†Zhen, Q.¬†Fang, J.¬†Sun, W.¬†Liu, W.¬†Jiang, H.¬†Bao, X.¬†Zhou, Smap: Single-shot
multi-person absolute 3d pose estimation, in: Computer Vision‚ÄìECCV 2020:
16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part
XV 16, Springer, 2020, pp. 550‚Äì566.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib117.2.2.1" style="font-size:90%;">[117]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib117.4.1" style="font-size:90%;">
A.¬†Benzine, B.¬†Luvison, Q.¬†C. Pham, C.¬†Achard, Single-shot 3d multi-person pose
estimation in complex images, Pattern Recognition 112 (2021) 107534.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib118.2.2.1" style="font-size:90%;">[118]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib118.4.1" style="font-size:90%;">
D.¬†Mehta, O.¬†Sotnychenko, F.¬†Mueller, W.¬†Xu, S.¬†Sridhar, G.¬†Pons-Moll,
C.¬†Theobalt, Single-shot multi-person 3d pose estimation from monocular rgb,
in: 2018 International Conference on 3D Vision (3DV), IEEE, 2018, pp.
120‚Äì130.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib119.2.2.1" style="font-size:90%;">[119]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib119.4.1" style="font-size:90%;">
G.¬†Rogez, P.¬†Weinzaepfel, C.¬†Schmid, Lcr-net++: Multi-person 2d and 3d pose
detection in natural images, IEEE transactions on pattern analysis and
machine intelligence 42¬†(5) (2019) 1146‚Äì1161.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib120.2.2.1" style="font-size:90%;">[120]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib120.4.1" style="font-size:90%;">
L.¬†Jin, C.¬†Xu, X.¬†Wang, Y.¬†Xiao, Y.¬†Guo, X.¬†Nie, J.¬†Zhao, Single-stage is
enough: Multi-person absolute 3d pose estimation, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.
13086‚Äì13095.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib121.2.2.1" style="font-size:90%;">[121]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib121.4.1" style="font-size:90%;">
Y.¬†Cheng, B.¬†Wang, R.¬†T. Tan, Dual networks based 3d multi-person pose
estimation from monocular video, IEEE Transactions on Pattern Analysis and
Machine Intelligence 45¬†(2) (2022) 1636‚Äì1651.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib122.2.2.1" style="font-size:90%;">[122]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib122.4.1" style="font-size:90%;">
Z.¬†Tang, Y.¬†Hao, J.¬†Li, R.¬†Hong, Ftcm: Frequency-temporal collaborative module
for efficient 3d human pose estimation in video, IEEE Transactions on
Circuits and Systems for Video Technology (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib123.2.2.1" style="font-size:90%;">[123]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib123.4.1" style="font-size:90%;">
B.¬†Artacho, A.¬†Savakis, Unipose: Unified human pose estimation in single images
and videos, in: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, 2020, pp. 7035‚Äì7044.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib124.2.2.1" style="font-size:90%;">[124]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib124.4.1" style="font-size:90%;">
A.¬†Zanfir, E.¬†Marinoiu, M.¬†Zanfir, A.-I. Popa, C.¬†Sminchisescu, Deep network
for the integrated 3d sensing of multiple people in natural images, Advances
in neural information processing systems 31 (2018).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib125.2.2.1" style="font-size:90%;">[125]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib125.4.1" style="font-size:90%;">
A.¬†Newell, K.¬†Yang, J.¬†Deng, Stacked hourglass networks for human pose
estimation, in: Computer Vision‚ÄìECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14,
Springer, 2016, pp. 483‚Äì499.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib126.2.2.1" style="font-size:90%;">[126]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib126.4.1" style="font-size:90%;">
Y.¬†Rong, Z.¬†Liu, C.¬†Li, K.¬†Cao, C.¬†C. Loy, Delving deep into hybrid annotations
for 3d human recovery in the wild, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2019, pp. 5340‚Äì5348.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib127.2.2.1" style="font-size:90%;">[127]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib127.4.1" style="font-size:90%;">
Z.¬†Li, B.¬†Xu, H.¬†Huang, C.¬†Lu, Y.¬†Guo, Deep two-stream video inference for
human body pose and shape estimation, in: Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision, 2022, pp. 430‚Äì439.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib128.2.2.1" style="font-size:90%;">[128]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib128.4.1" style="font-size:90%;">
K.¬†Yang, R.¬†Gu, M.¬†Wang, M.¬†Toyoura, G.¬†Xu, Lasor: Learning accurate 3d human
pose and shape via synthetic occlusion-aware data and neural mesh rendering,
IEEE Transactions on Image Processing 31 (2022) 1938‚Äì1948.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib129.2.2.1" style="font-size:90%;">[129]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib129.4.1" style="font-size:90%;">
Z.¬†Li, J.¬†Liu, Z.¬†Zhang, S.¬†Xu, Y.¬†Yan, Cliff: Carrying location information in
full frames into human pose and shape estimation, in: Computer Vision‚ÄìECCV
2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022,
Proceedings, Part V, Springer, 2022, pp. 590‚Äì606.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib130.2.2.1" style="font-size:90%;">[130]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib130.4.1" style="font-size:90%;">
M.¬†Kocabas, C.-H.¬†P. Huang, O.¬†Hilliges, M.¬†J. Black, Pare: Part attention
regressor for 3d human body estimation, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2021, pp. 11127‚Äì11137.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib131.2.2.1" style="font-size:90%;">[131]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib131.4.1" style="font-size:90%;">
K.¬†Lin, L.¬†Wang, Z.¬†Liu, Mesh graphormer, in: Proceedings of the IEEE/CVF
international conference on computer vision, 2021, pp. 12939‚Äì12948.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib132.2.2.1" style="font-size:90%;">[132]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib132.4.1" style="font-size:90%;">
W.-L. Wei, J.-C. Lin, T.-L. Liu, H.-Y.¬†M. Liao, Capturing humans in motion:
temporal-attentive 3d human pose and shape estimation from monocular video,
in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 13211‚Äì13220.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib133.2.2.1" style="font-size:90%;">[133]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib133.4.1" style="font-size:90%;">
Z.¬†Qiu, Q.¬†Yang, J.¬†Wang, H.¬†Feng, J.¬†Han, E.¬†Ding, C.¬†Xu, D.¬†Fu, J.¬†Wang,
Psvt: End-to-end multi-person 3d pose and shape estimation with progressive
video transformers, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2023, pp. 21254‚Äì21263.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib134.2.2.1" style="font-size:90%;">[134]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib134.4.1" style="font-size:90%;">
J.¬†Cho, K.¬†Youwang, T.-H. Oh, Cross-attention of disentangled modalities for 3d
human mesh recovery with transformers, in: Computer Vision‚ÄìECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings,
Part I, Springer, 2022, pp. 342‚Äì359.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib135.2.2.1" style="font-size:90%;">[135]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib135.4.1" style="font-size:90%;">
Y.¬†Xue, J.¬†Chen, Y.¬†Zhang, C.¬†Yu, H.¬†Ma, H.¬†Ma, 3d human mesh reconstruction by
learning to sample joint adaptive tokens for transformers, in: Proceedings of
the 30th ACM International Conference on Multimedia, 2022, pp. 6765‚Äì6773.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib136.2.2.1" style="font-size:90%;">[136]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib136.4.1" style="font-size:90%;">
K.¬†Lin, L.¬†Wang, Z.¬†Liu, End-to-end human pose and mesh reconstruction with
transformers, in: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition, 2021, pp. 1954‚Äì1963.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib137.2.2.1" style="font-size:90%;">[137]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib137.4.1" style="font-size:90%;">
A.¬†Kanazawa, J.¬†Y. Zhang, P.¬†Felsen, J.¬†Malik, Learning 3d human dynamics from
video, in: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, 2019, pp. 5614‚Äì5623.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib138.2.2.1" style="font-size:90%;">[138]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib138.4.1" style="font-size:90%;">
M.¬†Kocabas, N.¬†Athanasiou, M.¬†J. Black, Vibe: Video inference for human body
pose and shape estimation, in: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, 2020, pp. 5253‚Äì5263.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib139.2.2.1" style="font-size:90%;">[139]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib139.4.1" style="font-size:90%;">
H.¬†Choi, G.¬†Moon, J.¬†Y. Chang, K.¬†M. Lee, Beyond static features for temporally
consistent 3d human pose and shape from a video, in: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, 2021, pp.
1964‚Äì1973.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib140.2.2.1" style="font-size:90%;">[140]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib140.4.1" style="font-size:90%;">
Z.¬†Wan, Z.¬†Li, M.¬†Tian, J.¬†Liu, S.¬†Yi, H.¬†Li, Encoder-decoder with multi-level
attention for 3d human shape and pose estimation, in: Proceedings of the
IEEE/CVF International Conference on Computer Vision, 2021, pp. 13033‚Äì13042.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib141.2.2.1" style="font-size:90%;">[141]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib141.4.1" style="font-size:90%;">
Z.¬†Wang, S.¬†Ostadabbas, Live stream temporally embedded 3d human body pose and
shape estimation, arXiv preprint arXiv:2207.12537 (2022).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib142.2.2.1" style="font-size:90%;">[142]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib142.4.1" style="font-size:90%;">
X.¬†Shen, Z.¬†Yang, X.¬†Wang, J.¬†Ma, C.¬†Zhou, Y.¬†Yang, Global-to-local modeling
for video-based 3d human pose and shape estimation, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
8887‚Äì8896.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib143.2.2.1" style="font-size:90%;">[143]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib143.4.1" style="font-size:90%;">
Z.¬†Dong, J.¬†Song, X.¬†Chen, C.¬†Guo, O.¬†Hilliges, Shape-aware multi-person pose
estimation from multi-view images, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2021, pp. 11158‚Äì11168.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib144.2.2.1" style="font-size:90%;">[144]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib144.4.1" style="font-size:90%;">
A.¬†Sengupta, I.¬†Budvytis, R.¬†Cipolla, Probabilistic 3d human shape and pose
estimation from multiple unconstrained images in the wild, in: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp.
16094‚Äì16104.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib145.2.2.1" style="font-size:90%;">[145]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib145.4.1" style="font-size:90%;">
L.¬†Zhuo, J.¬†Cao, Q.¬†Wang, B.¬†Zhang, L.¬†Bo, Towards stable human pose estimation
via cross-view fusion and foot stabilization, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2023, pp. 650‚Äì659.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib146.2.2.1" style="font-size:90%;">[146]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib146.4.1" style="font-size:90%;">
T.¬†Fan, K.¬†V. Alwala, D.¬†Xiang, W.¬†Xu, T.¬†Murphey, M.¬†Mukadam, Revitalizing
optimization for 3d human pose and shape estimation: A sparse constrained
formulation, in: Proceedings of the IEEE/CVF International Conference on
Computer Vision, 2021, pp. 11457‚Äì11466.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib147.2.2.1" style="font-size:90%;">[147]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib147.4.1" style="font-size:90%;">
J.¬†Zhang, D.¬†Yu, J.¬†H. Liew, X.¬†Nie, J.¬†Feng, Body meshes as points, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2021, pp. 546‚Äì556.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib148.2.2.1" style="font-size:90%;">[148]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib148.4.1" style="font-size:90%;">
C.¬†Zheng, M.¬†Mendieta, T.¬†Yang, C.¬†Chen, Heater: An efficient and unified
network for human reconstruction via heatmap-based transformer, arXiv
preprint arXiv:2205.15448 (2022).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib149.2.2.1" style="font-size:90%;">[149]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib149.4.1" style="font-size:90%;">
Z.¬†Dou, Q.¬†Wu, C.¬†Lin, Z.¬†Cao, Q.¬†Wu, W.¬†Wan, T.¬†Komura, W.¬†Wang, Tore: Token
reduction for efficient human mesh recovery with transformer, in: Proceedings
of the IEEE/CVF International Conference on Computer Vision, 2023, pp.
15143‚Äì15155.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib150.2.2.1" style="font-size:90%;">[150]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib150.4.1" style="font-size:90%;">
G.¬†Pavlakos, N.¬†Kolotouros, K.¬†Daniilidis, Texturepose: Supervising human mesh
estimation with texture consistency, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2019, pp. 803‚Äì812.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib151.2.2.1" style="font-size:90%;">[151]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib151.4.1" style="font-size:90%;">
H.¬†Zhang, J.¬†Cao, G.¬†Lu, W.¬†Ouyang, Z.¬†Sun, Learning 3d human shape and pose
from dense body parts, IEEE Transactions on Pattern Analysis and Machine
Intelligence 44¬†(5) (2020) 2610‚Äì2627.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib152.2.2.1" style="font-size:90%;">[152]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib152.4.1" style="font-size:90%;">
W.¬†Zeng, W.¬†Ouyang, P.¬†Luo, W.¬†Liu, X.¬†Wang, 3d human mesh regression with
dense correspondence, in: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, 2020, pp. 7054‚Äì7063.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib153.2.2.1" style="font-size:90%;">[153]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib153.4.1" style="font-size:90%;">
T.¬†Zhang, B.¬†Huang, Y.¬†Wang, Object-occluded human shape and pose estimation
from a single color image, in: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, 2020, pp. 7376‚Äì7385.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib154.2.2.1" style="font-size:90%;">[154]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib154.4.1" style="font-size:90%;">
Y.¬†Sun, Q.¬†Bao, W.¬†Liu, Y.¬†Fu, M.¬†J. Black, T.¬†Mei, Monocular, one-stage,
regression of multiple 3d people, in: Proceedings of the IEEE/CVF
international conference on computer vision, 2021, pp. 11179‚Äì11188.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib155.2.2.1" style="font-size:90%;">[155]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib155.4.1" style="font-size:90%;">
H.¬†Choi, G.¬†Moon, J.¬†Park, K.¬†M. Lee, Learning to estimate robust 3d human mesh
from in-the-wild crowded scenes, in: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2022, pp. 1475‚Äì1484.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib156.2.2.1" style="font-size:90%;">[156]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib156.4.1" style="font-size:90%;">
W.¬†Zhu, X.¬†Ma, Z.¬†Liu, L.¬†Liu, W.¬†Wu, Y.¬†Wang, Motionbert: A unified
perspective on learning human motion representations, in: Proceedings of the
IEEE/CVF International Conference on Computer Vision, 2023, pp. 15085‚Äì15099.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib157.2.2.1" style="font-size:90%;">[157]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib157.4.1" style="font-size:90%;">
R.¬†A. Guler, I.¬†Kokkinos, Holopose: Holistic 3d human reconstruction
in-the-wild, in: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2019, pp. 10884‚Äì10894.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib158.2.2.1" style="font-size:90%;">[158]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib158.4.1" style="font-size:90%;">
Y.¬†Sun, Y.¬†Ye, W.¬†Liu, W.¬†Gao, Y.¬†Fu, T.¬†Mei, Human mesh recovery from
monocular images via a skeleton-disentangled representation, in: Proceedings
of the IEEE/CVF international conference on computer vision, 2019, pp.
5349‚Äì5358.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib159.2.2.1" style="font-size:90%;">[159]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib159.4.1" style="font-size:90%;">
J.¬†Li, C.¬†Xu, Z.¬†Chen, S.¬†Bian, L.¬†Yang, C.¬†Lu, Hybrik: A hybrid
analytical-neural inverse kinematics solution for 3d human pose and shape
estimation, in: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, 2021, pp. 3383‚Äì3393.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib160.2.2.1" style="font-size:90%;">[160]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib160.4.1" style="font-size:90%;">
J.¬†Li, S.¬†Bian, Q.¬†Liu, J.¬†Tang, F.¬†Wang, C.¬†Lu, Niki: Neural inverse
kinematics with invertible neural networks for 3d human pose and shape
estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2023, pp. 12933‚Äì12942.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib161.2.2.1" style="font-size:90%;">[161]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib161.4.1" style="font-size:90%;">
G.-H. Lee, S.-W. Lee, Uncertainty-aware human mesh recovery from video by
learning part-based 3d dynamics, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2021, pp. 12375‚Äì12384.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib162.2.2.1" style="font-size:90%;">[162]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib162.4.1" style="font-size:90%;">
A.¬†Sengupta, I.¬†Budvytis, R.¬†Cipolla, Hierarchical kinematic probability
distributions for 3d human shape and pose estimation from images in the wild,
in: Proceedings of the IEEE/CVF international conference on computer vision,
2021, pp. 11219‚Äì11229.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib163.2.2.1" style="font-size:90%;">[163]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib163.4.1" style="font-size:90%;">
D.¬†Wang, S.¬†Zhang, 3d human mesh recovery with sequentially global rotation
estimation, in: Proceedings of the IEEE/CVF International Conference on
Computer Vision, 2023, pp. 14953‚Äì14962.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib164.2.2.1" style="font-size:90%;">[164]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib164.4.1" style="font-size:90%;">
N.¬†Kolotouros, G.¬†Pavlakos, M.¬†J. Black, K.¬†Daniilidis, Learning to reconstruct
3d human pose and shape via model-fitting in the loop, in: Proceedings of the
IEEE/CVF international conference on computer vision, 2019, pp. 2252‚Äì2261.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib165.2.2.1" style="font-size:90%;">[165]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib165.4.1" style="font-size:90%;">
Y.¬†Wang, K.¬†Daniilidis, Refit: Recurrent fitting network for 3d human recovery,
in: Proceedings of the IEEE/CVF International Conference on Computer Vision,
2023, pp. 14644‚Äì14654.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib166.2.2.1" style="font-size:90%;">[166]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib166.4.1" style="font-size:90%;">
W.¬†Jiang, N.¬†Kolotouros, G.¬†Pavlakos, X.¬†Zhou, K.¬†Daniilidis, Coherent
reconstruction of multiple humans from a single image, in: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, 2020, pp.
5579‚Äì5588.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib167.2.2.1" style="font-size:90%;">[167]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib167.4.1" style="font-size:90%;">
M.¬†Madadi, H.¬†Bertiche, S.¬†Escalera, Deep unsupervised 3d human body
reconstruction from a sparse set of landmarks, International Journal of
Computer Vision 129¬†(8) (2021) 2499‚Äì2512.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib168.2.2.1" style="font-size:90%;">[168]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib168.4.1" style="font-size:90%;">
S.¬†Guan, J.¬†Xu, M.¬†Z. He, Y.¬†Wang, B.¬†Ni, X.¬†Yang, Out-of-domain human mesh
reconstruction via dynamic bilevel online adaptation, IEEE Transactions on
Pattern Analysis and Machine Intelligence 45¬†(4) (2022) 5070‚Äì5086.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib169.2.2.1" style="font-size:90%;">[169]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib169.4.1" style="font-size:90%;">
B.¬†Huang, T.¬†Zhang, Y.¬†Wang, Pose2uv: Single-shot multiperson mesh recovery
with deep uv prior, IEEE Transactions on Image Processing 31 (2022)
4679‚Äì4692.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib170.2.2.1" style="font-size:90%;">[170]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib170.4.1" style="font-size:90%;">
J.¬†Li, Z.¬†Yang, X.¬†Wang, J.¬†Ma, C.¬†Zhou, Y.¬†Yang, Jotr: 3d joint contrastive
learning with transformers for occluded human mesh recovery, in: Proceedings
of the IEEE/CVF International Conference on Computer Vision, 2023, pp.
9110‚Äì9121.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib171.2.2.1" style="font-size:90%;">[171]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib171.4.1" style="font-size:90%;">
H.¬†Nam, D.¬†S. Jung, Y.¬†Oh, K.¬†M. Lee, Cyclic test-time adaptation on monocular
video for 3d human mesh reconstruction, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2023, pp. 14829‚Äì14839.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib172.2.2.1" style="font-size:90%;">[172]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib172.4.1" style="font-size:90%;">
T.¬†Alldieck, M.¬†Magnor, B.¬†L. Bhatnagar, C.¬†Theobalt, G.¬†Pons-Moll, Learning to
reconstruct people in clothing from a single rgb camera, in: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp.
1175‚Äì1186.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib173.2.2.1" style="font-size:90%;">[173]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib173.4.1" style="font-size:90%;">
B.¬†L. Bhatnagar, G.¬†Tiwari, C.¬†Theobalt, G.¬†Pons-Moll, Multi-garment net:
Learning to dress 3d people from images, in: Proceedings of the IEEE/CVF
international conference on computer vision, 2019, pp. 5420‚Äì5430.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib174.2.2.1" style="font-size:90%;">[174]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib174.4.1" style="font-size:90%;">
T.¬†Alldieck, G.¬†Pons-Moll, C.¬†Theobalt, M.¬†Magnor, Tex2shape: Detailed full
human body geometry from a single image, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2019, pp. 2293‚Äì2303.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib175.2.2.1" style="font-size:90%;">[175]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib175.4.1" style="font-size:90%;">
B.¬†Jiang, J.¬†Zhang, Y.¬†Hong, J.¬†Luo, L.¬†Liu, H.¬†Bao, Bcnet: Learning body and
cloth shape from a single image, in: Computer Vision‚ÄìECCV 2020: 16th
European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part XX
16, Springer, 2020, pp. 18‚Äì35.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib176.2.2.1" style="font-size:90%;">[176]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib176.4.1" style="font-size:90%;">
M.-P. Forte, P.¬†Kulits, C.-H.¬†P. Huang, V.¬†Choutas, D.¬†Tzionas, K.¬†J.
Kuchenbecker, M.¬†J. Black, Reconstructing signing avatars from video using
linguistic priors, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2023, pp. 12791‚Äì12801.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib177.2.2.1" style="font-size:90%;">[177]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib177.4.1" style="font-size:90%;">
B.¬†Zhang, Y.¬†Wang, X.¬†Deng, Y.¬†Zhang, P.¬†Tan, C.¬†Ma, H.¬†Wang, Interacting
two-hand 3d pose and shape reconstruction from single color image, in:
Proceedings of the IEEE/CVF International Conference on Computer Vision,
2021, pp. 11354‚Äì11363.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib178.2.2.1" style="font-size:90%;">[178]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib178.4.1" style="font-size:90%;">
Y.¬†Chen, Z.¬†Tu, D.¬†Kang, R.¬†Chen, L.¬†Bao, Z.¬†Zhang, J.¬†Yuan, Joint hand-object
3d reconstruction from a single image with cross-branch feature fusion, IEEE
Transactions on Image Processing 30 (2021) 4008‚Äì4021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib179.2.2.1" style="font-size:90%;">[179]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib179.4.1" style="font-size:90%;">
M.¬†Hassan, V.¬†Choutas, D.¬†Tzionas, M.¬†J. Black, Resolving 3d human pose
ambiguities with 3d scene constraints, in: Proceedings of the IEEE/CVF
international conference on computer vision, 2019, pp. 2282‚Äì2292.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib180.2.2.1" style="font-size:90%;">[180]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib180.4.1" style="font-size:90%;">
V.¬†Choutas, G.¬†Pavlakos, T.¬†Bolkart, D.¬†Tzionas, M.¬†J. Black, Monocular
expressive body regression through body-driven attention, in: Computer
Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28,
2020, Proceedings, Part X 16, Springer, 2020, pp. 20‚Äì40.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib181.2.2.1" style="font-size:90%;">[181]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib181.4.1" style="font-size:90%;">
Y.¬†Rong, T.¬†Shiratori, H.¬†Joo, Frankmocap: A monocular 3d whole-body pose
estimation system via regression and integration, in: Proceedings of the
IEEE/CVF International Conference on Computer Vision, 2021, pp. 1749‚Äì1759.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib182.2.2.1" style="font-size:90%;">[182]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib182.4.1" style="font-size:90%;">
Y.¬†Feng, V.¬†Choutas, T.¬†Bolkart, D.¬†Tzionas, M.¬†J. Black, Collaborative
regression of expressive bodies using moderation, in: 2021 International
Conference on 3D Vision (3DV), IEEE, 2021, pp. 792‚Äì804.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib183.2.2.1" style="font-size:90%;">[183]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib183.4.1" style="font-size:90%;">
G.¬†Moon, H.¬†Choi, K.¬†M. Lee, Accurate 3d hand pose estimation for whole-body 3d
human mesh estimation, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2022, pp. 2308‚Äì2317.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib184.2.2.1" style="font-size:90%;">[184]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib184.4.1" style="font-size:90%;">
H.¬†Zhang, Y.¬†Tian, Y.¬†Zhang, M.¬†Li, L.¬†An, Z.¬†Sun, Y.¬†Liu, Pymaf-x: Towards
well-aligned full-body model regression from monocular images, IEEE
Transactions on Pattern Analysis and Machine Intelligence (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib185.2.2.1" style="font-size:90%;">[185]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib185.4.1" style="font-size:90%;">
J.¬†Lin, A.¬†Zeng, H.¬†Wang, L.¬†Zhang, Y.¬†Li, One-stage 3d whole-body mesh
recovery with component aware transformer, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2023, pp.
21159‚Äì21168.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib186.2.2.1" style="font-size:90%;">[186]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib186.4.1" style="font-size:90%;">
J.¬†Li, S.¬†Bian, C.¬†Xu, Z.¬†Chen, L.¬†Yang, C.¬†Lu, Hybrik-x: Hybrid
analytical-neural inverse kinematics for whole-body mesh recovery, arXiv
preprint arXiv:2304.05690 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib187.2.2.1" style="font-size:90%;">[187]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib187.4.1" style="font-size:90%;">
D.¬†Smith, M.¬†Loper, X.¬†Hu, P.¬†Mavroidis, J.¬†Romero, Facsimile: Fast and
accurate scans from an image in less than a second, in: Proceedings of the
IEEE/CVF international conference on computer vision, 2019, pp. 5330‚Äì5339.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib188.2.2.1" style="font-size:90%;">[188]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib188.4.1" style="font-size:90%;">
S.¬†S. Jinka, R.¬†Chacko, A.¬†Sharma, P.¬†Narayanan, Peeledhuman: Robust shape
representation for textured 3d human body reconstruction, in: 2020
International Conference on 3D Vision (3DV), IEEE, 2020, pp. 879‚Äì888.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib189.2.2.1" style="font-size:90%;">[189]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib189.4.1" style="font-size:90%;">
Z.¬†Zhang, L.¬†Sun, Z.¬†Yang, L.¬†Chen, Y.¬†Yang, Global-correlated 3d-decoupling
transformer for clothed avatar reconstruction, arXiv preprint
arXiv:2309.13524 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib190.2.2.1" style="font-size:90%;">[190]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib190.4.1" style="font-size:90%;">
Y.¬†Xue, B.¬†L. Bhatnagar, R.¬†Marin, N.¬†Sarafianos, Y.¬†Xu, G.¬†Pons-Moll, T.¬†Tung,
Nsf: Neural surface fields for human modeling from monocular depth, in:
Proceedings of the IEEE/CVF International Conference on Computer Vision,
2023, pp. 15049‚Äì15060.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib191.2.2.1" style="font-size:90%;">[191]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib191.4.1" style="font-size:90%;">
E.¬†G√§rtner, M.¬†Andriluka, E.¬†Coumans, C.¬†Sminchisescu, Differentiable
dynamics for articulated 3d human motion reconstruction, in: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.
13190‚Äì13200.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib192.2.2.1" style="font-size:90%;">[192]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib192.4.1" style="font-size:90%;">
Z.¬†Dong, X.¬†Chen, J.¬†Yang, M.¬†J. Black, O.¬†Hilliges, A.¬†Geiger, Ag3d: Learning
to generate 3d avatars from 2d image collections, arXiv preprint
arXiv:2305.02312 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib193.2.2.1" style="font-size:90%;">[193]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib193.4.1" style="font-size:90%;">
S.¬†Saito, Z.¬†Huang, R.¬†Natsume, S.¬†Morishima, A.¬†Kanazawa, H.¬†Li, Pifu:
Pixel-aligned implicit function for high-resolution clothed human
digitization, in: Proceedings of the IEEE/CVF international conference on
computer vision, 2019, pp. 2304‚Äì2314.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib194.2.2.1" style="font-size:90%;">[194]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib194.4.1" style="font-size:90%;">
S.¬†Saito, T.¬†Simon, J.¬†Saragih, H.¬†Joo, Pifuhd: Multi-level pixel-aligned
implicit function for high-resolution 3d human digitization, in: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020,
pp. 84‚Äì93.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib195.2.2.1" style="font-size:90%;">[195]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib195.4.1" style="font-size:90%;">
Z.¬†Huang, Y.¬†Xu, C.¬†Lassner, H.¬†Li, T.¬†Tung, Arch: Animatable reconstruction of
clothed humans, in: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2020, pp. 3093‚Äì3102.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib196.2.2.1" style="font-size:90%;">[196]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib196.4.1" style="font-size:90%;">
T.¬†He, Y.¬†Xu, S.¬†Saito, S.¬†Soatto, T.¬†Tung, Arch++: Animation-ready clothed
human reconstruction revisited, in: Proceedings of the IEEE/CVF international
conference on computer vision, 2021, pp. 11046‚Äì11056.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib197.2.2.1" style="font-size:90%;">[197]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib197.4.1" style="font-size:90%;">
T.¬†Liao, X.¬†Zhang, Y.¬†Xiu, H.¬†Yi, X.¬†Liu, G.-J. Qi, Y.¬†Zhang, X.¬†Wang, X.¬†Zhu,
Z.¬†Lei, High-fidelity clothed avatar reconstruction from a single image, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2023, pp. 8662‚Äì8672.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib198.2.2.1" style="font-size:90%;">[198]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib198.4.1" style="font-size:90%;">
T.¬†He, J.¬†Collomosse, H.¬†Jin, S.¬†Soatto, Geo-pifu: Geometry and pixel aligned
implicit functions for single-view human reconstruction, Advances in Neural
Information Processing Systems 33 (2020) 9276‚Äì9287.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib199.2.2.1" style="font-size:90%;">[199]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib199.4.1" style="font-size:90%;">
S.¬†Peng, Y.¬†Zhang, Y.¬†Xu, Q.¬†Wang, Q.¬†Shuai, H.¬†Bao, X.¬†Zhou, Neural body:
Implicit neural representations with structured latent codes for novel view
synthesis of dynamic humans, in: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2021, pp. 9054‚Äì9063.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib200.2.2.1" style="font-size:90%;">[200]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib200.4.1" style="font-size:90%;">
Y.¬†Zhang, P.¬†Ji, A.¬†Wang, J.¬†Mei, A.¬†Kortylewski, A.¬†Yuille, 3d-aware neural
body fitting for occlusion robust 3d human pose estimation, in: Proceedings
of the IEEE/CVF International Conference on Computer Vision, 2023, pp.
9399‚Äì9410.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib201.2.2.1" style="font-size:90%;">[201]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib201.4.1" style="font-size:90%;">
X.¬†Gao, J.¬†Yang, J.¬†Kim, S.¬†Peng, Z.¬†Liu, X.¬†Tong, Mps-nerf: Generalizable 3d
human rendering from multiview images, IEEE Transactions on Pattern Analysis
and Machine Intelligence (2022).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib202.2.2.1" style="font-size:90%;">[202]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib202.4.1" style="font-size:90%;">
L.¬†G. Foo, J.¬†Gong, H.¬†Rahmani, J.¬†Liu, Distribution-aligned diffusion for
human mesh recovery, in: Proceedings of the IEEE/CVF International Conference
on Computer Vision, 2023, pp. 9221‚Äì9232.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib203.2.2.1" style="font-size:90%;">[203]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib203.4.1" style="font-size:90%;">
H.¬†Zhu, X.¬†Zuo, S.¬†Wang, X.¬†Cao, R.¬†Yang, Detailed human shape estimation from
a single image by hierarchical mesh deformation, in: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, 2019, pp.
4491‚Äì4500.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib204.2.2.1" style="font-size:90%;">[204]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib204.4.1" style="font-size:90%;">
B.¬†L. Bhatnagar, C.¬†Sminchisescu, C.¬†Theobalt, G.¬†Pons-Moll, Combining implicit
function learning and parametric models for 3d human reconstruction, in:
Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August
23‚Äì28, 2020, Proceedings, Part II 16, Springer, 2020, pp. 311‚Äì329.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib205.2.2.1" style="font-size:90%;">[205]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib205.4.1" style="font-size:90%;">
H.¬†Zhu, X.¬†Zuo, H.¬†Yang, S.¬†Wang, X.¬†Cao, R.¬†Yang, Detailed avatar recovery
from single image, IEEE Transactions on Pattern Analysis and Machine
Intelligence 44¬†(11) (2021) 7363‚Äì7379.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib206.2.2.1" style="font-size:90%;">[206]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib206.4.1" style="font-size:90%;">
Y.¬†Xiu, J.¬†Yang, D.¬†Tzionas, M.¬†J. Black, Icon: Implicit clothed humans
obtained from normals, in: 2022 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), IEEE, 2022, pp. 13286‚Äì13296.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib207.2.2.1" style="font-size:90%;">[207]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib207.4.1" style="font-size:90%;">
Y.¬†Xiu, J.¬†Yang, X.¬†Cao, D.¬†Tzionas, M.¬†J. Black, Econ: Explicit clothed humans
optimized via normal integration, in: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2023, pp. 512‚Äì523.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib208.2.2.1" style="font-size:90%;">[208]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib208.4.1" style="font-size:90%;">
X.¬†Zhang, J.¬†Zhang, R.¬†Chacko, H.¬†Xu, G.¬†Song, Y.¬†Yang, J.¬†Feng, Getavatar:
Generative textured meshes for animatable human avatars, in: Proceedings of
the IEEE/CVF International Conference on Computer Vision, 2023, pp.
2273‚Äì2282.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib209.2.2.1" style="font-size:90%;">[209]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib209.4.1" style="font-size:90%;">
D.¬†Svitov, D.¬†Gudkov, R.¬†Bashirov, V.¬†Lempitsky, Dinar: Diffusion inpainting of
neural textures for one-shot human avatars, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2023, pp. 7062‚Äì7072.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib210.2.2.1" style="font-size:90%;">[210]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib210.4.1" style="font-size:90%;">
X.¬†Pan, Z.¬†Yang, J.¬†Ma, C.¬†Zhou, Y.¬†Yang, Transhuman: A transformer-based human
representation for generalizable neural human rendering, in: Proceedings of
the IEEE/CVF International conference on computer vision, 2023, pp.
3544‚Äì3555.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib211.2.2.1" style="font-size:90%;">[211]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib211.4.1" style="font-size:90%;">
Y.¬†Liu, X.¬†Huang, M.¬†Qin, Q.¬†Lin, H.¬†Wang, Animatable 3d gaussian: Fast and
high-quality reconstruction of multiple human avatars, arXiv preprint
arXiv:2311.16482 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib212.2.2.1" style="font-size:90%;">[212]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib212.4.1" style="font-size:90%;">
A.¬†Vaswani, N.¬†Shazeer, N.¬†Parmar, J.¬†Uszkoreit, L.¬†Jones, A.¬†N. Gomez,
≈Å.¬†Kaiser, I.¬†Polosukhin, Attention is all you need, Advances in neural
information processing systems 30 (2017).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib213.2.2.1" style="font-size:90%;">[213]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib213.4.1" style="font-size:90%;">
J.¬†Devlin, M.-W. Chang, K.¬†Lee, K.¬†Toutanova, Bert: Pre-training of deep
bidirectional transformers for language understanding, arXiv preprint
arXiv:1810.04805 (2018).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib214.2.2.1" style="font-size:90%;">[214]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib214.4.1" style="font-size:90%;">
T.¬†Brown, B.¬†Mann, N.¬†Ryder, M.¬†Subbiah, J.¬†D. Kaplan, P.¬†Dhariwal,
A.¬†Neelakantan, P.¬†Shyam, G.¬†Sastry, A.¬†Askell, et¬†al., Language models are
few-shot learners, Advances in neural information processing systems 33
(2020) 1877‚Äì1901.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib215.2.2.1" style="font-size:90%;">[215]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib215.4.1" style="font-size:90%;">
Z.¬†Liu, Y.¬†Lin, Y.¬†Cao, H.¬†Hu, Y.¬†Wei, Z.¬†Zhang, S.¬†Lin, B.¬†Guo, Swin
transformer: Hierarchical vision transformer using shifted windows, in:
Proceedings of the IEEE/CVF international conference on computer vision,
2021, pp. 10012‚Äì10022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib216.2.2.1" style="font-size:90%;">[216]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib216.4.1" style="font-size:90%;">
Y.¬†Xu, S.-C. Zhu, T.¬†Tung, Denserac: Joint 3d pose and shape estimation by
dense render-and-compare, in: Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2019, pp. 7760‚Äì7770.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib217.2.2.1" style="font-size:90%;">[217]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib217.4.1" style="font-size:90%;">
S.¬†Guan, J.¬†Xu, Y.¬†Wang, B.¬†Ni, X.¬†Yang, Bilevel online adaptation for
out-of-domain human mesh reconstruction, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2021, pp.
10472‚Äì10481.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib218.2.2.1" style="font-size:90%;">[218]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib218.4.1" style="font-size:90%;">
H.¬†Zhang, Y.¬†Tian, X.¬†Zhou, W.¬†Ouyang, Y.¬†Liu, L.¬†Wang, Z.¬†Sun, Pymaf: 3d human
pose and shape regression with pyramidal mesh alignment feedback loop, in:
Proceedings of the IEEE/CVF International Conference on Computer Vision,
2021, pp. 11446‚Äì11456.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib219.2.2.1" style="font-size:90%;">[219]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib219.4.1" style="font-size:90%;">
B.¬†Mildenhall, P.¬†P. Srinivasan, M.¬†Tancik, J.¬†T. Barron, R.¬†Ramamoorthi,
R.¬†Ng, Nerf: Representing scenes as neural radiance fields for view
synthesis, Communications of the ACM 65¬†(1) (2021) 99‚Äì106.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib220.2.2.1" style="font-size:90%;">[220]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib220.4.1" style="font-size:90%;">
B.¬†Kerbl, G.¬†Kopanas, T.¬†Leimk√ºhler, G.¬†Drettakis, 3d gaussian splatting
for real-time radiance field rendering, ACM Transactions on Graphics 42¬†(4)
(2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib221.2.2.1" style="font-size:90%;">[221]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib221.4.1" style="font-size:90%;">
C.¬†Yan, D.¬†Qu, D.¬†Wang, D.¬†Xu, Z.¬†Wang, B.¬†Zhao, X.¬†Li, Gs-slam: Dense visual
slam with 3d gaussian splatting, arXiv preprint arXiv:2311.11700 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib222.2.2.1" style="font-size:90%;">[222]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib222.4.1" style="font-size:90%;">
X.¬†Liu, X.¬†Zhan, J.¬†Tang, Y.¬†Shan, G.¬†Zeng, D.¬†Lin, X.¬†Liu, Z.¬†Liu,
Humangaussian: Text-driven 3d human generation with gaussian splatting, arXiv
preprint arXiv:2311.17061 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib223.2.2.1" style="font-size:90%;">[223]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib223.4.1" style="font-size:90%;">
G.¬†Wu, T.¬†Yi, J.¬†Fang, L.¬†Xie, X.¬†Zhang, W.¬†Wei, W.¬†Liu, Q.¬†Tian, X.¬†Wang, 4d
gaussian splatting for real-time dynamic scene rendering, arXiv preprint
arXiv:2310.08528 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib224.2.2.1" style="font-size:90%;">[224]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib224.4.1" style="font-size:90%;">
Z.¬†Chen, F.¬†Wang, H.¬†Liu, Text-to-3d using gaussian splatting, arXiv preprint
arXiv:2309.16585 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib225.2.2.1" style="font-size:90%;">[225]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib225.4.1" style="font-size:90%;">
C.¬†Ionescu, D.¬†Papava, V.¬†Olaru, C.¬†Sminchisescu, Human3.6M: Large Scale
Datasets and Predictive Methods for 3D Human Sensing in Natural
Environments, IEEE Transactions on Pattern Analysis and Machine
Intelligence 36¬†(7) (2014) 1325‚Äì1339.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TPAMI.2013.248" style="font-size:90%;" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/TPAMI.2013.248</span></a><span class="ltx_text" id="bib.bib225.5.1" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib226.2.2.1" style="font-size:90%;">[226]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib226.4.1" style="font-size:90%;">
Y.¬†Yang, D.¬†Ramanan, Articulated human detection with flexible mixtures of
parts, IEEE Transactions on Pattern Analysis and Machine Intelligence 35¬†(12)
(2013) 2878‚Äì2890.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TPAMI.2012.261" style="font-size:90%;" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/TPAMI.2012.261</span></a><span class="ltx_text" id="bib.bib226.5.1" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib227.2.2.1" style="font-size:90%;">[227]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib227.4.1" style="font-size:90%;">
T.¬†Von¬†Marcard, R.¬†Henschel, M.¬†J. Black, B.¬†Rosenhahn, G.¬†Pons-Moll,
Recovering accurate 3d human pose in the wild using imus and a moving camera,
in: Proceedings of the European conference on computer vision (ECCV), 2018,
pp. 601‚Äì617.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib228.2.2.1" style="font-size:90%;">[228]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib228.4.1" style="font-size:90%;">
D.¬†Mehta, H.¬†Rhodin, D.¬†Casas, P.¬†Fua, O.¬†Sotnychenko, W.¬†Xu, C.¬†Theobalt,
Monocular 3d human pose estimation in the wild using improved cnn
supervision, in: 2017 international conference on 3D vision (3DV), IEEE,
2017, pp. 506‚Äì516.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib229.2.2.1" style="font-size:90%;">[229]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib229.4.1" style="font-size:90%;">
L.¬†Sigal, A.¬†O. Balan, M.¬†J. Black, Humaneva: Synchronized video and motion
capture dataset and baseline algorithm for evaluation of articulated human
motion, International journal of computer vision 87¬†(1-2) (2010) 4.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib230.2.2.1" style="font-size:90%;">[230]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib230.4.1" style="font-size:90%;">
H.¬†Joo, H.¬†Liu, L.¬†Tan, L.¬†Gui, B.¬†Nabbe, I.¬†Matthews, T.¬†Kanade, S.¬†Nobuhara,
Y.¬†Sheikh, Panoptic studio: A massively multiview system for social motion
capture, in: Proceedings of the IEEE International Conference on Computer
Vision (ICCV), 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib231.2.2.1" style="font-size:90%;">[231]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib231.4.1" style="font-size:90%;">
G.¬†Varol, J.¬†Romero, X.¬†Martin, N.¬†Mahmood, M.¬†J. Black, I.¬†Laptev, C.¬†Schmid,
Learning from synthetic humans, in: Proceedings of the IEEE conference on
computer vision and pattern recognition, 2017, pp. 109‚Äì117.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib232.2.2.1" style="font-size:90%;">[232]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib232.4.1" style="font-size:90%;">
L.¬†Muller, A.¬†A. Osman, S.¬†Tang, C.-H.¬†P. Huang, M.¬†J. Black, On self-contact
and human pose, in: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2021, pp. 9990‚Äì9999.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib233.2.2.1" style="font-size:90%;">[233]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib233.4.1" style="font-size:90%;">
N.¬†Mahmood, N.¬†Ghorbani, N.¬†F. Troje, G.¬†Pons-Moll, M.¬†J. Black, Amass: Archive
of motion capture as surface shapes, in: Proceedings of the IEEE/CVF
international conference on computer vision, 2019, pp. 5442‚Äì5451.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib234.2.2.1" style="font-size:90%;">[234]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib234.4.1" style="font-size:90%;">
R.¬†A. G√ºler, N.¬†Neverova, I.¬†Kokkinos, Densepose: Dense human pose
estimation in the wild, in: Proceedings of the IEEE conference on computer
vision and pattern recognition, 2018, pp. 7297‚Äì7306.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib235.2.2.1" style="font-size:90%;">[235]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib235.4.1" style="font-size:90%;">
C.¬†Lassner, J.¬†Romero, M.¬†Kiefel, F.¬†Bogo, M.¬†J. Black, P.¬†V. Gehler, Unite the
people: Closing the loop between 3d and 2d human representations, in:
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2017, pp. 6050‚Äì6059.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib236.2.2.1" style="font-size:90%;">[236]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib236.4.1" style="font-size:90%;">
Z.¬†Zheng, T.¬†Yu, Y.¬†Wei, Q.¬†Dai, Y.¬†Liu, Deephuman: 3d human reconstruction
from a single image, in: Proceedings of the IEEE/CVF International Conference
on Computer Vision, 2019, pp. 7739‚Äì7749.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib237.2.2.1" style="font-size:90%;">[237]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib237.4.1" style="font-size:90%;">
W.¬†Zhao, W.¬†Wang, Y.¬†Tian, Graformer: Graph-oriented transformer for 3d pose
estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2022, pp. 20438‚Äì20447.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib238.2.2.1" style="font-size:90%;">[238]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib238.4.1" style="font-size:90%;">
B.¬†X. Yu, Z.¬†Zhang, Y.¬†Liu, S.-h. Zhong, Y.¬†Liu, C.¬†W. Chen, Gla-gcn:
Global-local adaptive graph convolutional network for 3d human pose
estimation from monocular video, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2023, pp. 8818‚Äì8829.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib239.2.2.1" style="font-size:90%;">[239]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib239.4.1" style="font-size:90%;">
H.¬†Ci, M.¬†Wu, W.¬†Zhu, X.¬†Ma, H.¬†Dong, F.¬†Zhong, Y.¬†Wang, Gfpose: Learning 3d
human pose prior with gradient fields, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2023, pp. 4800‚Äì4810.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib240.2.2.1" style="font-size:90%;">[240]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib240.4.1" style="font-size:90%;">
K.¬†Lee, W.¬†Kim, S.¬†Lee, From human pose similarity metric to 3d human pose
estimator: Temporal propagating lstm networks, IEEE transactions on pattern
analysis and machine intelligence 45¬†(2) (2022) 1781‚Äì1797.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib241.3.2.1" style="font-size:90%;">[241]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib241.5.1" style="font-size:90%;">
A.¬†Zeng, X.¬†Ju, L.¬†Yang, R.¬†Gao, X.¬†Zhu, B.¬†Dai, Q.¬†Xu, Deciwatch: A simple
baseline for 10</span><math alttext="\times" class="ltx_Math" display="inline" id="bib.bib241.1.m1.1"><semantics id="bib.bib241.1.m1.1a"><mo id="bib.bib241.1.m1.1.1" mathsize="90%" xref="bib.bib241.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="bib.bib241.1.m1.1b"><times id="bib.bib241.1.m1.1.1.cmml" xref="bib.bib241.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="bib.bib241.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="bib.bib241.1.m1.1d">√ó</annotation></semantics></math><span class="ltx_text" id="bib.bib241.6.2" style="font-size:90%;"> efficient 2d and 3d pose estimation, in: European
Conference on Computer Vision, Springer, 2022, pp. 607‚Äì624.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib242.2.2.1" style="font-size:90%;">[242]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib242.4.1" style="font-size:90%;">
J.¬†Gong, L.¬†G. Foo, Z.¬†Fan, Q.¬†Ke, H.¬†Rahmani, J.¬†Liu, Diffpose: Toward more
reliable 3d pose estimation, in: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2023, pp. 13041‚Äì13051.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib243.2.2.1" style="font-size:90%;">[243]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib243.4.1" style="font-size:90%;">
K.¬†Holmquist, B.¬†Wandt, Diffpose: Multi-hypothesis human pose estimation using
diffusion models, in: Proceedings of the IEEE/CVF International Conference on
Computer Vision, 2023, pp. 15977‚Äì15987.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib244.2.2.1" style="font-size:90%;">[244]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib244.4.1" style="font-size:90%;">
X.¬†Ma, J.¬†Su, C.¬†Wang, W.¬†Zhu, Y.¬†Wang, 3d human mesh estimation from virtual
markers, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2023, pp. 534‚Äì543.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib245.2.2.1" style="font-size:90%;">[245]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib245.4.1" style="font-size:90%;">
J.¬†Kim, M.-G. Gwon, H.¬†Park, H.¬†Kwon, G.-M. Um, W.¬†Kim, Sampling is matter:
Point-guided 3d human mesh reconstruction, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2023, pp.
12880‚Äì12889.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib246.2.2.1" style="font-size:90%;">[246]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib246.4.1" style="font-size:90%;">
K.¬†Shetty, A.¬†Birkhold, S.¬†Jaganathan, N.¬†Strobel, M.¬†Kowarschik, A.¬†Maier,
B.¬†Egger, Pliks: A pseudo-linear inverse kinematic solver for 3d human body
estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2023, pp. 574‚Äì584.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib247.2.2.1" style="font-size:90%;">[247]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib247.4.1" style="font-size:90%;">
Q.¬†Fang, K.¬†Chen, Y.¬†Fan, Q.¬†Shuai, J.¬†Li, W.¬†Zhang, Learning analytical
posterior probability for human mesh recovery, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
8781‚Äì8791.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib248.2.2.1" style="font-size:90%;">[248]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib248.4.1" style="font-size:90%;">
C.¬†Zheng, X.¬†Liu, G.-J. Qi, C.¬†Chen, Potter: Pooling attention transformer for
efficient human mesh recovery, in: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2023, pp. 1611‚Äì1620.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib249.2.2.1" style="font-size:90%;">[249]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib249.4.1" style="font-size:90%;">
Q.¬†Liu, A.¬†Kortylewski, A.¬†L. Yuille, Poseexaminer: Automated testing of
out-of-distribution robustness in human pose and shape estimation, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2023, pp. 672‚Äì681.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib250.2.2.1" style="font-size:90%;">[250]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib250.4.1" style="font-size:90%;">
H.¬†Cho, Y.¬†Cho, J.¬†Ahn, J.¬†Kim, Implicit 3d human mesh recovery using
consistency with pose and shape from unseen-view, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
21148‚Äì21158.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib251">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib251.2.2.1" style="font-size:90%;">[251]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib251.4.1" style="font-size:90%;">
T.¬†Simon, H.¬†Joo, I.¬†Matthews, Y.¬†Sheikh, Hand keypoint detection in single
images using multiview bootstrapping, in: Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition, 2017, pp. 1145‚Äì1153.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib252">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib252.2.2.1" style="font-size:90%;">[252]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib252.4.1" style="font-size:90%;">
T.-Y. Lin, M.¬†Maire, S.¬†Belongie, J.¬†Hays, P.¬†Perona, D.¬†Ramanan,
P.¬†Doll√°r, C.¬†L. Zitnick, Microsoft coco: Common objects in context, in:
Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland,
September 6-12, 2014, Proceedings, Part V 13, Springer, 2014, pp. 740‚Äì755.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib253">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib253.2.2.1" style="font-size:90%;">[253]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib253.4.1" style="font-size:90%;">
K.¬†Aberman, P.¬†Li, D.¬†Lischinski, O.¬†Sorkine-Hornung, D.¬†Cohen-Or, B.¬†Chen,
Skeleton-aware networks for deep motion retargeting, ACM Transactions on
Graphics (TOG) 39¬†(4) (2020) 62‚Äì1.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib254">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib254.2.2.1" style="font-size:90%;">[254]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib254.4.1" style="font-size:90%;">
Z.¬†Yang, W.¬†Zhu, W.¬†Wu, C.¬†Qian, Q.¬†Zhou, B.¬†Zhou, C.¬†C. Loy, Transmomo:
Invariance-driven unsupervised video motion retargeting, in: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.
5306‚Äì5315.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib255">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib255.2.2.1" style="font-size:90%;">[255]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib255.4.1" style="font-size:90%;">
W.-Y. Yu, L.-M. Po, R.¬†C. Cheung, Y.¬†Zhao, Y.¬†Xue, K.¬†Li, Bidirectionally
deformable motion modulation for video-based human pose transfer, in:
Proceedings of the IEEE/CVF International Conference on Computer Vision,
2023, pp. 7502‚Äì7512.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib256">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib256.2.2.1" style="font-size:90%;">[256]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib256.4.1" style="font-size:90%;">
T.¬†L. Gomes, R.¬†Martins, J.¬†Ferreira, R.¬†Azevedo, G.¬†Torres, E.¬†R. Nascimento,
A shape-aware retargeting approach to transfer human motion and appearance in
monocular videos, International Journal of Computer Vision 129¬†(7) (2021)
2057‚Äì2075.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib257">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib257.2.2.1" style="font-size:90%;">[257]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib257.4.1" style="font-size:90%;">
W.¬†Zhu, Z.¬†Yang, Z.¬†Di, W.¬†Wu, Y.¬†Wang, C.¬†C. Loy, Mocanet: Motion retargeting
in-the-wild via canonicalization networks, in: Proceedings of the AAAI
Conference on Artificial Intelligence, Vol.¬†36, 2022, pp. 3617‚Äì3625.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib258">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib258.2.2.1" style="font-size:90%;">[258]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib258.4.1" style="font-size:90%;">
L.¬†Mo, H.¬†Li, C.¬†Zou, Y.¬†Zhang, M.¬†Yang, Y.¬†Yang, M.¬†Tan, Towards accurate
facial motion retargeting with identity-consistent and expression-exclusive
constraints, in: Proceedings of the AAAI Conference on Artificial
Intelligence, Vol.¬†36, 2022, pp. 1981‚Äì1989.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib259">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib259.2.2.1" style="font-size:90%;">[259]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib259.4.1" style="font-size:90%;">
X.¬†Chen, M.¬†Mihajlovic, S.¬†Wang, S.¬†Prokudin, S.¬†Tang, Morphable diffusion:
3d-consistent diffusion for single-image avatar creation, arXiv preprint
arXiv:2401.04728 (2024).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib260">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib260.2.2.1" style="font-size:90%;">[260]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib260.4.1" style="font-size:90%;">
Z.¬†Su, L.¬†Hu, S.¬†Lin, H.¬†Zhang, S.¬†Zhang, J.¬†Thies, Y.¬†Liu, Caphy: Capturing
physical properties for animatable human avatars, in: Proceedings of the
IEEE/CVF International Conference on Computer Vision, 2023, pp. 14150‚Äì14160.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib261">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib261.2.2.1" style="font-size:90%;">[261]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib261.4.1" style="font-size:90%;">
Z.¬†Luo, J.¬†Cao, K.¬†Kitani, W.¬†Xu, et¬†al., Perpetual humanoid control for
real-time simulated avatars, in: Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2023, pp. 10895‚Äì10904.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib262">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib262.2.2.1" style="font-size:90%;">[262]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib262.4.1" style="font-size:90%;">
H.¬†Yang, D.¬†Yan, L.¬†Zhang, Y.¬†Sun, D.¬†Li, S.¬†J. Maybank, Feedback graph
convolutional network for skeleton-based action recognition, IEEE
Transactions on Image Processing 31 (2021) 164‚Äì175.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib263">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib263.2.2.1" style="font-size:90%;">[263]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib263.4.1" style="font-size:90%;">
V.¬†Mazzia, S.¬†Angarano, F.¬†Salvetti, F.¬†Angelini, M.¬†Chiaberge, Action
transformer: A self-attention model for short-time pose-based human action
recognition, Pattern Recognition 124 (2022) 108487.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib264">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib264.2.2.1" style="font-size:90%;">[264]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib264.4.1" style="font-size:90%;">
Z.¬†Lu, H.¬†Wang, Z.¬†Chang, G.¬†Yang, H.¬†P. Shum, Hard no-box adversarial attack
on skeleton-based human action recognition with skeleton-motion-informed
gradient, in: Proceedings of the IEEE/CVF International Conference on
Computer Vision, 2023, pp. 4597‚Äì4606.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib265">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib265.2.2.1" style="font-size:90%;">[265]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib265.4.1" style="font-size:90%;">
C.¬†Bian, W.¬†Feng, L.¬†Wan, S.¬†Wang, Structural knowledge distillation for
efficient skeleton-based action recognition, IEEE Transactions on Image
Processing 30 (2021) 2963‚Äì2976.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib266">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib266.2.2.1" style="font-size:90%;">[266]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib266.4.1" style="font-size:90%;">
D.¬†C. Luvizon, D.¬†Picard, H.¬†Tabia, Multi-task deep learning for real-time 3d
human pose estimation and action recognition, IEEE transactions on pattern
analysis and machine intelligence 43¬†(8) (2020) 2752‚Äì2764.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib267">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib267.2.2.1" style="font-size:90%;">[267]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib267.4.1" style="font-size:90%;">
Q.¬†Bao, W.¬†Liu, Y.¬†Cheng, B.¬†Zhou, T.¬†Mei, Pose-guided tracking-by-detection:
Robust multi-person pose tracking, IEEE Transactions on Multimedia 23 (2020)
161‚Äì175.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib268">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib268.2.2.1" style="font-size:90%;">[268]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib268.4.1" style="font-size:90%;">
N.¬†D. Reddy, L.¬†Guigues, L.¬†Pishchulin, J.¬†Eledath, S.¬†G. Narasimhan,
Tessetrack: End-to-end learnable multi-person articulated 3d pose tracking,
in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2021, pp. 15190‚Äì15200.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib269">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib269.2.2.1" style="font-size:90%;">[269]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib269.4.1" style="font-size:90%;">
S.¬†Goel, G.¬†Pavlakos, J.¬†Rajasegaran, A.¬†Kanazawa, J.¬†Malik, Humans in 4d:
Reconstructing and tracking humans with transformers, arXiv preprint
arXiv:2305.20091 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib270">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib270.2.2.1" style="font-size:90%;">[270]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib270.4.1" style="font-size:90%;">
Y.¬†Sun, Q.¬†Bao, W.¬†Liu, T.¬†Mei, M.¬†J. Black, Trace: 5d temporal regression of
avatars with dynamic cameras in 3d environments, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
8856‚Äì8866.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib271">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib271.2.2.1" style="font-size:90%;">[271]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib271.4.1" style="font-size:90%;">
Y.¬†Dai, C.¬†Wen, H.¬†Wu, Y.¬†Guo, L.¬†Chen, C.¬†Wang, Indoor 3d human trajectory
reconstruction using surveillance camera videos and point clouds, IEEE
Transactions on Circuits and Systems for Video Technology 32¬†(4) (2021)
2482‚Äì2495.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib272">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib272.2.2.1" style="font-size:90%;">[272]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib272.4.1" style="font-size:90%;">
M.¬†Kocabas, Y.¬†Yuan, P.¬†Molchanov, Y.¬†Guo, M.¬†J. Black, O.¬†Hilliges, J.¬†Kautz,
U.¬†Iqbal, Pace: Human and camera motion estimation from in-the-wild videos,
arXiv preprint arXiv:2310.13768 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib273">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib273.2.2.1" style="font-size:90%;">[273]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib273.4.1" style="font-size:90%;">
A.¬†Habibian, D.¬†Abati, T.¬†S. Cohen, B.¬†E. Bejnordi, Skip-convolutions for
efficient video processing, in: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2021, pp. 2695‚Äì2704.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib274">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib274.2.2.1" style="font-size:90%;">[274]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib274.4.1" style="font-size:90%;">
Y.¬†Tay, M.¬†Dehghani, D.¬†Bahri, D.¬†Metzler, Efficient transformers: A survey,
ACM Computing Surveys 55¬†(6) (2022) 1‚Äì28.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib275">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib275.2.2.1" style="font-size:90%;">[275]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib275.4.1" style="font-size:90%;">
L.¬†G. Foo, J.¬†Gong, Z.¬†Fan, J.¬†Liu, System-status-aware adaptive network for
online streaming video understanding, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2023, pp.
10514‚Äì10523.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib276">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib276.2.2.1" style="font-size:90%;">[276]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib276.4.1" style="font-size:90%;">
R.¬†Anil, A.¬†M. Dai, O.¬†Firat, M.¬†Johnson, D.¬†Lepikhin, A.¬†Passos, S.¬†Shakeri,
E.¬†Taropa, P.¬†Bailey, Z.¬†Chen, et¬†al., Palm 2 technical report, arXiv
preprint arXiv:2305.10403 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib277">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib277.2.2.1" style="font-size:90%;">[277]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib277.4.1" style="font-size:90%;">
J.¬†Achiam, S.¬†Adler, S.¬†Agarwal, L.¬†Ahmad, I.¬†Akkaya, F.¬†L. Aleman, D.¬†Almeida,
J.¬†Altenschmidt, S.¬†Altman, S.¬†Anadkat, et¬†al., Gpt-4 technical report, arXiv
preprint arXiv:2303.08774 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib278">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib278.2.2.1" style="font-size:90%;">[278]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib278.4.1" style="font-size:90%;">
A.¬†Kirillov, E.¬†Mintun, N.¬†Ravi, H.¬†Mao, C.¬†Rolland, L.¬†Gustafson, T.¬†Xiao,
S.¬†Whitehead, A.¬†C. Berg, W.-Y. Lo, et¬†al., Segment anything, arXiv preprint
arXiv:2304.02643 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib279">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib279.2.2.1" style="font-size:90%;">[279]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib279.4.1" style="font-size:90%;">
J.¬†Yang, M.¬†Gao, Z.¬†Li, S.¬†Gao, F.¬†Wang, F.¬†Zheng, Track anything: Segment
anything meets videos, arXiv preprint arXiv:2304.11968 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib280">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib280.2.2.1" style="font-size:90%;">[280]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib280.4.1" style="font-size:90%;">
Y.¬†Ci, Y.¬†Wang, M.¬†Chen, S.¬†Tang, L.¬†Bai, F.¬†Zhu, R.¬†Zhao, F.¬†Yu, D.¬†Qi,
W.¬†Ouyang, Unihcp: A unified model for human-centric perceptions, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2023, pp. 17840‚Äì17852.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib281">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib281.2.2.1" style="font-size:90%;">[281]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib281.4.1" style="font-size:90%;">
Y.¬†Feng, J.¬†Lin, S.¬†K. Dwivedi, Y.¬†Sun, P.¬†Patel, M.¬†J. Black, Posegpt:
Chatting about 3d human pose, arXiv preprint arXiv:2311.18836 (2023).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib282">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib282.2.2.1" style="font-size:90%;">[282]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib282.4.1" style="font-size:90%;">
H.¬†Yi, C.-H.¬†P. Huang, D.¬†Tzionas, M.¬†Kocabas, M.¬†Hassan, S.¬†Tang, J.¬†Thies,
M.¬†J. Black, Human-aware object placement for visual environment
reconstruction, in: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2022, pp. 3959‚Äì3970.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib283">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib283.2.2.1" style="font-size:90%;">[283]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib283.4.1" style="font-size:90%;">
R.¬†Jiang, C.¬†Wang, J.¬†Zhang, M.¬†Chai, M.¬†He, D.¬†Chen, J.¬†Liao, Avatarcraft:
Transforming text into neural human avatars with parameterized shape and pose
control, in: Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2023, pp. 14371‚Äì14382.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul  3 02:07:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
