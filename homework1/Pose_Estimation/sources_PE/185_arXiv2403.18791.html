<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Object Pose Estimation via the Aggregation of Diffusion Features</title>
<!--Generated on Sat Jun  1 15:24:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.18791v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S1" title="In Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S2" title="In Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S2.SS0.SSS0.Px1" title="In 2 Related work ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title">Indirect methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S2.SS0.SSS0.Px2" title="In 2 Related work ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title">Direct methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S2.SS0.SSS0.Px3" title="In 2 Related work ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title">Template-Based Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S2.SS0.SSS0.Px4" title="In 2 Related work ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title">Unseen Object Pose Estimation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3" title="In Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS1" title="In 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Object pose estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS2" title="In 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Motivation: feature matters</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS2.SSS0.Px1" title="In 3.2 Motivation: feature matters ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title">Summarization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS3" title="In 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Diffusion features</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS3.SSS1" title="In 3.3 Diffusion features ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Diffusion process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS3.SSS2" title="In 3.3 Diffusion features ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Diffusion features aggregation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS3.SSS2.Px1" title="In 3.3.2 Diffusion features aggregation ‣ 3.3 Diffusion features ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title">Arch. (a)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS3.SSS2.Px2" title="In 3.3.2 Diffusion features aggregation ‣ 3.3 Diffusion features ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title">Arch. (b)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS3.SSS2.Px3" title="In 3.3.2 Diffusion features aggregation ‣ 3.3 Diffusion features ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title">Arch. (c)</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4" title="In Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS1" title="In 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental settings</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS1.SSS1" title="In 4.1 Experimental settings ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Implementation details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS1.SSS2" title="In 4.1 Experimental settings ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Training and test</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS1.SSS3" title="In 4.1 Experimental settings ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS1.SSS4" title="In 4.1 Experimental settings ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Evalutation metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS2" title="In 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Ablation study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS2.SSS1" title="In 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Timestep</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS2.SSS2" title="In 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Other models pre-trained at large scale</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS3" title="In 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Comparison with the state-of-the-art</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS3.SSS1" title="In 4.3 Comparison with the state-of-the-art ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>LINEMOD and Occluded-LINEMOD</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS3.SSS2" title="In 4.3 Comparison with the state-of-the-art ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>T-LESS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS4" title="In 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Visualization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS5" title="In 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Efficiency</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S5" title="In Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S5.SS0.SSS0.Px1" title="In 5 Conclusion ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_title">Acknowledgement</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Object Pose Estimation via the Aggregation of Diffusion Features</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianfu Wang<sup class="ltx_sup" id="id1.1.id1">1,2,3</sup>, Guosheng Hu<sup class="ltx_sup" id="id2.2.id2">4</sup>, Hongguang Wang<sup class="ltx_sup" id="id3.3.id3">1,2</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id4.4.id4">1</sup>State Key Laboratory of Robotics, Shenyang Institute of Automation,
Chinese Academy of Sciences
<br class="ltx_break"/><sup class="ltx_sup" id="id5.5.id5">2</sup>Institutes for Robotics and Intelligent Manufacturing,
Chinese Academy of Sciences
<br class="ltx_break"/><sup class="ltx_sup" id="id6.6.id6">3</sup>University of Chinese Academy of Sciences, Beijing, 100049, China
<br class="ltx_break"/><sup class="ltx_sup" id="id7.7.id7">4</sup>Oosto, Belfast, U.K.
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id8.8.id8" style="font-size:90%;">wangtianfu100@gmail.com, huguosheng100@gmail.com, hgwang@sia.cn</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks. However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features.
To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects.
Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation.
To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity,
greatly improving
the generalizability of object pose estimation.
Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS.
In particular, our method achieves higher accuracy than the previous best arts on <em class="ltx_emph ltx_font_italic" id="id9.id1.1">unseen</em> objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at <a class="ltx_ref ltx_href" href="https://github.com/Tianfu18/diff-feats-pose" title="">https://github.com/Tianfu18/diff-feats-pose</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="192" id="S1.F1.g1" src="x1.png" width="941"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.16.1.1" style="font-size:113%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.17.2" style="font-size:113%;">
Unseen object pose estimation of one state-of-the-art method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> and our method.
(a) We render an image using the ground truth pose of an unseen object.
(b) Template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> learns image features by fine-tuning a self-supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> pre-trained model.
(c) Our method aggregrates features of different granularity from a diffusion model to achieve better pose estimation than template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> on the unseen object. The snowflake and flame symbols represent ‘parameters frozen’ and ‘fine-tune’, respectively.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Estimating the pose of objects from images is crucial for various real-world applications, including robot manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>, augmented reality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, and many others. Thanks to the advancements in deep learning, the performance of object pose estimation has significantly improved.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The most popular methods for object pose estimation can be categorized into three approaches: indirect methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, direct methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, and template-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>.
Template-based methods involve matching the input image of an object to a set of template images with various poses rendered by 3D models to achieve the object’s pose. In this work, we focus on template-based methods due to their simplicity, increasing popularity, and promising accuracy.
</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Despite the success of the aforementioned methods, image-based object pose estimation methods have not yet become widely adopted in real-world applications. In particular, the ability to handle unseen objects without the need for retraining is crucial for quick and convenient adaptation to new scenarios. Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> have begun to tackle this challenge. The method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> introduces a novel architecture with multiple decoders to adapt to different objects.
The results indeed demonstrate the interesting generalization to unseen objects, however, these objects must share great similarity with the training objects. Methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> propose the use of local object representations for template matching. Their methods have achieved interesting results, but a noticeable performance gap remains between seen and unseen objects. For example, the accuracy of one state-of-the-art method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> is 99.1% on Seen LM dataset and 94.4% on <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">Unseen</em> LM dataset, resulting in a performance gap of approximately 4.7%.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The performance gap in the existing methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>
is empirically found to mainly result from
the inadequacy of their discriminative features.
This finding, which will be detailed in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS2" title="3.2 Motivation: feature matters ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, inspires us
to learn image representations which can generalize well to unseen objects.
On the other hand, we realize that text-to-image diffusion models have already proven their ability to generate high-quality images with robust features which can generalize well to different scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>.
This promising generalizability can be attributed to four main factors: 1) Text supervision with rich semantic content can lead to highly discriminative features. 2) Diffusion models encode the information at various timesteps, capturing a spectrum of granularity and diverse attributes. 3) Diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> are found capable of encoding 3D characteristics, including scene geometry, depth, etc. 4) Diffusion models, like Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, benefit from extensive training data, which enables them to learn discriminative features across a wide range of scenarios.
Based on these advantages, we are inspired to explore the aggregation of diffusion features to generate discriminative features for object pose estimation as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S1.F1" title="In 1 Introduction ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To achieve this, we propose three architectures, namely, Arch. (a), Arch. (b) and Arch. (c), to aggregate diffusion features into an optimal feature for object pose estimation. First, we design a vanilla aggregation network, Arch. (a), which aligns the features to the same dimension through linear mapping and then aggregates them via element-wise addition. However, the limited capability of this linear network constrains its performance. To enhance the network’s fitting capability, we introduce Arch. (b), which replaces the linear mapping with a bottleneck module consisting of three convolution layers and ReLU functions. Finally, to obtain the optimal weights for different features, we design a context-aware weight aggregation network, Arch. (c), which learns the weights based on the context. To verify the efficacy of our aggregation methods, we conducted experiments on three popular benchmarks: LINEMOD (LM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, Occlusion-LINEMOD (O-LM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, and T-LESS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. On these benchmarks, our network achieves superior results over state-of-the-art methods and significantly reduces the performance gap between seen and <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">unseen</em> objects.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, this work has the following contributions:</p>
</div>
<div class="ltx_para" id="S1.p7">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We have an in-depth analysis on the diffusion features, which exhibit great potential for modeling <em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.1">unseen</em> objects, and creatively incorporate diffusion features into object pose estimation. To our knowledge, we are the first to innovatively and systematically investigate the aggregation of diffusion features for object pose estimation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose three aggregation networks which can effectively capture different dynamics of the diffusion features, leading to the promising generalizability of object pose estimation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our approach greatly outperforms
the state-of-the-art methods on three benchmark datasets, LINEMOD (LM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, Occlusion-LINEMOD (O-LM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, and T-LESS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>.
In particular, our method performs significantly better than other methods on <em class="ltx_emph ltx_font_italic" id="S1.I1.i3.p1.1.1">unseen</em> datasets, 98.2% vs. 93.5% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> on Unseen LM, 85.9% vs. 76.3% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> on Unseen O-LM, showing the strong generalizability of our methods.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Estimating the pose of objects is a fundamental task in computer vision.
In recent years, object pose estimation has been dominated by learning-based approaches which achieve great progress.
Thus, in this section, we review those commonly employed learning-based approaches.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Indirect methods</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Many learning-based approaches are based on establishing 2D-3D correspondences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, followed by PnP and RANSAC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> to estimate the pose. These methods primarily focus on the ways of obtaining accurate 2D-3D correspondences. BB8 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> regresses the 2D coordinates of projected 3D bounding box corners. PVNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> regresses pixel-wise unit vectors pointing to 2D projections of a set of 3D keypoints. Methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> employ an encoder-decoder network to regress pixel-level dense correspondences, which are the 2D coordinates of the object surface.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Direct methods</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Direct methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> treat pose estimation as a regression task, directly outputting the object’s pose. In addition, SSD-6D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> divides the pose space into categories, transforming it into a classification problem. Some recent methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> make the indirect method’s PnP process differentiable and use the 2D-3D correspondence from the indirect approach as a surrogate task.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Template-Based Methods</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Template-based methods
involve determining the object’s pose by matching a query image of the object with one of a set of templates, which are rendered images of the 3D model in various poses. One line of methods focuses on obtaining discriminative representations. Method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> trains a network to acquire discriminative features of objects and use them for matching during testing. Building on this, method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> considers the exact pose differences between training samples to obtain more discriminative representations. AAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> proposes learning representations by using a denoising autoencoder to recover the original image from an augmented image. Another line of methods includes DeepIM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, which proposes iterative refinement by regressing a pose difference between a render of the pose hypothesis and the input image. CosyPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> is built on DeepIM, introducing improvements like a better architecture, a continuous rotation parametrization, etc.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Unseen Object Pose Estimation</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p1.1">Recently, unseen object pose estimation attracted a growing interest.
Some prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> demonstrated that template-based methods could exhibit a certain degree of generalization to unseen objects. Therefore, recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> have explored template-based methods specifically tailored for unseen objects. Due to the inherent challenges in estimating the pose of unseen objects, these studies simplify the problem by assuming that the object is already localized in 2D and only focus on estimating the 3D pose (3D orientation).
Despite the success achieved by these methods, they still exhibit noticeable performance gap between seen and unseen objects. In our research, we also concentrate on 3D pose estimation of objects, aiming at minimizing the performance gap between seen and unseen objects.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we first
formulate the task of object pose estimation (<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS1" title="3.1 Object pose estimation ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>);
We then explore the significance of features and identify the suitability of the intermediate features generated by the text-to-image diffusion model for object pose estimation (<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS2" title="3.2 Motivation: feature matters ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>). Finally, we propose feature aggregation methods that aggregate intermediate features from the diffusion model to achieve the optimal feature for object pose estimation (<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS3" title="3.3 Diffusion features ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Object pose estimation</h3>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel" id="S3.F2.2">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.F2.2.1" style="width:7.1pt;height:123.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:123.6pt;transform:translate(-58.23pt,-57.46pt) rotate(-90deg) ;">
<p class="ltx_p" id="S3.F2.2.1.1"><span class="ltx_text" id="S3.F2.2.1.1.1" style="font-size:80%;">Template     Query</span></p>
</span></div>
<figure class="ltx_figure" id="S3.F2.sf1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S3.F2.sf1.1" style="width:52.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="226" id="S3.F2.sf1.1.g1" src="x2.png" width="111"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F2.sf1.1.1"><span class="ltx_text" id="S3.F2.sf1.1.1.1" style="font-size:80%;">Image</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S3.F2.sf1.2" style="width:52.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="226" id="S3.F2.sf1.2.g1" src="x3.png" width="111"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F2.sf1.2.1"><span class="ltx_text" id="S3.F2.sf1.2.1.1" style="font-size:80%;">Template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite></span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S3.F2.sf1.3" style="width:52.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="226" id="S3.F2.sf1.3.g1" src="x4.png" width="111"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F2.sf1.3.1"><span class="ltx_text" id="S3.F2.sf1.3.1.1" style="font-size:80%;">Layer 5</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S3.F2.sf1.4" style="width:52.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="226" id="S3.F2.sf1.4.g1" src="x5.png" width="111"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F2.sf1.4.1"><span class="ltx_text" id="S3.F2.sf1.4.1.1" style="font-size:80%;">Layer 12</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf1.7.1.1" style="font-size:113%;">(a)</span> </span></figcaption>
</figure>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S3.F2.sf2.1" style="width:52.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="226" id="S3.F2.sf2.1.g1" src="x6.png" width="111"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F2.sf2.1.1"><span class="ltx_text" id="S3.F2.sf2.1.1.1" style="font-size:80%;">Image</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S3.F2.sf2.2" style="width:52.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="226" id="S3.F2.sf2.2.g1" src="x7.png" width="111"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F2.sf2.2.1"><span class="ltx_text" id="S3.F2.sf2.2.1.1" style="font-size:80%;">Template-pose
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite></span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S3.F2.sf2.3" style="width:52.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="226" id="S3.F2.sf2.3.g1" src="x8.png" width="111"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F2.sf2.3.1"><span class="ltx_text" id="S3.F2.sf2.3.1.1" style="font-size:80%;">Layer 5</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S3.F2.sf2.4" style="width:52.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="226" id="S3.F2.sf2.4.g1" src="x9.png" width="111"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F2.sf2.4.1"><span class="ltx_text" id="S3.F2.sf2.4.1.1" style="font-size:80%;">Layer 12</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf2.7.1.1" style="font-size:113%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.8.1.1" style="font-size:113%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.9.2" style="font-size:113%;">Feature visualization of LINEMOD.
For query and template images, we visualize their 3 features from template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, Layer 5 and Layer 12 of a diffusion model. These features are projected to a PCA space, and the values of top 3 principal components are assigned to RGB values respectively for visualization. The more similar the colors of two feature images in one column, the more similar in feature space. The two features in one green box are very similar.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Given an input object image, the task of object pose estimation is to predict the class of the object and estimate the rigid transformation from the object coordinate system to the camera coordinate system. Currently, the most popular methods for object pose estimation are mainly categorized into three approaches: indirect methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, direct methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, and template-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>. In this work, we focus on template-based methods due to their simplicity and promising generalizability.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.2">Template-based methods usually use 3D models to render a set of template images with the annotations of object classes and poses, which can be naturally achieved during the rendering process.
The input image <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_I</annotation></semantics></math> and these template images are mapped to the feature space via <math alttext="\Phi_{encoder}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" mathvariant="normal" xref="S3.SS1.p2.2.m2.1.1.2.cmml">Φ</mi><mrow id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.cmml">e</mi><mo id="S3.SS1.p2.2.m2.1.1.3.1" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml">n</mi><mo id="S3.SS1.p2.2.m2.1.1.3.1a" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.2.m2.1.1.3.4" xref="S3.SS1.p2.2.m2.1.1.3.4.cmml">c</mi><mo id="S3.SS1.p2.2.m2.1.1.3.1b" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.2.m2.1.1.3.5" xref="S3.SS1.p2.2.m2.1.1.3.5.cmml">o</mi><mo id="S3.SS1.p2.2.m2.1.1.3.1c" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.2.m2.1.1.3.6" xref="S3.SS1.p2.2.m2.1.1.3.6.cmml">d</mi><mo id="S3.SS1.p2.2.m2.1.1.3.1d" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.2.m2.1.1.3.7" xref="S3.SS1.p2.2.m2.1.1.3.7.cmml">e</mi><mo id="S3.SS1.p2.2.m2.1.1.3.1e" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.2.m2.1.1.3.8" xref="S3.SS1.p2.2.m2.1.1.3.8.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">Φ</ci><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><times id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3.1"></times><ci id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">𝑒</ci><ci id="S3.SS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3">𝑛</ci><ci id="S3.SS1.p2.2.m2.1.1.3.4.cmml" xref="S3.SS1.p2.2.m2.1.1.3.4">𝑐</ci><ci id="S3.SS1.p2.2.m2.1.1.3.5.cmml" xref="S3.SS1.p2.2.m2.1.1.3.5">𝑜</ci><ci id="S3.SS1.p2.2.m2.1.1.3.6.cmml" xref="S3.SS1.p2.2.m2.1.1.3.6">𝑑</ci><ci id="S3.SS1.p2.2.m2.1.1.3.7.cmml" xref="S3.SS1.p2.2.m2.1.1.3.7">𝑒</ci><ci id="S3.SS1.p2.2.m2.1.1.3.8.cmml" xref="S3.SS1.p2.2.m2.1.1.3.8">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\Phi_{encoder}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">roman_Φ start_POSTSUBSCRIPT italic_e italic_n italic_c italic_o italic_d italic_e italic_r end_POSTSUBSCRIPT</annotation></semantics></math> in order to compute their similarity:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{F}=\Phi_{encoder}\left(I\right)" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.2" xref="S3.E1.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.2.2" xref="S3.E1.m1.1.2.2.cmml">ℱ</mi><mo id="S3.E1.m1.1.2.1" xref="S3.E1.m1.1.2.1.cmml">=</mo><mrow id="S3.E1.m1.1.2.3" xref="S3.E1.m1.1.2.3.cmml"><msub id="S3.E1.m1.1.2.3.2" xref="S3.E1.m1.1.2.3.2.cmml"><mi id="S3.E1.m1.1.2.3.2.2" mathvariant="normal" xref="S3.E1.m1.1.2.3.2.2.cmml">Φ</mi><mrow id="S3.E1.m1.1.2.3.2.3" xref="S3.E1.m1.1.2.3.2.3.cmml"><mi id="S3.E1.m1.1.2.3.2.3.2" xref="S3.E1.m1.1.2.3.2.3.2.cmml">e</mi><mo id="S3.E1.m1.1.2.3.2.3.1" xref="S3.E1.m1.1.2.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.3.2.3.3" xref="S3.E1.m1.1.2.3.2.3.3.cmml">n</mi><mo id="S3.E1.m1.1.2.3.2.3.1a" xref="S3.E1.m1.1.2.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.3.2.3.4" xref="S3.E1.m1.1.2.3.2.3.4.cmml">c</mi><mo id="S3.E1.m1.1.2.3.2.3.1b" xref="S3.E1.m1.1.2.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.3.2.3.5" xref="S3.E1.m1.1.2.3.2.3.5.cmml">o</mi><mo id="S3.E1.m1.1.2.3.2.3.1c" xref="S3.E1.m1.1.2.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.3.2.3.6" xref="S3.E1.m1.1.2.3.2.3.6.cmml">d</mi><mo id="S3.E1.m1.1.2.3.2.3.1d" xref="S3.E1.m1.1.2.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.3.2.3.7" xref="S3.E1.m1.1.2.3.2.3.7.cmml">e</mi><mo id="S3.E1.m1.1.2.3.2.3.1e" xref="S3.E1.m1.1.2.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.3.2.3.8" xref="S3.E1.m1.1.2.3.2.3.8.cmml">r</mi></mrow></msub><mo id="S3.E1.m1.1.2.3.1" xref="S3.E1.m1.1.2.3.1.cmml">⁢</mo><mrow id="S3.E1.m1.1.2.3.3.2" xref="S3.E1.m1.1.2.3.cmml"><mo id="S3.E1.m1.1.2.3.3.2.1" xref="S3.E1.m1.1.2.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">I</mi><mo id="S3.E1.m1.1.2.3.3.2.2" xref="S3.E1.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.2.cmml" xref="S3.E1.m1.1.2"><eq id="S3.E1.m1.1.2.1.cmml" xref="S3.E1.m1.1.2.1"></eq><ci id="S3.E1.m1.1.2.2.cmml" xref="S3.E1.m1.1.2.2">ℱ</ci><apply id="S3.E1.m1.1.2.3.cmml" xref="S3.E1.m1.1.2.3"><times id="S3.E1.m1.1.2.3.1.cmml" xref="S3.E1.m1.1.2.3.1"></times><apply id="S3.E1.m1.1.2.3.2.cmml" xref="S3.E1.m1.1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.2.1.cmml" xref="S3.E1.m1.1.2.3.2">subscript</csymbol><ci id="S3.E1.m1.1.2.3.2.2.cmml" xref="S3.E1.m1.1.2.3.2.2">Φ</ci><apply id="S3.E1.m1.1.2.3.2.3.cmml" xref="S3.E1.m1.1.2.3.2.3"><times id="S3.E1.m1.1.2.3.2.3.1.cmml" xref="S3.E1.m1.1.2.3.2.3.1"></times><ci id="S3.E1.m1.1.2.3.2.3.2.cmml" xref="S3.E1.m1.1.2.3.2.3.2">𝑒</ci><ci id="S3.E1.m1.1.2.3.2.3.3.cmml" xref="S3.E1.m1.1.2.3.2.3.3">𝑛</ci><ci id="S3.E1.m1.1.2.3.2.3.4.cmml" xref="S3.E1.m1.1.2.3.2.3.4">𝑐</ci><ci id="S3.E1.m1.1.2.3.2.3.5.cmml" xref="S3.E1.m1.1.2.3.2.3.5">𝑜</ci><ci id="S3.E1.m1.1.2.3.2.3.6.cmml" xref="S3.E1.m1.1.2.3.2.3.6">𝑑</ci><ci id="S3.E1.m1.1.2.3.2.3.7.cmml" xref="S3.E1.m1.1.2.3.2.3.7">𝑒</ci><ci id="S3.E1.m1.1.2.3.2.3.8.cmml" xref="S3.E1.m1.1.2.3.2.3.8">𝑟</ci></apply></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{F}=\Phi_{encoder}\left(I\right)</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">caligraphic_F = roman_Φ start_POSTSUBSCRIPT italic_e italic_n italic_c italic_o italic_d italic_e italic_r end_POSTSUBSCRIPT ( italic_I )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.4">The class <math alttext="\hat{c}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m1.1"><semantics id="S3.SS1.p2.3.m1.1a"><mover accent="true" id="S3.SS1.p2.3.m1.1.1" xref="S3.SS1.p2.3.m1.1.1.cmml"><mi id="S3.SS1.p2.3.m1.1.1.2" xref="S3.SS1.p2.3.m1.1.1.2.cmml">c</mi><mo id="S3.SS1.p2.3.m1.1.1.1" xref="S3.SS1.p2.3.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m1.1b"><apply id="S3.SS1.p2.3.m1.1.1.cmml" xref="S3.SS1.p2.3.m1.1.1"><ci id="S3.SS1.p2.3.m1.1.1.1.cmml" xref="S3.SS1.p2.3.m1.1.1.1">^</ci><ci id="S3.SS1.p2.3.m1.1.1.2.cmml" xref="S3.SS1.p2.3.m1.1.1.2">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m1.1c">\hat{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m1.1d">over^ start_ARG italic_c end_ARG</annotation></semantics></math> and the pose <math alttext="\hat{P}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m2.1"><semantics id="S3.SS1.p2.4.m2.1a"><mover accent="true" id="S3.SS1.p2.4.m2.1.1" xref="S3.SS1.p2.4.m2.1.1.cmml"><mi id="S3.SS1.p2.4.m2.1.1.2" xref="S3.SS1.p2.4.m2.1.1.2.cmml">P</mi><mo id="S3.SS1.p2.4.m2.1.1.1" xref="S3.SS1.p2.4.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m2.1b"><apply id="S3.SS1.p2.4.m2.1.1.cmml" xref="S3.SS1.p2.4.m2.1.1"><ci id="S3.SS1.p2.4.m2.1.1.1.cmml" xref="S3.SS1.p2.4.m2.1.1.1">^</ci><ci id="S3.SS1.p2.4.m2.1.1.2.cmml" xref="S3.SS1.p2.4.m2.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m2.1c">\hat{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m2.1d">over^ start_ARG italic_P end_ARG</annotation></semantics></math> of the most similar template image is then assigned to the given real image to achive the object pose estimation.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Clearly, we can observe that acquiring an effective feature <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">caligraphic_F</annotation></semantics></math> through <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.E1" title="In 3.1 Object pose estimation ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">1</span></a> is pivotal for computing the similarity between one real and template image, therefore, is important for the successful estimation of object pose.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Motivation: feature matters</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In recent years, object pose estimation has achieved significant success, e.g. template-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, on seen objects. However, these methods tend to struggle when dealing with <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.1">unseen</em> objects. To address this issue, recent methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> propose to encode images into local features, resulting in interesting performance for unseen objects. In spite of the considerable progress, their methods still exhibit a noticeable performance gap between seen and unseen objects.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Despite the advancements in feature engineering, generalizing these improvements to <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.1">unseen</em> objects remains a significant challenge.
<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.F2" title="In 3.1 Object pose estimation ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> illustrates this challenge by selecting
two images (query) from the LINEMOD dataset. One image contains a <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.2">seen</em> object, the lamp, while the other showcases an <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.3">unseen</em> cat. Subsequently, we render the corresponding template images (e.g. the leftmost one in the 2nd row) using the same pose as the query.
We then visualize the features of query and template in the feature space of the state-of-the-art method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. We project these features to a PCA space and assign RGB channels for visualization.
The more similar color channels indicate more similar features. Remarkably, for the <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.4">seen</em> object lamp, the features of the query and template exhibit significant similarity. However, for the <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.5">unseen</em> object cat, notable differences emerge. This discovery underscores the primary focus of this paper: the pursuit of a generalized object features to accurately estimate the pose of unseen objects.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">To obtain the discriminative features with strong generalization capability, we realize the recent text-to-image diffusion models have demonstrated strong features to generate high-quality images. As discussed in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S1" title="1 Introduction ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, a number of recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> have demonstrated that diffusion features generalize well to different scenarios.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Since image features are important for object pose estimation and the diffusion features can potentially offer discriminative features, we are inspired to investigate the feasibility of using diffusion features for object pose estimation. To verify this assumption, we employ a well-known diffusion model, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, for this investigation.
When an image is fed into Stable Diffusion, it generates a set of intermediate features from a UNet. As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.F2" title="In 3.1 Object pose estimation ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we visualize the intermediate features from both Layer 5 and Layer 12 of the UNet at timestep <math alttext="t=0" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">t</mi><mo id="S3.SS2.p4.1.m1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><eq id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1"></eq><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">𝑡</ci><cn id="S3.SS2.p4.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">t=0</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_t = 0</annotation></semantics></math>.
Interestingly, the features from Layer 5 can well measure the similarity of query and template for the object lamp; in contrast, Layer 12 is the best for the object cat.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Summarization</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">(1) The features of the state-of-the-art method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> cannot well measure the similarity of unseen objects; (2) The features of Stable Diffusion show great potential to model <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.1">unseen</em> objects. For example, we do not use the lamp and cat images from LINEMOD dataset to finetune the model, however, Stable Diffusion performs quite well; (3) The features of Stable Diffusion from different layers can capture different dynamics, e.g. Layer 5 and 12 are the best for the lamp and cat respectively. It inspires us to investigate the aggregation of features from different layers to achieve strong generalization capability on unseen objects.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel" id="S3.F3.2">
<figure class="ltx_figure ltx_align_center" id="S3.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="221" id="S3.F3.sf1.g1" src="x10.png" width="286"/>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf1.3.1.1" style="font-size:113%;">(a)</span> </span></figcaption>
</figure><svg class="ltx_picture ltx_centering" height="158.27" id="S3.F3.pic1" overflow="visible" version="1.1" width="1"><g color="#808080" fill="#808080" stroke="#808080" stroke-dasharray="3.0pt,2.0pt" stroke-dashoffset="0.0pt" stroke-width="0.56906pt" transform="translate(0,158.27) matrix(1 0 0 -1 0 0) translate(0.39,0) translate(0,79.13)"><path d="M 0 -78.74 L 0 78.74" style="fill:none"></path></g></svg>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel" id="S3.F3.3">
<figure class="ltx_figure ltx_align_center" id="S3.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="222" id="S3.F3.sf2.g1" src="x11.png" width="285"/>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf2.3.1.1" style="font-size:113%;">(b)</span> </span></figcaption>
</figure><svg class="ltx_picture ltx_centering" height="158.27" id="S3.F3.pic2" overflow="visible" version="1.1" width="1"><g color="#808080" fill="#808080" stroke="#808080" stroke-dasharray="3.0pt,2.0pt" stroke-dashoffset="0.0pt" stroke-width="0.56906pt" transform="translate(0,158.27) matrix(1 0 0 -1 0 0) translate(0.39,0) translate(0,79.13)"><path d="M 0 -78.74 L 0 78.74" style="fill:none"></path></g></svg>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="223" id="S3.F3.sf3.g1" src="x12.png" width="285"/>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf3.3.1.1" style="font-size:113%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.6.1.1" style="font-size:113%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.7.2" style="font-size:113%;">Diffusion aggregation methods. Arch. (a) is a vanilla aggregation, Arch. (b) is nonlinear aggregation, and Arch. (c) is context-aware weight aggregation.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Diffusion features</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In this section, we first introduce the diffusion process in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS3.SSS1" title="3.3.1 Diffusion process ‣ 3.3 Diffusion features ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3.1</span></a>, based on which we propose the solutions of diffusion feature aggregations in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS3.SSS2" title="3.3.2 Diffusion features aggregation ‣ 3.3 Diffusion features ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3.2</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Diffusion process</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.15">Text-to-image diffusion models involve both forward and reverse processes. One widely used sampling procedure for diffusion models is DDIM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, as defined:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon_{t},\text{ where }%
\epsilon_{t}\sim\mathcal{N}(0,1)" class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.3.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><mi id="S3.E2.m1.3.3.1.1.2.2" xref="S3.E2.m1.3.3.1.1.2.2.cmml">x</mi><mi id="S3.E2.m1.3.3.1.1.2.3" xref="S3.E2.m1.3.3.1.1.2.3.cmml">t</mi></msub><mo id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml"><mrow id="S3.E2.m1.3.3.1.1.3.2" xref="S3.E2.m1.3.3.1.1.3.2.cmml"><msqrt id="S3.E2.m1.3.3.1.1.3.2.2" xref="S3.E2.m1.3.3.1.1.3.2.2.cmml"><msub id="S3.E2.m1.3.3.1.1.3.2.2.2" xref="S3.E2.m1.3.3.1.1.3.2.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.3.2.2.2.2" xref="S3.E2.m1.3.3.1.1.3.2.2.2.2.cmml">α</mi><mi id="S3.E2.m1.3.3.1.1.3.2.2.2.3" xref="S3.E2.m1.3.3.1.1.3.2.2.2.3.cmml">t</mi></msub></msqrt><mo id="S3.E2.m1.3.3.1.1.3.2.1" xref="S3.E2.m1.3.3.1.1.3.2.1.cmml">⁢</mo><msub id="S3.E2.m1.3.3.1.1.3.2.3" xref="S3.E2.m1.3.3.1.1.3.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.2.3.2" xref="S3.E2.m1.3.3.1.1.3.2.3.2.cmml">x</mi><mn id="S3.E2.m1.3.3.1.1.3.2.3.3" xref="S3.E2.m1.3.3.1.1.3.2.3.3.cmml">0</mn></msub></mrow><mo id="S3.E2.m1.3.3.1.1.3.1" xref="S3.E2.m1.3.3.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.3.3.1.1.3.3" xref="S3.E2.m1.3.3.1.1.3.3.cmml"><msqrt id="S3.E2.m1.3.3.1.1.3.3.2" xref="S3.E2.m1.3.3.1.1.3.3.2.cmml"><mrow id="S3.E2.m1.3.3.1.1.3.3.2.2" xref="S3.E2.m1.3.3.1.1.3.3.2.2.cmml"><mn id="S3.E2.m1.3.3.1.1.3.3.2.2.2" xref="S3.E2.m1.3.3.1.1.3.3.2.2.2.cmml">1</mn><mo id="S3.E2.m1.3.3.1.1.3.3.2.2.1" xref="S3.E2.m1.3.3.1.1.3.3.2.2.1.cmml">−</mo><msub id="S3.E2.m1.3.3.1.1.3.3.2.2.3" xref="S3.E2.m1.3.3.1.1.3.3.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.3.2.2.3.2" xref="S3.E2.m1.3.3.1.1.3.3.2.2.3.2.cmml">α</mi><mi id="S3.E2.m1.3.3.1.1.3.3.2.2.3.3" xref="S3.E2.m1.3.3.1.1.3.3.2.2.3.3.cmml">t</mi></msub></mrow></msqrt><mo id="S3.E2.m1.3.3.1.1.3.3.1" xref="S3.E2.m1.3.3.1.1.3.3.1.cmml">⁢</mo><msub id="S3.E2.m1.3.3.1.1.3.3.3" xref="S3.E2.m1.3.3.1.1.3.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.3.3.2" xref="S3.E2.m1.3.3.1.1.3.3.3.2.cmml">ϵ</mi><mi id="S3.E2.m1.3.3.1.1.3.3.3.3" xref="S3.E2.m1.3.3.1.1.3.3.3.3.cmml">t</mi></msub></mrow></mrow></mrow><mo id="S3.E2.m1.4.4.2.3" xref="S3.E2.m1.4.4.3a.cmml">,</mo><mrow id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.2.cmml"><mrow id="S3.E2.m1.4.4.2.2.2" xref="S3.E2.m1.4.4.2.2.2.cmml"><mtext id="S3.E2.m1.4.4.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2a.cmml"> where </mtext><mo id="S3.E2.m1.4.4.2.2.2.1" xref="S3.E2.m1.4.4.2.2.2.1.cmml">⁢</mo><msub id="S3.E2.m1.4.4.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.3.cmml"><mi id="S3.E2.m1.4.4.2.2.2.3.2" xref="S3.E2.m1.4.4.2.2.2.3.2.cmml">ϵ</mi><mi id="S3.E2.m1.4.4.2.2.2.3.3" xref="S3.E2.m1.4.4.2.2.2.3.3.cmml">t</mi></msub></mrow><mo id="S3.E2.m1.4.4.2.2.1" xref="S3.E2.m1.4.4.2.2.1.cmml">∼</mo><mrow id="S3.E2.m1.4.4.2.2.3" xref="S3.E2.m1.4.4.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.2.2.3.2" xref="S3.E2.m1.4.4.2.2.3.2.cmml">𝒩</mi><mo id="S3.E2.m1.4.4.2.2.3.1" xref="S3.E2.m1.4.4.2.2.3.1.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.2.2.3.3.2" xref="S3.E2.m1.4.4.2.2.3.3.1.cmml"><mo id="S3.E2.m1.4.4.2.2.3.3.2.1" stretchy="false" xref="S3.E2.m1.4.4.2.2.3.3.1.cmml">(</mo><mn id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">0</mn><mo id="S3.E2.m1.4.4.2.2.3.3.2.2" xref="S3.E2.m1.4.4.2.2.3.3.1.cmml">,</mo><mn id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">1</mn><mo id="S3.E2.m1.4.4.2.2.3.3.2.3" stretchy="false" xref="S3.E2.m1.4.4.2.2.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.3a.cmml" xref="S3.E2.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1"><eq id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"></eq><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2">𝑥</ci><ci id="S3.E2.m1.3.3.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3">𝑡</ci></apply><apply id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3"><plus id="S3.E2.m1.3.3.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1"></plus><apply id="S3.E2.m1.3.3.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2"><times id="S3.E2.m1.3.3.1.1.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.2.1"></times><apply id="S3.E2.m1.3.3.1.1.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2"><root id="S3.E2.m1.3.3.1.1.3.2.2a.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2"></root><apply id="S3.E2.m1.3.3.1.1.3.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2.2.2">𝛼</ci><ci id="S3.E2.m1.3.3.1.1.3.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2.2.3">𝑡</ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.3.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3.2">𝑥</ci><cn id="S3.E2.m1.3.3.1.1.3.2.3.3.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.3.2.3.3">0</cn></apply></apply><apply id="S3.E2.m1.3.3.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3"><times id="S3.E2.m1.3.3.1.1.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1"></times><apply id="S3.E2.m1.3.3.1.1.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2"><root id="S3.E2.m1.3.3.1.1.3.3.2a.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2"></root><apply id="S3.E2.m1.3.3.1.1.3.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2"><minus id="S3.E2.m1.3.3.1.1.3.3.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2.1"></minus><cn id="S3.E2.m1.3.3.1.1.3.3.2.2.2.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.3.3.2.2.2">1</cn><apply id="S3.E2.m1.3.3.1.1.3.3.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.3.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.3.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2.3.2">𝛼</ci><ci id="S3.E2.m1.3.3.1.1.3.3.2.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2.3.3">𝑡</ci></apply></apply></apply><apply id="S3.E2.m1.3.3.1.1.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.3.2">italic-ϵ</ci><ci id="S3.E2.m1.3.3.1.1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S3.E2.m1.4.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2"><csymbol cd="latexml" id="S3.E2.m1.4.4.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.1">similar-to</csymbol><apply id="S3.E2.m1.4.4.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2"><times id="S3.E2.m1.4.4.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.1"></times><ci id="S3.E2.m1.4.4.2.2.2.2a.cmml" xref="S3.E2.m1.4.4.2.2.2.2"><mtext id="S3.E2.m1.4.4.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2"> where </mtext></ci><apply id="S3.E2.m1.4.4.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.2.2.2.3">subscript</csymbol><ci id="S3.E2.m1.4.4.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.2.2.2.3.2">italic-ϵ</ci><ci id="S3.E2.m1.4.4.2.2.2.3.3.cmml" xref="S3.E2.m1.4.4.2.2.2.3.3">𝑡</ci></apply></apply><apply id="S3.E2.m1.4.4.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.3"><times id="S3.E2.m1.4.4.2.2.3.1.cmml" xref="S3.E2.m1.4.4.2.2.3.1"></times><ci id="S3.E2.m1.4.4.2.2.3.2.cmml" xref="S3.E2.m1.4.4.2.2.3.2">𝒩</ci><interval closure="open" id="S3.E2.m1.4.4.2.2.3.3.1.cmml" xref="S3.E2.m1.4.4.2.2.3.3.2"><cn id="S3.E2.m1.1.1.cmml" type="integer" xref="S3.E2.m1.1.1">0</cn><cn id="S3.E2.m1.2.2.cmml" type="integer" xref="S3.E2.m1.2.2">1</cn></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">x_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon_{t},\text{ where }%
\epsilon_{t}\sim\mathcal{N}(0,1)</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = square-root start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + square-root start_ARG 1 - italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG italic_ϵ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , where italic_ϵ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∼ caligraphic_N ( 0 , 1 )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p1.14">where <math alttext="x_{0}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.1.m1.1"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><msub id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS1.p1.1.m1.1.1.2" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml">x</mi><mn id="S3.SS3.SSS1.p1.1.m1.1.1.3" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><apply id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.2">𝑥</ci><cn id="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.1.m1.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> denotes the initial clean sample, <math alttext="\alpha_{t}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.2.m2.1"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><msub id="S3.SS3.SSS1.p1.2.m2.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p1.2.m2.1.1.2" xref="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml">α</mi><mi id="S3.SS3.SSS1.p1.2.m2.1.1.3" xref="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><apply id="S3.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.2">𝛼</ci><ci id="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">\alpha_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.2.m2.1d">italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> denotes the pre-defined noise schedule, and <math alttext="\epsilon_{t}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.3.m3.1"><semantics id="S3.SS3.SSS1.p1.3.m3.1a"><msub id="S3.SS3.SSS1.p1.3.m3.1.1" xref="S3.SS3.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS1.p1.3.m3.1.1.2" xref="S3.SS3.SSS1.p1.3.m3.1.1.2.cmml">ϵ</mi><mi id="S3.SS3.SSS1.p1.3.m3.1.1.3" xref="S3.SS3.SSS1.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.3.m3.1b"><apply id="S3.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.2">italic-ϵ</ci><ci id="S3.SS3.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.3.m3.1c">\epsilon_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.3.m3.1d">italic_ϵ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> denotes the randomly generated noise. During the forward process, Gaussian noise is gradually added to the clean sample over <math alttext="T" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.4.m4.1"><semantics id="S3.SS3.SSS1.p1.4.m4.1a"><mi id="S3.SS3.SSS1.p1.4.m4.1.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.4.m4.1b"><ci id="S3.SS3.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.4.m4.1d">italic_T</annotation></semantics></math> steps, resulting in a sequence of noisy samples: <math alttext="x_{1},x_{2},\ldots,x_{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.5.m5.4"><semantics id="S3.SS3.SSS1.p1.5.m5.4a"><mrow id="S3.SS3.SSS1.p1.5.m5.4.4.3" xref="S3.SS3.SSS1.p1.5.m5.4.4.4.cmml"><msub id="S3.SS3.SSS1.p1.5.m5.2.2.1.1" xref="S3.SS3.SSS1.p1.5.m5.2.2.1.1.cmml"><mi id="S3.SS3.SSS1.p1.5.m5.2.2.1.1.2" xref="S3.SS3.SSS1.p1.5.m5.2.2.1.1.2.cmml">x</mi><mn id="S3.SS3.SSS1.p1.5.m5.2.2.1.1.3" xref="S3.SS3.SSS1.p1.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS1.p1.5.m5.4.4.3.4" xref="S3.SS3.SSS1.p1.5.m5.4.4.4.cmml">,</mo><msub id="S3.SS3.SSS1.p1.5.m5.3.3.2.2" xref="S3.SS3.SSS1.p1.5.m5.3.3.2.2.cmml"><mi id="S3.SS3.SSS1.p1.5.m5.3.3.2.2.2" xref="S3.SS3.SSS1.p1.5.m5.3.3.2.2.2.cmml">x</mi><mn id="S3.SS3.SSS1.p1.5.m5.3.3.2.2.3" xref="S3.SS3.SSS1.p1.5.m5.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.SSS1.p1.5.m5.4.4.3.5" xref="S3.SS3.SSS1.p1.5.m5.4.4.4.cmml">,</mo><mi id="S3.SS3.SSS1.p1.5.m5.1.1" mathvariant="normal" xref="S3.SS3.SSS1.p1.5.m5.1.1.cmml">…</mi><mo id="S3.SS3.SSS1.p1.5.m5.4.4.3.6" xref="S3.SS3.SSS1.p1.5.m5.4.4.4.cmml">,</mo><msub id="S3.SS3.SSS1.p1.5.m5.4.4.3.3" xref="S3.SS3.SSS1.p1.5.m5.4.4.3.3.cmml"><mi id="S3.SS3.SSS1.p1.5.m5.4.4.3.3.2" xref="S3.SS3.SSS1.p1.5.m5.4.4.3.3.2.cmml">x</mi><mi id="S3.SS3.SSS1.p1.5.m5.4.4.3.3.3" xref="S3.SS3.SSS1.p1.5.m5.4.4.3.3.3.cmml">T</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.5.m5.4b"><list id="S3.SS3.SSS1.p1.5.m5.4.4.4.cmml" xref="S3.SS3.SSS1.p1.5.m5.4.4.3"><apply id="S3.SS3.SSS1.p1.5.m5.2.2.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.2.2.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.5.m5.2.2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.5.m5.2.2.1.1.2">𝑥</ci><cn id="S3.SS3.SSS1.p1.5.m5.2.2.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.5.m5.2.2.1.1.3">1</cn></apply><apply id="S3.SS3.SSS1.p1.5.m5.3.3.2.2.cmml" xref="S3.SS3.SSS1.p1.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.5.m5.3.3.2.2.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.3.3.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p1.5.m5.3.3.2.2.2.cmml" xref="S3.SS3.SSS1.p1.5.m5.3.3.2.2.2">𝑥</ci><cn id="S3.SS3.SSS1.p1.5.m5.3.3.2.2.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.5.m5.3.3.2.2.3">2</cn></apply><ci id="S3.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1">…</ci><apply id="S3.SS3.SSS1.p1.5.m5.4.4.3.3.cmml" xref="S3.SS3.SSS1.p1.5.m5.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.5.m5.4.4.3.3.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.4.4.3.3">subscript</csymbol><ci id="S3.SS3.SSS1.p1.5.m5.4.4.3.3.2.cmml" xref="S3.SS3.SSS1.p1.5.m5.4.4.3.3.2">𝑥</ci><ci id="S3.SS3.SSS1.p1.5.m5.4.4.3.3.3.cmml" xref="S3.SS3.SSS1.p1.5.m5.4.4.3.3.3">𝑇</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.5.m5.4c">x_{1},x_{2},\ldots,x_{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.5.m5.4d">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math>. As <math alttext="T" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.6.m6.1"><semantics id="S3.SS3.SSS1.p1.6.m6.1a"><mi id="S3.SS3.SSS1.p1.6.m6.1.1" xref="S3.SS3.SSS1.p1.6.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.6.m6.1b"><ci id="S3.SS3.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS1.p1.6.m6.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.6.m6.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.6.m6.1d">italic_T</annotation></semantics></math> approaches infinity, <math alttext="x_{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.7.m7.1"><semantics id="S3.SS3.SSS1.p1.7.m7.1a"><msub id="S3.SS3.SSS1.p1.7.m7.1.1" xref="S3.SS3.SSS1.p1.7.m7.1.1.cmml"><mi id="S3.SS3.SSS1.p1.7.m7.1.1.2" xref="S3.SS3.SSS1.p1.7.m7.1.1.2.cmml">x</mi><mi id="S3.SS3.SSS1.p1.7.m7.1.1.3" xref="S3.SS3.SSS1.p1.7.m7.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.7.m7.1b"><apply id="S3.SS3.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS3.SSS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.7.m7.1.1.1.cmml" xref="S3.SS3.SSS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.7.m7.1.1.2.cmml" xref="S3.SS3.SSS1.p1.7.m7.1.1.2">𝑥</ci><ci id="S3.SS3.SSS1.p1.7.m7.1.1.3.cmml" xref="S3.SS3.SSS1.p1.7.m7.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.7.m7.1c">x_{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.7.m7.1d">italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> converges to an isotropic Gaussian distribution. In the reverse process, the diffusion model is sampled by iteratively denoising <math alttext="x_{T}\sim\mathcal{N}(0,1)" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.8.m8.2"><semantics id="S3.SS3.SSS1.p1.8.m8.2a"><mrow id="S3.SS3.SSS1.p1.8.m8.2.3" xref="S3.SS3.SSS1.p1.8.m8.2.3.cmml"><msub id="S3.SS3.SSS1.p1.8.m8.2.3.2" xref="S3.SS3.SSS1.p1.8.m8.2.3.2.cmml"><mi id="S3.SS3.SSS1.p1.8.m8.2.3.2.2" xref="S3.SS3.SSS1.p1.8.m8.2.3.2.2.cmml">x</mi><mi id="S3.SS3.SSS1.p1.8.m8.2.3.2.3" xref="S3.SS3.SSS1.p1.8.m8.2.3.2.3.cmml">T</mi></msub><mo id="S3.SS3.SSS1.p1.8.m8.2.3.1" xref="S3.SS3.SSS1.p1.8.m8.2.3.1.cmml">∼</mo><mrow id="S3.SS3.SSS1.p1.8.m8.2.3.3" xref="S3.SS3.SSS1.p1.8.m8.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p1.8.m8.2.3.3.2" xref="S3.SS3.SSS1.p1.8.m8.2.3.3.2.cmml">𝒩</mi><mo id="S3.SS3.SSS1.p1.8.m8.2.3.3.1" xref="S3.SS3.SSS1.p1.8.m8.2.3.3.1.cmml">⁢</mo><mrow id="S3.SS3.SSS1.p1.8.m8.2.3.3.3.2" xref="S3.SS3.SSS1.p1.8.m8.2.3.3.3.1.cmml"><mo id="S3.SS3.SSS1.p1.8.m8.2.3.3.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p1.8.m8.2.3.3.3.1.cmml">(</mo><mn id="S3.SS3.SSS1.p1.8.m8.1.1" xref="S3.SS3.SSS1.p1.8.m8.1.1.cmml">0</mn><mo id="S3.SS3.SSS1.p1.8.m8.2.3.3.3.2.2" xref="S3.SS3.SSS1.p1.8.m8.2.3.3.3.1.cmml">,</mo><mn id="S3.SS3.SSS1.p1.8.m8.2.2" xref="S3.SS3.SSS1.p1.8.m8.2.2.cmml">1</mn><mo id="S3.SS3.SSS1.p1.8.m8.2.3.3.3.2.3" stretchy="false" xref="S3.SS3.SSS1.p1.8.m8.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.8.m8.2b"><apply id="S3.SS3.SSS1.p1.8.m8.2.3.cmml" xref="S3.SS3.SSS1.p1.8.m8.2.3"><csymbol cd="latexml" id="S3.SS3.SSS1.p1.8.m8.2.3.1.cmml" xref="S3.SS3.SSS1.p1.8.m8.2.3.1">similar-to</csymbol><apply id="S3.SS3.SSS1.p1.8.m8.2.3.2.cmml" xref="S3.SS3.SSS1.p1.8.m8.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.8.m8.2.3.2.1.cmml" xref="S3.SS3.SSS1.p1.8.m8.2.3.2">subscript</csymbol><ci id="S3.SS3.SSS1.p1.8.m8.2.3.2.2.cmml" xref="S3.SS3.SSS1.p1.8.m8.2.3.2.2">𝑥</ci><ci id="S3.SS3.SSS1.p1.8.m8.2.3.2.3.cmml" xref="S3.SS3.SSS1.p1.8.m8.2.3.2.3">𝑇</ci></apply><apply id="S3.SS3.SSS1.p1.8.m8.2.3.3.cmml" xref="S3.SS3.SSS1.p1.8.m8.2.3.3"><times id="S3.SS3.SSS1.p1.8.m8.2.3.3.1.cmml" xref="S3.SS3.SSS1.p1.8.m8.2.3.3.1"></times><ci id="S3.SS3.SSS1.p1.8.m8.2.3.3.2.cmml" xref="S3.SS3.SSS1.p1.8.m8.2.3.3.2">𝒩</ci><interval closure="open" id="S3.SS3.SSS1.p1.8.m8.2.3.3.3.1.cmml" xref="S3.SS3.SSS1.p1.8.m8.2.3.3.3.2"><cn id="S3.SS3.SSS1.p1.8.m8.1.1.cmml" type="integer" xref="S3.SS3.SSS1.p1.8.m8.1.1">0</cn><cn id="S3.SS3.SSS1.p1.8.m8.2.2.cmml" type="integer" xref="S3.SS3.SSS1.p1.8.m8.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.8.m8.2c">x_{T}\sim\mathcal{N}(0,1)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.8.m8.2d">italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ∼ caligraphic_N ( 0 , 1 )</annotation></semantics></math> conditioned on a text prompt <math alttext="y" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.9.m9.1"><semantics id="S3.SS3.SSS1.p1.9.m9.1a"><mi id="S3.SS3.SSS1.p1.9.m9.1.1" xref="S3.SS3.SSS1.p1.9.m9.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.9.m9.1b"><ci id="S3.SS3.SSS1.p1.9.m9.1.1.cmml" xref="S3.SS3.SSS1.p1.9.m9.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.9.m9.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.9.m9.1d">italic_y</annotation></semantics></math>. Specifically, at each denoising step for <math alttext="t=1,\ldots,T" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.10.m10.3"><semantics id="S3.SS3.SSS1.p1.10.m10.3a"><mrow id="S3.SS3.SSS1.p1.10.m10.3.4" xref="S3.SS3.SSS1.p1.10.m10.3.4.cmml"><mi id="S3.SS3.SSS1.p1.10.m10.3.4.2" xref="S3.SS3.SSS1.p1.10.m10.3.4.2.cmml">t</mi><mo id="S3.SS3.SSS1.p1.10.m10.3.4.1" xref="S3.SS3.SSS1.p1.10.m10.3.4.1.cmml">=</mo><mrow id="S3.SS3.SSS1.p1.10.m10.3.4.3.2" xref="S3.SS3.SSS1.p1.10.m10.3.4.3.1.cmml"><mn id="S3.SS3.SSS1.p1.10.m10.1.1" xref="S3.SS3.SSS1.p1.10.m10.1.1.cmml">1</mn><mo id="S3.SS3.SSS1.p1.10.m10.3.4.3.2.1" xref="S3.SS3.SSS1.p1.10.m10.3.4.3.1.cmml">,</mo><mi id="S3.SS3.SSS1.p1.10.m10.2.2" mathvariant="normal" xref="S3.SS3.SSS1.p1.10.m10.2.2.cmml">…</mi><mo id="S3.SS3.SSS1.p1.10.m10.3.4.3.2.2" xref="S3.SS3.SSS1.p1.10.m10.3.4.3.1.cmml">,</mo><mi id="S3.SS3.SSS1.p1.10.m10.3.3" xref="S3.SS3.SSS1.p1.10.m10.3.3.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.10.m10.3b"><apply id="S3.SS3.SSS1.p1.10.m10.3.4.cmml" xref="S3.SS3.SSS1.p1.10.m10.3.4"><eq id="S3.SS3.SSS1.p1.10.m10.3.4.1.cmml" xref="S3.SS3.SSS1.p1.10.m10.3.4.1"></eq><ci id="S3.SS3.SSS1.p1.10.m10.3.4.2.cmml" xref="S3.SS3.SSS1.p1.10.m10.3.4.2">𝑡</ci><list id="S3.SS3.SSS1.p1.10.m10.3.4.3.1.cmml" xref="S3.SS3.SSS1.p1.10.m10.3.4.3.2"><cn id="S3.SS3.SSS1.p1.10.m10.1.1.cmml" type="integer" xref="S3.SS3.SSS1.p1.10.m10.1.1">1</cn><ci id="S3.SS3.SSS1.p1.10.m10.2.2.cmml" xref="S3.SS3.SSS1.p1.10.m10.2.2">…</ci><ci id="S3.SS3.SSS1.p1.10.m10.3.3.cmml" xref="S3.SS3.SSS1.p1.10.m10.3.3">𝑇</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.10.m10.3c">t=1,\ldots,T</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.10.m10.3d">italic_t = 1 , … , italic_T</annotation></semantics></math>, <math alttext="x_{t-1}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.11.m11.1"><semantics id="S3.SS3.SSS1.p1.11.m11.1a"><msub id="S3.SS3.SSS1.p1.11.m11.1.1" xref="S3.SS3.SSS1.p1.11.m11.1.1.cmml"><mi id="S3.SS3.SSS1.p1.11.m11.1.1.2" xref="S3.SS3.SSS1.p1.11.m11.1.1.2.cmml">x</mi><mrow id="S3.SS3.SSS1.p1.11.m11.1.1.3" xref="S3.SS3.SSS1.p1.11.m11.1.1.3.cmml"><mi id="S3.SS3.SSS1.p1.11.m11.1.1.3.2" xref="S3.SS3.SSS1.p1.11.m11.1.1.3.2.cmml">t</mi><mo id="S3.SS3.SSS1.p1.11.m11.1.1.3.1" xref="S3.SS3.SSS1.p1.11.m11.1.1.3.1.cmml">−</mo><mn id="S3.SS3.SSS1.p1.11.m11.1.1.3.3" xref="S3.SS3.SSS1.p1.11.m11.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.11.m11.1b"><apply id="S3.SS3.SSS1.p1.11.m11.1.1.cmml" xref="S3.SS3.SSS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.11.m11.1.1.1.cmml" xref="S3.SS3.SSS1.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.11.m11.1.1.2.cmml" xref="S3.SS3.SSS1.p1.11.m11.1.1.2">𝑥</ci><apply id="S3.SS3.SSS1.p1.11.m11.1.1.3.cmml" xref="S3.SS3.SSS1.p1.11.m11.1.1.3"><minus id="S3.SS3.SSS1.p1.11.m11.1.1.3.1.cmml" xref="S3.SS3.SSS1.p1.11.m11.1.1.3.1"></minus><ci id="S3.SS3.SSS1.p1.11.m11.1.1.3.2.cmml" xref="S3.SS3.SSS1.p1.11.m11.1.1.3.2">𝑡</ci><cn id="S3.SS3.SSS1.p1.11.m11.1.1.3.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.11.m11.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.11.m11.1c">x_{t-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.11.m11.1d">italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math> is determined using both <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.12.m12.1"><semantics id="S3.SS3.SSS1.p1.12.m12.1a"><msub id="S3.SS3.SSS1.p1.12.m12.1.1" xref="S3.SS3.SSS1.p1.12.m12.1.1.cmml"><mi id="S3.SS3.SSS1.p1.12.m12.1.1.2" xref="S3.SS3.SSS1.p1.12.m12.1.1.2.cmml">x</mi><mi id="S3.SS3.SSS1.p1.12.m12.1.1.3" xref="S3.SS3.SSS1.p1.12.m12.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.12.m12.1b"><apply id="S3.SS3.SSS1.p1.12.m12.1.1.cmml" xref="S3.SS3.SSS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.12.m12.1.1.1.cmml" xref="S3.SS3.SSS1.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.12.m12.1.1.2.cmml" xref="S3.SS3.SSS1.p1.12.m12.1.1.2">𝑥</ci><ci id="S3.SS3.SSS1.p1.12.m12.1.1.3.cmml" xref="S3.SS3.SSS1.p1.12.m12.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.12.m12.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.12.m12.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and the text prompt <math alttext="y" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.13.m13.1"><semantics id="S3.SS3.SSS1.p1.13.m13.1a"><mi id="S3.SS3.SSS1.p1.13.m13.1.1" xref="S3.SS3.SSS1.p1.13.m13.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.13.m13.1b"><ci id="S3.SS3.SSS1.p1.13.m13.1.1.cmml" xref="S3.SS3.SSS1.p1.13.m13.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.13.m13.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.13.m13.1d">italic_y</annotation></semantics></math>. After the final denoising step, <math alttext="x_{0}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.14.m14.1"><semantics id="S3.SS3.SSS1.p1.14.m14.1a"><msub id="S3.SS3.SSS1.p1.14.m14.1.1" xref="S3.SS3.SSS1.p1.14.m14.1.1.cmml"><mi id="S3.SS3.SSS1.p1.14.m14.1.1.2" xref="S3.SS3.SSS1.p1.14.m14.1.1.2.cmml">x</mi><mn id="S3.SS3.SSS1.p1.14.m14.1.1.3" xref="S3.SS3.SSS1.p1.14.m14.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.14.m14.1b"><apply id="S3.SS3.SSS1.p1.14.m14.1.1.cmml" xref="S3.SS3.SSS1.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.14.m14.1.1.1.cmml" xref="S3.SS3.SSS1.p1.14.m14.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.14.m14.1.1.2.cmml" xref="S3.SS3.SSS1.p1.14.m14.1.1.2">𝑥</ci><cn id="S3.SS3.SSS1.p1.14.m14.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.14.m14.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.14.m14.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.14.m14.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is transformed back to the clean sample.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">In practice, our objective is to obtain features directly from clean images without relying on conditional prompts. Specifically, we achieve this by employing an unconditioned text embedding
and
running Stable Diffusion only once with a very small timestep, such as <math alttext="t=0" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.1.m1.1"><semantics id="S3.SS3.SSS1.p2.1.m1.1a"><mrow id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.1.m1.1.1.2" xref="S3.SS3.SSS1.p2.1.m1.1.1.2.cmml">t</mi><mo id="S3.SS3.SSS1.p2.1.m1.1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS3.SSS1.p2.1.m1.1.1.3" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.1b"><apply id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1"><eq id="S3.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.1"></eq><ci id="S3.SS3.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.2">𝑡</ci><cn id="S3.SS3.SSS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.1c">t=0</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.1.m1.1d">italic_t = 0</annotation></semantics></math>. With such a small timestep, from the perspective of the diffusion model, clean images are predominantly treated as denoised images.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Diffusion features aggregation</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS3.SSS1" title="3.3.1 Diffusion process ‣ 3.3 Diffusion features ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3.1</span></a>, we introduce the feature extraction from a diffusion model. Motivated by the findings in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.F2" title="In 3.1 Object pose estimation ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, the aggregation of features extracted from different layers can potentially generalize well to different objects and scenarios. Thus, in this section, we investigate the ways of aggregation.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.7">Given a set of raw diffusion features <math alttext="\{f_{1},f_{2},\ldots,f_{n}\}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.1.m1.4"><semantics id="S3.SS3.SSS2.p2.1.m1.4a"><mrow id="S3.SS3.SSS2.p2.1.m1.4.4.3" xref="S3.SS3.SSS2.p2.1.m1.4.4.4.cmml"><mo id="S3.SS3.SSS2.p2.1.m1.4.4.3.4" stretchy="false" xref="S3.SS3.SSS2.p2.1.m1.4.4.4.cmml">{</mo><msub id="S3.SS3.SSS2.p2.1.m1.2.2.1.1" xref="S3.SS3.SSS2.p2.1.m1.2.2.1.1.cmml"><mi id="S3.SS3.SSS2.p2.1.m1.2.2.1.1.2" xref="S3.SS3.SSS2.p2.1.m1.2.2.1.1.2.cmml">f</mi><mn id="S3.SS3.SSS2.p2.1.m1.2.2.1.1.3" xref="S3.SS3.SSS2.p2.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS2.p2.1.m1.4.4.3.5" xref="S3.SS3.SSS2.p2.1.m1.4.4.4.cmml">,</mo><msub id="S3.SS3.SSS2.p2.1.m1.3.3.2.2" xref="S3.SS3.SSS2.p2.1.m1.3.3.2.2.cmml"><mi id="S3.SS3.SSS2.p2.1.m1.3.3.2.2.2" xref="S3.SS3.SSS2.p2.1.m1.3.3.2.2.2.cmml">f</mi><mn id="S3.SS3.SSS2.p2.1.m1.3.3.2.2.3" xref="S3.SS3.SSS2.p2.1.m1.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.SSS2.p2.1.m1.4.4.3.6" xref="S3.SS3.SSS2.p2.1.m1.4.4.4.cmml">,</mo><mi id="S3.SS3.SSS2.p2.1.m1.1.1" mathvariant="normal" xref="S3.SS3.SSS2.p2.1.m1.1.1.cmml">…</mi><mo id="S3.SS3.SSS2.p2.1.m1.4.4.3.7" xref="S3.SS3.SSS2.p2.1.m1.4.4.4.cmml">,</mo><msub id="S3.SS3.SSS2.p2.1.m1.4.4.3.3" xref="S3.SS3.SSS2.p2.1.m1.4.4.3.3.cmml"><mi id="S3.SS3.SSS2.p2.1.m1.4.4.3.3.2" xref="S3.SS3.SSS2.p2.1.m1.4.4.3.3.2.cmml">f</mi><mi id="S3.SS3.SSS2.p2.1.m1.4.4.3.3.3" xref="S3.SS3.SSS2.p2.1.m1.4.4.3.3.3.cmml">n</mi></msub><mo id="S3.SS3.SSS2.p2.1.m1.4.4.3.8" stretchy="false" xref="S3.SS3.SSS2.p2.1.m1.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.1.m1.4b"><set id="S3.SS3.SSS2.p2.1.m1.4.4.4.cmml" xref="S3.SS3.SSS2.p2.1.m1.4.4.3"><apply id="S3.SS3.SSS2.p2.1.m1.2.2.1.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS3.SSS2.p2.1.m1.2.2.1.1.2">𝑓</ci><cn id="S3.SS3.SSS2.p2.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.SS3.SSS2.p2.1.m1.2.2.1.1.3">1</cn></apply><apply id="S3.SS3.SSS2.p2.1.m1.3.3.2.2.cmml" xref="S3.SS3.SSS2.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.1.m1.3.3.2.2.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS3.SSS2.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS3.SSS2.p2.1.m1.3.3.2.2.2">𝑓</ci><cn id="S3.SS3.SSS2.p2.1.m1.3.3.2.2.3.cmml" type="integer" xref="S3.SS3.SSS2.p2.1.m1.3.3.2.2.3">2</cn></apply><ci id="S3.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1">…</ci><apply id="S3.SS3.SSS2.p2.1.m1.4.4.3.3.cmml" xref="S3.SS3.SSS2.p2.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.1.m1.4.4.3.3.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.4.4.3.3">subscript</csymbol><ci id="S3.SS3.SSS2.p2.1.m1.4.4.3.3.2.cmml" xref="S3.SS3.SSS2.p2.1.m1.4.4.3.3.2">𝑓</ci><ci id="S3.SS3.SSS2.p2.1.m1.4.4.3.3.3.cmml" xref="S3.SS3.SSS2.p2.1.m1.4.4.3.3.3">𝑛</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.1.m1.4c">\{f_{1},f_{2},\ldots,f_{n}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.1.m1.4d">{ italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math> from <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.2.m2.1"><semantics id="S3.SS3.SSS2.p2.2.m2.1a"><mi id="S3.SS3.SSS2.p2.2.m2.1.1" xref="S3.SS3.SSS2.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.2.m2.1b"><ci id="S3.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS2.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.2.m2.1d">italic_n</annotation></semantics></math> layers in a diffusion model, we need to employ trainable architectures to aggregate these features into a fused one <math alttext="\hat{\mathcal{F}}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.3.m3.1"><semantics id="S3.SS3.SSS2.p2.3.m3.1a"><mover accent="true" id="S3.SS3.SSS2.p2.3.m3.1.1" xref="S3.SS3.SSS2.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS2.p2.3.m3.1.1.2" xref="S3.SS3.SSS2.p2.3.m3.1.1.2.cmml">ℱ</mi><mo id="S3.SS3.SSS2.p2.3.m3.1.1.1" xref="S3.SS3.SSS2.p2.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.3.m3.1b"><apply id="S3.SS3.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS2.p2.3.m3.1.1"><ci id="S3.SS3.SSS2.p2.3.m3.1.1.1.cmml" xref="S3.SS3.SSS2.p2.3.m3.1.1.1">^</ci><ci id="S3.SS3.SSS2.p2.3.m3.1.1.2.cmml" xref="S3.SS3.SSS2.p2.3.m3.1.1.2">ℱ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.3.m3.1c">\hat{\mathcal{F}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.3.m3.1d">over^ start_ARG caligraphic_F end_ARG</annotation></semantics></math>. The aggregation involves two essential components, an extractor <math alttext="\Phi_{\text{ext}}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.4.m4.1"><semantics id="S3.SS3.SSS2.p2.4.m4.1a"><msub id="S3.SS3.SSS2.p2.4.m4.1.1" xref="S3.SS3.SSS2.p2.4.m4.1.1.cmml"><mi id="S3.SS3.SSS2.p2.4.m4.1.1.2" mathvariant="normal" xref="S3.SS3.SSS2.p2.4.m4.1.1.2.cmml">Φ</mi><mtext id="S3.SS3.SSS2.p2.4.m4.1.1.3" xref="S3.SS3.SSS2.p2.4.m4.1.1.3a.cmml">ext</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.4.m4.1b"><apply id="S3.SS3.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.4.m4.1.1.1.cmml" xref="S3.SS3.SSS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p2.4.m4.1.1.2.cmml" xref="S3.SS3.SSS2.p2.4.m4.1.1.2">Φ</ci><ci id="S3.SS3.SSS2.p2.4.m4.1.1.3a.cmml" xref="S3.SS3.SSS2.p2.4.m4.1.1.3"><mtext id="S3.SS3.SSS2.p2.4.m4.1.1.3.cmml" mathsize="70%" xref="S3.SS3.SSS2.p2.4.m4.1.1.3">ext</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.4.m4.1c">\Phi_{\text{ext}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.4.m4.1d">roman_Φ start_POSTSUBSCRIPT ext end_POSTSUBSCRIPT</annotation></semantics></math> and an aggregator <math alttext="\Phi_{\text{aggr}}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.5.m5.1"><semantics id="S3.SS3.SSS2.p2.5.m5.1a"><msub id="S3.SS3.SSS2.p2.5.m5.1.1" xref="S3.SS3.SSS2.p2.5.m5.1.1.cmml"><mi id="S3.SS3.SSS2.p2.5.m5.1.1.2" mathvariant="normal" xref="S3.SS3.SSS2.p2.5.m5.1.1.2.cmml">Φ</mi><mtext id="S3.SS3.SSS2.p2.5.m5.1.1.3" xref="S3.SS3.SSS2.p2.5.m5.1.1.3a.cmml">aggr</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.5.m5.1b"><apply id="S3.SS3.SSS2.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.5.m5.1.1.1.cmml" xref="S3.SS3.SSS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p2.5.m5.1.1.2.cmml" xref="S3.SS3.SSS2.p2.5.m5.1.1.2">Φ</ci><ci id="S3.SS3.SSS2.p2.5.m5.1.1.3a.cmml" xref="S3.SS3.SSS2.p2.5.m5.1.1.3"><mtext id="S3.SS3.SSS2.p2.5.m5.1.1.3.cmml" mathsize="70%" xref="S3.SS3.SSS2.p2.5.m5.1.1.3">aggr</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.5.m5.1c">\Phi_{\text{aggr}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.5.m5.1d">roman_Φ start_POSTSUBSCRIPT aggr end_POSTSUBSCRIPT</annotation></semantics></math>. Specifically, <math alttext="\Phi_{\text{ext}}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.6.m6.1"><semantics id="S3.SS3.SSS2.p2.6.m6.1a"><msub id="S3.SS3.SSS2.p2.6.m6.1.1" xref="S3.SS3.SSS2.p2.6.m6.1.1.cmml"><mi id="S3.SS3.SSS2.p2.6.m6.1.1.2" mathvariant="normal" xref="S3.SS3.SSS2.p2.6.m6.1.1.2.cmml">Φ</mi><mtext id="S3.SS3.SSS2.p2.6.m6.1.1.3" xref="S3.SS3.SSS2.p2.6.m6.1.1.3a.cmml">ext</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.6.m6.1b"><apply id="S3.SS3.SSS2.p2.6.m6.1.1.cmml" xref="S3.SS3.SSS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.6.m6.1.1.1.cmml" xref="S3.SS3.SSS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p2.6.m6.1.1.2.cmml" xref="S3.SS3.SSS2.p2.6.m6.1.1.2">Φ</ci><ci id="S3.SS3.SSS2.p2.6.m6.1.1.3a.cmml" xref="S3.SS3.SSS2.p2.6.m6.1.1.3"><mtext id="S3.SS3.SSS2.p2.6.m6.1.1.3.cmml" mathsize="70%" xref="S3.SS3.SSS2.p2.6.m6.1.1.3">ext</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.6.m6.1c">\Phi_{\text{ext}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.6.m6.1d">roman_Φ start_POSTSUBSCRIPT ext end_POSTSUBSCRIPT</annotation></semantics></math> is employed to align the raw diffusion features to the same dimension and to learn task-specific features adapting to object pose estimation;
while <math alttext="\Phi_{\text{aggr}}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.7.m7.1"><semantics id="S3.SS3.SSS2.p2.7.m7.1a"><msub id="S3.SS3.SSS2.p2.7.m7.1.1" xref="S3.SS3.SSS2.p2.7.m7.1.1.cmml"><mi id="S3.SS3.SSS2.p2.7.m7.1.1.2" mathvariant="normal" xref="S3.SS3.SSS2.p2.7.m7.1.1.2.cmml">Φ</mi><mtext id="S3.SS3.SSS2.p2.7.m7.1.1.3" xref="S3.SS3.SSS2.p2.7.m7.1.1.3a.cmml">aggr</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.7.m7.1b"><apply id="S3.SS3.SSS2.p2.7.m7.1.1.cmml" xref="S3.SS3.SSS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.7.m7.1.1.1.cmml" xref="S3.SS3.SSS2.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p2.7.m7.1.1.2.cmml" xref="S3.SS3.SSS2.p2.7.m7.1.1.2">Φ</ci><ci id="S3.SS3.SSS2.p2.7.m7.1.1.3a.cmml" xref="S3.SS3.SSS2.p2.7.m7.1.1.3"><mtext id="S3.SS3.SSS2.p2.7.m7.1.1.3.cmml" mathsize="70%" xref="S3.SS3.SSS2.p2.7.m7.1.1.3">aggr</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.7.m7.1c">\Phi_{\text{aggr}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.7.m7.1d">roman_Φ start_POSTSUBSCRIPT aggr end_POSTSUBSCRIPT</annotation></semantics></math> is employed to aggregate the extracted features. This process can be standardized as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathcal{F}}=\Phi_{\text{aggr}}\left(\{\Phi_{\text{ext}}^{1}\left(f_{1}%
\right),\Phi_{\text{ext}}^{2}\left(f_{2}\right),\ldots,\Phi_{\text{ext}}^{n}%
\left(f_{n}\right)\}\right)" class="ltx_Math" display="block" id="S3.E3.m1.2"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mover accent="true" id="S3.E3.m1.2.2.3" xref="S3.E3.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.2.3.2" xref="S3.E3.m1.2.2.3.2.cmml">ℱ</mi><mo id="S3.E3.m1.2.2.3.1" xref="S3.E3.m1.2.2.3.1.cmml">^</mo></mover><mo id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml">=</mo><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.cmml"><msub id="S3.E3.m1.2.2.1.3" xref="S3.E3.m1.2.2.1.3.cmml"><mi id="S3.E3.m1.2.2.1.3.2" mathvariant="normal" xref="S3.E3.m1.2.2.1.3.2.cmml">Φ</mi><mtext id="S3.E3.m1.2.2.1.3.3" xref="S3.E3.m1.2.2.1.3.3a.cmml">aggr</mtext></msub><mo id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.cmml"><mo id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.4.cmml"><mo id="S3.E3.m1.2.2.1.1.1.1.3.4" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.1.4.cmml">{</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml"><msubsup id="S3.E3.m1.2.2.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.2" mathvariant="normal" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.2.cmml">Φ</mi><mtext id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.3a.cmml">ext</mtext><mn id="S3.E3.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.3.cmml">1</mn></msubsup><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.1.1.3.5" xref="S3.E3.m1.2.2.1.1.1.1.4.cmml">,</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.2.2" xref="S3.E3.m1.2.2.1.1.1.1.2.2.cmml"><msubsup id="S3.E3.m1.2.2.1.1.1.1.2.2.3" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.2" mathvariant="normal" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.2.cmml">Φ</mi><mtext id="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.3" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.3a.cmml">ext</mtext><mn id="S3.E3.m1.2.2.1.1.1.1.2.2.3.3" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3.3.cmml">2</mn></msubsup><mo id="S3.E3.m1.2.2.1.1.1.1.2.2.2" xref="S3.E3.m1.2.2.1.1.1.1.2.2.2.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.cmml"><mo id="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.cmml">(</mo><msub id="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.2.cmml">f</mi><mn id="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.3.cmml">2</mn></msub><mo id="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.1.1.3.6" xref="S3.E3.m1.2.2.1.1.1.1.4.cmml">,</mo><mi id="S3.E3.m1.1.1" mathvariant="normal" xref="S3.E3.m1.1.1.cmml">…</mi><mo id="S3.E3.m1.2.2.1.1.1.1.3.7" xref="S3.E3.m1.2.2.1.1.1.1.4.cmml">,</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.3.3" xref="S3.E3.m1.2.2.1.1.1.1.3.3.cmml"><msubsup id="S3.E3.m1.2.2.1.1.1.1.3.3.3" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.2" mathvariant="normal" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.2.cmml">Φ</mi><mtext id="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.3" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.3a.cmml">ext</mtext><mi id="S3.E3.m1.2.2.1.1.1.1.3.3.3.3" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3.3.cmml">n</mi></msubsup><mo id="S3.E3.m1.2.2.1.1.1.1.3.3.2" xref="S3.E3.m1.2.2.1.1.1.1.3.3.2.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.3.3.1.1" xref="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.cmml"><mo id="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.cmml">(</mo><msub id="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.2.cmml">f</mi><mi id="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.3.cmml">n</mi></msub><mo id="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.1.1.3.8" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.1.4.cmml">}</mo></mrow><mo id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><eq id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"></eq><apply id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.3"><ci id="S3.E3.m1.2.2.3.1.cmml" xref="S3.E3.m1.2.2.3.1">^</ci><ci id="S3.E3.m1.2.2.3.2.cmml" xref="S3.E3.m1.2.2.3.2">ℱ</ci></apply><apply id="S3.E3.m1.2.2.1.cmml" xref="S3.E3.m1.2.2.1"><times id="S3.E3.m1.2.2.1.2.cmml" xref="S3.E3.m1.2.2.1.2"></times><apply id="S3.E3.m1.2.2.1.3.cmml" xref="S3.E3.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.3.1.cmml" xref="S3.E3.m1.2.2.1.3">subscript</csymbol><ci id="S3.E3.m1.2.2.1.3.2.cmml" xref="S3.E3.m1.2.2.1.3.2">Φ</ci><ci id="S3.E3.m1.2.2.1.3.3a.cmml" xref="S3.E3.m1.2.2.1.3.3"><mtext id="S3.E3.m1.2.2.1.3.3.cmml" mathsize="70%" xref="S3.E3.m1.2.2.1.3.3">aggr</mtext></ci></apply><set id="S3.E3.m1.2.2.1.1.1.1.4.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3"><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2"></times><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.2">Φ</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.3a.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.3"><mtext id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.3.cmml" mathsize="70%" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.3">ext</mtext></ci></apply><cn id="S3.E3.m1.2.2.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.3">1</cn></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S3.E3.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2"><times id="S3.E3.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2.2"></times><apply id="S3.E3.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.2.2.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.2">Φ</ci><ci id="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.3a.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.3"><mtext id="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.3.cmml" mathsize="70%" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3.2.3">ext</mtext></ci></apply><cn id="S3.E3.m1.2.2.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E3.m1.2.2.1.1.1.1.2.2.3.3">2</cn></apply><apply id="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.2">𝑓</ci><cn id="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.2.2.1.1.1.1.2.2.1.1.1.3">2</cn></apply></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">…</ci><apply id="S3.E3.m1.2.2.1.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3"><times id="S3.E3.m1.2.2.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.2"></times><apply id="S3.E3.m1.2.2.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.2">Φ</ci><ci id="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.3a.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.3"><mtext id="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.3.cmml" mathsize="70%" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3.2.3">ext</mtext></ci></apply><ci id="S3.E3.m1.2.2.1.1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.3.3">𝑛</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.2">𝑓</ci><ci id="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3.1.1.1.3">𝑛</ci></apply></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\hat{\mathcal{F}}=\Phi_{\text{aggr}}\left(\{\Phi_{\text{ext}}^{1}\left(f_{1}%
\right),\Phi_{\text{ext}}^{2}\left(f_{2}\right),\ldots,\Phi_{\text{ext}}^{n}%
\left(f_{n}\right)\}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.2d">over^ start_ARG caligraphic_F end_ARG = roman_Φ start_POSTSUBSCRIPT aggr end_POSTSUBSCRIPT ( { roman_Φ start_POSTSUBSCRIPT ext end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , roman_Φ start_POSTSUBSCRIPT ext end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , … , roman_Φ start_POSTSUBSCRIPT ext end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) } )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS2.p2.8">In this section we discuss several aggregation architectures for diffusion features.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Arch. (a)</h5>
<div class="ltx_para" id="S3.SS3.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS2.Px1.p1.1">First, we design a vanilla aggregation network, illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.F3.sf1" title="In Figure 3 ‣ Summarization ‣ 3.2 Motivation: feature matters ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3(a)</span></a>. Given a set of diffusion features, we first upsample them to a standard resolution. Next, we utilize <math alttext="3\times 3" class="ltx_Math" display="inline" id="S3.SS3.SSS2.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS2.Px1.p1.1.m1.1a"><mrow id="S3.SS3.SSS2.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS2.Px1.p1.1.m1.1.1.cmml"><mn id="S3.SS3.SSS2.Px1.p1.1.m1.1.1.2" xref="S3.SS3.SSS2.Px1.p1.1.m1.1.1.2.cmml">3</mn><mo id="S3.SS3.SSS2.Px1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS2.Px1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.SSS2.Px1.p1.1.m1.1.1.3" xref="S3.SS3.SSS2.Px1.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p1.1.m1.1b"><apply id="S3.SS3.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.Px1.p1.1.m1.1.1"><times id="S3.SS3.SSS2.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.Px1.p1.1.m1.1.1.1"></times><cn id="S3.SS3.SSS2.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.SSS2.Px1.p1.1.m1.1.1.2">3</cn><cn id="S3.SS3.SSS2.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS2.Px1.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p1.1.m1.1c">3\times 3</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.Px1.p1.1.m1.1d">3 × 3</annotation></semantics></math> convolution layers to learn features specialized for object pose estimation
and project them to the same channel count. Finally, we aggregate them directly through element-wise addition as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathcal{F}}=\sum_{i=1}^{n}\Phi_{\text{ext}}^{i}(f_{i})" class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mover accent="true" id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">ℱ</mi><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">^</mo></mover><mo id="S3.E4.m1.1.1.2" rspace="0.111em" xref="S3.E4.m1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><munderover id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.2.2.2" movablelimits="false" xref="S3.E4.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E4.m1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.2.2.3.2" xref="S3.E4.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E4.m1.1.1.1.2.2.3.1" xref="S3.E4.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E4.m1.1.1.1.2.2.3.3" xref="S3.E4.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><msubsup id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.2.2" mathvariant="normal" xref="S3.E4.m1.1.1.1.1.3.2.2.cmml">Φ</mi><mtext id="S3.E4.m1.1.1.1.1.3.2.3" xref="S3.E4.m1.1.1.1.1.3.2.3a.cmml">ext</mtext><mi id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml">i</mi></msubsup><mo id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"></eq><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><ci id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1">^</ci><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">ℱ</ci></apply><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><apply id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.2">superscript</csymbol><apply id="S3.E4.m1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.2">subscript</csymbol><sum id="S3.E4.m1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.2.2.2"></sum><apply id="S3.E4.m1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.2.2.3"><eq id="S3.E4.m1.1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.1.1.1.2.2.3.1"></eq><ci id="S3.E4.m1.1.1.1.2.2.3.2.cmml" xref="S3.E4.m1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.E4.m1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E4.m1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.2.3">𝑛</ci></apply><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"></times><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3">superscript</csymbol><apply id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2">Φ</ci><ci id="S3.E4.m1.1.1.1.1.3.2.3a.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3"><mtext id="S3.E4.m1.1.1.1.1.3.2.3.cmml" mathsize="70%" xref="S3.E4.m1.1.1.1.1.3.2.3">ext</mtext></ci></apply><ci id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2">𝑓</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\hat{\mathcal{F}}=\sum_{i=1}^{n}\Phi_{\text{ext}}^{i}(f_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">over^ start_ARG caligraphic_F end_ARG = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_Φ start_POSTSUBSCRIPT ext end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Arch. (b)</h5>
<div class="ltx_para" id="S3.SS3.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.Px2.p1.1">Since the vanilla aggregation network only performs linear mapping on the original features and lacks any nonlinearity, it falls short in capturing complex data patterns and nonlinearities. Hence, we design a nonlinear aggregation network, illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.F3.sf2" title="In Figure 3 ‣ Summarization ‣ 3.2 Motivation: feature matters ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3(b)</span></a>. In this architecture, we substituted <math alttext="\Phi_{\text{ext}}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.Px2.p1.1.m1.1"><semantics id="S3.SS3.SSS2.Px2.p1.1.m1.1a"><msub id="S3.SS3.SSS2.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS2.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS2.Px2.p1.1.m1.1.1.2" mathvariant="normal" xref="S3.SS3.SSS2.Px2.p1.1.m1.1.1.2.cmml">Φ</mi><mtext id="S3.SS3.SSS2.Px2.p1.1.m1.1.1.3" xref="S3.SS3.SSS2.Px2.p1.1.m1.1.1.3a.cmml">ext</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px2.p1.1.m1.1b"><apply id="S3.SS3.SSS2.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS2.Px2.p1.1.m1.1.1.2">Φ</ci><ci id="S3.SS3.SSS2.Px2.p1.1.m1.1.1.3a.cmml" xref="S3.SS3.SSS2.Px2.p1.1.m1.1.1.3"><mtext id="S3.SS3.SSS2.Px2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS3.SSS2.Px2.p1.1.m1.1.1.3">ext</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px2.p1.1.m1.1c">\Phi_{\text{ext}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.Px2.p1.1.m1.1d">roman_Φ start_POSTSUBSCRIPT ext end_POSTSUBSCRIPT</annotation></semantics></math> from the vanilla aggregation network with a bottleneck layer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> to introduce nonlinearity.
Specifically, the bottleneck layer consists of three convolutions and ReLU functions with a skip connection.
However, as the number of parameters increases, the limited training data adversely affects the generalization of the aggregated features. In response to this challenge, we take inspiration from ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> and initialize the last convolution in the bottleneck with zero values.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Arch. (c)</h5>
<div class="ltx_para" id="S3.SS3.SSS2.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS2.Px3.p1.3">During aggregation, Arch. (a) and (b) implicitly set the weights of each feature as 1 for element-wise addition. However, the optimal weights can be learned to lead to better aggregation. Thus, we design a so-called context-aware weight aggregation network that learns the weights based on the context, as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.F3.sf3" title="In Figure 3 ‣ Summarization ‣ 3.2 Motivation: feature matters ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3(c)</span></a>. Specifically, we start by upsampling all features and then passing them through an extractor composed of bottleneck layers same as Arch. (b):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h_{i}=\Phi_{\text{ext}}^{i}(f_{i})" class="ltx_Math" display="block" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><msub id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.3.2.cmml">h</mi><mi id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml">i</mi></msub><mo id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml"><msubsup id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.3.2.2" mathvariant="normal" xref="S3.E5.m1.1.1.1.3.2.2.cmml">Φ</mi><mtext id="S3.E5.m1.1.1.1.3.2.3" xref="S3.E5.m1.1.1.1.3.2.3a.cmml">ext</mtext><mi id="S3.E5.m1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.3.3.cmml">i</mi></msubsup><mo id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2"></eq><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2">ℎ</ci><ci id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3">𝑖</ci></apply><apply id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><times id="S3.E5.m1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.2"></times><apply id="S3.E5.m1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.3">superscript</csymbol><apply id="S3.E5.m1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.1.3.2.2">Φ</ci><ci id="S3.E5.m1.1.1.1.3.2.3a.cmml" xref="S3.E5.m1.1.1.1.3.2.3"><mtext id="S3.E5.m1.1.1.1.3.2.3.cmml" mathsize="70%" xref="S3.E5.m1.1.1.1.3.2.3">ext</mtext></ci></apply><ci id="S3.E5.m1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2">𝑓</ci><ci id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">h_{i}=\Phi_{\text{ext}}^{i}(f_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_Φ start_POSTSUBSCRIPT ext end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS2.Px3.p1.2">Subsequently, we apply an average pooling layer to transform each <math alttext="h_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.Px3.p1.1.m1.1"><semantics id="S3.SS3.SSS2.Px3.p1.1.m1.1a"><msub id="S3.SS3.SSS2.Px3.p1.1.m1.1.1" xref="S3.SS3.SSS2.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS2.Px3.p1.1.m1.1.1.2" xref="S3.SS3.SSS2.Px3.p1.1.m1.1.1.2.cmml">h</mi><mi id="S3.SS3.SSS2.Px3.p1.1.m1.1.1.3" xref="S3.SS3.SSS2.Px3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px3.p1.1.m1.1b"><apply id="S3.SS3.SSS2.Px3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS2.Px3.p1.1.m1.1.1.2">ℎ</ci><ci id="S3.SS3.SSS2.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS2.Px3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px3.p1.1.m1.1c">h_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.Px3.p1.1.m1.1d">italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> into one-dimensional features <math alttext="l_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.Px3.p1.2.m2.1"><semantics id="S3.SS3.SSS2.Px3.p1.2.m2.1a"><msub id="S3.SS3.SSS2.Px3.p1.2.m2.1.1" xref="S3.SS3.SSS2.Px3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS2.Px3.p1.2.m2.1.1.2" xref="S3.SS3.SSS2.Px3.p1.2.m2.1.1.2.cmml">l</mi><mi id="S3.SS3.SSS2.Px3.p1.2.m2.1.1.3" xref="S3.SS3.SSS2.Px3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px3.p1.2.m2.1b"><apply id="S3.SS3.SSS2.Px3.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS2.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS2.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS2.Px3.p1.2.m2.1.1.2">𝑙</ci><ci id="S3.SS3.SSS2.Px3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS2.Px3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px3.p1.2.m2.1c">l_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.Px3.p1.2.m2.1d">italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Following this, we concatenate all these one-dimensional features to represent the entire context. Finally, we employ a multilayer perceptron (MLP) and a softmax to determine the weights associated with each feature:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\{\mathbf{w}_{1},\mathbf{w}_{2},\ldots,\mathbf{w}_{n}\}=\text{softmax}(\text{%
MLP}([l_{1},l_{2},\ldots,l_{n}]))" class="ltx_Math" display="block" id="S3.E6.m1.6"><semantics id="S3.E6.m1.6a"><mrow id="S3.E6.m1.6.6" xref="S3.E6.m1.6.6.cmml"><mrow id="S3.E6.m1.5.5.3.3" xref="S3.E6.m1.5.5.3.4.cmml"><mo id="S3.E6.m1.5.5.3.3.4" stretchy="false" xref="S3.E6.m1.5.5.3.4.cmml">{</mo><msub id="S3.E6.m1.3.3.1.1.1" xref="S3.E6.m1.3.3.1.1.1.cmml"><mi id="S3.E6.m1.3.3.1.1.1.2" xref="S3.E6.m1.3.3.1.1.1.2.cmml">𝐰</mi><mn id="S3.E6.m1.3.3.1.1.1.3" xref="S3.E6.m1.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S3.E6.m1.5.5.3.3.5" xref="S3.E6.m1.5.5.3.4.cmml">,</mo><msub id="S3.E6.m1.4.4.2.2.2" xref="S3.E6.m1.4.4.2.2.2.cmml"><mi id="S3.E6.m1.4.4.2.2.2.2" xref="S3.E6.m1.4.4.2.2.2.2.cmml">𝐰</mi><mn id="S3.E6.m1.4.4.2.2.2.3" xref="S3.E6.m1.4.4.2.2.2.3.cmml">2</mn></msub><mo id="S3.E6.m1.5.5.3.3.6" xref="S3.E6.m1.5.5.3.4.cmml">,</mo><mi id="S3.E6.m1.1.1" mathvariant="normal" xref="S3.E6.m1.1.1.cmml">…</mi><mo id="S3.E6.m1.5.5.3.3.7" xref="S3.E6.m1.5.5.3.4.cmml">,</mo><msub id="S3.E6.m1.5.5.3.3.3" xref="S3.E6.m1.5.5.3.3.3.cmml"><mi id="S3.E6.m1.5.5.3.3.3.2" xref="S3.E6.m1.5.5.3.3.3.2.cmml">𝐰</mi><mi id="S3.E6.m1.5.5.3.3.3.3" xref="S3.E6.m1.5.5.3.3.3.3.cmml">n</mi></msub><mo id="S3.E6.m1.5.5.3.3.8" stretchy="false" xref="S3.E6.m1.5.5.3.4.cmml">}</mo></mrow><mo id="S3.E6.m1.6.6.5" xref="S3.E6.m1.6.6.5.cmml">=</mo><mrow id="S3.E6.m1.6.6.4" xref="S3.E6.m1.6.6.4.cmml"><mtext id="S3.E6.m1.6.6.4.3" xref="S3.E6.m1.6.6.4.3a.cmml">softmax</mtext><mo id="S3.E6.m1.6.6.4.2" xref="S3.E6.m1.6.6.4.2.cmml">⁢</mo><mrow id="S3.E6.m1.6.6.4.1.1" xref="S3.E6.m1.6.6.4.1.1.1.cmml"><mo id="S3.E6.m1.6.6.4.1.1.2" stretchy="false" xref="S3.E6.m1.6.6.4.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.6.6.4.1.1.1" xref="S3.E6.m1.6.6.4.1.1.1.cmml"><mtext id="S3.E6.m1.6.6.4.1.1.1.3" xref="S3.E6.m1.6.6.4.1.1.1.3a.cmml">MLP</mtext><mo id="S3.E6.m1.6.6.4.1.1.1.2" xref="S3.E6.m1.6.6.4.1.1.1.2.cmml">⁢</mo><mrow id="S3.E6.m1.6.6.4.1.1.1.1.1" xref="S3.E6.m1.6.6.4.1.1.1.cmml"><mo id="S3.E6.m1.6.6.4.1.1.1.1.1.2" stretchy="false" xref="S3.E6.m1.6.6.4.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.4.cmml"><mo id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.4" stretchy="false" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.4.cmml">[</mo><msub id="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.2.cmml">l</mi><mn id="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.5" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.2" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.2.cmml">l</mi><mn id="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.3" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.3.cmml">2</mn></msub><mo id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.6" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.4.cmml">,</mo><mi id="S3.E6.m1.2.2" mathvariant="normal" xref="S3.E6.m1.2.2.cmml">…</mi><mo id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.7" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.2" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.2.cmml">l</mi><mi id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.3" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.3.cmml">n</mi></msub><mo id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.8" stretchy="false" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.4.cmml">]</mo></mrow><mo id="S3.E6.m1.6.6.4.1.1.1.1.1.3" stretchy="false" xref="S3.E6.m1.6.6.4.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.6.6.4.1.1.3" stretchy="false" xref="S3.E6.m1.6.6.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.6b"><apply id="S3.E6.m1.6.6.cmml" xref="S3.E6.m1.6.6"><eq id="S3.E6.m1.6.6.5.cmml" xref="S3.E6.m1.6.6.5"></eq><set id="S3.E6.m1.5.5.3.4.cmml" xref="S3.E6.m1.5.5.3.3"><apply id="S3.E6.m1.3.3.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.1.2">𝐰</ci><cn id="S3.E6.m1.3.3.1.1.1.3.cmml" type="integer" xref="S3.E6.m1.3.3.1.1.1.3">1</cn></apply><apply id="S3.E6.m1.4.4.2.2.2.cmml" xref="S3.E6.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.2.2.2.1.cmml" xref="S3.E6.m1.4.4.2.2.2">subscript</csymbol><ci id="S3.E6.m1.4.4.2.2.2.2.cmml" xref="S3.E6.m1.4.4.2.2.2.2">𝐰</ci><cn id="S3.E6.m1.4.4.2.2.2.3.cmml" type="integer" xref="S3.E6.m1.4.4.2.2.2.3">2</cn></apply><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">…</ci><apply id="S3.E6.m1.5.5.3.3.3.cmml" xref="S3.E6.m1.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.5.5.3.3.3.1.cmml" xref="S3.E6.m1.5.5.3.3.3">subscript</csymbol><ci id="S3.E6.m1.5.5.3.3.3.2.cmml" xref="S3.E6.m1.5.5.3.3.3.2">𝐰</ci><ci id="S3.E6.m1.5.5.3.3.3.3.cmml" xref="S3.E6.m1.5.5.3.3.3.3">𝑛</ci></apply></set><apply id="S3.E6.m1.6.6.4.cmml" xref="S3.E6.m1.6.6.4"><times id="S3.E6.m1.6.6.4.2.cmml" xref="S3.E6.m1.6.6.4.2"></times><ci id="S3.E6.m1.6.6.4.3a.cmml" xref="S3.E6.m1.6.6.4.3"><mtext id="S3.E6.m1.6.6.4.3.cmml" xref="S3.E6.m1.6.6.4.3">softmax</mtext></ci><apply id="S3.E6.m1.6.6.4.1.1.1.cmml" xref="S3.E6.m1.6.6.4.1.1"><times id="S3.E6.m1.6.6.4.1.1.1.2.cmml" xref="S3.E6.m1.6.6.4.1.1.1.2"></times><ci id="S3.E6.m1.6.6.4.1.1.1.3a.cmml" xref="S3.E6.m1.6.6.4.1.1.1.3"><mtext id="S3.E6.m1.6.6.4.1.1.1.3.cmml" xref="S3.E6.m1.6.6.4.1.1.1.3">MLP</mtext></ci><list id="S3.E6.m1.6.6.4.1.1.1.1.1.1.4.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.3"><apply id="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.2">𝑙</ci><cn id="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.2">𝑙</ci><cn id="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.3.cmml" type="integer" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.2.2.3">2</cn></apply><ci id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2">…</ci><apply id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.2">𝑙</ci><ci id="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m1.6.6.4.1.1.1.1.1.1.3.3.3">𝑛</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.6c">\{\mathbf{w}_{1},\mathbf{w}_{2},\ldots,\mathbf{w}_{n}\}=\text{softmax}(\text{%
MLP}([l_{1},l_{2},\ldots,l_{n}]))</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.6d">{ bold_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_w start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_w start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } = softmax ( MLP ( [ italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_l start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS2.Px3.p1.4">Finally, these weights are used to perform a weighted sum of all the features to achieve the aggregation:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathcal{F}}=\sum_{i=1}^{n}\mathbf{w}_{i}\cdot h_{i}" class="ltx_Math" display="block" id="S3.E7.m1.1"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><mover accent="true" id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.1.1.2.2" xref="S3.E7.m1.1.1.2.2.cmml">ℱ</mi><mo id="S3.E7.m1.1.1.2.1" xref="S3.E7.m1.1.1.2.1.cmml">^</mo></mover><mo id="S3.E7.m1.1.1.1" rspace="0.111em" xref="S3.E7.m1.1.1.1.cmml">=</mo><mrow id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><munderover id="S3.E7.m1.1.1.3.1" xref="S3.E7.m1.1.1.3.1.cmml"><mo id="S3.E7.m1.1.1.3.1.2.2" movablelimits="false" xref="S3.E7.m1.1.1.3.1.2.2.cmml">∑</mo><mrow id="S3.E7.m1.1.1.3.1.2.3" xref="S3.E7.m1.1.1.3.1.2.3.cmml"><mi id="S3.E7.m1.1.1.3.1.2.3.2" xref="S3.E7.m1.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.E7.m1.1.1.3.1.2.3.1" xref="S3.E7.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.E7.m1.1.1.3.1.2.3.3" xref="S3.E7.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E7.m1.1.1.3.1.3" xref="S3.E7.m1.1.1.3.1.3.cmml">n</mi></munderover><mrow id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml"><msub id="S3.E7.m1.1.1.3.2.2" xref="S3.E7.m1.1.1.3.2.2.cmml"><mi id="S3.E7.m1.1.1.3.2.2.2" xref="S3.E7.m1.1.1.3.2.2.2.cmml">𝐰</mi><mi id="S3.E7.m1.1.1.3.2.2.3" xref="S3.E7.m1.1.1.3.2.2.3.cmml">i</mi></msub><mo id="S3.E7.m1.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.E7.m1.1.1.3.2.1.cmml">⋅</mo><msub id="S3.E7.m1.1.1.3.2.3" xref="S3.E7.m1.1.1.3.2.3.cmml"><mi id="S3.E7.m1.1.1.3.2.3.2" xref="S3.E7.m1.1.1.3.2.3.2.cmml">h</mi><mi id="S3.E7.m1.1.1.3.2.3.3" xref="S3.E7.m1.1.1.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><eq id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"></eq><apply id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2"><ci id="S3.E7.m1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.2.1">^</ci><ci id="S3.E7.m1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.2.2">ℱ</ci></apply><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><apply id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.1.1.cmml" xref="S3.E7.m1.1.1.3.1">superscript</csymbol><apply id="S3.E7.m1.1.1.3.1.2.cmml" xref="S3.E7.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.1.2.1.cmml" xref="S3.E7.m1.1.1.3.1">subscript</csymbol><sum id="S3.E7.m1.1.1.3.1.2.2.cmml" xref="S3.E7.m1.1.1.3.1.2.2"></sum><apply id="S3.E7.m1.1.1.3.1.2.3.cmml" xref="S3.E7.m1.1.1.3.1.2.3"><eq id="S3.E7.m1.1.1.3.1.2.3.1.cmml" xref="S3.E7.m1.1.1.3.1.2.3.1"></eq><ci id="S3.E7.m1.1.1.3.1.2.3.2.cmml" xref="S3.E7.m1.1.1.3.1.2.3.2">𝑖</ci><cn id="S3.E7.m1.1.1.3.1.2.3.3.cmml" type="integer" xref="S3.E7.m1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E7.m1.1.1.3.1.3.cmml" xref="S3.E7.m1.1.1.3.1.3">𝑛</ci></apply><apply id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2"><ci id="S3.E7.m1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.3.2.1">⋅</ci><apply id="S3.E7.m1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.2.2.1.cmml" xref="S3.E7.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.2.2.cmml" xref="S3.E7.m1.1.1.3.2.2.2">𝐰</ci><ci id="S3.E7.m1.1.1.3.2.2.3.cmml" xref="S3.E7.m1.1.1.3.2.2.3">𝑖</ci></apply><apply id="S3.E7.m1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.2.3.1.cmml" xref="S3.E7.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.3.2.cmml" xref="S3.E7.m1.1.1.3.2.3.2">ℎ</ci><ci id="S3.E7.m1.1.1.3.2.3.3.cmml" xref="S3.E7.m1.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\hat{\mathcal{F}}=\sum_{i=1}^{n}\mathbf{w}_{i}\cdot h_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.1d">over^ start_ARG caligraphic_F end_ARG = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT bold_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⋅ italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we first introduce the implementation details, training and test, the datasets, and metrics used for the evaluation (<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS1" title="4.1 Experimental settings ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>). Subsequently, we present an ablation study for timestep, aggregation methods and other pretrained models (<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS2" title="4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>). Finally, we compare our method with the state-of-the-art methods on LM, O-LM and T-LESS datasets (<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.SS3" title="4.3 Comparison with the state-of-the-art ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental settings</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Implementation details</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">In our experiments, we extract features from Stable Diffusion (SD) v1-5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, a generative latent diffusion model trained on LAION-5B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>.
We feed images with a resolution of 512 into this SD model and aggregate features from all layers of its UNet into output features with a dimensionality of 32<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.1"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mo id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><times id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.1.m1.1d">×</annotation></semantics></math>32. We train our network for 20 epochs with a learning rate of 1e-3 for LM dataset and 1e-4 for T-LESS dataset. Throughout all experiments, we directly feed images into the SD model without adding any noise.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Training and test</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.2">To adapt our method to object pose estimation, we freeze the weights of SD model and only train our aggregation networks with pose estimation supervision.
During training, following template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, we create <math alttext="1" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1.1"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mn id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><cn id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.1.m1.1d">1</annotation></semantics></math> positive pair template and <math alttext="M-1" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.2.m2.1"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mrow id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS2.p1.2.m2.1.1.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml">M</mi><mo id="S4.SS1.SSS2.p1.2.m2.1.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS1.SSS2.p1.2.m2.1.1.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><apply id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1"><minus id="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.1"></minus><ci id="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.2">𝑀</ci><cn id="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.SSS2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">M-1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.2.m2.1d">italic_M - 1</annotation></semantics></math> negative pair templates for each real image. We train our model to maximize the agreement between the representations of samples in positive pairs while minimizing that of negative pairs using the InfoNCE loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>. In testing, we find the template that is most similar to the input image and assign the identity and pose information annotated in the template to the input image to complete pose estimation.
We use the same similarity measure as template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. We first calculate the cosine similarity between features, then apply a threshold and template mask for filtering, and finally, we take the average of the remaining values as the final similarity measurement.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Following existing works in object pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, we use three popular object pose estimation benchmarks: LINEMOD (LM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, Occlusion-LINEMOD (O-LM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, and T-LESS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. LM and O-LM are standard benchmarks for evaluating object pose estimation methods.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1">The LM dataset consists of 13 sequences, each containing ground truth poses for a single object. CAD models for all the objects are provided. The O-LM dataset is an extension of the LM dataset, with objects in the dataset being heavily occluded, making pose estimation highly challenging. We follow the approach outlined in template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> to split the cropped data into three non-overlapping groups based on the depicted objects, reserving 10% of the data for testing on seen objects. Our model is trained on LM and tested on both LM and O-LM. Following the protocol of Wohlhart <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p2.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>, we render 301 templates per object. The input image is cropped at the center of objects with the ground-truth pose, following the approach used in previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p3">
<p class="ltx_p" id="S4.SS1.SSS3.p3.1">T-LESS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> comprises 30 objects characterized by a lack of distinct textures or distinguishable colors. These objects are symmetric and similar in shape and size, presenting challenging scenarios with substantial clutter and occlusion. For T-LESS, we adopt the testing methodology established in previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>. We split the 30 objects into two groups: objects 1-18 are considered as seen objects, and objects 19-30 as unseen objects. Our model is trained on the dataset that includes only the seen objects, and testing is performed on the complete test dataset. Following the protocol of template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, we render <math alttext="21,672" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p3.1.m1.2"><semantics id="S4.SS1.SSS3.p3.1.m1.2a"><mrow id="S4.SS1.SSS3.p3.1.m1.2.3.2" xref="S4.SS1.SSS3.p3.1.m1.2.3.1.cmml"><mn id="S4.SS1.SSS3.p3.1.m1.1.1" xref="S4.SS1.SSS3.p3.1.m1.1.1.cmml">21</mn><mo id="S4.SS1.SSS3.p3.1.m1.2.3.2.1" xref="S4.SS1.SSS3.p3.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.SSS3.p3.1.m1.2.2" xref="S4.SS1.SSS3.p3.1.m1.2.2.cmml">672</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p3.1.m1.2b"><list id="S4.SS1.SSS3.p3.1.m1.2.3.1.cmml" xref="S4.SS1.SSS3.p3.1.m1.2.3.2"><cn id="S4.SS1.SSS3.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS3.p3.1.m1.1.1">21</cn><cn id="S4.SS1.SSS3.p3.1.m1.2.2.cmml" type="integer" xref="S4.SS1.SSS3.p3.1.m1.2.2">672</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p3.1.m1.2c">21,672</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p3.1.m1.2d">21 , 672</annotation></semantics></math> templates per object. Similar to previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, we use the ground-truth bounding box to crop the input image.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Evalutation metrics</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.6">For the LM and O-LM datasets, we calculate pose error by computing the geodesic distance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> between two rotations:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d(\hat{\mathbf{R}},\mathbf{R})=\arccos(\frac{\text{tr}(\mathbf{R}^{T}\hat{%
\mathbf{R}})-1}{2})/\pi" class="ltx_Math" display="block" id="S4.E8.m1.4"><semantics id="S4.E8.m1.4a"><mrow id="S4.E8.m1.4.5" xref="S4.E8.m1.4.5.cmml"><mrow id="S4.E8.m1.4.5.2" xref="S4.E8.m1.4.5.2.cmml"><mi id="S4.E8.m1.4.5.2.2" xref="S4.E8.m1.4.5.2.2.cmml">d</mi><mo id="S4.E8.m1.4.5.2.1" xref="S4.E8.m1.4.5.2.1.cmml">⁢</mo><mrow id="S4.E8.m1.4.5.2.3.2" xref="S4.E8.m1.4.5.2.3.1.cmml"><mo id="S4.E8.m1.4.5.2.3.2.1" stretchy="false" xref="S4.E8.m1.4.5.2.3.1.cmml">(</mo><mover accent="true" id="S4.E8.m1.2.2" xref="S4.E8.m1.2.2.cmml"><mi id="S4.E8.m1.2.2.2" xref="S4.E8.m1.2.2.2.cmml">𝐑</mi><mo id="S4.E8.m1.2.2.1" xref="S4.E8.m1.2.2.1.cmml">^</mo></mover><mo id="S4.E8.m1.4.5.2.3.2.2" xref="S4.E8.m1.4.5.2.3.1.cmml">,</mo><mi id="S4.E8.m1.3.3" xref="S4.E8.m1.3.3.cmml">𝐑</mi><mo id="S4.E8.m1.4.5.2.3.2.3" stretchy="false" xref="S4.E8.m1.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E8.m1.4.5.1" xref="S4.E8.m1.4.5.1.cmml">=</mo><mrow id="S4.E8.m1.4.5.3" xref="S4.E8.m1.4.5.3.cmml"><mrow id="S4.E8.m1.4.5.3.2.2" xref="S4.E8.m1.4.5.3.2.1.cmml"><mi id="S4.E8.m1.4.4" xref="S4.E8.m1.4.4.cmml">arccos</mi><mo id="S4.E8.m1.4.5.3.2.2a" xref="S4.E8.m1.4.5.3.2.1.cmml">⁡</mo><mrow id="S4.E8.m1.4.5.3.2.2.1" xref="S4.E8.m1.4.5.3.2.1.cmml"><mo id="S4.E8.m1.4.5.3.2.2.1.1" stretchy="false" xref="S4.E8.m1.4.5.3.2.1.cmml">(</mo><mfrac id="S4.E8.m1.1.1" xref="S4.E8.m1.1.1.cmml"><mrow id="S4.E8.m1.1.1.1" xref="S4.E8.m1.1.1.1.cmml"><mrow id="S4.E8.m1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.cmml"><mtext id="S4.E8.m1.1.1.1.1.3" xref="S4.E8.m1.1.1.1.1.3a.cmml">tr</mtext><mo id="S4.E8.m1.1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E8.m1.1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.1.1.cmml"><mo id="S4.E8.m1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E8.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E8.m1.1.1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.1.1.cmml"><msup id="S4.E8.m1.1.1.1.1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E8.m1.1.1.1.1.1.1.1.2.2" xref="S4.E8.m1.1.1.1.1.1.1.1.2.2.cmml">𝐑</mi><mi id="S4.E8.m1.1.1.1.1.1.1.1.2.3" xref="S4.E8.m1.1.1.1.1.1.1.1.2.3.cmml">T</mi></msup><mo id="S4.E8.m1.1.1.1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mover accent="true" id="S4.E8.m1.1.1.1.1.1.1.1.3" xref="S4.E8.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E8.m1.1.1.1.1.1.1.1.3.2" xref="S4.E8.m1.1.1.1.1.1.1.1.3.2.cmml">𝐑</mi><mo id="S4.E8.m1.1.1.1.1.1.1.1.3.1" xref="S4.E8.m1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S4.E8.m1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E8.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E8.m1.1.1.1.2" xref="S4.E8.m1.1.1.1.2.cmml">−</mo><mn id="S4.E8.m1.1.1.1.3" xref="S4.E8.m1.1.1.1.3.cmml">1</mn></mrow><mn id="S4.E8.m1.1.1.3" xref="S4.E8.m1.1.1.3.cmml">2</mn></mfrac><mo id="S4.E8.m1.4.5.3.2.2.1.2" stretchy="false" xref="S4.E8.m1.4.5.3.2.1.cmml">)</mo></mrow></mrow><mo id="S4.E8.m1.4.5.3.1" xref="S4.E8.m1.4.5.3.1.cmml">/</mo><mi id="S4.E8.m1.4.5.3.3" xref="S4.E8.m1.4.5.3.3.cmml">π</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E8.m1.4b"><apply id="S4.E8.m1.4.5.cmml" xref="S4.E8.m1.4.5"><eq id="S4.E8.m1.4.5.1.cmml" xref="S4.E8.m1.4.5.1"></eq><apply id="S4.E8.m1.4.5.2.cmml" xref="S4.E8.m1.4.5.2"><times id="S4.E8.m1.4.5.2.1.cmml" xref="S4.E8.m1.4.5.2.1"></times><ci id="S4.E8.m1.4.5.2.2.cmml" xref="S4.E8.m1.4.5.2.2">𝑑</ci><interval closure="open" id="S4.E8.m1.4.5.2.3.1.cmml" xref="S4.E8.m1.4.5.2.3.2"><apply id="S4.E8.m1.2.2.cmml" xref="S4.E8.m1.2.2"><ci id="S4.E8.m1.2.2.1.cmml" xref="S4.E8.m1.2.2.1">^</ci><ci id="S4.E8.m1.2.2.2.cmml" xref="S4.E8.m1.2.2.2">𝐑</ci></apply><ci id="S4.E8.m1.3.3.cmml" xref="S4.E8.m1.3.3">𝐑</ci></interval></apply><apply id="S4.E8.m1.4.5.3.cmml" xref="S4.E8.m1.4.5.3"><divide id="S4.E8.m1.4.5.3.1.cmml" xref="S4.E8.m1.4.5.3.1"></divide><apply id="S4.E8.m1.4.5.3.2.1.cmml" xref="S4.E8.m1.4.5.3.2.2"><arccos id="S4.E8.m1.4.4.cmml" xref="S4.E8.m1.4.4"></arccos><apply id="S4.E8.m1.1.1.cmml" xref="S4.E8.m1.1.1"><divide id="S4.E8.m1.1.1.2.cmml" xref="S4.E8.m1.1.1"></divide><apply id="S4.E8.m1.1.1.1.cmml" xref="S4.E8.m1.1.1.1"><minus id="S4.E8.m1.1.1.1.2.cmml" xref="S4.E8.m1.1.1.1.2"></minus><apply id="S4.E8.m1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1"><times id="S4.E8.m1.1.1.1.1.2.cmml" xref="S4.E8.m1.1.1.1.1.2"></times><ci id="S4.E8.m1.1.1.1.1.3a.cmml" xref="S4.E8.m1.1.1.1.1.3"><mtext id="S4.E8.m1.1.1.1.1.3.cmml" xref="S4.E8.m1.1.1.1.1.3">tr</mtext></ci><apply id="S4.E8.m1.1.1.1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1"><times id="S4.E8.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.1"></times><apply id="S4.E8.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S4.E8.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.2.2">𝐑</ci><ci id="S4.E8.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S4.E8.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.3"><ci id="S4.E8.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S4.E8.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.3.2">𝐑</ci></apply></apply></apply><cn id="S4.E8.m1.1.1.1.3.cmml" type="integer" xref="S4.E8.m1.1.1.1.3">1</cn></apply><cn id="S4.E8.m1.1.1.3.cmml" type="integer" xref="S4.E8.m1.1.1.3">2</cn></apply></apply><ci id="S4.E8.m1.4.5.3.3.cmml" xref="S4.E8.m1.4.5.3.3">𝜋</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8.m1.4c">d(\hat{\mathbf{R}},\mathbf{R})=\arccos(\frac{\text{tr}(\mathbf{R}^{T}\hat{%
\mathbf{R}})-1}{2})/\pi</annotation><annotation encoding="application/x-llamapun" id="S4.E8.m1.4d">italic_d ( over^ start_ARG bold_R end_ARG , bold_R ) = roman_arccos ( divide start_ARG tr ( bold_R start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT over^ start_ARG bold_R end_ARG ) - 1 end_ARG start_ARG 2 end_ARG ) / italic_π</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.SSS4.p1.3">Here, <math alttext="\mathbf{R}" class="ltx_Math" display="inline" id="S4.SS1.SSS4.p1.1.m1.1"><semantics id="S4.SS1.SSS4.p1.1.m1.1a"><mi id="S4.SS1.SSS4.p1.1.m1.1.1" xref="S4.SS1.SSS4.p1.1.m1.1.1.cmml">𝐑</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.1.m1.1b"><ci id="S4.SS1.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS4.p1.1.m1.1.1">𝐑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.1.m1.1c">\mathbf{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS4.p1.1.m1.1d">bold_R</annotation></semantics></math> denotes the ground truth 3D orientation, <math alttext="\hat{\mathbf{R}}" class="ltx_Math" display="inline" id="S4.SS1.SSS4.p1.2.m2.1"><semantics id="S4.SS1.SSS4.p1.2.m2.1a"><mover accent="true" id="S4.SS1.SSS4.p1.2.m2.1.1" xref="S4.SS1.SSS4.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS4.p1.2.m2.1.1.2" xref="S4.SS1.SSS4.p1.2.m2.1.1.2.cmml">𝐑</mi><mo id="S4.SS1.SSS4.p1.2.m2.1.1.1" xref="S4.SS1.SSS4.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.2.m2.1b"><apply id="S4.SS1.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS4.p1.2.m2.1.1"><ci id="S4.SS1.SSS4.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS4.p1.2.m2.1.1.1">^</ci><ci id="S4.SS1.SSS4.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS4.p1.2.m2.1.1.2">𝐑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.2.m2.1c">\hat{\mathbf{R}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS4.p1.2.m2.1d">over^ start_ARG bold_R end_ARG</annotation></semantics></math> denotes the predicted 3D orientation, and <span class="ltx_text ltx_markedasmath" id="S4.SS1.SSS4.p1.3.1">tr</span> denotes the trace. In the case of unseen object, the object class is unknown during testing. The accuracy is defined as the percentage of test images for which the best angle error is below a specific threshold and the predicted object class is correct. It can be computed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{ Acc. }=\begin{cases}1&amp;\text{ if }d(\hat{\mathbf{R}},\mathbf{R})&lt;\lambda%
\text{ and }\hat{c}=c\\
0&amp;\text{ otherwise }\end{cases}" class="ltx_Math" display="block" id="S4.E9.m1.4"><semantics id="S4.E9.m1.4a"><mrow id="S4.E9.m1.4.5" xref="S4.E9.m1.4.5.cmml"><mtext id="S4.E9.m1.4.5.2" xref="S4.E9.m1.4.5.2a.cmml"> Acc. </mtext><mo id="S4.E9.m1.4.5.1" xref="S4.E9.m1.4.5.1.cmml">=</mo><mrow id="S4.E9.m1.4.4" xref="S4.E9.m1.4.5.3.1.cmml"><mo id="S4.E9.m1.4.4.5" xref="S4.E9.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S4.E9.m1.4.4.4" rowspacing="0pt" xref="S4.E9.m1.4.5.3.1.cmml"><mtr id="S4.E9.m1.4.4.4a" xref="S4.E9.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E9.m1.4.4.4b" xref="S4.E9.m1.4.5.3.1.cmml"><mn id="S4.E9.m1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E9.m1.4.4.4c" xref="S4.E9.m1.4.5.3.1.cmml"><mrow id="S4.E9.m1.2.2.2.2.2.1" xref="S4.E9.m1.2.2.2.2.2.1.cmml"><mrow id="S4.E9.m1.2.2.2.2.2.1.4" xref="S4.E9.m1.2.2.2.2.2.1.4.cmml"><mtext id="S4.E9.m1.2.2.2.2.2.1.4.2" xref="S4.E9.m1.2.2.2.2.2.1.4.2a.cmml"> if </mtext><mo id="S4.E9.m1.2.2.2.2.2.1.4.1" xref="S4.E9.m1.2.2.2.2.2.1.4.1.cmml">⁢</mo><mi id="S4.E9.m1.2.2.2.2.2.1.4.3" xref="S4.E9.m1.2.2.2.2.2.1.4.3.cmml">d</mi><mo id="S4.E9.m1.2.2.2.2.2.1.4.1a" xref="S4.E9.m1.2.2.2.2.2.1.4.1.cmml">⁢</mo><mrow id="S4.E9.m1.2.2.2.2.2.1.4.4.2" xref="S4.E9.m1.2.2.2.2.2.1.4.4.1.cmml"><mo id="S4.E9.m1.2.2.2.2.2.1.4.4.2.1" stretchy="false" xref="S4.E9.m1.2.2.2.2.2.1.4.4.1.cmml">(</mo><mover accent="true" id="S4.E9.m1.2.2.2.2.2.1.1" xref="S4.E9.m1.2.2.2.2.2.1.1.cmml"><mi id="S4.E9.m1.2.2.2.2.2.1.1.2" xref="S4.E9.m1.2.2.2.2.2.1.1.2.cmml">𝐑</mi><mo id="S4.E9.m1.2.2.2.2.2.1.1.1" xref="S4.E9.m1.2.2.2.2.2.1.1.1.cmml">^</mo></mover><mo id="S4.E9.m1.2.2.2.2.2.1.4.4.2.2" xref="S4.E9.m1.2.2.2.2.2.1.4.4.1.cmml">,</mo><mi id="S4.E9.m1.2.2.2.2.2.1.2" xref="S4.E9.m1.2.2.2.2.2.1.2.cmml">𝐑</mi><mo id="S4.E9.m1.2.2.2.2.2.1.4.4.2.3" stretchy="false" xref="S4.E9.m1.2.2.2.2.2.1.4.4.1.cmml">)</mo></mrow></mrow><mo id="S4.E9.m1.2.2.2.2.2.1.5" xref="S4.E9.m1.2.2.2.2.2.1.5.cmml">&lt;</mo><mrow id="S4.E9.m1.2.2.2.2.2.1.6" xref="S4.E9.m1.2.2.2.2.2.1.6.cmml"><mi id="S4.E9.m1.2.2.2.2.2.1.6.2" xref="S4.E9.m1.2.2.2.2.2.1.6.2.cmml">λ</mi><mo id="S4.E9.m1.2.2.2.2.2.1.6.1" xref="S4.E9.m1.2.2.2.2.2.1.6.1.cmml">⁢</mo><mtext id="S4.E9.m1.2.2.2.2.2.1.6.3" xref="S4.E9.m1.2.2.2.2.2.1.6.3a.cmml"> and </mtext><mo id="S4.E9.m1.2.2.2.2.2.1.6.1a" xref="S4.E9.m1.2.2.2.2.2.1.6.1.cmml">⁢</mo><mover accent="true" id="S4.E9.m1.2.2.2.2.2.1.6.4" xref="S4.E9.m1.2.2.2.2.2.1.6.4.cmml"><mi id="S4.E9.m1.2.2.2.2.2.1.6.4.2" xref="S4.E9.m1.2.2.2.2.2.1.6.4.2.cmml">c</mi><mo id="S4.E9.m1.2.2.2.2.2.1.6.4.1" xref="S4.E9.m1.2.2.2.2.2.1.6.4.1.cmml">^</mo></mover></mrow><mo id="S4.E9.m1.2.2.2.2.2.1.7" xref="S4.E9.m1.2.2.2.2.2.1.7.cmml">=</mo><mi id="S4.E9.m1.2.2.2.2.2.1.8" xref="S4.E9.m1.2.2.2.2.2.1.8.cmml">c</mi></mrow></mtd></mtr><mtr id="S4.E9.m1.4.4.4d" xref="S4.E9.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E9.m1.4.4.4e" xref="S4.E9.m1.4.5.3.1.cmml"><mn id="S4.E9.m1.3.3.3.3.1.1" xref="S4.E9.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E9.m1.4.4.4f" xref="S4.E9.m1.4.5.3.1.cmml"><mtext id="S4.E9.m1.4.4.4.4.2.1" xref="S4.E9.m1.4.4.4.4.2.1a.cmml"> otherwise </mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E9.m1.4b"><apply id="S4.E9.m1.4.5.cmml" xref="S4.E9.m1.4.5"><eq id="S4.E9.m1.4.5.1.cmml" xref="S4.E9.m1.4.5.1"></eq><ci id="S4.E9.m1.4.5.2a.cmml" xref="S4.E9.m1.4.5.2"><mtext id="S4.E9.m1.4.5.2.cmml" xref="S4.E9.m1.4.5.2"> Acc. </mtext></ci><apply id="S4.E9.m1.4.5.3.1.cmml" xref="S4.E9.m1.4.4"><csymbol cd="latexml" id="S4.E9.m1.4.5.3.1.1.cmml" xref="S4.E9.m1.4.4.5">cases</csymbol><cn id="S4.E9.m1.1.1.1.1.1.1.cmml" type="integer" xref="S4.E9.m1.1.1.1.1.1.1">1</cn><apply id="S4.E9.m1.2.2.2.2.2.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1"><and id="S4.E9.m1.2.2.2.2.2.1a.cmml" xref="S4.E9.m1.2.2.2.2.2.1"></and><apply id="S4.E9.m1.2.2.2.2.2.1b.cmml" xref="S4.E9.m1.2.2.2.2.2.1"><lt id="S4.E9.m1.2.2.2.2.2.1.5.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5"></lt><apply id="S4.E9.m1.2.2.2.2.2.1.4.cmml" xref="S4.E9.m1.2.2.2.2.2.1.4"><times id="S4.E9.m1.2.2.2.2.2.1.4.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.4.1"></times><ci id="S4.E9.m1.2.2.2.2.2.1.4.2a.cmml" xref="S4.E9.m1.2.2.2.2.2.1.4.2"><mtext id="S4.E9.m1.2.2.2.2.2.1.4.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.4.2"> if </mtext></ci><ci id="S4.E9.m1.2.2.2.2.2.1.4.3.cmml" xref="S4.E9.m1.2.2.2.2.2.1.4.3">𝑑</ci><interval closure="open" id="S4.E9.m1.2.2.2.2.2.1.4.4.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.4.4.2"><apply id="S4.E9.m1.2.2.2.2.2.1.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.1"><ci id="S4.E9.m1.2.2.2.2.2.1.1.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.1.1">^</ci><ci id="S4.E9.m1.2.2.2.2.2.1.1.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.1.2">𝐑</ci></apply><ci id="S4.E9.m1.2.2.2.2.2.1.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.2">𝐑</ci></interval></apply><apply id="S4.E9.m1.2.2.2.2.2.1.6.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6"><times id="S4.E9.m1.2.2.2.2.2.1.6.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.1"></times><ci id="S4.E9.m1.2.2.2.2.2.1.6.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.2">𝜆</ci><ci id="S4.E9.m1.2.2.2.2.2.1.6.3a.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.3"><mtext id="S4.E9.m1.2.2.2.2.2.1.6.3.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.3"> and </mtext></ci><apply id="S4.E9.m1.2.2.2.2.2.1.6.4.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.4"><ci id="S4.E9.m1.2.2.2.2.2.1.6.4.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.4.1">^</ci><ci id="S4.E9.m1.2.2.2.2.2.1.6.4.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.4.2">𝑐</ci></apply></apply></apply><apply id="S4.E9.m1.2.2.2.2.2.1c.cmml" xref="S4.E9.m1.2.2.2.2.2.1"><eq id="S4.E9.m1.2.2.2.2.2.1.7.cmml" xref="S4.E9.m1.2.2.2.2.2.1.7"></eq><share href="https://arxiv.org/html/2403.18791v2#S4.E9.m1.2.2.2.2.2.1.6.cmml" id="S4.E9.m1.2.2.2.2.2.1d.cmml" xref="S4.E9.m1.2.2.2.2.2.1"></share><ci id="S4.E9.m1.2.2.2.2.2.1.8.cmml" xref="S4.E9.m1.2.2.2.2.2.1.8">𝑐</ci></apply></apply><cn id="S4.E9.m1.3.3.3.3.1.1.cmml" type="integer" xref="S4.E9.m1.3.3.3.3.1.1">0</cn><ci id="S4.E9.m1.4.4.4.4.2.1a.cmml" xref="S4.E9.m1.4.4.4.4.2.1"><mtext id="S4.E9.m1.4.4.4.4.2.1.cmml" xref="S4.E9.m1.4.4.4.4.2.1"> otherwise </mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E9.m1.4c">\text{ Acc. }=\begin{cases}1&amp;\text{ if }d(\hat{\mathbf{R}},\mathbf{R})&lt;\lambda%
\text{ and }\hat{c}=c\\
0&amp;\text{ otherwise }\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S4.E9.m1.4d">Acc. = { start_ROW start_CELL 1 end_CELL start_CELL if italic_d ( over^ start_ARG bold_R end_ARG , bold_R ) &lt; italic_λ and over^ start_ARG italic_c end_ARG = italic_c end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.SSS4.p1.5">Here, <math alttext="c" class="ltx_Math" display="inline" id="S4.SS1.SSS4.p1.4.m1.1"><semantics id="S4.SS1.SSS4.p1.4.m1.1a"><mi id="S4.SS1.SSS4.p1.4.m1.1.1" xref="S4.SS1.SSS4.p1.4.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.4.m1.1b"><ci id="S4.SS1.SSS4.p1.4.m1.1.1.cmml" xref="S4.SS1.SSS4.p1.4.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.4.m1.1c">c</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS4.p1.4.m1.1d">italic_c</annotation></semantics></math> denotes the ground truth class. Following template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, we use the Acc15 metric and the <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS1.SSS4.p1.5.m2.1"><semantics id="S4.SS1.SSS4.p1.5.m2.1a"><mi id="S4.SS1.SSS4.p1.5.m2.1.1" xref="S4.SS1.SSS4.p1.5.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.5.m2.1b"><ci id="S4.SS1.SSS4.p1.5.m2.1.1.cmml" xref="S4.SS1.SSS4.p1.5.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.5.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS4.p1.5.m2.1d">italic_λ</annotation></semantics></math> is set to 15.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p2">
<p class="ltx_p" id="S4.SS1.SSS4.p2.1">Because most of the objects in T-LESS are symmetric, we follow the protocol of previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> and use the recall under the Visible Surface Discrepancy (VSD) metric with a threshold of 0.3. Additionally, we predict the translation using the projective distance estimation of SSD-6D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, following the same approach as previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation study</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Timestep</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">As mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S3.SS2" title="3.2 Motivation: feature matters ‣ 3 Methodology ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, the diffusion model encodes more specific information at various timesteps.
To understand the impact of timestep,
we conduct a performance comparison by extracting features from the SD model at different timesteps.
We employ our vanilla aggregation network, Arch. (a), to conduct experiments on objects included in split #1 of both Seen LM and Seen O-LM to quickly identify the best timestep. We sampled eleven timesteps from 0 to 1000, evenly spaced.
The results on seen objects of split 1 of the LM and O-LM datasets are presented in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.F4" title="In 4.2.1 Timestep ‣ 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>. The results show that the accuracies on LM and O-LM decrease as the timestep increases. This is because the diffusion model assumes less noise in the input when the timestep is small, which matches the scenarios in our images.
Therefore, we set the timestep to 0 for the remaining experiments.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S4.F4.g1" src="x13.png" width="414"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.7.2.1" style="font-size:113%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.2.1" style="font-size:113%;">Ablation on timestep <math alttext="t" class="ltx_Math" display="inline" id="S4.F4.2.1.m1.1"><semantics id="S4.F4.2.1.m1.1b"><mi id="S4.F4.2.1.m1.1.1" xref="S4.F4.2.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.F4.2.1.m1.1c"><ci id="S4.F4.2.1.m1.1.1.cmml" xref="S4.F4.2.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.2.1.m1.1d">t</annotation><annotation encoding="application/x-llamapun" id="S4.F4.2.1.m1.1e">italic_t</annotation></semantics></math>. The accuracy is measured for the model with extracting features from Stable Diffusion at different timesteps on LM and O-LM datasets.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.2" style="width:433.6pt;height:145.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-131.2pt,44.1pt) scale(0.623016114348458,0.623016114348458) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.1.1.1.1" rowspan="2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.1.1.1.2" rowspan="2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.2.1">Backbone</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T1.2.1.1.1.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.3.1">Split #1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T1.2.1.1.1.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.4.1">Split #2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T1.2.1.1.1.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.5.1">Split #3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T1.2.1.1.1.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.6.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.2.2">
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.2.2.1" style="padding-left:2.5pt;padding-right:2.5pt;">Seen</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.2.2.2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.2.2.2.1">Unseen</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.2.2.3" style="padding-left:2.5pt;padding-right:2.5pt;">Seen</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.2.2.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.2.2.4.1">Unseen</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.2.2.5" style="padding-left:2.5pt;padding-right:2.5pt;">Seen</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.2.2.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.2.2.6.1">Unseen</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.2.2.7" style="padding-left:2.5pt;padding-right:2.5pt;">Seen</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.2.2.8" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.2.2.8.1">Unseen</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.1.3.3.1" style="padding-left:2.5pt;padding-right:2.5pt;">Balntas <span class="ltx_text ltx_font_italic" id="S4.T1.2.1.3.3.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.1.3.3.2" style="padding-left:2.5pt;padding-right:2.5pt;">Wohlhart <span class="ltx_text ltx_font_italic" id="S4.T1.2.1.3.3.2.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.3" style="padding-left:2.5pt;padding-right:2.5pt;">87.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.1.3.3.4" style="padding-left:2.5pt;padding-right:2.5pt;">13.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.5" style="padding-left:2.5pt;padding-right:2.5pt;">83.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.1.3.3.6" style="padding-left:2.5pt;padding-right:2.5pt;">15.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.7" style="padding-left:2.5pt;padding-right:2.5pt;">85.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.1.3.3.8" style="padding-left:2.5pt;padding-right:2.5pt;">18.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.9" style="padding-left:2.5pt;padding-right:2.5pt;">85.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.10" style="padding-left:2.5pt;padding-right:2.5pt;">15.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.4.4.1" style="padding-left:2.5pt;padding-right:2.5pt;">Wohlhart <span class="ltx_text ltx_font_italic" id="S4.T1.2.1.4.4.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.4.4.2" style="padding-left:2.5pt;padding-right:2.5pt;">Wohlhart <span class="ltx_text ltx_font_italic" id="S4.T1.2.1.4.4.2.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.3" style="padding-left:2.5pt;padding-right:2.5pt;">89.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.4.4.4" style="padding-left:2.5pt;padding-right:2.5pt;">14.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.5" style="padding-left:2.5pt;padding-right:2.5pt;">85.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.4.4.6" style="padding-left:2.5pt;padding-right:2.5pt;">16.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.7" style="padding-left:2.5pt;padding-right:2.5pt;">83.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.4.4.8" style="padding-left:2.5pt;padding-right:2.5pt;">16.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.9" style="padding-left:2.5pt;padding-right:2.5pt;">86.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.10" style="padding-left:2.5pt;padding-right:2.5pt;">16.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.5.5.1" style="padding-left:2.5pt;padding-right:2.5pt;">MPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.5.5.2" style="padding-left:2.5pt;padding-right:2.5pt;">MPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.3" style="padding-left:2.5pt;padding-right:2.5pt;">91.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.5.5.4" style="padding-left:2.5pt;padding-right:2.5pt;">36.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.5" style="padding-left:2.5pt;padding-right:2.5pt;">87.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.5.5.6" style="padding-left:2.5pt;padding-right:2.5pt;">39.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.7" style="padding-left:2.5pt;padding-right:2.5pt;">87.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.5.5.8" style="padding-left:2.5pt;padding-right:2.5pt;">39.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.9" style="padding-left:2.5pt;padding-right:2.5pt;">88.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.10" style="padding-left:2.5pt;padding-right:2.5pt;">38.3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.6.6.1" style="padding-left:2.5pt;padding-right:2.5pt;">Zhao <span class="ltx_text ltx_font_italic" id="S4.T1.2.1.6.6.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.6.6.2" style="padding-left:2.5pt;padding-right:2.5pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.3" style="padding-left:2.5pt;padding-right:2.5pt;">96.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.6.6.4" style="padding-left:2.5pt;padding-right:2.5pt;">87.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.5" style="padding-left:2.5pt;padding-right:2.5pt;">94.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.6.6.6" style="padding-left:2.5pt;padding-right:2.5pt;">76.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.7" style="padding-left:2.5pt;padding-right:2.5pt;">93.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.6.6.8" style="padding-left:2.5pt;padding-right:2.5pt;">73.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.9" style="padding-left:2.5pt;padding-right:2.5pt;">93.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.10" style="padding-left:2.5pt;padding-right:2.5pt;">79.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.7.7.1" style="padding-left:2.5pt;padding-right:2.5pt;">Template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.7.7.2" style="padding-left:2.5pt;padding-right:2.5pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.7.7.3" style="padding-left:2.5pt;padding-right:2.5pt;">99.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.7.7.4" style="padding-left:2.5pt;padding-right:2.5pt;">94.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.7.7.5" style="padding-left:2.5pt;padding-right:2.5pt;">99.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.7.7.6" style="padding-left:2.5pt;padding-right:2.5pt;">97.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.7.7.7" style="padding-left:2.5pt;padding-right:2.5pt;">99.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.7.7.8" style="padding-left:2.5pt;padding-right:2.5pt;">88.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.7.7.9" style="padding-left:2.5pt;padding-right:2.5pt;">99.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.7.7.10" style="padding-left:2.5pt;padding-right:2.5pt;">93.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.1.8.8.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (VA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.1.8.8.2" style="padding-left:2.5pt;padding-right:2.5pt;">SD-V1-5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.8.8.3" style="padding-left:2.5pt;padding-right:2.5pt;">99.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.1.8.8.4" style="padding-left:2.5pt;padding-right:2.5pt;">97.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.8.8.5" style="padding-left:2.5pt;padding-right:2.5pt;">99.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.1.8.8.6" style="padding-left:2.5pt;padding-right:2.5pt;">99.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.8.8.7" style="padding-left:2.5pt;padding-right:2.5pt;">99.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.1.8.8.8" style="padding-left:2.5pt;padding-right:2.5pt;">95.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.8.8.9" style="padding-left:2.5pt;padding-right:2.5pt;">99.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.8.8.10" style="padding-left:2.5pt;padding-right:2.5pt;">98.3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.9.9.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (NA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.9.9.2" style="padding-left:2.5pt;padding-right:2.5pt;">SD-V1-5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.9.9.3" style="padding-left:2.5pt;padding-right:2.5pt;">99.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.9.9.4" style="padding-left:2.5pt;padding-right:2.5pt;">97.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.9.9.5" style="padding-left:2.5pt;padding-right:2.5pt;">99.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.9.9.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.9.9.6.1">99.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.9.9.7" style="padding-left:2.5pt;padding-right:2.5pt;">99.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.9.9.8" style="padding-left:2.5pt;padding-right:2.5pt;">96.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.9.9.9" style="padding-left:2.5pt;padding-right:2.5pt;">99.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.9.9.10" style="padding-left:2.5pt;padding-right:2.5pt;">98.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.10.10.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (CWA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.10.10.2" style="padding-left:2.5pt;padding-right:2.5pt;">SD-V1-5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.10.10.3" style="padding-left:2.5pt;padding-right:2.5pt;">99.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.10.10.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.10.10.4.1">98.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.10.10.5" style="padding-left:2.5pt;padding-right:2.5pt;">99.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.10.10.6" style="padding-left:2.5pt;padding-right:2.5pt;">98.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.10.10.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.10.10.7.1">99.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.10.10.8" style="padding-left:2.5pt;padding-right:2.5pt;">96.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.10.10.9" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.10.10.9.1">99.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.10.10.10" style="padding-left:2.5pt;padding-right:2.5pt;">98.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.1.11.11.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (CWA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.1.11.11.2" style="padding-left:2.5pt;padding-right:2.5pt;">SD-V2-0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.11.11.3" style="padding-left:2.5pt;padding-right:2.5pt;">99.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.1.11.11.4" style="padding-left:2.5pt;padding-right:2.5pt;">98.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.11.11.5" style="padding-left:2.5pt;padding-right:2.5pt;">99.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.1.11.11.6" style="padding-left:2.5pt;padding-right:2.5pt;">99.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.11.11.7" style="padding-left:2.5pt;padding-right:2.5pt;">99.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.1.11.11.8" style="padding-left:2.5pt;padding-right:2.5pt;">97.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.11.11.9" style="padding-left:2.5pt;padding-right:2.5pt;">99.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.11.11.10" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.11.11.10.1">98.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.12.12.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (CWA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.1.12.12.2" style="padding-left:2.5pt;padding-right:2.5pt;">OpenCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.12.12.3" style="padding-left:2.5pt;padding-right:2.5pt;">99.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.12.12.4" style="padding-left:2.5pt;padding-right:2.5pt;">97.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.12.12.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.12.12.5.1">99.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.12.12.6" style="padding-left:2.5pt;padding-right:2.5pt;">98.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.12.12.7" style="padding-left:2.5pt;padding-right:2.5pt;">99.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.1.12.12.8" style="padding-left:2.5pt;padding-right:2.5pt;">95.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.12.12.9" style="padding-left:2.5pt;padding-right:2.5pt;">99.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.12.12.10" style="padding-left:2.5pt;padding-right:2.5pt;">97.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T1.2.1.13.13.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (CWA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T1.2.1.13.13.2" style="padding-left:2.5pt;padding-right:2.5pt;">DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.1.13.13.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.13.13.3.1">99.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.2.1.13.13.4" style="padding-left:2.5pt;padding-right:2.5pt;">96.6</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.1.13.13.5" style="padding-left:2.5pt;padding-right:2.5pt;">99.6</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.2.1.13.13.6" style="padding-left:2.5pt;padding-right:2.5pt;">99.1</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.1.13.13.7" style="padding-left:2.5pt;padding-right:2.5pt;">99.5</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.2.1.13.13.8" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.13.13.8.1">97.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.1.13.13.9" style="padding-left:2.5pt;padding-right:2.5pt;">99.6</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.1.13.13.10" style="padding-left:2.5pt;padding-right:2.5pt;">97.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Results on LM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>. VA represents vanilla aggregation, NA represents nonlinear aggregation, and CWA represents context-aware weight aggregation.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:433.6pt;height:145.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-131.2pt,44.1pt) scale(0.623016114348458,0.623016114348458) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.1" rowspan="2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2" rowspan="2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.1">Backbone</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T2.2.1.1.1.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1">Split #1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T2.2.1.1.1.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.1">Split #2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T2.2.1.1.1.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.1">Split #3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.2.1.1.1.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.2.2">
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.2.2.1" style="padding-left:2.5pt;padding-right:2.5pt;">Seen</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.2.2.2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.2.2.2.1">Unseen</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.2.2.3" style="padding-left:2.5pt;padding-right:2.5pt;">Seen</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.2.2.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.2.2.4.1">Unseen</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.2.2.5" style="padding-left:2.5pt;padding-right:2.5pt;">Seen</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.2.2.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.2.2.6.1">Unseen</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.2.2.7" style="padding-left:2.5pt;padding-right:2.5pt;">Seen</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.2.2.8" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.2.2.8.1">Unseen</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.3.3.1" style="padding-left:2.5pt;padding-right:2.5pt;">Balntas <span class="ltx_text ltx_font_italic" id="S4.T2.2.1.3.3.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.3.3.2" style="padding-left:2.5pt;padding-right:2.5pt;">Wohlhart <span class="ltx_text ltx_font_italic" id="S4.T2.2.1.3.3.2.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.3" style="padding-left:2.5pt;padding-right:2.5pt;">19.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.3.3.4" style="padding-left:2.5pt;padding-right:2.5pt;">9.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.5" style="padding-left:2.5pt;padding-right:2.5pt;">23.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.3.3.6" style="padding-left:2.5pt;padding-right:2.5pt;">5.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.7" style="padding-left:2.5pt;padding-right:2.5pt;">15.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.3.3.8" style="padding-left:2.5pt;padding-right:2.5pt;">5.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.9" style="padding-left:2.5pt;padding-right:2.5pt;">19.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.10" style="padding-left:2.5pt;padding-right:2.5pt;">6.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.4.4.1" style="padding-left:2.5pt;padding-right:2.5pt;">Wohlhart <span class="ltx_text ltx_font_italic" id="S4.T2.2.1.4.4.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.4.4.2" style="padding-left:2.5pt;padding-right:2.5pt;">Wohlhart <span class="ltx_text ltx_font_italic" id="S4.T2.2.1.4.4.2.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.3" style="padding-left:2.5pt;padding-right:2.5pt;">18.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.4.4.4" style="padding-left:2.5pt;padding-right:2.5pt;">8.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.5" style="padding-left:2.5pt;padding-right:2.5pt;">21.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.4.4.6" style="padding-left:2.5pt;padding-right:2.5pt;">7.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.7" style="padding-left:2.5pt;padding-right:2.5pt;">17.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.4.4.8" style="padding-left:2.5pt;padding-right:2.5pt;">7.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.9" style="padding-left:2.5pt;padding-right:2.5pt;">19.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.10" style="padding-left:2.5pt;padding-right:2.5pt;">7.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.5.5.1" style="padding-left:2.5pt;padding-right:2.5pt;">MPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.5.5.2" style="padding-left:2.5pt;padding-right:2.5pt;">MPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.3" style="padding-left:2.5pt;padding-right:2.5pt;">31.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.5.5.4" style="padding-left:2.5pt;padding-right:2.5pt;">18.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.5" style="padding-left:2.5pt;padding-right:2.5pt;">34.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.5.5.6" style="padding-left:2.5pt;padding-right:2.5pt;">15.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.7" style="padding-left:2.5pt;padding-right:2.5pt;">29.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.5.5.8" style="padding-left:2.5pt;padding-right:2.5pt;">17.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.9" style="padding-left:2.5pt;padding-right:2.5pt;">31.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.10" style="padding-left:2.5pt;padding-right:2.5pt;">17.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.6.6.1" style="padding-left:2.5pt;padding-right:2.5pt;">Zhao <span class="ltx_text ltx_font_italic" id="S4.T2.2.1.6.6.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.6.6.2" style="padding-left:2.5pt;padding-right:2.5pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.3" style="padding-left:2.5pt;padding-right:2.5pt;">54.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.6.6.4" style="padding-left:2.5pt;padding-right:2.5pt;">40.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.5" style="padding-left:2.5pt;padding-right:2.5pt;">63.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.6.6.6" style="padding-left:2.5pt;padding-right:2.5pt;">32.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.7" style="padding-left:2.5pt;padding-right:2.5pt;">49.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.6.6.8" style="padding-left:2.5pt;padding-right:2.5pt;">37.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.9" style="padding-left:2.5pt;padding-right:2.5pt;">56.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.10" style="padding-left:2.5pt;padding-right:2.5pt;">36.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.7.7.1" style="padding-left:2.5pt;padding-right:2.5pt;">Template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.7.7.2" style="padding-left:2.5pt;padding-right:2.5pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.3" style="padding-left:2.5pt;padding-right:2.5pt;">77.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.7.7.4" style="padding-left:2.5pt;padding-right:2.5pt;">71.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.5" style="padding-left:2.5pt;padding-right:2.5pt;">84.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.7.7.6" style="padding-left:2.5pt;padding-right:2.5pt;">72.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.7" style="padding-left:2.5pt;padding-right:2.5pt;">76.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.7.7.8" style="padding-left:2.5pt;padding-right:2.5pt;">85.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.9" style="padding-left:2.5pt;padding-right:2.5pt;">79.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.10" style="padding-left:2.5pt;padding-right:2.5pt;">76.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.8.8.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (VA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.8.8.2" style="padding-left:2.5pt;padding-right:2.5pt;">SD-V1-5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.8.8.3" style="padding-left:2.5pt;padding-right:2.5pt;">80.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.8.8.4" style="padding-left:2.5pt;padding-right:2.5pt;">79.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.8.8.5" style="padding-left:2.5pt;padding-right:2.5pt;">87.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.8.8.6" style="padding-left:2.5pt;padding-right:2.5pt;">78.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.8.8.7" style="padding-left:2.5pt;padding-right:2.5pt;">80.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.8.8.8" style="padding-left:2.5pt;padding-right:2.5pt;">95.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.8.8.9" style="padding-left:2.5pt;padding-right:2.5pt;">82.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.8.8.10" style="padding-left:2.5pt;padding-right:2.5pt;">84.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.9.9.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (NA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.9.9.2" style="padding-left:2.5pt;padding-right:2.5pt;">SD-V1-5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.3" style="padding-left:2.5pt;padding-right:2.5pt;">81.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.9.9.4" style="padding-left:2.5pt;padding-right:2.5pt;">81.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.9.9.5.1">88.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.9.9.6" style="padding-left:2.5pt;padding-right:2.5pt;">79.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.7" style="padding-left:2.5pt;padding-right:2.5pt;">79.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.9.9.8" style="padding-left:2.5pt;padding-right:2.5pt;">96.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.9" style="padding-left:2.5pt;padding-right:2.5pt;">83.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.10" style="padding-left:2.5pt;padding-right:2.5pt;">85.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.10.10.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (CWA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.10.10.2" style="padding-left:2.5pt;padding-right:2.5pt;">SD-V1-5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.10.10.3" style="padding-left:2.5pt;padding-right:2.5pt;">81.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.10.10.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.10.10.4.1">81.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.10.10.5" style="padding-left:2.5pt;padding-right:2.5pt;">86.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.10.10.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.10.10.6.1">79.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.10.10.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.10.10.7.1">80.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.10.10.8" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.10.10.8.1">96.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.10.10.9" style="padding-left:2.5pt;padding-right:2.5pt;">82.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.10.10.10" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.10.10.10.1">85.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.11.11.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (CWA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.11.11.2" style="padding-left:2.5pt;padding-right:2.5pt;">SD-V2-0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.11.11.3" style="padding-left:2.5pt;padding-right:2.5pt;">82.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.11.11.4" style="padding-left:2.5pt;padding-right:2.5pt;">81.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.11.11.5" style="padding-left:2.5pt;padding-right:2.5pt;">88.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.11.11.6" style="padding-left:2.5pt;padding-right:2.5pt;">79.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.11.11.7" style="padding-left:2.5pt;padding-right:2.5pt;">79.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.11.11.8" style="padding-left:2.5pt;padding-right:2.5pt;">95.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.11.11.9" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.11.11.9.1">83.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.11.11.10" style="padding-left:2.5pt;padding-right:2.5pt;">85.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.12.12.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (CWA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.12.12.2" style="padding-left:2.5pt;padding-right:2.5pt;">OpenCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.12.12.3" style="padding-left:2.5pt;padding-right:2.5pt;">79.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.12.12.4" style="padding-left:2.5pt;padding-right:2.5pt;">76.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.12.12.5" style="padding-left:2.5pt;padding-right:2.5pt;">85.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.12.12.6" style="padding-left:2.5pt;padding-right:2.5pt;">73.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.12.12.7" style="padding-left:2.5pt;padding-right:2.5pt;">80.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.1.12.12.8" style="padding-left:2.5pt;padding-right:2.5pt;">91.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.12.12.9" style="padding-left:2.5pt;padding-right:2.5pt;">81.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.12.12.10" style="padding-left:2.5pt;padding-right:2.5pt;">81.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T2.2.1.13.13.1" style="padding-left:2.5pt;padding-right:2.5pt;">Ours (CWA)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T2.2.1.13.13.2" style="padding-left:2.5pt;padding-right:2.5pt;">DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.1.13.13.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.13.13.3.1">84.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.2.1.13.13.4" style="padding-left:2.5pt;padding-right:2.5pt;">73.7</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.1.13.13.5" style="padding-left:2.5pt;padding-right:2.5pt;">96.1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.2.1.13.13.6" style="padding-left:2.5pt;padding-right:2.5pt;">74.3</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.1.13.13.7" style="padding-left:2.5pt;padding-right:2.5pt;">79.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.2.1.13.13.8" style="padding-left:2.5pt;padding-right:2.5pt;">86.3</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.1.13.13.9" style="padding-left:2.5pt;padding-right:2.5pt;">83.1</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.1.13.13.10" style="padding-left:2.5pt;padding-right:2.5pt;">78.1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Results on O-LM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>. VA represents vanilla aggregation, NA represents nonlinear aggregation, and CWA represents context-aware weight aggregation.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel" id="S4.F5.2">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.F5.2.1" style="width:6.8pt;height:121.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:121.9pt;transform:translate(-57.51pt,-57.51pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.F5.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.F5.2.1.1.1">O-LM<span class="ltx_text ltx_font_medium" id="S4.F5.2.1.1.1.1"></span></span></p>
</span></div>
<figure class="ltx_figure" id="S4.F5.sf1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F5.sf1.3" style="width:44.7pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="70" id="S4.F5.sf1.1.g1" src="extracted/5636903/figures/qr/linemod/0/query.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf1.2.g2" src="extracted/5636903/figures/qr/linemod/8/query.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf1.3.g3" src="extracted/5636903/figures/qr/linemod/10/query.png" width="70"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.sf1.3.1"><span class="ltx_text" id="S4.F5.sf1.3.1.1" style="font-size:80%;">Query</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F5.sf1.6" style="width:44.7pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="70" id="S4.F5.sf1.4.g1" src="extracted/5636903/figures/qr/linemod/0/000238.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf1.5.g2" src="extracted/5636903/figures/qr/linemod/8/001428.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf1.6.g3" src="extracted/5636903/figures/qr/linemod/10/gt.png" width="70"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.sf1.6.1"><span class="ltx_text" id="S4.F5.sf1.6.1.1" style="font-size:80%;">GT</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F5.sf1.9" style="width:44.7pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="70" id="S4.F5.sf1.7.g1" src="extracted/5636903/figures/qr/linemod/0/ori.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf1.8.g2" src="extracted/5636903/figures/qr/linemod/8/ori.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf1.9.g3" src="extracted/5636903/figures/qr/linemod/10/ori.png" width="70"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.sf1.9.1"><span class="ltx_text" id="S4.F5.sf1.9.1.1" style="font-size:80%;">Template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite></span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F5.sf1.12" style="width:44.7pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="70" id="S4.F5.sf1.10.g1" src="extracted/5636903/figures/qr/linemod/0/diffusion.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf1.11.g2" src="extracted/5636903/figures/qr/linemod/8/diffusion.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf1.12.g3" src="extracted/5636903/figures/qr/linemod/10/diffusion.png" width="70"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.sf1.12.1"><span class="ltx_text" id="S4.F5.sf1.12.1.1" style="font-size:80%;">Ours</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf1.15.1.1" style="font-size:113%;">(a)</span> </span></figcaption>
</figure>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.F5.2.2" style="width:6.8pt;height:128.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:128.7pt;transform:translate(-60.94pt,-60.94pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.F5.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.F5.2.2.1.1">T-LESS<span class="ltx_text ltx_font_medium" id="S4.F5.2.2.1.1.1"></span></span></p>
</span></div>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F5.sf2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F5.sf2.3" style="width:44.7pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="70" id="S4.F5.sf2.1.g1" src="extracted/5636903/figures/qr/tless/19/query.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf2.2.g2" src="extracted/5636903/figures/qr/tless/21/query.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf2.3.g3" src="extracted/5636903/figures/qr/tless/25/query.png" width="70"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.sf2.3.1"><span class="ltx_text" id="S4.F5.sf2.3.1.1" style="font-size:80%;">Query</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F5.sf2.6" style="width:44.7pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="70" id="S4.F5.sf2.4.g1" src="extracted/5636903/figures/qr/tless/19/gt.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf2.5.g2" src="extracted/5636903/figures/qr/tless/21/gt.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf2.6.g3" src="extracted/5636903/figures/qr/tless/25/gt.png" width="70"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.sf2.6.1"><span class="ltx_text" id="S4.F5.sf2.6.1.1" style="font-size:80%;">GT</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F5.sf2.9" style="width:44.7pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="70" id="S4.F5.sf2.7.g1" src="extracted/5636903/figures/qr/tless/19/ori.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf2.8.g2" src="extracted/5636903/figures/qr/tless/21/ori.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf2.9.g3" src="extracted/5636903/figures/qr/tless/25/ori.png" width="70"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.sf2.9.1"><span class="ltx_text" id="S4.F5.sf2.9.1.1" style="font-size:80%;">Template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite></span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F5.sf2.12" style="width:44.7pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="70" id="S4.F5.sf2.10.g1" src="extracted/5636903/figures/qr/tless/19/diffusion.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf2.11.g2" src="extracted/5636903/figures/qr/tless/21/diffusion.png" width="70"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="70" id="S4.F5.sf2.12.g3" src="extracted/5636903/figures/qr/tless/25/diffusion.png" width="70"/>
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.sf2.12.1"><span class="ltx_text" id="S4.F5.sf2.12.1.1" style="font-size:80%;">Ours</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf2.15.1.1" style="font-size:113%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.5.1.1" style="font-size:113%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.6.2" style="font-size:113%;">Qualitative results on unseen objects of O-LM (left) and T-LESS (right). </span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">To evaluate the performance of the proposed aggregation networks, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.T1" title="In 4.2.1 Timestep ‣ 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.T2" title="In 4.2.1 Timestep ‣ 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>
compare these networks.
Notably, nearly all datasets achieve better results with the nonlinear aggregation network than with the vanilla aggregation network. This is because vanilla aggregation is not well-suited for adapting to downstream tasks. By introducing nonlinearity, the aggregation network can better fit the downstream task, resulting in improved performance.
Furthermore, we observe significant performance improvements, especially on unseen objects, when using the context-aware weight aggregator. This is due to the context-aware weight aggregator’s ability to adapt its aggregation weights to different inputs, improving the aggregation network’s ability to generalize to unseen data.
The results confirm that the proposed extractors and aggregators can significantly improve the performance of the aggregation network. Based on these experimental results, we select the context-aware weight aggregation network for the remaining experiments.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Other models pre-trained at large scale</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">In this section, we compare the features of other <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.1">big</em> models.
We first use another SD model, SDv2-0, which
differs from SDv1-5 in its text encoder, i.e. SDv1-5 with CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> and SDv2-0 with OpenCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>.
Same as SDv1-5, we set the timestep to 0 and extract features from all layers in the UNet. Furthermore, our method can be easily incorporated into other pre-trained large visual models. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.T1" title="In 4.2.1 Timestep ‣ 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.T2" title="In 4.2.1 Timestep ‣ 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we have also applied our aggregation network to other models, including DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> pre-trained on LVD-142M dataset and OpenCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> pre-trained on LAION <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> dataset. We use the best checkpoints available on their GitHub repositories, ViT-G for DINOv2 and OpenCLIP. Similar to Stable Diffusion, we extract features from all transformer blocks of these models. We aggregate the intermediate features using the context-aware aggregation network, which is the best-performing network we obtained. The results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.T1" title="In 4.2.1 Timestep ‣ 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.T2" title="In 4.2.1 Timestep ‣ 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. We can see that Stable Diffusion v1-5 achieves the best results in most of the benckmarks. This indicates that the intermediate features of the diffusion model have superior capabilities compared to the discriminative models for object pose estimation tasks.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison with the state-of-the-art</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>LINEMOD and Occluded-LINEMOD</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">We evaluate our method in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.T1" title="In 4.2.1 Timestep ‣ 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.T2" title="In 4.2.1 Timestep ‣ 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, our method achieves significantly better results compared to previous state-of-the-art methods
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>.
In particular, on the unseen O-LM test set, our model achieves an average accuracy 85.9%, outperforming template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> by 9.6%. This shows the superior generalization capability of our proposed solutions. Wohlhart <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.1.1">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> and Balntas <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.1.2">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> use a carefully designed network to learn feature embeddings with pose and class discriminative capabilities through the careful design of triplets and pairs loss functions. These methods use global representations and are more likely to fail when objects are occluded. Template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, which uses InfoNCE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> loss and local per-patch representations masked by template masks, achieves better performance in the presence of occlusion, but there is a large performance gap between seen asnd unseen objects, 99.1% on Seen LM and 93.5% Unseen LM. In contrast, our method significantly reduces this gap, achieving 99.7% on Seen LM and 98.2% Unseen LM. This means that our method generalizes well to unseen objects.
The strong generalizability shows the efficacy of our proposed solutions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>T-LESS</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">The T-LESS dataset consists of particularly challenging texture-less rigid objects in highly cluttered scenes. In <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.T3" title="In 4.3.2 T-LESS ‣ 4.3 Comparison with the state-of-the-art ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we present the results of our method compared to state-of-the-art approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. Our method outperforms all the methods in this comparison. Notably, our method achieves higher accuracy with fewer templates compared to template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, 71.03% vs. 58.87%. When template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> uses the same number of templates as our method, we outperform it by 11.73% for seen objects and 14.35% for unseen objects. This demonstrates the enhanced discriminative power of the features obtained from the diffusion model. Notably, our method can even achieve superior results for unseen objects compared to seen objects, highlighting the strong generalizability of our method.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.5" style="width:233.5pt;height:79.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.0pt,23.2pt) scale(0.632014995078645,0.632014995078645) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.5.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.5.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T3.5.5.6.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.6.1.1.1" style="font-size:80%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T3.5.5.6.1.2" rowspan="2"><span class="ltx_text" id="S4.T3.5.5.6.1.2.1" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.5.5.6.1.2.1.1">
<span class="ltx_tr" id="S4.T3.5.5.6.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.5.5.6.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.6.1.2.1.1.1.1.1">Number</span></span></span>
<span class="ltx_tr" id="S4.T3.5.5.6.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.5.5.6.1.2.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.6.1.2.1.1.2.1.1">templates</span></span></span>
</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T3.5.5.6.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.6.1.3.1" style="font-size:80%;">Recall VSD</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.7.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.7.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.7.2.1.1" style="font-size:80%;">Obj. 1-18</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.7.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.7.2.2.1" style="font-size:80%;">Obj. 19-30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.7.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.7.2.3.1" style="font-size:80%;">Avg</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.1.2">
<span class="ltx_text" id="S4.T3.1.1.1.2.1" style="font-size:80%;">Implicit </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.1.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a><span class="ltx_text" id="S4.T3.1.1.1.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.1.1"><math alttext="92\mathrm{~{}K}" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml"><mn id="S4.T3.1.1.1.1.m1.1.1.2" mathsize="80%" xref="S4.T3.1.1.1.1.m1.1.1.2.cmml">92</mn><mo id="S4.T3.1.1.1.1.m1.1.1.1" lspace="0.270em" xref="S4.T3.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.1.1.1.1.m1.1.1.3" mathsize="80%" mathvariant="normal" xref="S4.T3.1.1.1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"><times id="S4.T3.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1.1"></times><cn id="S4.T3.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.1.1.1.1.m1.1.1.2">92</cn><ci id="S4.T3.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.1.m1.1.1.3">K</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">92\mathrm{~{}K}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">92 roman_K</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.3"><span class="ltx_text" id="S4.T3.1.1.1.3.1" style="font-size:80%;">35.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.4"><span class="ltx_text" id="S4.T3.1.1.1.4.1" style="font-size:80%;">42.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.5"><span class="ltx_text" id="S4.T3.1.1.1.5.1" style="font-size:80%;">38.34</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.2.2.2">
<span class="ltx_text" id="S4.T3.2.2.2.2.1" style="font-size:80%;">MPL </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.2.2.2.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a><span class="ltx_text" id="S4.T3.2.2.2.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.2.2.2.1"><math alttext="92\mathrm{~{}K}" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.m1.1"><semantics id="S4.T3.2.2.2.1.m1.1a"><mrow id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml"><mn id="S4.T3.2.2.2.1.m1.1.1.2" mathsize="80%" xref="S4.T3.2.2.2.1.m1.1.1.2.cmml">92</mn><mo id="S4.T3.2.2.2.1.m1.1.1.1" lspace="0.270em" xref="S4.T3.2.2.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.2.2.2.1.m1.1.1.3" mathsize="80%" mathvariant="normal" xref="S4.T3.2.2.2.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><apply id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1"><times id="S4.T3.2.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.1"></times><cn id="S4.T3.2.2.2.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.2.2.2.1.m1.1.1.2">92</cn><ci id="S4.T3.2.2.2.1.m1.1.1.3.cmml" xref="S4.T3.2.2.2.1.m1.1.1.3">K</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">92\mathrm{~{}K}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.m1.1d">92 roman_K</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.3"><span class="ltx_text" id="S4.T3.2.2.2.3.1" style="font-size:80%;">35.25</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.4"><span class="ltx_text" id="S4.T3.2.2.2.4.1" style="font-size:80%;">33.17</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.5"><span class="ltx_text" id="S4.T3.2.2.2.5.1" style="font-size:80%;">34.42</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.3.3.2">
<span class="ltx_text" id="S4.T3.3.3.3.2.1" style="font-size:80%;">Template-pose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.3.3.3.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a><span class="ltx_text" id="S4.T3.3.3.3.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.3.3.3.1"><math alttext="92\mathrm{~{}K}" class="ltx_Math" display="inline" id="S4.T3.3.3.3.1.m1.1"><semantics id="S4.T3.3.3.3.1.m1.1a"><mrow id="S4.T3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.cmml"><mn id="S4.T3.3.3.3.1.m1.1.1.2" mathsize="80%" xref="S4.T3.3.3.3.1.m1.1.1.2.cmml">92</mn><mo id="S4.T3.3.3.3.1.m1.1.1.1" lspace="0.270em" xref="S4.T3.3.3.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.3.3.3.1.m1.1.1.3" mathsize="80%" mathvariant="normal" xref="S4.T3.3.3.3.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.m1.1b"><apply id="S4.T3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1"><times id="S4.T3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1.1"></times><cn id="S4.T3.3.3.3.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.3.3.3.1.m1.1.1.2">92</cn><ci id="S4.T3.3.3.3.1.m1.1.1.3.cmml" xref="S4.T3.3.3.3.1.m1.1.1.3">K</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.m1.1c">92\mathrm{~{}K}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.1.m1.1d">92 roman_K</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.3"><span class="ltx_text" id="S4.T3.3.3.3.3.1" style="font-size:80%;">59.62</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.4"><span class="ltx_text" id="S4.T3.3.3.3.4.1" style="font-size:80%;">57.75</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.5"><span class="ltx_text" id="S4.T3.3.3.3.5.1" style="font-size:80%;">58.87</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.4.4.4.2">
<span class="ltx_text" id="S4.T3.4.4.4.2.1" style="font-size:80%;">Template-pose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.4.4.4.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a><span class="ltx_text" id="S4.T3.4.4.4.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.4.4.4.1"><math alttext="21\mathrm{~{}K}" class="ltx_Math" display="inline" id="S4.T3.4.4.4.1.m1.1"><semantics id="S4.T3.4.4.4.1.m1.1a"><mrow id="S4.T3.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.1.m1.1.1.cmml"><mn id="S4.T3.4.4.4.1.m1.1.1.2" mathsize="80%" xref="S4.T3.4.4.4.1.m1.1.1.2.cmml">21</mn><mo id="S4.T3.4.4.4.1.m1.1.1.1" lspace="0.270em" xref="S4.T3.4.4.4.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.4.4.4.1.m1.1.1.3" mathsize="80%" mathvariant="normal" xref="S4.T3.4.4.4.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><apply id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1"><times id="S4.T3.4.4.4.1.m1.1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1.1"></times><cn id="S4.T3.4.4.4.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.4.4.4.1.m1.1.1.2">21</cn><ci id="S4.T3.4.4.4.1.m1.1.1.3.cmml" xref="S4.T3.4.4.4.1.m1.1.1.3">K</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">21\mathrm{~{}K}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.1.m1.1d">21 roman_K</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.4.3"><span class="ltx_text" id="S4.T3.4.4.4.3.1" style="font-size:80%;">59.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.4.4"><span class="ltx_text" id="S4.T3.4.4.4.4.1" style="font-size:80%;">56.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.4.5"><span class="ltx_text" id="S4.T3.4.4.4.5.1" style="font-size:80%;">58.25</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.5.5.5.2"><span class="ltx_text" id="S4.T3.5.5.5.2.1" style="font-size:80%;">Ours (CWA)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T3.5.5.5.1"><math alttext="21\mathrm{~{}K}" class="ltx_Math" display="inline" id="S4.T3.5.5.5.1.m1.1"><semantics id="S4.T3.5.5.5.1.m1.1a"><mrow id="S4.T3.5.5.5.1.m1.1.1" xref="S4.T3.5.5.5.1.m1.1.1.cmml"><mn id="S4.T3.5.5.5.1.m1.1.1.2" mathsize="80%" xref="S4.T3.5.5.5.1.m1.1.1.2.cmml">21</mn><mo id="S4.T3.5.5.5.1.m1.1.1.1" lspace="0.270em" xref="S4.T3.5.5.5.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.5.5.5.1.m1.1.1.3" mathsize="80%" mathvariant="normal" xref="S4.T3.5.5.5.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b"><apply id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1"><times id="S4.T3.5.5.5.1.m1.1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1.1"></times><cn id="S4.T3.5.5.5.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.5.5.5.1.m1.1.1.2">21</cn><ci id="S4.T3.5.5.5.1.m1.1.1.3.cmml" xref="S4.T3.5.5.5.1.m1.1.1.3">K</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">21\mathrm{~{}K}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.1.m1.1d">21 roman_K</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.5.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.5.3.1" style="font-size:80%;">70.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.5.4"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.5.4.1" style="font-size:80%;">71.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.5.5.1" style="font-size:80%;">71.03</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.12.1.1" style="font-size:113%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.13.2" style="font-size:113%;">Results on T-LESS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. </span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Visualization</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To better demonstrate the effectiveness of our aggregation method, we provide several qualitative results in <a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.F5" title="In 4.2.1 Timestep ‣ 4.2 Ablation study ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. We compare the estimated pose of our methods and the state-of-the-art method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. We find that our method performs better in challenging scenarios, e.g. objects with occlusions in T-LESS, showing the stronger generalizability and discriminative capability of our method.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Efficiency</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#S4.T4" title="In 4.5 Efficiency ‣ 4 Experiment ‣ Object Pose Estimation via the Aggregation of Diffusion Features"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a> displays the comparison of trainable parameter size with template-pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>.
The proposed method outperforms template-pose with superior performance and approximately half the trainable parameters.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.2.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.2.1.1.2">
<span class="ltx_text" id="S4.T4.2.1.1.2.1" style="font-size:80%;">Template-pose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T4.2.1.1.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.18791v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a><span class="ltx_text" id="S4.T4.2.1.1.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.1.3"><span class="ltx_text" id="S4.T4.2.1.1.3.1" style="font-size:80%;">VA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.1.4"><span class="ltx_text" id="S4.T4.2.1.1.4.1" style="font-size:80%;">NA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.1.5"><span class="ltx_text" id="S4.T4.2.1.1.5.1" style="font-size:80%;">CWA</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.2.2.2.1"><span class="ltx_text" id="S4.T4.2.2.2.1.1" style="font-size:80%;">Params (M)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.2.2.2.2"><span class="ltx_text" id="S4.T4.2.2.2.2.1" style="font-size:80%;">24.04</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T4.2.2.2.3"><span class="ltx_text" id="S4.T4.2.2.2.3.1" style="font-size:80%;">2.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T4.2.2.2.4"><span class="ltx_text" id="S4.T4.2.2.2.4.1" style="font-size:80%;">13.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T4.2.2.2.5"><span class="ltx_text" id="S4.T4.2.2.2.5.1" style="font-size:80%;">13.29</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.5.1.1" style="font-size:113%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.6.2" style="font-size:113%;">Comparison of trainable parameter size.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, we conduct an analysis of inaccurate object pose estimation, particularly for unseen objects. Our findings identify insufficient feature generalization as the primary culprit for these inaccuracies. To address this challenge, we propose three novel aggregation networks specifically designed to effectively aggregate diffusion features, exhibiting superior generalizability for object pose estimation. We evaluate our method on three standard benchmark datasets, demonstrating superior performance and improved generalization to unseen objects compared to existing methods. We hope that our findings and proposed method will serve as a catalyst for further advancements in this field.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Acknowledgement</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">This work was funded in part by NSFC (No.52335003), and FRPSIA (Grant 2022JC3K06 and No.2023000479).</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Balntas et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Vassileios Balntas, Andreas Doumanoglou, Caner Sahin, Juil Sock, Rigas Kouskouridas, and Tae-Kyun Kim.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Pose guided rgbd feature learning for 3d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.10.2" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span class="ltx_text" id="bib.bib1.11.3" style="font-size:90%;">, pages 3856–3864, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Brachmann et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Learning 6d object pose estimation using 3d object coordinates.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.10.2" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part II 13</em><span class="ltx_text" id="bib.bib2.11.3" style="font-size:90%;">, pages 536–551. Springer, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Chen et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Hansheng Chen, Pichao Wang, Fan Wang, Wei Tian, Lu Xiong, and Hao Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Epro-pnp: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib3.11.3" style="font-size:90%;">, pages 2781–2790, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Collet et al. [2011]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Alvaro Collet, Manuel Martinez, and Siddhartha S Srinivasa.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">The moped framework: Object recognition and pose estimation for manipulation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.9.1" style="font-size:90%;">The international journal of robotics research</em><span class="ltx_text" id="bib.bib4.10.2" style="font-size:90%;">, 30(10):1284–1306, 2011.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Do et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
T Do, Trung Pham, Ming Cai, and Ian Reid.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Real-time monocular object instance 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.9.1" style="font-size:90%;">2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.4.4.1" style="font-size:90%;">Hartley and Zisserman [2003]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.6.1" style="font-size:90%;">
Richard Hartley and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.7.1" style="font-size:90%;">Multiple view geometry in computer vision</em><span class="ltx_text" id="bib.bib6.8.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.9.1" style="font-size:90%;">Cambridge university press, 2003.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.4.4.1" style="font-size:90%;">Haugaard and Buch [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.6.1" style="font-size:90%;">
Rasmus Laurvig Haugaard and Anders Glent Buch.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">Surfemb: Dense and continuous correspondence distributions for object pose estimation with learnt surface embeddings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.9.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib7.10.3" style="font-size:90%;">, pages 6749–6758, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">He et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">He et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">Momentum contrast for unsupervised visual representation learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib9.11.3" style="font-size:90%;">, pages 9729–9738, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Hinterstoisser et al. [2013]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary Bradski, Kurt Konolige, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.10.2" style="font-size:90%;">Computer Vision–ACCV 2012: 11th Asian Conference on Computer Vision, Daejeon, Korea, November 5-9, 2012, Revised Selected Papers, Part I 11</em><span class="ltx_text" id="bib.bib10.11.3" style="font-size:90%;">, pages 548–562. Springer, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Hodaň et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Tomáš Hodaň, Jiří Matas, and Štěpán Obdržálek.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">On evaluation of 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.10.2" style="font-size:90%;">Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14</em><span class="ltx_text" id="bib.bib11.11.3" style="font-size:90%;">, pages 606–619. Springer, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">Hodan et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Tomáš Hodan, Pavel Haluza, Štepán Obdržálek, Jiri Matas, Manolis Lourakis, and Xenophon Zabulis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">T-less: An rgb-d dataset for 6d pose estimation of texture-less objects.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.10.2" style="font-size:90%;">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</em><span class="ltx_text" id="bib.bib12.11.3" style="font-size:90%;">, pages 880–888. IEEE, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Hodan et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Tomas Hodan, Daniel Barath, and Jiri Matas.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">Epos: Estimating 6d pose of objects with symmetries.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib13.11.3" style="font-size:90%;">, pages 11703–11712, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Ilharco et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Openclip, 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.9.1" style="font-size:90%;">If you use this software, please cite it as below.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Kehl et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">, pages 1521–1529, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Labbé et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Yann Labbé, Justin Carpentier, Mathieu Aubry, and Josef Sivic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">Cosypose: Consistent multi-view multi-object 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.10.2" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16</em><span class="ltx_text" id="bib.bib16.11.3" style="font-size:90%;">, pages 574–591. Springer, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Lepetit et al. [2009]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Ep n p: An accurate o (n) solution to the p n p problem.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.9.1" style="font-size:90%;">International journal of computer vision</em><span class="ltx_text" id="bib.bib17.10.2" style="font-size:90%;">, 81:155–166, 2009.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Li et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">Deepim: Deep iterative matching for 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.10.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV)</em><span class="ltx_text" id="bib.bib18.11.3" style="font-size:90%;">, pages 683–698, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Li et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Zhigang Li, Gu Wang, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib19.11.3" style="font-size:90%;">, pages 7678–7687, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Li et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Guiding text-to-image diffusion model towards grounded generation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.9.1" style="font-size:90%;">arXiv preprint arXiv:2301.05221</em><span class="ltx_text" id="bib.bib20.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Luo et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">Diffusion hyperfeatures: Searching through time and space for semantic correspondence.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.14334</em><span class="ltx_text" id="bib.bib21.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Manhardt et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Fabian Manhardt, Diego Martin Arroyo, Christian Rupprecht, Benjamin Busam, Tolga Birdal, Nassir Navab, and Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Explaining the ambiguity of object detection and 6d pose from visual data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib22.11.3" style="font-size:90%;">, pages 6841–6850, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Marchand et al. [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Eric Marchand, Hideaki Uchiyama, and Fabien Spindler.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Pose estimation for augmented reality: a hands-on survey.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.9.1" style="font-size:90%;">IEEE transactions on visualization and computer graphics</em><span class="ltx_text" id="bib.bib23.10.2" style="font-size:90%;">, 22(12):2633–2651, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Nguyen et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Van Nguyen Nguyen, Yinlin Hu, Yang Xiao, Mathieu Salzmann, and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Templates for 3d object pose estimation revisited: Generalization to new objects and robustness to occlusions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib24.11.3" style="font-size:90%;">, pages 6771–6780, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Oord et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">Representation learning with contrastive predictive coding.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.9.1" style="font-size:90%;">arXiv preprint arXiv:1807.03748</em><span class="ltx_text" id="bib.bib25.10.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Oquab et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">Dinov2: Learning robust visual features without supervision.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.07193</em><span class="ltx_text" id="bib.bib26.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Park et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Kiru Park, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib27.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib27.11.3" style="font-size:90%;">, pages 7668–7677, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Peng et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">Pvnet: Pixel-wise voting network for 6dof pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib28.11.3" style="font-size:90%;">, pages 4561–4570, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.4.4.1" style="font-size:90%;">Rad and Lepetit [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.6.1" style="font-size:90%;">
Mahdi Rad and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib29.9.2" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span class="ltx_text" id="bib.bib29.10.3" style="font-size:90%;">, pages 3828–3836, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Radford et al. [2021a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib30.10.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib30.11.3" style="font-size:90%;">, 2021a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Radford et al. [2021b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib31.10.2" style="font-size:90%;">International conference on machine learning</em><span class="ltx_text" id="bib.bib31.11.3" style="font-size:90%;">, pages 8748–8763. PMLR, 2021b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Rombach et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib32.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib32.11.3" style="font-size:90%;">, pages 10684–10695, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Schuhmann et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">Laion-5b: An open large-scale dataset for training next generation image-text models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib33.10.2" style="font-size:90%;">, 35:25278–25294, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Song et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Jiaming Song, Chenlin Meng, and Stefano Ermon.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Denoising diffusion implicit models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.9.1" style="font-size:90%;">arXiv preprint arXiv:2010.02502</em><span class="ltx_text" id="bib.bib34.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Sundermeyer et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Implicit 3d orientation learning for 6d object detection from rgb images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.2" style="font-size:90%;">Proceedings of the european conference on computer vision (ECCV)</em><span class="ltx_text" id="bib.bib35.11.3" style="font-size:90%;">, pages 699–715, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Sundermeyer et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Martin Sundermeyer, Maximilian Durner, En Yen Puang, Zoltan-Csaba Marton, Narunas Vaskevicius, Kai O Arras, and Rudolph Triebel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">Multi-path learning for object pose estimation across domains.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib36.11.3" style="font-size:90%;">, pages 13916–13925, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Tang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Emergent correspondence from image diffusion.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.9.1" style="font-size:90%;">arXiv preprint arXiv:2306.03881</em><span class="ltx_text" id="bib.bib37.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Tremblay et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Deep object pose estimation for semantic robotic grasping of household objects.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.9.1" style="font-size:90%;">arXiv preprint arXiv:1809.10790</em><span class="ltx_text" id="bib.bib38.10.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Wang et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib39.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib39.11.3" style="font-size:90%;">, pages 16611–16621, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.4.4.1" style="font-size:90%;">Wohlhart and Lepetit [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.6.1" style="font-size:90%;">
Paul Wohlhart and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">Learning descriptors for object recognition and 3d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib40.9.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib40.10.3" style="font-size:90%;">, pages 3109–3118, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Xiang et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.9.1" style="font-size:90%;">arXiv preprint arXiv:1711.00199</em><span class="ltx_text" id="bib.bib41.10.2" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Xu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">Open-vocabulary panoptic segmentation with text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib42.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib42.11.3" style="font-size:90%;">, pages 2955–2966, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Zakharov et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Dpod: 6d pose object detector and refiner.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib43.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span class="ltx_text" id="bib.bib43.11.3" style="font-size:90%;">, pages 1941–1950, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">Zhan et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.8.1" style="font-size:90%;">What does stable diffusion know about the 3d scene?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.9.1" style="font-size:90%;">arXiv preprint arXiv:2310.06836</em><span class="ltx_text" id="bib.bib44.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">Adding conditional control to text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib45.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib45.11.3" style="font-size:90%;">, pages 3836–3847, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.5.5.1" style="font-size:90%;">Zhao et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">
Chen Zhao, Yinlin Hu, and Mathieu Salzmann.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.8.1" style="font-size:90%;">Fusing local similarities for retrieval-based 3d orientation estimation of unseen objects.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib46.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib46.11.3" style="font-size:90%;">, pages 106–122. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">Zhu et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
Menglong Zhu, Konstantinos G Derpanis, Yinfei Yang, Samarth Brahmbhatt, Mabel Zhang, Cody Phillips, Matthieu Lecce, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">Single image 3d object detection and pose estimation for grasping.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib47.10.2" style="font-size:90%;">2014 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib47.11.3" style="font-size:90%;">, pages 3936–3943. IEEE, 2014.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Jun  1 15:24:12 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
