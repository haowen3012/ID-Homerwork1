<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Marrying NeRF with Feature Matching for One-step Pose Estimation</title>
<!--Generated on Thu May  2 19:06:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.00891v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S1" title="In Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S2" title="In Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Works</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S2.SS1" title="In II Related Works â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Deep Learning Based Pose Estimation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S2.SS2" title="In II Related Works â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Render-and-compare Based Pose Estimation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S2.SS3" title="In II Related Works â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Keypoint-Matching-Based Pose Estimation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S3" title="In Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4" title="In Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4.SS1" title="In IV Method â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">One-step Pose Estimation via Feature Matching</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4.SS1.SSS1" title="In IV-A One-step Pose Estimation via Feature Matching â€£ IV Method â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>Matching</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4.SS1.SSS2" title="In IV-A One-step Pose Estimation via Feature Matching â€£ IV Method â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>Lifting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4.SS1.SSS3" title="In IV-A One-step Pose Estimation via Feature Matching â€£ IV Method â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>3 </span>PnP</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4.SS2" title="In IV Method â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">3D Consistent Point Mining</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4.SS3" title="In IV Method â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Keypoint-guided Occlusion Robust Refinement</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5" title="In Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS1" title="In V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Comparison Methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS2" title="In V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Results on Synthetic Dataset</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS2.SSS1" title="In V-B Results on Synthetic Dataset â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span>1 </span>Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS2.SSS2" title="In V-B Results on Synthetic Dataset â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span>2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS3" title="In V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Results on Real World Scene</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS3.SSS1" title="In V-C Results on Real World Scene â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>1 </span>Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS3.SSS2" title="In V-C Results on Real World Scene â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS4" title="In V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Results on Occluded Dataset</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS4.SSS1" title="In V-D Results on Occluded Dataset â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span>1 </span>Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS4.SSS2" title="In V-D Results on Occluded Dataset â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span>2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS5" title="In V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span> </span><span class="ltx_text ltx_font_italic">Efficiency</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.SS6" title="In V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-F</span> </span><span class="ltx_text ltx_font_italic">Ablation Studies</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S6" title="In Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Marrying NeRF with Feature Matching for One-step Pose Estimation
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_tabular ltx_align_middle" id="id1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="id1.1.1.1">
<span class="ltx_td ltx_align_center" id="id1.1.1.1.2">Ronghan Chen<sup class="ltx_sup" id="id1.1.1.1.2.1">1,2,3</sup></span>
<span class="ltx_td ltx_align_center" id="id1.1.1.1.1">Yang Cong<sup class="ltx_sup" id="id1.1.1.1.1.1">4</sup><sup class="ltx_sup" id="id1.1.1.1.1.2"><span class="ltx_text ltx_font_italic" id="id1.1.1.1.1.2.1">âˆ—</span></sup></span>
<span class="ltx_td ltx_align_center" id="id1.1.1.1.3">Yu Ren<sup class="ltx_sup" id="id1.1.1.1.3.1">1,2,3</sup></span></span>
</span>
</span>
<br class="ltx_break"/><sup class="ltx_sup" id="id5.2.id1">1</sup>State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences

<br class="ltx_break"/><sup class="ltx_sup" id="id6.3.id2">2</sup>Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences
<br class="ltx_break"/><sup class="ltx_sup" id="id7.4.id3">3</sup>University of Chinese Academy of Sciences
<br class="ltx_break"/><sup class="ltx_sup" id="id8.5.id4">4</sup>College of Automation Science and Engineering, South China University of Technology
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.6.id5" style="font-size:90%;">chenronghan@sia.cn, congyang81@gmail.com, renyu0414@gmail.com</span>
</span><span class="ltx_author_notes">This work is supported in part by the National Key Research and Development Program of China under Grant 2019YFB1310300 and the National Nature Science Foundation of China under Grant 61821005.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.1">Given the image collection of an object, we aim at building a real-time image-based pose estimation method, which requires neither its CAD model nor hours of object-specific training. Recent NeRF-based methods provide a promising solution by directly optimizing the pose from pixel loss between rendered and target images. However, during inference, they require long converging time, and suffer from local minima, making them impractical for real-time robot applications. We aim at solving this problem by marrying image matching with NeRF. With 2D matches and depth rendered by NeRF, we directly solve the pose in one step by building 2D-3D correspondences between target and initial view, thus allowing for real-time prediction. Moreover, to improve the accuracy of 2D-3D correspondences, we propose a 3D consistent point mining strategy, which effectively discards unfaithful points reconstruted by NeRF. Moreover, current NeRF-based methods naively optimizing pixel loss fail at occluded images. Thus, we further propose a 2D matches based sampling strategy to preclude the occluded area. Experimental results on representative datasets prove that our method outperforms state-of-the-art methods, and improves inference efficiency by 90<math alttext="\times" class="ltx_Math" display="inline" id="id2.1.m1.1"><semantics id="id2.1.m1.1a"><mo id="id2.1.m1.1.1" xref="id2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="id2.1.m1.1b"><times id="id2.1.m1.1.1.cmml" xref="id2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id2.1.m1.1d">Ã—</annotation></semantics></math>, achieving real-time prediction at 6 FPS.</p>
</div>
<div class="ltx_logical-block" id="id4">
<div class="ltx_para" id="id4.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="id3.g1" src=""/>
</div>
<figure class="ltx_figure ltx_align_center" id="S0.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.4.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.5.2" style="font-size:90%;">Given an object image with unknown pose, we propose a NeRF-based pose estimation method, which reduces the hundreds of optimization steps in former NeRF-based method to only <span class="ltx_text ltx_font_italic" id="S0.F1.5.2.1">one</span> step, while avoiding being stuck in local minima, and obtaining more accurate poses. As a result, with only 5 minutes training of a fast NeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib1" title="">1</a>]</cite>, our method achieves CAD model-free real-time pose estimation on <span class="ltx_text ltx_font_bold ltx_font_italic" id="S0.F1.5.2.2">novel</span> objects at 6FPS.</span></figcaption>
</figure>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>The corresponding author is Prof. Yang Cong. The work is supported in part by National Key R&amp;D Program of China under Grant 2023YFB4704800, and NSFC under Grant 62225310, 62127807.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Object pose estimation has wide applications in robot manipulation, augmented reality (AR) and mobile roboticsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib2" title="">2</a>]</cite>. Traditional methods typically require the CAD model of the object in advance, and searching for handcrafted featuresÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib4" title="">4</a>]</cite> between the preregistered images or templatesand the target image. However, obtaining such high-quality CAD model can be difficult and labor-intensitive, or requires specialized high-end scanners.
Recent methods have been applying deep neural network to regress the posesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib9" title="">9</a>]</cite>. However, they can only estimate poses of known instancesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib7" title="">7</a>]</cite> or similar ones from the same categoryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib11" title="">11</a>]</cite>, and have to retrain on novel objects for hours. Moreover, they require large amount of training data, which is tedious to collect and annotate. Thus, it is difficult to apply such methods in real world due to unaffordable training time and human labor.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To further avoid tedious retraining for each novel object, recent methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib13" title="">13</a>]</cite> learn from the traditional pipeline of SfM (Struction-from-Motion) to estimate object poses via feature matching. Given a small set of multi-view images, they first reconstruct sparse point cloud of the object via SfM, and then form 2D-3D correspondences to estimate the pose by solving the PnPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib14" title="">14</a>]</cite> problem. Unfortunately, such methods rely on forming stably repeatable correspondences across all input frames, which usually cannot be guaranteed, thus leading to large pose error.
On the other hand, recent advances in NeRF (Neural Radiance FieldsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib1" title="">1</a>]</cite>) provide a mechanism for capturing complex 3D geometry in a few minutes. Following former render-and-compare methods for pose estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib20" title="">20</a>]</cite>, iNeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite> first trains a NeRF from image collection, and then during testing, it optimizes the pose by minimizing dense pixel error between the rendered and target image. Such dense supervision allows iNeRF to achieve more accurate alignment, but it also requires hundreds of iterations taking minutes. Moreover, its convergence relies on good initialization, and typically fails at large pose differences or occlusion.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we try to combine the best of both worlds by marrying image matching with NeRF to achieve <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">real-time</span> image-based pose estimation, without hundred steps of optimization. With 2D pixel matches and corresponding depth rendered by NeRF, we can build 2D-3D correspondences, and directly solve the pose with PnPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib14" title="">14</a>]</cite>. This significantly reduces the iteration number and allows for real-time inference for NeRF based method. Moreover, comparing to former keypoint-based methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib13" title="">13</a>]</cite>, this eases the difficulty of building 2D-3D correspondences in traditional SFM-based methods, which needs to find 2D matches between multiple input frames and the target image. With NeRF, our method only matches between two images once, and can convert arbitrary 2D matches to 2D-3D correspondences by backprojecting NeRF rendered depth into 3D space.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Moreover, owing to the implicit nature of NeRF, the rendered depth can be noisy and unfaithfulÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib23" title="">23</a>]</cite>. To improve the quality of 2D-3D correspondences, we further propose a 3D consistent point mining strategy to discard unfaithful and noisy 3D points reconstructed by NeRF so that the PnP can obtain more accurate poses. Specifically, we render the 3D points from nearby viewpoints and regard the variation of them as the 3D consistency.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our method also allows for further pose refinement from pixel error, like former render-and-compare methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite>. However, this process is sensitive to occlusion, which bakpropagtes false gradient to the pose. We notice that the matching points indicate unoccluded area, and propose a matching point based sampling strategy for loss computation.
In experiments, we show that our proposed method improves the efficiency over former NeRF based methods by <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">90 times</span>, and can inference in real-time at 6FPS, while achieving higher pose accuracy and stronger robustness to occlusion.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our contributions are three folds:
1) An efficient NeRF based pose estimation method is proposed by introducing image matching, which allows real-time image-based inference, and is free of CAD model or hours of pretraining.
2) We propose a 3D consistent point mining strategy to detect and discard unfaithful points reconstructed by NeRF to enable more accurate pose estimation.
3) In contrast to former render-and-compare based methods, our method can overcome the occlusion problem with a matching point based sampling strategy.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Works</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Deep Learning Based Pose Estimation</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Recently, deep neural networks have led a series of breakthroughs for pose estimation. Some methods<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib24" title="">24</a>]</cite> focus on regressing the object pose directly. Some methods<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib27" title="">27</a>]</cite> train neural networks to build the 2D-3D correspondence first and then apply the PnP algorithm to compute the 6-DoF poses. OSOPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib28" title="">28</a>]</cite> proposes a one-shot method by first using a textured 3D template to match target image, and then solve the pose from dense 2D-3D correspondences constructing by image matching. Recently, some methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib11" title="">11</a>]</cite> leverage category-level representation to estimate both the pose and the scale of novel instances within the same category. Although great success has been made, they have to either obtain high-quality CAD models or spend expensive costs to collect and annotate large amount of data, severely limiting their application in the real world.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Render-and-compare Based Pose Estimation</span>
</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">Traditional</span> render and compare methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib30" title="">30</a>]</cite> first render the 3D CAD model and compare with input 2D images, and then minimize the error to optimize the pose. So they typically require high-quality 3D models in advance, which cannot be applied in our CAD-free setting. Though 3D models can be obtained from multi-view images via differentiable rendersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib18" title="">18</a>]</cite>, the reconstruction and rendering quality is limited, and may fail at complex real-world scenes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">NeRF-based.</span>
Neural Radiance FieldsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib15" title="">15</a>]</cite> provide a remedy for render-and-compare based strategy with its remarkable improvement in rendering quality. INeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite> first proposes to estimate the pose by inverting NeRF, <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.2">i.e.</span>, optimizing the pose from image difference. Though achieving accurate results, it requires hundreds of iterations, and struggles at converging to correct poses. To solve these problems, Loc-NeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib33" title="">33</a>]</cite> andÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>]</cite> propose to use Monte Carlo sampling to improve the efficiency and robustness to local minima. However, these methods still require optimization, thus cannot achieve real-time estimation. On the other hand, NeRF has been used in SLAM methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib36" title="">36</a>]</cite>, where the camera poses are required to be estimated. iMapÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib34" title="">34</a>]</cite> and NICE-SLAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib35" title="">35</a>]</cite> use depth captured by RGB-D camera to supervise the localization process. In contrast, our method aims at estimating pose from RGB images.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Keypoint-Matching-Based Pose Estimation</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Traditional methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib39" title="">39</a>]</cite> use hand-crafted features like SIFT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib40" title="">40</a>]</cite>, FAST<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib41" title="">41</a>]</cite> and ORB<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib42" title="">42</a>]</cite> to match interest points between training images and a pre-built 3D model to solve the pose. Nowadays, some methods introduce deep learning to improve accuracy of matches. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib44" title="">44</a>]</cite> train a classifier to distinguish inliers and outliers. SuperGlue<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib45" title="">45</a>]</cite> designs self- and cross-attention layers to enhance the exploration of features relations. Recently, OnePose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib12" title="">12</a>]</cite> assigns features to SfM reconstructed 3D points by aggregating image features from multiple training views, and directly matches with target images. Onepose++Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib13" title="">13</a>]</cite> later improves it with a keypoint-free reconstruction framework. However, they still rely on large training data to train feature aggregation networks limiting their application.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Background</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.14"><span class="ltx_text ltx_font_bold" id="S3.p1.14.1">NeRF.</span> Given multi-view images with annotated camera parameters, NeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib15" title="">15</a>]</cite> represents scenes via a 5D function:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{c},\sigma=\Phi(\mathbf{x},\mathbf{d})," class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.1.cmml"><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">ğœ</mi><mo id="S3.E1.m1.5.5.1.1.2.2.1" xref="S3.E1.m1.5.5.1.1.2.1.cmml">,</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">Ïƒ</mi></mrow><mo id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.3.2" mathvariant="normal" xref="S3.E1.m1.5.5.1.1.3.2.cmml">Î¦</mi><mo id="S3.E1.m1.5.5.1.1.3.1" xref="S3.E1.m1.5.5.1.1.3.1.cmml">â¢</mo><mrow id="S3.E1.m1.5.5.1.1.3.3.2" xref="S3.E1.m1.5.5.1.1.3.3.1.cmml"><mo id="S3.E1.m1.5.5.1.1.3.3.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.3.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">ğ±</mi><mo id="S3.E1.m1.5.5.1.1.3.3.2.2" xref="S3.E1.m1.5.5.1.1.3.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">ğ</mi><mo id="S3.E1.m1.5.5.1.1.3.3.2.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"></eq><list id="S3.E1.m1.5.5.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">ğœ</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">ğœ</ci></list><apply id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"><times id="S3.E1.m1.5.5.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.1"></times><ci id="S3.E1.m1.5.5.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2">Î¦</ci><interval closure="open" id="S3.E1.m1.5.5.1.1.3.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğ±</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ğ</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\mathbf{c},\sigma=\Phi(\mathbf{x},\mathbf{d}),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">bold_c , italic_Ïƒ = roman_Î¦ ( bold_x , bold_d ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p1.8">which maps the query point location <math alttext="\mathbf{x}\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">ğ±</mi><mo id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mi id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">â„</mi><mn id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><in id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></in><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">ğ±</ci><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">â„</ci><cn id="S3.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S3.p1.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathbf{x}\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">bold_x âˆˆ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> to its density <math alttext="\sigma\in\mathbb{R}^{1}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mrow id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">Ïƒ</mi><mo id="S3.p1.2.m2.1.1.1" xref="S3.p1.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml"><mi id="S3.p1.2.m2.1.1.3.2" xref="S3.p1.2.m2.1.1.3.2.cmml">â„</mi><mn id="S3.p1.2.m2.1.1.3.3" xref="S3.p1.2.m2.1.1.3.3.cmml">1</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><in id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1"></in><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ğœ</ci><apply id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.3.1.cmml" xref="S3.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.p1.2.m2.1.1.3.2.cmml" xref="S3.p1.2.m2.1.1.3.2">â„</ci><cn id="S3.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\sigma\in\mathbb{R}^{1}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_Ïƒ âˆˆ blackboard_R start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, and view-dependent color <math alttext="\mathbf{c}\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mrow id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">ğœ</mi><mo id="S3.p1.3.m3.1.1.1" xref="S3.p1.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml"><mi id="S3.p1.3.m3.1.1.3.2" xref="S3.p1.3.m3.1.1.3.2.cmml">â„</mi><mn id="S3.p1.3.m3.1.1.3.3" xref="S3.p1.3.m3.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><in id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1.1"></in><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">ğœ</ci><apply id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.3.1.cmml" xref="S3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.p1.3.m3.1.1.3.2.cmml" xref="S3.p1.3.m3.1.1.3.2">â„</ci><cn id="S3.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S3.p1.3.m3.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\mathbf{c}\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">bold_c âˆˆ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> at direction <math alttext="\mathbf{d}\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mrow id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">ğ</mi><mo id="S3.p1.4.m4.1.1.1" xref="S3.p1.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml"><mi id="S3.p1.4.m4.1.1.3.2" xref="S3.p1.4.m4.1.1.3.2.cmml">â„</mi><mn id="S3.p1.4.m4.1.1.3.3" xref="S3.p1.4.m4.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><in id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1"></in><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">ğ</ci><apply id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.3.1.cmml" xref="S3.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.p1.4.m4.1.1.3.2.cmml" xref="S3.p1.4.m4.1.1.3.2">â„</ci><cn id="S3.p1.4.m4.1.1.3.3.cmml" type="integer" xref="S3.p1.4.m4.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\mathbf{d}\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">bold_d âˆˆ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>. It reconstructs the scene implicitly, and is able to render freeview images. To render an image from view <math alttext="P" class="ltx_Math" display="inline" id="S3.p1.5.m5.1"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.1d">italic_P</annotation></semantics></math>,
the color <math alttext="\hat{C}(\mathbf{p},P)" class="ltx_Math" display="inline" id="S3.p1.6.m6.2"><semantics id="S3.p1.6.m6.2a"><mrow id="S3.p1.6.m6.2.3" xref="S3.p1.6.m6.2.3.cmml"><mover accent="true" id="S3.p1.6.m6.2.3.2" xref="S3.p1.6.m6.2.3.2.cmml"><mi id="S3.p1.6.m6.2.3.2.2" xref="S3.p1.6.m6.2.3.2.2.cmml">C</mi><mo id="S3.p1.6.m6.2.3.2.1" xref="S3.p1.6.m6.2.3.2.1.cmml">^</mo></mover><mo id="S3.p1.6.m6.2.3.1" xref="S3.p1.6.m6.2.3.1.cmml">â¢</mo><mrow id="S3.p1.6.m6.2.3.3.2" xref="S3.p1.6.m6.2.3.3.1.cmml"><mo id="S3.p1.6.m6.2.3.3.2.1" stretchy="false" xref="S3.p1.6.m6.2.3.3.1.cmml">(</mo><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">ğ©</mi><mo id="S3.p1.6.m6.2.3.3.2.2" xref="S3.p1.6.m6.2.3.3.1.cmml">,</mo><mi id="S3.p1.6.m6.2.2" xref="S3.p1.6.m6.2.2.cmml">P</mi><mo id="S3.p1.6.m6.2.3.3.2.3" stretchy="false" xref="S3.p1.6.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.2b"><apply id="S3.p1.6.m6.2.3.cmml" xref="S3.p1.6.m6.2.3"><times id="S3.p1.6.m6.2.3.1.cmml" xref="S3.p1.6.m6.2.3.1"></times><apply id="S3.p1.6.m6.2.3.2.cmml" xref="S3.p1.6.m6.2.3.2"><ci id="S3.p1.6.m6.2.3.2.1.cmml" xref="S3.p1.6.m6.2.3.2.1">^</ci><ci id="S3.p1.6.m6.2.3.2.2.cmml" xref="S3.p1.6.m6.2.3.2.2">ğ¶</ci></apply><interval closure="open" id="S3.p1.6.m6.2.3.3.1.cmml" xref="S3.p1.6.m6.2.3.3.2"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">ğ©</ci><ci id="S3.p1.6.m6.2.2.cmml" xref="S3.p1.6.m6.2.2">ğ‘ƒ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.2c">\hat{C}(\mathbf{p},P)</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.2d">over^ start_ARG italic_C end_ARG ( bold_p , italic_P )</annotation></semantics></math> of a pixel <math alttext="\mathbf{p}\in\mathbb{R}^{2}" class="ltx_Math" display="inline" id="S3.p1.7.m7.1"><semantics id="S3.p1.7.m7.1a"><mrow id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml"><mi id="S3.p1.7.m7.1.1.2" xref="S3.p1.7.m7.1.1.2.cmml">ğ©</mi><mo id="S3.p1.7.m7.1.1.1" xref="S3.p1.7.m7.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.7.m7.1.1.3" xref="S3.p1.7.m7.1.1.3.cmml"><mi id="S3.p1.7.m7.1.1.3.2" xref="S3.p1.7.m7.1.1.3.2.cmml">â„</mi><mn id="S3.p1.7.m7.1.1.3.3" xref="S3.p1.7.m7.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><apply id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1"><in id="S3.p1.7.m7.1.1.1.cmml" xref="S3.p1.7.m7.1.1.1"></in><ci id="S3.p1.7.m7.1.1.2.cmml" xref="S3.p1.7.m7.1.1.2">ğ©</ci><apply id="S3.p1.7.m7.1.1.3.cmml" xref="S3.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.p1.7.m7.1.1.3.1.cmml" xref="S3.p1.7.m7.1.1.3">superscript</csymbol><ci id="S3.p1.7.m7.1.1.3.2.cmml" xref="S3.p1.7.m7.1.1.3.2">â„</ci><cn id="S3.p1.7.m7.1.1.3.3.cmml" type="integer" xref="S3.p1.7.m7.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">\mathbf{p}\in\mathbb{R}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m7.1d">bold_p âˆˆ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> is obtained by accumulating the color along rays <math alttext="\mathbf{r}" class="ltx_Math" display="inline" id="S3.p1.8.m8.1"><semantics id="S3.p1.8.m8.1a"><mi id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml">ğ«</mi><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><ci id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1">ğ«</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">\mathbf{r}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.8.m8.1d">bold_r</annotation></semantics></math> that passes the pixel, following the volume rendering techniqueÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib46" title="">46</a>]</cite>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{C}(\mathbf{p},P)=\sum_{i=i}^{N}\omega_{i}\mathbf{c}_{i}," class="ltx_Math" display="block" id="S3.E2.m1.3"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><mover accent="true" id="S3.E2.m1.3.3.1.1.2.2" xref="S3.E2.m1.3.3.1.1.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.2.cmml">C</mi><mo id="S3.E2.m1.3.3.1.1.2.2.1" xref="S3.E2.m1.3.3.1.1.2.2.1.cmml">^</mo></mover><mo id="S3.E2.m1.3.3.1.1.2.1" xref="S3.E2.m1.3.3.1.1.2.1.cmml">â¢</mo><mrow id="S3.E2.m1.3.3.1.1.2.3.2" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml"><mo id="S3.E2.m1.3.3.1.1.2.3.2.1" stretchy="false" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">ğ©</mi><mo id="S3.E2.m1.3.3.1.1.2.3.2.2" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">P</mi><mo id="S3.E2.m1.3.3.1.1.2.3.2.3" stretchy="false" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.1" rspace="0.111em" xref="S3.E2.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml"><munderover id="S3.E2.m1.3.3.1.1.3.1" xref="S3.E2.m1.3.3.1.1.3.1.cmml"><mo id="S3.E2.m1.3.3.1.1.3.1.2.2" movablelimits="false" xref="S3.E2.m1.3.3.1.1.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.3.3.1.1.3.1.2.3" xref="S3.E2.m1.3.3.1.1.3.1.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.1.2.3.2" xref="S3.E2.m1.3.3.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.3.1.2.3.1" xref="S3.E2.m1.3.3.1.1.3.1.2.3.1.cmml">=</mo><mi id="S3.E2.m1.3.3.1.1.3.1.2.3.3" xref="S3.E2.m1.3.3.1.1.3.1.2.3.3.cmml">i</mi></mrow><mi id="S3.E2.m1.3.3.1.1.3.1.3" xref="S3.E2.m1.3.3.1.1.3.1.3.cmml">N</mi></munderover><mrow id="S3.E2.m1.3.3.1.1.3.2" xref="S3.E2.m1.3.3.1.1.3.2.cmml"><msub id="S3.E2.m1.3.3.1.1.3.2.2" xref="S3.E2.m1.3.3.1.1.3.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.3.2.2.2" xref="S3.E2.m1.3.3.1.1.3.2.2.2.cmml">Ï‰</mi><mi id="S3.E2.m1.3.3.1.1.3.2.2.3" xref="S3.E2.m1.3.3.1.1.3.2.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.3.3.1.1.3.2.1" xref="S3.E2.m1.3.3.1.1.3.2.1.cmml">â¢</mo><msub id="S3.E2.m1.3.3.1.1.3.2.3" xref="S3.E2.m1.3.3.1.1.3.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.2.3.2" xref="S3.E2.m1.3.3.1.1.3.2.3.2.cmml">ğœ</mi><mi id="S3.E2.m1.3.3.1.1.3.2.3.3" xref="S3.E2.m1.3.3.1.1.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"></eq><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><times id="S3.E2.m1.3.3.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2.1"></times><apply id="S3.E2.m1.3.3.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2"><ci id="S3.E2.m1.3.3.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2.2.1">^</ci><ci id="S3.E2.m1.3.3.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2">ğ¶</ci></apply><interval closure="open" id="S3.E2.m1.3.3.1.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.2.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ğ©</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">ğ‘ƒ</ci></interval></apply><apply id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3"><apply id="S3.E2.m1.3.3.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.3.1.2.cmml" xref="S3.E2.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.3.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.1.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.3.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.1.2.3"><eq id="S3.E2.m1.3.3.1.1.3.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.3.1.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.1.2.3.2">ğ‘–</ci><ci id="S3.E2.m1.3.3.1.1.3.1.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.1.2.3.3">ğ‘–</ci></apply></apply><ci id="S3.E2.m1.3.3.1.1.3.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3.1.3">ğ‘</ci></apply><apply id="S3.E2.m1.3.3.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2"><times id="S3.E2.m1.3.3.1.1.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.2.1"></times><apply id="S3.E2.m1.3.3.1.1.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2.2">ğœ”</ci><ci id="S3.E2.m1.3.3.1.1.3.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2.3">ğ‘–</ci></apply><apply id="S3.E2.m1.3.3.1.1.3.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3.2">ğœ</ci><ci id="S3.E2.m1.3.3.1.1.3.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\hat{C}(\mathbf{p},P)=\sum_{i=i}^{N}\omega_{i}\mathbf{c}_{i},</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">over^ start_ARG italic_C end_ARG ( bold_p , italic_P ) = âˆ‘ start_POSTSUBSCRIPT italic_i = italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_Ï‰ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p1.12">where <math alttext="\omega_{i}=\sum_{i=i}^{N}T_{i}(1-\exp(\sigma_{i}\delta_{i}))" class="ltx_Math" display="inline" id="S3.p1.9.m1.2"><semantics id="S3.p1.9.m1.2a"><mrow id="S3.p1.9.m1.2.2" xref="S3.p1.9.m1.2.2.cmml"><msub id="S3.p1.9.m1.2.2.3" xref="S3.p1.9.m1.2.2.3.cmml"><mi id="S3.p1.9.m1.2.2.3.2" xref="S3.p1.9.m1.2.2.3.2.cmml">Ï‰</mi><mi id="S3.p1.9.m1.2.2.3.3" xref="S3.p1.9.m1.2.2.3.3.cmml">i</mi></msub><mo id="S3.p1.9.m1.2.2.2" rspace="0.111em" xref="S3.p1.9.m1.2.2.2.cmml">=</mo><mrow id="S3.p1.9.m1.2.2.1" xref="S3.p1.9.m1.2.2.1.cmml"><msubsup id="S3.p1.9.m1.2.2.1.2" xref="S3.p1.9.m1.2.2.1.2.cmml"><mo id="S3.p1.9.m1.2.2.1.2.2.2" xref="S3.p1.9.m1.2.2.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.p1.9.m1.2.2.1.2.2.3" xref="S3.p1.9.m1.2.2.1.2.2.3.cmml"><mi id="S3.p1.9.m1.2.2.1.2.2.3.2" xref="S3.p1.9.m1.2.2.1.2.2.3.2.cmml">i</mi><mo id="S3.p1.9.m1.2.2.1.2.2.3.1" xref="S3.p1.9.m1.2.2.1.2.2.3.1.cmml">=</mo><mi id="S3.p1.9.m1.2.2.1.2.2.3.3" xref="S3.p1.9.m1.2.2.1.2.2.3.3.cmml">i</mi></mrow><mi id="S3.p1.9.m1.2.2.1.2.3" xref="S3.p1.9.m1.2.2.1.2.3.cmml">N</mi></msubsup><mrow id="S3.p1.9.m1.2.2.1.1" xref="S3.p1.9.m1.2.2.1.1.cmml"><msub id="S3.p1.9.m1.2.2.1.1.3" xref="S3.p1.9.m1.2.2.1.1.3.cmml"><mi id="S3.p1.9.m1.2.2.1.1.3.2" xref="S3.p1.9.m1.2.2.1.1.3.2.cmml">T</mi><mi id="S3.p1.9.m1.2.2.1.1.3.3" xref="S3.p1.9.m1.2.2.1.1.3.3.cmml">i</mi></msub><mo id="S3.p1.9.m1.2.2.1.1.2" xref="S3.p1.9.m1.2.2.1.1.2.cmml">â¢</mo><mrow id="S3.p1.9.m1.2.2.1.1.1.1" xref="S3.p1.9.m1.2.2.1.1.1.1.1.cmml"><mo id="S3.p1.9.m1.2.2.1.1.1.1.2" stretchy="false" xref="S3.p1.9.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.p1.9.m1.2.2.1.1.1.1.1" xref="S3.p1.9.m1.2.2.1.1.1.1.1.cmml"><mn id="S3.p1.9.m1.2.2.1.1.1.1.1.3" xref="S3.p1.9.m1.2.2.1.1.1.1.1.3.cmml">1</mn><mo id="S3.p1.9.m1.2.2.1.1.1.1.1.2" xref="S3.p1.9.m1.2.2.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.p1.9.m1.1.1" xref="S3.p1.9.m1.1.1.cmml">exp</mi><mo id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1a" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.2.cmml">â¡</mo><mrow id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.2.cmml"><mo id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">Ïƒ</mi><mi id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><msub id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml">Î´</mi><mi id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.3" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.p1.9.m1.2.2.1.1.1.1.3" stretchy="false" xref="S3.p1.9.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.9.m1.2b"><apply id="S3.p1.9.m1.2.2.cmml" xref="S3.p1.9.m1.2.2"><eq id="S3.p1.9.m1.2.2.2.cmml" xref="S3.p1.9.m1.2.2.2"></eq><apply id="S3.p1.9.m1.2.2.3.cmml" xref="S3.p1.9.m1.2.2.3"><csymbol cd="ambiguous" id="S3.p1.9.m1.2.2.3.1.cmml" xref="S3.p1.9.m1.2.2.3">subscript</csymbol><ci id="S3.p1.9.m1.2.2.3.2.cmml" xref="S3.p1.9.m1.2.2.3.2">ğœ”</ci><ci id="S3.p1.9.m1.2.2.3.3.cmml" xref="S3.p1.9.m1.2.2.3.3">ğ‘–</ci></apply><apply id="S3.p1.9.m1.2.2.1.cmml" xref="S3.p1.9.m1.2.2.1"><apply id="S3.p1.9.m1.2.2.1.2.cmml" xref="S3.p1.9.m1.2.2.1.2"><csymbol cd="ambiguous" id="S3.p1.9.m1.2.2.1.2.1.cmml" xref="S3.p1.9.m1.2.2.1.2">superscript</csymbol><apply id="S3.p1.9.m1.2.2.1.2.2.cmml" xref="S3.p1.9.m1.2.2.1.2"><csymbol cd="ambiguous" id="S3.p1.9.m1.2.2.1.2.2.1.cmml" xref="S3.p1.9.m1.2.2.1.2">subscript</csymbol><sum id="S3.p1.9.m1.2.2.1.2.2.2.cmml" xref="S3.p1.9.m1.2.2.1.2.2.2"></sum><apply id="S3.p1.9.m1.2.2.1.2.2.3.cmml" xref="S3.p1.9.m1.2.2.1.2.2.3"><eq id="S3.p1.9.m1.2.2.1.2.2.3.1.cmml" xref="S3.p1.9.m1.2.2.1.2.2.3.1"></eq><ci id="S3.p1.9.m1.2.2.1.2.2.3.2.cmml" xref="S3.p1.9.m1.2.2.1.2.2.3.2">ğ‘–</ci><ci id="S3.p1.9.m1.2.2.1.2.2.3.3.cmml" xref="S3.p1.9.m1.2.2.1.2.2.3.3">ğ‘–</ci></apply></apply><ci id="S3.p1.9.m1.2.2.1.2.3.cmml" xref="S3.p1.9.m1.2.2.1.2.3">ğ‘</ci></apply><apply id="S3.p1.9.m1.2.2.1.1.cmml" xref="S3.p1.9.m1.2.2.1.1"><times id="S3.p1.9.m1.2.2.1.1.2.cmml" xref="S3.p1.9.m1.2.2.1.1.2"></times><apply id="S3.p1.9.m1.2.2.1.1.3.cmml" xref="S3.p1.9.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.p1.9.m1.2.2.1.1.3.1.cmml" xref="S3.p1.9.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.p1.9.m1.2.2.1.1.3.2.cmml" xref="S3.p1.9.m1.2.2.1.1.3.2">ğ‘‡</ci><ci id="S3.p1.9.m1.2.2.1.1.3.3.cmml" xref="S3.p1.9.m1.2.2.1.1.3.3">ğ‘–</ci></apply><apply id="S3.p1.9.m1.2.2.1.1.1.1.1.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1"><minus id="S3.p1.9.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.2"></minus><cn id="S3.p1.9.m1.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S3.p1.9.m1.2.2.1.1.1.1.1.3">1</cn><apply id="S3.p1.9.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1"><exp id="S3.p1.9.m1.1.1.cmml" xref="S3.p1.9.m1.1.1"></exp><apply id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1"><times id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">ğœ</ci><ci id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.2">ğ›¿</ci><ci id="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.p1.9.m1.2.2.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m1.2c">\omega_{i}=\sum_{i=i}^{N}T_{i}(1-\exp(\sigma_{i}\delta_{i}))</annotation><annotation encoding="application/x-llamapun" id="S3.p1.9.m1.2d">italic_Ï‰ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = âˆ‘ start_POSTSUBSCRIPT italic_i = italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 1 - roman_exp ( italic_Ïƒ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_Î´ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) )</annotation></semantics></math> is the weight of each ray point, <math alttext="T_{i}=\exp(-\sum_{j=1}^{i-1}{\sigma_{j}\delta_{j}})" class="ltx_Math" display="inline" id="S3.p1.10.m2.2"><semantics id="S3.p1.10.m2.2a"><mrow id="S3.p1.10.m2.2.2" xref="S3.p1.10.m2.2.2.cmml"><msub id="S3.p1.10.m2.2.2.3" xref="S3.p1.10.m2.2.2.3.cmml"><mi id="S3.p1.10.m2.2.2.3.2" xref="S3.p1.10.m2.2.2.3.2.cmml">T</mi><mi id="S3.p1.10.m2.2.2.3.3" xref="S3.p1.10.m2.2.2.3.3.cmml">i</mi></msub><mo id="S3.p1.10.m2.2.2.2" xref="S3.p1.10.m2.2.2.2.cmml">=</mo><mrow id="S3.p1.10.m2.2.2.1.1" xref="S3.p1.10.m2.2.2.1.2.cmml"><mi id="S3.p1.10.m2.1.1" xref="S3.p1.10.m2.1.1.cmml">exp</mi><mo id="S3.p1.10.m2.2.2.1.1a" xref="S3.p1.10.m2.2.2.1.2.cmml">â¡</mo><mrow id="S3.p1.10.m2.2.2.1.1.1" xref="S3.p1.10.m2.2.2.1.2.cmml"><mo id="S3.p1.10.m2.2.2.1.1.1.2" stretchy="false" xref="S3.p1.10.m2.2.2.1.2.cmml">(</mo><mrow id="S3.p1.10.m2.2.2.1.1.1.1" xref="S3.p1.10.m2.2.2.1.1.1.1.cmml"><mo id="S3.p1.10.m2.2.2.1.1.1.1a" xref="S3.p1.10.m2.2.2.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.p1.10.m2.2.2.1.1.1.1.2" xref="S3.p1.10.m2.2.2.1.1.1.1.2.cmml"><msubsup id="S3.p1.10.m2.2.2.1.1.1.1.2.1" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.cmml"><mo id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.2" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.2.cmml">âˆ‘</mo><mrow id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.cmml"><mi id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.2" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.2.cmml">j</mi><mo id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.1" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.1.cmml">=</mo><mn id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.3" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.3.cmml">1</mn></mrow><mrow id="S3.p1.10.m2.2.2.1.1.1.1.2.1.3" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.cmml"><mi id="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.2" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.2.cmml">i</mi><mo id="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.1" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.1.cmml">âˆ’</mo><mn id="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.3" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.3.cmml">1</mn></mrow></msubsup><mrow id="S3.p1.10.m2.2.2.1.1.1.1.2.2" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.cmml"><msub id="S3.p1.10.m2.2.2.1.1.1.1.2.2.2" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.cmml"><mi id="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.2" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.2.cmml">Ïƒ</mi><mi id="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.3" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.3.cmml">j</mi></msub><mo id="S3.p1.10.m2.2.2.1.1.1.1.2.2.1" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.1.cmml">â¢</mo><msub id="S3.p1.10.m2.2.2.1.1.1.1.2.2.3" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.cmml"><mi id="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.2" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.2.cmml">Î´</mi><mi id="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.3" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.3.cmml">j</mi></msub></mrow></mrow></mrow><mo id="S3.p1.10.m2.2.2.1.1.1.3" stretchy="false" xref="S3.p1.10.m2.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.10.m2.2b"><apply id="S3.p1.10.m2.2.2.cmml" xref="S3.p1.10.m2.2.2"><eq id="S3.p1.10.m2.2.2.2.cmml" xref="S3.p1.10.m2.2.2.2"></eq><apply id="S3.p1.10.m2.2.2.3.cmml" xref="S3.p1.10.m2.2.2.3"><csymbol cd="ambiguous" id="S3.p1.10.m2.2.2.3.1.cmml" xref="S3.p1.10.m2.2.2.3">subscript</csymbol><ci id="S3.p1.10.m2.2.2.3.2.cmml" xref="S3.p1.10.m2.2.2.3.2">ğ‘‡</ci><ci id="S3.p1.10.m2.2.2.3.3.cmml" xref="S3.p1.10.m2.2.2.3.3">ğ‘–</ci></apply><apply id="S3.p1.10.m2.2.2.1.2.cmml" xref="S3.p1.10.m2.2.2.1.1"><exp id="S3.p1.10.m2.1.1.cmml" xref="S3.p1.10.m2.1.1"></exp><apply id="S3.p1.10.m2.2.2.1.1.1.1.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1"><minus id="S3.p1.10.m2.2.2.1.1.1.1.1.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1"></minus><apply id="S3.p1.10.m2.2.2.1.1.1.1.2.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2"><apply id="S3.p1.10.m2.2.2.1.1.1.1.2.1.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.p1.10.m2.2.2.1.1.1.1.2.1.1.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1">superscript</csymbol><apply id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.1.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1">subscript</csymbol><sum id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.2.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.2"></sum><apply id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3"><eq id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.1.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.1"></eq><ci id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.2.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.2">ğ‘—</ci><cn id="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.3.cmml" type="integer" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.2.3.3">1</cn></apply></apply><apply id="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.3"><minus id="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.1.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.1"></minus><ci id="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.2.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.2">ğ‘–</ci><cn id="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.3.cmml" type="integer" xref="S3.p1.10.m2.2.2.1.1.1.1.2.1.3.3">1</cn></apply></apply><apply id="S3.p1.10.m2.2.2.1.1.1.1.2.2.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2"><times id="S3.p1.10.m2.2.2.1.1.1.1.2.2.1.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.1"></times><apply id="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.1.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.2.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.2">ğœ</ci><ci id="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.3.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.2.3">ğ‘—</ci></apply><apply id="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.1.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.3">subscript</csymbol><ci id="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.2.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.2">ğ›¿</ci><ci id="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.3.cmml" xref="S3.p1.10.m2.2.2.1.1.1.1.2.2.3.3">ğ‘—</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m2.2c">T_{i}=\exp(-\sum_{j=1}^{i-1}{\sigma_{j}\delta_{j}})</annotation><annotation encoding="application/x-llamapun" id="S3.p1.10.m2.2d">italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_exp ( - âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT italic_Ïƒ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_Î´ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math>, and <math alttext="\delta_{i}" class="ltx_Math" display="inline" id="S3.p1.11.m3.1"><semantics id="S3.p1.11.m3.1a"><msub id="S3.p1.11.m3.1.1" xref="S3.p1.11.m3.1.1.cmml"><mi id="S3.p1.11.m3.1.1.2" xref="S3.p1.11.m3.1.1.2.cmml">Î´</mi><mi id="S3.p1.11.m3.1.1.3" xref="S3.p1.11.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.11.m3.1b"><apply id="S3.p1.11.m3.1.1.cmml" xref="S3.p1.11.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.11.m3.1.1.1.cmml" xref="S3.p1.11.m3.1.1">subscript</csymbol><ci id="S3.p1.11.m3.1.1.2.cmml" xref="S3.p1.11.m3.1.1.2">ğ›¿</ci><ci id="S3.p1.11.m3.1.1.3.cmml" xref="S3.p1.11.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m3.1c">\delta_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.11.m3.1d">italic_Î´ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the sample step along the ray. Similarly, we can also render an approximate depth at pixel <math alttext="\mathbf{p}" class="ltx_Math" display="inline" id="S3.p1.12.m4.1"><semantics id="S3.p1.12.m4.1a"><mi id="S3.p1.12.m4.1.1" xref="S3.p1.12.m4.1.1.cmml">ğ©</mi><annotation-xml encoding="MathML-Content" id="S3.p1.12.m4.1b"><ci id="S3.p1.12.m4.1.1.cmml" xref="S3.p1.12.m4.1.1">ğ©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.12.m4.1c">\mathbf{p}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.12.m4.1d">bold_p</annotation></semantics></math> by</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{z}(\mathbf{p},P)=\sum_{i=i}^{N}\omega_{i}t_{i}," class="ltx_Math" display="block" id="S3.E3.m1.3"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.2" xref="S3.E3.m1.3.3.1.1.2.cmml"><mover accent="true" id="S3.E3.m1.3.3.1.1.2.2" xref="S3.E3.m1.3.3.1.1.2.2.cmml"><mi id="S3.E3.m1.3.3.1.1.2.2.2" xref="S3.E3.m1.3.3.1.1.2.2.2.cmml">z</mi><mo id="S3.E3.m1.3.3.1.1.2.2.1" xref="S3.E3.m1.3.3.1.1.2.2.1.cmml">^</mo></mover><mo id="S3.E3.m1.3.3.1.1.2.1" xref="S3.E3.m1.3.3.1.1.2.1.cmml">â¢</mo><mrow id="S3.E3.m1.3.3.1.1.2.3.2" xref="S3.E3.m1.3.3.1.1.2.3.1.cmml"><mo id="S3.E3.m1.3.3.1.1.2.3.2.1" stretchy="false" xref="S3.E3.m1.3.3.1.1.2.3.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">ğ©</mi><mo id="S3.E3.m1.3.3.1.1.2.3.2.2" xref="S3.E3.m1.3.3.1.1.2.3.1.cmml">,</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">P</mi><mo id="S3.E3.m1.3.3.1.1.2.3.2.3" stretchy="false" xref="S3.E3.m1.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.1.1.1" rspace="0.111em" xref="S3.E3.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.1.3.cmml"><munderover id="S3.E3.m1.3.3.1.1.3.1" xref="S3.E3.m1.3.3.1.1.3.1.cmml"><mo id="S3.E3.m1.3.3.1.1.3.1.2.2" movablelimits="false" xref="S3.E3.m1.3.3.1.1.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.3.3.1.1.3.1.2.3" xref="S3.E3.m1.3.3.1.1.3.1.2.3.cmml"><mi id="S3.E3.m1.3.3.1.1.3.1.2.3.2" xref="S3.E3.m1.3.3.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.E3.m1.3.3.1.1.3.1.2.3.1" xref="S3.E3.m1.3.3.1.1.3.1.2.3.1.cmml">=</mo><mi id="S3.E3.m1.3.3.1.1.3.1.2.3.3" xref="S3.E3.m1.3.3.1.1.3.1.2.3.3.cmml">i</mi></mrow><mi id="S3.E3.m1.3.3.1.1.3.1.3" xref="S3.E3.m1.3.3.1.1.3.1.3.cmml">N</mi></munderover><mrow id="S3.E3.m1.3.3.1.1.3.2" xref="S3.E3.m1.3.3.1.1.3.2.cmml"><msub id="S3.E3.m1.3.3.1.1.3.2.2" xref="S3.E3.m1.3.3.1.1.3.2.2.cmml"><mi id="S3.E3.m1.3.3.1.1.3.2.2.2" xref="S3.E3.m1.3.3.1.1.3.2.2.2.cmml">Ï‰</mi><mi id="S3.E3.m1.3.3.1.1.3.2.2.3" xref="S3.E3.m1.3.3.1.1.3.2.2.3.cmml">i</mi></msub><mo id="S3.E3.m1.3.3.1.1.3.2.1" xref="S3.E3.m1.3.3.1.1.3.2.1.cmml">â¢</mo><msub id="S3.E3.m1.3.3.1.1.3.2.3" xref="S3.E3.m1.3.3.1.1.3.2.3.cmml"><mi id="S3.E3.m1.3.3.1.1.3.2.3.2" xref="S3.E3.m1.3.3.1.1.3.2.3.2.cmml">t</mi><mi id="S3.E3.m1.3.3.1.1.3.2.3.3" xref="S3.E3.m1.3.3.1.1.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><mo id="S3.E3.m1.3.3.1.2" xref="S3.E3.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1"><eq id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"></eq><apply id="S3.E3.m1.3.3.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.2"><times id="S3.E3.m1.3.3.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.2.1"></times><apply id="S3.E3.m1.3.3.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.2.2"><ci id="S3.E3.m1.3.3.1.1.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.2.2.1">^</ci><ci id="S3.E3.m1.3.3.1.1.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2">ğ‘§</ci></apply><interval closure="open" id="S3.E3.m1.3.3.1.1.2.3.1.cmml" xref="S3.E3.m1.3.3.1.1.2.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğ©</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">ğ‘ƒ</ci></interval></apply><apply id="S3.E3.m1.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3"><apply id="S3.E3.m1.3.3.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.1.1.cmml" xref="S3.E3.m1.3.3.1.1.3.1">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.3.1.2.cmml" xref="S3.E3.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.1">subscript</csymbol><sum id="S3.E3.m1.3.3.1.1.3.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.1.2.2"></sum><apply id="S3.E3.m1.3.3.1.1.3.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.3.1.2.3"><eq id="S3.E3.m1.3.3.1.1.3.1.2.3.1.cmml" xref="S3.E3.m1.3.3.1.1.3.1.2.3.1"></eq><ci id="S3.E3.m1.3.3.1.1.3.1.2.3.2.cmml" xref="S3.E3.m1.3.3.1.1.3.1.2.3.2">ğ‘–</ci><ci id="S3.E3.m1.3.3.1.1.3.1.2.3.3.cmml" xref="S3.E3.m1.3.3.1.1.3.1.2.3.3">ğ‘–</ci></apply></apply><ci id="S3.E3.m1.3.3.1.1.3.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3.1.3">ğ‘</ci></apply><apply id="S3.E3.m1.3.3.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2"><times id="S3.E3.m1.3.3.1.1.3.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.1"></times><apply id="S3.E3.m1.3.3.1.1.3.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.3.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2.2">ğœ”</ci><ci id="S3.E3.m1.3.3.1.1.3.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2.3">ğ‘–</ci></apply><apply id="S3.E3.m1.3.3.1.1.3.2.3.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.2.3.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.3.2.3.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2">ğ‘¡</ci><ci id="S3.E3.m1.3.3.1.1.3.2.3.3.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">\hat{z}(\mathbf{p},P)=\sum_{i=i}^{N}\omega_{i}t_{i},</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.3d">over^ start_ARG italic_z end_ARG ( bold_p , italic_P ) = âˆ‘ start_POSTSUBSCRIPT italic_i = italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_Ï‰ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p1.13">where <math alttext="t_{i}\in\mathbb{R}^{1}" class="ltx_Math" display="inline" id="S3.p1.13.m1.1"><semantics id="S3.p1.13.m1.1a"><mrow id="S3.p1.13.m1.1.1" xref="S3.p1.13.m1.1.1.cmml"><msub id="S3.p1.13.m1.1.1.2" xref="S3.p1.13.m1.1.1.2.cmml"><mi id="S3.p1.13.m1.1.1.2.2" xref="S3.p1.13.m1.1.1.2.2.cmml">t</mi><mi id="S3.p1.13.m1.1.1.2.3" xref="S3.p1.13.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.p1.13.m1.1.1.1" xref="S3.p1.13.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.13.m1.1.1.3" xref="S3.p1.13.m1.1.1.3.cmml"><mi id="S3.p1.13.m1.1.1.3.2" xref="S3.p1.13.m1.1.1.3.2.cmml">â„</mi><mn id="S3.p1.13.m1.1.1.3.3" xref="S3.p1.13.m1.1.1.3.3.cmml">1</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.13.m1.1b"><apply id="S3.p1.13.m1.1.1.cmml" xref="S3.p1.13.m1.1.1"><in id="S3.p1.13.m1.1.1.1.cmml" xref="S3.p1.13.m1.1.1.1"></in><apply id="S3.p1.13.m1.1.1.2.cmml" xref="S3.p1.13.m1.1.1.2"><csymbol cd="ambiguous" id="S3.p1.13.m1.1.1.2.1.cmml" xref="S3.p1.13.m1.1.1.2">subscript</csymbol><ci id="S3.p1.13.m1.1.1.2.2.cmml" xref="S3.p1.13.m1.1.1.2.2">ğ‘¡</ci><ci id="S3.p1.13.m1.1.1.2.3.cmml" xref="S3.p1.13.m1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.p1.13.m1.1.1.3.cmml" xref="S3.p1.13.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.13.m1.1.1.3.1.cmml" xref="S3.p1.13.m1.1.1.3">superscript</csymbol><ci id="S3.p1.13.m1.1.1.3.2.cmml" xref="S3.p1.13.m1.1.1.3.2">â„</ci><cn id="S3.p1.13.m1.1.1.3.3.cmml" type="integer" xref="S3.p1.13.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.13.m1.1c">t_{i}\in\mathbb{R}^{1}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.13.m1.1d">italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> is the depth at each ray point.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.6.3.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.4.2" style="font-size:90%;">Framework of the one-step pose estimation via feature matching strategy. Given the initial pose, we use NeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib1" title="">1</a>]</cite> to render an RGB image <math alttext="I_{r}" class="ltx_Math" display="inline" id="S3.F2.3.1.m1.1"><semantics id="S3.F2.3.1.m1.1b"><msub id="S3.F2.3.1.m1.1.1" xref="S3.F2.3.1.m1.1.1.cmml"><mi id="S3.F2.3.1.m1.1.1.2" xref="S3.F2.3.1.m1.1.1.2.cmml">I</mi><mi id="S3.F2.3.1.m1.1.1.3" xref="S3.F2.3.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.3.1.m1.1c"><apply id="S3.F2.3.1.m1.1.1.cmml" xref="S3.F2.3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.3.1.m1.1.1.1.cmml" xref="S3.F2.3.1.m1.1.1">subscript</csymbol><ci id="S3.F2.3.1.m1.1.1.2.cmml" xref="S3.F2.3.1.m1.1.1.2">ğ¼</ci><ci id="S3.F2.3.1.m1.1.1.3.cmml" xref="S3.F2.3.1.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.1.m1.1d">I_{r}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.3.1.m1.1e">italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>, and a depth image <math alttext="D" class="ltx_Math" display="inline" id="S3.F2.4.2.m2.1"><semantics id="S3.F2.4.2.m2.1b"><mi id="S3.F2.4.2.m2.1.1" xref="S3.F2.4.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.F2.4.2.m2.1c"><ci id="S3.F2.4.2.m2.1.1.cmml" xref="S3.F2.4.2.m2.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.2.m2.1d">D</annotation><annotation encoding="application/x-llamapun" id="S3.F2.4.2.m2.1e">italic_D</annotation></semantics></math>. Then, an off-the-shelf image matcherÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib47" title="">47</a>]</cite> is applied to generate 2D-2D matches between the rendered and target image. Given location of matched 2D points and its depth rendered by NeRF, the 3D coordinates can be obtained, thus forming 2D-3D matches, from which the pose is finally solved via PnP+RANSAC.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">InstantNGP.</span>
The original NeRF suffers from tediously lengthy training, and is infeasible to run in real time. InstantNGPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib1" title="">1</a>]</cite> improves the efficiency by decomposing the scene into a multi-resolution hash table and tiny MLP. It significantly reduces the training time to 5min, and allows for real-time rendering. For application in pose estimation, this makes fast online training of novel objects, and real-time inference possible. So we use it as default NeRF model.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.3"><span class="ltx_text ltx_font_bold" id="S3.p3.3.1">NeRF-based Pose Estimation.</span> INeRF first proposes to estimate the pose of a novel object with NeRF. It first trains a NeRF model <math alttext="\Phi" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" mathvariant="normal" xref="S3.p3.1.m1.1.1.cmml">Î¦</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">Î¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\Phi</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">roman_Î¦</annotation></semantics></math> with multi-view images of the object. Then, during inference, given a new target image <math alttext="I_{t}" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><msub id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">I</mi><mi id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">ğ¼</ci><ci id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">I_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, iNeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite> recovers the camera pose <math alttext="T\in SE(3)" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mrow id="S3.p3.3.m3.1.2" xref="S3.p3.3.m3.1.2.cmml"><mi id="S3.p3.3.m3.1.2.2" xref="S3.p3.3.m3.1.2.2.cmml">T</mi><mo id="S3.p3.3.m3.1.2.1" xref="S3.p3.3.m3.1.2.1.cmml">âˆˆ</mo><mrow id="S3.p3.3.m3.1.2.3" xref="S3.p3.3.m3.1.2.3.cmml"><mi id="S3.p3.3.m3.1.2.3.2" xref="S3.p3.3.m3.1.2.3.2.cmml">S</mi><mo id="S3.p3.3.m3.1.2.3.1" xref="S3.p3.3.m3.1.2.3.1.cmml">â¢</mo><mi id="S3.p3.3.m3.1.2.3.3" xref="S3.p3.3.m3.1.2.3.3.cmml">E</mi><mo id="S3.p3.3.m3.1.2.3.1a" xref="S3.p3.3.m3.1.2.3.1.cmml">â¢</mo><mrow id="S3.p3.3.m3.1.2.3.4.2" xref="S3.p3.3.m3.1.2.3.cmml"><mo id="S3.p3.3.m3.1.2.3.4.2.1" stretchy="false" xref="S3.p3.3.m3.1.2.3.cmml">(</mo><mn id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">3</mn><mo id="S3.p3.3.m3.1.2.3.4.2.2" stretchy="false" xref="S3.p3.3.m3.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.2.cmml" xref="S3.p3.3.m3.1.2"><in id="S3.p3.3.m3.1.2.1.cmml" xref="S3.p3.3.m3.1.2.1"></in><ci id="S3.p3.3.m3.1.2.2.cmml" xref="S3.p3.3.m3.1.2.2">ğ‘‡</ci><apply id="S3.p3.3.m3.1.2.3.cmml" xref="S3.p3.3.m3.1.2.3"><times id="S3.p3.3.m3.1.2.3.1.cmml" xref="S3.p3.3.m3.1.2.3.1"></times><ci id="S3.p3.3.m3.1.2.3.2.cmml" xref="S3.p3.3.m3.1.2.3.2">ğ‘†</ci><ci id="S3.p3.3.m3.1.2.3.3.cmml" xref="S3.p3.3.m3.1.2.3.3">ğ¸</ci><cn id="S3.p3.3.m3.1.1.cmml" type="integer" xref="S3.p3.3.m3.1.1">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">T\in SE(3)</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_T âˆˆ italic_S italic_E ( 3 )</annotation></semantics></math> by optimizing:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="~{}\hat{T}=\underset{T\in\operatorname{SE}(3)}{\operatorname{argmin}}\,\|\Phi(%
T)-I_{t}\|_{2}," class="ltx_Math" display="block" id="S3.E4.m1.4"><semantics id="S3.E4.m1.4a"><mrow id="S3.E4.m1.4.4.1" xref="S3.E4.m1.4.4.1.1.cmml"><mrow id="S3.E4.m1.4.4.1.1" xref="S3.E4.m1.4.4.1.1.cmml"><mover accent="true" id="S3.E4.m1.4.4.1.1.3" xref="S3.E4.m1.4.4.1.1.3.cmml"><mi id="S3.E4.m1.4.4.1.1.3.2" xref="S3.E4.m1.4.4.1.1.3.2.cmml">T</mi><mo id="S3.E4.m1.4.4.1.1.3.1" xref="S3.E4.m1.4.4.1.1.3.1.cmml">^</mo></mover><mo id="S3.E4.m1.4.4.1.1.2" xref="S3.E4.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.4.4.1.1.1" xref="S3.E4.m1.4.4.1.1.1.cmml"><munder accentunder="true" id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml"><mi id="S3.E4.m1.2.2.3" xref="S3.E4.m1.2.2.3.cmml">argmin</mi><mrow id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.2.cmml"><mi id="S3.E4.m1.2.2.2.4" xref="S3.E4.m1.2.2.2.4.cmml">T</mi><mo id="S3.E4.m1.2.2.2.3" xref="S3.E4.m1.2.2.2.3.cmml">âˆˆ</mo><mrow id="S3.E4.m1.2.2.2.5.2" xref="S3.E4.m1.2.2.2.5.1.cmml"><mi id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">SE</mi><mo id="S3.E4.m1.2.2.2.5.2a" xref="S3.E4.m1.2.2.2.5.1.cmml">â¡</mo><mrow id="S3.E4.m1.2.2.2.5.2.1" xref="S3.E4.m1.2.2.2.5.1.cmml"><mo id="S3.E4.m1.2.2.2.5.2.1.1" stretchy="false" xref="S3.E4.m1.2.2.2.5.1.cmml">(</mo><mn id="S3.E4.m1.2.2.2.2" xref="S3.E4.m1.2.2.2.2.cmml">3</mn><mo id="S3.E4.m1.2.2.2.5.2.1.2" stretchy="false" xref="S3.E4.m1.2.2.2.5.1.cmml">)</mo></mrow></mrow></mrow></munder><mo id="S3.E4.m1.4.4.1.1.1.2" xref="S3.E4.m1.4.4.1.1.1.2.cmml">â¢</mo><msub id="S3.E4.m1.4.4.1.1.1.1" xref="S3.E4.m1.4.4.1.1.1.1.cmml"><mrow id="S3.E4.m1.4.4.1.1.1.1.1.1" xref="S3.E4.m1.4.4.1.1.1.1.1.2.cmml"><mo id="S3.E4.m1.4.4.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.4.4.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E4.m1.4.4.1.1.1.1.1.1.1" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.4.4.1.1.1.1.1.1.1.2" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.2" mathvariant="normal" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.2.cmml">Î¦</mi><mo id="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.1" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">â¢</mo><mrow id="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.3.2.1" stretchy="false" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">T</mi><mo id="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.3.2.2" stretchy="false" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E4.m1.4.4.1.1.1.1.1.1.1.3" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.2.cmml">I</mi><mi id="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo id="S3.E4.m1.4.4.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.4.4.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E4.m1.4.4.1.1.1.1.3" xref="S3.E4.m1.4.4.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow><mo id="S3.E4.m1.4.4.1.2" xref="S3.E4.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.4b"><apply id="S3.E4.m1.4.4.1.1.cmml" xref="S3.E4.m1.4.4.1"><eq id="S3.E4.m1.4.4.1.1.2.cmml" xref="S3.E4.m1.4.4.1.1.2"></eq><apply id="S3.E4.m1.4.4.1.1.3.cmml" xref="S3.E4.m1.4.4.1.1.3"><ci id="S3.E4.m1.4.4.1.1.3.1.cmml" xref="S3.E4.m1.4.4.1.1.3.1">^</ci><ci id="S3.E4.m1.4.4.1.1.3.2.cmml" xref="S3.E4.m1.4.4.1.1.3.2">ğ‘‡</ci></apply><apply id="S3.E4.m1.4.4.1.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1"><times id="S3.E4.m1.4.4.1.1.1.2.cmml" xref="S3.E4.m1.4.4.1.1.1.2"></times><apply id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2"><apply id="S3.E4.m1.2.2.2.cmml" xref="S3.E4.m1.2.2.2"><in id="S3.E4.m1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.3"></in><ci id="S3.E4.m1.2.2.2.4.cmml" xref="S3.E4.m1.2.2.2.4">ğ‘‡</ci><apply id="S3.E4.m1.2.2.2.5.1.cmml" xref="S3.E4.m1.2.2.2.5.2"><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">SE</ci><cn id="S3.E4.m1.2.2.2.2.cmml" type="integer" xref="S3.E4.m1.2.2.2.2">3</cn></apply></apply><ci id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.3">argmin</ci></apply><apply id="S3.E4.m1.4.4.1.1.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.1.1.1.1.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1"><minus id="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.1"></minus><apply id="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.2"><times id="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.1"></times><ci id="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.2.2">Î¦</ci><ci id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">ğ‘‡</ci></apply><apply id="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.2">ğ¼</ci><ci id="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.3.3">ğ‘¡</ci></apply></apply></apply><cn id="S3.E4.m1.4.4.1.1.1.1.3.cmml" type="integer" xref="S3.E4.m1.4.4.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.4c">~{}\hat{T}=\underset{T\in\operatorname{SE}(3)}{\operatorname{argmin}}\,\|\Phi(%
T)-I_{t}\|_{2},</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.4d">over^ start_ARG italic_T end_ARG = start_UNDERACCENT italic_T âˆˆ roman_SE ( 3 ) end_UNDERACCENT start_ARG roman_argmin end_ARG âˆ¥ roman_Î¦ ( italic_T ) - italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p3.7">where <math alttext="\Phi(T)" class="ltx_Math" display="inline" id="S3.p3.4.m1.1"><semantics id="S3.p3.4.m1.1a"><mrow id="S3.p3.4.m1.1.2" xref="S3.p3.4.m1.1.2.cmml"><mi id="S3.p3.4.m1.1.2.2" mathvariant="normal" xref="S3.p3.4.m1.1.2.2.cmml">Î¦</mi><mo id="S3.p3.4.m1.1.2.1" xref="S3.p3.4.m1.1.2.1.cmml">â¢</mo><mrow id="S3.p3.4.m1.1.2.3.2" xref="S3.p3.4.m1.1.2.cmml"><mo id="S3.p3.4.m1.1.2.3.2.1" stretchy="false" xref="S3.p3.4.m1.1.2.cmml">(</mo><mi id="S3.p3.4.m1.1.1" xref="S3.p3.4.m1.1.1.cmml">T</mi><mo id="S3.p3.4.m1.1.2.3.2.2" stretchy="false" xref="S3.p3.4.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.4.m1.1b"><apply id="S3.p3.4.m1.1.2.cmml" xref="S3.p3.4.m1.1.2"><times id="S3.p3.4.m1.1.2.1.cmml" xref="S3.p3.4.m1.1.2.1"></times><ci id="S3.p3.4.m1.1.2.2.cmml" xref="S3.p3.4.m1.1.2.2">Î¦</ci><ci id="S3.p3.4.m1.1.1.cmml" xref="S3.p3.4.m1.1.1">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m1.1c">\Phi(T)</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m1.1d">roman_Î¦ ( italic_T )</annotation></semantics></math> denotes NeRF rendered image from view <math alttext="T" class="ltx_Math" display="inline" id="S3.p3.5.m2.1"><semantics id="S3.p3.5.m2.1a"><mi id="S3.p3.5.m2.1.1" xref="S3.p3.5.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.p3.5.m2.1b"><ci id="S3.p3.5.m2.1.1.cmml" xref="S3.p3.5.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.p3.5.m2.1d">italic_T</annotation></semantics></math>, and the function denotes an L2 loss between <math alttext="\Phi(T)" class="ltx_Math" display="inline" id="S3.p3.6.m3.1"><semantics id="S3.p3.6.m3.1a"><mrow id="S3.p3.6.m3.1.2" xref="S3.p3.6.m3.1.2.cmml"><mi id="S3.p3.6.m3.1.2.2" mathvariant="normal" xref="S3.p3.6.m3.1.2.2.cmml">Î¦</mi><mo id="S3.p3.6.m3.1.2.1" xref="S3.p3.6.m3.1.2.1.cmml">â¢</mo><mrow id="S3.p3.6.m3.1.2.3.2" xref="S3.p3.6.m3.1.2.cmml"><mo id="S3.p3.6.m3.1.2.3.2.1" stretchy="false" xref="S3.p3.6.m3.1.2.cmml">(</mo><mi id="S3.p3.6.m3.1.1" xref="S3.p3.6.m3.1.1.cmml">T</mi><mo id="S3.p3.6.m3.1.2.3.2.2" stretchy="false" xref="S3.p3.6.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.6.m3.1b"><apply id="S3.p3.6.m3.1.2.cmml" xref="S3.p3.6.m3.1.2"><times id="S3.p3.6.m3.1.2.1.cmml" xref="S3.p3.6.m3.1.2.1"></times><ci id="S3.p3.6.m3.1.2.2.cmml" xref="S3.p3.6.m3.1.2.2">Î¦</ci><ci id="S3.p3.6.m3.1.1.cmml" xref="S3.p3.6.m3.1.1">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m3.1c">\Phi(T)</annotation><annotation encoding="application/x-llamapun" id="S3.p3.6.m3.1d">roman_Î¦ ( italic_T )</annotation></semantics></math> and the target image <math alttext="I_{t}" class="ltx_Math" display="inline" id="S3.p3.7.m4.1"><semantics id="S3.p3.7.m4.1a"><msub id="S3.p3.7.m4.1.1" xref="S3.p3.7.m4.1.1.cmml"><mi id="S3.p3.7.m4.1.1.2" xref="S3.p3.7.m4.1.1.2.cmml">I</mi><mi id="S3.p3.7.m4.1.1.3" xref="S3.p3.7.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.7.m4.1b"><apply id="S3.p3.7.m4.1.1.cmml" xref="S3.p3.7.m4.1.1"><csymbol cd="ambiguous" id="S3.p3.7.m4.1.1.1.cmml" xref="S3.p3.7.m4.1.1">subscript</csymbol><ci id="S3.p3.7.m4.1.1.2.cmml" xref="S3.p3.7.m4.1.1.2">ğ¼</ci><ci id="S3.p3.7.m4.1.1.3.cmml" xref="S3.p3.7.m4.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.7.m4.1c">I_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.7.m4.1d">italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. The NeRF weights are fixed in optimization.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Method</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our method aims at improving the convergence speed of NeRF-based pose estimation method. The key insight is to marry feature matching with NeRF to directly solve the pose from 2D-3D correspondences via PnP, which we introduce inÂ <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4.SS1" title="IV-A One-step Pose Estimation via Feature Matching â€£ IV Method â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>. Moreover, owing to the implicit nature of NeRF, 3D coordinates lifted from 2D pixels can be noisy and unfaithful. Thus, in Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4.SS2" title="IV-B 3D Consistent Point Mining â€£ IV Method â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>, we improve the 3D consistency by introducing a <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">3D consistent point mining</span> strategy before solving the pose. So far, without any refinement, our result is already more accurate than iNeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite> in most cases, which needs hundreds steps of refinement. Our method also allows further optimization to refine the initial pose. However, we notice that current pixel error (Eq.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S3.E4" title="In III Background â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a>) cannot handle occluded images. For this, we propose a keypoint-guided occlusion robust refinement to tackle the occlusion problem, which is introduced in Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4.SS3" title="IV-C Keypoint-guided Occlusion Robust Refinement â€£ IV Method â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">One-step Pose Estimation via Feature Matching</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Optimizing the pose from the photometric loss between rendered and target image following the formulation of iNeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite> (Eq.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S3.E4" title="In III Background â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a>) can be extremely challenging, due to highly non-convex objective function. As a result, current methods are prone to being stuck in local minima. Here, we propose to estimate the pose by marrying image matching with NeRF. As shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S3.F2" title="Figure 2 â€£ III Background â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a>, the method has three main steps:</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.5.1.1">IV-A</span>1 </span>Matching</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.8">To estimate the pose of the target image <math alttext="I_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.1"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><msub id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml">I</mi><mi id="S4.SS1.SSS1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.2">ğ¼</ci><ci id="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">I_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.1.m1.1d">italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, we first render an image <math alttext="I_{r}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.2.m2.1"><semantics id="S4.SS1.SSS1.p1.2.m2.1a"><msub id="S4.SS1.SSS1.p1.2.m2.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS1.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.cmml">I</mi><mi id="S4.SS1.SSS1.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.2.m2.1b"><apply id="S4.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2">ğ¼</ci><ci id="S4.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.1c">I_{r}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> from the initial guess of camera pose <math alttext="P" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.3.m3.1"><semantics id="S4.SS1.SSS1.p1.3.m3.1a"><mi id="S4.SS1.SSS1.p1.3.m3.1.1" xref="S4.SS1.SSS1.p1.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.3.m3.1b"><ci id="S4.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.3.m3.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.3.m3.1d">italic_P</annotation></semantics></math> with the trained NeRF model. Then, a pretrained off-the-shelf image matching modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib48" title="">48</a>]</cite> is applied to form 2D-2D matches <math alttext="[\mathbf{q}_{i},\mathbf{p}_{i}]" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.4.m4.2"><semantics id="S4.SS1.SSS1.p1.4.m4.2a"><mrow id="S4.SS1.SSS1.p1.4.m4.2.2.2" xref="S4.SS1.SSS1.p1.4.m4.2.2.3.cmml"><mo id="S4.SS1.SSS1.p1.4.m4.2.2.2.3" stretchy="false" xref="S4.SS1.SSS1.p1.4.m4.2.2.3.cmml">[</mo><msub id="S4.SS1.SSS1.p1.4.m4.1.1.1.1" xref="S4.SS1.SSS1.p1.4.m4.1.1.1.1.cmml"><mi id="S4.SS1.SSS1.p1.4.m4.1.1.1.1.2" xref="S4.SS1.SSS1.p1.4.m4.1.1.1.1.2.cmml">ğª</mi><mi id="S4.SS1.SSS1.p1.4.m4.1.1.1.1.3" xref="S4.SS1.SSS1.p1.4.m4.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.SSS1.p1.4.m4.2.2.2.4" xref="S4.SS1.SSS1.p1.4.m4.2.2.3.cmml">,</mo><msub id="S4.SS1.SSS1.p1.4.m4.2.2.2.2" xref="S4.SS1.SSS1.p1.4.m4.2.2.2.2.cmml"><mi id="S4.SS1.SSS1.p1.4.m4.2.2.2.2.2" xref="S4.SS1.SSS1.p1.4.m4.2.2.2.2.2.cmml">ğ©</mi><mi id="S4.SS1.SSS1.p1.4.m4.2.2.2.2.3" xref="S4.SS1.SSS1.p1.4.m4.2.2.2.2.3.cmml">i</mi></msub><mo id="S4.SS1.SSS1.p1.4.m4.2.2.2.5" stretchy="false" xref="S4.SS1.SSS1.p1.4.m4.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.4.m4.2b"><interval closure="closed" id="S4.SS1.SSS1.p1.4.m4.2.2.3.cmml" xref="S4.SS1.SSS1.p1.4.m4.2.2.2"><apply id="S4.SS1.SSS1.p1.4.m4.1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.4.m4.1.1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.4.m4.1.1.1.1.2.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.1.1.2">ğª</ci><ci id="S4.SS1.SSS1.p1.4.m4.1.1.1.1.3.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.1.1.3">ğ‘–</ci></apply><apply id="S4.SS1.SSS1.p1.4.m4.2.2.2.2.cmml" xref="S4.SS1.SSS1.p1.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.4.m4.2.2.2.2.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.2.2.2.2">subscript</csymbol><ci id="S4.SS1.SSS1.p1.4.m4.2.2.2.2.2.cmml" xref="S4.SS1.SSS1.p1.4.m4.2.2.2.2.2">ğ©</ci><ci id="S4.SS1.SSS1.p1.4.m4.2.2.2.2.3.cmml" xref="S4.SS1.SSS1.p1.4.m4.2.2.2.2.3">ğ‘–</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.4.m4.2c">[\mathbf{q}_{i},\mathbf{p}_{i}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.4.m4.2d">[ bold_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]</annotation></semantics></math> between the target image <math alttext="I_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.5.m5.1"><semantics id="S4.SS1.SSS1.p1.5.m5.1a"><msub id="S4.SS1.SSS1.p1.5.m5.1.1" xref="S4.SS1.SSS1.p1.5.m5.1.1.cmml"><mi id="S4.SS1.SSS1.p1.5.m5.1.1.2" xref="S4.SS1.SSS1.p1.5.m5.1.1.2.cmml">I</mi><mi id="S4.SS1.SSS1.p1.5.m5.1.1.3" xref="S4.SS1.SSS1.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.5.m5.1b"><apply id="S4.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS1.p1.5.m5.1.1.2">ğ¼</ci><ci id="S4.SS1.SSS1.p1.5.m5.1.1.3.cmml" xref="S4.SS1.SSS1.p1.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.5.m5.1c">I_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.5.m5.1d">italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and the rendered image <math alttext="I_{r}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.6.m6.1"><semantics id="S4.SS1.SSS1.p1.6.m6.1a"><msub id="S4.SS1.SSS1.p1.6.m6.1.1" xref="S4.SS1.SSS1.p1.6.m6.1.1.cmml"><mi id="S4.SS1.SSS1.p1.6.m6.1.1.2" xref="S4.SS1.SSS1.p1.6.m6.1.1.2.cmml">I</mi><mi id="S4.SS1.SSS1.p1.6.m6.1.1.3" xref="S4.SS1.SSS1.p1.6.m6.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.6.m6.1b"><apply id="S4.SS1.SSS1.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.SSS1.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.SSS1.p1.6.m6.1.1.2">ğ¼</ci><ci id="S4.SS1.SSS1.p1.6.m6.1.1.3.cmml" xref="S4.SS1.SSS1.p1.6.m6.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.6.m6.1c">I_{r}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.6.m6.1d">italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>, with <math alttext="\mathbf{q}_{i}\in I_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.7.m7.1"><semantics id="S4.SS1.SSS1.p1.7.m7.1a"><mrow id="S4.SS1.SSS1.p1.7.m7.1.1" xref="S4.SS1.SSS1.p1.7.m7.1.1.cmml"><msub id="S4.SS1.SSS1.p1.7.m7.1.1.2" xref="S4.SS1.SSS1.p1.7.m7.1.1.2.cmml"><mi id="S4.SS1.SSS1.p1.7.m7.1.1.2.2" xref="S4.SS1.SSS1.p1.7.m7.1.1.2.2.cmml">ğª</mi><mi id="S4.SS1.SSS1.p1.7.m7.1.1.2.3" xref="S4.SS1.SSS1.p1.7.m7.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS1.SSS1.p1.7.m7.1.1.1" xref="S4.SS1.SSS1.p1.7.m7.1.1.1.cmml">âˆˆ</mo><msub id="S4.SS1.SSS1.p1.7.m7.1.1.3" xref="S4.SS1.SSS1.p1.7.m7.1.1.3.cmml"><mi id="S4.SS1.SSS1.p1.7.m7.1.1.3.2" xref="S4.SS1.SSS1.p1.7.m7.1.1.3.2.cmml">I</mi><mi id="S4.SS1.SSS1.p1.7.m7.1.1.3.3" xref="S4.SS1.SSS1.p1.7.m7.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.7.m7.1b"><apply id="S4.SS1.SSS1.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1"><in id="S4.SS1.SSS1.p1.7.m7.1.1.1.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1.1"></in><apply id="S4.SS1.SSS1.p1.7.m7.1.1.2.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.7.m7.1.1.2.1.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS1.p1.7.m7.1.1.2.2.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1.2.2">ğª</ci><ci id="S4.SS1.SSS1.p1.7.m7.1.1.2.3.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1.2.3">ğ‘–</ci></apply><apply id="S4.SS1.SSS1.p1.7.m7.1.1.3.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.7.m7.1.1.3.1.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1.3">subscript</csymbol><ci id="S4.SS1.SSS1.p1.7.m7.1.1.3.2.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1.3.2">ğ¼</ci><ci id="S4.SS1.SSS1.p1.7.m7.1.1.3.3.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.7.m7.1c">\mathbf{q}_{i}\in I_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.7.m7.1d">bold_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{p}_{i}\in I_{r}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.8.m8.1"><semantics id="S4.SS1.SSS1.p1.8.m8.1a"><mrow id="S4.SS1.SSS1.p1.8.m8.1.1" xref="S4.SS1.SSS1.p1.8.m8.1.1.cmml"><msub id="S4.SS1.SSS1.p1.8.m8.1.1.2" xref="S4.SS1.SSS1.p1.8.m8.1.1.2.cmml"><mi id="S4.SS1.SSS1.p1.8.m8.1.1.2.2" xref="S4.SS1.SSS1.p1.8.m8.1.1.2.2.cmml">ğ©</mi><mi id="S4.SS1.SSS1.p1.8.m8.1.1.2.3" xref="S4.SS1.SSS1.p1.8.m8.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS1.SSS1.p1.8.m8.1.1.1" xref="S4.SS1.SSS1.p1.8.m8.1.1.1.cmml">âˆˆ</mo><msub id="S4.SS1.SSS1.p1.8.m8.1.1.3" xref="S4.SS1.SSS1.p1.8.m8.1.1.3.cmml"><mi id="S4.SS1.SSS1.p1.8.m8.1.1.3.2" xref="S4.SS1.SSS1.p1.8.m8.1.1.3.2.cmml">I</mi><mi id="S4.SS1.SSS1.p1.8.m8.1.1.3.3" xref="S4.SS1.SSS1.p1.8.m8.1.1.3.3.cmml">r</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.8.m8.1b"><apply id="S4.SS1.SSS1.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1"><in id="S4.SS1.SSS1.p1.8.m8.1.1.1.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1.1"></in><apply id="S4.SS1.SSS1.p1.8.m8.1.1.2.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.8.m8.1.1.2.1.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS1.p1.8.m8.1.1.2.2.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1.2.2">ğ©</ci><ci id="S4.SS1.SSS1.p1.8.m8.1.1.2.3.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1.2.3">ğ‘–</ci></apply><apply id="S4.SS1.SSS1.p1.8.m8.1.1.3.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.8.m8.1.1.3.1.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1.3">subscript</csymbol><ci id="S4.SS1.SSS1.p1.8.m8.1.1.3.2.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1.3.2">ğ¼</ci><ci id="S4.SS1.SSS1.p1.8.m8.1.1.3.3.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1.3.3">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.8.m8.1c">\mathbf{p}_{i}\in I_{r}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.8.m8.1d">bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>. We apply the recent proposed transformer-based image matching method LoFTRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib47" title="">47</a>]</cite> in all our experiments.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.5.1.1">IV-A</span>2 </span>Lifting</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.10">We then convert 2D-2D matches <math alttext="[\mathbf{q}_{i},\mathbf{p}_{i}]" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1.2"><semantics id="S4.SS1.SSS2.p1.1.m1.2a"><mrow id="S4.SS1.SSS2.p1.1.m1.2.2.2" xref="S4.SS1.SSS2.p1.1.m1.2.2.3.cmml"><mo id="S4.SS1.SSS2.p1.1.m1.2.2.2.3" stretchy="false" xref="S4.SS1.SSS2.p1.1.m1.2.2.3.cmml">[</mo><msub id="S4.SS1.SSS2.p1.1.m1.1.1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.1.cmml"><mi id="S4.SS1.SSS2.p1.1.m1.1.1.1.1.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.1.2.cmml">ğª</mi><mi id="S4.SS1.SSS2.p1.1.m1.1.1.1.1.3" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.SSS2.p1.1.m1.2.2.2.4" xref="S4.SS1.SSS2.p1.1.m1.2.2.3.cmml">,</mo><msub id="S4.SS1.SSS2.p1.1.m1.2.2.2.2" xref="S4.SS1.SSS2.p1.1.m1.2.2.2.2.cmml"><mi id="S4.SS1.SSS2.p1.1.m1.2.2.2.2.2" xref="S4.SS1.SSS2.p1.1.m1.2.2.2.2.2.cmml">ğ©</mi><mi id="S4.SS1.SSS2.p1.1.m1.2.2.2.2.3" xref="S4.SS1.SSS2.p1.1.m1.2.2.2.2.3.cmml">i</mi></msub><mo id="S4.SS1.SSS2.p1.1.m1.2.2.2.5" stretchy="false" xref="S4.SS1.SSS2.p1.1.m1.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.2b"><interval closure="closed" id="S4.SS1.SSS2.p1.1.m1.2.2.3.cmml" xref="S4.SS1.SSS2.p1.1.m1.2.2.2"><apply id="S4.SS1.SSS2.p1.1.m1.1.1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p1.1.m1.1.1.1.1.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.1.2">ğª</ci><ci id="S4.SS1.SSS2.p1.1.m1.1.1.1.1.3.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S4.SS1.SSS2.p1.1.m1.2.2.2.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.1.m1.2.2.2.2.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.SS1.SSS2.p1.1.m1.2.2.2.2.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.2.2.2.2.2">ğ©</ci><ci id="S4.SS1.SSS2.p1.1.m1.2.2.2.2.3.cmml" xref="S4.SS1.SSS2.p1.1.m1.2.2.2.2.3">ğ‘–</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.2c">[\mathbf{q}_{i},\mathbf{p}_{i}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.1.m1.2d">[ bold_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]</annotation></semantics></math> between target image <math alttext="I_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.2.m2.1"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><msub id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS2.p1.2.m2.1.1.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml">I</mi><mi id="S4.SS1.SSS2.p1.2.m2.1.1.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><apply id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.2">ğ¼</ci><ci id="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">I_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and rendered image <math alttext="I_{r}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.3.m3.1"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><msub id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS2.p1.3.m3.1.1.2" xref="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml">I</mi><mi id="S4.SS1.SSS2.p1.3.m3.1.1.3" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><apply id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.2">ğ¼</ci><ci id="S4.SS1.SSS2.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">I_{r}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.3.m3.1d">italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> to 2D-3D correspondences <math alttext="[\mathbf{q}_{i},\mathbf{x}_{i}]" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.4.m4.2"><semantics id="S4.SS1.SSS2.p1.4.m4.2a"><mrow id="S4.SS1.SSS2.p1.4.m4.2.2.2" xref="S4.SS1.SSS2.p1.4.m4.2.2.3.cmml"><mo id="S4.SS1.SSS2.p1.4.m4.2.2.2.3" stretchy="false" xref="S4.SS1.SSS2.p1.4.m4.2.2.3.cmml">[</mo><msub id="S4.SS1.SSS2.p1.4.m4.1.1.1.1" xref="S4.SS1.SSS2.p1.4.m4.1.1.1.1.cmml"><mi id="S4.SS1.SSS2.p1.4.m4.1.1.1.1.2" xref="S4.SS1.SSS2.p1.4.m4.1.1.1.1.2.cmml">ğª</mi><mi id="S4.SS1.SSS2.p1.4.m4.1.1.1.1.3" xref="S4.SS1.SSS2.p1.4.m4.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.SSS2.p1.4.m4.2.2.2.4" xref="S4.SS1.SSS2.p1.4.m4.2.2.3.cmml">,</mo><msub id="S4.SS1.SSS2.p1.4.m4.2.2.2.2" xref="S4.SS1.SSS2.p1.4.m4.2.2.2.2.cmml"><mi id="S4.SS1.SSS2.p1.4.m4.2.2.2.2.2" xref="S4.SS1.SSS2.p1.4.m4.2.2.2.2.2.cmml">ğ±</mi><mi id="S4.SS1.SSS2.p1.4.m4.2.2.2.2.3" xref="S4.SS1.SSS2.p1.4.m4.2.2.2.2.3.cmml">i</mi></msub><mo id="S4.SS1.SSS2.p1.4.m4.2.2.2.5" stretchy="false" xref="S4.SS1.SSS2.p1.4.m4.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.4.m4.2b"><interval closure="closed" id="S4.SS1.SSS2.p1.4.m4.2.2.3.cmml" xref="S4.SS1.SSS2.p1.4.m4.2.2.2"><apply id="S4.SS1.SSS2.p1.4.m4.1.1.1.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.4.m4.1.1.1.1.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p1.4.m4.1.1.1.1.2.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.1.1.2">ğª</ci><ci id="S4.SS1.SSS2.p1.4.m4.1.1.1.1.3.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.1.1.3">ğ‘–</ci></apply><apply id="S4.SS1.SSS2.p1.4.m4.2.2.2.2.cmml" xref="S4.SS1.SSS2.p1.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.4.m4.2.2.2.2.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.2.2.2.2">subscript</csymbol><ci id="S4.SS1.SSS2.p1.4.m4.2.2.2.2.2.cmml" xref="S4.SS1.SSS2.p1.4.m4.2.2.2.2.2">ğ±</ci><ci id="S4.SS1.SSS2.p1.4.m4.2.2.2.2.3.cmml" xref="S4.SS1.SSS2.p1.4.m4.2.2.2.2.3">ğ‘–</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.4.m4.2c">[\mathbf{q}_{i},\mathbf{x}_{i}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.4.m4.2d">[ bold_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]</annotation></semantics></math>. We achieve this by lifting the matched 2D pixels <math alttext="\mathbf{p}\in\mathbb{R}^{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.5.m5.1"><semantics id="S4.SS1.SSS2.p1.5.m5.1a"><mrow id="S4.SS1.SSS2.p1.5.m5.1.1" xref="S4.SS1.SSS2.p1.5.m5.1.1.cmml"><mi id="S4.SS1.SSS2.p1.5.m5.1.1.2" xref="S4.SS1.SSS2.p1.5.m5.1.1.2.cmml">ğ©</mi><mo id="S4.SS1.SSS2.p1.5.m5.1.1.1" xref="S4.SS1.SSS2.p1.5.m5.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS1.SSS2.p1.5.m5.1.1.3" xref="S4.SS1.SSS2.p1.5.m5.1.1.3.cmml"><mi id="S4.SS1.SSS2.p1.5.m5.1.1.3.2" xref="S4.SS1.SSS2.p1.5.m5.1.1.3.2.cmml">â„</mi><mn id="S4.SS1.SSS2.p1.5.m5.1.1.3.3" xref="S4.SS1.SSS2.p1.5.m5.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.5.m5.1b"><apply id="S4.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1"><in id="S4.SS1.SSS2.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1.1"></in><ci id="S4.SS1.SSS2.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1.2">ğ©</ci><apply id="S4.SS1.SSS2.p1.5.m5.1.1.3.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.5.m5.1.1.3.1.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS2.p1.5.m5.1.1.3.2.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1.3.2">â„</ci><cn id="S4.SS1.SSS2.p1.5.m5.1.1.3.3.cmml" type="integer" xref="S4.SS1.SSS2.p1.5.m5.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.5.m5.1c">\mathbf{p}\in\mathbb{R}^{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.5.m5.1d">bold_p âˆˆ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> in NeRF rendered image <math alttext="I_{r}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.6.m6.1"><semantics id="S4.SS1.SSS2.p1.6.m6.1a"><msub id="S4.SS1.SSS2.p1.6.m6.1.1" xref="S4.SS1.SSS2.p1.6.m6.1.1.cmml"><mi id="S4.SS1.SSS2.p1.6.m6.1.1.2" xref="S4.SS1.SSS2.p1.6.m6.1.1.2.cmml">I</mi><mi id="S4.SS1.SSS2.p1.6.m6.1.1.3" xref="S4.SS1.SSS2.p1.6.m6.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.6.m6.1b"><apply id="S4.SS1.SSS2.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.6.m6.1.1.1.cmml" xref="S4.SS1.SSS2.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p1.6.m6.1.1.2.cmml" xref="S4.SS1.SSS2.p1.6.m6.1.1.2">ğ¼</ci><ci id="S4.SS1.SSS2.p1.6.m6.1.1.3.cmml" xref="S4.SS1.SSS2.p1.6.m6.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.6.m6.1c">I_{r}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.6.m6.1d">italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> to 3D space. Specifically, we first obtain the depth <math alttext="\hat{z}_{i}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.7.m7.1"><semantics id="S4.SS1.SSS2.p1.7.m7.1a"><msub id="S4.SS1.SSS2.p1.7.m7.1.1" xref="S4.SS1.SSS2.p1.7.m7.1.1.cmml"><mover accent="true" id="S4.SS1.SSS2.p1.7.m7.1.1.2" xref="S4.SS1.SSS2.p1.7.m7.1.1.2.cmml"><mi id="S4.SS1.SSS2.p1.7.m7.1.1.2.2" xref="S4.SS1.SSS2.p1.7.m7.1.1.2.2.cmml">z</mi><mo id="S4.SS1.SSS2.p1.7.m7.1.1.2.1" xref="S4.SS1.SSS2.p1.7.m7.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS1.SSS2.p1.7.m7.1.1.3" xref="S4.SS1.SSS2.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.7.m7.1b"><apply id="S4.SS1.SSS2.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.7.m7.1.1.1.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.1">subscript</csymbol><apply id="S4.SS1.SSS2.p1.7.m7.1.1.2.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.1.2"><ci id="S4.SS1.SSS2.p1.7.m7.1.1.2.1.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.1.2.1">^</ci><ci id="S4.SS1.SSS2.p1.7.m7.1.1.2.2.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.1.2.2">ğ‘§</ci></apply><ci id="S4.SS1.SSS2.p1.7.m7.1.1.3.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.7.m7.1c">\hat{z}_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.7.m7.1d">over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from the depth map <math alttext="D" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.8.m8.1"><semantics id="S4.SS1.SSS2.p1.8.m8.1a"><mi id="S4.SS1.SSS2.p1.8.m8.1.1" xref="S4.SS1.SSS2.p1.8.m8.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.8.m8.1b"><ci id="S4.SS1.SSS2.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS2.p1.8.m8.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.8.m8.1c">D</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.8.m8.1d">italic_D</annotation></semantics></math> rendered by the trained NeRF model following Eq.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S3.E3" title="In III Background â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a>. Then, the 3D coordinate of the corresponding point <math alttext="\hat{\mathbf{x}}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.9.m9.1"><semantics id="S4.SS1.SSS2.p1.9.m9.1a"><mover accent="true" id="S4.SS1.SSS2.p1.9.m9.1.1" xref="S4.SS1.SSS2.p1.9.m9.1.1.cmml"><mi id="S4.SS1.SSS2.p1.9.m9.1.1.2" xref="S4.SS1.SSS2.p1.9.m9.1.1.2.cmml">ğ±</mi><mo id="S4.SS1.SSS2.p1.9.m9.1.1.1" xref="S4.SS1.SSS2.p1.9.m9.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.9.m9.1b"><apply id="S4.SS1.SSS2.p1.9.m9.1.1.cmml" xref="S4.SS1.SSS2.p1.9.m9.1.1"><ci id="S4.SS1.SSS2.p1.9.m9.1.1.1.cmml" xref="S4.SS1.SSS2.p1.9.m9.1.1.1">^</ci><ci id="S4.SS1.SSS2.p1.9.m9.1.1.2.cmml" xref="S4.SS1.SSS2.p1.9.m9.1.1.2">ğ±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.9.m9.1c">\hat{\mathbf{x}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.9.m9.1d">over^ start_ARG bold_x end_ARG</annotation></semantics></math> is obtained via backprojection, and transformed to world space via current camera pose <math alttext="P" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.10.m10.1"><semantics id="S4.SS1.SSS2.p1.10.m10.1a"><mi id="S4.SS1.SSS2.p1.10.m10.1.1" xref="S4.SS1.SSS2.p1.10.m10.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.10.m10.1b"><ci id="S4.SS1.SSS2.p1.10.m10.1.1.cmml" xref="S4.SS1.SSS2.p1.10.m10.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.10.m10.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.10.m10.1d">italic_P</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathbf{x}}_{i}=P\hat{z}_{i}K^{-1}\mathbf{p}_{i}" class="ltx_Math" display="block" id="S4.E5.m1.1"><semantics id="S4.E5.m1.1a"><mrow id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml"><msub id="S4.E5.m1.1.1.2" xref="S4.E5.m1.1.1.2.cmml"><mover accent="true" id="S4.E5.m1.1.1.2.2" xref="S4.E5.m1.1.1.2.2.cmml"><mi id="S4.E5.m1.1.1.2.2.2" xref="S4.E5.m1.1.1.2.2.2.cmml">ğ±</mi><mo id="S4.E5.m1.1.1.2.2.1" xref="S4.E5.m1.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.E5.m1.1.1.2.3" xref="S4.E5.m1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E5.m1.1.1.1" xref="S4.E5.m1.1.1.1.cmml">=</mo><mrow id="S4.E5.m1.1.1.3" xref="S4.E5.m1.1.1.3.cmml"><mi id="S4.E5.m1.1.1.3.2" xref="S4.E5.m1.1.1.3.2.cmml">P</mi><mo id="S4.E5.m1.1.1.3.1" xref="S4.E5.m1.1.1.3.1.cmml">â¢</mo><msub id="S4.E5.m1.1.1.3.3" xref="S4.E5.m1.1.1.3.3.cmml"><mover accent="true" id="S4.E5.m1.1.1.3.3.2" xref="S4.E5.m1.1.1.3.3.2.cmml"><mi id="S4.E5.m1.1.1.3.3.2.2" xref="S4.E5.m1.1.1.3.3.2.2.cmml">z</mi><mo id="S4.E5.m1.1.1.3.3.2.1" xref="S4.E5.m1.1.1.3.3.2.1.cmml">^</mo></mover><mi id="S4.E5.m1.1.1.3.3.3" xref="S4.E5.m1.1.1.3.3.3.cmml">i</mi></msub><mo id="S4.E5.m1.1.1.3.1a" xref="S4.E5.m1.1.1.3.1.cmml">â¢</mo><msup id="S4.E5.m1.1.1.3.4" xref="S4.E5.m1.1.1.3.4.cmml"><mi id="S4.E5.m1.1.1.3.4.2" xref="S4.E5.m1.1.1.3.4.2.cmml">K</mi><mrow id="S4.E5.m1.1.1.3.4.3" xref="S4.E5.m1.1.1.3.4.3.cmml"><mo id="S4.E5.m1.1.1.3.4.3a" xref="S4.E5.m1.1.1.3.4.3.cmml">âˆ’</mo><mn id="S4.E5.m1.1.1.3.4.3.2" xref="S4.E5.m1.1.1.3.4.3.2.cmml">1</mn></mrow></msup><mo id="S4.E5.m1.1.1.3.1b" xref="S4.E5.m1.1.1.3.1.cmml">â¢</mo><msub id="S4.E5.m1.1.1.3.5" xref="S4.E5.m1.1.1.3.5.cmml"><mi id="S4.E5.m1.1.1.3.5.2" xref="S4.E5.m1.1.1.3.5.2.cmml">ğ©</mi><mi id="S4.E5.m1.1.1.3.5.3" xref="S4.E5.m1.1.1.3.5.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.1b"><apply id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1"><eq id="S4.E5.m1.1.1.1.cmml" xref="S4.E5.m1.1.1.1"></eq><apply id="S4.E5.m1.1.1.2.cmml" xref="S4.E5.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.2.1.cmml" xref="S4.E5.m1.1.1.2">subscript</csymbol><apply id="S4.E5.m1.1.1.2.2.cmml" xref="S4.E5.m1.1.1.2.2"><ci id="S4.E5.m1.1.1.2.2.1.cmml" xref="S4.E5.m1.1.1.2.2.1">^</ci><ci id="S4.E5.m1.1.1.2.2.2.cmml" xref="S4.E5.m1.1.1.2.2.2">ğ±</ci></apply><ci id="S4.E5.m1.1.1.2.3.cmml" xref="S4.E5.m1.1.1.2.3">ğ‘–</ci></apply><apply id="S4.E5.m1.1.1.3.cmml" xref="S4.E5.m1.1.1.3"><times id="S4.E5.m1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.3.1"></times><ci id="S4.E5.m1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.3.2">ğ‘ƒ</ci><apply id="S4.E5.m1.1.1.3.3.cmml" xref="S4.E5.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.3.3.1.cmml" xref="S4.E5.m1.1.1.3.3">subscript</csymbol><apply id="S4.E5.m1.1.1.3.3.2.cmml" xref="S4.E5.m1.1.1.3.3.2"><ci id="S4.E5.m1.1.1.3.3.2.1.cmml" xref="S4.E5.m1.1.1.3.3.2.1">^</ci><ci id="S4.E5.m1.1.1.3.3.2.2.cmml" xref="S4.E5.m1.1.1.3.3.2.2">ğ‘§</ci></apply><ci id="S4.E5.m1.1.1.3.3.3.cmml" xref="S4.E5.m1.1.1.3.3.3">ğ‘–</ci></apply><apply id="S4.E5.m1.1.1.3.4.cmml" xref="S4.E5.m1.1.1.3.4"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.3.4.1.cmml" xref="S4.E5.m1.1.1.3.4">superscript</csymbol><ci id="S4.E5.m1.1.1.3.4.2.cmml" xref="S4.E5.m1.1.1.3.4.2">ğ¾</ci><apply id="S4.E5.m1.1.1.3.4.3.cmml" xref="S4.E5.m1.1.1.3.4.3"><minus id="S4.E5.m1.1.1.3.4.3.1.cmml" xref="S4.E5.m1.1.1.3.4.3"></minus><cn id="S4.E5.m1.1.1.3.4.3.2.cmml" type="integer" xref="S4.E5.m1.1.1.3.4.3.2">1</cn></apply></apply><apply id="S4.E5.m1.1.1.3.5.cmml" xref="S4.E5.m1.1.1.3.5"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.3.5.1.cmml" xref="S4.E5.m1.1.1.3.5">subscript</csymbol><ci id="S4.E5.m1.1.1.3.5.2.cmml" xref="S4.E5.m1.1.1.3.5.2">ğ©</ci><ci id="S4.E5.m1.1.1.3.5.3.cmml" xref="S4.E5.m1.1.1.3.5.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.1c">\hat{\mathbf{x}}_{i}=P\hat{z}_{i}K^{-1}\mathbf{p}_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.E5.m1.1d">over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_P over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_K start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS3.5.1.1">IV-A</span>3 </span>PnP</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">After obtaining the 2D-3D correspondences, the pose is computed via PnPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib14" title="">14</a>]</cite> with RANSACÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib49" title="">49</a>]</cite>. The above procedure already allows us to obtain good pose with only one rendering step, which is much faster than former NeRF based baselinesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>]</cite>. However, there may exists error due to inaccurate feature matches. In the following, we introduce a strategy to further improve the performance.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">3D Consistent Point Mining</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In the above framework, one of the key factors that affect the pose accuracy is the precision of the 2D-3D matches, which are computed by a trained NeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib15" title="">15</a>]</cite> model as stated inÂ <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S4.SS1.SSS2" title="IV-A2 Lifting â€£ IV-A One-step Pose Estimation via Feature Matching â€£ IV Method â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>2</span></a>. However, owing to the implicit nature of NeRF, the learned scene geometry can be unfaithful and noisyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib23" title="">23</a>]</cite>. Moreover, the estimated 3D coordinates can be inconsistent when rendering from different views, resulting in large pose error. These problems become severer when the training images are limited, or the camera poses are noisy.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.3">To counter the above problem, we propose to preclude the inconsistent 3D points by introducing a 3D consistent point mining strategy. Specifically, for each 3D keypoint <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">bold_x</annotation></semantics></math> that is lifted from a matched 2D pixel <math alttext="\mathbf{p}" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">ğ©</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">ğ©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\mathbf{p}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">bold_p</annotation></semantics></math>, its consistency <math alttext="m" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1"><semantics id="S4.SS2.p2.3.m3.1a"><mi id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><ci id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">italic_m</annotation></semantics></math> is evaluated by re-estimating the 3D coordinates from nearby views, and computing how well these points are aligned with each other.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.8">Specifically, given the current view <math alttext="P" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_P</annotation></semantics></math> and estimated 3D keypoint <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mi id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">bold_x</annotation></semantics></math>, we first sample <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1"><semantics id="S4.SS2.p3.3.m3.1a"><mi id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><ci id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m3.1d">italic_k</annotation></semantics></math> nearby views <math alttext="\mathcal{P}=\{P_{i}\}_{i=1}^{k}" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1"><semantics id="S4.SS2.p3.4.m4.1a"><mrow id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.4.m4.1.1.3" xref="S4.SS2.p3.4.m4.1.1.3.cmml">ğ’«</mi><mo id="S4.SS2.p3.4.m4.1.1.2" xref="S4.SS2.p3.4.m4.1.1.2.cmml">=</mo><msubsup id="S4.SS2.p3.4.m4.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.cmml"><mrow id="S4.SS2.p3.4.m4.1.1.1.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.1.1.2.cmml"><mo id="S4.SS2.p3.4.m4.1.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p3.4.m4.1.1.1.1.1.2.cmml">{</mo><msub id="S4.SS2.p3.4.m4.1.1.1.1.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.2" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.2.cmml">P</mi><mi id="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.3" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS2.p3.4.m4.1.1.1.1.1.1.3" stretchy="false" xref="S4.SS2.p3.4.m4.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS2.p3.4.m4.1.1.1.1.3" xref="S4.SS2.p3.4.m4.1.1.1.1.3.cmml"><mi id="S4.SS2.p3.4.m4.1.1.1.1.3.2" xref="S4.SS2.p3.4.m4.1.1.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p3.4.m4.1.1.1.1.3.1" xref="S4.SS2.p3.4.m4.1.1.1.1.3.1.cmml">=</mo><mn id="S4.SS2.p3.4.m4.1.1.1.1.3.3" xref="S4.SS2.p3.4.m4.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p3.4.m4.1.1.1.3" xref="S4.SS2.p3.4.m4.1.1.1.3.cmml">k</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><apply id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1"><eq id="S4.SS2.p3.4.m4.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.2"></eq><ci id="S4.SS2.p3.4.m4.1.1.3.cmml" xref="S4.SS2.p3.4.m4.1.1.3">ğ’«</ci><apply id="S4.SS2.p3.4.m4.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.1.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1">superscript</csymbol><apply id="S4.SS2.p3.4.m4.1.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.1.1.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1">subscript</csymbol><set id="S4.SS2.p3.4.m4.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1"><apply id="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.2">ğ‘ƒ</ci><ci id="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S4.SS2.p3.4.m4.1.1.1.1.3.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.3"><eq id="S4.SS2.p3.4.m4.1.1.1.1.3.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.3.1"></eq><ci id="S4.SS2.p3.4.m4.1.1.1.1.3.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.3.2">ğ‘–</ci><cn id="S4.SS2.p3.4.m4.1.1.1.1.3.3.cmml" type="integer" xref="S4.SS2.p3.4.m4.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS2.p3.4.m4.1.1.1.3.cmml" xref="S4.SS2.p3.4.m4.1.1.1.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">\mathcal{P}=\{P_{i}\}_{i=1}^{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.m4.1d">caligraphic_P = { italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>. Then, we shoot rays <math alttext="R=\{\mathbf{r}_{i}\}_{i=1}^{k}" class="ltx_Math" display="inline" id="S4.SS2.p3.5.m5.1"><semantics id="S4.SS2.p3.5.m5.1a"><mrow id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml"><mi id="S4.SS2.p3.5.m5.1.1.3" xref="S4.SS2.p3.5.m5.1.1.3.cmml">R</mi><mo id="S4.SS2.p3.5.m5.1.1.2" xref="S4.SS2.p3.5.m5.1.1.2.cmml">=</mo><msubsup id="S4.SS2.p3.5.m5.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.cmml"><mrow id="S4.SS2.p3.5.m5.1.1.1.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.1.1.2.cmml"><mo id="S4.SS2.p3.5.m5.1.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p3.5.m5.1.1.1.1.1.2.cmml">{</mo><msub id="S4.SS2.p3.5.m5.1.1.1.1.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.2" xref="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.2.cmml">ğ«</mi><mi id="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.3" xref="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS2.p3.5.m5.1.1.1.1.1.1.3" stretchy="false" xref="S4.SS2.p3.5.m5.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS2.p3.5.m5.1.1.1.1.3" xref="S4.SS2.p3.5.m5.1.1.1.1.3.cmml"><mi id="S4.SS2.p3.5.m5.1.1.1.1.3.2" xref="S4.SS2.p3.5.m5.1.1.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p3.5.m5.1.1.1.1.3.1" xref="S4.SS2.p3.5.m5.1.1.1.1.3.1.cmml">=</mo><mn id="S4.SS2.p3.5.m5.1.1.1.1.3.3" xref="S4.SS2.p3.5.m5.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p3.5.m5.1.1.1.3" xref="S4.SS2.p3.5.m5.1.1.1.3.cmml">k</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><apply id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1"><eq id="S4.SS2.p3.5.m5.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.2"></eq><ci id="S4.SS2.p3.5.m5.1.1.3.cmml" xref="S4.SS2.p3.5.m5.1.1.3">ğ‘…</ci><apply id="S4.SS2.p3.5.m5.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.5.m5.1.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.1">superscript</csymbol><apply id="S4.SS2.p3.5.m5.1.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.5.m5.1.1.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.1">subscript</csymbol><set id="S4.SS2.p3.5.m5.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.1.1"><apply id="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.2">ğ«</ci><ci id="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S4.SS2.p3.5.m5.1.1.1.1.3.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.3"><eq id="S4.SS2.p3.5.m5.1.1.1.1.3.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.3.1"></eq><ci id="S4.SS2.p3.5.m5.1.1.1.1.3.2.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.3.2">ğ‘–</ci><cn id="S4.SS2.p3.5.m5.1.1.1.1.3.3.cmml" type="integer" xref="S4.SS2.p3.5.m5.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS2.p3.5.m5.1.1.1.3.cmml" xref="S4.SS2.p3.5.m5.1.1.1.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">R=\{\mathbf{r}_{i}\}_{i=1}^{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.5.m5.1d">italic_R = { bold_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> that pass the 3D keypoint <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S4.SS2.p3.6.m6.1"><semantics id="S4.SS2.p3.6.m6.1a"><mi id="S4.SS2.p3.6.m6.1.1" xref="S4.SS2.p3.6.m6.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.m6.1b"><ci id="S4.SS2.p3.6.m6.1.1.cmml" xref="S4.SS2.p3.6.m6.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.m6.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.6.m6.1d">bold_x</annotation></semantics></math> from each view in <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="S4.SS2.p3.7.m7.1"><semantics id="S4.SS2.p3.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.7.m7.1.1" xref="S4.SS2.p3.7.m7.1.1.cmml">ğ’«</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.7.m7.1b"><ci id="S4.SS2.p3.7.m7.1.1.cmml" xref="S4.SS2.p3.7.m7.1.1">ğ’«</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.7.m7.1c">\mathcal{P}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.7.m7.1d">caligraphic_P</annotation></semantics></math>, and estimate the 3D coordinates <math alttext="X=\{\mathbf{x}_{i}\}_{i=1}^{k}" class="ltx_Math" display="inline" id="S4.SS2.p3.8.m8.1"><semantics id="S4.SS2.p3.8.m8.1a"><mrow id="S4.SS2.p3.8.m8.1.1" xref="S4.SS2.p3.8.m8.1.1.cmml"><mi id="S4.SS2.p3.8.m8.1.1.3" xref="S4.SS2.p3.8.m8.1.1.3.cmml">X</mi><mo id="S4.SS2.p3.8.m8.1.1.2" xref="S4.SS2.p3.8.m8.1.1.2.cmml">=</mo><msubsup id="S4.SS2.p3.8.m8.1.1.1" xref="S4.SS2.p3.8.m8.1.1.1.cmml"><mrow id="S4.SS2.p3.8.m8.1.1.1.1.1.1" xref="S4.SS2.p3.8.m8.1.1.1.1.1.2.cmml"><mo id="S4.SS2.p3.8.m8.1.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p3.8.m8.1.1.1.1.1.2.cmml">{</mo><msub id="S4.SS2.p3.8.m8.1.1.1.1.1.1.1" xref="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.2" xref="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.2.cmml">ğ±</mi><mi id="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.3" xref="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS2.p3.8.m8.1.1.1.1.1.1.3" stretchy="false" xref="S4.SS2.p3.8.m8.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS2.p3.8.m8.1.1.1.1.3" xref="S4.SS2.p3.8.m8.1.1.1.1.3.cmml"><mi id="S4.SS2.p3.8.m8.1.1.1.1.3.2" xref="S4.SS2.p3.8.m8.1.1.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p3.8.m8.1.1.1.1.3.1" xref="S4.SS2.p3.8.m8.1.1.1.1.3.1.cmml">=</mo><mn id="S4.SS2.p3.8.m8.1.1.1.1.3.3" xref="S4.SS2.p3.8.m8.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p3.8.m8.1.1.1.3" xref="S4.SS2.p3.8.m8.1.1.1.3.cmml">k</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.8.m8.1b"><apply id="S4.SS2.p3.8.m8.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1"><eq id="S4.SS2.p3.8.m8.1.1.2.cmml" xref="S4.SS2.p3.8.m8.1.1.2"></eq><ci id="S4.SS2.p3.8.m8.1.1.3.cmml" xref="S4.SS2.p3.8.m8.1.1.3">ğ‘‹</ci><apply id="S4.SS2.p3.8.m8.1.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.8.m8.1.1.1.2.cmml" xref="S4.SS2.p3.8.m8.1.1.1">superscript</csymbol><apply id="S4.SS2.p3.8.m8.1.1.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.8.m8.1.1.1.1.2.cmml" xref="S4.SS2.p3.8.m8.1.1.1">subscript</csymbol><set id="S4.SS2.p3.8.m8.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.8.m8.1.1.1.1.1.1"><apply id="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.2">ğ±</ci><ci id="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p3.8.m8.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S4.SS2.p3.8.m8.1.1.1.1.3.cmml" xref="S4.SS2.p3.8.m8.1.1.1.1.3"><eq id="S4.SS2.p3.8.m8.1.1.1.1.3.1.cmml" xref="S4.SS2.p3.8.m8.1.1.1.1.3.1"></eq><ci id="S4.SS2.p3.8.m8.1.1.1.1.3.2.cmml" xref="S4.SS2.p3.8.m8.1.1.1.1.3.2">ğ‘–</ci><cn id="S4.SS2.p3.8.m8.1.1.1.1.3.3.cmml" type="integer" xref="S4.SS2.p3.8.m8.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS2.p3.8.m8.1.1.1.3.cmml" xref="S4.SS2.p3.8.m8.1.1.1.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.8.m8.1c">X=\{\mathbf{x}_{i}\}_{i=1}^{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.8.m8.1d">italic_X = { bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> on these rays:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="X=\hat{Z}\cdot\operatorname{norm}({\mathcal{P}}P^{-1}\mathbf{x})," class="ltx_Math" display="block" id="S4.E6.m1.2"><semantics id="S4.E6.m1.2a"><mrow id="S4.E6.m1.2.2.1" xref="S4.E6.m1.2.2.1.1.cmml"><mrow id="S4.E6.m1.2.2.1.1" xref="S4.E6.m1.2.2.1.1.cmml"><mi id="S4.E6.m1.2.2.1.1.3" xref="S4.E6.m1.2.2.1.1.3.cmml">X</mi><mo id="S4.E6.m1.2.2.1.1.2" xref="S4.E6.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E6.m1.2.2.1.1.1" xref="S4.E6.m1.2.2.1.1.1.cmml"><mover accent="true" id="S4.E6.m1.2.2.1.1.1.3" xref="S4.E6.m1.2.2.1.1.1.3.cmml"><mi id="S4.E6.m1.2.2.1.1.1.3.2" xref="S4.E6.m1.2.2.1.1.1.3.2.cmml">Z</mi><mo id="S4.E6.m1.2.2.1.1.1.3.1" xref="S4.E6.m1.2.2.1.1.1.3.1.cmml">^</mo></mover><mo id="S4.E6.m1.2.2.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S4.E6.m1.2.2.1.1.1.2.cmml">â‹…</mo><mrow id="S4.E6.m1.2.2.1.1.1.1.1" xref="S4.E6.m1.2.2.1.1.1.1.2.cmml"><mi id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml">norm</mi><mo id="S4.E6.m1.2.2.1.1.1.1.1a" xref="S4.E6.m1.2.2.1.1.1.1.2.cmml">â¡</mo><mrow id="S4.E6.m1.2.2.1.1.1.1.1.1" xref="S4.E6.m1.2.2.1.1.1.1.2.cmml"><mo id="S4.E6.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S4.E6.m1.2.2.1.1.1.1.2.cmml">(</mo><mrow id="S4.E6.m1.2.2.1.1.1.1.1.1.1" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E6.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.cmml">ğ’«</mi><mo id="S4.E6.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.1.cmml">â¢</mo><msup id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">P</mi><mrow id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.cmml"><mo id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3a" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.cmml">âˆ’</mo><mn id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml">1</mn></mrow></msup><mo id="S4.E6.m1.2.2.1.1.1.1.1.1.1.1a" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.E6.m1.2.2.1.1.1.1.1.1.1.4" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.4.cmml">ğ±</mi></mrow><mo id="S4.E6.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S4.E6.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E6.m1.2.2.1.2" xref="S4.E6.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.2b"><apply id="S4.E6.m1.2.2.1.1.cmml" xref="S4.E6.m1.2.2.1"><eq id="S4.E6.m1.2.2.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.2"></eq><ci id="S4.E6.m1.2.2.1.1.3.cmml" xref="S4.E6.m1.2.2.1.1.3">ğ‘‹</ci><apply id="S4.E6.m1.2.2.1.1.1.cmml" xref="S4.E6.m1.2.2.1.1.1"><ci id="S4.E6.m1.2.2.1.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.1.2">â‹…</ci><apply id="S4.E6.m1.2.2.1.1.1.3.cmml" xref="S4.E6.m1.2.2.1.1.1.3"><ci id="S4.E6.m1.2.2.1.1.1.3.1.cmml" xref="S4.E6.m1.2.2.1.1.1.3.1">^</ci><ci id="S4.E6.m1.2.2.1.1.1.3.2.cmml" xref="S4.E6.m1.2.2.1.1.1.3.2">ğ‘</ci></apply><apply id="S4.E6.m1.2.2.1.1.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1"><ci id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1">norm</ci><apply id="S4.E6.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1"><times id="S4.E6.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.1"></times><ci id="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.2">ğ’«</ci><apply id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.2">ğ‘ƒ</ci><apply id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3"><minus id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3"></minus><cn id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml" type="integer" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.2">1</cn></apply></apply><ci id="S4.E6.m1.2.2.1.1.1.1.1.1.1.4.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.4">ğ±</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.2c">X=\hat{Z}\cdot\operatorname{norm}({\mathcal{P}}P^{-1}\mathbf{x}),</annotation><annotation encoding="application/x-llamapun" id="S4.E6.m1.2d">italic_X = over^ start_ARG italic_Z end_ARG â‹… roman_norm ( caligraphic_P italic_P start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_x ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p3.11">where <math alttext="\hat{Z}=\{\hat{z}_{i}\}_{i=1}^{k}" class="ltx_Math" display="inline" id="S4.SS2.p3.9.m1.1"><semantics id="S4.SS2.p3.9.m1.1a"><mrow id="S4.SS2.p3.9.m1.1.1" xref="S4.SS2.p3.9.m1.1.1.cmml"><mover accent="true" id="S4.SS2.p3.9.m1.1.1.3" xref="S4.SS2.p3.9.m1.1.1.3.cmml"><mi id="S4.SS2.p3.9.m1.1.1.3.2" xref="S4.SS2.p3.9.m1.1.1.3.2.cmml">Z</mi><mo id="S4.SS2.p3.9.m1.1.1.3.1" xref="S4.SS2.p3.9.m1.1.1.3.1.cmml">^</mo></mover><mo id="S4.SS2.p3.9.m1.1.1.2" xref="S4.SS2.p3.9.m1.1.1.2.cmml">=</mo><msubsup id="S4.SS2.p3.9.m1.1.1.1" xref="S4.SS2.p3.9.m1.1.1.1.cmml"><mrow id="S4.SS2.p3.9.m1.1.1.1.1.1.1" xref="S4.SS2.p3.9.m1.1.1.1.1.1.2.cmml"><mo id="S4.SS2.p3.9.m1.1.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p3.9.m1.1.1.1.1.1.2.cmml">{</mo><msub id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2.2" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2.2.cmml">z</mi><mo id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2.1" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.3" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS2.p3.9.m1.1.1.1.1.1.1.3" stretchy="false" xref="S4.SS2.p3.9.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS2.p3.9.m1.1.1.1.1.3" xref="S4.SS2.p3.9.m1.1.1.1.1.3.cmml"><mi id="S4.SS2.p3.9.m1.1.1.1.1.3.2" xref="S4.SS2.p3.9.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p3.9.m1.1.1.1.1.3.1" xref="S4.SS2.p3.9.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S4.SS2.p3.9.m1.1.1.1.1.3.3" xref="S4.SS2.p3.9.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p3.9.m1.1.1.1.3" xref="S4.SS2.p3.9.m1.1.1.1.3.cmml">k</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.9.m1.1b"><apply id="S4.SS2.p3.9.m1.1.1.cmml" xref="S4.SS2.p3.9.m1.1.1"><eq id="S4.SS2.p3.9.m1.1.1.2.cmml" xref="S4.SS2.p3.9.m1.1.1.2"></eq><apply id="S4.SS2.p3.9.m1.1.1.3.cmml" xref="S4.SS2.p3.9.m1.1.1.3"><ci id="S4.SS2.p3.9.m1.1.1.3.1.cmml" xref="S4.SS2.p3.9.m1.1.1.3.1">^</ci><ci id="S4.SS2.p3.9.m1.1.1.3.2.cmml" xref="S4.SS2.p3.9.m1.1.1.3.2">ğ‘</ci></apply><apply id="S4.SS2.p3.9.m1.1.1.1.cmml" xref="S4.SS2.p3.9.m1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.9.m1.1.1.1.2.cmml" xref="S4.SS2.p3.9.m1.1.1.1">superscript</csymbol><apply id="S4.SS2.p3.9.m1.1.1.1.1.cmml" xref="S4.SS2.p3.9.m1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.9.m1.1.1.1.1.2.cmml" xref="S4.SS2.p3.9.m1.1.1.1">subscript</csymbol><set id="S4.SS2.p3.9.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1"><apply id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2"><ci id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.2.2">ğ‘§</ci></apply><ci id="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p3.9.m1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S4.SS2.p3.9.m1.1.1.1.1.3.cmml" xref="S4.SS2.p3.9.m1.1.1.1.1.3"><eq id="S4.SS2.p3.9.m1.1.1.1.1.3.1.cmml" xref="S4.SS2.p3.9.m1.1.1.1.1.3.1"></eq><ci id="S4.SS2.p3.9.m1.1.1.1.1.3.2.cmml" xref="S4.SS2.p3.9.m1.1.1.1.1.3.2">ğ‘–</ci><cn id="S4.SS2.p3.9.m1.1.1.1.1.3.3.cmml" type="integer" xref="S4.SS2.p3.9.m1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS2.p3.9.m1.1.1.1.3.cmml" xref="S4.SS2.p3.9.m1.1.1.1.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.9.m1.1c">\hat{Z}=\{\hat{z}_{i}\}_{i=1}^{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.9.m1.1d">over^ start_ARG italic_Z end_ARG = { over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> is the depth value of rays <math alttext="R" class="ltx_Math" display="inline" id="S4.SS2.p3.10.m2.1"><semantics id="S4.SS2.p3.10.m2.1a"><mi id="S4.SS2.p3.10.m2.1.1" xref="S4.SS2.p3.10.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.10.m2.1b"><ci id="S4.SS2.p3.10.m2.1.1.cmml" xref="S4.SS2.p3.10.m2.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.10.m2.1c">R</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.10.m2.1d">italic_R</annotation></semantics></math> estimated by NeRF, and <math alttext="\operatorname{norm}(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.p3.11.m3.2"><semantics id="S4.SS2.p3.11.m3.2a"><mrow id="S4.SS2.p3.11.m3.2.3.2" xref="S4.SS2.p3.11.m3.2.3.1.cmml"><mi id="S4.SS2.p3.11.m3.1.1" xref="S4.SS2.p3.11.m3.1.1.cmml">norm</mi><mo id="S4.SS2.p3.11.m3.2.3.2a" xref="S4.SS2.p3.11.m3.2.3.1.cmml">â¡</mo><mrow id="S4.SS2.p3.11.m3.2.3.2.1" xref="S4.SS2.p3.11.m3.2.3.1.cmml"><mo id="S4.SS2.p3.11.m3.2.3.2.1.1" stretchy="false" xref="S4.SS2.p3.11.m3.2.3.1.cmml">(</mo><mo id="S4.SS2.p3.11.m3.2.2" lspace="0em" rspace="0em" xref="S4.SS2.p3.11.m3.2.2.cmml">â‹…</mo><mo id="S4.SS2.p3.11.m3.2.3.2.1.2" stretchy="false" xref="S4.SS2.p3.11.m3.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.11.m3.2b"><apply id="S4.SS2.p3.11.m3.2.3.1.cmml" xref="S4.SS2.p3.11.m3.2.3.2"><ci id="S4.SS2.p3.11.m3.1.1.cmml" xref="S4.SS2.p3.11.m3.1.1">norm</ci><ci id="S4.SS2.p3.11.m3.2.2.cmml" xref="S4.SS2.p3.11.m3.2.2">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.11.m3.2c">\operatorname{norm}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.11.m3.2d">roman_norm ( â‹… )</annotation></semantics></math> denotes vector normalization.
We measure the point consistency with the location variance:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="m=\frac{1}{k}||X-\mathbf{x}||_{2}^{2}," class="ltx_Math" display="block" id="S4.E7.m1.1"><semantics id="S4.E7.m1.1a"><mrow id="S4.E7.m1.1.1.1" xref="S4.E7.m1.1.1.1.1.cmml"><mrow id="S4.E7.m1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.cmml"><mi id="S4.E7.m1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.3.cmml">m</mi><mo id="S4.E7.m1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.2.cmml">=</mo><mrow id="S4.E7.m1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.cmml"><mfrac id="S4.E7.m1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.3.cmml"><mn id="S4.E7.m1.1.1.1.1.1.3.2" xref="S4.E7.m1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S4.E7.m1.1.1.1.1.1.3.3" xref="S4.E7.m1.1.1.1.1.1.3.3.cmml">k</mi></mfrac><mo id="S4.E7.m1.1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.1.2.cmml">â¢</mo><msubsup id="S4.E7.m1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.cmml"><mrow id="S4.E7.m1.1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S4.E7.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S4.E7.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mo id="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">ğ±</mi></mrow><mo id="S4.E7.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S4.E7.m1.1.1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.1.1.3.cmml">2</mn><mn id="S4.E7.m1.1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow><mo id="S4.E7.m1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.1b"><apply id="S4.E7.m1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1"><eq id="S4.E7.m1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.2"></eq><ci id="S4.E7.m1.1.1.1.1.3.cmml" xref="S4.E7.m1.1.1.1.1.3">ğ‘š</ci><apply id="S4.E7.m1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1"><times id="S4.E7.m1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1.2"></times><apply id="S4.E7.m1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.1.1.1.1.1.3"><divide id="S4.E7.m1.1.1.1.1.1.3.1.cmml" xref="S4.E7.m1.1.1.1.1.1.3"></divide><cn id="S4.E7.m1.1.1.1.1.1.3.2.cmml" type="integer" xref="S4.E7.m1.1.1.1.1.1.3.2">1</cn><ci id="S4.E7.m1.1.1.1.1.1.3.3.cmml" xref="S4.E7.m1.1.1.1.1.1.3.3">ğ‘˜</ci></apply><apply id="S4.E7.m1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.E7.m1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E7.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1.1.3">ğ±</ci></apply></apply><cn id="S4.E7.m1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E7.m1.1.1.1.1.1.1.1.3">2</cn></apply><cn id="S4.E7.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E7.m1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.1c">m=\frac{1}{k}||X-\mathbf{x}||_{2}^{2},</annotation><annotation encoding="application/x-llamapun" id="S4.E7.m1.1d">italic_m = divide start_ARG 1 end_ARG start_ARG italic_k end_ARG | | italic_X - bold_x | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p3.15">where larger <math alttext="m" class="ltx_Math" display="inline" id="S4.SS2.p3.12.m1.1"><semantics id="S4.SS2.p3.12.m1.1a"><mi id="S4.SS2.p3.12.m1.1.1" xref="S4.SS2.p3.12.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.12.m1.1b"><ci id="S4.SS2.p3.12.m1.1.1.cmml" xref="S4.SS2.p3.12.m1.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.12.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.12.m1.1d">italic_m</annotation></semantics></math> indicates lower consistency. Finally, we introduce a threshold <math alttext="\gamma" class="ltx_Math" display="inline" id="S4.SS2.p3.13.m2.1"><semantics id="S4.SS2.p3.13.m2.1a"><mi id="S4.SS2.p3.13.m2.1.1" xref="S4.SS2.p3.13.m2.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.13.m2.1b"><ci id="S4.SS2.p3.13.m2.1.1.cmml" xref="S4.SS2.p3.13.m2.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.13.m2.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.13.m2.1d">italic_Î³</annotation></semantics></math> to discard the points whose consistency <math alttext="m&gt;\gamma" class="ltx_Math" display="inline" id="S4.SS2.p3.14.m3.1"><semantics id="S4.SS2.p3.14.m3.1a"><mrow id="S4.SS2.p3.14.m3.1.1" xref="S4.SS2.p3.14.m3.1.1.cmml"><mi id="S4.SS2.p3.14.m3.1.1.2" xref="S4.SS2.p3.14.m3.1.1.2.cmml">m</mi><mo id="S4.SS2.p3.14.m3.1.1.1" xref="S4.SS2.p3.14.m3.1.1.1.cmml">&gt;</mo><mi id="S4.SS2.p3.14.m3.1.1.3" xref="S4.SS2.p3.14.m3.1.1.3.cmml">Î³</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.14.m3.1b"><apply id="S4.SS2.p3.14.m3.1.1.cmml" xref="S4.SS2.p3.14.m3.1.1"><gt id="S4.SS2.p3.14.m3.1.1.1.cmml" xref="S4.SS2.p3.14.m3.1.1.1"></gt><ci id="S4.SS2.p3.14.m3.1.1.2.cmml" xref="S4.SS2.p3.14.m3.1.1.2">ğ‘š</ci><ci id="S4.SS2.p3.14.m3.1.1.3.cmml" xref="S4.SS2.p3.14.m3.1.1.3">ğ›¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.14.m3.1c">m&gt;\gamma</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.14.m3.1d">italic_m &gt; italic_Î³</annotation></semantics></math>, where <math alttext="\gamma" class="ltx_Math" display="inline" id="S4.SS2.p3.15.m4.1"><semantics id="S4.SS2.p3.15.m4.1a"><mi id="S4.SS2.p3.15.m4.1.1" xref="S4.SS2.p3.15.m4.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.15.m4.1b"><ci id="S4.SS2.p3.15.m4.1.1.cmml" xref="S4.SS2.p3.15.m4.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.15.m4.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.15.m4.1d">italic_Î³</annotation></semantics></math> is determined empirically.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Keypoint-guided Occlusion Robust Refinement</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Current NeRF-based method cannot estimate the pose of occluded images. The reason is that the photometric loss computed from occluded area will backpropagate false gradients to the pose, which will aggravate the issue of being stuck in local minima.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.2">Our image-matching based strategy provides a solution to this problem. Assuming the image matcher to be accurate enough, the matched keypoints naturally provide cues for unoccluded area, thus preventing the false gradients. We propose to compute the photometric loss with a new matched keypoint-guided sampling strategy. Specifically, after predicting matches, we apply <math alttext="5\times 5" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">5</mn><mo id="S4.SS3.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS3.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn id="S4.SS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.2">5</cn><cn id="S4.SS3.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">5\times 5</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">5 Ã— 5</annotation></semantics></math> morphological dilation around the matched keypoint for <math alttext="n" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mi id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><ci id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">italic_n</annotation></semantics></math> times to obtain the sample region.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Experiments</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We evaluate the pose estimation performance of our proposed method on NeRF synthetic datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib15" title="">15</a>]</cite> and complex real-world scene from LLFF datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib50" title="">50</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Comparison Methods</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We evaluate our method by comparing against state-of-the-art NeRF based pose estimation methods, and image matching based method:</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p2.1.1">iNeRF<span class="ltx_text ltx_font_medium" id="S5.SS1.p2.1.1.1">Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite></span></span> is the first method to estimate object poses by inverting neural radiance fields. It computes photometric loss between the rendered and target images, and backpropagate through NeRFâ€™s framework to optimize the pose.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p3.1.1">pi-NeRF<span class="ltx_text ltx_font_medium" id="S5.SS1.p3.1.1.1">Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>]</cite></span></span> improves the efficiency of iNeRF by using instant-NGPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib1" title="">1</a>]</cite>. It overcomes the local minimum by parallelly optimizing and pruning Monte Carlo sampled poses.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p4.1.1">LoFTRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib47" title="">47</a>]</cite><span class="ltx_text ltx_font_medium" id="S5.SS1.p4.1.1.1"> </span></span>, where we directly solve the pose from 2D matches estimated by LoFTR via epipolar geometry. The translation evaluation is omitted due to scale ambiguity.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p5.1.1">Ours (1-step)</span> To demonstrate the significance of the proposed feature matching strategy, we build Ours (1-step) baseline. It takes the PnP solved pose as final results, and does not apply further pose refinement.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Results on Synthetic Dataset</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS2.SSS1.5.1.1">V-B</span>1 </span>Setting</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">We choose Instant-ngpÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib1" title="">1</a>]</cite> as the NeRF model, and train it on all the training images. For evaluation, we follow iNeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite> to choose 5 test images from test set to estimate the pose. For each image, 5 initial poses are sampled by rotating around a random axis by a random angle within <math alttext="[10^{\circ},40^{\circ}]" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p1.1.m1.2"><semantics id="S5.SS2.SSS1.p1.1.m1.2a"><mrow id="S5.SS2.SSS1.p1.1.m1.2.2.2" xref="S5.SS2.SSS1.p1.1.m1.2.2.3.cmml"><mo id="S5.SS2.SSS1.p1.1.m1.2.2.2.3" stretchy="false" xref="S5.SS2.SSS1.p1.1.m1.2.2.3.cmml">[</mo><msup id="S5.SS2.SSS1.p1.1.m1.1.1.1.1" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.1.cmml"><mn id="S5.SS2.SSS1.p1.1.m1.1.1.1.1.2" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.1.2.cmml">10</mn><mo id="S5.SS2.SSS1.p1.1.m1.1.1.1.1.3" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.1.3.cmml">âˆ˜</mo></msup><mo id="S5.SS2.SSS1.p1.1.m1.2.2.2.4" xref="S5.SS2.SSS1.p1.1.m1.2.2.3.cmml">,</mo><msup id="S5.SS2.SSS1.p1.1.m1.2.2.2.2" xref="S5.SS2.SSS1.p1.1.m1.2.2.2.2.cmml"><mn id="S5.SS2.SSS1.p1.1.m1.2.2.2.2.2" xref="S5.SS2.SSS1.p1.1.m1.2.2.2.2.2.cmml">40</mn><mo id="S5.SS2.SSS1.p1.1.m1.2.2.2.2.3" xref="S5.SS2.SSS1.p1.1.m1.2.2.2.2.3.cmml">âˆ˜</mo></msup><mo id="S5.SS2.SSS1.p1.1.m1.2.2.2.5" stretchy="false" xref="S5.SS2.SSS1.p1.1.m1.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p1.1.m1.2b"><interval closure="closed" id="S5.SS2.SSS1.p1.1.m1.2.2.3.cmml" xref="S5.SS2.SSS1.p1.1.m1.2.2.2"><apply id="S5.SS2.SSS1.p1.1.m1.1.1.1.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS1.p1.1.m1.1.1.1.1.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.1">superscript</csymbol><cn id="S5.SS2.SSS1.p1.1.m1.1.1.1.1.2.cmml" type="integer" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.1.2">10</cn><compose id="S5.SS2.SSS1.p1.1.m1.1.1.1.1.3.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.1.3"></compose></apply><apply id="S5.SS2.SSS1.p1.1.m1.2.2.2.2.cmml" xref="S5.SS2.SSS1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.SSS1.p1.1.m1.2.2.2.2.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.2.2.2.2">superscript</csymbol><cn id="S5.SS2.SSS1.p1.1.m1.2.2.2.2.2.cmml" type="integer" xref="S5.SS2.SSS1.p1.1.m1.2.2.2.2.2">40</cn><compose id="S5.SS2.SSS1.p1.1.m1.2.2.2.2.3.cmml" xref="S5.SS2.SSS1.p1.1.m1.2.2.2.2.3"></compose></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p1.1.m1.2c">[10^{\circ},40^{\circ}]</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p1.1.m1.2d">[ 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT , 40 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT ]</annotation></semantics></math>, and translating along a random vector by length within 0.2. To explore the performance limits, such initial pose perturbation is severer than former workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>]</cite>, so the results of the comparison methods may be worse than the results reported in the original paper. All comparison methods except for iNeRF<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/salykovaa/inerf</span></span></span> are evaluated with the official implementation.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T1.10.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S5.T1.11.2" style="font-size:90%;">6-DoF pose estimation Results on the NeRF Synthetic and LLFF datasets, where RE / TE denote rotation / translation error, respectively. mRE / mTE denote mean rotation / translation error over all subjects.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T1.8.8.9">Method</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.3.3">RE<math alttext="&lt;" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><lt id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">&lt;</annotation></semantics></math><math alttext="5^{\circ}" class="ltx_Math" display="inline" id="S5.T1.2.2.2.m2.1"><semantics id="S5.T1.2.2.2.m2.1a"><msup id="S5.T1.2.2.2.m2.1.1" xref="S5.T1.2.2.2.m2.1.1.cmml"><mn id="S5.T1.2.2.2.m2.1.1.2" xref="S5.T1.2.2.2.m2.1.1.2.cmml">5</mn><mo id="S5.T1.2.2.2.m2.1.1.3" xref="S5.T1.2.2.2.m2.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m2.1b"><apply id="S5.T1.2.2.2.m2.1.1.cmml" xref="S5.T1.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.T1.2.2.2.m2.1.1.1.cmml" xref="S5.T1.2.2.2.m2.1.1">superscript</csymbol><cn id="S5.T1.2.2.2.m2.1.1.2.cmml" type="integer" xref="S5.T1.2.2.2.m2.1.1.2">5</cn><compose id="S5.T1.2.2.2.m2.1.1.3.cmml" xref="S5.T1.2.2.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m2.1c">5^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.m2.1d">5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>(<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.3.3.3.m3.1"><semantics id="S5.T1.3.3.3.m3.1a"><mo id="S5.T1.3.3.3.m3.1.1" stretchy="false" xref="S5.T1.3.3.3.m3.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.m3.1b"><ci id="S5.T1.3.3.3.m3.1.1.cmml" xref="S5.T1.3.3.3.m3.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.m3.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.m3.1d">â†‘</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.6.6.6">TE<math alttext="&lt;" class="ltx_Math" display="inline" id="S5.T1.4.4.4.m1.1"><semantics id="S5.T1.4.4.4.m1.1a"><mo id="S5.T1.4.4.4.m1.1.1" xref="S5.T1.4.4.4.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.m1.1b"><lt id="S5.T1.4.4.4.m1.1.1.cmml" xref="S5.T1.4.4.4.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.m1.1d">&lt;</annotation></semantics></math><math alttext="0.05" class="ltx_Math" display="inline" id="S5.T1.5.5.5.m2.1"><semantics id="S5.T1.5.5.5.m2.1a"><mn id="S5.T1.5.5.5.m2.1.1" xref="S5.T1.5.5.5.m2.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.5.m2.1b"><cn id="S5.T1.5.5.5.m2.1.1.cmml" type="float" xref="S5.T1.5.5.5.m2.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.5.m2.1c">0.05</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.5.m2.1d">0.05</annotation></semantics></math>(<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.6.6.6.m3.1"><semantics id="S5.T1.6.6.6.m3.1a"><mo id="S5.T1.6.6.6.m3.1.1" stretchy="false" xref="S5.T1.6.6.6.m3.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.6.m3.1b"><ci id="S5.T1.6.6.6.m3.1.1.cmml" xref="S5.T1.6.6.6.m3.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.6.m3.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.6.6.m3.1d">â†‘</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.7.7.7">mRE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.7.7.7.m1.1"><semantics id="S5.T1.7.7.7.m1.1a"><mo id="S5.T1.7.7.7.m1.1.1" stretchy="false" xref="S5.T1.7.7.7.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.7.m1.1b"><ci id="S5.T1.7.7.7.m1.1.1.cmml" xref="S5.T1.7.7.7.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.7.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.7.m1.1d">â†“</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.8.8.8">mTE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.8.8.8.m1.1"><semantics id="S5.T1.8.8.8.m1.1a"><mo id="S5.T1.8.8.8.m1.1.1" stretchy="false" xref="S5.T1.8.8.8.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.8.m1.1b"><ci id="S5.T1.8.8.8.m1.1.1.cmml" xref="S5.T1.8.8.8.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.8.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.8.8.m1.1d">â†“</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="5" id="S5.T1.8.9.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.8.9.1.1.1">NeRF Synthetic Dataset</span></th>
</tr>
<tr class="ltx_tr" id="S5.T1.8.10.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.8.10.2.1">iNeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.10.2.2">0.585</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.10.2.3">0.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.10.2.4">10.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.10.2.5">0.559</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.11.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.8.11.3.1">pi-NeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.8.11.3.2">0.24</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.11.3.3">0.04</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.11.3.4">15.83</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.11.3.5">1.073</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.12.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.8.12.4.1">LoFTRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib47" title="">47</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.8.12.4.2">0.785</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.12.4.3">-</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.12.4.4">6.15</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.12.4.5">-</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.13.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.8.13.5.1">Ours (1-step)</th>
<td class="ltx_td ltx_align_center" id="S5.T1.8.13.5.2">0.945</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.13.5.3">0.75</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.13.5.4">1.57</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.13.5.5">0.096</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.14.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.8.14.6.1">Ours</th>
<td class="ltx_td ltx_align_center" id="S5.T1.8.14.6.2"><span class="ltx_text ltx_font_bold" id="S5.T1.8.14.6.2.1">0.95</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.14.6.3"><span class="ltx_text ltx_font_bold" id="S5.T1.8.14.6.3.1">0.88</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.14.6.4"><span class="ltx_text ltx_font_bold" id="S5.T1.8.14.6.4.1">1.25</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.14.6.5"><span class="ltx_text ltx_font_bold" id="S5.T1.8.14.6.5.1">0.077</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.15.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="5" id="S5.T1.8.15.7.1"><span class="ltx_text ltx_font_bold" id="S5.T1.8.15.7.1.1">LLFF Dataset</span></th>
</tr>
<tr class="ltx_tr" id="S5.T1.8.16.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.8.16.8.1">iNeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.16.8.2">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.16.8.3">0.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.16.8.4">16.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.16.8.5">0.0618</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.17.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.8.17.9.1">pi-NeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.8.17.9.2">0.00</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.17.9.3">0.00</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.17.9.4">133.37</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.17.9.5">3.999</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.18.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.8.18.10.1">LoFTRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib47" title="">47</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.8.18.10.2">0.994</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.18.10.3">-</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.18.10.4">0.667</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.18.10.5">-</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.19.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.8.19.11.1">Ours (1-step)</th>
<td class="ltx_td ltx_align_center" id="S5.T1.8.19.11.2"><span class="ltx_text ltx_font_bold" id="S5.T1.8.19.11.2.1">1.00</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.19.11.3"><span class="ltx_text ltx_font_bold" id="S5.T1.8.19.11.3.1">1.00</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.19.11.4">0.325</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.19.11.5"><span class="ltx_text ltx_font_bold" id="S5.T1.8.19.11.5.1">0.0027</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.20.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T1.8.20.12.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.8.20.12.2"><span class="ltx_text ltx_font_bold" id="S5.T1.8.20.12.2.1">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.8.20.12.3"><span class="ltx_text ltx_font_bold" id="S5.T1.8.20.12.3.1">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.8.20.12.4"><span class="ltx_text ltx_font_bold" id="S5.T1.8.20.12.4.1">0.135</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.8.20.12.5"><span class="ltx_text ltx_font_bold" id="S5.T1.8.20.12.5.1">0.0008</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS2.SSS2.6.1.1">V-B</span>2 </span>Results</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.2">As shown in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.T1" title="TABLE I â€£ V-B1 Setting â€£ V-B Results on Synthetic Dataset â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">I</span></a>, we report the pose correctness, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.2.1">i.e.</span>, the rate of poses with rotation error <math alttext="&lt;5^{\circ}" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.1.m1.1"><semantics id="S5.SS2.SSS2.p1.1.m1.1a"><mrow id="S5.SS2.SSS2.p1.1.m1.1.1" xref="S5.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.SSS2.p1.1.m1.1.1.2" xref="S5.SS2.SSS2.p1.1.m1.1.1.2.cmml"></mi><mo id="S5.SS2.SSS2.p1.1.m1.1.1.1" xref="S5.SS2.SSS2.p1.1.m1.1.1.1.cmml">&lt;</mo><msup id="S5.SS2.SSS2.p1.1.m1.1.1.3" xref="S5.SS2.SSS2.p1.1.m1.1.1.3.cmml"><mn id="S5.SS2.SSS2.p1.1.m1.1.1.3.2" xref="S5.SS2.SSS2.p1.1.m1.1.1.3.2.cmml">5</mn><mo id="S5.SS2.SSS2.p1.1.m1.1.1.3.3" xref="S5.SS2.SSS2.p1.1.m1.1.1.3.3.cmml">âˆ˜</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.1.m1.1b"><apply id="S5.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1"><lt id="S5.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S5.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.2">absent</csymbol><apply id="S5.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.3">superscript</csymbol><cn id="S5.SS2.SSS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S5.SS2.SSS2.p1.1.m1.1.1.3.2">5</cn><compose id="S5.SS2.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.3.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.1.m1.1c">&lt;5^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.1.m1.1d">&lt; 5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>, and translation error <math alttext="&lt;5" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.2.m2.1"><semantics id="S5.SS2.SSS2.p1.2.m2.1a"><mrow id="S5.SS2.SSS2.p1.2.m2.1.1" xref="S5.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.SSS2.p1.2.m2.1.1.2" xref="S5.SS2.SSS2.p1.2.m2.1.1.2.cmml"></mi><mo id="S5.SS2.SSS2.p1.2.m2.1.1.1" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.SSS2.p1.2.m2.1.1.3" xref="S5.SS2.SSS2.p1.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.2.m2.1b"><apply id="S5.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1"><lt id="S5.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1"></lt><csymbol cd="latexml" id="S5.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.2">absent</csymbol><cn id="S5.SS2.SSS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S5.SS2.SSS2.p1.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.2.m2.1c">&lt;5</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.2.m2.1d">&lt; 5</annotation></semantics></math> units, and mean rotation (mRE) and translation error (mTE).</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.6">On NeRF Synthetic dataset, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p2.6.1">Ours (1-step)</span> already outperforms NeRF-based methods by 36<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p2.1.m1.1"><semantics id="S5.SS2.SSS2.p2.1.m1.1a"><mo id="S5.SS2.SSS2.p2.1.m1.1.1" xref="S5.SS2.SSS2.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S5.SS2.SSS2.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p2.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p2.1.m1.1d">%</annotation></semantics></math> and 19<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p2.2.m2.1"><semantics id="S5.SS2.SSS2.p2.2.m2.1a"><mo id="S5.SS2.SSS2.p2.2.m2.1.1" xref="S5.SS2.SSS2.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p2.2.m2.1b"><csymbol cd="latexml" id="S5.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S5.SS2.SSS2.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p2.2.m2.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p2.2.m2.1d">%</annotation></semantics></math> in terms of the rotation and translation accuracy. Moreover, pi-NeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>]</cite> achieves worse performance that iNeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite>. We assume the reason is that pi-NeRF fails to guess good initial pose under such severe pose perturbation, and abandoning the interest region based pixel loss used in iNeRF makes the convergence even harder. Our method is also superior than direct solving pose from LoFTRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib47" title="">47</a>]</cite> 2D matches via epipolar geometry, which indicating that our idea of combining 2D matches with 3D information provided by NeRF can complement each other, and further boost the performance. With post refinement of 40 steps, our full method can further boost the correctness of rotation and translation from 94.5<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p2.3.m3.1"><semantics id="S5.SS2.SSS2.p2.3.m3.1a"><mo id="S5.SS2.SSS2.p2.3.m3.1.1" xref="S5.SS2.SSS2.p2.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p2.3.m3.1b"><csymbol cd="latexml" id="S5.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S5.SS2.SSS2.p2.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p2.3.m3.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p2.3.m3.1d">%</annotation></semantics></math> / 75<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p2.4.m4.1"><semantics id="S5.SS2.SSS2.p2.4.m4.1a"><mo id="S5.SS2.SSS2.p2.4.m4.1.1" xref="S5.SS2.SSS2.p2.4.m4.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p2.4.m4.1b"><csymbol cd="latexml" id="S5.SS2.SSS2.p2.4.m4.1.1.cmml" xref="S5.SS2.SSS2.p2.4.m4.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p2.4.m4.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p2.4.m4.1d">%</annotation></semantics></math> to 95<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p2.5.m5.1"><semantics id="S5.SS2.SSS2.p2.5.m5.1a"><mo id="S5.SS2.SSS2.p2.5.m5.1.1" xref="S5.SS2.SSS2.p2.5.m5.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p2.5.m5.1b"><csymbol cd="latexml" id="S5.SS2.SSS2.p2.5.m5.1.1.cmml" xref="S5.SS2.SSS2.p2.5.m5.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p2.5.m5.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p2.5.m5.1d">%</annotation></semantics></math> / 88<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p2.6.m6.1"><semantics id="S5.SS2.SSS2.p2.6.m6.1a"><mo id="S5.SS2.SSS2.p2.6.m6.1.1" xref="S5.SS2.SSS2.p2.6.m6.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p2.6.m6.1b"><csymbol cd="latexml" id="S5.SS2.SSS2.p2.6.m6.1.1.cmml" xref="S5.SS2.SSS2.p2.6.m6.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p2.6.m6.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p2.6.m6.1d">%</annotation></semantics></math>.
Such few steps of optimization is much less than iNeRF (300 steps), and pi-NeRF (2500 steps), thanks to the accurate initial pose obtained by 1-step pose solving strategy. It can effectively alleviates the local minima suffered by pure optimization based method. The qualitative results shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.F3" title="Figure 3 â€£ V-B2 Results â€£ V-B Results on Synthetic Dataset â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a> shows that our method achieves nearly perfect alignment under large initial pose differences.</p>
</div>
<figure class="ltx_figure" id="S5.SS2.SSS2.1">
<div class="ltx_block" id="S5.SS2.SSS2.1.1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_missing ltx_missing_image" id="S5.SS2.SSS2.1.g1" src=""/>
<figure class="ltx_figure" id="S5.F3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S5.F3.3.2" style="font-size:90%;">Qualitative results of pose estimation on NeRF syntheticÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib15" title="">15</a>]</cite> and real-world LLFF datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib50" title="">50</a>]</cite>. We visualize the results by overlying the target image and NeRF rendering image from the estimated pose.</span></figcaption>
</figure>
</div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Results on Real World Scene</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS1.5.1.1">V-C</span>1 </span>Setting</h4>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">We evaluate on 4 complex scenes captured by LLFFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib50" title="">50</a>]</cite> including <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS1.p1.1.1">Fern</span>, <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS1.p1.1.2">Fortress</span>, <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS1.p1.1.3">Horns</span>, and <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS1.p1.1.4">Room</span> following iNeRF. The model and protocol for pose initialization is the same as in iNeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite>. Here, the scenes are captured from forward view. So the setting is closer to the visual localization task in SLAM.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS2.6.1.1">V-C</span>2 </span>Results</h4>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">On real-world scene, similar to NeRF Synthetic dataset, our method achieves the best results. This dataset is more challenging, because the scenes are captured with forward-facing images, which will result in larger image differences under the same rotation angle. As a result, it leads to performance degradation for the comparison methods. On the contrary, our method even achieves better results (100<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p1.1.m1.1"><semantics id="S5.SS3.SSS2.p1.1.m1.1a"><mo id="S5.SS3.SSS2.p1.1.m1.1.1" xref="S5.SS3.SSS2.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS2.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS3.SSS2.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS2.p1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS2.p1.1.m1.1d">%</annotation></semantics></math>). This verifies the robustness of our method to large pose variations. Compared to synthetic dataset, the improved performance may be because the matcherÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib47" title="">47</a>]</cite> performs better on real-world data.</p>
</div>
<figure class="ltx_figure" id="S5.SS3.SSS2.1">
<div class="ltx_block" id="S5.SS3.SSS2.1.1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_missing ltx_missing_image" id="S5.SS3.SSS2.1.g1" src=""/>
<figure class="ltx_figure ltx_align_center" id="S5.F4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.3.2" style="font-size:90%;">Qualitative results of pose estimation on synthesized occluded data. The comparison methods fail to align the occluded images after hundreds of iterations, while our method aligns well in one step.</span></figcaption>
</figure>
</div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">Results on Occluded Dataset</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS4.SSS1.5.1.1">V-D</span>1 </span>Setting</h4>
<div class="ltx_para" id="S5.SS4.SSS1.p1">
<p class="ltx_p" id="S5.SS4.SSS1.p1.1">We further explore the performance of our proposed method on occluded dataset. The occluded data is synthesized by composing the NeRF synthetic and LLFF dataset. The LLFF real-world images are used as background, and objects from synthetic dataset are randomly transformed and added as foreground. See the leftmost column of Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.F4" title="Figure 4 â€£ V-C2 Results â€£ V-C Results on Real World Scene â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a> for an example.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS4.SSS2.5.1.1">V-D</span>2 </span>Results</h4>
<div class="ltx_para" id="S5.SS4.SSS2.p1">
<p class="ltx_p" id="S5.SS4.SSS2.p1.1">We show qualitative pose estimation results on occluded dataset in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.F4" title="Figure 4 â€£ V-C2 Results â€£ V-C Results on Real World Scene â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a>. For <span class="ltx_text ltx_font_bold" id="S5.SS4.SSS2.p1.1.1">iNeRF</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite>, taking fortress (second row) as an example, with the main object being occluded, the photometric loss computed from repeated pattern of the table cannot guide the pose optimization.
For <span class="ltx_text ltx_font_bold" id="S5.SS4.SSS2.p1.1.2">pi-NeRF</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>]</cite>, Monte Carlo sampling strategy also fail on occluded images, because the photometric error from occluded area can no longer be used as a criterion to sample correct pose.
On the contrary, <span class="ltx_text ltx_font_bold" id="S5.SS4.SSS2.p1.1.3">our method</span> bypasses the occluded area by leveraging the power of matching methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib47" title="">47</a>]</cite>, and still solves good pose in one step. The effect of occlusion robust matching area guided sampling strategy is analyzed in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.T2" title="TABLE II â€£ V-F Ablation Studies â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">II</span></a> of ablation study .</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS5.5.1.1">V-E</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS5.6.2">Efficiency</span>
</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.4">Efficiency is one of our key advantages. We evaluate the efficiency on NeRF Synthetic dataset, and run all experiments on one RTX3090 GPU. Without post refinement, our method runs at 6FPS, and is <span class="ltx_text ltx_font_bold" id="S5.SS5.p1.1.1">90<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS5.p1.1.1.m1.1"><semantics id="S5.SS5.p1.1.1.m1.1a"><mo id="S5.SS5.p1.1.1.m1.1.1" xref="S5.SS5.p1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.1.m1.1b"><times id="S5.SS5.p1.1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p1.1.1.m1.1d">Ã—</annotation></semantics></math></span> faster than former best methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>]</cite>, including <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS5.p1.2.m1.1"><semantics id="S5.SS5.p1.2.m1.1a"><mo id="S5.SS5.p1.2.m1.1.1" xref="S5.SS5.p1.2.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.2.m1.1b"><csymbol cd="latexml" id="S5.SS5.p1.2.m1.1.1.cmml" xref="S5.SS5.p1.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.2.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p1.2.m1.1d">âˆ¼</annotation></semantics></math>50ms for rendering, <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS5.p1.3.m2.1"><semantics id="S5.SS5.p1.3.m2.1a"><mo id="S5.SS5.p1.3.m2.1.1" xref="S5.SS5.p1.3.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.3.m2.1b"><csymbol cd="latexml" id="S5.SS5.p1.3.m2.1.1.cmml" xref="S5.SS5.p1.3.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.3.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p1.3.m2.1d">âˆ¼</annotation></semantics></math>60ms for matching, <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS5.p1.4.m3.1"><semantics id="S5.SS5.p1.4.m3.1a"><mo id="S5.SS5.p1.4.m3.1.1" xref="S5.SS5.p1.4.m3.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.4.m3.1b"><csymbol cd="latexml" id="S5.SS5.p1.4.m3.1.1.cmml" xref="S5.SS5.p1.4.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.4.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p1.4.m3.1d">âˆ¼</annotation></semantics></math>15ms for PnP+RANSAC. For post refinement, our method requires much less iterations (40) comparing to iNeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite> (300) and pi-NeRFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib17" title="">17</a>]</cite> (2500), which also boost the efficiency of pose refinement. We attribute the reduction to the good initialization of our keypoint based strategy. Optimizing Instant-NGPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib1" title="">1</a>]</cite> for 40 steps takes about 400ms.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS6.5.1.1">V-F</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS6.6.2">Ablation Studies</span>
</h3>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.5.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S5.T2.6.2" style="font-size:90%;">Results of ablation studies on the proposed 3D Consistent Points Mining Strategy (3D Consis.), and Keypoint-guided Occlusion Robust Refinement (KOR) strategy, where rot. and trans. denote rotation and translation.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.3" style="width:195.1pt;height:68pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-83.2pt,29.0pt) scale(0.539624111435305,0.539624111435305) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.3.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.3.3.3.4"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.3.3.3.5">Balines</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.2.2.2.2">Mean rot. error (<sup class="ltx_sup" id="S5.T2.2.2.2.2.1">âˆ˜</sup>) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.2.2.2.2.m2.1"><semantics id="S5.T2.2.2.2.2.m2.1a"><mo id="S5.T2.2.2.2.2.m2.1.1" stretchy="false" xref="S5.T2.2.2.2.2.m2.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.m2.1b"><ci id="S5.T2.2.2.2.2.m2.1.1.cmml" xref="S5.T2.2.2.2.2.m2.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.m2.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.3.3.3">Mean trans. error (cm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.3.3.3.3.m1.1"><semantics id="S5.T2.3.3.3.3.m1.1a"><mo id="S5.T2.3.3.3.3.m1.1.1" stretchy="false" xref="S5.T2.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.3.3.4.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T2.3.3.4.1.1"></th>
<th class="ltx_td ltx_nopad_l ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.3.3.4.1.2"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T2.3.3.4.1.3">NeRF Synthetic Dataset</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.3.3.5.2.1">I</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.3.5.2.2">w/o 3D Consis. (1-step)</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.3.3.5.2.3">2.08</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.5.2.4">0.133</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.3.3.6.3.1">II</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.3.6.3.2">w/ 3D Consis. (1-step)</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.3.3.6.3.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.6.3.3.1">1.57</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.6.3.4"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.6.3.4.1">0.096</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.7.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S5.T2.3.3.7.4.1"></th>
<th class="ltx_td ltx_nopad_l ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.3.3.7.4.2"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T2.3.3.7.4.3">Occluded LLFF Dataset</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.3.3.8.5.1">III</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.3.8.5.2">w/o KOR Refine.</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.3.3.8.5.3">0.562</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.8.5.4">0.33</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.3.3.9.6.1">IV</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T2.3.3.9.6.2">w/ KOR Refine</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.3.3.9.6.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.9.6.3.1">0.518</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.3.9.6.4"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.9.6.4.1">0.29</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">We explore the effectiveness of each proposed components, including the 3D consistent point mining strategy and the keypoint-guided sampling strategy for occlusion robust pose refinement.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S5.F5.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.3.2" style="font-size:90%;">Visualization of the points discarded by 3D consistent point mining strategy. </span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS6.p2.1.1">3D Consistent Point Mining.</span> To explore how 3D consistent point mining works, we first visualize the inconsistent points that are discarded in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.F5" title="Figure 5 â€£ V-F Ablation Studies â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">5</span></a>. In the Lego images, the inconsistent points typically locate near the silhouette, where NeRF cannot reconstruct well, and slight mismatch of pixels may cause large change of depth. Similarly, in the fortress image, the discarded points also appear near the fortress edge. Other discarded points are at the corner of the image, whose geometry is also not well-defined as they are rarely seen. In conclusion, the proposed 3D consistent point mining strategy can automatically detect and discard unstable 3D points learned by NeRF, thus improving the pose accuracy.</p>
</div>
<div class="ltx_para" id="S5.SS6.p3">
<p class="ltx_p" id="S5.SS6.p3.1">We also evaluate 3D Consistent Point Mining strategy quantitatively on NeRF synthetic dataset in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.T2" title="TABLE II â€£ V-F Ablation Studies â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">II</span></a>. Here, we report the 1-step results without post refinement. The results validate its effectiveness.</p>
</div>
<div class="ltx_para" id="S5.SS6.p4">
<p class="ltx_p" id="S5.SS6.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS6.p4.1.1">Keypoint-guided Occlusion Robust Refinement.</span></p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S5.F6.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S5.F6.3.2" style="font-size:90%;">Visualization of the effect of the proposed keypoint-guided Occlusion Robust Refinement strategy (KOR). </span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS6.p5">
<p class="ltx_p" id="S5.SS6.p5.1">As shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.F6" title="Figure 6 â€£ V-F Ablation Studies â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">6</span></a>, the proposed <span class="ltx_text ltx_font_italic" id="S5.SS6.p5.1.1">keypoint-guided occlusion robust</span> (KOR) strategy achieves accurate alignment under severe occlusion, while the interest area sampling strategyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib16" title="">16</a>]</cite> (w/refinement, w/o KOR) suffers from local minima. Quantitatively, as shown in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#S5.T2" title="TABLE II â€£ V-F Ablation Studies â€£ V Experiments â€£ Marrying NeRF with Feature Matching for One-step Pose Estimation"><span class="ltx_text ltx_ref_tag">II</span></a>, KOR reduces rotation and translation error by <math alttext="4.4\%" class="ltx_Math" display="inline" id="S5.SS6.p5.1.m1.1"><semantics id="S5.SS6.p5.1.m1.1a"><mrow id="S5.SS6.p5.1.m1.1.1" xref="S5.SS6.p5.1.m1.1.1.cmml"><mn id="S5.SS6.p5.1.m1.1.1.2" xref="S5.SS6.p5.1.m1.1.1.2.cmml">4.4</mn><mo id="S5.SS6.p5.1.m1.1.1.1" xref="S5.SS6.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.p5.1.m1.1b"><apply id="S5.SS6.p5.1.m1.1.1.cmml" xref="S5.SS6.p5.1.m1.1.1"><csymbol cd="latexml" id="S5.SS6.p5.1.m1.1.1.1.cmml" xref="S5.SS6.p5.1.m1.1.1.1">percent</csymbol><cn id="S5.SS6.p5.1.m1.1.1.2.cmml" type="float" xref="S5.SS6.p5.1.m1.1.1.2">4.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p5.1.m1.1c">4.4\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.p5.1.m1.1d">4.4 %</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We have proposed a fast NeRF-based framework for imaged-based, CAD-free novel object pose estimation. By introducing keypoint matching, our method can directly solve the pose with one step, and is free of long optimization time and local minima. Moreover, we propose a 3D consistent point mining strategy to improve the quality of 2D-3D correspondences, and a matching keypoint based sampling strategy to improve the robustness to occluded images. Experiments demonstrate our superior performance and robustness to occlusion. For future work, we hope that this method can be extended to robot manipulation or recent neural field based SLAM tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.00891v1#bib.bib36" title="">36</a>]</cite> to push the efficiency limit of localization.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T.Â MÃ¼ller, A.Â Evans, C.Â Schied, and A.Â Keller, â€œInstant neural graphics
primitives with a multiresolution hash encoding,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">ACM Trans. Graph.</em>,
vol.Â 41, no.Â 4, pp. 102:1â€“102:15, Jul. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y.Â Cong, R.Â Chen, B.Â Ma, H.Â Liu, D.Â Hou, and C.Â Yang, â€œA comprehensive study
of 3-d vision-based robot manipulation,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE Trans. Cybern.</em>,
vol.Â 53, no.Â 3, pp. 1682â€“1698, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D.Â G. Lowe, â€œObject recognition from local scale-invariant features,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the International Conference on Computer Vision,
Kerkyra, Corfu, Greece, September 20-25, 1999</em>.Â Â Â IEEE Computer Society, 1999, pp. 1150â€“1157.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S.Â Hinterstoisser, C.Â Cagniart, S.Â Ilic, P.Â Sturm, N.Â Navab, P.Â Fua, and
V.Â Lepetit, â€œGradient response maps for real-time detection of textureless
objects,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE transactions on pattern analysis and machine
intelligence</em>, vol.Â 34, no.Â 5, pp. 876â€“888, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y.Â Xiang, T.Â Schmidt, V.Â Narayanan, and D.Â Fox, â€œPosecnn: A convolutional
neural network for 6d object pose estimation in cluttered scenes,â€
<em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Robotics: Science and Systems XIV</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C.Â Wang, D.Â Xu, Y.Â Zhu, R.Â MartÃ­n-MartÃ­n, C.Â Lu, L.Â Fei-Fei, and
S.Â Savarese, â€œDensefusion: 6d object pose estimation by iterative dense
fusion,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, 2019, pp. 3343â€“3352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S.Â Peng, Y.Â Liu, Q.Â Huang, X.Â Zhou, and H.Â Bao, â€œPvnet: Pixel-wise voting
network for 6dof pose estimation,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 4561â€“4570.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y.Â Di, R.Â Zhang, Z.Â Lou, F.Â Manhardt, X.Â Ji, N.Â Navab, and F.Â Tombari,
â€œGpv-pose: Category-level object pose estimation via geometry-guided
point-wise voting,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 2022, pp. 6781â€“6791.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
X.Â Chen, Z.Â Dong, J.Â Song, A.Â Geiger, and O.Â Hilliges, â€œCategory level object
pose estimation via neural analysis-by-synthesis,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Computer
Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28,
2020, Proceedings, Part XXVI 16</em>.Â Â Â Springer, 2020, pp. 139â€“156.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T.Â Lee, B.-U. Lee, M.Â Kim, and I.Â S. Kweon, â€œCategory-level metric scale
object shape and pose estimation,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">IEEE Robotics and Automation
Letters</em>, vol.Â 6, no.Â 4, pp. 8575â€“8582, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K.Â Chen, S.Â James, C.Â Sui, Y.-H. Liu, P.Â Abbeel, and Q.Â Dou, â€œStereopose:
Category-level 6d transparent object pose estimation from stereo images via
back-view nocs,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2023 IEEE International Conference on Robotics and
Automation (ICRA)</em>.Â Â Â IEEE, 2023, pp.
2855â€“2861.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J.Â Sun, Z.Â Wang, S.Â Zhang, X.Â He, H.Â Zhao, G.Â Zhang, and X.Â Zhou, â€œOnepose:
One-shot object pose estimation without cad models,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022,
pp. 6825â€“6834.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
X.Â He, J.Â Sun, Y.Â Wang, D.Â Huang, H.Â Bao, and X.Â Zhou, â€œOnepose++:
Keypoint-free one-shot object pose estimation without CAD models,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Advances in Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
V.Â Lepetit, F.Â Moreno-Noguer, and P.Â Fua, â€œEp n p: An accurate o (n) solution
to the p n p problem,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">International journal of computer vision</em>,
vol.Â 81, pp. 155â€“166, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
B.Â Mildenhall, P.Â P. Srinivasan, M.Â Tancik, J.Â T. Barron, R.Â Ramamoorthi, and
R.Â Ng, â€œNerf: Representing scenes as neural radiance fields for view
synthesis,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ECCV</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
L.Â Yen-Chen, P.Â Florence, J.Â T. Barron, A.Â Rodriguez, P.Â Isola, and T.-Y. Lin,
â€œinerf: Inverting neural radiance fields for pose estimation,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">2021 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</em>.Â Â Â IEEE, 2021, pp.
1323â€“1330.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y.Â Lin, T.Â MÃ¼ller, J.Â Tremblay, B.Â Wen, S.Â Tyree, A.Â Evans, P.Â A. Vela, and
S.Â Birchfield, â€œParallel inversion of neural radiance fields for robust pose
estimation,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">2023 IEEE International Conference on Robotics and
Automation (ICRA)</em>.Â Â Â IEEE, 2023, pp.
9377â€“9384.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
K.Â Park, A.Â Mousavian, Y.Â Xiang, and D.Â Fox, â€œLatentfusion: End-to-end
differentiable reconstruction and rendering for unseen object pose
estimation,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition</em>, 2020, pp. 10â€‰710â€“10â€‰719.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A.Â Palazzi, L.Â Bergamini, S.Â Calderara, and R.Â Cucchiara, â€œEnd-to-end 6-dof
object pose estimation through differentiable rasterization,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the European Conference on Computer Vision (ECCV)
Workshops</em>, 2018, pp. 0â€“0.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
W.Â Chen, H.Â Ling, J.Â Gao, E.Â Smith, J.Â Lehtinen, A.Â Jacobson, and S.Â Fidler,
â€œLearning to predict 3d objects with an interpolation-based differentiable
renderer,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in neural information processing systems</em>,
vol.Â 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
L.Â Yariv, J.Â Gu, Y.Â Kasten, and Y.Â Lipman, â€œVolume rendering of neural
implicit surfaces,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Advances in Neural Information Processing
Systems</em>, vol.Â 34, pp. 4805â€“4815, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
L.Â Yariv, Y.Â Kasten, D.Â Moran, M.Â Galun, M.Â Atzmon, B.Â Ronen, and Y.Â Lipman,
â€œMultiview neural surface reconstruction by disentangling geometry and
appearance,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in Neural Information Processing Systems</em>,
vol.Â 33, pp. 2492â€“2502, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
P.Â Wang, L.Â Liu, Y.Â Liu, C.Â Theobalt, T.Â Komura, and W.Â Wang, â€œNeus: Learning
neural implicit surfaces by volume rendering for multi-view reconstruction,â€
<em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2106.10689</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
W.Â Kehl, F.Â Manhardt, F.Â Tombari, S.Â Ilic, and N.Â Navab, â€œSsd-6d: Making
rgb-based 3d detection and 6d pose estimation great again,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE international conference on computer vision</em>,
2017, pp. 1521â€“1529.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M.Â Rad and V.Â Lepetit, â€œBb8: A scalable, accurate, robust to partial occlusion
method for predicting the 3d poses of challenging objects without using
depth,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE international conference on
computer vision</em>, 2017, pp. 3828â€“3836.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
B.Â Tekin, S.Â N. Sinha, and P.Â Fua, â€œReal-time seamless single shot 6d object
pose prediction,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>, 2018, pp. 292â€“301.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
C.Â Song, J.Â Song, and Q.Â Huang, â€œHybridpose: 6d object pose estimation under
hybrid representations,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition</em>, 2020, pp. 431â€“440.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
I.Â Shugurov, F.Â Li, B.Â Busam, and S.Â Ilic, â€œOsop: A multi-stage one shot
object pose estimation framework,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 6835â€“6844.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Y.Â LabbÃ©, L.Â Manuelli, A.Â Mousavian, S.Â Tyree, S.Â Birchfield, J.Â Tremblay,
J.Â Carpentier, M.Â Aubry, D.Â Fox, and J.Â Sivic, â€œMegapose: 6d pose estimation
of novel objects via render &amp; compare,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 6th
Conference on Robot Learning (CoRL)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
G.Â Ponimatkin, Y.Â LabbÃ©, B.Â Russell, M.Â Aubry, and J.Â Sivic, â€œFocal length
and object pose estimation via render and compare,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022,
pp. 3825â€“3834.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S.Â Liu, T.Â Li, W.Â Chen, and H.Â Li, â€œSoft rasterizer: A differentiable renderer
for image-based 3d reasoning,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2019, pp. 7708â€“7717.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
F.Â Petersen, B.Â Goldluecke, C.Â Borgelt, and O.Â Deussen, â€œGenDR: A Generalized
Differentiable Renderer,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">IEEE/CVF International Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
D.Â Maggio, M.Â Abate, J.Â Shi, C.Â Mario, and L.Â Carlone, â€œLoc-nerf: Monte carlo
localization using neural radiance fields,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">2023 IEEE International
Conference on Robotics and Automation (ICRA)</em>.Â Â Â IEEE, 2023, pp. 4018â€“4025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
E.Â Sucar, S.Â Liu, J.Â Ortiz, and A.Â J. Davison, â€œimap: Implicit mapping and
positioning in real-time,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2021, pp. 6229â€“6238.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Z.Â Zhu, S.Â Peng, V.Â Larsson, W.Â Xu, H.Â Bao, Z.Â Cui, M.Â R. Oswald, and
M.Â Pollefeys, â€œNice-slam: Neural implicit scalable encoding for slam,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2022, pp. 12â€‰786â€“12â€‰796.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
F.Â Tosi, Y.Â Zhang, Z.Â Gong, E.Â SandstrÃ¶m, S.Â Mattoccia, M.Â R. Oswald, and
M.Â Poggi, â€œHow nerfs and 3d gaussian splatting are reshaping slam: a
survey,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2402.13255</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
J.Â Tang, S.Â Miller, A.Â Singh, and P.Â Abbeel, â€œA textured object recognition
pipeline for color and depth image data,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">2012 IEEE International
Conference on Robotics and Automation</em>.Â Â Â IEEE, 2012, pp. 3467â€“3474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
M.Â Martinez, A.Â Collet, and S.Â S. Srinivasa, â€œMoped: A scalable and low
latency object recognition and pose estimation system,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">2010 IEEE
International Conference on Robotics and Automation</em>.Â Â Â IEEE, 2010, pp. 2043â€“2049.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A.Â Collet, D.Â Berenson, S.Â S. Srinivasa, and D.Â Ferguson, â€œObject recognition
and full pose registration from a single image for robotic manipulation,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">2009 IEEE International Conference on Robotics and Automation</em>.Â Â Â IEEE, 2009, pp. 48â€“55.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
D.Â G. Lowe, â€œDistinctive image features from scale-invariant keypoints,â€
<em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">International journal of computer vision</em>, vol.Â 60, pp. 91â€“110, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
E.Â Rosten and T.Â Drummond, â€œMachine learning for high-speed corner
detection,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Computer Visionâ€“ECCV 2006: 9th European Conference on
Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part I 9</em>.Â Â Â Springer, 2006, pp. 430â€“443.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
E.Â Rublee, V.Â Rabaud, K.Â Konolige, and G.Â Bradski, â€œOrb: An efficient
alternative to sift or surf,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">2011 International conference on
computer vision</em>.Â Â Â Ieee, 2011, pp.
2564â€“2571.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
K.Â M. Yi, E.Â Trulls, Y.Â Ono, V.Â Lepetit, M.Â Salzmann, and P.Â Fua, â€œLearning to
find good correspondences,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the IEEE conference on
computer vision and pattern recognition</em>, 2018, pp. 2666â€“2674.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
R.Â Ranftl and V.Â Koltun, â€œDeep fundamental matrix estimation,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the European conference on computer vision (ECCV)</em>,
2018, pp. 284â€“299.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
P.-E. Sarlin, D.Â DeTone, T.Â Malisiewicz, and A.Â Rabinovich, â€œSuperglue:
Learning feature matching with graph neural networks,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020,
pp. 4938â€“4947.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
J.Â T. Kajiya and B.Â P. VonÂ Herzen, â€œRay tracing volume densities,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">ACM
SIGGRAPH computer graphics</em>, vol.Â 18, no.Â 3, pp. 165â€“174, 1984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
J.Â Sun, Z.Â Shen, Y.Â Wang, H.Â Bao, and X.Â Zhou, â€œLoftr: Detector-free local
feature matching with transformers,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>, 2021, pp. 8922â€“8931.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
P.-E. Sarlin, A.Â Unagar, M.Â Larsson, H.Â Germain, C.Â Toft, V.Â Larsson,
M.Â Pollefeys, V.Â Lepetit, L.Â Hammarstrand, F.Â Kahl <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">etÂ al.</em>, â€œBack to
the feature: Learning robust camera localization from pixels to pose,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib48.2.2">Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition</em>, 2021, pp. 3247â€“3257.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
M.Â A. Fischler and R.Â C. Bolles, â€œRandom sample consensus: a paradigm for
model fitting with applications to image analysis and automated
cartography,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Communications of the ACM</em>, vol.Â 24, no.Â 6, pp.
381â€“395, 1981.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
B.Â Mildenhall, P.Â P. Srinivasan, R.Â Ortiz-Cayon, N.Â K. Kalantari,
R.Â Ramamoorthi, R.Â Ng, and A.Â Kar, â€œLocal light field fusion: Practical view
synthesis with prescriptive sampling guidelines,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">ACM Transactions on
Graphics (TOG)</em>, vol.Â 38, no.Â 4, pp. 1â€“14, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
V.Â Yugay, Y.Â Li, T.Â Gevers, and M.Â R. Oswald, â€œGaussian-slam: Photo-realistic
dense slam with gaussian splatting,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2312.10070</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
H.Â Matsuki, R.Â Murai, P.Â H. Kelly, and A.Â J. Davison, â€œGaussian splatting
slam,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2312.06741</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
C.Â Yan, D.Â Qu, D.Â Wang, D.Â Xu, Z.Â Wang, B.Â Zhao, and X.Â Li, â€œGs-slam: Dense
visual slam with 3d gaussian splatting,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint
arXiv:2311.11700</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
N.Â Keetha, J.Â Karhade, K.Â M. Jatavallabhula, G.Â Yang, S.Â Scherer, D.Â Ramanan,
and J.Â Luiten, â€œSplatam: Splat, track &amp; map 3d gaussians for dense rgb-d
slam,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2312.02126</em>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu May  2 19:06:36 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
