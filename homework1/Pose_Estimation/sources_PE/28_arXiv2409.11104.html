<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB</title>
<!--Generated on Tue Sep 17 11:54:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="3D Pose Estimation Privileged Information RGB-based Hallucination" lang="en" name="keywords"/>
<base href="/html/2409.11104v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S1" title="In Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S2" title="In Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S2.SS1" title="In 2 Related Work â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Privileged information</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S2.SS2" title="In 2 Related Work â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>3D Human Pose Estimation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3" title="In Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3.SS1" title="In 3 Proposed Method â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Depth-based Privileged Information</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3.SS1.SSS1" title="In 3.1 Depth-based Privileged Information â€£ 3 Proposed Method â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Depth-based pretraining.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3.SS1.SSS2" title="In 3.1 Depth-based Privileged Information â€£ 3 Proposed Method â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>RGB training.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3.SS2" title="In 3 Proposed Method â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Pose Estimation Branches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3.SS3" title="In 3 Proposed Method â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Losses</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4" title="In Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Validation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.SS1" title="In 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.SS2" title="In 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.SS3" title="In 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.SS4" title="In 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Quantitative results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.SS5" title="In 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Qualitative Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.SS6" title="In 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Execution Time Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S5" title="In Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">(eccv)                Package eccv Warning: Package â€˜hyperrefâ€™ is loaded with option â€˜pagebackrefâ€™, which is *not* recommended for camera-ready version</p>
</div>
<span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
University of Modena and Reggio Emilia, Modena, 41100, Italy
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{a.simoni, guido.borghi, roberto.vezzani}@unimore.it</span></span></span>
</span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of Florence, Firenze, 50134, Italy
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{francesco.marchetti, lorenzo.seidenari}@unifi.it</span></span></span>
</span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>University of Siena, Siena, 53100, Italy
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id3.1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>federico.becattini@unisi.it</span></span></span>
</span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Toyota Motor Europe
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id4.1"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">email: </span>{davide.davoli, lorenzo.garattoni, gianpiero.francesca}@toyota-europe.com</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Alessandro Simoni

</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francesco Marchetti
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guido Borghi
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Federico Becattini
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Davide Davoli
</span><span class="ltx_author_notes">44</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lorenzo Garattoni
</span><span class="ltx_author_notes">44</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gianpiero Francesca
</span><span class="ltx_author_notes">44</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lorenzo Seidenari
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Roberto Vezzani
</span><span class="ltx_author_notes">11</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Despite the recent advances in computer vision research, estimating the 3D human pose from single RGB images remains a challenging task, as multiple 3D poses can correspond to the same 2D projection on the image.
In this context, depth data could help to disambiguate the 2D information by providing additional constraints about the distance between objects in the scene and the camera. Unfortunately, the acquisition of accurate depth data is limited to indoor spaces and usually is tied to specific depth technologies and devices, thus limiting generalization capabilities.
In this paper, we propose a method able to leverage the benefits of depth information without compromising its broader applicability and adaptability in a predominantly RGB-camera-centric landscape.
Our approach consists of a heatmap-based 3D pose estimator that, leveraging the paradigm of Privileged Information, is able to hallucinate depth information from the RGB frames given at inference time.
More precisely, depth information is used exclusively during training by enforcing our RGB-based hallucination network to learn similar features to a backbone pre-trained only on depth data.
This approach proves to be effective even when dealing with limited and small datasets.
Experimental results reveal that the paradigm of Privileged Information significantly enhances the modelâ€™s performance, enabling efficient extraction of depth information by using only RGB images.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>3D Pose Estimation Privileged Information RGB-based Hallucination
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The ability to estimate the absolute 3D pose of humans from images is a key technology for several applications, such as human-robot interactionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib35" title="">35</a>]</cite>, activity recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib17" title="">17</a>]</cite>, augmented and virtual realityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib16" title="">16</a>]</cite>, smart factoriesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib43" title="">43</a>]</cite>.
However, despite the recent advances of artificial intelligence and computer vision in the area of video analysisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib40" title="">40</a>]</cite>, estimating the absolute 3D human pose from monocular RGB images is a problem that has not yet been completely solved. Indeed, the direct estimation from monocular RGB images can be viewed as an ill-posed problem due to the depth ambiguity, <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">i.e</em>.<span class="ltx_text" id="S1.p1.1.2"></span> the fact that different 3D skeleton configurations could correspond to the same 2D projection on the image plane <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="287" id="S1.F1.g1" src="extracted/5849741/figures/initial.png" width="389"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Overview of the proposed 3D human pose estimator (3D HPE) approach. The method is based on the paradigm of Privileged InformationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib41" title="">41</a>]</cite>, consisting of providing additional depth data during the training phase. At inference time, the system works only with RGB images.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The use of depth data could alleviate the depth-ambiguity problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib33" title="">33</a>]</cite>. Unfortunately, depth cameras are generally limited to indoor environments and less versatile than RGB cameras, which are therefore preferably deployed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib19" title="">19</a>]</cite>.
In particular, depth acquisition systems have several drawbacks: stereo cameras are limited by their baseline; multi-camera systems are cumbersome to deploy, require synchronization, and are much more prone to failure; time-of-flight cameras have limited range. Even LiDAR systems are expensive and hard to deploy at scale. Moreover, even if very precise, they may provide a too sparse point cloud to reliably estimate the position of small-volume objects such as peopleâ€™s joints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib33" title="">33</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Therefore, in this paper, we propose to leverage the Privileged InformationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib41" title="">41</a>]</cite> paradigm, presenting a 3D human pose estimation model that is based only on RGB input images during inference, and that receives side depth information exclusively at training time, as shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">1</span></a>.
More specifically, this paradigm enables training an RGB-based hallucination network conditioned by a hallucination loss that forces its feature maps to mimic those produced by the same backbone trained on depth data.
Thus, the hallucination network learns to extract cues from RGB images that are similar to those that can be extracted from depth images. This eliminates the need for depth input at inference time and simplifies the deployment of the method.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We apply this paradigm to a heatmap-based method that predicts the 3D pose of humans in world coordinates directly from RGB images.
Specifically, at inference time, the proposed architecture is composed of two input networks, consisting of the effective Small HRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib37" title="">37</a>]</cite> backbone that allows us to keep limited the GPU memory requirements. The extracted feature maps are then concatenated and, through the heatmap-based representation referred as Semi-Perspective Decoupled Heatmaps (SPDH)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib35" title="">35</a>]</cite>, the 3D human pose is finally predicted, as detailed in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In summary, the contributions of this work are the following:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">we apply the paradigm of Privileged Information in the vision-based 3D human pose estimation task; to the best of our knowledge, this is the first investigation in this context, paving the way for future related works;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">we prove that the additional depth information positively contributes to the final 3D pose estimation, even if based only on RGB frames at inference time. In other words, we show the possibility of limiting the acquisition of depth data only for the training stage.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Privileged information</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The first explicit formulation of a method leveraging Privileged Information (also called <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">Side Information</span>) was introduced by Vapnik <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.2">et al</em>.<span class="ltx_text" id="S2.SS1.p1.1.3"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib41" title="">41</a>]</cite>. The main idea was to leverage additional knowledge only during training in order to improve system performance at the time of testing: this learning method is usually referred to as <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.4">Learning Using Privileged Information</span> (LUPI).
Further worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib34" title="">34</a>]</cite> demonstrated the strength of this approach when multi-modal data is available at least at training time, yielding remarkable improvements in several domains by hallucinating different modalitiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib29" title="">29</a>]</cite>. For instance, Chen <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.5">et al</em>.<span class="ltx_text" id="S2.SS1.p1.1.6"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib5" title="">5</a>]</cite> hallucinated a bird-eye view of an urban scene while driving an autonomous agent, whereas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib1" title="">1</a>]</cite> leveraged privileged infrared information for re-identification purposes.
Applied to different architectures, Xu <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.7">et al</em>.<span class="ltx_text" id="S2.SS1.p1.1.8"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib45" title="">45</a>]</cite> proposed to use depth images to improve distance metric learning for the person re-identification task on RGB images.
A similar approach has been used by Hoffman <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.9">et al</em>.<span class="ltx_text" id="S2.SS1.p1.1.10"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib9" title="">9</a>]</cite>, where the authors trained a CNN for RGB-based object recognition by incorporating insights into the training phase. The hallucination branch, trained on RGB images, is capable of mimicking medium-level features of the depth branch, improving the final accuracy score.
Conversely, Borghi <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.11">et al</em>.<span class="ltx_text" id="S2.SS1.p1.1.12"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib2" title="">2</a>]</cite> proposed to use RGB images as side information in a vision-based system based on depth maps for the face verification taskÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Recently, the paradigm has been adopted for the 3D hand pose estimation taskÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib47" title="">47</a>]</cite>: depth data is used during the training to improve the accuracy of the method which predicts the hand pose from RGB images.
Finally, the adoption of side information has been applied to the human pose estimation task for enabling the recognition of the correct postureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib15" title="">15</a>]</cite>.
Differently from our work, their method focuses on the 2D pose estimation task in low-light scenarios. Here the Privileged Information paradigm is leveraged to disambiguate images with low light intensity.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>3D Human Pose Estimation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Recently, the task of single-person 3D pose estimation from single RGB images has become popular. Three main approaches can be identified in the literature. In the first family, methods detect the 2D pose and then project keypoints in the 3D space <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib20" title="">20</a>]</cite>. In the second case, methods jointly estimate 2D and 3D poses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib39" title="">39</a>]</cite>. Finally, in the third case, literature methods predict the final 3D pose directly from monocular and single RGB imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The large majority of the literature works belong to the first category, and use publicly available off-the-shelf 2D human pose estimation methods or present a specific module in their pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib4" title="">4</a>]</cite>.
We observe that these methods can suffer the presence of occlusions, due to which the lifted keypoints can be on the wrong surface and then will project to an inaccurate 3D location.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The other approaches are also not exempt from drawbacks. The joint learning of 2D and 3D poses is often based on large-scale datasets with only 2D pose ground truth annotations, and the final 3D pose is retrieved through structure or anatomical skeleton priors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib28" title="">28</a>]</cite>. The need for large annotated skeleton datasets can be prohibitive for certain applications, such as pose estimation in constrained or privacy-sensitive work environments. In our work, we aim to develop a system that does not rely on any prior assumption about the skeleton anatomy.
On the other hand, the methods that directly predict the 3D pose from monocular images are based on fine discretizations of the 3D space around the human body, and they usually require a significant computational effort in terms of GPU memory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib27" title="">27</a>]</cite>.
The proposed method leverages a lightweight architecture that performs a 3D pose estimation of the human body directly from RGB images. Furthermore, only at training time, it relies on visual features learned from depth images in order to extract additional cues for a better understanding of the 3D scene captured by the camera.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="254" id="S2.F2.g1" src="extracted/5849741/figures/general.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">The general overview of the proposed method.
Adopting the Privileged Learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib42" title="">42</a>]</cite> paradigm, RGB and additional depth data are provided during the training stage. In this manner, through a specific loss, we force the hallucination network, to extract features resembling the ones learned by a model pretrained on depth images. These features are concatenated to the visual RGB features and then given as input to the pose estimation branch based on the SPDH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib35" title="">35</a>]</cite> representation. Finally, the 3D human pose is predicted in world coordinates.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we describe in detail the proposed method which leverages depth-based Privileged Information during training and estimates the 3D human pose using an intermediate heatmap-based representation (SPDH)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib35" title="">35</a>]</cite>.
A general overview of the adopted architecture is depicted in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S2.F2" title="Figure 2 â€£ 2.2 3D Human Pose Estimation â€£ 2 Related Work â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Depth-based Privileged Information</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.11">The paradigm of Privileged Information is applied to the visual encoding step and consists of a two-step training procedure. The network backbone is replicated three times: <math alttext="F_{depth}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">F</mi><mrow id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">d</mi><mo id="S3.SS1.p1.1.m1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS1.p1.1.m1.1.1.3.1a" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.4" xref="S3.SS1.p1.1.m1.1.1.3.4.cmml">p</mi><mo id="S3.SS1.p1.1.m1.1.1.3.1b" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.5" xref="S3.SS1.p1.1.m1.1.1.3.5.cmml">t</mi><mo id="S3.SS1.p1.1.m1.1.1.3.1c" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.6" xref="S3.SS1.p1.1.m1.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ¹</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><times id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.1"></times><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">ğ‘‘</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3">ğ‘’</ci><ci id="S3.SS1.p1.1.m1.1.1.3.4.cmml" xref="S3.SS1.p1.1.m1.1.1.3.4">ğ‘</ci><ci id="S3.SS1.p1.1.m1.1.1.3.5.cmml" xref="S3.SS1.p1.1.m1.1.1.3.5">ğ‘¡</ci><ci id="S3.SS1.p1.1.m1.1.1.3.6.cmml" xref="S3.SS1.p1.1.m1.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">F_{depth}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="F_{RGB}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">F</mi><mrow id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.3.2.cmml">R</mi><mo id="S3.SS1.p1.2.m2.1.1.3.1" xref="S3.SS1.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.2.m2.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.cmml">G</mi><mo id="S3.SS1.p1.2.m2.1.1.3.1a" xref="S3.SS1.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.2.m2.1.1.3.4" xref="S3.SS1.p1.2.m2.1.1.3.4.cmml">B</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ¹</ci><apply id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><times id="S3.SS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3.1"></times><ci id="S3.SS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2">ğ‘…</ci><ci id="S3.SS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3">ğº</ci><ci id="S3.SS1.p1.2.m2.1.1.3.4.cmml" xref="S3.SS1.p1.2.m2.1.1.3.4">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">F_{RGB}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_F start_POSTSUBSCRIPT italic_R italic_G italic_B end_POSTSUBSCRIPT</annotation></semantics></math> e <math alttext="F_{hall}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">F</mi><mrow id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">h</mi><mo id="S3.SS1.p1.3.m3.1.1.3.1" xref="S3.SS1.p1.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml">a</mi><mo id="S3.SS1.p1.3.m3.1.1.3.1a" xref="S3.SS1.p1.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.3.m3.1.1.3.4" xref="S3.SS1.p1.3.m3.1.1.3.4.cmml">l</mi><mo id="S3.SS1.p1.3.m3.1.1.3.1b" xref="S3.SS1.p1.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.3.m3.1.1.3.5" xref="S3.SS1.p1.3.m3.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ¹</ci><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><times id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3.1"></times><ci id="S3.SS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.2">â„</ci><ci id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3">ğ‘</ci><ci id="S3.SS1.p1.3.m3.1.1.3.4.cmml" xref="S3.SS1.p1.3.m3.1.1.3.4">ğ‘™</ci><ci id="S3.SS1.p1.3.m3.1.1.3.5.cmml" xref="S3.SS1.p1.3.m3.1.1.3.5">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">F_{hall}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_F start_POSTSUBSCRIPT italic_h italic_a italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math>. <math alttext="F_{depth}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">F</mi><mrow id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.p1.4.m4.1.1.3.2" xref="S3.SS1.p1.4.m4.1.1.3.2.cmml">d</mi><mo id="S3.SS1.p1.4.m4.1.1.3.1" xref="S3.SS1.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.4.m4.1.1.3.3" xref="S3.SS1.p1.4.m4.1.1.3.3.cmml">e</mi><mo id="S3.SS1.p1.4.m4.1.1.3.1a" xref="S3.SS1.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.4.m4.1.1.3.4" xref="S3.SS1.p1.4.m4.1.1.3.4.cmml">p</mi><mo id="S3.SS1.p1.4.m4.1.1.3.1b" xref="S3.SS1.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.4.m4.1.1.3.5" xref="S3.SS1.p1.4.m4.1.1.3.5.cmml">t</mi><mo id="S3.SS1.p1.4.m4.1.1.3.1c" xref="S3.SS1.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.4.m4.1.1.3.6" xref="S3.SS1.p1.4.m4.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ¹</ci><apply id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><times id="S3.SS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.p1.4.m4.1.1.3.1"></times><ci id="S3.SS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.p1.4.m4.1.1.3.2">ğ‘‘</ci><ci id="S3.SS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3">ğ‘’</ci><ci id="S3.SS1.p1.4.m4.1.1.3.4.cmml" xref="S3.SS1.p1.4.m4.1.1.3.4">ğ‘</ci><ci id="S3.SS1.p1.4.m4.1.1.3.5.cmml" xref="S3.SS1.p1.4.m4.1.1.3.5">ğ‘¡</ci><ci id="S3.SS1.p1.4.m4.1.1.3.6.cmml" xref="S3.SS1.p1.4.m4.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">F_{depth}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_F start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math> is trained individually on depth data, while <math alttext="F_{RGB}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">F</mi><mrow id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3.2" xref="S3.SS1.p1.5.m5.1.1.3.2.cmml">R</mi><mo id="S3.SS1.p1.5.m5.1.1.3.1" xref="S3.SS1.p1.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.5.m5.1.1.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.cmml">G</mi><mo id="S3.SS1.p1.5.m5.1.1.3.1a" xref="S3.SS1.p1.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.5.m5.1.1.3.4" xref="S3.SS1.p1.5.m5.1.1.3.4.cmml">B</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">ğ¹</ci><apply id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3"><times id="S3.SS1.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3.1"></times><ci id="S3.SS1.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.2">ğ‘…</ci><ci id="S3.SS1.p1.5.m5.1.1.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3">ğº</ci><ci id="S3.SS1.p1.5.m5.1.1.3.4.cmml" xref="S3.SS1.p1.5.m5.1.1.3.4">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">F_{RGB}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_F start_POSTSUBSCRIPT italic_R italic_G italic_B end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="F_{hall}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">F</mi><mrow id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.3.2" xref="S3.SS1.p1.6.m6.1.1.3.2.cmml">h</mi><mo id="S3.SS1.p1.6.m6.1.1.3.1" xref="S3.SS1.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.6.m6.1.1.3.3" xref="S3.SS1.p1.6.m6.1.1.3.3.cmml">a</mi><mo id="S3.SS1.p1.6.m6.1.1.3.1a" xref="S3.SS1.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.6.m6.1.1.3.4" xref="S3.SS1.p1.6.m6.1.1.3.4.cmml">l</mi><mo id="S3.SS1.p1.6.m6.1.1.3.1b" xref="S3.SS1.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.6.m6.1.1.3.5" xref="S3.SS1.p1.6.m6.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">ğ¹</ci><apply id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3"><times id="S3.SS1.p1.6.m6.1.1.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.3.1"></times><ci id="S3.SS1.p1.6.m6.1.1.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.3.2">â„</ci><ci id="S3.SS1.p1.6.m6.1.1.3.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3.3">ğ‘</ci><ci id="S3.SS1.p1.6.m6.1.1.3.4.cmml" xref="S3.SS1.p1.6.m6.1.1.3.4">ğ‘™</ci><ci id="S3.SS1.p1.6.m6.1.1.3.5.cmml" xref="S3.SS1.p1.6.m6.1.1.3.5">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">F_{hall}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_F start_POSTSUBSCRIPT italic_h italic_a italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math> are trained simultaneously on RGB data only. While <math alttext="F_{depth}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">F</mi><mrow id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml"><mi id="S3.SS1.p1.7.m7.1.1.3.2" xref="S3.SS1.p1.7.m7.1.1.3.2.cmml">d</mi><mo id="S3.SS1.p1.7.m7.1.1.3.1" xref="S3.SS1.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.7.m7.1.1.3.3" xref="S3.SS1.p1.7.m7.1.1.3.3.cmml">e</mi><mo id="S3.SS1.p1.7.m7.1.1.3.1a" xref="S3.SS1.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.7.m7.1.1.3.4" xref="S3.SS1.p1.7.m7.1.1.3.4.cmml">p</mi><mo id="S3.SS1.p1.7.m7.1.1.3.1b" xref="S3.SS1.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.7.m7.1.1.3.5" xref="S3.SS1.p1.7.m7.1.1.3.5.cmml">t</mi><mo id="S3.SS1.p1.7.m7.1.1.3.1c" xref="S3.SS1.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.7.m7.1.1.3.6" xref="S3.SS1.p1.7.m7.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ¹</ci><apply id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3"><times id="S3.SS1.p1.7.m7.1.1.3.1.cmml" xref="S3.SS1.p1.7.m7.1.1.3.1"></times><ci id="S3.SS1.p1.7.m7.1.1.3.2.cmml" xref="S3.SS1.p1.7.m7.1.1.3.2">ğ‘‘</ci><ci id="S3.SS1.p1.7.m7.1.1.3.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3.3">ğ‘’</ci><ci id="S3.SS1.p1.7.m7.1.1.3.4.cmml" xref="S3.SS1.p1.7.m7.1.1.3.4">ğ‘</ci><ci id="S3.SS1.p1.7.m7.1.1.3.5.cmml" xref="S3.SS1.p1.7.m7.1.1.3.5">ğ‘¡</ci><ci id="S3.SS1.p1.7.m7.1.1.3.6.cmml" xref="S3.SS1.p1.7.m7.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">F_{depth}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_F start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="F_{RGB}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">F</mi><mrow id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml"><mi id="S3.SS1.p1.8.m8.1.1.3.2" xref="S3.SS1.p1.8.m8.1.1.3.2.cmml">R</mi><mo id="S3.SS1.p1.8.m8.1.1.3.1" xref="S3.SS1.p1.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.8.m8.1.1.3.3" xref="S3.SS1.p1.8.m8.1.1.3.3.cmml">G</mi><mo id="S3.SS1.p1.8.m8.1.1.3.1a" xref="S3.SS1.p1.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.8.m8.1.1.3.4" xref="S3.SS1.p1.8.m8.1.1.3.4.cmml">B</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">ğ¹</ci><apply id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3"><times id="S3.SS1.p1.8.m8.1.1.3.1.cmml" xref="S3.SS1.p1.8.m8.1.1.3.1"></times><ci id="S3.SS1.p1.8.m8.1.1.3.2.cmml" xref="S3.SS1.p1.8.m8.1.1.3.2">ğ‘…</ci><ci id="S3.SS1.p1.8.m8.1.1.3.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3">ğº</ci><ci id="S3.SS1.p1.8.m8.1.1.3.4.cmml" xref="S3.SS1.p1.8.m8.1.1.3.4">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">F_{RGB}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_F start_POSTSUBSCRIPT italic_R italic_G italic_B end_POSTSUBSCRIPT</annotation></semantics></math> learn a domain-specific encoding of the scene, <math alttext="F_{hall}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><msub id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">F</mi><mrow id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml"><mi id="S3.SS1.p1.9.m9.1.1.3.2" xref="S3.SS1.p1.9.m9.1.1.3.2.cmml">h</mi><mo id="S3.SS1.p1.9.m9.1.1.3.1" xref="S3.SS1.p1.9.m9.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.9.m9.1.1.3.3" xref="S3.SS1.p1.9.m9.1.1.3.3.cmml">a</mi><mo id="S3.SS1.p1.9.m9.1.1.3.1a" xref="S3.SS1.p1.9.m9.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.9.m9.1.1.3.4" xref="S3.SS1.p1.9.m9.1.1.3.4.cmml">l</mi><mo id="S3.SS1.p1.9.m9.1.1.3.1b" xref="S3.SS1.p1.9.m9.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.9.m9.1.1.3.5" xref="S3.SS1.p1.9.m9.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">ğ¹</ci><apply id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3"><times id="S3.SS1.p1.9.m9.1.1.3.1.cmml" xref="S3.SS1.p1.9.m9.1.1.3.1"></times><ci id="S3.SS1.p1.9.m9.1.1.3.2.cmml" xref="S3.SS1.p1.9.m9.1.1.3.2">â„</ci><ci id="S3.SS1.p1.9.m9.1.1.3.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3.3">ğ‘</ci><ci id="S3.SS1.p1.9.m9.1.1.3.4.cmml" xref="S3.SS1.p1.9.m9.1.1.3.4">ğ‘™</ci><ci id="S3.SS1.p1.9.m9.1.1.3.5.cmml" xref="S3.SS1.p1.9.m9.1.1.3.5">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">F_{hall}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">italic_F start_POSTSUBSCRIPT italic_h italic_a italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math> is able to capture an intermediate representation between RGB and depth by means of a hallucination loss term. Considering the replication of the visual backbone, we opted for a lightweight version of the well-known HRNet backboneÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib36" title="">36</a>]</cite>, called SmallHRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib37" title="">37</a>]</cite>.
This operation limits the number of model parameters (<math alttext="28.5" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m10.1"><semantics id="S3.SS1.p1.10.m10.1a"><mn id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml">28.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><cn id="S3.SS1.p1.10.m10.1.1.cmml" type="float" xref="S3.SS1.p1.10.m10.1.1">28.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">28.5</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.10.m10.1d">28.5</annotation></semantics></math>M vs <math alttext="3.9" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11.1"><semantics id="S3.SS1.p1.11.m11.1a"><mn id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml">3.9</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><cn id="S3.SS1.p1.11.m11.1.1.cmml" type="float" xref="S3.SS1.p1.11.m11.1.1">3.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">3.9</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.11.m11.1d">3.9</annotation></semantics></math>M, for HRNet and SmallHRNet, respectively) and the GPU memory requirements while maintaining good performances in terms of quality and speed.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The whole training procedure is based on two different and sequential steps, as described in the following.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Depth-based pretraining.</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.2">The model <math alttext="F_{depth}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.1.m1.1"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><msub id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS1.p1.1.m1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.1.1.3.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.2.cmml">d</mi><mo id="S3.SS1.SSS1.p1.1.m1.1.1.3.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS1.p1.1.m1.1.1.3.3" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS1.SSS1.p1.1.m1.1.1.3.1a" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS1.p1.1.m1.1.1.3.4" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.4.cmml">p</mi><mo id="S3.SS1.SSS1.p1.1.m1.1.1.3.1b" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS1.p1.1.m1.1.1.3.5" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.5.cmml">t</mi><mo id="S3.SS1.SSS1.p1.1.m1.1.1.3.1c" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS1.p1.1.m1.1.1.3.6" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.2">ğ¹</ci><apply id="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3"><times id="S3.SS1.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.2">ğ‘‘</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.3">ğ‘’</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.4">ğ‘</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.1.3.5.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.5">ğ‘¡</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.1.3.6.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">F_{depth}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math> (color purple in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S2.F2" title="Figure 2 â€£ 2.2 3D Human Pose Estimation â€£ 2 Related Work â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">2</span></a>) is trained using only depth data as input, without the other two backbones.
This model is based on the SPDH representation to solve the task of 3D pose estimation.
The generated embedding <math alttext="f_{depth}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.2.m2.1"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><msub id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml">f</mi><mrow id="S3.SS1.SSS1.p1.2.m2.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.2.cmml">d</mi><mo id="S3.SS1.SSS1.p1.2.m2.1.1.3.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.3.cmml">e</mi><mo id="S3.SS1.SSS1.p1.2.m2.1.1.3.1a" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.4" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.4.cmml">p</mi><mo id="S3.SS1.SSS1.p1.2.m2.1.1.3.1b" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.5" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.5.cmml">t</mi><mo id="S3.SS1.SSS1.p1.2.m2.1.1.3.1c" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.6" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><apply id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.2">ğ‘“</ci><apply id="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3"><times id="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.1"></times><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.2">ğ‘‘</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.3">ğ‘’</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.4.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.4">ğ‘</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.5.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.5">ğ‘¡</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.6.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">f_{depth}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.2.m2.1d">italic_f start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math> are then fed in the Pose Estimation module to decode the SPDH heatmaps.
The training pipeline is visually summarized in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3.F3" title="Figure 3 â€£ 3.1.1 Depth-based pretraining. â€£ 3.1 Depth-based Privileged Information â€£ 3 Proposed Method â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="178" id="S3.F3.g1" src="extracted/5849741/figures/baseline_depth.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.4.2.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.2.1" style="font-size:90%;">Visual representation of the <math alttext="F_{depth}" class="ltx_Math" display="inline" id="S3.F3.2.1.m1.1"><semantics id="S3.F3.2.1.m1.1b"><msub id="S3.F3.2.1.m1.1.1" xref="S3.F3.2.1.m1.1.1.cmml"><mi id="S3.F3.2.1.m1.1.1.2" xref="S3.F3.2.1.m1.1.1.2.cmml">F</mi><mrow id="S3.F3.2.1.m1.1.1.3" xref="S3.F3.2.1.m1.1.1.3.cmml"><mi id="S3.F3.2.1.m1.1.1.3.2" xref="S3.F3.2.1.m1.1.1.3.2.cmml">d</mi><mo id="S3.F3.2.1.m1.1.1.3.1" xref="S3.F3.2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.F3.2.1.m1.1.1.3.3" xref="S3.F3.2.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.F3.2.1.m1.1.1.3.1b" xref="S3.F3.2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.F3.2.1.m1.1.1.3.4" xref="S3.F3.2.1.m1.1.1.3.4.cmml">p</mi><mo id="S3.F3.2.1.m1.1.1.3.1c" xref="S3.F3.2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.F3.2.1.m1.1.1.3.5" xref="S3.F3.2.1.m1.1.1.3.5.cmml">t</mi><mo id="S3.F3.2.1.m1.1.1.3.1d" xref="S3.F3.2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.F3.2.1.m1.1.1.3.6" xref="S3.F3.2.1.m1.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F3.2.1.m1.1c"><apply id="S3.F3.2.1.m1.1.1.cmml" xref="S3.F3.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F3.2.1.m1.1.1.1.cmml" xref="S3.F3.2.1.m1.1.1">subscript</csymbol><ci id="S3.F3.2.1.m1.1.1.2.cmml" xref="S3.F3.2.1.m1.1.1.2">ğ¹</ci><apply id="S3.F3.2.1.m1.1.1.3.cmml" xref="S3.F3.2.1.m1.1.1.3"><times id="S3.F3.2.1.m1.1.1.3.1.cmml" xref="S3.F3.2.1.m1.1.1.3.1"></times><ci id="S3.F3.2.1.m1.1.1.3.2.cmml" xref="S3.F3.2.1.m1.1.1.3.2">ğ‘‘</ci><ci id="S3.F3.2.1.m1.1.1.3.3.cmml" xref="S3.F3.2.1.m1.1.1.3.3">ğ‘’</ci><ci id="S3.F3.2.1.m1.1.1.3.4.cmml" xref="S3.F3.2.1.m1.1.1.3.4">ğ‘</ci><ci id="S3.F3.2.1.m1.1.1.3.5.cmml" xref="S3.F3.2.1.m1.1.1.3.5">ğ‘¡</ci><ci id="S3.F3.2.1.m1.1.1.3.6.cmml" xref="S3.F3.2.1.m1.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.1.m1.1d">F_{depth}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.2.1.m1.1e">italic_F start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math> model (SmallHRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib37" title="">37</a>]</cite> backbone) that predicts the 3D human pose exploiting the SPDH representation. This is the first step of the whole training procedure based on the Privileged Information paradigm.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>RGB training.</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.9">Once the depth model <math alttext="F_{depth}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.1.m1.1"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><msub id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.1.1.3.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.2.cmml">d</mi><mo id="S3.SS1.SSS2.p1.1.m1.1.1.3.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.1.m1.1.1.3.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS1.SSS2.p1.1.m1.1.1.3.1a" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.1.m1.1.1.3.4" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.4.cmml">p</mi><mo id="S3.SS1.SSS2.p1.1.m1.1.1.3.1b" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.1.m1.1.1.3.5" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.5.cmml">t</mi><mo id="S3.SS1.SSS2.p1.1.m1.1.1.3.1c" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.1.m1.1.1.3.6" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.2">ğ¹</ci><apply id="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3"><times id="S3.SS1.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.2">ğ‘‘</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.3">ğ‘’</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.4">ğ‘</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3.5.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.5">ğ‘¡</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3.6.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">F_{depth}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math> has been trained, its weights are frozen and two other instances of the backbone are activated (blue and orange components in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S2.F2" title="Figure 2 â€£ 2.2 3D Human Pose Estimation â€£ 2 Related Work â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">2</span></a>). The two networks <math alttext="F_{RGB}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.2.m2.1"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><msub id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p1.2.m2.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS2.p1.2.m2.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.2.m2.1.1.3.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.2.cmml">R</mi><mo id="S3.SS1.SSS2.p1.2.m2.1.1.3.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.2.m2.1.1.3.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.3.cmml">G</mi><mo id="S3.SS1.SSS2.p1.2.m2.1.1.3.1a" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.2.m2.1.1.3.4" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.4.cmml">B</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.2">ğ¹</ci><apply id="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3"><times id="S3.SS1.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.1"></times><ci id="S3.SS1.SSS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.2">ğ‘…</ci><ci id="S3.SS1.SSS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.3">ğº</ci><ci id="S3.SS1.SSS2.p1.2.m2.1.1.3.4.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.4">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">F_{RGB}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.2.m2.1d">italic_F start_POSTSUBSCRIPT italic_R italic_G italic_B end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="F_{hall}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.3.m3.1"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><msub id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS2.p1.3.m3.1.1.2" xref="S3.SS1.SSS2.p1.3.m3.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS2.p1.3.m3.1.1.3" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.3.m3.1.1.3.2" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.2.cmml">h</mi><mo id="S3.SS1.SSS2.p1.3.m3.1.1.3.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.3.m3.1.1.3.3" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.3.cmml">a</mi><mo id="S3.SS1.SSS2.p1.3.m3.1.1.3.1a" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.3.m3.1.1.3.4" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.4.cmml">l</mi><mo id="S3.SS1.SSS2.p1.3.m3.1.1.3.1b" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.3.m3.1.1.3.5" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><apply id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.2">ğ¹</ci><apply id="S3.SS1.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3"><times id="S3.SS1.SSS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.1"></times><ci id="S3.SS1.SSS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.2">â„</ci><ci id="S3.SS1.SSS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.3">ğ‘</ci><ci id="S3.SS1.SSS2.p1.3.m3.1.1.3.4.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.4">ğ‘™</ci><ci id="S3.SS1.SSS2.p1.3.m3.1.1.3.5.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.5">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">F_{hall}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.3.m3.1d">italic_F start_POSTSUBSCRIPT italic_h italic_a italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math> receive as input an RGB image and generate different and complementary embeddings <math alttext="f_{RGB}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.4.m4.1"><semantics id="S3.SS1.SSS2.p1.4.m4.1a"><msub id="S3.SS1.SSS2.p1.4.m4.1.1" xref="S3.SS1.SSS2.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS2.p1.4.m4.1.1.2" xref="S3.SS1.SSS2.p1.4.m4.1.1.2.cmml">f</mi><mrow id="S3.SS1.SSS2.p1.4.m4.1.1.3" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.4.m4.1.1.3.2" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.2.cmml">R</mi><mo id="S3.SS1.SSS2.p1.4.m4.1.1.3.1" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.4.m4.1.1.3.3" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.3.cmml">G</mi><mo id="S3.SS1.SSS2.p1.4.m4.1.1.3.1a" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.4.m4.1.1.3.4" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.4.cmml">B</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.4.m4.1b"><apply id="S3.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1.2">ğ‘“</ci><apply id="S3.SS1.SSS2.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1.3"><times id="S3.SS1.SSS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.1"></times><ci id="S3.SS1.SSS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.2">ğ‘…</ci><ci id="S3.SS1.SSS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.3">ğº</ci><ci id="S3.SS1.SSS2.p1.4.m4.1.1.3.4.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.4">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.4.m4.1c">f_{RGB}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.4.m4.1d">italic_f start_POSTSUBSCRIPT italic_R italic_G italic_B end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{hall}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.5.m5.1"><semantics id="S3.SS1.SSS2.p1.5.m5.1a"><msub id="S3.SS1.SSS2.p1.5.m5.1.1" xref="S3.SS1.SSS2.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS2.p1.5.m5.1.1.2" xref="S3.SS1.SSS2.p1.5.m5.1.1.2.cmml">f</mi><mrow id="S3.SS1.SSS2.p1.5.m5.1.1.3" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.5.m5.1.1.3.2" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.2.cmml">h</mi><mo id="S3.SS1.SSS2.p1.5.m5.1.1.3.1" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.5.m5.1.1.3.3" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.3.cmml">a</mi><mo id="S3.SS1.SSS2.p1.5.m5.1.1.3.1a" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.5.m5.1.1.3.4" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.4.cmml">l</mi><mo id="S3.SS1.SSS2.p1.5.m5.1.1.3.1b" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.5.m5.1.1.3.5" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.5.m5.1b"><apply id="S3.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1.2">ğ‘“</ci><apply id="S3.SS1.SSS2.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1.3"><times id="S3.SS1.SSS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.1"></times><ci id="S3.SS1.SSS2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.2">â„</ci><ci id="S3.SS1.SSS2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.3">ğ‘</ci><ci id="S3.SS1.SSS2.p1.5.m5.1.1.3.4.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.4">ğ‘™</ci><ci id="S3.SS1.SSS2.p1.5.m5.1.1.3.5.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1.3.5">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.5.m5.1c">f_{hall}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.5.m5.1d">italic_f start_POSTSUBSCRIPT italic_h italic_a italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math> thanks to a hallucination loss that forces the hallucination network <math alttext="F_{hall}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.6.m6.1"><semantics id="S3.SS1.SSS2.p1.6.m6.1a"><msub id="S3.SS1.SSS2.p1.6.m6.1.1" xref="S3.SS1.SSS2.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS2.p1.6.m6.1.1.2" xref="S3.SS1.SSS2.p1.6.m6.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS2.p1.6.m6.1.1.3" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.6.m6.1.1.3.2" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.2.cmml">h</mi><mo id="S3.SS1.SSS2.p1.6.m6.1.1.3.1" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.6.m6.1.1.3.3" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.3.cmml">a</mi><mo id="S3.SS1.SSS2.p1.6.m6.1.1.3.1a" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.6.m6.1.1.3.4" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.4.cmml">l</mi><mo id="S3.SS1.SSS2.p1.6.m6.1.1.3.1b" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.6.m6.1.1.3.5" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.6.m6.1b"><apply id="S3.SS1.SSS2.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1.2">ğ¹</ci><apply id="S3.SS1.SSS2.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1.3"><times id="S3.SS1.SSS2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.1"></times><ci id="S3.SS1.SSS2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.2">â„</ci><ci id="S3.SS1.SSS2.p1.6.m6.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.3">ğ‘</ci><ci id="S3.SS1.SSS2.p1.6.m6.1.1.3.4.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.4">ğ‘™</ci><ci id="S3.SS1.SSS2.p1.6.m6.1.1.3.5.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.5">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.6.m6.1c">F_{hall}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.6.m6.1d">italic_F start_POSTSUBSCRIPT italic_h italic_a italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math> (orange) to produce intermediate features similar to the ones produced by the backbone <math alttext="F_{depth}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.7.m7.1"><semantics id="S3.SS1.SSS2.p1.7.m7.1a"><msub id="S3.SS1.SSS2.p1.7.m7.1.1" xref="S3.SS1.SSS2.p1.7.m7.1.1.cmml"><mi id="S3.SS1.SSS2.p1.7.m7.1.1.2" xref="S3.SS1.SSS2.p1.7.m7.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS2.p1.7.m7.1.1.3" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.7.m7.1.1.3.2" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.2.cmml">d</mi><mo id="S3.SS1.SSS2.p1.7.m7.1.1.3.1" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.7.m7.1.1.3.3" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.3.cmml">e</mi><mo id="S3.SS1.SSS2.p1.7.m7.1.1.3.1a" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.7.m7.1.1.3.4" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.4.cmml">p</mi><mo id="S3.SS1.SSS2.p1.7.m7.1.1.3.1b" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.7.m7.1.1.3.5" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.5.cmml">t</mi><mo id="S3.SS1.SSS2.p1.7.m7.1.1.3.1c" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.7.m7.1.1.3.6" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.7.m7.1b"><apply id="S3.SS1.SSS2.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1.2">ğ¹</ci><apply id="S3.SS1.SSS2.p1.7.m7.1.1.3.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1.3"><times id="S3.SS1.SSS2.p1.7.m7.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.1"></times><ci id="S3.SS1.SSS2.p1.7.m7.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.2">ğ‘‘</ci><ci id="S3.SS1.SSS2.p1.7.m7.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.3">ğ‘’</ci><ci id="S3.SS1.SSS2.p1.7.m7.1.1.3.4.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.4">ğ‘</ci><ci id="S3.SS1.SSS2.p1.7.m7.1.1.3.5.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.5">ğ‘¡</ci><ci id="S3.SS1.SSS2.p1.7.m7.1.1.3.6.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.7.m7.1c">F_{depth}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.7.m7.1d">italic_F start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math>. During this second training step as well as during inference, the heatmaps are decoded from the concatenation of two embeddings, coming from the two RGB backbones (<math alttext="F_{RGB}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.8.m8.1"><semantics id="S3.SS1.SSS2.p1.8.m8.1a"><msub id="S3.SS1.SSS2.p1.8.m8.1.1" xref="S3.SS1.SSS2.p1.8.m8.1.1.cmml"><mi id="S3.SS1.SSS2.p1.8.m8.1.1.2" xref="S3.SS1.SSS2.p1.8.m8.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS2.p1.8.m8.1.1.3" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.8.m8.1.1.3.2" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.2.cmml">R</mi><mo id="S3.SS1.SSS2.p1.8.m8.1.1.3.1" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.8.m8.1.1.3.3" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.3.cmml">G</mi><mo id="S3.SS1.SSS2.p1.8.m8.1.1.3.1a" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.8.m8.1.1.3.4" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.4.cmml">B</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.8.m8.1b"><apply id="S3.SS1.SSS2.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.8.m8.1.1.1.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.8.m8.1.1.2.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1.2">ğ¹</ci><apply id="S3.SS1.SSS2.p1.8.m8.1.1.3.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1.3"><times id="S3.SS1.SSS2.p1.8.m8.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.1"></times><ci id="S3.SS1.SSS2.p1.8.m8.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.2">ğ‘…</ci><ci id="S3.SS1.SSS2.p1.8.m8.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.3">ğº</ci><ci id="S3.SS1.SSS2.p1.8.m8.1.1.3.4.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.4">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.8.m8.1c">F_{RGB}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.8.m8.1d">italic_F start_POSTSUBSCRIPT italic_R italic_G italic_B end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="F_{hall}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.9.m9.1"><semantics id="S3.SS1.SSS2.p1.9.m9.1a"><msub id="S3.SS1.SSS2.p1.9.m9.1.1" xref="S3.SS1.SSS2.p1.9.m9.1.1.cmml"><mi id="S3.SS1.SSS2.p1.9.m9.1.1.2" xref="S3.SS1.SSS2.p1.9.m9.1.1.2.cmml">F</mi><mrow id="S3.SS1.SSS2.p1.9.m9.1.1.3" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.9.m9.1.1.3.2" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.2.cmml">h</mi><mo id="S3.SS1.SSS2.p1.9.m9.1.1.3.1" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.9.m9.1.1.3.3" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.3.cmml">a</mi><mo id="S3.SS1.SSS2.p1.9.m9.1.1.3.1a" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.9.m9.1.1.3.4" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.4.cmml">l</mi><mo id="S3.SS1.SSS2.p1.9.m9.1.1.3.1b" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p1.9.m9.1.1.3.5" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.9.m9.1b"><apply id="S3.SS1.SSS2.p1.9.m9.1.1.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.9.m9.1.1.1.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.9.m9.1.1.2.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1.2">ğ¹</ci><apply id="S3.SS1.SSS2.p1.9.m9.1.1.3.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1.3"><times id="S3.SS1.SSS2.p1.9.m9.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.1"></times><ci id="S3.SS1.SSS2.p1.9.m9.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.2">â„</ci><ci id="S3.SS1.SSS2.p1.9.m9.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.3">ğ‘</ci><ci id="S3.SS1.SSS2.p1.9.m9.1.1.3.4.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.4">ğ‘™</ci><ci id="S3.SS1.SSS2.p1.9.m9.1.1.3.5.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.5">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.9.m9.1c">F_{hall}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.9.m9.1d">italic_F start_POSTSUBSCRIPT italic_h italic_a italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math>).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pose Estimation Branches</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.4">Another key element of the proposed system is the direct estimation of the 3D pose through heatmap generation. In particular, we selected the Semi-Perspective Decoupled Heatmaps (SPDH)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib35" title="">35</a>]</cite> representation which maps each 3D joint location into two decoupled bi-dimensional spaces: the <math alttext="uv" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">u</mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ‘¢</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">uv</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_u italic_v</annotation></semantics></math>, <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.4.1">i.e</em>.<span class="ltx_text" id="S3.SS2.p1.4.2"></span> the camera image plane, and the <math alttext="uz" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">u</mi><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">â¢</mo><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ‘¢</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">uz</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_u italic_z</annotation></semantics></math> space, the projection of <math alttext="z" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_z</annotation></semantics></math>-values along the <math alttext="u" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_u</annotation></semantics></math> dimension. The two heatmaps are generated from a common embedding obtained concatenating the visual features extracted by the RGB backbone and the hallucination network.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="279" id="S3.F4.g1" src="extracted/5849741/figures/pe_branch.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">A detailed visualization of the pose estimation branch that leverages the fusion of multi-resolution features to improve the accuracy of the predicted SPDH heatmaps. The input is the concatenation of the feature maps extracted through the hallucination network and the RGB-based one (see Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">1</span></a>).</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.6">The SPDH output is generated by a double-branch module, as detailed in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3.F4" title="Figure 4 â€£ 3.2 Pose Estimation Branches â€£ 3 Proposed Method â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">4</span></a>, that leverages multi-resolution features to improve the joint localization precision. In particular, the concatenated features are given as input to a <math alttext="uv" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">u</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ğ‘¢</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">uv</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_u italic_v</annotation></semantics></math> and <math alttext="uz" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">u</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">â¢</mo><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ‘¢</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">uz</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_u italic_z</annotation></semantics></math> branch. Each branch is composed of <math alttext="3" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mn id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><cn id="S3.SS2.p2.3.m3.1.1.cmml" type="integer" xref="S3.SS2.p2.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">3</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">3</annotation></semantics></math> convolutional layers with kernel size <math alttext="3\times 3" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mn id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">3</mn><mo id="S3.SS2.p2.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.4.m4.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></times><cn id="S3.SS2.p2.4.m4.1.1.2.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1.2">3</cn><cn id="S3.SS2.p2.4.m4.1.1.3.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">3\times 3</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">3 Ã— 3</annotation></semantics></math> that predict a set of heatmaps with increasing resolution of <math alttext="\{\frac{1}{4},\frac{1}{2},1\}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.3"><semantics id="S3.SS2.p2.5.m5.3a"><mrow id="S3.SS2.p2.5.m5.3.4.2" xref="S3.SS2.p2.5.m5.3.4.1.cmml"><mo id="S3.SS2.p2.5.m5.3.4.2.1" stretchy="false" xref="S3.SS2.p2.5.m5.3.4.1.cmml">{</mo><mfrac id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mn id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">1</mn><mn id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">4</mn></mfrac><mo id="S3.SS2.p2.5.m5.3.4.2.2" xref="S3.SS2.p2.5.m5.3.4.1.cmml">,</mo><mfrac id="S3.SS2.p2.5.m5.2.2" xref="S3.SS2.p2.5.m5.2.2.cmml"><mn id="S3.SS2.p2.5.m5.2.2.2" xref="S3.SS2.p2.5.m5.2.2.2.cmml">1</mn><mn id="S3.SS2.p2.5.m5.2.2.3" xref="S3.SS2.p2.5.m5.2.2.3.cmml">2</mn></mfrac><mo id="S3.SS2.p2.5.m5.3.4.2.3" xref="S3.SS2.p2.5.m5.3.4.1.cmml">,</mo><mn id="S3.SS2.p2.5.m5.3.3" xref="S3.SS2.p2.5.m5.3.3.cmml">1</mn><mo id="S3.SS2.p2.5.m5.3.4.2.4" stretchy="false" xref="S3.SS2.p2.5.m5.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.3b"><set id="S3.SS2.p2.5.m5.3.4.1.cmml" xref="S3.SS2.p2.5.m5.3.4.2"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><divide id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"></divide><cn id="S3.SS2.p2.5.m5.1.1.2.cmml" type="integer" xref="S3.SS2.p2.5.m5.1.1.2">1</cn><cn id="S3.SS2.p2.5.m5.1.1.3.cmml" type="integer" xref="S3.SS2.p2.5.m5.1.1.3">4</cn></apply><apply id="S3.SS2.p2.5.m5.2.2.cmml" xref="S3.SS2.p2.5.m5.2.2"><divide id="S3.SS2.p2.5.m5.2.2.1.cmml" xref="S3.SS2.p2.5.m5.2.2"></divide><cn id="S3.SS2.p2.5.m5.2.2.2.cmml" type="integer" xref="S3.SS2.p2.5.m5.2.2.2">1</cn><cn id="S3.SS2.p2.5.m5.2.2.3.cmml" type="integer" xref="S3.SS2.p2.5.m5.2.2.3">2</cn></apply><cn id="S3.SS2.p2.5.m5.3.3.cmml" type="integer" xref="S3.SS2.p2.5.m5.3.3">1</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.3c">\{\frac{1}{4},\frac{1}{2},1\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.3d">{ divide start_ARG 1 end_ARG start_ARG 4 end_ARG , divide start_ARG 1 end_ARG start_ARG 2 end_ARG , 1 }</annotation></semantics></math> with respect to the input image size. The upsampling operation is made by a residual block with a transpose convolutional layer. Finally, the multi-resolution outputs are upsampled and summed together in order to obtain the final prediction of dimension <math alttext="B\times J\times H\times W" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">B</mi><mo id="S3.SS2.p2.6.m6.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.6.m6.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">J</mi><mo id="S3.SS2.p2.6.m6.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.6.m6.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p2.6.m6.1.1.4" xref="S3.SS2.p2.6.m6.1.1.4.cmml">H</mi><mo id="S3.SS2.p2.6.m6.1.1.1b" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.6.m6.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p2.6.m6.1.1.5" xref="S3.SS2.p2.6.m6.1.1.5.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><times id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></times><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">ğµ</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">ğ½</ci><ci id="S3.SS2.p2.6.m6.1.1.4.cmml" xref="S3.SS2.p2.6.m6.1.1.4">ğ»</ci><ci id="S3.SS2.p2.6.m6.1.1.5.cmml" xref="S3.SS2.p2.6.m6.1.1.5">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">B\times J\times H\times W</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_B Ã— italic_J Ã— italic_H Ã— italic_W</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Losses</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The proposed method is trained end-to-end to optimize the SPDH generation leveraging the Privileged Information paradigm through intermediate features learning. We use a standard Mean Squared Error (MSE) loss for the privileged information task and a masked MSE for the pose estimation task:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{PI}=||f_{depth}-f_{hall}||_{2}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">â„’</mi><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">P</mi><mo id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">I</mi></mrow></msub><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><msub id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml">f</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2.cmml">d</mi><mo id="S3.E1.m1.1.1.1.1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.1.1.2.3.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.3.cmml">e</mi><mo id="S3.E1.m1.1.1.1.1.1.1.2.3.1a" xref="S3.E1.m1.1.1.1.1.1.1.2.3.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.2.3.4" xref="S3.E1.m1.1.1.1.1.1.1.2.3.4.cmml">p</mi><mo id="S3.E1.m1.1.1.1.1.1.1.2.3.1b" xref="S3.E1.m1.1.1.1.1.1.1.2.3.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.2.3.5" xref="S3.E1.m1.1.1.1.1.1.1.2.3.5.cmml">t</mi><mo id="S3.E1.m1.1.1.1.1.1.1.2.3.1c" xref="S3.E1.m1.1.1.1.1.1.1.2.3.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.2.3.6" xref="S3.E1.m1.1.1.1.1.1.1.2.3.6.cmml">h</mi></mrow></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">f</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.3.2.cmml">h</mi><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.3.3.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.3.cmml">a</mi><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.1a" xref="S3.E1.m1.1.1.1.1.1.1.3.3.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.4" xref="S3.E1.m1.1.1.1.1.1.1.3.3.4.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.1b" xref="S3.E1.m1.1.1.1.1.1.1.3.3.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.5" xref="S3.E1.m1.1.1.1.1.1.1.3.3.5.cmml">l</mi></mrow></msub></mrow><mo id="S3.E1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">â„’</ci><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><times id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">ğ‘ƒ</ci><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">ğ¼</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">ğ‘“</ci><apply id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3"><times id="S3.E1.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2">ğ‘‘</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.3">ğ‘’</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.4">ğ‘</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.5.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.5">ğ‘¡</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.6.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.6">â„</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">ğ‘“</ci><apply id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.2">â„</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.3">ğ‘</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.4">ğ‘™</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.5.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.5">ğ‘™</ci></apply></apply></apply></apply><cn id="S3.E1.m1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{L}_{PI}=||f_{depth}-f_{hall}||_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_P italic_I end_POSTSUBSCRIPT = | | italic_f start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT italic_h italic_a italic_l italic_l end_POSTSUBSCRIPT | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{PE}=\frac{1}{|\mathcal{J}|}\sum_{j\in\mathcal{J}}m_{j}\cdot||H_{j%
}-\widehat{H}_{j}||_{2}" class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><msub id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.3.2" xref="S3.E2.m1.2.2.3.2.cmml">â„’</mi><mrow id="S3.E2.m1.2.2.3.3" xref="S3.E2.m1.2.2.3.3.cmml"><mi id="S3.E2.m1.2.2.3.3.2" xref="S3.E2.m1.2.2.3.3.2.cmml">P</mi><mo id="S3.E2.m1.2.2.3.3.1" xref="S3.E2.m1.2.2.3.3.1.cmml">â¢</mo><mi id="S3.E2.m1.2.2.3.3.3" xref="S3.E2.m1.2.2.3.3.3.cmml">E</mi></mrow></msub><mo id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mfrac id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mn id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">1</mn><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.3.1" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">ğ’¥</mi><mo id="S3.E2.m1.1.1.1.3.2" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><munder id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.2.2" movablelimits="false" xref="S3.E2.m1.2.2.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.2.2.1.1.2.3" xref="S3.E2.m1.2.2.1.1.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.2.3.2" xref="S3.E2.m1.2.2.1.1.2.3.2.cmml">j</mi><mo id="S3.E2.m1.2.2.1.1.2.3.1" xref="S3.E2.m1.2.2.1.1.2.3.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.2.3.3" xref="S3.E2.m1.2.2.1.1.2.3.3.cmml">ğ’¥</mi></mrow></munder><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.3.2.cmml">m</mi><mi id="S3.E2.m1.2.2.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.3.3.cmml">j</mi></msub><mo id="S3.E2.m1.2.2.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.2.2.1.1.1.2.cmml">â‹…</mo><msub id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">H</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml">H</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E2.m1.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"></eq><apply id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.3">subscript</csymbol><ci id="S3.E2.m1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.3.2">â„’</ci><apply id="S3.E2.m1.2.2.3.3.cmml" xref="S3.E2.m1.2.2.3.3"><times id="S3.E2.m1.2.2.3.3.1.cmml" xref="S3.E2.m1.2.2.3.3.1"></times><ci id="S3.E2.m1.2.2.3.3.2.cmml" xref="S3.E2.m1.2.2.3.3.2">ğ‘ƒ</ci><ci id="S3.E2.m1.2.2.3.3.3.cmml" xref="S3.E2.m1.2.2.3.3.3">ğ¸</ci></apply></apply><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><times id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></times><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><divide id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1"></divide><cn id="S3.E2.m1.1.1.3.cmml" type="integer" xref="S3.E2.m1.1.1.3">1</cn><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.3"><abs id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.3.1"></abs><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ğ’¥</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1"><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2"></sum><apply id="S3.E2.m1.2.2.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3"><in id="S3.E2.m1.2.2.1.1.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.2.3.1"></in><ci id="S3.E2.m1.2.2.1.1.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.2.3.2">ğ‘—</ci><ci id="S3.E2.m1.2.2.1.1.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3.3">ğ’¥</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><ci id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2">â‹…</ci><apply id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.3.2">ğ‘š</ci><ci id="S3.E2.m1.2.2.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3.3">ğ‘—</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2">ğ»</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.3">ğ‘—</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2"><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.2">ğ»</ci></apply><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply></apply><cn id="S3.E2.m1.2.2.1.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\mathcal{L}_{PE}=\frac{1}{|\mathcal{J}|}\sum_{j\in\mathcal{J}}m_{j}\cdot||H_{j%
}-\widehat{H}_{j}||_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">caligraphic_L start_POSTSUBSCRIPT italic_P italic_E end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | caligraphic_J | end_ARG âˆ‘ start_POSTSUBSCRIPT italic_j âˆˆ caligraphic_J end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT â‹… | | italic_H start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - over^ start_ARG italic_H end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\mathcal{L}_{PI}+\mathcal{L}_{PE}" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">â„’</mi><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><msub id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml">â„’</mi><mrow id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3.cmml"><mi id="S3.E3.m1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.3.2.3.2.cmml">P</mi><mo id="S3.E3.m1.1.1.3.2.3.1" xref="S3.E3.m1.1.1.3.2.3.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.2.3.3" xref="S3.E3.m1.1.1.3.2.3.3.cmml">I</mi></mrow></msub><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><msub id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml">â„’</mi><mrow id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.3.3.3.2.cmml">P</mi><mo id="S3.E3.m1.1.1.3.3.3.1" xref="S3.E3.m1.1.1.3.3.3.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.3.3.3.3.cmml">E</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><ci id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">â„’</ci><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><plus id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2">â„’</ci><apply id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3"><times id="S3.E3.m1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.3.2.3.1"></times><ci id="S3.E3.m1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.3.2.3.2">ğ‘ƒ</ci><ci id="S3.E3.m1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.3.2.3.3">ğ¼</ci></apply></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2">â„’</ci><apply id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3"><times id="S3.E3.m1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.3.1"></times><ci id="S3.E3.m1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.3.2">ğ‘ƒ</ci><ci id="S3.E3.m1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3.3">ğ¸</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\mathcal{L}=\mathcal{L}_{PI}+\mathcal{L}_{PE}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">caligraphic_L = caligraphic_L start_POSTSUBSCRIPT italic_P italic_I end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_P italic_E end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p2.8">where <math alttext="\mathcal{L}_{PI}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">â„’</mi><mrow id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">P</mi><mo id="S3.SS3.p2.1.m1.1.1.3.1" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">I</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">â„’</ci><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><times id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.1"></times><ci id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">ğ‘ƒ</ci><ci id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3">ğ¼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\mathcal{L}_{PI}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_P italic_I end_POSTSUBSCRIPT</annotation></semantics></math> is the hallucination loss between the visual features extracted from the depth image and the ones extracted from the RGB frame, and <math alttext="\mathcal{L}_{PE}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">â„’</mi><mrow id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">P</mi><mo id="S3.SS3.p2.2.m2.1.1.3.1" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">â„’</ci><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><times id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.1"></times><ci id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">ğ‘ƒ</ci><ci id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3">ğ¸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\mathcal{L}_{PE}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT italic_P italic_E end_POSTSUBSCRIPT</annotation></semantics></math> is the pose estimation loss between the predicted heatmap <math alttext="\widehat{H}_{j}" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><msub id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mover accent="true" id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2.2" xref="S3.SS3.p2.3.m3.1.1.2.2.cmml">H</mi><mo id="S3.SS3.p2.3.m3.1.1.2.1" xref="S3.SS3.p2.3.m3.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">subscript</csymbol><apply id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2"><ci id="S3.SS3.p2.3.m3.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.2.1">^</ci><ci id="S3.SS3.p2.3.m3.1.1.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2.2">ğ»</ci></apply><ci id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\widehat{H}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">over^ start_ARG italic_H end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> and the ground truth heatmap <math alttext="H_{j}" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><msub id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">H</mi><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">ğ»</ci><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">H_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">italic_H start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>. Moreover, <math alttext="m_{j}" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><msub id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mi id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">m</mi><mi id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">ğ‘š</ci><ci id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">m_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">italic_m start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is a visibility mask for each joint and <math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.1"><semantics id="S3.SS3.p2.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">ğ’¥</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><ci id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">ğ’¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.1d">caligraphic_J</annotation></semantics></math> is the total number of visible joints considering the <math alttext="uv" class="ltx_Math" display="inline" id="S3.SS3.p2.7.m7.1"><semantics id="S3.SS3.p2.7.m7.1a"><mrow id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml"><mi id="S3.SS3.p2.7.m7.1.1.2" xref="S3.SS3.p2.7.m7.1.1.2.cmml">u</mi><mo id="S3.SS3.p2.7.m7.1.1.1" xref="S3.SS3.p2.7.m7.1.1.1.cmml">â¢</mo><mi id="S3.SS3.p2.7.m7.1.1.3" xref="S3.SS3.p2.7.m7.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><apply id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1"><times id="S3.SS3.p2.7.m7.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1"></times><ci id="S3.SS3.p2.7.m7.1.1.2.cmml" xref="S3.SS3.p2.7.m7.1.1.2">ğ‘¢</ci><ci id="S3.SS3.p2.7.m7.1.1.3.cmml" xref="S3.SS3.p2.7.m7.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">uv</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.7.m7.1d">italic_u italic_v</annotation></semantics></math> and <math alttext="uz" class="ltx_Math" display="inline" id="S3.SS3.p2.8.m8.1"><semantics id="S3.SS3.p2.8.m8.1a"><mrow id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml"><mi id="S3.SS3.p2.8.m8.1.1.2" xref="S3.SS3.p2.8.m8.1.1.2.cmml">u</mi><mo id="S3.SS3.p2.8.m8.1.1.1" xref="S3.SS3.p2.8.m8.1.1.1.cmml">â¢</mo><mi id="S3.SS3.p2.8.m8.1.1.3" xref="S3.SS3.p2.8.m8.1.1.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.1b"><apply id="S3.SS3.p2.8.m8.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1"><times id="S3.SS3.p2.8.m8.1.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1.1"></times><ci id="S3.SS3.p2.8.m8.1.1.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2">ğ‘¢</ci><ci id="S3.SS3.p2.8.m8.1.1.3.cmml" xref="S3.SS3.p2.8.m8.1.1.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.1c">uz</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.8.m8.1d">italic_u italic_z</annotation></semantics></math> spaces.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Validation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we explain the experimental settings to evaluate the proposed method, describing the dataset used, the metrics, and the training procedure. Moreover, we present quantitative and qualitative results together with an analysis of the execution time of the system.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="193" id="S4.F5.g1" src="extracted/5849741/figures/dataset.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">Some visual examples of the Kinect Human Pose DatasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib48" title="">48</a>]</cite> in terms of RGB (first row) and depth images (second row). As shown, a variety of body poses, environments, and subject distances, characterize the dataset. Depth maps are depicted in 8-bit format only for visualization.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">It is important to note that to train and test the proposed system we need a dataset containing RGB and depth input data and 3D annotations of the body joints.
Unfortunately, only a few datasets are available in the literature, of which we adopt the Kinect Human Pose DatasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib48" title="">48</a>]</cite> due to its amount of frames, great variety of poses, and the presence of accurate manual 3D joint annotations.
All available datasets based on the Kinect SDK automatic annotations are not eligible for our task, because of the inaccuracy of the annotations.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.12"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.12.1">Kinect Human Pose DatasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib48" title="">48</a>]</cite>.</span> This dataset is composed of two sub-datasets for Human Pose Estimation from RGB-D sensors, namely Multi-View Kinect Dataset (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.12.2">MKV</span>) and the Captury Dataset (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.12.3">CAP</span>). Each sample provides the RGB image (<math alttext="1920\times 1080" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">1920</mn><mo id="S4.SS1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><times id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></times><cn id="S4.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.2">1920</cn><cn id="S4.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">1920\times 1080</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">1920 Ã— 1080</annotation></semantics></math> pixels), the depth map (<math alttext="512\times 424" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mn id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">512</mn><mo id="S4.SS1.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">424</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><times id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></times><cn id="S4.SS1.p2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1.2">512</cn><cn id="S4.SS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1.3">424</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">512\times 424</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">512 Ã— 424</annotation></semantics></math> pixels), 3D Human Pose annotation (<math alttext="14" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><mn id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><cn id="S4.SS1.p2.3.m3.1.1.cmml" type="integer" xref="S4.SS1.p2.3.m3.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">14</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">14</annotation></semantics></math> keypoints), 3D Kinect SDK Human Pose prediction (<math alttext="25" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.1"><semantics id="S4.SS1.p2.4.m4.1a"><mn id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><cn id="S4.SS1.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS1.p2.4.m4.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">25</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.1d">25</annotation></semantics></math> keypoints), and camera calibration parameters. The video sequences consist of <math alttext="5" class="ltx_Math" display="inline" id="S4.SS1.p2.5.m5.1"><semantics id="S4.SS1.p2.5.m5.1a"><mn id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><cn id="S4.SS1.p2.5.m5.1.1.cmml" type="integer" xref="S4.SS1.p2.5.m5.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.5.m5.1d">5</annotation></semantics></math> actors (<math alttext="2" class="ltx_Math" display="inline" id="S4.SS1.p2.6.m6.1"><semantics id="S4.SS1.p2.6.m6.1a"><mn id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><cn id="S4.SS1.p2.6.m6.1.1.cmml" type="integer" xref="S4.SS1.p2.6.m6.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.6.m6.1d">2</annotation></semantics></math> female and <math alttext="3" class="ltx_Math" display="inline" id="S4.SS1.p2.7.m7.1"><semantics id="S4.SS1.p2.7.m7.1a"><mn id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><cn id="S4.SS1.p2.7.m7.1.1.cmml" type="integer" xref="S4.SS1.p2.7.m7.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.7.m7.1d">3</annotation></semantics></math> male), <math alttext="3" class="ltx_Math" display="inline" id="S4.SS1.p2.8.m8.1"><semantics id="S4.SS1.p2.8.m8.1a"><mn id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><cn id="S4.SS1.p2.8.m8.1.1.cmml" type="integer" xref="S4.SS1.p2.8.m8.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.8.m8.1d">3</annotation></semantics></math> locations, and <math alttext="4" class="ltx_Math" display="inline" id="S4.SS1.p2.9.m9.1"><semantics id="S4.SS1.p2.9.m9.1a"><mn id="S4.SS1.p2.9.m9.1.1" xref="S4.SS1.p2.9.m9.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.9.m9.1b"><cn id="S4.SS1.p2.9.m9.1.1.cmml" type="integer" xref="S4.SS1.p2.9.m9.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.9.m9.1c">4</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.9.m9.1d">4</annotation></semantics></math> viewpoints with a frame rate of <math alttext="10" class="ltx_Math" display="inline" id="S4.SS1.p2.10.m10.1"><semantics id="S4.SS1.p2.10.m10.1a"><mn id="S4.SS1.p2.10.m10.1.1" xref="S4.SS1.p2.10.m10.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.10.m10.1b"><cn id="S4.SS1.p2.10.m10.1.1.cmml" type="integer" xref="S4.SS1.p2.10.m10.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.10.m10.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.10.m10.1d">10</annotation></semantics></math>Â Hz. The 3D annotations are obtained with a post-processing operation that leverages standard triangulation techniques to lift in the 3D space the 2D poses predicted by an off-the-shelf human pose detector.
A visual sample of the dataset is reported in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.F5" title="Figure 5 â€£ 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">5</span></a>.
In our experiments, we focused on the <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.12.4">MKV</span> dataset. Since our method is influenced by the scene appearance and the original training split contains an important domain shift between the training and the test set, for our experiments we created a more balanced split that we will release together with the code of the proposed method. The new split contains <math alttext="17360" class="ltx_Math" display="inline" id="S4.SS1.p2.11.m11.1"><semantics id="S4.SS1.p2.11.m11.1a"><mn id="S4.SS1.p2.11.m11.1.1" xref="S4.SS1.p2.11.m11.1.1.cmml">17360</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.11.m11.1b"><cn id="S4.SS1.p2.11.m11.1.1.cmml" type="integer" xref="S4.SS1.p2.11.m11.1.1">17360</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.11.m11.1c">17360</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.11.m11.1d">17360</annotation></semantics></math> training samples (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.12.5">MKV-t-v2</span>) and <math alttext="3886" class="ltx_Math" display="inline" id="S4.SS1.p2.12.m12.1"><semantics id="S4.SS1.p2.12.m12.1a"><mn id="S4.SS1.p2.12.m12.1.1" xref="S4.SS1.p2.12.m12.1.1.cmml">3886</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.12.m12.1b"><cn id="S4.SS1.p2.12.m12.1.1.cmml" type="integer" xref="S4.SS1.p2.12.m12.1.1">3886</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.12.m12.1c">3886</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.12.m12.1d">3886</annotation></semantics></math> test samples (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.12.6">MKV-e-v2</span>).</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To evaluate the accuracy of our 3D pose predictions, we use the average Mean Per Joint Position Error (MPJPE)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib48" title="">48</a>]</cite> and the mean Average Precision (mAP). The first metric is defined as the mean of the L<sub class="ltx_sub" id="S4.SS2.p1.1.1">2</sub> distances between each predicted joint and its corresponding ground truth positions (expressed in centimeters); it conveys the error in the 3D world in terms of translation and rotation, so the lower the better. The latter metric is defined as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{mAP}=\frac{1}{|N|}\,\sum_{j\in N}\,\big{(}\lVert\mathbf{y}_{j}-\widehat{%
\mathbf{y}}_{j}\rVert_{2}&lt;\tau\big{)}" class="ltx_Math" display="block" id="S4.E4.m1.2"><semantics id="S4.E4.m1.2a"><mrow id="S4.E4.m1.2.2" xref="S4.E4.m1.2.2.cmml"><mtext id="S4.E4.m1.2.2.3" xref="S4.E4.m1.2.2.3a.cmml">mAP</mtext><mo id="S4.E4.m1.2.2.2" xref="S4.E4.m1.2.2.2.cmml">=</mo><mrow id="S4.E4.m1.2.2.1" xref="S4.E4.m1.2.2.1.cmml"><mfrac id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml"><mn id="S4.E4.m1.1.1.3" xref="S4.E4.m1.1.1.3.cmml">1</mn><mrow id="S4.E4.m1.1.1.1.3" xref="S4.E4.m1.1.1.1.2.cmml"><mo id="S4.E4.m1.1.1.1.3.1" stretchy="false" xref="S4.E4.m1.1.1.1.2.1.cmml">|</mo><mi id="S4.E4.m1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.cmml">N</mi><mo id="S4.E4.m1.1.1.1.3.2" stretchy="false" xref="S4.E4.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.E4.m1.2.2.1.2" lspace="0.337em" xref="S4.E4.m1.2.2.1.2.cmml">â¢</mo><mrow id="S4.E4.m1.2.2.1.1" xref="S4.E4.m1.2.2.1.1.cmml"><munder id="S4.E4.m1.2.2.1.1.2" xref="S4.E4.m1.2.2.1.1.2.cmml"><mo id="S4.E4.m1.2.2.1.1.2.2" movablelimits="false" rspace="0em" xref="S4.E4.m1.2.2.1.1.2.2.cmml">âˆ‘</mo><mrow id="S4.E4.m1.2.2.1.1.2.3" xref="S4.E4.m1.2.2.1.1.2.3.cmml"><mi id="S4.E4.m1.2.2.1.1.2.3.2" xref="S4.E4.m1.2.2.1.1.2.3.2.cmml">j</mi><mo id="S4.E4.m1.2.2.1.1.2.3.1" xref="S4.E4.m1.2.2.1.1.2.3.1.cmml">âˆˆ</mo><mi id="S4.E4.m1.2.2.1.1.2.3.3" xref="S4.E4.m1.2.2.1.1.2.3.3.cmml">N</mi></mrow></munder><mrow id="S4.E4.m1.2.2.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.cmml"><mo id="S4.E4.m1.2.2.1.1.1.1.2" maxsize="120%" minsize="120%" xref="S4.E4.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.2.2.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.cmml"><msub id="S4.E4.m1.2.2.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mo fence="true" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.2.1.cmml">âˆ¥</mo><mrow id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">ğ²</mi><mi id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.2.cmml">ğ²</mi><mo id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo fence="true" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.3" lspace="0em" rspace="0.1389em" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S4.E4.m1.2.2.1.1.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.3.cmml">2</mn></msub><mo id="S4.E4.m1.2.2.1.1.1.1.1.2" lspace="0.1389em" xref="S4.E4.m1.2.2.1.1.1.1.1.2.cmml">&lt;</mo><mi id="S4.E4.m1.2.2.1.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.1.3.cmml">Ï„</mi></mrow><mo id="S4.E4.m1.2.2.1.1.1.1.3" maxsize="120%" minsize="120%" xref="S4.E4.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.2b"><apply id="S4.E4.m1.2.2.cmml" xref="S4.E4.m1.2.2"><eq id="S4.E4.m1.2.2.2.cmml" xref="S4.E4.m1.2.2.2"></eq><ci id="S4.E4.m1.2.2.3a.cmml" xref="S4.E4.m1.2.2.3"><mtext id="S4.E4.m1.2.2.3.cmml" xref="S4.E4.m1.2.2.3">mAP</mtext></ci><apply id="S4.E4.m1.2.2.1.cmml" xref="S4.E4.m1.2.2.1"><times id="S4.E4.m1.2.2.1.2.cmml" xref="S4.E4.m1.2.2.1.2"></times><apply id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1"><divide id="S4.E4.m1.1.1.2.cmml" xref="S4.E4.m1.1.1"></divide><cn id="S4.E4.m1.1.1.3.cmml" type="integer" xref="S4.E4.m1.1.1.3">1</cn><apply id="S4.E4.m1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.3"><abs id="S4.E4.m1.1.1.1.2.1.cmml" xref="S4.E4.m1.1.1.1.3.1"></abs><ci id="S4.E4.m1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1">ğ‘</ci></apply></apply><apply id="S4.E4.m1.2.2.1.1.cmml" xref="S4.E4.m1.2.2.1.1"><apply id="S4.E4.m1.2.2.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.2.1.cmml" xref="S4.E4.m1.2.2.1.1.2">subscript</csymbol><sum id="S4.E4.m1.2.2.1.1.2.2.cmml" xref="S4.E4.m1.2.2.1.1.2.2"></sum><apply id="S4.E4.m1.2.2.1.1.2.3.cmml" xref="S4.E4.m1.2.2.1.1.2.3"><in id="S4.E4.m1.2.2.1.1.2.3.1.cmml" xref="S4.E4.m1.2.2.1.1.2.3.1"></in><ci id="S4.E4.m1.2.2.1.1.2.3.2.cmml" xref="S4.E4.m1.2.2.1.1.2.3.2">ğ‘—</ci><ci id="S4.E4.m1.2.2.1.1.2.3.3.cmml" xref="S4.E4.m1.2.2.1.1.2.3.3">ğ‘</ci></apply></apply><apply id="S4.E4.m1.2.2.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1"><lt id="S4.E4.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2"></lt><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1"><minus id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">ğ²</ci><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.3">ğ‘—</ci></apply><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2"><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.2">ğ²</ci></apply><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply></apply><cn id="S4.E4.m1.2.2.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E4.m1.2.2.1.1.1.1.1.1.3">2</cn></apply><ci id="S4.E4.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3">ğœ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.2c">\text{mAP}=\frac{1}{|N|}\,\sum_{j\in N}\,\big{(}\lVert\mathbf{y}_{j}-\widehat{%
\mathbf{y}}_{j}\rVert_{2}&lt;\tau\big{)}</annotation><annotation encoding="application/x-llamapun" id="S4.E4.m1.2d">mAP = divide start_ARG 1 end_ARG start_ARG | italic_N | end_ARG âˆ‘ start_POSTSUBSCRIPT italic_j âˆˆ italic_N end_POSTSUBSCRIPT ( âˆ¥ bold_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - over^ start_ARG bold_y end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT &lt; italic_Ï„ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p1.5">where <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m1.1"><semantics id="S4.SS2.p1.2.m1.1a"><mi id="S4.SS2.p1.2.m1.1.1" xref="S4.SS2.p1.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m1.1b"><ci id="S4.SS2.p1.2.m1.1.1.cmml" xref="S4.SS2.p1.2.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m1.1d">italic_N</annotation></semantics></math> is the overall number of joints, <math alttext="\mathbf{y}_{j}" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m2.1"><semantics id="S4.SS2.p1.3.m2.1a"><msub id="S4.SS2.p1.3.m2.1.1" xref="S4.SS2.p1.3.m2.1.1.cmml"><mi id="S4.SS2.p1.3.m2.1.1.2" xref="S4.SS2.p1.3.m2.1.1.2.cmml">ğ²</mi><mi id="S4.SS2.p1.3.m2.1.1.3" xref="S4.SS2.p1.3.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m2.1b"><apply id="S4.SS2.p1.3.m2.1.1.cmml" xref="S4.SS2.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m2.1.1.1.cmml" xref="S4.SS2.p1.3.m2.1.1">subscript</csymbol><ci id="S4.SS2.p1.3.m2.1.1.2.cmml" xref="S4.SS2.p1.3.m2.1.1.2">ğ²</ci><ci id="S4.SS2.p1.3.m2.1.1.3.cmml" xref="S4.SS2.p1.3.m2.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m2.1c">\mathbf{y}_{j}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m2.1d">bold_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the predicted joint position while <math alttext="\widehat{\mathbf{y}}_{j}" class="ltx_Math" display="inline" id="S4.SS2.p1.4.m3.1"><semantics id="S4.SS2.p1.4.m3.1a"><msub id="S4.SS2.p1.4.m3.1.1" xref="S4.SS2.p1.4.m3.1.1.cmml"><mover accent="true" id="S4.SS2.p1.4.m3.1.1.2" xref="S4.SS2.p1.4.m3.1.1.2.cmml"><mi id="S4.SS2.p1.4.m3.1.1.2.2" xref="S4.SS2.p1.4.m3.1.1.2.2.cmml">ğ²</mi><mo id="S4.SS2.p1.4.m3.1.1.2.1" xref="S4.SS2.p1.4.m3.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS2.p1.4.m3.1.1.3" xref="S4.SS2.p1.4.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m3.1b"><apply id="S4.SS2.p1.4.m3.1.1.cmml" xref="S4.SS2.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.4.m3.1.1.1.cmml" xref="S4.SS2.p1.4.m3.1.1">subscript</csymbol><apply id="S4.SS2.p1.4.m3.1.1.2.cmml" xref="S4.SS2.p1.4.m3.1.1.2"><ci id="S4.SS2.p1.4.m3.1.1.2.1.cmml" xref="S4.SS2.p1.4.m3.1.1.2.1">^</ci><ci id="S4.SS2.p1.4.m3.1.1.2.2.cmml" xref="S4.SS2.p1.4.m3.1.1.2.2">ğ²</ci></apply><ci id="S4.SS2.p1.4.m3.1.1.3.cmml" xref="S4.SS2.p1.4.m3.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m3.1c">\widehat{\mathbf{y}}_{j}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.4.m3.1d">over^ start_ARG bold_y end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the ground truth. This metric represents the accuracy of the MPJPE using different thresholds (<math alttext="\tau=\{6,8,10\}" class="ltx_Math" display="inline" id="S4.SS2.p1.5.m4.3"><semantics id="S4.SS2.p1.5.m4.3a"><mrow id="S4.SS2.p1.5.m4.3.4" xref="S4.SS2.p1.5.m4.3.4.cmml"><mi id="S4.SS2.p1.5.m4.3.4.2" xref="S4.SS2.p1.5.m4.3.4.2.cmml">Ï„</mi><mo id="S4.SS2.p1.5.m4.3.4.1" xref="S4.SS2.p1.5.m4.3.4.1.cmml">=</mo><mrow id="S4.SS2.p1.5.m4.3.4.3.2" xref="S4.SS2.p1.5.m4.3.4.3.1.cmml"><mo id="S4.SS2.p1.5.m4.3.4.3.2.1" stretchy="false" xref="S4.SS2.p1.5.m4.3.4.3.1.cmml">{</mo><mn id="S4.SS2.p1.5.m4.1.1" xref="S4.SS2.p1.5.m4.1.1.cmml">6</mn><mo id="S4.SS2.p1.5.m4.3.4.3.2.2" xref="S4.SS2.p1.5.m4.3.4.3.1.cmml">,</mo><mn id="S4.SS2.p1.5.m4.2.2" xref="S4.SS2.p1.5.m4.2.2.cmml">8</mn><mo id="S4.SS2.p1.5.m4.3.4.3.2.3" xref="S4.SS2.p1.5.m4.3.4.3.1.cmml">,</mo><mn id="S4.SS2.p1.5.m4.3.3" xref="S4.SS2.p1.5.m4.3.3.cmml">10</mn><mo id="S4.SS2.p1.5.m4.3.4.3.2.4" stretchy="false" xref="S4.SS2.p1.5.m4.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m4.3b"><apply id="S4.SS2.p1.5.m4.3.4.cmml" xref="S4.SS2.p1.5.m4.3.4"><eq id="S4.SS2.p1.5.m4.3.4.1.cmml" xref="S4.SS2.p1.5.m4.3.4.1"></eq><ci id="S4.SS2.p1.5.m4.3.4.2.cmml" xref="S4.SS2.p1.5.m4.3.4.2">ğœ</ci><set id="S4.SS2.p1.5.m4.3.4.3.1.cmml" xref="S4.SS2.p1.5.m4.3.4.3.2"><cn id="S4.SS2.p1.5.m4.1.1.cmml" type="integer" xref="S4.SS2.p1.5.m4.1.1">6</cn><cn id="S4.SS2.p1.5.m4.2.2.cmml" type="integer" xref="S4.SS2.p1.5.m4.2.2">8</cn><cn id="S4.SS2.p1.5.m4.3.3.cmml" type="integer" xref="S4.SS2.p1.5.m4.3.3">10</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m4.3c">\tau=\{6,8,10\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.5.m4.3d">italic_Ï„ = { 6 , 8 , 10 }</annotation></semantics></math> centimeters in our experiments) and it improves the understanding of the actual performance of the methods.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Training</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.4">As mentioned in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3.SS1" title="3.1 Depth-based Privileged Information â€£ 3 Proposed Method â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">3.1</span></a>, the training procedure is performed in two steps; the first involves a pretraining of the architecture using only depth data as input, while the latter leverages the privileged information for training on RGB images.
In order to prevent model overfitting, we apply the same data augmentation techniques on both RGB and depth data. In particular, we rotate the images with a range of <math alttext="[-5,+5]" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.2"><semantics id="S4.SS3.p1.1.m1.2a"><mrow id="S4.SS3.p1.1.m1.2.2.2" xref="S4.SS3.p1.1.m1.2.2.3.cmml"><mo id="S4.SS3.p1.1.m1.2.2.2.3" stretchy="false" xref="S4.SS3.p1.1.m1.2.2.3.cmml">[</mo><mrow id="S4.SS3.p1.1.m1.1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.1.cmml"><mo id="S4.SS3.p1.1.m1.1.1.1.1a" xref="S4.SS3.p1.1.m1.1.1.1.1.cmml">âˆ’</mo><mn id="S4.SS3.p1.1.m1.1.1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.1.1.2.cmml">5</mn></mrow><mo id="S4.SS3.p1.1.m1.2.2.2.4" xref="S4.SS3.p1.1.m1.2.2.3.cmml">,</mo><mrow id="S4.SS3.p1.1.m1.2.2.2.2" xref="S4.SS3.p1.1.m1.2.2.2.2.cmml"><mo id="S4.SS3.p1.1.m1.2.2.2.2a" xref="S4.SS3.p1.1.m1.2.2.2.2.cmml">+</mo><mn id="S4.SS3.p1.1.m1.2.2.2.2.2" xref="S4.SS3.p1.1.m1.2.2.2.2.2.cmml">5</mn></mrow><mo id="S4.SS3.p1.1.m1.2.2.2.5" stretchy="false" xref="S4.SS3.p1.1.m1.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.2b"><interval closure="closed" id="S4.SS3.p1.1.m1.2.2.3.cmml" xref="S4.SS3.p1.1.m1.2.2.2"><apply id="S4.SS3.p1.1.m1.1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1.1"><minus id="S4.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1.1"></minus><cn id="S4.SS3.p1.1.m1.1.1.1.1.2.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.1.1.2">5</cn></apply><apply id="S4.SS3.p1.1.m1.2.2.2.2.cmml" xref="S4.SS3.p1.1.m1.2.2.2.2"><plus id="S4.SS3.p1.1.m1.2.2.2.2.1.cmml" xref="S4.SS3.p1.1.m1.2.2.2.2"></plus><cn id="S4.SS3.p1.1.m1.2.2.2.2.2.cmml" type="integer" xref="S4.SS3.p1.1.m1.2.2.2.2.2">5</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.2c">[-5,+5]</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.2d">[ - 5 , + 5 ]</annotation></semantics></math> degrees, pixel are randomly translated with a maximum range of <math alttext="15\%" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">15</mn><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1">percent</csymbol><cn id="S4.SS3.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">15\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">15 %</annotation></semantics></math> of the width and <math alttext="2\%" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.1"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">2</mn><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1">percent</csymbol><cn id="S4.SS3.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS3.p1.3.m3.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">2\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.1d">2 %</annotation></semantics></math> of height and horizontal flip. Only during the first step of the whole training procedure, we randomly translate the Z axis of the depth image in the range <math alttext="[-30cm,+30cm]" class="ltx_Math" display="inline" id="S4.SS3.p1.4.m4.2"><semantics id="S4.SS3.p1.4.m4.2a"><mrow id="S4.SS3.p1.4.m4.2.2.2" xref="S4.SS3.p1.4.m4.2.2.3.cmml"><mo id="S4.SS3.p1.4.m4.2.2.2.3" stretchy="false" xref="S4.SS3.p1.4.m4.2.2.3.cmml">[</mo><mrow id="S4.SS3.p1.4.m4.1.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.1.cmml"><mo id="S4.SS3.p1.4.m4.1.1.1.1a" xref="S4.SS3.p1.4.m4.1.1.1.1.cmml">âˆ’</mo><mrow id="S4.SS3.p1.4.m4.1.1.1.1.2" xref="S4.SS3.p1.4.m4.1.1.1.1.2.cmml"><mn id="S4.SS3.p1.4.m4.1.1.1.1.2.2" xref="S4.SS3.p1.4.m4.1.1.1.1.2.2.cmml">30</mn><mo id="S4.SS3.p1.4.m4.1.1.1.1.2.1" xref="S4.SS3.p1.4.m4.1.1.1.1.2.1.cmml">â¢</mo><mi id="S4.SS3.p1.4.m4.1.1.1.1.2.3" xref="S4.SS3.p1.4.m4.1.1.1.1.2.3.cmml">c</mi><mo id="S4.SS3.p1.4.m4.1.1.1.1.2.1a" xref="S4.SS3.p1.4.m4.1.1.1.1.2.1.cmml">â¢</mo><mi id="S4.SS3.p1.4.m4.1.1.1.1.2.4" xref="S4.SS3.p1.4.m4.1.1.1.1.2.4.cmml">m</mi></mrow></mrow><mo id="S4.SS3.p1.4.m4.2.2.2.4" xref="S4.SS3.p1.4.m4.2.2.3.cmml">,</mo><mrow id="S4.SS3.p1.4.m4.2.2.2.2" xref="S4.SS3.p1.4.m4.2.2.2.2.cmml"><mo id="S4.SS3.p1.4.m4.2.2.2.2a" xref="S4.SS3.p1.4.m4.2.2.2.2.cmml">+</mo><mrow id="S4.SS3.p1.4.m4.2.2.2.2.2" xref="S4.SS3.p1.4.m4.2.2.2.2.2.cmml"><mn id="S4.SS3.p1.4.m4.2.2.2.2.2.2" xref="S4.SS3.p1.4.m4.2.2.2.2.2.2.cmml">30</mn><mo id="S4.SS3.p1.4.m4.2.2.2.2.2.1" xref="S4.SS3.p1.4.m4.2.2.2.2.2.1.cmml">â¢</mo><mi id="S4.SS3.p1.4.m4.2.2.2.2.2.3" xref="S4.SS3.p1.4.m4.2.2.2.2.2.3.cmml">c</mi><mo id="S4.SS3.p1.4.m4.2.2.2.2.2.1a" xref="S4.SS3.p1.4.m4.2.2.2.2.2.1.cmml">â¢</mo><mi id="S4.SS3.p1.4.m4.2.2.2.2.2.4" xref="S4.SS3.p1.4.m4.2.2.2.2.2.4.cmml">m</mi></mrow></mrow><mo id="S4.SS3.p1.4.m4.2.2.2.5" stretchy="false" xref="S4.SS3.p1.4.m4.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.2b"><interval closure="closed" id="S4.SS3.p1.4.m4.2.2.3.cmml" xref="S4.SS3.p1.4.m4.2.2.2"><apply id="S4.SS3.p1.4.m4.1.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1"><minus id="S4.SS3.p1.4.m4.1.1.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1"></minus><apply id="S4.SS3.p1.4.m4.1.1.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1.2"><times id="S4.SS3.p1.4.m4.1.1.1.1.2.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1.2.1"></times><cn id="S4.SS3.p1.4.m4.1.1.1.1.2.2.cmml" type="integer" xref="S4.SS3.p1.4.m4.1.1.1.1.2.2">30</cn><ci id="S4.SS3.p1.4.m4.1.1.1.1.2.3.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1.2.3">ğ‘</ci><ci id="S4.SS3.p1.4.m4.1.1.1.1.2.4.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1.2.4">ğ‘š</ci></apply></apply><apply id="S4.SS3.p1.4.m4.2.2.2.2.cmml" xref="S4.SS3.p1.4.m4.2.2.2.2"><plus id="S4.SS3.p1.4.m4.2.2.2.2.1.cmml" xref="S4.SS3.p1.4.m4.2.2.2.2"></plus><apply id="S4.SS3.p1.4.m4.2.2.2.2.2.cmml" xref="S4.SS3.p1.4.m4.2.2.2.2.2"><times id="S4.SS3.p1.4.m4.2.2.2.2.2.1.cmml" xref="S4.SS3.p1.4.m4.2.2.2.2.2.1"></times><cn id="S4.SS3.p1.4.m4.2.2.2.2.2.2.cmml" type="integer" xref="S4.SS3.p1.4.m4.2.2.2.2.2.2">30</cn><ci id="S4.SS3.p1.4.m4.2.2.2.2.2.3.cmml" xref="S4.SS3.p1.4.m4.2.2.2.2.2.3">ğ‘</ci><ci id="S4.SS3.p1.4.m4.2.2.2.2.2.4.cmml" xref="S4.SS3.p1.4.m4.2.2.2.2.2.4">ğ‘š</ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.2c">[-30cm,+30cm]</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.4.m4.2d">[ - 30 italic_c italic_m , + 30 italic_c italic_m ]</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.5">We use AdamÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib13" title="">13</a>]</cite> as an optimizer with a learning rate of <math alttext="1e^{-3}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">1</mn><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">â¢</mo><msup id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml"><mi id="S4.SS3.p2.1.m1.1.1.3.2" xref="S4.SS3.p2.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.SS3.p2.1.m1.1.1.3.3" xref="S4.SS3.p2.1.m1.1.1.3.3.cmml"><mo id="S4.SS3.p2.1.m1.1.1.3.3a" xref="S4.SS3.p2.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S4.SS3.p2.1.m1.1.1.3.3.2" xref="S4.SS3.p2.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn id="S4.SS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.2">1</cn><apply id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.3.1.cmml" xref="S4.SS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.3.2.cmml" xref="S4.SS3.p2.1.m1.1.1.3.2">ğ‘’</ci><apply id="S4.SS3.p2.1.m1.1.1.3.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3.3"><minus id="S4.SS3.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS3.p2.1.m1.1.1.3.3"></minus><cn id="S4.SS3.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">1e^{-3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">1 italic_e start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT</annotation></semantics></math> and a decay step with a factor <math alttext="10" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mn id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><cn id="S4.SS3.p2.2.m2.1.1.cmml" type="integer" xref="S4.SS3.p2.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">10</annotation></semantics></math> at the <math alttext="50\%" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mn id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">50</mn><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1">percent</csymbol><cn id="S4.SS3.p2.3.m3.1.1.2.cmml" type="integer" xref="S4.SS3.p2.3.m3.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">50\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">50 %</annotation></semantics></math> and <math alttext="75\%" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mrow id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml"><mn id="S4.SS3.p2.4.m4.1.1.2" xref="S4.SS3.p2.4.m4.1.1.2.cmml">75</mn><mo id="S4.SS3.p2.4.m4.1.1.1" xref="S4.SS3.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><apply id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS3.p2.4.m4.1.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1.1">percent</csymbol><cn id="S4.SS3.p2.4.m4.1.1.2.cmml" type="integer" xref="S4.SS3.p2.4.m4.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">75\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">75 %</annotation></semantics></math> of the training. Each network is trained for <math alttext="30" class="ltx_Math" display="inline" id="S4.SS3.p2.5.m5.1"><semantics id="S4.SS3.p2.5.m5.1a"><mn id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><cn id="S4.SS3.p2.5.m5.1.1.cmml" type="integer" xref="S4.SS3.p2.5.m5.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">30</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.5.m5.1d">30</annotation></semantics></math> epochs.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.17">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T1.1.1.2"></td>
<td class="ltx_td ltx_border_tt" id="S4.T1.1.1.3"></td>
<td class="ltx_td ltx_border_tt" id="S4.T1.1.1.4"></td>
<td class="ltx_td ltx_border_tt" id="S4.T1.1.1.5"></td>
<td class="ltx_td ltx_border_tt" id="S4.T1.1.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1">mAP</span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_border_tt" id="S4.T1.1.1.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.5">
<td class="ltx_td ltx_align_left" id="S4.T1.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T1.5.5.5.1">Model</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.5.5.6"><span class="ltx_text ltx_font_bold" id="S4.T1.5.5.6.1">Input</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.1"><math alttext="\mathbf{F_{depth}}" class="ltx_Math" display="inline" id="S4.T1.2.2.1.m1.1"><semantics id="S4.T1.2.2.1.m1.1a"><msub id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml"><mi id="S4.T1.2.2.1.m1.1.1.2" xref="S4.T1.2.2.1.m1.1.1.2.cmml">ğ…</mi><mi id="S4.T1.2.2.1.m1.1.1.3" xref="S4.T1.2.2.1.m1.1.1.3.cmml">ğğğ©ğ­ğ¡</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><apply id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T1.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.1.m1.1.1.2">ğ…</ci><ci id="S4.T1.2.2.1.m1.1.1.3.cmml" xref="S4.T1.2.2.1.m1.1.1.3">ğğğ©ğ­ğ¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">\mathbf{F_{depth}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.1.m1.1d">bold_F start_POSTSUBSCRIPT bold_depth end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.2"><math alttext="\mathbf{F_{RGB}}" class="ltx_Math" display="inline" id="S4.T1.3.3.2.m1.1"><semantics id="S4.T1.3.3.2.m1.1a"><msub id="S4.T1.3.3.2.m1.1.1" xref="S4.T1.3.3.2.m1.1.1.cmml"><mi id="S4.T1.3.3.2.m1.1.1.2" xref="S4.T1.3.3.2.m1.1.1.2.cmml">ğ…</mi><mi id="S4.T1.3.3.2.m1.1.1.3" xref="S4.T1.3.3.2.m1.1.1.3.cmml">ğ‘ğ†ğ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.2.m1.1b"><apply id="S4.T1.3.3.2.m1.1.1.cmml" xref="S4.T1.3.3.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.2.m1.1.1.1.cmml" xref="S4.T1.3.3.2.m1.1.1">subscript</csymbol><ci id="S4.T1.3.3.2.m1.1.1.2.cmml" xref="S4.T1.3.3.2.m1.1.1.2">ğ…</ci><ci id="S4.T1.3.3.2.m1.1.1.3.cmml" xref="S4.T1.3.3.2.m1.1.1.3">ğ‘ğ†ğ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.2.m1.1c">\mathbf{F_{RGB}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.2.m1.1d">bold_F start_POSTSUBSCRIPT bold_RGB end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.3"><math alttext="\mathbf{F_{hall}}" class="ltx_Math" display="inline" id="S4.T1.4.4.3.m1.1"><semantics id="S4.T1.4.4.3.m1.1a"><msub id="S4.T1.4.4.3.m1.1.1" xref="S4.T1.4.4.3.m1.1.1.cmml"><mi id="S4.T1.4.4.3.m1.1.1.2" xref="S4.T1.4.4.3.m1.1.1.2.cmml">ğ…</mi><mi id="S4.T1.4.4.3.m1.1.1.3" xref="S4.T1.4.4.3.m1.1.1.3.cmml">ğ¡ğšğ¥ğ¥</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.3.m1.1b"><apply id="S4.T1.4.4.3.m1.1.1.cmml" xref="S4.T1.4.4.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.4.4.3.m1.1.1.1.cmml" xref="S4.T1.4.4.3.m1.1.1">subscript</csymbol><ci id="S4.T1.4.4.3.m1.1.1.2.cmml" xref="S4.T1.4.4.3.m1.1.1.2">ğ…</ci><ci id="S4.T1.4.4.3.m1.1.1.3.cmml" xref="S4.T1.4.4.3.m1.1.1.3">ğ¡ğšğ¥ğ¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.3.m1.1c">\mathbf{F_{hall}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.3.m1.1d">bold_F start_POSTSUBSCRIPT bold_hall end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.5.7"><span class="ltx_text ltx_font_bold" id="S4.T1.5.5.7.1">6cm</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.5.8"><span class="ltx_text ltx_font_bold" id="S4.T1.5.5.8.1">8cm</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.5.9"><span class="ltx_text ltx_font_bold" id="S4.T1.5.5.9.1">10cm</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.5.4">
<span class="ltx_text ltx_font_bold" id="S4.T1.5.5.4.1">MPJPE</span> (cm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.5.5.4.m1.1"><semantics id="S4.T1.5.5.4.m1.1a"><mo id="S4.T1.5.5.4.m1.1.1" stretchy="false" xref="S4.T1.5.5.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.4.m1.1b"><ci id="S4.T1.5.5.4.m1.1.1.cmml" xref="S4.T1.5.5.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.4.m1.1d">â†“</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.17.18.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.17.18.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.17.18.1.1.1">SmallHRNet</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.17.18.1.2"><span class="ltx_text ltx_font_italic" id="S4.T1.17.18.1.2.1">Depth</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.18.1.3">âœ“</td>
<td class="ltx_td ltx_border_t" id="S4.T1.17.18.1.4"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.17.18.1.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.18.1.6"><span class="ltx_text ltx_font_italic" id="S4.T1.17.18.1.6.1">50.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.18.1.7"><span class="ltx_text ltx_font_italic" id="S4.T1.17.18.1.7.1">61.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.18.1.8"><span class="ltx_text ltx_font_italic" id="S4.T1.17.18.1.8.1">68.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.18.1.9"><span class="ltx_text ltx_font_italic" id="S4.T1.17.18.1.9.1">11.09</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.9.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.9.9.5">SmallHRNet</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.9.9.6">RGB</td>
<td class="ltx_td ltx_border_t" id="S4.T1.9.9.7"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.9.8">âœ“</td>
<td class="ltx_td ltx_border_t" id="S4.T1.9.9.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.6.1"><math alttext="43.42" class="ltx_Math" display="inline" id="S4.T1.6.6.1.m1.1"><semantics id="S4.T1.6.6.1.m1.1a"><mn id="S4.T1.6.6.1.m1.1.1" xref="S4.T1.6.6.1.m1.1.1.cmml">43.42</mn><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.1.m1.1b"><cn id="S4.T1.6.6.1.m1.1.1.cmml" type="float" xref="S4.T1.6.6.1.m1.1.1">43.42</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.1.m1.1c">43.42</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.1.m1.1d">43.42</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.2"><math alttext="52.17" class="ltx_Math" display="inline" id="S4.T1.7.7.2.m1.1"><semantics id="S4.T1.7.7.2.m1.1a"><mn id="S4.T1.7.7.2.m1.1.1" xref="S4.T1.7.7.2.m1.1.1.cmml">52.17</mn><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.2.m1.1b"><cn id="S4.T1.7.7.2.m1.1.1.cmml" type="float" xref="S4.T1.7.7.2.m1.1.1">52.17</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.2.m1.1c">52.17</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.7.2.m1.1d">52.17</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.8.8.3"><math alttext="58.61" class="ltx_Math" display="inline" id="S4.T1.8.8.3.m1.1"><semantics id="S4.T1.8.8.3.m1.1a"><mn id="S4.T1.8.8.3.m1.1.1" xref="S4.T1.8.8.3.m1.1.1.cmml">58.61</mn><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.3.m1.1b"><cn id="S4.T1.8.8.3.m1.1.1.cmml" type="float" xref="S4.T1.8.8.3.m1.1.1">58.61</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.3.m1.1c">58.61</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.8.3.m1.1d">58.61</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.9.4"><math alttext="14.96" class="ltx_Math" display="inline" id="S4.T1.9.9.4.m1.1"><semantics id="S4.T1.9.9.4.m1.1a"><mn id="S4.T1.9.9.4.m1.1.1" xref="S4.T1.9.9.4.m1.1.1.cmml">14.96</mn><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.4.m1.1b"><cn id="S4.T1.9.9.4.m1.1.1.cmml" type="float" xref="S4.T1.9.9.4.m1.1.1">14.96</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.4.m1.1c">14.96</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.9.4.m1.1d">14.96</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T1.13.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.13.13.5">SmallHRNet</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.13.13.6">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.13.13.7"><span class="ltx_text ltx_font_italic" id="S4.T1.13.13.7.1">frozen</span></td>
<td class="ltx_td ltx_border_t" id="S4.T1.13.13.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.13.13.9">âœ“</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.1"><math alttext="44.43" class="ltx_Math" display="inline" id="S4.T1.10.10.1.m1.1"><semantics id="S4.T1.10.10.1.m1.1a"><mn id="S4.T1.10.10.1.m1.1.1" xref="S4.T1.10.10.1.m1.1.1.cmml">44.43</mn><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.1.m1.1b"><cn id="S4.T1.10.10.1.m1.1.1.cmml" type="float" xref="S4.T1.10.10.1.m1.1.1">44.43</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.1.m1.1c">44.43</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.10.1.m1.1d">44.43</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.11.11.2"><math alttext="54.78" class="ltx_Math" display="inline" id="S4.T1.11.11.2.m1.1"><semantics id="S4.T1.11.11.2.m1.1a"><mn id="S4.T1.11.11.2.m1.1.1" xref="S4.T1.11.11.2.m1.1.1.cmml">54.78</mn><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.2.m1.1b"><cn id="S4.T1.11.11.2.m1.1.1.cmml" type="float" xref="S4.T1.11.11.2.m1.1.1">54.78</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.2.m1.1c">54.78</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.11.2.m1.1d">54.78</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.12.12.3"><math alttext="60.56" class="ltx_Math" display="inline" id="S4.T1.12.12.3.m1.1"><semantics id="S4.T1.12.12.3.m1.1a"><mn id="S4.T1.12.12.3.m1.1.1" xref="S4.T1.12.12.3.m1.1.1.cmml">60.56</mn><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.3.m1.1b"><cn id="S4.T1.12.12.3.m1.1.1.cmml" type="float" xref="S4.T1.12.12.3.m1.1.1">60.56</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.3.m1.1c">60.56</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.12.3.m1.1d">60.56</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.13.13.4"><math alttext="13.90" class="ltx_Math" display="inline" id="S4.T1.13.13.4.m1.1"><semantics id="S4.T1.13.13.4.m1.1a"><mn id="S4.T1.13.13.4.m1.1.1" xref="S4.T1.13.13.4.m1.1.1.cmml">13.90</mn><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.4.m1.1b"><cn id="S4.T1.13.13.4.m1.1.1.cmml" type="float" xref="S4.T1.13.13.4.m1.1.1">13.90</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.4.m1.1c">13.90</annotation><annotation encoding="application/x-llamapun" id="S4.T1.13.13.4.m1.1d">13.90</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T1.17.17">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.17.17.5">SmallHRNet</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.17.17.6">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.17.17.7"><span class="ltx_text ltx_font_italic" id="S4.T1.17.17.7.1">frozen</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.17.17.8">âœ“</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.17.17.9">âœ“</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.14.14.1"><math alttext="\mathbf{47.77}" class="ltx_Math" display="inline" id="S4.T1.14.14.1.m1.1"><semantics id="S4.T1.14.14.1.m1.1a"><mn class="ltx_mathvariant_bold" id="S4.T1.14.14.1.m1.1.1" mathvariant="bold" xref="S4.T1.14.14.1.m1.1.1.cmml">47.77</mn><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.1.m1.1b"><cn id="S4.T1.14.14.1.m1.1.1.cmml" type="float" xref="S4.T1.14.14.1.m1.1.1">47.77</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.1.m1.1c">\mathbf{47.77}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.14.14.1.m1.1d">bold_47.77</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.15.15.2"><math alttext="\mathbf{58.13}" class="ltx_Math" display="inline" id="S4.T1.15.15.2.m1.1"><semantics id="S4.T1.15.15.2.m1.1a"><mn class="ltx_mathvariant_bold" id="S4.T1.15.15.2.m1.1.1" mathvariant="bold" xref="S4.T1.15.15.2.m1.1.1.cmml">58.13</mn><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.2.m1.1b"><cn id="S4.T1.15.15.2.m1.1.1.cmml" type="float" xref="S4.T1.15.15.2.m1.1.1">58.13</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.2.m1.1c">\mathbf{58.13}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.15.15.2.m1.1d">bold_58.13</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.16.16.3"><math alttext="\mathbf{65.19}" class="ltx_Math" display="inline" id="S4.T1.16.16.3.m1.1"><semantics id="S4.T1.16.16.3.m1.1a"><mn class="ltx_mathvariant_bold" id="S4.T1.16.16.3.m1.1.1" mathvariant="bold" xref="S4.T1.16.16.3.m1.1.1.cmml">65.19</mn><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.3.m1.1b"><cn id="S4.T1.16.16.3.m1.1.1.cmml" type="float" xref="S4.T1.16.16.3.m1.1.1">65.19</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.3.m1.1c">\mathbf{65.19}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.16.16.3.m1.1d">bold_65.19</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.17.17.4"><math alttext="\mathbf{12.35}" class="ltx_Math" display="inline" id="S4.T1.17.17.4.m1.1"><semantics id="S4.T1.17.17.4.m1.1a"><mn class="ltx_mathvariant_bold" id="S4.T1.17.17.4.m1.1.1" mathvariant="bold" xref="S4.T1.17.17.4.m1.1.1.cmml">12.35</mn><annotation-xml encoding="MathML-Content" id="S4.T1.17.17.4.m1.1b"><cn id="S4.T1.17.17.4.m1.1.1.cmml" type="float" xref="S4.T1.17.17.4.m1.1.1">12.35</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.17.4.m1.1c">\mathbf{12.35}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.17.17.4.m1.1d">bold_12.35</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.20.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.21.2" style="font-size:90%;">Comparison on <span class="ltx_text ltx_font_italic" id="S4.T1.21.2.1">MKV-e-v2</span> between SmallHRNet baselines trained on depth and RGB images and our depth-based privileged information approach trained with and without the RGB-based backbone.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Quantitative results</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.3">Results of the proposed method are reported in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.T1" title="Table 1 â€£ 4.3 Training â€£ 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">1</span></a>.
In the first line, we test the SmallHRNet model trained only with depth data, following the step discussed in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3.SS1.SSS1" title="3.1.1 Depth-based pretraining. â€£ 3.1 Depth-based Privileged Information â€£ 3 Proposed Method â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">3.1.1</span></a> and visually summarized in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S3.F3" title="Figure 3 â€£ 3.1.1 Depth-based pretraining. â€£ 3.1 Depth-based Privileged Information â€£ 3 Proposed Method â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">3</span></a>. It is important to note that this test represents the upper bound for the performance as we solve the 3D pose estimation task with the same model but using depth images as input data both for training and inference.
In the second line, we report the performance of the method using only the model trained on RGB images. These results are then used as a reference to quantify the boost of performances achieved by our method thanks to the introduction of Privileged Information in an RGB-based system.
We tested our Privileged Information approach in two configurations: the first one uses only the hallucination network, while the latter leverages both the hallucinated features and the embedding representing the RGB domain. As can be seen in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.T1" title="Table 1 â€£ 4.3 Training â€£ 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">1</span></a>, the Privileged Information paradigm improves the performance even with just the hallucination network with respect to the RGB-only baseline. When adding the domain-specific features of the RGB images, the system improves even more obtaining a <math alttext="-2.66" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mo id="S4.SS4.p1.1.m1.1.1a" xref="S4.SS4.p1.1.m1.1.1.cmml">âˆ’</mo><mn id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">2.66</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><minus id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"></minus><cn id="S4.SS4.p1.1.m1.1.1.2.cmml" type="float" xref="S4.SS4.p1.1.m1.1.1.2">2.66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">-2.66</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">- 2.66</annotation></semantics></math> in terms of MPJPE and <math alttext="+4.5\%" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><mrow id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mo id="S4.SS4.p1.2.m2.1.1a" xref="S4.SS4.p1.2.m2.1.1.cmml">+</mo><mrow id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml"><mn id="S4.SS4.p1.2.m2.1.1.2.2" xref="S4.SS4.p1.2.m2.1.1.2.2.cmml">4.5</mn><mo id="S4.SS4.p1.2.m2.1.1.2.1" xref="S4.SS4.p1.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><plus id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"></plus><apply id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS4.p1.2.m2.1.1.2.1.cmml" xref="S4.SS4.p1.2.m2.1.1.2.1">percent</csymbol><cn id="S4.SS4.p1.2.m2.1.1.2.2.cmml" type="float" xref="S4.SS4.p1.2.m2.1.1.2.2">4.5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">+4.5\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">+ 4.5 %</annotation></semantics></math> in terms of mAP with a threshold <math alttext="\tau=10" class="ltx_Math" display="inline" id="S4.SS4.p1.3.m3.1"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">Ï„</mi><mo id="S4.SS4.p1.3.m3.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><eq id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1"></eq><ci id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2">ğœ</ci><cn id="S4.SS4.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS4.p1.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">\tau=10</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.3.m3.1d">italic_Ï„ = 10</annotation></semantics></math>cm.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="258" id="S4.F6.g1" src="extracted/5849741/figures/final.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">Sample outputs of the proposed method. The ground truth and the predicted uv-heatmaps are superimposed on the input RGB frame in the first (ground truth) and second (prediction) lines, respectively. In the bottom right corners are depicted the uz-heatmaps. In the third line, it is represented the predicted 3D human pose. As shown, our method is able to correctly handle a variety of poses and backgrounds, producing plausible final poses. </span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Qualitative Analysis</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">To provide better insights of the capabilities of the proposed approach, we provide some qualitative results.
A few samples of the predictions of our approach are depicted in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.F6" title="Figure 6 â€£ 4.4 Quantitative results â€£ 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">6</span></a>, showing a comparison between the ground truth and the predicted poses in terms of SPDH representation and 3D skeleton.
It can be seen how the model is able to accurately estimate the position of the joints in the image plane, as well as the pose of the humans in the 3D space.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.3">We then perform an additional experiment, projecting features extracted from our model with T-SNEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib18" title="">18</a>]</cite>. We sample 500 frames from the dataset and extract the corresponding features using the three branches in our model, <math alttext="F_{depth}" class="ltx_Math" display="inline" id="S4.SS5.p2.1.m1.1"><semantics id="S4.SS5.p2.1.m1.1a"><msub id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml"><mi id="S4.SS5.p2.1.m1.1.1.2" xref="S4.SS5.p2.1.m1.1.1.2.cmml">F</mi><mrow id="S4.SS5.p2.1.m1.1.1.3" xref="S4.SS5.p2.1.m1.1.1.3.cmml"><mi id="S4.SS5.p2.1.m1.1.1.3.2" xref="S4.SS5.p2.1.m1.1.1.3.2.cmml">d</mi><mo id="S4.SS5.p2.1.m1.1.1.3.1" xref="S4.SS5.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS5.p2.1.m1.1.1.3.3" xref="S4.SS5.p2.1.m1.1.1.3.3.cmml">e</mi><mo id="S4.SS5.p2.1.m1.1.1.3.1a" xref="S4.SS5.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS5.p2.1.m1.1.1.3.4" xref="S4.SS5.p2.1.m1.1.1.3.4.cmml">p</mi><mo id="S4.SS5.p2.1.m1.1.1.3.1b" xref="S4.SS5.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS5.p2.1.m1.1.1.3.5" xref="S4.SS5.p2.1.m1.1.1.3.5.cmml">t</mi><mo id="S4.SS5.p2.1.m1.1.1.3.1c" xref="S4.SS5.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS5.p2.1.m1.1.1.3.6" xref="S4.SS5.p2.1.m1.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><apply id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p2.1.m1.1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS5.p2.1.m1.1.1.2.cmml" xref="S4.SS5.p2.1.m1.1.1.2">ğ¹</ci><apply id="S4.SS5.p2.1.m1.1.1.3.cmml" xref="S4.SS5.p2.1.m1.1.1.3"><times id="S4.SS5.p2.1.m1.1.1.3.1.cmml" xref="S4.SS5.p2.1.m1.1.1.3.1"></times><ci id="S4.SS5.p2.1.m1.1.1.3.2.cmml" xref="S4.SS5.p2.1.m1.1.1.3.2">ğ‘‘</ci><ci id="S4.SS5.p2.1.m1.1.1.3.3.cmml" xref="S4.SS5.p2.1.m1.1.1.3.3">ğ‘’</ci><ci id="S4.SS5.p2.1.m1.1.1.3.4.cmml" xref="S4.SS5.p2.1.m1.1.1.3.4">ğ‘</ci><ci id="S4.SS5.p2.1.m1.1.1.3.5.cmml" xref="S4.SS5.p2.1.m1.1.1.3.5">ğ‘¡</ci><ci id="S4.SS5.p2.1.m1.1.1.3.6.cmml" xref="S4.SS5.p2.1.m1.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">F_{depth}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.1.m1.1d">italic_F start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="F_{RGB}" class="ltx_Math" display="inline" id="S4.SS5.p2.2.m2.1"><semantics id="S4.SS5.p2.2.m2.1a"><msub id="S4.SS5.p2.2.m2.1.1" xref="S4.SS5.p2.2.m2.1.1.cmml"><mi id="S4.SS5.p2.2.m2.1.1.2" xref="S4.SS5.p2.2.m2.1.1.2.cmml">F</mi><mrow id="S4.SS5.p2.2.m2.1.1.3" xref="S4.SS5.p2.2.m2.1.1.3.cmml"><mi id="S4.SS5.p2.2.m2.1.1.3.2" xref="S4.SS5.p2.2.m2.1.1.3.2.cmml">R</mi><mo id="S4.SS5.p2.2.m2.1.1.3.1" xref="S4.SS5.p2.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S4.SS5.p2.2.m2.1.1.3.3" xref="S4.SS5.p2.2.m2.1.1.3.3.cmml">G</mi><mo id="S4.SS5.p2.2.m2.1.1.3.1a" xref="S4.SS5.p2.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S4.SS5.p2.2.m2.1.1.3.4" xref="S4.SS5.p2.2.m2.1.1.3.4.cmml">B</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><apply id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p2.2.m2.1.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS5.p2.2.m2.1.1.2.cmml" xref="S4.SS5.p2.2.m2.1.1.2">ğ¹</ci><apply id="S4.SS5.p2.2.m2.1.1.3.cmml" xref="S4.SS5.p2.2.m2.1.1.3"><times id="S4.SS5.p2.2.m2.1.1.3.1.cmml" xref="S4.SS5.p2.2.m2.1.1.3.1"></times><ci id="S4.SS5.p2.2.m2.1.1.3.2.cmml" xref="S4.SS5.p2.2.m2.1.1.3.2">ğ‘…</ci><ci id="S4.SS5.p2.2.m2.1.1.3.3.cmml" xref="S4.SS5.p2.2.m2.1.1.3.3">ğº</ci><ci id="S4.SS5.p2.2.m2.1.1.3.4.cmml" xref="S4.SS5.p2.2.m2.1.1.3.4">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">F_{RGB}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.2.m2.1d">italic_F start_POSTSUBSCRIPT italic_R italic_G italic_B end_POSTSUBSCRIPT</annotation></semantics></math> e <math alttext="F_{hall}" class="ltx_Math" display="inline" id="S4.SS5.p2.3.m3.1"><semantics id="S4.SS5.p2.3.m3.1a"><msub id="S4.SS5.p2.3.m3.1.1" xref="S4.SS5.p2.3.m3.1.1.cmml"><mi id="S4.SS5.p2.3.m3.1.1.2" xref="S4.SS5.p2.3.m3.1.1.2.cmml">F</mi><mrow id="S4.SS5.p2.3.m3.1.1.3" xref="S4.SS5.p2.3.m3.1.1.3.cmml"><mi id="S4.SS5.p2.3.m3.1.1.3.2" xref="S4.SS5.p2.3.m3.1.1.3.2.cmml">h</mi><mo id="S4.SS5.p2.3.m3.1.1.3.1" xref="S4.SS5.p2.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S4.SS5.p2.3.m3.1.1.3.3" xref="S4.SS5.p2.3.m3.1.1.3.3.cmml">a</mi><mo id="S4.SS5.p2.3.m3.1.1.3.1a" xref="S4.SS5.p2.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S4.SS5.p2.3.m3.1.1.3.4" xref="S4.SS5.p2.3.m3.1.1.3.4.cmml">l</mi><mo id="S4.SS5.p2.3.m3.1.1.3.1b" xref="S4.SS5.p2.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S4.SS5.p2.3.m3.1.1.3.5" xref="S4.SS5.p2.3.m3.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.3.m3.1b"><apply id="S4.SS5.p2.3.m3.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p2.3.m3.1.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS5.p2.3.m3.1.1.2.cmml" xref="S4.SS5.p2.3.m3.1.1.2">ğ¹</ci><apply id="S4.SS5.p2.3.m3.1.1.3.cmml" xref="S4.SS5.p2.3.m3.1.1.3"><times id="S4.SS5.p2.3.m3.1.1.3.1.cmml" xref="S4.SS5.p2.3.m3.1.1.3.1"></times><ci id="S4.SS5.p2.3.m3.1.1.3.2.cmml" xref="S4.SS5.p2.3.m3.1.1.3.2">â„</ci><ci id="S4.SS5.p2.3.m3.1.1.3.3.cmml" xref="S4.SS5.p2.3.m3.1.1.3.3">ğ‘</ci><ci id="S4.SS5.p2.3.m3.1.1.3.4.cmml" xref="S4.SS5.p2.3.m3.1.1.3.4">ğ‘™</ci><ci id="S4.SS5.p2.3.m3.1.1.3.5.cmml" xref="S4.SS5.p2.3.m3.1.1.3.5">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.3.m3.1c">F_{hall}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.3.m3.1d">italic_F start_POSTSUBSCRIPT italic_h italic_a italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math>. The aim of this experiment is to qualitatively inspect whether the learned hallucinated features are indeed close to the real features that we would extract when having depth data available.
Interestingly, the 2D T-SNE projection shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#S4.F7" title="Figure 7 â€£ 4.5 Qualitative Analysis â€£ 4 Experimental Validation â€£ Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB"><span class="ltx_text ltx_ref_tag">7</span></a>, clearly highlights the proximity of the features obtained from depth data and the hallucinated ones inferred from RGB data. This further confirms the effectiveness of our approach.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="370" id="S4.F7.g1" src="x1.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">T-SNE of features extracted from RGB data and depth data, as well as hallucinated depth features generated from RGB data. For each of the three modalities, we extracted features from 500 samples. Hallucinated depth features are projected in close proximity to the original depth features, demonstrating the effectiveness of the method.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Execution Time Analysis</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.2">In the final part of the experimental analysis, we focus on the investigation of speed execution.
We compute the inference on a computer equipped with an Intel Core i7-7700K (4.50GHz) and an Nvidia GeForce GTX 1080 Ti, obtaining <math alttext="33" class="ltx_Math" display="inline" id="S4.SS6.p1.1.m1.1"><semantics id="S4.SS6.p1.1.m1.1a"><mn id="S4.SS6.p1.1.m1.1.1" xref="S4.SS6.p1.1.m1.1.1.cmml">33</mn><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.1.m1.1b"><cn id="S4.SS6.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS6.p1.1.m1.1.1">33</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.1.m1.1c">33</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p1.1.m1.1d">33</annotation></semantics></math> frame per second with a video memory occupation of about <math alttext="2.0" class="ltx_Math" display="inline" id="S4.SS6.p1.2.m2.1"><semantics id="S4.SS6.p1.2.m2.1a"><mn id="S4.SS6.p1.2.m2.1.1" xref="S4.SS6.p1.2.m2.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.2.m2.1b"><cn id="S4.SS6.p1.2.m2.1.1.cmml" type="float" xref="S4.SS6.p1.2.m2.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.2.m2.1c">2.0</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p1.2.m2.1d">2.0</annotation></semantics></math> GB.
These values assure real-time performance of the proposed system and enable its use in devices with limited computational power.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we presented a method for 3D human pose estimation from RGB images based on the Privileged Information paradigm.
In particular, we showed how it is possible to use depth data to improve the performance of a system based only on RGB images at inference time. We proved that the Privileged Information paradigm is effective even when applied to limited and small datasets. Moreover, the paradigm is general and can be applied to boost the performance of any method for 3D human pose estimation. In this paper, we applied it to a method that represents 3D poses as semi-perspective decoupled heatmaps and observed a significant improvement in performance.
Possible future works include collecting a larger dataset with accurate 3D joint annotations in order to broaden the experimental validation, and testing the paradigm using estimated depth from RGB (<em class="ltx_emph ltx_font_italic" id="S5.p1.1.1">e.g</em>.<span class="ltx_text" id="S5.p1.1.2"></span>,Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11104v1#bib.bib26" title="">26</a>]</cite>), which would remove the need for depth data completely.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was partially supported by the Piano per lo Sviluppo della Ricerca (PSR 2023) of the University of Siena - project FEATHER: Forecasting and Estimation of Actions and Trajectories for Human-robot intERactions.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">This work was supported by the European Commission under European Horizon 2020 Programme, grant number 951911â€”AI4Media.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Part of the work was funded within the project â€œAI platform with digital twins of interacting robots and peopleâ€, FAR Dipartimentale 2022 DIEF - Unimore.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alehdaghi, M., Josi, A., Cruz, R.M., Granger, E.: Visible-infrared person re-identification using privileged intermediate information. In: European Conference on Computer Vision. pp. 720â€“737. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Borghi, G., Pini, S., Grazioli, F., Vezzani, R., Cucchiara, R., etÂ al.: Face verification from depth using privileged information. In: Proceedings of the British Machine Vision Conference. p.Â 303 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Borghi, G., Pini, S., Vezzani, R., Cucchiara, R.: Driver face verification with depth maps. Sensors <span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">19</span>(15), Â 3361 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, C.H., Ramanan, D.: 3d human pose estimation= 2d pose estimation+ matching. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7035â€“7043 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, D., Zhou, B., Koltun, V., KrÃ¤henbÃ¼hl, P.: Learning by cheating. In: Conference on Robot Learning. pp. 66â€“75. PMLR (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.: Cascaded pyramid network for multi-person pose estimation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7103â€“7112 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Dabral, R., Mundhada, A., Kusupati, U., Afaque, S., Sharma, A., Jain, A.: Learning 3d human pose from structure and motion. In: Proceedings of the European conference on computer vision (ECCV). pp. 668â€“683 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Fabbri, M., Lanzi, F., Calderara, S., Alletto, S., Cucchiara, R.: Compressed volumetric heatmaps for multi-person 3d pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7204â€“7213 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Hoffman, J., Gupta, S., Darrell, T.: Learning with side information through modality hallucination. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 826â€“834 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Hossain, M.R.I., Little, J.J.: Exploiting temporal information for 3d human pose estimation. In: Proceedings of the European conference on computer vision (ECCV). pp. 68â€“84 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J.: End-to-end recovery of human shape and pose. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7122â€“7131 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Kim, D., Ka, W., Ahn, P., Joo, D., Chun, S., Kim, J.: Global-local path networks for monocular depth estimation with vertical cutdepth. arXiv preprint arXiv:2201.07436 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Lee, K., Lee, I., Lee, S.: Propagating lstm: 3d pose estimation based on joint interdependency. In: Proceedings of the European conference on computer vision (ECCV). pp. 119â€“135 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Lee, S., Rim, J., Jeong, B., Kim, G., Woo, B., Lee, H., Cho, S., Kwak, S.: Human pose estimation in extremely low-light conditions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 704â€“714 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Lin, H.Y., Chen, T.W.: Augmented reality with human body interaction based on monocular 3d pose estimation. In: International Conference on Advanced Concepts for Intelligent Vision Systems. pp. 321â€“331. Springer (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Luvizon, D.C., Picard, D., Tabia, H.: 2d/3d pose estimation and action recognition using multitask deep learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5137â€“5146 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
VanÂ der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning research <span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">9</span>(11) (2008)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Mankoff, K.D., Russo, T.A.: The kinect: a low-cost, high-resolution, short-range 3d camera. Earth Surface Processes and Landforms <span class="ltx_text ltx_font_bold" id="bib.bib19.1.1">38</span>(9), 926â€“936 (2013)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Martinez, J., Hossain, R., Romero, J., Little, J.J.: A simple yet effective baseline for 3d human pose estimation. In: Proceedings of the IEEE international conference on computer vision. pp. 2640â€“2649 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko, O., Xu, W., Theobalt, C.: Monocular 3d human pose estimation in the wild using improved cnn supervision. In: 2017 international conference on 3D vision (3DV). pp. 506â€“516. IEEE (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Mehta, D., Sridhar, S., Sotnychenko, O., Rhodin, H., Shafiei, M., Seidel, H.P., Xu, W., Casas, D., Theobalt, C.: Vnect: Real-time 3d human pose estimation with a single rgb camera. Acm transactions on graphics (tog) <span class="ltx_text ltx_font_bold" id="bib.bib22.1.1">36</span>(4), 1â€“14 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM <span class="ltx_text ltx_font_bold" id="bib.bib23.1.1">65</span>(1), 99â€“106 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Moreno-Noguer, F.: 3d human pose estimation from a single image via distance matrix regression. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2823â€“2832 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Nibali, A., He, Z., Morgan, S., Prendergast, L.: 3d human pose estimation with 2d marginal heatmaps. In: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 1477â€“1485. IEEE (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Oquab, M., Darcet, T., Moutakanni, T., Vo, H.V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Howes, R., Huang, P.Y., Xu, H., Sharma, V., Li, S.W., Galuba, W., Rabbat, M., Assran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski, P.: Dinov2: Learning robust visual features without supervision (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Pavlakos, G., Zhou, X., Derpanis, K.G., Daniilidis, K.: Coarse-to-fine volumetric prediction for single-image 3d human pose. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7025â€“7034 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Pavlakos, G., Zhu, L., Zhou, X., Daniilidis, K.: Learning to estimate 3d human pose and shape from a single color image. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 459â€“468 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Pini, S., Borghi, G., Vezzani, R., Cucchiara, R.: Video synthesis from intensity and event frames. In: Image Analysis and Processingâ€“ICIAP 2019: 20th International Conference, Trento, Italy, September 9â€“13, 2019, Proceedings, Part I 20. pp. 313â€“323. Springer (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 12179â€“12188 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Ranftl, R., Lasinger, K., Hafner, D., Schindler, K., Koltun, V.: Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib31.1.1">44</span>(3), 1623â€“1637 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
SÃ¡rÃ¡ndi, I., Linder, T., Arras, K.O., Leibe, B.: Synthetic occlusion augmentation with volumetric heatmaps for the 2018 eccv posetrack challenge on 3d human pose estimation. arXiv preprint arXiv:1809.04987 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Sarbolandi, H., Lefloch, D., Kolb, A.: Kinect range sensing: Structured-light versus time-of-flight kinect. Computer vision and image understanding <span class="ltx_text ltx_font_bold" id="bib.bib33.1.1">139</span>, 1â€“20 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Sharmanska, V., Quadrianto, N., Lampert, C.H.: Learning to rank using privileged information. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 825â€“832 (2013)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Simoni, A., Pini, S., Borghi, G., Vezzani, R.: Semi-perspective decoupled heatmaps for 3d robot pose estimation from depth maps. IEEE Robotics and Automation Letters <span class="ltx_text ltx_font_bold" id="bib.bib35.1.1">7</span>(4), 11569â€“11576 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution representation learning for human pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5693â€“5703 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Sun, K., Zhao, Y., Jiang, B., Cheng, T., Xiao, B., Liu, D., Mu, Y., Wang, X., Liu, W., Wang, J.: High-resolution representations for labeling pixels and regions. arXiv preprint arXiv:1904.04514 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Sun, X., Xiao, B., Wei, F., Liang, S., Wei, Y.: Integral human pose regression. In: Proceedings of the European conference on computer vision (ECCV). pp. 529â€“545 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Tekin, B., MÃ¡rquez-Neila, P., Salzmann, M., Fua, P.: Learning to fuse 2d and 3d image cues for monocular body pose estimation. In: Proceedings of the IEEE international conference on computer vision. pp. 3941â€“3950 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Toshev, A., Szegedy, C.: Deeppose: Human pose estimation via deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1653â€“1660 (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Vapnik, V., Vashist, A.: A new learning paradigm: Learning using privileged information. Neural networks <span class="ltx_text ltx_font_bold" id="bib.bib41.1.1">22</span>(5-6), 544â€“557 (2009)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Vapnik, V., Vashist, A., Pavlovitch, N.: Learning using hidden information (learning with teacher). In: Proceedings of the International Joint Conference on Neural Networks. pp. 3188â€“3195. IEEE (2009)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Wu, H., Wu, X., Huang, K., Ma, H.: Human pose recognition based on openpose and application in safety detection of intelligent factory (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Xing, E., Jordan, M., Russell, S.J., Ng, A.: Distance metric learning with application to clustering with side-information. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib44.1.1">15</span> (2002)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Xu, X., Li, W., Xu, D.: Distance metric learning using privileged information for face verification and person re-identification. IEEE Transactions on Neural Networks and Learning Systems <span class="ltx_text ltx_font_bold" id="bib.bib45.1.1">26</span>(12), 3150â€“3162 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Yang, W., Ouyang, W., Wang, X., Ren, J., Li, H., Wang, X.: 3d human pose estimation in the wild by adversarial learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5255â€“5264 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Yuan, S., Stenger, B., Kim, T.K.: 3d hand pose estimation from rgb using privileged learning with depth data. In: Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Zimmermann, C., Welschehold, T., Dornhege, C., Burgard, W., Brox, T.: 3d human pose estimation in rgbd images for robotic task learning. In: Proceedings of the IEEE International Conference on Robotics and Automation. pp. 1986â€“1992. IEEE (2018)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 17 11:54:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
