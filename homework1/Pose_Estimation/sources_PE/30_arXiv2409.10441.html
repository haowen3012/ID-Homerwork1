<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera</title>
<!--Generated on Mon Sep 16 16:16:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.10441v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S1" title="In CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S2" title="In CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Previous Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S2.SS1" title="In II Previous Work ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Camera-to-Robot Pose Estimation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S2.SS2" title="In II Previous Work ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Robot Part Detection</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S3" title="In CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S3.SS1" title="In III Methodology ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Few-shot Learning for Robot Parts Detection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S3.SS2" title="In III Methodology ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Camera-to-Robot Pose Estimation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4" title="In CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments and Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4.SS1" title="In IV Experiments and Results ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Implementation Details</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4.SS2" title="In IV Experiments and Results ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Few-shot Learning for Robot Parts Detection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4.SS3" title="In IV Experiments and Results ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Experiment on DREAM-real Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4.SS4" title="In IV Experiments and Results ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Experiment on Panda Manipulation Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4.SS5" title="In IV Experiments and Results ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_italic">Experiment on DROID Dataset</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S5" title="In CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions
<br class="ltx_break"/>Using a Single Camera
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jingpei Lu<sup class="ltx_sup" id="id11.11.id1"><span class="ltx_text ltx_font_italic" id="id11.11.id1.1">1,†</span></sup>, Zekai Liang<sup class="ltx_sup" id="id12.12.id2"><span class="ltx_text ltx_font_italic" id="id12.12.id2.1">1,†</span></sup>, Tristin Xie<sup class="ltx_sup" id="id13.13.id3"><span class="ltx_text ltx_font_italic" id="id13.13.id3.1">1</span></sup>, Florian Ritcher<sup class="ltx_sup" id="id14.14.id4"><span class="ltx_text ltx_font_italic" id="id14.14.id4.1">1</span></sup>, Shan Lin<sup class="ltx_sup" id="id15.15.id5"><span class="ltx_text ltx_font_italic" id="id15.15.id5.1">1</span></sup>, Sainan Liu<sup class="ltx_sup" id="id16.16.id6"><span class="ltx_text ltx_font_italic" id="id16.16.id6.1">2</span></sup>, Michael C. Yip<sup class="ltx_sup" id="id17.17.id7"><span class="ltx_text ltx_font_italic" id="id17.17.id7.1">1</span></sup>
</span><span class="ltx_author_notes"><math alttext="\dagger" class="ltx_Math" display="inline" id="id8.8.m1.1"><semantics id="id8.8.m1.1a"><mo id="id8.8.m1.1.1" xref="id8.8.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id8.8.m1.1b"><ci id="id8.8.m1.1.1.cmml" xref="id8.8.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id8.8.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id8.8.m1.1d">†</annotation></semantics></math> These authors contributed equally.<sup class="ltx_sup" id="id18.18.id1"><span class="ltx_text ltx_font_italic" id="id18.18.id1.1">1</span></sup>Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA 92093 USA.<span class="ltx_text ltx_font_typewriter" id="id19.19.id2" style="font-size:90%;">{jil360, z9liang, tyx001, frichter,
shl102, yip}@ucsd.edu</span><sup class="ltx_sup" id="id20.20.id1"><span class="ltx_text ltx_font_italic" id="id20.20.id1.1">2</span></sup>Intel Labs, USA. <span class="ltx_text ltx_font_typewriter" id="id21.21.id2" style="font-size:90%;">sainan.liu@intel.com</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id22.id1">Camera-to-robot calibration is crucial for vision-based robot control and requires effort to make it accurate. Recent advancements in markerless pose estimation methods have eliminated the need for time-consuming physical setups for camera-to-robot calibration. While the existing markerless pose estimation methods have demonstrated impressive accuracy without the need for cumbersome setups, they rely on the assumption that all the robot joints are visible within the camera’s field of view. However, in practice, robots usually move in and out of view, and some portion of the robot may stay out-of-frame during the whole manipulation task due to real-world constraints, leading to a lack of sufficient visual features and subsequent failure of these approaches. To address this challenge and enhance the applicability to vision-based robot control, we propose a novel framework capable of estimating the robot pose with partially visible robot manipulators. Our approach leverages the Vision-Language Models for fine-grained robot components detection, and integrates it into a keypoint-based pose estimation network, which enables more robust performance in varied operational conditions.
The framework is evaluated on both public robot datasets and self-collected partial-view datasets to demonstrate our robustness and generalizability. As a result, this method is effective for robot pose estimation in a wider range of real-world manipulation scenarios.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Estimating the Camera-to-Robot transform is crucial for manipulation, as it links the visual feedback from the camera to the space where the robot is operating, enabling accurate model-based robot arm manipulation with visual observations.
Calibrating the Camera-to-Robot transform requires a significant amount of effort. Traditional calibration methods, such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib3" title="">3</a>]</cite>, usually place fixed fiducial markers on the end-effector, collect images of several robot joint angles, and compute the transformation. These techniques have proved their advantage in generalizability and availability for different environments and robots.
However, such a procedure requires modification to the robotic system, which is not always possible, such as in instances where a dataset has already been collected <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib5" title="">5</a>]</cite>.
Furthermore, the accuracy of the fiducial marker calibration approach is limited to the accuracy of the fiducial location relative to the robot.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The recent development of deep learning methods makes the markerless robot pose estimation possible, which can generally be divided into keypoint-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib8" title="">8</a>]</cite> and rendering-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib11" title="">11</a>]</cite>. Contrary to classical approaches that need fiducial markers, deep learning-based pose estimation methods don’t require cumbersome physical setups for calibration. Instead, they utilize deep neural networks for feature extraction or segmentation. The robot pose is then estimated using the keypoint features or the segmented robot masks.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<p class="ltx_p ltx_align_center" id="S1.F1.1"><span class="ltx_text" id="S1.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="570" id="S1.F1.1.1.g1" src="x1.png" width="829"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.4.2" style="font-size:90%;">In real-world robot manipulation scenarios, the camera does not always capture all the robot links, and the visibility of robot links changes from time to time. Our method leverages the limited available visual features within the camera view and achieves state-of-the-art performance on robot pose estimation.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Despite considerable efforts to improve camera-to-robot pose estimation’s flexibility, existing works have made a strong assumption that the entire robot arm is fully visible from the camera view. However, in real-world manipulation scenarios, the operating space is often limited, thus setting spatial constraints on camera placement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib5" title="">5</a>]</cite>.
Additionally, the shape and size of target objects further complicate the trade-off between the capturing robot body and the manipulation targets. In such scenarios, the camera placement is typically driven more by the demands of the manipulation tasks instead of the need to observe all robot joints motion, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag">1</span></a>. Consequently, video sequences with partially visible robot arm make up the majority of robotics manipulation datasets, such as Open X-Embodiment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib4" title="">4</a>]</cite> and DROID <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib5" title="">5</a>]</cite>, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S1.F2" title="Figure 2 ‣ I Introduction ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag">2</span></a>. Existing methods often fail in situations where robots are only partially visible due to these real world constraints. Therefore, being able to estimate the robot pose with partial views is important from the practical aspect of robot manipulation scenarios where the camera can only capture a portion of the robot.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we introduce a novel framework for Camera-to-Robot Pose Estimation that extends the markerless robot pose estimation to partially visible scenarios. Our method integrates the Vision-Language Model (VLM) to detect the visible robot components and dynamically select the keypoints from visible robot links for the pose estimation. Moreover, we also improve keypoint detection performance by introducing the distribution-aware coordinate representation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib14" title="">14</a>]</cite> to our previous development for the pose estimation network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib7" title="">7</a>]</cite>. We evaluate our framework on both fully visible and partially visible setups and achieve state-of-the-art performance on the public robot pose dataset and our self-collected dataset.
In summary, our contributions are threefold:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a framework for markerless camera-to-robot pose estimation for more practical manipulation setups, where often only parts of the robot can be observed from a camera.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We show the benefits of using the vision-language foundation model with few-shot learning for robot part detection and integrate it into the pose estimation framework.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We show that our method achieves state-of-the-art performance compared to the existing methods on the benchmarking dataset while demonstrating the capability of estimating accurate camera extrinsic information for large-scale robot manipulation datasets.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F2">
<p class="ltx_p ltx_align_center" id="S1.F2.1"><span class="ltx_text" id="S1.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="713" id="S1.F2.1.1.g1" src="x2.png" width="1196"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S1.F2.4.2" style="font-size:90%;">Sample images are from DROID robot learning dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib5" title="">5</a>]</cite>. Often, only certain parts of the robot are visible in the camera view, and sometimes none of them are visible.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Previous Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Camera-to-Robot Pose Estimation</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Traditionally, the camera-to-robot pose is calibrated using the fiducial markers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib2" title="">2</a>]</cite>.
For articulated robots, the fiducial markers provide 2D point features on the robots, the 3D position of the markers can be calculated using robot kinematics, and the robot pose can be derived by solving a Perspective-n-Point problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib18" title="">18</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">As the field evolved, there was a shift towards markerless pose estimation. Initial efforts in this direction utilized depth cameras to localize articulated robots <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib22" title="">22</a>]</cite>. With the rise of Deep Neural Networks (DNNs), a new paradigm emerged. DNNs, with their advantages of extracting point features without the need for markers, have significantly enhanced the performance of markerless pose estimation for articulated robots <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib25" title="">25</a>]</cite>. Beyond keypoint-based methods, recent works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib28" title="">28</a>]</cite> have demonstrated the potential of rendering-based methods. Benefiting from the dense correspondence provided by robot masks, rendering-based methods achieve state-of-the-art performance on robot pose estimation, but with compromise on the processing speed due to iterative render-and-compare. Most recently, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib7" title="">7</a>]</cite> proposed CtRNet, which uses robot masks to supervise the keypoint detector, achieving comparable performance to rendering-based methods while maintaining real-time inference speed.
Nonetheless, existing methods focus on scenarios where the robot manipulator is fully observable.
In real-world manipulations, it’s non-trivial to set up the camera and the manipulator such that all the robot links stay within the camera view during the episodes, thereby diminishing the generalizability of existing methods when dealing with less constrained, real-world environments. In contrast, our proposed method overcomes this limitation by integrating a vision-language foundation model to detect the visibility of different robot parts and dynamically select the keypoints from visible robot parts for pose estimation.
</p>
</div>
<figure class="ltx_figure" id="S2.F3">
<p class="ltx_p ltx_align_center" id="S2.F3.1"><span class="ltx_text" id="S2.F3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="338" id="S2.F3.1.1.g1" src="x3.png" width="1076"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.4.2" style="font-size:90%;">Model inference pipeline. CtRNet-X estimates camera-to-robot transform given the images and the corresponding joint angles. The framework uses a set of structured prompts and the fine-tuned CLIP model to detect which robot parts are visible and dynamically adjusts the keypoint selection. The keypoint detector outputs 2D keypoints, and the corresponding 3D keypoints are obtained from the robot forward kinematics. Finally, a PnP solver is utilized to estimate the camera-to-robot transformation matrix given the selected keypoint correspondence. </span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Robot Part Detection</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">With the rapid development of deep learning, convolutional neural networks (CNNs) have demonstrated superior performance in object detection. The introduction of residual connections in ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib29" title="">29</a>]</cite> makes it easier to construct deeper CNN architectures, hence achieving human-level performance on image recognition. However, deep neural networks typically require very large datasets to learn, which demands substantial hardware resources as well as labeling efforts. Furthermore, data is often not available due to not only the nature of the problem or privacy concerns but also the cost of data preparation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Recent developments of Vision-Language Models (VLMs), such as CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib31" title="">31</a>]</cite>, have achieved remarkable performance on open-vocabulary object detection through training on large-scale datasets collected from the Internet. Recent research has focused on customizing the training and fine-tuning the CLIP model for specific downstream tasks. For example, CoOp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib32" title="">32</a>]</cite> optimizes the tokenized prompt vectors while freezing the model, reducing training time and maintaining the model’s performance. Some following works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib33" title="">33</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib34" title="">34</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib35" title="">35</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib36" title="">36</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib37" title="">37</a>]</cite> further improve prompt learning’s capability. Additionally, Parameter-Efficient Fine-Tuning (PEFT) like Bitfit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib38" title="">38</a>]</cite> and Clip-adapter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib39" title="">39</a>]</cite> focuses on VLM model optimization while attempting to minimize the number of training parameters, balancing training time and performance. However, the existing work mainly considers individual and common object classification, whereas their performance on fine-grained object parts detection is still unexplored. In this work, we explore several methods to tackle this challenge and demonstrate an effective way of fine-grained robot parts detection with few-shot learning samples.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce our framework for camera-to-robot pose estimation. The inference pipeline of our framework is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S2.F3" title="Figure 3 ‣ II-A Camera-to-Robot Pose Estimation ‣ II Previous Work ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag">3</span></a>. Our framework builds upon CtRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib7" title="">7</a>]</cite>, extending it to handle partially visible scenarios.
We utilize the vision-language foundation model, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib31" title="">31</a>]</cite>, to identify the visible robot parts in the image frame hence selecting which keypoints to use for robot pose estimation.
Moreover, we also incorporate the Distribution-Aware coordinate Representation of Keypoint method (DARK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib14" title="">14</a>]</cite>) into our framework to further enhance the performance of keypoint detection.
In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S3.SS1" title="III-A Few-shot Learning for Robot Parts Detection ‣ III Methodology ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>, we detail our approach to fine-tune the CLIP model for robot part detection. In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S3.SS2" title="III-B Camera-to-Robot Pose Estimation ‣ III Methodology ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>, we first provide an overview of CtRNet and then introduce our improvements.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Few-shot Learning for Robot Parts Detection</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">VLM Fine-tuning</span>. Different from classifying the individual objects within images, our problem lies in detecting the components of the robot. Although vision-language foundation models demonstrate the capability of zero-shot object classification, they do not perform well in detecting robot parts. This is because the training data from the Internet lacks fine-grained semantic labels (e.g. robot end-effector, robot base).
In order to fine-tune the VLM to detect the robot parts in the images, we collected a small number of samples from the robot learning dataset, DROID <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib5" title="">5</a>]</cite>, and investigated the few-shot transfer capability of the popular vision-language foundation model, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib31" title="">31</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Training a large model like CLIP with a small dataset can be challenging. To address this, we employ the parameter-efficient fine-tuning method, Low-Rank Adaptation (LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib40" title="">40</a>]</cite>), and a strategic prompting method for fine-tuning the CLIP.
LoRA provides inspiration on freezing the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture. We add the LoRA module on both text and image encoders of the CLIP. For a forward linear passes <math alttext="h=W_{0}x" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">h</mi><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><msub id="S3.SS1.p2.1.m1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.3.2.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.2.2" xref="S3.SS1.p2.1.m1.1.1.3.2.2.cmml">W</mi><mn id="S3.SS1.p2.1.m1.1.1.3.2.3" xref="S3.SS1.p2.1.m1.1.1.3.2.3.cmml">0</mn></msub><mo id="S3.SS1.p2.1.m1.1.1.3.1" xref="S3.SS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.1.m1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml">x</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><eq id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></eq><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ℎ</ci><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><times id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3.1"></times><apply id="S3.SS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.3.2.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.3.2.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2.2">𝑊</ci><cn id="S3.SS1.p2.1.m1.1.1.3.2.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.3.2.3">0</cn></apply><ci id="S3.SS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">h=W_{0}x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_h = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x</annotation></semantics></math>, we apply LoRA such that</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h=W_{0}x+\Delta Wx=W_{0}x+BAx" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">h</mi><mo id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.1.1.4" xref="S3.E1.m1.1.1.4.cmml"><mrow id="S3.E1.m1.1.1.4.2" xref="S3.E1.m1.1.1.4.2.cmml"><msub id="S3.E1.m1.1.1.4.2.2" xref="S3.E1.m1.1.1.4.2.2.cmml"><mi id="S3.E1.m1.1.1.4.2.2.2" xref="S3.E1.m1.1.1.4.2.2.2.cmml">W</mi><mn id="S3.E1.m1.1.1.4.2.2.3" xref="S3.E1.m1.1.1.4.2.2.3.cmml">0</mn></msub><mo id="S3.E1.m1.1.1.4.2.1" xref="S3.E1.m1.1.1.4.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.4.2.3" xref="S3.E1.m1.1.1.4.2.3.cmml">x</mi></mrow><mo id="S3.E1.m1.1.1.4.1" xref="S3.E1.m1.1.1.4.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.4.3" xref="S3.E1.m1.1.1.4.3.cmml"><mi id="S3.E1.m1.1.1.4.3.2" mathvariant="normal" xref="S3.E1.m1.1.1.4.3.2.cmml">Δ</mi><mo id="S3.E1.m1.1.1.4.3.1" xref="S3.E1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.4.3.3" xref="S3.E1.m1.1.1.4.3.3.cmml">W</mi><mo id="S3.E1.m1.1.1.4.3.1a" xref="S3.E1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.4.3.4" xref="S3.E1.m1.1.1.4.3.4.cmml">x</mi></mrow></mrow><mo id="S3.E1.m1.1.1.5" xref="S3.E1.m1.1.1.5.cmml">=</mo><mrow id="S3.E1.m1.1.1.6" xref="S3.E1.m1.1.1.6.cmml"><mrow id="S3.E1.m1.1.1.6.2" xref="S3.E1.m1.1.1.6.2.cmml"><msub id="S3.E1.m1.1.1.6.2.2" xref="S3.E1.m1.1.1.6.2.2.cmml"><mi id="S3.E1.m1.1.1.6.2.2.2" xref="S3.E1.m1.1.1.6.2.2.2.cmml">W</mi><mn id="S3.E1.m1.1.1.6.2.2.3" xref="S3.E1.m1.1.1.6.2.2.3.cmml">0</mn></msub><mo id="S3.E1.m1.1.1.6.2.1" xref="S3.E1.m1.1.1.6.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.6.2.3" xref="S3.E1.m1.1.1.6.2.3.cmml">x</mi></mrow><mo id="S3.E1.m1.1.1.6.1" xref="S3.E1.m1.1.1.6.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.6.3" xref="S3.E1.m1.1.1.6.3.cmml"><mi id="S3.E1.m1.1.1.6.3.2" xref="S3.E1.m1.1.1.6.3.2.cmml">B</mi><mo id="S3.E1.m1.1.1.6.3.1" xref="S3.E1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.6.3.3" xref="S3.E1.m1.1.1.6.3.3.cmml">A</mi><mo id="S3.E1.m1.1.1.6.3.1a" xref="S3.E1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.6.3.4" xref="S3.E1.m1.1.1.6.3.4.cmml">x</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><and id="S3.E1.m1.1.1a.cmml" xref="S3.E1.m1.1.1"></and><apply id="S3.E1.m1.1.1b.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"></eq><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">ℎ</ci><apply id="S3.E1.m1.1.1.4.cmml" xref="S3.E1.m1.1.1.4"><plus id="S3.E1.m1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.4.1"></plus><apply id="S3.E1.m1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.4.2"><times id="S3.E1.m1.1.1.4.2.1.cmml" xref="S3.E1.m1.1.1.4.2.1"></times><apply id="S3.E1.m1.1.1.4.2.2.cmml" xref="S3.E1.m1.1.1.4.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.4.2.2.1.cmml" xref="S3.E1.m1.1.1.4.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.4.2.2.2.cmml" xref="S3.E1.m1.1.1.4.2.2.2">𝑊</ci><cn id="S3.E1.m1.1.1.4.2.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.4.2.2.3">0</cn></apply><ci id="S3.E1.m1.1.1.4.2.3.cmml" xref="S3.E1.m1.1.1.4.2.3">𝑥</ci></apply><apply id="S3.E1.m1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.4.3"><times id="S3.E1.m1.1.1.4.3.1.cmml" xref="S3.E1.m1.1.1.4.3.1"></times><ci id="S3.E1.m1.1.1.4.3.2.cmml" xref="S3.E1.m1.1.1.4.3.2">Δ</ci><ci id="S3.E1.m1.1.1.4.3.3.cmml" xref="S3.E1.m1.1.1.4.3.3">𝑊</ci><ci id="S3.E1.m1.1.1.4.3.4.cmml" xref="S3.E1.m1.1.1.4.3.4">𝑥</ci></apply></apply></apply><apply id="S3.E1.m1.1.1c.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.5.cmml" xref="S3.E1.m1.1.1.5"></eq><share href="https://arxiv.org/html/2409.10441v1#S3.E1.m1.1.1.4.cmml" id="S3.E1.m1.1.1d.cmml" xref="S3.E1.m1.1.1"></share><apply id="S3.E1.m1.1.1.6.cmml" xref="S3.E1.m1.1.1.6"><plus id="S3.E1.m1.1.1.6.1.cmml" xref="S3.E1.m1.1.1.6.1"></plus><apply id="S3.E1.m1.1.1.6.2.cmml" xref="S3.E1.m1.1.1.6.2"><times id="S3.E1.m1.1.1.6.2.1.cmml" xref="S3.E1.m1.1.1.6.2.1"></times><apply id="S3.E1.m1.1.1.6.2.2.cmml" xref="S3.E1.m1.1.1.6.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.6.2.2.1.cmml" xref="S3.E1.m1.1.1.6.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.6.2.2.2.cmml" xref="S3.E1.m1.1.1.6.2.2.2">𝑊</ci><cn id="S3.E1.m1.1.1.6.2.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.6.2.2.3">0</cn></apply><ci id="S3.E1.m1.1.1.6.2.3.cmml" xref="S3.E1.m1.1.1.6.2.3">𝑥</ci></apply><apply id="S3.E1.m1.1.1.6.3.cmml" xref="S3.E1.m1.1.1.6.3"><times id="S3.E1.m1.1.1.6.3.1.cmml" xref="S3.E1.m1.1.1.6.3.1"></times><ci id="S3.E1.m1.1.1.6.3.2.cmml" xref="S3.E1.m1.1.1.6.3.2">𝐵</ci><ci id="S3.E1.m1.1.1.6.3.3.cmml" xref="S3.E1.m1.1.1.6.3.3">𝐴</ci><ci id="S3.E1.m1.1.1.6.3.4.cmml" xref="S3.E1.m1.1.1.6.3.4">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">h=W_{0}x+\Delta Wx=W_{0}x+BAx</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_h = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + roman_Δ italic_W italic_x = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + italic_B italic_A italic_x</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.10">where the <math alttext="W_{0}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m1.1"><semantics id="S3.SS1.p2.2.m1.1a"><msub id="S3.SS1.p2.2.m1.1.1" xref="S3.SS1.p2.2.m1.1.1.cmml"><mi id="S3.SS1.p2.2.m1.1.1.2" xref="S3.SS1.p2.2.m1.1.1.2.cmml">W</mi><mn id="S3.SS1.p2.2.m1.1.1.3" xref="S3.SS1.p2.2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m1.1b"><apply id="S3.SS1.p2.2.m1.1.1.cmml" xref="S3.SS1.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m1.1.1.1.cmml" xref="S3.SS1.p2.2.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m1.1.1.2.cmml" xref="S3.SS1.p2.2.m1.1.1.2">𝑊</ci><cn id="S3.SS1.p2.2.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m1.1c">W_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m1.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is the pre-trained weight matrix of the self-attention module of CLIP, and <math alttext="\Delta W" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m2.1"><semantics id="S3.SS1.p2.3.m2.1a"><mrow id="S3.SS1.p2.3.m2.1.1" xref="S3.SS1.p2.3.m2.1.1.cmml"><mi id="S3.SS1.p2.3.m2.1.1.2" mathvariant="normal" xref="S3.SS1.p2.3.m2.1.1.2.cmml">Δ</mi><mo id="S3.SS1.p2.3.m2.1.1.1" xref="S3.SS1.p2.3.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS1.p2.3.m2.1.1.3" xref="S3.SS1.p2.3.m2.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m2.1b"><apply id="S3.SS1.p2.3.m2.1.1.cmml" xref="S3.SS1.p2.3.m2.1.1"><times id="S3.SS1.p2.3.m2.1.1.1.cmml" xref="S3.SS1.p2.3.m2.1.1.1"></times><ci id="S3.SS1.p2.3.m2.1.1.2.cmml" xref="S3.SS1.p2.3.m2.1.1.2">Δ</ci><ci id="S3.SS1.p2.3.m2.1.1.3.cmml" xref="S3.SS1.p2.3.m2.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m2.1c">\Delta W</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m2.1d">roman_Δ italic_W</annotation></semantics></math> is the trainable weight matrix. Specifically, <math alttext="W_{0}\in\mathbb{R}^{d\times k}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m3.1"><semantics id="S3.SS1.p2.4.m3.1a"><mrow id="S3.SS1.p2.4.m3.1.1" xref="S3.SS1.p2.4.m3.1.1.cmml"><msub id="S3.SS1.p2.4.m3.1.1.2" xref="S3.SS1.p2.4.m3.1.1.2.cmml"><mi id="S3.SS1.p2.4.m3.1.1.2.2" xref="S3.SS1.p2.4.m3.1.1.2.2.cmml">W</mi><mn id="S3.SS1.p2.4.m3.1.1.2.3" xref="S3.SS1.p2.4.m3.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS1.p2.4.m3.1.1.1" xref="S3.SS1.p2.4.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.4.m3.1.1.3" xref="S3.SS1.p2.4.m3.1.1.3.cmml"><mi id="S3.SS1.p2.4.m3.1.1.3.2" xref="S3.SS1.p2.4.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.4.m3.1.1.3.3" xref="S3.SS1.p2.4.m3.1.1.3.3.cmml"><mi id="S3.SS1.p2.4.m3.1.1.3.3.2" xref="S3.SS1.p2.4.m3.1.1.3.3.2.cmml">d</mi><mo id="S3.SS1.p2.4.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.4.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.4.m3.1.1.3.3.3" xref="S3.SS1.p2.4.m3.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m3.1b"><apply id="S3.SS1.p2.4.m3.1.1.cmml" xref="S3.SS1.p2.4.m3.1.1"><in id="S3.SS1.p2.4.m3.1.1.1.cmml" xref="S3.SS1.p2.4.m3.1.1.1"></in><apply id="S3.SS1.p2.4.m3.1.1.2.cmml" xref="S3.SS1.p2.4.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m3.1.1.2.1.cmml" xref="S3.SS1.p2.4.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.4.m3.1.1.2.2.cmml" xref="S3.SS1.p2.4.m3.1.1.2.2">𝑊</ci><cn id="S3.SS1.p2.4.m3.1.1.2.3.cmml" type="integer" xref="S3.SS1.p2.4.m3.1.1.2.3">0</cn></apply><apply id="S3.SS1.p2.4.m3.1.1.3.cmml" xref="S3.SS1.p2.4.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m3.1.1.3.1.cmml" xref="S3.SS1.p2.4.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.4.m3.1.1.3.2.cmml" xref="S3.SS1.p2.4.m3.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.4.m3.1.1.3.3.cmml" xref="S3.SS1.p2.4.m3.1.1.3.3"><times id="S3.SS1.p2.4.m3.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m3.1.1.3.3.1"></times><ci id="S3.SS1.p2.4.m3.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m3.1.1.3.3.2">𝑑</ci><ci id="S3.SS1.p2.4.m3.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m3.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m3.1c">W_{0}\in\mathbb{R}^{d\times k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m3.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="B\in\mathbb{R}^{d\times r}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m4.1"><semantics id="S3.SS1.p2.5.m4.1a"><mrow id="S3.SS1.p2.5.m4.1.1" xref="S3.SS1.p2.5.m4.1.1.cmml"><mi id="S3.SS1.p2.5.m4.1.1.2" xref="S3.SS1.p2.5.m4.1.1.2.cmml">B</mi><mo id="S3.SS1.p2.5.m4.1.1.1" xref="S3.SS1.p2.5.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.5.m4.1.1.3" xref="S3.SS1.p2.5.m4.1.1.3.cmml"><mi id="S3.SS1.p2.5.m4.1.1.3.2" xref="S3.SS1.p2.5.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.5.m4.1.1.3.3" xref="S3.SS1.p2.5.m4.1.1.3.3.cmml"><mi id="S3.SS1.p2.5.m4.1.1.3.3.2" xref="S3.SS1.p2.5.m4.1.1.3.3.2.cmml">d</mi><mo id="S3.SS1.p2.5.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.5.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.5.m4.1.1.3.3.3" xref="S3.SS1.p2.5.m4.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m4.1b"><apply id="S3.SS1.p2.5.m4.1.1.cmml" xref="S3.SS1.p2.5.m4.1.1"><in id="S3.SS1.p2.5.m4.1.1.1.cmml" xref="S3.SS1.p2.5.m4.1.1.1"></in><ci id="S3.SS1.p2.5.m4.1.1.2.cmml" xref="S3.SS1.p2.5.m4.1.1.2">𝐵</ci><apply id="S3.SS1.p2.5.m4.1.1.3.cmml" xref="S3.SS1.p2.5.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m4.1.1.3.1.cmml" xref="S3.SS1.p2.5.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.5.m4.1.1.3.2.cmml" xref="S3.SS1.p2.5.m4.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.5.m4.1.1.3.3.cmml" xref="S3.SS1.p2.5.m4.1.1.3.3"><times id="S3.SS1.p2.5.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.5.m4.1.1.3.3.1"></times><ci id="S3.SS1.p2.5.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.5.m4.1.1.3.3.2">𝑑</ci><ci id="S3.SS1.p2.5.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.5.m4.1.1.3.3.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m4.1c">B\in\mathbb{R}^{d\times r}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m4.1d">italic_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_r end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="A\in\mathbb{R}^{r\times k}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m5.1"><semantics id="S3.SS1.p2.6.m5.1a"><mrow id="S3.SS1.p2.6.m5.1.1" xref="S3.SS1.p2.6.m5.1.1.cmml"><mi id="S3.SS1.p2.6.m5.1.1.2" xref="S3.SS1.p2.6.m5.1.1.2.cmml">A</mi><mo id="S3.SS1.p2.6.m5.1.1.1" xref="S3.SS1.p2.6.m5.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.6.m5.1.1.3" xref="S3.SS1.p2.6.m5.1.1.3.cmml"><mi id="S3.SS1.p2.6.m5.1.1.3.2" xref="S3.SS1.p2.6.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.6.m5.1.1.3.3" xref="S3.SS1.p2.6.m5.1.1.3.3.cmml"><mi id="S3.SS1.p2.6.m5.1.1.3.3.2" xref="S3.SS1.p2.6.m5.1.1.3.3.2.cmml">r</mi><mo id="S3.SS1.p2.6.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.6.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.6.m5.1.1.3.3.3" xref="S3.SS1.p2.6.m5.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m5.1b"><apply id="S3.SS1.p2.6.m5.1.1.cmml" xref="S3.SS1.p2.6.m5.1.1"><in id="S3.SS1.p2.6.m5.1.1.1.cmml" xref="S3.SS1.p2.6.m5.1.1.1"></in><ci id="S3.SS1.p2.6.m5.1.1.2.cmml" xref="S3.SS1.p2.6.m5.1.1.2">𝐴</ci><apply id="S3.SS1.p2.6.m5.1.1.3.cmml" xref="S3.SS1.p2.6.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m5.1.1.3.1.cmml" xref="S3.SS1.p2.6.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.6.m5.1.1.3.2.cmml" xref="S3.SS1.p2.6.m5.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.6.m5.1.1.3.3.cmml" xref="S3.SS1.p2.6.m5.1.1.3.3"><times id="S3.SS1.p2.6.m5.1.1.3.3.1.cmml" xref="S3.SS1.p2.6.m5.1.1.3.3.1"></times><ci id="S3.SS1.p2.6.m5.1.1.3.3.2.cmml" xref="S3.SS1.p2.6.m5.1.1.3.3.2">𝑟</ci><ci id="S3.SS1.p2.6.m5.1.1.3.3.3.cmml" xref="S3.SS1.p2.6.m5.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m5.1c">A\in\mathbb{R}^{r\times k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m5.1d">italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_r × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, where the <math alttext="r" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m6.1"><semantics id="S3.SS1.p2.7.m6.1a"><mi id="S3.SS1.p2.7.m6.1.1" xref="S3.SS1.p2.7.m6.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m6.1b"><ci id="S3.SS1.p2.7.m6.1.1.cmml" xref="S3.SS1.p2.7.m6.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m6.1c">r</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m6.1d">italic_r</annotation></semantics></math> represents the low intrinsic dimension and <math alttext="r\ll\min(d,k)" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m7.3"><semantics id="S3.SS1.p2.8.m7.3a"><mrow id="S3.SS1.p2.8.m7.3.4" xref="S3.SS1.p2.8.m7.3.4.cmml"><mi id="S3.SS1.p2.8.m7.3.4.2" xref="S3.SS1.p2.8.m7.3.4.2.cmml">r</mi><mo id="S3.SS1.p2.8.m7.3.4.1" xref="S3.SS1.p2.8.m7.3.4.1.cmml">≪</mo><mrow id="S3.SS1.p2.8.m7.3.4.3.2" xref="S3.SS1.p2.8.m7.3.4.3.1.cmml"><mi id="S3.SS1.p2.8.m7.1.1" xref="S3.SS1.p2.8.m7.1.1.cmml">min</mi><mo id="S3.SS1.p2.8.m7.3.4.3.2a" xref="S3.SS1.p2.8.m7.3.4.3.1.cmml">⁡</mo><mrow id="S3.SS1.p2.8.m7.3.4.3.2.1" xref="S3.SS1.p2.8.m7.3.4.3.1.cmml"><mo id="S3.SS1.p2.8.m7.3.4.3.2.1.1" stretchy="false" xref="S3.SS1.p2.8.m7.3.4.3.1.cmml">(</mo><mi id="S3.SS1.p2.8.m7.2.2" xref="S3.SS1.p2.8.m7.2.2.cmml">d</mi><mo id="S3.SS1.p2.8.m7.3.4.3.2.1.2" xref="S3.SS1.p2.8.m7.3.4.3.1.cmml">,</mo><mi id="S3.SS1.p2.8.m7.3.3" xref="S3.SS1.p2.8.m7.3.3.cmml">k</mi><mo id="S3.SS1.p2.8.m7.3.4.3.2.1.3" stretchy="false" xref="S3.SS1.p2.8.m7.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m7.3b"><apply id="S3.SS1.p2.8.m7.3.4.cmml" xref="S3.SS1.p2.8.m7.3.4"><csymbol cd="latexml" id="S3.SS1.p2.8.m7.3.4.1.cmml" xref="S3.SS1.p2.8.m7.3.4.1">much-less-than</csymbol><ci id="S3.SS1.p2.8.m7.3.4.2.cmml" xref="S3.SS1.p2.8.m7.3.4.2">𝑟</ci><apply id="S3.SS1.p2.8.m7.3.4.3.1.cmml" xref="S3.SS1.p2.8.m7.3.4.3.2"><min id="S3.SS1.p2.8.m7.1.1.cmml" xref="S3.SS1.p2.8.m7.1.1"></min><ci id="S3.SS1.p2.8.m7.2.2.cmml" xref="S3.SS1.p2.8.m7.2.2">𝑑</ci><ci id="S3.SS1.p2.8.m7.3.3.cmml" xref="S3.SS1.p2.8.m7.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m7.3c">r\ll\min(d,k)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m7.3d">italic_r ≪ roman_min ( italic_d , italic_k )</annotation></semantics></math>.
During the training, we only optimize the weights of these low-rank matrices <math alttext="B" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m8.1"><semantics id="S3.SS1.p2.9.m8.1a"><mi id="S3.SS1.p2.9.m8.1.1" xref="S3.SS1.p2.9.m8.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m8.1b"><ci id="S3.SS1.p2.9.m8.1.1.cmml" xref="S3.SS1.p2.9.m8.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m8.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m8.1d">italic_B</annotation></semantics></math> and <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m9.1"><semantics id="S3.SS1.p2.10.m9.1a"><mi id="S3.SS1.p2.10.m9.1.1" xref="S3.SS1.p2.10.m9.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m9.1b"><ci id="S3.SS1.p2.10.m9.1.1.cmml" xref="S3.SS1.p2.10.m9.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m9.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m9.1d">italic_A</annotation></semantics></math> which have significant fewer parameters, while freezing the pre-trained weight. This approach allows for fast and efficient fine-tuning.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Prompt Strategy</span>. The prompt plays a crucial role in VLMs as a good prompt strategy can improve the performance of the model without extra effort. A typical prompt for CLIP is formulated as <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p3.1.2">A photo of {object}</span>. For classification, the <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p3.1.3">{object}</span> is replaced with different class labels, and the CLIP will select the class based on the cosine similarity between the image embedding and text embedding.
In our scenario, the <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p3.1.4">{object}</span> is defined as the name of the robot components (e.g. robot base, robot end-effector). However, since the text of different robot parts is semantically similar, we have found that it is more effective to separate the queries for different components. Specifically, for each robot part, we input a pair of prompts (<span class="ltx_text ltx_font_typewriter" id="S3.SS1.p3.1.5">A photo of robot {component}</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p3.1.6">A photo without robot {component}</span>). CLIP conducts binary classification for each component individually, thereby eliminating ambiguity when choosing from semantically close text embeddings.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">To investigate the few-shot capability of VLM for robot part detection, we conducted a comparison of different few-shot learning approaches. These included parameter-efficient fine-tuning method (LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib40" title="">40</a>]</cite>), prompt learning method (CoOp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib32" title="">32</a>]</cite>), and traditional image classification using ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib29" title="">29</a>]</cite> on the dataset we scraped from DROID. The quantitative results can be found in Section IV-B.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Camera-to-Robot Pose Estimation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">CtRNet Overview</span>. The Camera-to-Robot Pose Estimation Network (CtRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib7" title="">7</a>]</cite>) is the pioneer method for end-to-end robot pose estimation. The CtRNet includes a segmentation network, a keypoint detection network, and a differentiable Perspective-n-Point solver (BPnP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib41" title="">41</a>]</cite>). During the inference time, given the image frames and corresponding joint angles, the keypoint detector predicts the 2D keypoint coordinates on the robot manipulator, and the PnP solver estimates the robot pose with 2D-3D keypoint associations. The CtRNet is pre-trained on the synthetic dataset with ground-truth labels of segmentation masks and keypoint coordinates and is fine-tuned in the real-world data without labels in a self-training manner. In the self-training phase, a differentiable renderer is utilized to compute the robot mask based on pose estimation, and the masks obtained from the segmentation network are leveraged to provide image-level supervision to optimize the keypoint detector.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.5"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.5.1">Model Training</span>. In this work, we follow the training strategy of the CtRNet with modified keypoint placement. The CtRNet defines the keypoint at the location of each robot joint. To ensure the framework has a sufficient number of keypoint to estimate the pose for each image frame with partial view, we place <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_N</annotation></semantics></math> (<math alttext="N\geq 4" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">N</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">≥</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><geq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></geq><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑁</ci><cn id="S3.SS2.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p2.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">N\geq 4</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_N ≥ 4</annotation></semantics></math>) number of keypoints for each robot link.
During the pre-training phase, CtRNet uses coordinate regression for training the keypoint detector, which minimizes the <math alttext="L_{2}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">L</mi><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝐿</ci><cn id="S3.SS2.p2.3.m3.1.1.3.cmml" type="integer" xref="S3.SS2.p2.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">L_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> distance between the ground-truth and predicted keypoint coordinates.
Inspired by DARK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib14" title="">14</a>]</cite>, we adapt heatmap regression for training the keypoint detector. The heatmap provides spatial support around the ground-truth location, taking into account contextual clues and the inherent ambiguity of the target position. Importantly, this approach can effectively reduce the risk of overfitting in the model during training, similar to the concept of class label smoothing regularization.
The heatmap regression minimizes the per-pixel <math alttext="L_{2}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">L</mi><mn id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝐿</ci><cn id="S3.SS2.p2.4.m4.1.1.3.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">L_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> distance between the predicted and ground-truth heatmap. In this work, we assume the heatmap should follow the Gaussian distribution. To supervise the heatmap prediction, the ground truth keypoint coordinates are encoded into the Gaussian heatmap, <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">caligraphic_D</annotation></semantics></math>, as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\small\mathcal{D}(u,v)=\frac{1}{2\pi\sigma^{2}}\exp\left(-\frac{(u-u^{*})^{2}+%
(v-v^{*})^{2}}{2\sigma^{2}}\right)" class="ltx_Math" display="block" id="S3.E2.m1.6"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml"><mrow id="S3.E2.m1.6.6.3" xref="S3.E2.m1.6.6.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.6.6.3.2" mathsize="90%" xref="S3.E2.m1.6.6.3.2.cmml">𝒟</mi><mo id="S3.E2.m1.6.6.3.1" xref="S3.E2.m1.6.6.3.1.cmml">⁢</mo><mrow id="S3.E2.m1.6.6.3.3.2" xref="S3.E2.m1.6.6.3.3.1.cmml"><mo id="S3.E2.m1.6.6.3.3.2.1" maxsize="90%" minsize="90%" xref="S3.E2.m1.6.6.3.3.1.cmml">(</mo><mi id="S3.E2.m1.3.3" mathsize="90%" xref="S3.E2.m1.3.3.cmml">u</mi><mo id="S3.E2.m1.6.6.3.3.2.2" mathsize="90%" xref="S3.E2.m1.6.6.3.3.1.cmml">,</mo><mi id="S3.E2.m1.4.4" mathsize="90%" xref="S3.E2.m1.4.4.cmml">v</mi><mo id="S3.E2.m1.6.6.3.3.2.3" maxsize="90%" minsize="90%" xref="S3.E2.m1.6.6.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.6.6.2" mathsize="90%" xref="S3.E2.m1.6.6.2.cmml">=</mo><mrow id="S3.E2.m1.6.6.1" xref="S3.E2.m1.6.6.1.cmml"><mfrac id="S3.E2.m1.6.6.1.3" xref="S3.E2.m1.6.6.1.3.cmml"><mn id="S3.E2.m1.6.6.1.3.2" mathsize="90%" xref="S3.E2.m1.6.6.1.3.2.cmml">1</mn><mrow id="S3.E2.m1.6.6.1.3.3" xref="S3.E2.m1.6.6.1.3.3.cmml"><mn id="S3.E2.m1.6.6.1.3.3.2" mathsize="90%" xref="S3.E2.m1.6.6.1.3.3.2.cmml">2</mn><mo id="S3.E2.m1.6.6.1.3.3.1" xref="S3.E2.m1.6.6.1.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.6.6.1.3.3.3" mathsize="90%" xref="S3.E2.m1.6.6.1.3.3.3.cmml">π</mi><mo id="S3.E2.m1.6.6.1.3.3.1a" xref="S3.E2.m1.6.6.1.3.3.1.cmml">⁢</mo><msup id="S3.E2.m1.6.6.1.3.3.4" xref="S3.E2.m1.6.6.1.3.3.4.cmml"><mi id="S3.E2.m1.6.6.1.3.3.4.2" mathsize="90%" xref="S3.E2.m1.6.6.1.3.3.4.2.cmml">σ</mi><mn id="S3.E2.m1.6.6.1.3.3.4.3" mathsize="90%" xref="S3.E2.m1.6.6.1.3.3.4.3.cmml">2</mn></msup></mrow></mfrac><mo id="S3.E2.m1.6.6.1.2" lspace="0.167em" xref="S3.E2.m1.6.6.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.6.6.1.1.1" xref="S3.E2.m1.6.6.1.1.2.cmml"><mi id="S3.E2.m1.5.5" mathsize="90%" xref="S3.E2.m1.5.5.cmml">exp</mi><mo id="S3.E2.m1.6.6.1.1.1a" xref="S3.E2.m1.6.6.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.6.6.1.1.1.1" xref="S3.E2.m1.6.6.1.1.2.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.2.cmml">(</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1a" mathsize="90%" xref="S3.E2.m1.6.6.1.1.1.1.1.cmml">−</mo><mfrac id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><msup id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2" mathsize="90%" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">u</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1" mathsize="90%" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msup id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.2" mathsize="90%" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml">u</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.3.3" mathsize="90%" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml">∗</mo></msup></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E2.m1.1.1.1.1.3" mathsize="90%" xref="S3.E2.m1.1.1.1.1.3.cmml">2</mn></msup><mo id="S3.E2.m1.2.2.2.3" mathsize="90%" xref="S3.E2.m1.2.2.2.3.cmml">+</mo><msup id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml"><mrow id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml"><mo id="S3.E2.m1.2.2.2.2.1.1.2" maxsize="90%" minsize="90%" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.1.1.2" mathsize="90%" xref="S3.E2.m1.2.2.2.2.1.1.1.2.cmml">v</mi><mo id="S3.E2.m1.2.2.2.2.1.1.1.1" mathsize="90%" xref="S3.E2.m1.2.2.2.2.1.1.1.1.cmml">−</mo><msup id="S3.E2.m1.2.2.2.2.1.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.2.2.1.1.1.3.2" mathsize="90%" xref="S3.E2.m1.2.2.2.2.1.1.1.3.2.cmml">v</mi><mo id="S3.E2.m1.2.2.2.2.1.1.1.3.3" mathsize="90%" xref="S3.E2.m1.2.2.2.2.1.1.1.3.3.cmml">∗</mo></msup></mrow><mo id="S3.E2.m1.2.2.2.2.1.1.3" maxsize="90%" minsize="90%" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow><mn id="S3.E2.m1.2.2.2.2.3" mathsize="90%" xref="S3.E2.m1.2.2.2.2.3.cmml">2</mn></msup></mrow><mrow id="S3.E2.m1.2.2.4" xref="S3.E2.m1.2.2.4.cmml"><mn id="S3.E2.m1.2.2.4.2" mathsize="90%" xref="S3.E2.m1.2.2.4.2.cmml">2</mn><mo id="S3.E2.m1.2.2.4.1" xref="S3.E2.m1.2.2.4.1.cmml">⁢</mo><msup id="S3.E2.m1.2.2.4.3" xref="S3.E2.m1.2.2.4.3.cmml"><mi id="S3.E2.m1.2.2.4.3.2" mathsize="90%" xref="S3.E2.m1.2.2.4.3.2.cmml">σ</mi><mn id="S3.E2.m1.2.2.4.3.3" mathsize="90%" xref="S3.E2.m1.2.2.4.3.3.cmml">2</mn></msup></mrow></mfrac></mrow><mo id="S3.E2.m1.6.6.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6"><eq id="S3.E2.m1.6.6.2.cmml" xref="S3.E2.m1.6.6.2"></eq><apply id="S3.E2.m1.6.6.3.cmml" xref="S3.E2.m1.6.6.3"><times id="S3.E2.m1.6.6.3.1.cmml" xref="S3.E2.m1.6.6.3.1"></times><ci id="S3.E2.m1.6.6.3.2.cmml" xref="S3.E2.m1.6.6.3.2">𝒟</ci><interval closure="open" id="S3.E2.m1.6.6.3.3.1.cmml" xref="S3.E2.m1.6.6.3.3.2"><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑢</ci><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝑣</ci></interval></apply><apply id="S3.E2.m1.6.6.1.cmml" xref="S3.E2.m1.6.6.1"><times id="S3.E2.m1.6.6.1.2.cmml" xref="S3.E2.m1.6.6.1.2"></times><apply id="S3.E2.m1.6.6.1.3.cmml" xref="S3.E2.m1.6.6.1.3"><divide id="S3.E2.m1.6.6.1.3.1.cmml" xref="S3.E2.m1.6.6.1.3"></divide><cn id="S3.E2.m1.6.6.1.3.2.cmml" type="integer" xref="S3.E2.m1.6.6.1.3.2">1</cn><apply id="S3.E2.m1.6.6.1.3.3.cmml" xref="S3.E2.m1.6.6.1.3.3"><times id="S3.E2.m1.6.6.1.3.3.1.cmml" xref="S3.E2.m1.6.6.1.3.3.1"></times><cn id="S3.E2.m1.6.6.1.3.3.2.cmml" type="integer" xref="S3.E2.m1.6.6.1.3.3.2">2</cn><ci id="S3.E2.m1.6.6.1.3.3.3.cmml" xref="S3.E2.m1.6.6.1.3.3.3">𝜋</ci><apply id="S3.E2.m1.6.6.1.3.3.4.cmml" xref="S3.E2.m1.6.6.1.3.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.3.3.4.1.cmml" xref="S3.E2.m1.6.6.1.3.3.4">superscript</csymbol><ci id="S3.E2.m1.6.6.1.3.3.4.2.cmml" xref="S3.E2.m1.6.6.1.3.3.4.2">𝜎</ci><cn id="S3.E2.m1.6.6.1.3.3.4.3.cmml" type="integer" xref="S3.E2.m1.6.6.1.3.3.4.3">2</cn></apply></apply></apply><apply id="S3.E2.m1.6.6.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1"><exp id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5"></exp><apply id="S3.E2.m1.6.6.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1"><minus id="S3.E2.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1"></minus><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><divide id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2"></divide><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><plus id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"></plus><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">𝑢</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2">𝑢</ci><times id="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3"></times></apply></apply><cn id="S3.E2.m1.1.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.3">2</cn></apply><apply id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">superscript</csymbol><apply id="S3.E2.m1.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1"><minus id="S3.E2.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1"></minus><ci id="S3.E2.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.2">𝑣</ci><apply id="S3.E2.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.2.2.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3.2">𝑣</ci><times id="S3.E2.m1.2.2.2.2.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3.3"></times></apply></apply><cn id="S3.E2.m1.2.2.2.2.3.cmml" type="integer" xref="S3.E2.m1.2.2.2.2.3">2</cn></apply></apply><apply id="S3.E2.m1.2.2.4.cmml" xref="S3.E2.m1.2.2.4"><times id="S3.E2.m1.2.2.4.1.cmml" xref="S3.E2.m1.2.2.4.1"></times><cn id="S3.E2.m1.2.2.4.2.cmml" type="integer" xref="S3.E2.m1.2.2.4.2">2</cn><apply id="S3.E2.m1.2.2.4.3.cmml" xref="S3.E2.m1.2.2.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.4.3.1.cmml" xref="S3.E2.m1.2.2.4.3">superscript</csymbol><ci id="S3.E2.m1.2.2.4.3.2.cmml" xref="S3.E2.m1.2.2.4.3.2">𝜎</ci><cn id="S3.E2.m1.2.2.4.3.3.cmml" type="integer" xref="S3.E2.m1.2.2.4.3.3">2</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">\small\mathcal{D}(u,v)=\frac{1}{2\pi\sigma^{2}}\exp\left(-\frac{(u-u^{*})^{2}+%
(v-v^{*})^{2}}{2\sigma^{2}}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.6d">caligraphic_D ( italic_u , italic_v ) = divide start_ARG 1 end_ARG start_ARG 2 italic_π italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG roman_exp ( - divide start_ARG ( italic_u - italic_u start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( italic_v - italic_v start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p2.8">where <math alttext="u,v" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m1.2"><semantics id="S3.SS2.p2.6.m1.2a"><mrow id="S3.SS2.p2.6.m1.2.3.2" xref="S3.SS2.p2.6.m1.2.3.1.cmml"><mi id="S3.SS2.p2.6.m1.1.1" xref="S3.SS2.p2.6.m1.1.1.cmml">u</mi><mo id="S3.SS2.p2.6.m1.2.3.2.1" xref="S3.SS2.p2.6.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.p2.6.m1.2.2" xref="S3.SS2.p2.6.m1.2.2.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m1.2b"><list id="S3.SS2.p2.6.m1.2.3.1.cmml" xref="S3.SS2.p2.6.m1.2.3.2"><ci id="S3.SS2.p2.6.m1.1.1.cmml" xref="S3.SS2.p2.6.m1.1.1">𝑢</ci><ci id="S3.SS2.p2.6.m1.2.2.cmml" xref="S3.SS2.p2.6.m1.2.2">𝑣</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m1.2c">u,v</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m1.2d">italic_u , italic_v</annotation></semantics></math> are pixel coordinates in the heatmap, <math alttext="u^{*},v^{*}" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m2.2"><semantics id="S3.SS2.p2.7.m2.2a"><mrow id="S3.SS2.p2.7.m2.2.2.2" xref="S3.SS2.p2.7.m2.2.2.3.cmml"><msup id="S3.SS2.p2.7.m2.1.1.1.1" xref="S3.SS2.p2.7.m2.1.1.1.1.cmml"><mi id="S3.SS2.p2.7.m2.1.1.1.1.2" xref="S3.SS2.p2.7.m2.1.1.1.1.2.cmml">u</mi><mo id="S3.SS2.p2.7.m2.1.1.1.1.3" xref="S3.SS2.p2.7.m2.1.1.1.1.3.cmml">∗</mo></msup><mo id="S3.SS2.p2.7.m2.2.2.2.3" xref="S3.SS2.p2.7.m2.2.2.3.cmml">,</mo><msup id="S3.SS2.p2.7.m2.2.2.2.2" xref="S3.SS2.p2.7.m2.2.2.2.2.cmml"><mi id="S3.SS2.p2.7.m2.2.2.2.2.2" xref="S3.SS2.p2.7.m2.2.2.2.2.2.cmml">v</mi><mo id="S3.SS2.p2.7.m2.2.2.2.2.3" xref="S3.SS2.p2.7.m2.2.2.2.2.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m2.2b"><list id="S3.SS2.p2.7.m2.2.2.3.cmml" xref="S3.SS2.p2.7.m2.2.2.2"><apply id="S3.SS2.p2.7.m2.1.1.1.1.cmml" xref="S3.SS2.p2.7.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m2.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m2.1.1.1.1">superscript</csymbol><ci id="S3.SS2.p2.7.m2.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m2.1.1.1.1.2">𝑢</ci><times id="S3.SS2.p2.7.m2.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m2.1.1.1.1.3"></times></apply><apply id="S3.SS2.p2.7.m2.2.2.2.2.cmml" xref="S3.SS2.p2.7.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m2.2.2.2.2.1.cmml" xref="S3.SS2.p2.7.m2.2.2.2.2">superscript</csymbol><ci id="S3.SS2.p2.7.m2.2.2.2.2.2.cmml" xref="S3.SS2.p2.7.m2.2.2.2.2.2">𝑣</ci><times id="S3.SS2.p2.7.m2.2.2.2.2.3.cmml" xref="S3.SS2.p2.7.m2.2.2.2.2.3"></times></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m2.2c">u^{*},v^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m2.2d">italic_u start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_v start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> are the ground truth keypoint coordinates, and <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.SS2.p2.8.m3.1"><semantics id="S3.SS2.p2.8.m3.1a"><mi id="S3.SS2.p2.8.m3.1.1" xref="S3.SS2.p2.8.m3.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m3.1b"><ci id="S3.SS2.p2.8.m3.1.1.cmml" xref="S3.SS2.p2.8.m3.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m3.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.8.m3.1d">italic_σ</annotation></semantics></math> is the predefined spatial variance.
After pre-training on the synthetic dataset, we conduct self-supervised training on the real-world data without labels. The objective of self-training is to optimize the neural network parameters by minimizing the difference between the segmentation robot mask and the robot mask rendered based on the predicted pose. We follow the same self-training strategy as the CtRNet and the details can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.14"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.14.1">Model Inferencing</span>. During the inference phase, we integrate the distribution-aware coordinate decoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib14" title="">14</a>]</cite> to extract 2D coordinates of keypoints from the predicted heatmap. We assume the predicted heatmap follows a 2D Gaussian distribution:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\small\mathcal{G}(\mathbf{x};\boldsymbol{\mu},\Sigma)=\frac{1}{(2\pi)|\Sigma|^%
{1/2}}\exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\Sigma^{-1%
}(\boldsymbol{x}-\boldsymbol{\mu})\right)" class="ltx_Math" display="block" id="S3.E3.m1.7"><semantics id="S3.E3.m1.7a"><mrow id="S3.E3.m1.7.7" xref="S3.E3.m1.7.7.cmml"><mrow id="S3.E3.m1.7.7.3" xref="S3.E3.m1.7.7.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.7.7.3.2" mathsize="90%" xref="S3.E3.m1.7.7.3.2.cmml">𝒢</mi><mo id="S3.E3.m1.7.7.3.1" xref="S3.E3.m1.7.7.3.1.cmml">⁢</mo><mrow id="S3.E3.m1.7.7.3.3.2" xref="S3.E3.m1.7.7.3.3.1.cmml"><mo id="S3.E3.m1.7.7.3.3.2.1" maxsize="90%" minsize="90%" xref="S3.E3.m1.7.7.3.3.1.cmml">(</mo><mi id="S3.E3.m1.3.3" mathsize="90%" xref="S3.E3.m1.3.3.cmml">𝐱</mi><mo id="S3.E3.m1.7.7.3.3.2.2" mathsize="90%" xref="S3.E3.m1.7.7.3.3.1.cmml">;</mo><mi id="S3.E3.m1.4.4" mathsize="90%" xref="S3.E3.m1.4.4.cmml">𝝁</mi><mo id="S3.E3.m1.7.7.3.3.2.3" mathsize="90%" xref="S3.E3.m1.7.7.3.3.1.cmml">,</mo><mi id="S3.E3.m1.5.5" mathsize="90%" mathvariant="normal" xref="S3.E3.m1.5.5.cmml">Σ</mi><mo id="S3.E3.m1.7.7.3.3.2.4" maxsize="90%" minsize="90%" xref="S3.E3.m1.7.7.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.7.7.2" mathsize="90%" xref="S3.E3.m1.7.7.2.cmml">=</mo><mrow id="S3.E3.m1.7.7.1" xref="S3.E3.m1.7.7.1.cmml"><mfrac id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mn id="S3.E3.m1.2.2.4" mathsize="90%" xref="S3.E3.m1.2.2.4.cmml">1</mn><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml"><mo id="S3.E3.m1.2.2.2.2.1.2" maxsize="90%" minsize="90%" xref="S3.E3.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml"><mn id="S3.E3.m1.2.2.2.2.1.1.2" mathsize="90%" xref="S3.E3.m1.2.2.2.2.1.1.2.cmml">2</mn><mo id="S3.E3.m1.2.2.2.2.1.1.1" xref="S3.E3.m1.2.2.2.2.1.1.1.cmml">⁢</mo><mi id="S3.E3.m1.2.2.2.2.1.1.3" mathsize="90%" xref="S3.E3.m1.2.2.2.2.1.1.3.cmml">π</mi></mrow><mo id="S3.E3.m1.2.2.2.2.1.3" maxsize="90%" minsize="90%" xref="S3.E3.m1.2.2.2.2.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">⁢</mo><msup id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.4.cmml"><mrow id="S3.E3.m1.2.2.2.4.2.2" xref="S3.E3.m1.2.2.2.4.2.1.cmml"><mo id="S3.E3.m1.2.2.2.4.2.2.1" maxsize="90%" minsize="90%" xref="S3.E3.m1.2.2.2.4.2.1.1.cmml">|</mo><mi id="S3.E3.m1.1.1.1.1" mathsize="90%" mathvariant="normal" xref="S3.E3.m1.1.1.1.1.cmml">Σ</mi><mo id="S3.E3.m1.2.2.2.4.2.2.2" maxsize="90%" minsize="90%" xref="S3.E3.m1.2.2.2.4.2.1.1.cmml">|</mo></mrow><mrow id="S3.E3.m1.2.2.2.4.3" xref="S3.E3.m1.2.2.2.4.3.cmml"><mn id="S3.E3.m1.2.2.2.4.3.2" mathsize="90%" xref="S3.E3.m1.2.2.2.4.3.2.cmml">1</mn><mo id="S3.E3.m1.2.2.2.4.3.1" maxsize="90%" minsize="90%" stretchy="true" symmetric="true" xref="S3.E3.m1.2.2.2.4.3.1.cmml">/</mo><mn id="S3.E3.m1.2.2.2.4.3.3" mathsize="90%" xref="S3.E3.m1.2.2.2.4.3.3.cmml">2</mn></mrow></msup></mrow></mfrac><mo id="S3.E3.m1.7.7.1.2" lspace="0.167em" xref="S3.E3.m1.7.7.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.7.7.1.1.1" xref="S3.E3.m1.7.7.1.1.2.cmml"><mi id="S3.E3.m1.6.6" mathsize="90%" xref="S3.E3.m1.6.6.cmml">exp</mi><mo id="S3.E3.m1.7.7.1.1.1a" xref="S3.E3.m1.7.7.1.1.2.cmml">⁡</mo><mrow id="S3.E3.m1.7.7.1.1.1.1" xref="S3.E3.m1.7.7.1.1.2.cmml"><mo id="S3.E3.m1.7.7.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.2.cmml">(</mo><mrow id="S3.E3.m1.7.7.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.cmml"><mo id="S3.E3.m1.7.7.1.1.1.1.1a" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.cmml">−</mo><mrow id="S3.E3.m1.7.7.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.1.2.cmml"><mfrac id="S3.E3.m1.7.7.1.1.1.1.1.2.4" xref="S3.E3.m1.7.7.1.1.1.1.1.2.4.cmml"><mn id="S3.E3.m1.7.7.1.1.1.1.1.2.4.2" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.2.4.2.cmml">1</mn><mn id="S3.E3.m1.7.7.1.1.1.1.1.2.4.3" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.2.4.3.cmml">2</mn></mfrac><mo id="S3.E3.m1.7.7.1.1.1.1.1.2.3" xref="S3.E3.m1.7.7.1.1.1.1.1.2.3.cmml">⁢</mo><msup id="S3.E3.m1.7.7.1.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.2" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.2.cmml">𝒙</mi><mo id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.1" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.3" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.3.cmml">𝝁</mi></mrow><mo id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.7.7.1.1.1.1.1.1.1.3" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.3.cmml">⊤</mo></msup><mo id="S3.E3.m1.7.7.1.1.1.1.1.2.3a" xref="S3.E3.m1.7.7.1.1.1.1.1.2.3.cmml">⁢</mo><msup id="S3.E3.m1.7.7.1.1.1.1.1.2.5" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.1.2.5.2" mathsize="90%" mathvariant="normal" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5.2.cmml">Σ</mi><mrow id="S3.E3.m1.7.7.1.1.1.1.1.2.5.3" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5.3.cmml"><mo id="S3.E3.m1.7.7.1.1.1.1.1.2.5.3a" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5.3.cmml">−</mo><mn id="S3.E3.m1.7.7.1.1.1.1.1.2.5.3.2" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5.3.2.cmml">1</mn></mrow></msup><mo id="S3.E3.m1.7.7.1.1.1.1.1.2.3b" xref="S3.E3.m1.7.7.1.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.cmml"><mo id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.2" maxsize="90%" minsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.cmml">(</mo><mrow id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.2" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.2.cmml">𝒙</mi><mo id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.1" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.1.cmml">−</mo><mi id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.3" mathsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.3.cmml">𝝁</mi></mrow><mo id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.3" maxsize="90%" minsize="90%" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.7.7.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.7b"><apply id="S3.E3.m1.7.7.cmml" xref="S3.E3.m1.7.7"><eq id="S3.E3.m1.7.7.2.cmml" xref="S3.E3.m1.7.7.2"></eq><apply id="S3.E3.m1.7.7.3.cmml" xref="S3.E3.m1.7.7.3"><times id="S3.E3.m1.7.7.3.1.cmml" xref="S3.E3.m1.7.7.3.1"></times><ci id="S3.E3.m1.7.7.3.2.cmml" xref="S3.E3.m1.7.7.3.2">𝒢</ci><list id="S3.E3.m1.7.7.3.3.1.cmml" xref="S3.E3.m1.7.7.3.3.2"><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">𝐱</ci><ci id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4">𝝁</ci><ci id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5">Σ</ci></list></apply><apply id="S3.E3.m1.7.7.1.cmml" xref="S3.E3.m1.7.7.1"><times id="S3.E3.m1.7.7.1.2.cmml" xref="S3.E3.m1.7.7.1.2"></times><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><divide id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2"></divide><cn id="S3.E3.m1.2.2.4.cmml" type="integer" xref="S3.E3.m1.2.2.4">1</cn><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><times id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3"></times><apply id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1"><times id="S3.E3.m1.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1"></times><cn id="S3.E3.m1.2.2.2.2.1.1.2.cmml" type="integer" xref="S3.E3.m1.2.2.2.2.1.1.2">2</cn><ci id="S3.E3.m1.2.2.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.3">𝜋</ci></apply><apply id="S3.E3.m1.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.4.1.cmml" xref="S3.E3.m1.2.2.2.4">superscript</csymbol><apply id="S3.E3.m1.2.2.2.4.2.1.cmml" xref="S3.E3.m1.2.2.2.4.2.2"><abs id="S3.E3.m1.2.2.2.4.2.1.1.cmml" xref="S3.E3.m1.2.2.2.4.2.2.1"></abs><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">Σ</ci></apply><apply id="S3.E3.m1.2.2.2.4.3.cmml" xref="S3.E3.m1.2.2.2.4.3"><divide id="S3.E3.m1.2.2.2.4.3.1.cmml" xref="S3.E3.m1.2.2.2.4.3.1"></divide><cn id="S3.E3.m1.2.2.2.4.3.2.cmml" type="integer" xref="S3.E3.m1.2.2.2.4.3.2">1</cn><cn id="S3.E3.m1.2.2.2.4.3.3.cmml" type="integer" xref="S3.E3.m1.2.2.2.4.3.3">2</cn></apply></apply></apply></apply><apply id="S3.E3.m1.7.7.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1"><exp id="S3.E3.m1.6.6.cmml" xref="S3.E3.m1.6.6"></exp><apply id="S3.E3.m1.7.7.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1"><minus id="S3.E3.m1.7.7.1.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1"></minus><apply id="S3.E3.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2"><times id="S3.E3.m1.7.7.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.3"></times><apply id="S3.E3.m1.7.7.1.1.1.1.1.2.4.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.4"><divide id="S3.E3.m1.7.7.1.1.1.1.1.2.4.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.4"></divide><cn id="S3.E3.m1.7.7.1.1.1.1.1.2.4.2.cmml" type="integer" xref="S3.E3.m1.7.7.1.1.1.1.1.2.4.2">1</cn><cn id="S3.E3.m1.7.7.1.1.1.1.1.2.4.3.cmml" type="integer" xref="S3.E3.m1.7.7.1.1.1.1.1.2.4.3">2</cn></apply><apply id="S3.E3.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.2">𝒙</ci><ci id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.3">𝝁</ci></apply><csymbol cd="latexml" id="S3.E3.m1.7.7.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.3">top</csymbol></apply><apply id="S3.E3.m1.7.7.1.1.1.1.1.2.5.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.2.5.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5">superscript</csymbol><ci id="S3.E3.m1.7.7.1.1.1.1.1.2.5.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5.2">Σ</ci><apply id="S3.E3.m1.7.7.1.1.1.1.1.2.5.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5.3"><minus id="S3.E3.m1.7.7.1.1.1.1.1.2.5.3.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5.3"></minus><cn id="S3.E3.m1.7.7.1.1.1.1.1.2.5.3.2.cmml" type="integer" xref="S3.E3.m1.7.7.1.1.1.1.1.2.5.3.2">1</cn></apply></apply><apply id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1"><minus id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.1"></minus><ci id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.2">𝒙</ci><ci id="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2.2.1.1.3">𝝁</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.7c">\small\mathcal{G}(\mathbf{x};\boldsymbol{\mu},\Sigma)=\frac{1}{(2\pi)|\Sigma|^%
{1/2}}\exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\Sigma^{-1%
}(\boldsymbol{x}-\boldsymbol{\mu})\right)</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.7d">caligraphic_G ( bold_x ; bold_italic_μ , roman_Σ ) = divide start_ARG 1 end_ARG start_ARG ( 2 italic_π ) | roman_Σ | start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_ARG roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( bold_italic_x - bold_italic_μ ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT roman_Σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_x - bold_italic_μ ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p3.4">where <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">bold_x</annotation></semantics></math> is the pixel location, <math alttext="\boldsymbol{\mu}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">𝝁</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">𝝁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\boldsymbol{\mu}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">bold_italic_μ</annotation></semantics></math> is the Gaussian mean and <math alttext="\Sigma" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" mathvariant="normal" xref="S3.SS2.p3.3.m3.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\Sigma</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">roman_Σ</annotation></semantics></math> is the distribution’s covariance. In order to estimate <math alttext="\boldsymbol{\mu}" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">𝝁</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">𝝁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\boldsymbol{\mu}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">bold_italic_μ</annotation></semantics></math>, we follow the maximum likelihood estimation principle and transform the distribution function to the Log-likelihood function:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{P}(\boldsymbol{x};\boldsymbol{\mu},\Sigma)=\ln(\mathcal{G})\\
=-\ln(2\pi)-\frac{1}{2}\ln(|\Sigma|)-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{%
\mu})^{\top}\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})" class="ltx_Math" display="block" id="S3.E4.m1.51"><semantics id="S3.E4.m1.51a"><mtable displaystyle="true" id="S3.E4.m1.51.51.8" rowspacing="0pt" xref="S3.E4.m1.47.47.4.cmml"><mtr id="S3.E4.m1.51.51.8a" xref="S3.E4.m1.47.47.4.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.51.51.8b" xref="S3.E4.m1.47.47.4.cmml"><mrow id="S3.E4.m1.13.13.13.13.13" xref="S3.E4.m1.47.47.4.cmml"><mrow id="S3.E4.m1.13.13.13.13.13.14" xref="S3.E4.m1.47.47.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml">𝒫</mi><mo id="S3.E4.m1.13.13.13.13.13.14.1" xref="S3.E4.m1.47.47.4a.cmml">⁢</mo><mrow id="S3.E4.m1.13.13.13.13.13.14.2" xref="S3.E4.m1.47.47.4.cmml"><mo id="S3.E4.m1.2.2.2.2.2.2" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">(</mo><mi id="S3.E4.m1.3.3.3.3.3.3" xref="S3.E4.m1.3.3.3.3.3.3.cmml">𝒙</mi><mo id="S3.E4.m1.4.4.4.4.4.4" xref="S3.E4.m1.47.47.4a.cmml">;</mo><mi id="S3.E4.m1.5.5.5.5.5.5" xref="S3.E4.m1.5.5.5.5.5.5.cmml">𝝁</mi><mo id="S3.E4.m1.6.6.6.6.6.6" xref="S3.E4.m1.47.47.4a.cmml">,</mo><mi id="S3.E4.m1.7.7.7.7.7.7" mathvariant="normal" xref="S3.E4.m1.7.7.7.7.7.7.cmml">Σ</mi><mo id="S3.E4.m1.8.8.8.8.8.8" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.9.9.9.9.9.9" xref="S3.E4.m1.9.9.9.9.9.9.cmml">=</mo><mrow id="S3.E4.m1.13.13.13.13.13.15" xref="S3.E4.m1.47.47.4.cmml"><mi id="S3.E4.m1.10.10.10.10.10.10" xref="S3.E4.m1.10.10.10.10.10.10.cmml">ln</mi><mo id="S3.E4.m1.13.13.13.13.13.15a" xref="S3.E4.m1.47.47.4a.cmml">⁡</mo><mrow id="S3.E4.m1.13.13.13.13.13.15.1" xref="S3.E4.m1.47.47.4.cmml"><mo id="S3.E4.m1.11.11.11.11.11.11" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.12.12.12.12.12.12" xref="S3.E4.m1.12.12.12.12.12.12.cmml">𝒢</mi><mo id="S3.E4.m1.13.13.13.13.13.13" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E4.m1.51.51.8c" xref="S3.E4.m1.47.47.4.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E4.m1.51.51.8d" xref="S3.E4.m1.47.47.4.cmml"><mrow id="S3.E4.m1.51.51.8.47.34.34" xref="S3.E4.m1.47.47.4.cmml"><mi id="S3.E4.m1.51.51.8.47.34.34.35" xref="S3.E4.m1.47.47.4a.cmml"></mi><mo id="S3.E4.m1.14.14.14.1.1.1" xref="S3.E4.m1.14.14.14.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.51.51.8.47.34.34.34" xref="S3.E4.m1.47.47.4.cmml"><mrow id="S3.E4.m1.48.48.5.44.31.31.31.1" xref="S3.E4.m1.47.47.4.cmml"><mo id="S3.E4.m1.48.48.5.44.31.31.31.1a" rspace="0.167em" xref="S3.E4.m1.47.47.4a.cmml">−</mo><mrow id="S3.E4.m1.48.48.5.44.31.31.31.1.1.1" xref="S3.E4.m1.47.47.4.cmml"><mi id="S3.E4.m1.16.16.16.3.3.3" xref="S3.E4.m1.16.16.16.3.3.3.cmml">ln</mi><mo id="S3.E4.m1.48.48.5.44.31.31.31.1.1.1a" xref="S3.E4.m1.47.47.4a.cmml">⁡</mo><mrow id="S3.E4.m1.48.48.5.44.31.31.31.1.1.1.1" xref="S3.E4.m1.47.47.4.cmml"><mo id="S3.E4.m1.17.17.17.4.4.4" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">(</mo><mrow id="S3.E4.m1.48.48.5.44.31.31.31.1.1.1.1.1" xref="S3.E4.m1.47.47.4.cmml"><mn id="S3.E4.m1.18.18.18.5.5.5" xref="S3.E4.m1.18.18.18.5.5.5.cmml">2</mn><mo id="S3.E4.m1.48.48.5.44.31.31.31.1.1.1.1.1.1" xref="S3.E4.m1.47.47.4a.cmml">⁢</mo><mi id="S3.E4.m1.19.19.19.6.6.6" xref="S3.E4.m1.19.19.19.6.6.6.cmml">π</mi></mrow><mo id="S3.E4.m1.20.20.20.7.7.7" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.21.21.21.8.8.8" xref="S3.E4.m1.21.21.21.8.8.8.cmml">−</mo><mrow id="S3.E4.m1.49.49.6.45.32.32.32.2" xref="S3.E4.m1.47.47.4.cmml"><mfrac id="S3.E4.m1.22.22.22.9.9.9" xref="S3.E4.m1.22.22.22.9.9.9.cmml"><mn id="S3.E4.m1.22.22.22.9.9.9.2" xref="S3.E4.m1.22.22.22.9.9.9.2.cmml">1</mn><mn id="S3.E4.m1.22.22.22.9.9.9.3" xref="S3.E4.m1.22.22.22.9.9.9.3.cmml">2</mn></mfrac><mo id="S3.E4.m1.49.49.6.45.32.32.32.2.2" lspace="0.167em" xref="S3.E4.m1.47.47.4a.cmml">⁢</mo><mrow id="S3.E4.m1.49.49.6.45.32.32.32.2.1.1" xref="S3.E4.m1.47.47.4.cmml"><mi id="S3.E4.m1.23.23.23.10.10.10" xref="S3.E4.m1.23.23.23.10.10.10.cmml">ln</mi><mo id="S3.E4.m1.49.49.6.45.32.32.32.2.1.1a" xref="S3.E4.m1.47.47.4a.cmml">⁡</mo><mrow id="S3.E4.m1.49.49.6.45.32.32.32.2.1.1.1" xref="S3.E4.m1.47.47.4.cmml"><mo id="S3.E4.m1.24.24.24.11.11.11" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">(</mo><mrow id="S3.E4.m1.49.49.6.45.32.32.32.2.1.1.1.1" xref="S3.E4.m1.47.47.4.cmml"><mo id="S3.E4.m1.25.25.25.12.12.12" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">|</mo><mi id="S3.E4.m1.26.26.26.13.13.13" mathvariant="normal" xref="S3.E4.m1.26.26.26.13.13.13.cmml">Σ</mi><mo id="S3.E4.m1.27.27.27.14.14.14" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">|</mo></mrow><mo id="S3.E4.m1.28.28.28.15.15.15" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.21.21.21.8.8.8a" xref="S3.E4.m1.21.21.21.8.8.8.cmml">−</mo><mrow id="S3.E4.m1.51.51.8.47.34.34.34.4" xref="S3.E4.m1.47.47.4.cmml"><mfrac id="S3.E4.m1.30.30.30.17.17.17" xref="S3.E4.m1.30.30.30.17.17.17.cmml"><mn id="S3.E4.m1.30.30.30.17.17.17.2" xref="S3.E4.m1.30.30.30.17.17.17.2.cmml">1</mn><mn id="S3.E4.m1.30.30.30.17.17.17.3" xref="S3.E4.m1.30.30.30.17.17.17.3.cmml">2</mn></mfrac><mo id="S3.E4.m1.51.51.8.47.34.34.34.4.3" xref="S3.E4.m1.47.47.4a.cmml">⁢</mo><msup id="S3.E4.m1.50.50.7.46.33.33.33.3.1" xref="S3.E4.m1.47.47.4.cmml"><mrow id="S3.E4.m1.50.50.7.46.33.33.33.3.1.1.1" xref="S3.E4.m1.47.47.4.cmml"><mo id="S3.E4.m1.31.31.31.18.18.18" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">(</mo><mrow id="S3.E4.m1.50.50.7.46.33.33.33.3.1.1.1.1" xref="S3.E4.m1.47.47.4.cmml"><mi id="S3.E4.m1.32.32.32.19.19.19" xref="S3.E4.m1.32.32.32.19.19.19.cmml">𝒙</mi><mo id="S3.E4.m1.33.33.33.20.20.20" xref="S3.E4.m1.33.33.33.20.20.20.cmml">−</mo><mi id="S3.E4.m1.34.34.34.21.21.21" xref="S3.E4.m1.34.34.34.21.21.21.cmml">𝝁</mi></mrow><mo id="S3.E4.m1.35.35.35.22.22.22" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">)</mo></mrow><mo id="S3.E4.m1.36.36.36.23.23.23.1" xref="S3.E4.m1.36.36.36.23.23.23.1.cmml">⊤</mo></msup><mo id="S3.E4.m1.51.51.8.47.34.34.34.4.3a" xref="S3.E4.m1.47.47.4a.cmml">⁢</mo><msup id="S3.E4.m1.51.51.8.47.34.34.34.4.4" xref="S3.E4.m1.47.47.4.cmml"><mi id="S3.E4.m1.37.37.37.24.24.24" mathvariant="normal" xref="S3.E4.m1.37.37.37.24.24.24.cmml">Σ</mi><mrow id="S3.E4.m1.38.38.38.25.25.25.1" xref="S3.E4.m1.38.38.38.25.25.25.1.cmml"><mo id="S3.E4.m1.38.38.38.25.25.25.1a" xref="S3.E4.m1.38.38.38.25.25.25.1.cmml">−</mo><mn id="S3.E4.m1.38.38.38.25.25.25.1.2" xref="S3.E4.m1.38.38.38.25.25.25.1.2.cmml">1</mn></mrow></msup><mo id="S3.E4.m1.51.51.8.47.34.34.34.4.3b" xref="S3.E4.m1.47.47.4a.cmml">⁢</mo><mrow id="S3.E4.m1.51.51.8.47.34.34.34.4.2.1" xref="S3.E4.m1.47.47.4.cmml"><mo id="S3.E4.m1.39.39.39.26.26.26" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">(</mo><mrow id="S3.E4.m1.51.51.8.47.34.34.34.4.2.1.1" xref="S3.E4.m1.47.47.4.cmml"><mi id="S3.E4.m1.40.40.40.27.27.27" xref="S3.E4.m1.40.40.40.27.27.27.cmml">𝒙</mi><mo id="S3.E4.m1.41.41.41.28.28.28" xref="S3.E4.m1.41.41.41.28.28.28.cmml">−</mo><mi id="S3.E4.m1.42.42.42.29.29.29" xref="S3.E4.m1.42.42.42.29.29.29.cmml">𝝁</mi></mrow><mo id="S3.E4.m1.43.43.43.30.30.30" stretchy="false" xref="S3.E4.m1.47.47.4a.cmml">)</mo></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E4.m1.51b"><apply id="S3.E4.m1.47.47.4.cmml" xref="S3.E4.m1.51.51.8"><and id="S3.E4.m1.47.47.4a.cmml" xref="S3.E4.m1.13.13.13.13.13.14.1"></and><apply id="S3.E4.m1.47.47.4b.cmml" xref="S3.E4.m1.51.51.8"><eq id="S3.E4.m1.9.9.9.9.9.9.cmml" xref="S3.E4.m1.9.9.9.9.9.9"></eq><apply id="S3.E4.m1.47.47.4.6.cmml" xref="S3.E4.m1.51.51.8"><times id="S3.E4.m1.47.47.4.6.1.cmml" xref="S3.E4.m1.13.13.13.13.13.14.1"></times><ci id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1">𝒫</ci><list id="S3.E4.m1.47.47.4.6.3.cmml" xref="S3.E4.m1.51.51.8"><ci id="S3.E4.m1.3.3.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.3.3">𝒙</ci><ci id="S3.E4.m1.5.5.5.5.5.5.cmml" xref="S3.E4.m1.5.5.5.5.5.5">𝝁</ci><ci id="S3.E4.m1.7.7.7.7.7.7.cmml" xref="S3.E4.m1.7.7.7.7.7.7">Σ</ci></list></apply><apply id="S3.E4.m1.47.47.4.8.cmml" xref="S3.E4.m1.51.51.8"><ln id="S3.E4.m1.10.10.10.10.10.10.cmml" xref="S3.E4.m1.10.10.10.10.10.10"></ln><ci id="S3.E4.m1.12.12.12.12.12.12.cmml" xref="S3.E4.m1.12.12.12.12.12.12">𝒢</ci></apply></apply><apply id="S3.E4.m1.47.47.4c.cmml" xref="S3.E4.m1.51.51.8"><eq id="S3.E4.m1.14.14.14.1.1.1.cmml" xref="S3.E4.m1.14.14.14.1.1.1"></eq><share href="https://arxiv.org/html/2409.10441v1#S3.E4.m1.47.47.4.8.cmml" id="S3.E4.m1.47.47.4d.cmml" xref="S3.E4.m1.13.13.13.13.13.14.1"></share><apply id="S3.E4.m1.47.47.4.4.cmml" xref="S3.E4.m1.51.51.8"><minus id="S3.E4.m1.21.21.21.8.8.8.cmml" xref="S3.E4.m1.21.21.21.8.8.8"></minus><apply id="S3.E4.m1.44.44.1.1.1.cmml" xref="S3.E4.m1.51.51.8"><minus id="S3.E4.m1.15.15.15.2.2.2.cmml" xref="S3.E4.m1.51.51.8"></minus><apply id="S3.E4.m1.44.44.1.1.1.1.2.cmml" xref="S3.E4.m1.51.51.8"><ln id="S3.E4.m1.16.16.16.3.3.3.cmml" xref="S3.E4.m1.16.16.16.3.3.3"></ln><apply id="S3.E4.m1.44.44.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.51.51.8"><times id="S3.E4.m1.44.44.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.51.51.8"></times><cn id="S3.E4.m1.18.18.18.5.5.5.cmml" type="integer" xref="S3.E4.m1.18.18.18.5.5.5">2</cn><ci id="S3.E4.m1.19.19.19.6.6.6.cmml" xref="S3.E4.m1.19.19.19.6.6.6">𝜋</ci></apply></apply></apply><apply id="S3.E4.m1.45.45.2.2.2.cmml" xref="S3.E4.m1.51.51.8"><times id="S3.E4.m1.45.45.2.2.2.2.cmml" xref="S3.E4.m1.13.13.13.13.13.14.1"></times><apply id="S3.E4.m1.22.22.22.9.9.9.cmml" xref="S3.E4.m1.22.22.22.9.9.9"><divide id="S3.E4.m1.22.22.22.9.9.9.1.cmml" xref="S3.E4.m1.22.22.22.9.9.9"></divide><cn id="S3.E4.m1.22.22.22.9.9.9.2.cmml" type="integer" xref="S3.E4.m1.22.22.22.9.9.9.2">1</cn><cn id="S3.E4.m1.22.22.22.9.9.9.3.cmml" type="integer" xref="S3.E4.m1.22.22.22.9.9.9.3">2</cn></apply><apply id="S3.E4.m1.45.45.2.2.2.1.2.cmml" xref="S3.E4.m1.51.51.8"><ln id="S3.E4.m1.23.23.23.10.10.10.cmml" xref="S3.E4.m1.23.23.23.10.10.10"></ln><apply id="S3.E4.m1.45.45.2.2.2.1.1.1.1.cmml" xref="S3.E4.m1.51.51.8"><abs id="S3.E4.m1.45.45.2.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.51.51.8"></abs><ci id="S3.E4.m1.26.26.26.13.13.13.cmml" xref="S3.E4.m1.26.26.26.13.13.13">Σ</ci></apply></apply></apply><apply id="S3.E4.m1.47.47.4.4.4.cmml" xref="S3.E4.m1.51.51.8"><times id="S3.E4.m1.47.47.4.4.4.3.cmml" xref="S3.E4.m1.13.13.13.13.13.14.1"></times><apply id="S3.E4.m1.30.30.30.17.17.17.cmml" xref="S3.E4.m1.30.30.30.17.17.17"><divide id="S3.E4.m1.30.30.30.17.17.17.1.cmml" xref="S3.E4.m1.30.30.30.17.17.17"></divide><cn id="S3.E4.m1.30.30.30.17.17.17.2.cmml" type="integer" xref="S3.E4.m1.30.30.30.17.17.17.2">1</cn><cn id="S3.E4.m1.30.30.30.17.17.17.3.cmml" type="integer" xref="S3.E4.m1.30.30.30.17.17.17.3">2</cn></apply><apply id="S3.E4.m1.46.46.3.3.3.1.cmml" xref="S3.E4.m1.51.51.8"><csymbol cd="ambiguous" id="S3.E4.m1.46.46.3.3.3.1.2.cmml" xref="S3.E4.m1.13.13.13.13.13.14.1">superscript</csymbol><apply id="S3.E4.m1.46.46.3.3.3.1.1.1.1.cmml" xref="S3.E4.m1.51.51.8"><minus id="S3.E4.m1.33.33.33.20.20.20.cmml" xref="S3.E4.m1.33.33.33.20.20.20"></minus><ci id="S3.E4.m1.32.32.32.19.19.19.cmml" xref="S3.E4.m1.32.32.32.19.19.19">𝒙</ci><ci id="S3.E4.m1.34.34.34.21.21.21.cmml" xref="S3.E4.m1.34.34.34.21.21.21">𝝁</ci></apply><csymbol cd="latexml" id="S3.E4.m1.36.36.36.23.23.23.1.cmml" xref="S3.E4.m1.36.36.36.23.23.23.1">top</csymbol></apply><apply id="S3.E4.m1.47.47.4.4.4.5.cmml" xref="S3.E4.m1.51.51.8"><csymbol cd="ambiguous" id="S3.E4.m1.47.47.4.4.4.5.1.cmml" xref="S3.E4.m1.13.13.13.13.13.14.1">superscript</csymbol><ci id="S3.E4.m1.37.37.37.24.24.24.cmml" xref="S3.E4.m1.37.37.37.24.24.24">Σ</ci><apply id="S3.E4.m1.38.38.38.25.25.25.1.cmml" xref="S3.E4.m1.38.38.38.25.25.25.1"><minus id="S3.E4.m1.38.38.38.25.25.25.1.1.cmml" xref="S3.E4.m1.38.38.38.25.25.25.1"></minus><cn id="S3.E4.m1.38.38.38.25.25.25.1.2.cmml" type="integer" xref="S3.E4.m1.38.38.38.25.25.25.1.2">1</cn></apply></apply><apply id="S3.E4.m1.47.47.4.4.4.2.1.1.cmml" xref="S3.E4.m1.51.51.8"><minus id="S3.E4.m1.41.41.41.28.28.28.cmml" xref="S3.E4.m1.41.41.41.28.28.28"></minus><ci id="S3.E4.m1.40.40.40.27.27.27.cmml" xref="S3.E4.m1.40.40.40.27.27.27">𝒙</ci><ci id="S3.E4.m1.42.42.42.29.29.29.cmml" xref="S3.E4.m1.42.42.42.29.29.29">𝝁</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.51c">\mathcal{P}(\boldsymbol{x};\boldsymbol{\mu},\Sigma)=\ln(\mathcal{G})\\
=-\ln(2\pi)-\frac{1}{2}\ln(|\Sigma|)-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{%
\mu})^{\top}\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.51d">start_ROW start_CELL caligraphic_P ( bold_italic_x ; bold_italic_μ , roman_Σ ) = roman_ln ( caligraphic_G ) end_CELL end_ROW start_ROW start_CELL = - roman_ln ( 2 italic_π ) - divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_ln ( | roman_Σ | ) - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( bold_italic_x - bold_italic_μ ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT roman_Σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_x - bold_italic_μ ) end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p3.9">Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib14" title="">14</a>]</cite>, we can approximate <math alttext="\boldsymbol{\mu}" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m1.1"><semantics id="S3.SS2.p3.5.m1.1a"><mi id="S3.SS2.p3.5.m1.1.1" xref="S3.SS2.p3.5.m1.1.1.cmml">𝝁</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m1.1b"><ci id="S3.SS2.p3.5.m1.1.1.cmml" xref="S3.SS2.p3.5.m1.1.1">𝝁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m1.1c">\boldsymbol{\mu}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m1.1d">bold_italic_μ</annotation></semantics></math> using the maximum activation <math alttext="\mathbf{m}" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m2.1"><semantics id="S3.SS2.p3.6.m2.1a"><mi id="S3.SS2.p3.6.m2.1.1" xref="S3.SS2.p3.6.m2.1.1.cmml">𝐦</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m2.1b"><ci id="S3.SS2.p3.6.m2.1.1.cmml" xref="S3.SS2.p3.6.m2.1.1">𝐦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m2.1c">\mathbf{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m2.1d">bold_m</annotation></semantics></math> of the predicted heatmap, as the maximum activation generally represents a good coarse prediction that approaches <math alttext="\boldsymbol{\mu}" class="ltx_Math" display="inline" id="S3.SS2.p3.7.m3.1"><semantics id="S3.SS2.p3.7.m3.1a"><mi id="S3.SS2.p3.7.m3.1.1" xref="S3.SS2.p3.7.m3.1.1.cmml">𝝁</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m3.1b"><ci id="S3.SS2.p3.7.m3.1.1.cmml" xref="S3.SS2.p3.7.m3.1.1">𝝁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m3.1c">\boldsymbol{\mu}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.7.m3.1d">bold_italic_μ</annotation></semantics></math>. Then, we can approximate <math alttext="\mathcal{P}(\boldsymbol{\mu})" class="ltx_Math" display="inline" id="S3.SS2.p3.8.m4.1"><semantics id="S3.SS2.p3.8.m4.1a"><mrow id="S3.SS2.p3.8.m4.1.2" xref="S3.SS2.p3.8.m4.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.8.m4.1.2.2" xref="S3.SS2.p3.8.m4.1.2.2.cmml">𝒫</mi><mo id="S3.SS2.p3.8.m4.1.2.1" xref="S3.SS2.p3.8.m4.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p3.8.m4.1.2.3.2" xref="S3.SS2.p3.8.m4.1.2.cmml"><mo id="S3.SS2.p3.8.m4.1.2.3.2.1" stretchy="false" xref="S3.SS2.p3.8.m4.1.2.cmml">(</mo><mi id="S3.SS2.p3.8.m4.1.1" xref="S3.SS2.p3.8.m4.1.1.cmml">𝝁</mi><mo id="S3.SS2.p3.8.m4.1.2.3.2.2" stretchy="false" xref="S3.SS2.p3.8.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m4.1b"><apply id="S3.SS2.p3.8.m4.1.2.cmml" xref="S3.SS2.p3.8.m4.1.2"><times id="S3.SS2.p3.8.m4.1.2.1.cmml" xref="S3.SS2.p3.8.m4.1.2.1"></times><ci id="S3.SS2.p3.8.m4.1.2.2.cmml" xref="S3.SS2.p3.8.m4.1.2.2">𝒫</ci><ci id="S3.SS2.p3.8.m4.1.1.cmml" xref="S3.SS2.p3.8.m4.1.1">𝝁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m4.1c">\mathcal{P}(\boldsymbol{\mu})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.8.m4.1d">caligraphic_P ( bold_italic_μ )</annotation></semantics></math> using the Taylor
series expansion evaluated at <math alttext="\mathbf{m}" class="ltx_Math" display="inline" id="S3.SS2.p3.9.m5.1"><semantics id="S3.SS2.p3.9.m5.1a"><mi id="S3.SS2.p3.9.m5.1.1" xref="S3.SS2.p3.9.m5.1.1.cmml">𝐦</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m5.1b"><ci id="S3.SS2.p3.9.m5.1.1.cmml" xref="S3.SS2.p3.9.m5.1.1">𝐦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m5.1c">\mathbf{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.9.m5.1d">bold_m</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{P}(\boldsymbol{\mu})=\mathcal{P}(\boldsymbol{m})+\mathcal{D}^{\prime}%
(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})\\
+\frac{1}{2}(\boldsymbol{\mu}-\boldsymbol{m})^{\top}\mathcal{D}^{\prime\prime}%
(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})" class="ltx_Math" display="block" id="S3.E5.m1.44"><semantics id="S3.E5.m1.44a"><mtable displaystyle="true" id="S3.E5.m1.44.44.6" rowspacing="0pt" xref="S3.E5.m1.41.41.3.cmml"><mtr id="S3.E5.m1.44.44.6a" xref="S3.E5.m1.41.41.3.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.44.44.6b" xref="S3.E5.m1.41.41.3.cmml"><mrow id="S3.E5.m1.42.42.4.39.21.21" xref="S3.E5.m1.41.41.3.cmml"><mrow id="S3.E5.m1.42.42.4.39.21.21.22" xref="S3.E5.m1.41.41.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml">𝒫</mi><mo id="S3.E5.m1.42.42.4.39.21.21.22.1" xref="S3.E5.m1.41.41.3.cmml">⁢</mo><mrow id="S3.E5.m1.42.42.4.39.21.21.22.2" xref="S3.E5.m1.41.41.3.cmml"><mo id="S3.E5.m1.2.2.2.2.2.2" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">(</mo><mi id="S3.E5.m1.3.3.3.3.3.3" xref="S3.E5.m1.3.3.3.3.3.3.cmml">𝝁</mi><mo id="S3.E5.m1.4.4.4.4.4.4" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.5.5.5.5.5.5" xref="S3.E5.m1.5.5.5.5.5.5.cmml">=</mo><mrow id="S3.E5.m1.42.42.4.39.21.21.21" xref="S3.E5.m1.41.41.3.cmml"><mrow id="S3.E5.m1.42.42.4.39.21.21.21.2" xref="S3.E5.m1.41.41.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.6.6.6.6.6.6" xref="S3.E5.m1.6.6.6.6.6.6.cmml">𝒫</mi><mo id="S3.E5.m1.42.42.4.39.21.21.21.2.1" xref="S3.E5.m1.41.41.3.cmml">⁢</mo><mrow id="S3.E5.m1.42.42.4.39.21.21.21.2.2" xref="S3.E5.m1.41.41.3.cmml"><mo id="S3.E5.m1.7.7.7.7.7.7" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">(</mo><mi id="S3.E5.m1.8.8.8.8.8.8" xref="S3.E5.m1.8.8.8.8.8.8.cmml">𝒎</mi><mo id="S3.E5.m1.9.9.9.9.9.9" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.10.10.10.10.10.10" xref="S3.E5.m1.10.10.10.10.10.10.cmml">+</mo><mrow id="S3.E5.m1.42.42.4.39.21.21.21.1" xref="S3.E5.m1.41.41.3.cmml"><msup id="S3.E5.m1.42.42.4.39.21.21.21.1.3" xref="S3.E5.m1.41.41.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.11.11.11.11.11.11" xref="S3.E5.m1.11.11.11.11.11.11.cmml">𝒟</mi><mo id="S3.E5.m1.12.12.12.12.12.12.1" xref="S3.E5.m1.12.12.12.12.12.12.1.cmml">′</mo></msup><mo id="S3.E5.m1.42.42.4.39.21.21.21.1.2" xref="S3.E5.m1.41.41.3.cmml">⁢</mo><mrow id="S3.E5.m1.42.42.4.39.21.21.21.1.4" xref="S3.E5.m1.41.41.3.cmml"><mo id="S3.E5.m1.13.13.13.13.13.13" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">(</mo><mi id="S3.E5.m1.14.14.14.14.14.14" xref="S3.E5.m1.14.14.14.14.14.14.cmml">𝒎</mi><mo id="S3.E5.m1.15.15.15.15.15.15" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">)</mo></mrow><mo id="S3.E5.m1.42.42.4.39.21.21.21.1.2a" xref="S3.E5.m1.41.41.3.cmml">⁢</mo><mrow id="S3.E5.m1.42.42.4.39.21.21.21.1.1.1" xref="S3.E5.m1.41.41.3.cmml"><mo id="S3.E5.m1.16.16.16.16.16.16" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">(</mo><mrow id="S3.E5.m1.42.42.4.39.21.21.21.1.1.1.1" xref="S3.E5.m1.41.41.3.cmml"><mi id="S3.E5.m1.17.17.17.17.17.17" xref="S3.E5.m1.17.17.17.17.17.17.cmml">𝝁</mi><mo id="S3.E5.m1.18.18.18.18.18.18" xref="S3.E5.m1.18.18.18.18.18.18.cmml">−</mo><mi id="S3.E5.m1.19.19.19.19.19.19" xref="S3.E5.m1.19.19.19.19.19.19.cmml">𝒎</mi></mrow><mo id="S3.E5.m1.20.20.20.20.20.20" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E5.m1.44.44.6c" xref="S3.E5.m1.41.41.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E5.m1.44.44.6d" xref="S3.E5.m1.41.41.3.cmml"><mrow id="S3.E5.m1.44.44.6.41.20.20" xref="S3.E5.m1.41.41.3.cmml"><mo id="S3.E5.m1.44.44.6.41.20.20a" xref="S3.E5.m1.41.41.3.cmml">+</mo><mrow id="S3.E5.m1.44.44.6.41.20.20.20" xref="S3.E5.m1.41.41.3.cmml"><mfrac id="S3.E5.m1.22.22.22.2.2.2" xref="S3.E5.m1.22.22.22.2.2.2.cmml"><mn id="S3.E5.m1.22.22.22.2.2.2.2" xref="S3.E5.m1.22.22.22.2.2.2.2.cmml">1</mn><mn id="S3.E5.m1.22.22.22.2.2.2.3" xref="S3.E5.m1.22.22.22.2.2.2.3.cmml">2</mn></mfrac><mo id="S3.E5.m1.44.44.6.41.20.20.20.3" xref="S3.E5.m1.41.41.3.cmml">⁢</mo><msup id="S3.E5.m1.43.43.5.40.19.19.19.1" xref="S3.E5.m1.41.41.3.cmml"><mrow id="S3.E5.m1.43.43.5.40.19.19.19.1.1.1" xref="S3.E5.m1.41.41.3.cmml"><mo id="S3.E5.m1.23.23.23.3.3.3" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">(</mo><mrow id="S3.E5.m1.43.43.5.40.19.19.19.1.1.1.1" xref="S3.E5.m1.41.41.3.cmml"><mi id="S3.E5.m1.24.24.24.4.4.4" xref="S3.E5.m1.24.24.24.4.4.4.cmml">𝝁</mi><mo id="S3.E5.m1.25.25.25.5.5.5" xref="S3.E5.m1.25.25.25.5.5.5.cmml">−</mo><mi id="S3.E5.m1.26.26.26.6.6.6" xref="S3.E5.m1.26.26.26.6.6.6.cmml">𝒎</mi></mrow><mo id="S3.E5.m1.27.27.27.7.7.7" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">)</mo></mrow><mo id="S3.E5.m1.28.28.28.8.8.8.1" xref="S3.E5.m1.28.28.28.8.8.8.1.cmml">⊤</mo></msup><mo id="S3.E5.m1.44.44.6.41.20.20.20.3a" xref="S3.E5.m1.41.41.3.cmml">⁢</mo><msup id="S3.E5.m1.44.44.6.41.20.20.20.4" xref="S3.E5.m1.41.41.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.29.29.29.9.9.9" xref="S3.E5.m1.29.29.29.9.9.9.cmml">𝒟</mi><mo id="S3.E5.m1.30.30.30.10.10.10.1" xref="S3.E5.m1.30.30.30.10.10.10.1.cmml">′′</mo></msup><mo id="S3.E5.m1.44.44.6.41.20.20.20.3b" xref="S3.E5.m1.41.41.3.cmml">⁢</mo><mrow id="S3.E5.m1.44.44.6.41.20.20.20.5" xref="S3.E5.m1.41.41.3.cmml"><mo id="S3.E5.m1.31.31.31.11.11.11" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">(</mo><mi id="S3.E5.m1.32.32.32.12.12.12" xref="S3.E5.m1.32.32.32.12.12.12.cmml">𝒎</mi><mo id="S3.E5.m1.33.33.33.13.13.13" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">)</mo></mrow><mo id="S3.E5.m1.44.44.6.41.20.20.20.3c" xref="S3.E5.m1.41.41.3.cmml">⁢</mo><mrow id="S3.E5.m1.44.44.6.41.20.20.20.2.1" xref="S3.E5.m1.41.41.3.cmml"><mo id="S3.E5.m1.34.34.34.14.14.14" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">(</mo><mrow id="S3.E5.m1.44.44.6.41.20.20.20.2.1.1" xref="S3.E5.m1.41.41.3.cmml"><mi id="S3.E5.m1.35.35.35.15.15.15" xref="S3.E5.m1.35.35.35.15.15.15.cmml">𝝁</mi><mo id="S3.E5.m1.36.36.36.16.16.16" xref="S3.E5.m1.36.36.36.16.16.16.cmml">−</mo><mi id="S3.E5.m1.37.37.37.17.17.17" xref="S3.E5.m1.37.37.37.17.17.17.cmml">𝒎</mi></mrow><mo id="S3.E5.m1.38.38.38.18.18.18" stretchy="false" xref="S3.E5.m1.41.41.3.cmml">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E5.m1.44b"><apply id="S3.E5.m1.41.41.3.cmml" xref="S3.E5.m1.44.44.6"><eq id="S3.E5.m1.5.5.5.5.5.5.cmml" xref="S3.E5.m1.5.5.5.5.5.5"></eq><apply id="S3.E5.m1.41.41.3.5.cmml" xref="S3.E5.m1.44.44.6"><times id="S3.E5.m1.41.41.3.5.1.cmml" xref="S3.E5.m1.44.44.6"></times><ci id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1">𝒫</ci><ci id="S3.E5.m1.3.3.3.3.3.3.cmml" xref="S3.E5.m1.3.3.3.3.3.3">𝝁</ci></apply><apply id="S3.E5.m1.41.41.3.3.cmml" xref="S3.E5.m1.44.44.6"><plus id="S3.E5.m1.10.10.10.10.10.10.cmml" xref="S3.E5.m1.10.10.10.10.10.10"></plus><apply id="S3.E5.m1.41.41.3.3.5.cmml" xref="S3.E5.m1.44.44.6"><times id="S3.E5.m1.41.41.3.3.5.1.cmml" xref="S3.E5.m1.44.44.6"></times><ci id="S3.E5.m1.6.6.6.6.6.6.cmml" xref="S3.E5.m1.6.6.6.6.6.6">𝒫</ci><ci id="S3.E5.m1.8.8.8.8.8.8.cmml" xref="S3.E5.m1.8.8.8.8.8.8">𝒎</ci></apply><apply id="S3.E5.m1.39.39.1.1.1.cmml" xref="S3.E5.m1.44.44.6"><times id="S3.E5.m1.39.39.1.1.1.2.cmml" xref="S3.E5.m1.44.44.6"></times><apply id="S3.E5.m1.39.39.1.1.1.3.cmml" xref="S3.E5.m1.44.44.6"><csymbol cd="ambiguous" id="S3.E5.m1.39.39.1.1.1.3.1.cmml" xref="S3.E5.m1.44.44.6">superscript</csymbol><ci id="S3.E5.m1.11.11.11.11.11.11.cmml" xref="S3.E5.m1.11.11.11.11.11.11">𝒟</ci><ci id="S3.E5.m1.12.12.12.12.12.12.1.cmml" xref="S3.E5.m1.12.12.12.12.12.12.1">′</ci></apply><ci id="S3.E5.m1.14.14.14.14.14.14.cmml" xref="S3.E5.m1.14.14.14.14.14.14">𝒎</ci><apply id="S3.E5.m1.39.39.1.1.1.1.1.1.cmml" xref="S3.E5.m1.44.44.6"><minus id="S3.E5.m1.18.18.18.18.18.18.cmml" xref="S3.E5.m1.18.18.18.18.18.18"></minus><ci id="S3.E5.m1.17.17.17.17.17.17.cmml" xref="S3.E5.m1.17.17.17.17.17.17">𝝁</ci><ci id="S3.E5.m1.19.19.19.19.19.19.cmml" xref="S3.E5.m1.19.19.19.19.19.19">𝒎</ci></apply></apply><apply id="S3.E5.m1.41.41.3.3.3.cmml" xref="S3.E5.m1.44.44.6"><times id="S3.E5.m1.41.41.3.3.3.3.cmml" xref="S3.E5.m1.44.44.6"></times><apply id="S3.E5.m1.22.22.22.2.2.2.cmml" xref="S3.E5.m1.22.22.22.2.2.2"><divide id="S3.E5.m1.22.22.22.2.2.2.1.cmml" xref="S3.E5.m1.22.22.22.2.2.2"></divide><cn id="S3.E5.m1.22.22.22.2.2.2.2.cmml" type="integer" xref="S3.E5.m1.22.22.22.2.2.2.2">1</cn><cn id="S3.E5.m1.22.22.22.2.2.2.3.cmml" type="integer" xref="S3.E5.m1.22.22.22.2.2.2.3">2</cn></apply><apply id="S3.E5.m1.40.40.2.2.2.1.cmml" xref="S3.E5.m1.44.44.6"><csymbol cd="ambiguous" id="S3.E5.m1.40.40.2.2.2.1.2.cmml" xref="S3.E5.m1.44.44.6">superscript</csymbol><apply id="S3.E5.m1.40.40.2.2.2.1.1.1.1.cmml" xref="S3.E5.m1.44.44.6"><minus id="S3.E5.m1.25.25.25.5.5.5.cmml" xref="S3.E5.m1.25.25.25.5.5.5"></minus><ci id="S3.E5.m1.24.24.24.4.4.4.cmml" xref="S3.E5.m1.24.24.24.4.4.4">𝝁</ci><ci id="S3.E5.m1.26.26.26.6.6.6.cmml" xref="S3.E5.m1.26.26.26.6.6.6">𝒎</ci></apply><csymbol cd="latexml" id="S3.E5.m1.28.28.28.8.8.8.1.cmml" xref="S3.E5.m1.28.28.28.8.8.8.1">top</csymbol></apply><apply id="S3.E5.m1.41.41.3.3.3.5.cmml" xref="S3.E5.m1.44.44.6"><csymbol cd="ambiguous" id="S3.E5.m1.41.41.3.3.3.5.1.cmml" xref="S3.E5.m1.44.44.6">superscript</csymbol><ci id="S3.E5.m1.29.29.29.9.9.9.cmml" xref="S3.E5.m1.29.29.29.9.9.9">𝒟</ci><ci id="S3.E5.m1.30.30.30.10.10.10.1.cmml" xref="S3.E5.m1.30.30.30.10.10.10.1">′′</ci></apply><ci id="S3.E5.m1.32.32.32.12.12.12.cmml" xref="S3.E5.m1.32.32.32.12.12.12">𝒎</ci><apply id="S3.E5.m1.41.41.3.3.3.2.1.1.cmml" xref="S3.E5.m1.44.44.6"><minus id="S3.E5.m1.36.36.36.16.16.16.cmml" xref="S3.E5.m1.36.36.36.16.16.16"></minus><ci id="S3.E5.m1.35.35.35.15.15.15.cmml" xref="S3.E5.m1.35.35.35.15.15.15">𝝁</ci><ci id="S3.E5.m1.37.37.37.17.17.17.cmml" xref="S3.E5.m1.37.37.37.17.17.17">𝒎</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.44c">\mathcal{P}(\boldsymbol{\mu})=\mathcal{P}(\boldsymbol{m})+\mathcal{D}^{\prime}%
(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})\\
+\frac{1}{2}(\boldsymbol{\mu}-\boldsymbol{m})^{\top}\mathcal{D}^{\prime\prime}%
(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.44d">start_ROW start_CELL caligraphic_P ( bold_italic_μ ) = caligraphic_P ( bold_italic_m ) + caligraphic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_m ) ( bold_italic_μ - bold_italic_m ) end_CELL end_ROW start_ROW start_CELL + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( bold_italic_μ - bold_italic_m ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT caligraphic_D start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT ( bold_italic_m ) ( bold_italic_μ - bold_italic_m ) end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p3.10">Solving the above equation, we can obtain the equation to estimate <math alttext="\boldsymbol{\mu}" class="ltx_Math" display="inline" id="S3.SS2.p3.10.m1.1"><semantics id="S3.SS2.p3.10.m1.1a"><mi id="S3.SS2.p3.10.m1.1.1" xref="S3.SS2.p3.10.m1.1.1.cmml">𝝁</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m1.1b"><ci id="S3.SS2.p3.10.m1.1.1.cmml" xref="S3.SS2.p3.10.m1.1.1">𝝁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.m1.1c">\boldsymbol{\mu}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.10.m1.1d">bold_italic_μ</annotation></semantics></math> as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\boldsymbol{\mu}=\boldsymbol{m}-\left(\mathcal{D}^{\prime\prime}(\boldsymbol{m%
})\right)^{-1}\mathcal{D}^{\prime}(\boldsymbol{m})" class="ltx_Math" display="block" id="S3.E6.m1.3"><semantics id="S3.E6.m1.3a"><mrow id="S3.E6.m1.3.3" xref="S3.E6.m1.3.3.cmml"><mi id="S3.E6.m1.3.3.3" xref="S3.E6.m1.3.3.3.cmml">𝝁</mi><mo id="S3.E6.m1.3.3.2" xref="S3.E6.m1.3.3.2.cmml">=</mo><mrow id="S3.E6.m1.3.3.1" xref="S3.E6.m1.3.3.1.cmml"><mi id="S3.E6.m1.3.3.1.3" xref="S3.E6.m1.3.3.1.3.cmml">𝒎</mi><mo id="S3.E6.m1.3.3.1.2" xref="S3.E6.m1.3.3.1.2.cmml">−</mo><mrow id="S3.E6.m1.3.3.1.1" xref="S3.E6.m1.3.3.1.1.cmml"><msup id="S3.E6.m1.3.3.1.1.1" xref="S3.E6.m1.3.3.1.1.1.cmml"><mrow id="S3.E6.m1.3.3.1.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.3.3.1.1.1.1.1.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.3.3.1.1.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.1.1.cmml"><msup id="S3.E6.m1.3.3.1.1.1.1.1.1.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2.2.cmml">𝒟</mi><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2.3.cmml">′′</mo></msup><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E6.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">𝒎</mi><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E6.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.3.3.1.1.1.1.1.3" xref="S3.E6.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.E6.m1.3.3.1.1.1.3" xref="S3.E6.m1.3.3.1.1.1.3.cmml"><mo id="S3.E6.m1.3.3.1.1.1.3a" xref="S3.E6.m1.3.3.1.1.1.3.cmml">−</mo><mn id="S3.E6.m1.3.3.1.1.1.3.2" xref="S3.E6.m1.3.3.1.1.1.3.2.cmml">1</mn></mrow></msup><mo id="S3.E6.m1.3.3.1.1.2" xref="S3.E6.m1.3.3.1.1.2.cmml">⁢</mo><msup id="S3.E6.m1.3.3.1.1.3" xref="S3.E6.m1.3.3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.3.3.1.1.3.2" xref="S3.E6.m1.3.3.1.1.3.2.cmml">𝒟</mi><mo id="S3.E6.m1.3.3.1.1.3.3" xref="S3.E6.m1.3.3.1.1.3.3.cmml">′</mo></msup><mo id="S3.E6.m1.3.3.1.1.2a" xref="S3.E6.m1.3.3.1.1.2.cmml">⁢</mo><mrow id="S3.E6.m1.3.3.1.1.4.2" xref="S3.E6.m1.3.3.1.1.cmml"><mo id="S3.E6.m1.3.3.1.1.4.2.1" stretchy="false" xref="S3.E6.m1.3.3.1.1.cmml">(</mo><mi id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml">𝒎</mi><mo id="S3.E6.m1.3.3.1.1.4.2.2" stretchy="false" xref="S3.E6.m1.3.3.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.3b"><apply id="S3.E6.m1.3.3.cmml" xref="S3.E6.m1.3.3"><eq id="S3.E6.m1.3.3.2.cmml" xref="S3.E6.m1.3.3.2"></eq><ci id="S3.E6.m1.3.3.3.cmml" xref="S3.E6.m1.3.3.3">𝝁</ci><apply id="S3.E6.m1.3.3.1.cmml" xref="S3.E6.m1.3.3.1"><minus id="S3.E6.m1.3.3.1.2.cmml" xref="S3.E6.m1.3.3.1.2"></minus><ci id="S3.E6.m1.3.3.1.3.cmml" xref="S3.E6.m1.3.3.1.3">𝒎</ci><apply id="S3.E6.m1.3.3.1.1.cmml" xref="S3.E6.m1.3.3.1.1"><times id="S3.E6.m1.3.3.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.2"></times><apply id="S3.E6.m1.3.3.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.1">superscript</csymbol><apply id="S3.E6.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1"><times id="S3.E6.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1"></times><apply id="S3.E6.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E6.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2.2">𝒟</ci><ci id="S3.E6.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2.3">′′</ci></apply><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝒎</ci></apply><apply id="S3.E6.m1.3.3.1.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.1.3"><minus id="S3.E6.m1.3.3.1.1.1.3.1.cmml" xref="S3.E6.m1.3.3.1.1.1.3"></minus><cn id="S3.E6.m1.3.3.1.1.1.3.2.cmml" type="integer" xref="S3.E6.m1.3.3.1.1.1.3.2">1</cn></apply></apply><apply id="S3.E6.m1.3.3.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.3.1.cmml" xref="S3.E6.m1.3.3.1.1.3">superscript</csymbol><ci id="S3.E6.m1.3.3.1.1.3.2.cmml" xref="S3.E6.m1.3.3.1.1.3.2">𝒟</ci><ci id="S3.E6.m1.3.3.1.1.3.3.cmml" xref="S3.E6.m1.3.3.1.1.3.3">′</ci></apply><ci id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2">𝒎</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.3c">\boldsymbol{\mu}=\boldsymbol{m}-\left(\mathcal{D}^{\prime\prime}(\boldsymbol{m%
})\right)^{-1}\mathcal{D}^{\prime}(\boldsymbol{m})</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.3d">bold_italic_μ = bold_italic_m - ( caligraphic_D start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT ( bold_italic_m ) ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT caligraphic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_m )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p3.13">where the <math alttext="\mathcal{D}^{\prime\prime}(\boldsymbol{m})" class="ltx_Math" display="inline" id="S3.SS2.p3.11.m1.1"><semantics id="S3.SS2.p3.11.m1.1a"><mrow id="S3.SS2.p3.11.m1.1.2" xref="S3.SS2.p3.11.m1.1.2.cmml"><msup id="S3.SS2.p3.11.m1.1.2.2" xref="S3.SS2.p3.11.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.11.m1.1.2.2.2" xref="S3.SS2.p3.11.m1.1.2.2.2.cmml">𝒟</mi><mo id="S3.SS2.p3.11.m1.1.2.2.3" xref="S3.SS2.p3.11.m1.1.2.2.3.cmml">′′</mo></msup><mo id="S3.SS2.p3.11.m1.1.2.1" xref="S3.SS2.p3.11.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p3.11.m1.1.2.3.2" xref="S3.SS2.p3.11.m1.1.2.cmml"><mo id="S3.SS2.p3.11.m1.1.2.3.2.1" stretchy="false" xref="S3.SS2.p3.11.m1.1.2.cmml">(</mo><mi id="S3.SS2.p3.11.m1.1.1" xref="S3.SS2.p3.11.m1.1.1.cmml">𝒎</mi><mo id="S3.SS2.p3.11.m1.1.2.3.2.2" stretchy="false" xref="S3.SS2.p3.11.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.11.m1.1b"><apply id="S3.SS2.p3.11.m1.1.2.cmml" xref="S3.SS2.p3.11.m1.1.2"><times id="S3.SS2.p3.11.m1.1.2.1.cmml" xref="S3.SS2.p3.11.m1.1.2.1"></times><apply id="S3.SS2.p3.11.m1.1.2.2.cmml" xref="S3.SS2.p3.11.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.11.m1.1.2.2.1.cmml" xref="S3.SS2.p3.11.m1.1.2.2">superscript</csymbol><ci id="S3.SS2.p3.11.m1.1.2.2.2.cmml" xref="S3.SS2.p3.11.m1.1.2.2.2">𝒟</ci><ci id="S3.SS2.p3.11.m1.1.2.2.3.cmml" xref="S3.SS2.p3.11.m1.1.2.2.3">′′</ci></apply><ci id="S3.SS2.p3.11.m1.1.1.cmml" xref="S3.SS2.p3.11.m1.1.1">𝒎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.11.m1.1c">\mathcal{D}^{\prime\prime}(\boldsymbol{m})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.11.m1.1d">caligraphic_D start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT ( bold_italic_m )</annotation></semantics></math> and <math alttext="\mathcal{D}^{\prime}(\boldsymbol{m})" class="ltx_Math" display="inline" id="S3.SS2.p3.12.m2.1"><semantics id="S3.SS2.p3.12.m2.1a"><mrow id="S3.SS2.p3.12.m2.1.2" xref="S3.SS2.p3.12.m2.1.2.cmml"><msup id="S3.SS2.p3.12.m2.1.2.2" xref="S3.SS2.p3.12.m2.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.12.m2.1.2.2.2" xref="S3.SS2.p3.12.m2.1.2.2.2.cmml">𝒟</mi><mo id="S3.SS2.p3.12.m2.1.2.2.3" xref="S3.SS2.p3.12.m2.1.2.2.3.cmml">′</mo></msup><mo id="S3.SS2.p3.12.m2.1.2.1" xref="S3.SS2.p3.12.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p3.12.m2.1.2.3.2" xref="S3.SS2.p3.12.m2.1.2.cmml"><mo id="S3.SS2.p3.12.m2.1.2.3.2.1" stretchy="false" xref="S3.SS2.p3.12.m2.1.2.cmml">(</mo><mi id="S3.SS2.p3.12.m2.1.1" xref="S3.SS2.p3.12.m2.1.1.cmml">𝒎</mi><mo id="S3.SS2.p3.12.m2.1.2.3.2.2" stretchy="false" xref="S3.SS2.p3.12.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.12.m2.1b"><apply id="S3.SS2.p3.12.m2.1.2.cmml" xref="S3.SS2.p3.12.m2.1.2"><times id="S3.SS2.p3.12.m2.1.2.1.cmml" xref="S3.SS2.p3.12.m2.1.2.1"></times><apply id="S3.SS2.p3.12.m2.1.2.2.cmml" xref="S3.SS2.p3.12.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.12.m2.1.2.2.1.cmml" xref="S3.SS2.p3.12.m2.1.2.2">superscript</csymbol><ci id="S3.SS2.p3.12.m2.1.2.2.2.cmml" xref="S3.SS2.p3.12.m2.1.2.2.2">𝒟</ci><ci id="S3.SS2.p3.12.m2.1.2.2.3.cmml" xref="S3.SS2.p3.12.m2.1.2.2.3">′</ci></apply><ci id="S3.SS2.p3.12.m2.1.1.cmml" xref="S3.SS2.p3.12.m2.1.1">𝒎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.12.m2.1c">\mathcal{D}^{\prime}(\boldsymbol{m})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.12.m2.1d">caligraphic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_m )</annotation></semantics></math> are the first and second image derivative of the predicted heatmap at the maximum activation <math alttext="\mathbf{m}" class="ltx_Math" display="inline" id="S3.SS2.p3.13.m3.1"><semantics id="S3.SS2.p3.13.m3.1a"><mi id="S3.SS2.p3.13.m3.1.1" xref="S3.SS2.p3.13.m3.1.1.cmml">𝐦</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.13.m3.1b"><ci id="S3.SS2.p3.13.m3.1.1.cmml" xref="S3.SS2.p3.13.m3.1.1">𝐦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.13.m3.1c">\mathbf{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.13.m3.1d">bold_m</annotation></semantics></math>. The detailed derivation can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">For robot manipulation tasks, the robot and camera positions often remain fixed throughout the episode. We can leverage this temporal consistency to improve the accuracy of estimation. Instead of estimating the robot pose for each single frame, we estimate it based on a batch of image frames from a single episode.
To perform batch estimation, we first use CLIP to predict which parts of the robot are visible in each image frame and select the keypoints that appear on the visible parts. Since we have more than enough keypoints to solve for the pose, we prioritize the keypoints with high confidence. We evaluate each keypoint based on the maximum activation value in the heatmap, denoted as <math alttext="|\mathbf{m}|" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.2.2" xref="S3.SS2.p4.1.m1.1.2.1.cmml"><mo id="S3.SS2.p4.1.m1.1.2.2.1" stretchy="false" xref="S3.SS2.p4.1.m1.1.2.1.1.cmml">|</mo><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">𝐦</mi><mo id="S3.SS2.p4.1.m1.1.2.2.2" stretchy="false" xref="S3.SS2.p4.1.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.2.1.cmml" xref="S3.SS2.p4.1.m1.1.2.2"><abs id="S3.SS2.p4.1.m1.1.2.1.1.cmml" xref="S3.SS2.p4.1.m1.1.2.2.1"></abs><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝐦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">|\mathbf{m}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">| bold_m |</annotation></semantics></math>, and filter out the keypoints with a maximum activation value below a certain threshold.
Finally, we combine all the reliable keypoints from multiple frames and use the PnP solver to estimate the robot pose.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments and Results</span>
</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">Comparsion of top-1 accuracy (%) and training time (s) of different fine-tuning methods on robot parts detection with few-shot learning. We evaluate the detection accuracy for the robot base (Base) and robot end-effector (EE). The results are based on the average of 3 seeds. Fine-tuning CLIP using LoRA achieves good performance with short training time.</span></figcaption>
<p class="ltx_p ltx_align_center" id="S4.T1.4"><span class="ltx_text ltx_inline-block" id="S4.T1.4.1" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.4.1.1" style="width:729.2pt;height:127pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T1.4.1.1.1"><span class="ltx_text" id="S4.T1.4.1.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.4.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S4.T1.4.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T1.4.1.1.1.1.1.1.1.1"><span class="ltx_text" id="S4.T1.4.1.1.1.1.1.1.1.1.1">Method</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3" id="S4.T1.4.1.1.1.1.1.1.1.2">0 shots</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3" id="S4.T1.4.1.1.1.1.1.1.1.3">4 shots</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3" id="S4.T1.4.1.1.1.1.1.1.1.4">8 shots</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3" id="S4.T1.4.1.1.1.1.1.1.1.5">16 shots</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3" id="S4.T1.4.1.1.1.1.1.1.1.6">32 shots</span></span>
<span class="ltx_tr" id="S4.T1.4.1.1.1.1.1.2.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.1">EE (%)</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.2">Base (%)</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.3">Time (s)</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.4">EE</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.5">Base</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.6">Time</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.7">EE</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.8">Base</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.9">Time</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.10">EE</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.11">Base</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.12">Time</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.13">EE</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.14">Base</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.1.1.1.1.1.2.2.15">Time</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S4.T1.4.1.1.1.1.1.3.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.1">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib29" title="">29</a>]</cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.2">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.3">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.4">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.5">62.76</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.6">62.23</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.7"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.3.1.7.1">31.17</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.8">70.56</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.9">69.43</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.10"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.3.1.10.1">46.14</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.11">79.43</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.12">79.46</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.13"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.3.1.13.1">72.90</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.14">91.13</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.15">77.23</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.1.1.1.1.3.1.16">138.91</span></span>
<span class="ltx_tr" id="S4.T1.4.1.1.1.1.1.4.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.1.1.1.1.1.4.2.1">ResNet152 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib29" title="">29</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.2">-</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.3">-</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.4">-</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.5">55.00</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.6">61.10</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.7">70.23</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.8">62.76</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.9">61.66</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.10">85.16</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.11">71.66</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.12">67.23</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.13">121.90</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.14">91.66</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.15">75.56</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.4.2.16">203.34</span></span>
<span class="ltx_tr" id="S4.T1.4.1.1.1.1.1.5.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.1.1.1.1.1.5.3.1">CoOp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib32" title="">32</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.2">75.00</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.3">63.33</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.4">-</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.5.3.5.1">82.76</span></span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.6">67.76</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.7">69.70</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.8"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.5.3.8.1">86.70</span></span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.9">73.33</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.10">87.10</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.11">87.23</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.12">72.23</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.13">138.82</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.14">93.33</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.15">68.90</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.5.3.16">159.79</span></span>
<span class="ltx_tr" id="S4.T1.4.1.1.1.1.1.6.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.1.1.1.1.1.6.4.1">CLIP (Full Fine-tuning)</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.2">75.00</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.3">63.33</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.4">-</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.5">76.66</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.6">72.20</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.7">316.06</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.8">82.23</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.9">72.76</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.10">330.69</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.11">86.13</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.12">75.00</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.13">365.27</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.14">90.00</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.15">80.00</span>
<span class="ltx_td ltx_align_center" id="S4.T1.4.1.1.1.1.1.6.4.16">450.81</span></span>
<span class="ltx_tr" id="S4.T1.4.1.1.1.1.1.7.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.1">CLIP (LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib40" title="">40</a>]</cite>)</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.2">75.00</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.3">63.33</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.4">-</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.5">76.66</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.6"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.7.5.6.1">81.13</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.7">40.70</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.8">81.10</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.9"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.7.5.9.1">80.56</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.10">57.89</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.11"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.7.5.11.1">92.76</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.12"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.7.5.12.1">80.56</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.13">89.51</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.14"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.7.5.14.1">96.70</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.15"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.7.5.15.1">87.23</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.1.1.1.1.7.5.16"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.7.5.16.1">108.33</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</figure>
<figure class="ltx_figure" id="S4.F4">
<p class="ltx_p ltx_align_center" id="S4.F4.1"><span class="ltx_text" id="S4.F4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="221" id="S4.F4.1.1.g1" src="x4.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.4.2" style="font-size:90%;">Qualitative results of our method on the real-world manipulation dataset DROID <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib5" title="">5</a>]</cite>. The first row is the raw image frames, the second is the robot masks rendered based on the estimation of the original CtRNet (orange), and the third row is the robot masks rendered based on the estimation of the CtRNet-X (blue).
As shown above, CtRNet fails under real-world conditions whereas our method exhibits greater generalizability.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.4.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S4.T2.5.2" style="font-size:90%;">Performance comparison of different methods on DREAM-real dataset. We report the overall keypoint accuracy for the mean ADD and AUC of ADD.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.2.2.3">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.2.4">Category</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.2.2">Mean (m) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.m1.1.1" stretchy="false" xref="S4.T2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.2.3.1.1">DREAM-F <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib24" title="">24</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.1.2">Keypoint</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.1.3">60.740</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.1.4">113.029</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.4.2.1">DREAM-Q <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib24" title="">24</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.2.2">Keypoint</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.2.3">56.988</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.2.4">59.284</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.5.3.1">DREAM-H <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib24" title="">24</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.3.2">Keypoint</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.3.3">68.584</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.3.4">17.477</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.6.4.1">RoboPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib26" title="">26</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.2">Rendering</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.3">80.094</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.4">0.020</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.7.5.1">CtRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib7" title="">7</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.7.5.2">Keypoint</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.7.5.3">85.962</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.7.5.4">0.020</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.2.8.6.1">CtRNet-X</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.8.6.2">Keypoint</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.8.6.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.8.6.3.1">86.231</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.8.6.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.8.6.4.1">0.014</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Implementation Details</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We implemented the framework in Pytorch. The CLIP for robot parts detection is fine-tuned using an NVIDIA RTX 3090 GPU, while the pre-training and self-training of the keypoint detector are conducted on an NVIDIA RTX A6000 GPU.
For few-shot learning comparisons, the network or prompt parameters are trained for 200 epochs, given the small sample size and the models converge quickly.
For fine-tuning the CLIP using LoRA, we apply low-rank matrices on the query, key and value matrices for both image and text encoders with <math alttext="r" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_r</annotation></semantics></math> = 2.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">For the keypoint detector, we pre-train the neural network on synthetic data from the DREAM dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib8" title="">8</a>]</cite> and follow the training parameters from the CtRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib7" title="">7</a>]</cite>. The standard deviation of the Gaussian heatmap is set to 6 pixels. In the robot manipulation scenarios, the most common visible components are the robot end-effector and robot base. Hence, we simplify our framework to specifically recognize the robot end-effector and robot base. We place 6 keypoints for each of the links representing the robot end-effector and robot base (see example in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S2.F3" title="Figure 3 ‣ II-A Camera-to-Robot Pose Estimation ‣ II Previous Work ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag">3</span></a>).
For self-training, we utilize the DREAM-real dataset and our self-collected Panda manipulation dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Few-shot Learning for Robot Parts Detection</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To examine the performance of VLM for robot parts detection, we compare popular fine-tuning methods, including Low-Rank Adaption <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib40" title="">40</a>]</cite>, prompt learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib32" title="">32</a>]</cite>, and full-fine-tuning. Moreover, we also include classical object recognition technique using ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib29" title="">29</a>]</cite> with ImageNet pretrained weights. For simplicity, we conduct the experiment on detecting the robot end-effector and robot base links. We train the networks on the training dataset scraped from DROID <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib5" title="">5</a>]</cite> and evaluate the performance on a test dataset that has various unseen environments. We experiment with 3 random seeds and report the average top-1 accuracy for each category. The results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4.T1" title="TABLE I ‣ IV Experiments and Results ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag">I</span></a>.
We found that increasing the number of learning samples improves performance in general. However, the improvement becomes marginal when the training dataset becomes larger. Fine-tuning the CLIP with LoRA achieves better performance overall and requires less training time when learning with 32 shots.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Experiment on DREAM-real Dataset</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.3">We benchmark the robot pose estimation performance of CtRNet-X on the DREAM-real <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib24" title="">24</a>]</cite> dataset. The DREAM-real dataset consists of real-world images of the Franka Emika Panda robot arm captured from three different camera setups with total of around 57k image frames. We evaluate our method, together with other state-of-the-art robot pose estimation methods, on a single-frame setup.
We adopt average distance (ADD) metric to evaluate the pose estimation accuracy,</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="ADD=\frac{1}{n}\sum_{i=1}^{n}\left\|\tilde{\mathbf{T}}_{b}^{c}\mathbf{p}_{i}-%
\mathbf{T}_{b}^{c}\mathbf{p}_{i}\right\|_{2}" class="ltx_Math" display="block" id="S4.E7.m1.1"><semantics id="S4.E7.m1.1a"><mrow id="S4.E7.m1.1.1" xref="S4.E7.m1.1.1.cmml"><mrow id="S4.E7.m1.1.1.3" xref="S4.E7.m1.1.1.3.cmml"><mi id="S4.E7.m1.1.1.3.2" xref="S4.E7.m1.1.1.3.2.cmml">A</mi><mo id="S4.E7.m1.1.1.3.1" xref="S4.E7.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.E7.m1.1.1.3.3" xref="S4.E7.m1.1.1.3.3.cmml">D</mi><mo id="S4.E7.m1.1.1.3.1a" xref="S4.E7.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.E7.m1.1.1.3.4" xref="S4.E7.m1.1.1.3.4.cmml">D</mi></mrow><mo id="S4.E7.m1.1.1.2" xref="S4.E7.m1.1.1.2.cmml">=</mo><mrow id="S4.E7.m1.1.1.1" xref="S4.E7.m1.1.1.1.cmml"><mfrac id="S4.E7.m1.1.1.1.3" xref="S4.E7.m1.1.1.1.3.cmml"><mn id="S4.E7.m1.1.1.1.3.2" xref="S4.E7.m1.1.1.1.3.2.cmml">1</mn><mi id="S4.E7.m1.1.1.1.3.3" xref="S4.E7.m1.1.1.1.3.3.cmml">n</mi></mfrac><mo id="S4.E7.m1.1.1.1.2" xref="S4.E7.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E7.m1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.cmml"><munderover id="S4.E7.m1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.2.cmml"><mo id="S4.E7.m1.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S4.E7.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E7.m1.1.1.1.1.2.2.3" xref="S4.E7.m1.1.1.1.1.2.2.3.cmml"><mi id="S4.E7.m1.1.1.1.1.2.2.3.2" xref="S4.E7.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E7.m1.1.1.1.1.2.2.3.1" xref="S4.E7.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E7.m1.1.1.1.1.2.2.3.3" xref="S4.E7.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E7.m1.1.1.1.1.2.3" xref="S4.E7.m1.1.1.1.1.2.3.cmml">n</mi></munderover><msub id="S4.E7.m1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.cmml"><mrow id="S4.E7.m1.1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.2.cmml"><mo id="S4.E7.m1.1.1.1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E7.m1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E7.m1.1.1.1.1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.cmml"><msubsup id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mover accent="true" id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">𝐓</mi><mo id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2.1" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml">~</mo></mover><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">b</mi><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">c</mi></msubsup><mo id="S4.E7.m1.1.1.1.1.1.1.1.1.2.1" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S4.E7.m1.1.1.1.1.1.1.1.1.2.3" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">𝐩</mi><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S4.E7.m1.1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.E7.m1.1.1.1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.cmml"><msubsup id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.2.2" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.2.2.cmml">𝐓</mi><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.2.3" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.2.3.cmml">b</mi><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">c</mi></msubsup><mo id="S4.E7.m1.1.1.1.1.1.1.1.1.3.1" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><msub id="S4.E7.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">𝐩</mi><mi id="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><mo id="S4.E7.m1.1.1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S4.E7.m1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.1b"><apply id="S4.E7.m1.1.1.cmml" xref="S4.E7.m1.1.1"><eq id="S4.E7.m1.1.1.2.cmml" xref="S4.E7.m1.1.1.2"></eq><apply id="S4.E7.m1.1.1.3.cmml" xref="S4.E7.m1.1.1.3"><times id="S4.E7.m1.1.1.3.1.cmml" xref="S4.E7.m1.1.1.3.1"></times><ci id="S4.E7.m1.1.1.3.2.cmml" xref="S4.E7.m1.1.1.3.2">𝐴</ci><ci id="S4.E7.m1.1.1.3.3.cmml" xref="S4.E7.m1.1.1.3.3">𝐷</ci><ci id="S4.E7.m1.1.1.3.4.cmml" xref="S4.E7.m1.1.1.3.4">𝐷</ci></apply><apply id="S4.E7.m1.1.1.1.cmml" xref="S4.E7.m1.1.1.1"><times id="S4.E7.m1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.2"></times><apply id="S4.E7.m1.1.1.1.3.cmml" xref="S4.E7.m1.1.1.1.3"><divide id="S4.E7.m1.1.1.1.3.1.cmml" xref="S4.E7.m1.1.1.1.3"></divide><cn id="S4.E7.m1.1.1.1.3.2.cmml" type="integer" xref="S4.E7.m1.1.1.1.3.2">1</cn><ci id="S4.E7.m1.1.1.1.3.3.cmml" xref="S4.E7.m1.1.1.1.3.3">𝑛</ci></apply><apply id="S4.E7.m1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1"><apply id="S4.E7.m1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.2.1.cmml" xref="S4.E7.m1.1.1.1.1.2">superscript</csymbol><apply id="S4.E7.m1.1.1.1.1.2.2.cmml" xref="S4.E7.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.2.2.1.cmml" xref="S4.E7.m1.1.1.1.1.2">subscript</csymbol><sum id="S4.E7.m1.1.1.1.1.2.2.2.cmml" xref="S4.E7.m1.1.1.1.1.2.2.2"></sum><apply id="S4.E7.m1.1.1.1.1.2.2.3.cmml" xref="S4.E7.m1.1.1.1.1.2.2.3"><eq id="S4.E7.m1.1.1.1.1.2.2.3.1.cmml" xref="S4.E7.m1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E7.m1.1.1.1.1.2.2.3.2.cmml" xref="S4.E7.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S4.E7.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S4.E7.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E7.m1.1.1.1.1.2.3.cmml" xref="S4.E7.m1.1.1.1.1.2.3">𝑛</ci></apply><apply id="S4.E7.m1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1">subscript</csymbol><apply id="S4.E7.m1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E7.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1"><minus id="S4.E7.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2"><times id="S4.E7.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2">superscript</csymbol><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2"><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2.1">~</ci><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.2.2">𝐓</ci></apply><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.2.3">𝑏</ci></apply><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.2.3">𝑐</ci></apply><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.2">𝐩</ci><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3"><times id="S4.E7.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.1"></times><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2">superscript</csymbol><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.2.2">𝐓</ci><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.2.3">𝑏</ci></apply><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.2.3">𝑐</ci></apply><apply id="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.2">𝐩</ci><ci id="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply><cn id="S4.E7.m1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E7.m1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.1c">ADD=\frac{1}{n}\sum_{i=1}^{n}\left\|\tilde{\mathbf{T}}_{b}^{c}\mathbf{p}_{i}-%
\mathbf{T}_{b}^{c}\mathbf{p}_{i}\right\|_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.E7.m1.1d">italic_A italic_D italic_D = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ∥ over~ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_T start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.p1.2">where <math alttext="\tilde{\mathbf{T}}_{b}^{c}" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><msubsup id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mover accent="true" id="S4.SS3.p1.1.m1.1.1.2.2" xref="S4.SS3.p1.1.m1.1.1.2.2.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2.2.2" xref="S4.SS3.p1.1.m1.1.1.2.2.2.cmml">𝐓</mi><mo id="S4.SS3.p1.1.m1.1.1.2.2.1" xref="S4.SS3.p1.1.m1.1.1.2.2.1.cmml">~</mo></mover><mi id="S4.SS3.p1.1.m1.1.1.2.3" xref="S4.SS3.p1.1.m1.1.1.2.3.cmml">b</mi><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">c</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">superscript</csymbol><apply id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.2.1.cmml" xref="S4.SS3.p1.1.m1.1.1">subscript</csymbol><apply id="S4.SS3.p1.1.m1.1.1.2.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2.2"><ci id="S4.SS3.p1.1.m1.1.1.2.2.1.cmml" xref="S4.SS3.p1.1.m1.1.1.2.2.1">~</ci><ci id="S4.SS3.p1.1.m1.1.1.2.2.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2.2.2">𝐓</ci></apply><ci id="S4.SS3.p1.1.m1.1.1.2.3.cmml" xref="S4.SS3.p1.1.m1.1.1.2.3">𝑏</ci></apply><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\tilde{\mathbf{T}}_{b}^{c}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">over~ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{T}_{b}^{c}" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><msubsup id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2.2" xref="S4.SS3.p1.2.m2.1.1.2.2.cmml">𝐓</mi><mi id="S4.SS3.p1.2.m2.1.1.2.3" xref="S4.SS3.p1.2.m2.1.1.2.3.cmml">b</mi><mi id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">c</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">superscript</csymbol><apply id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.2.1.cmml" xref="S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2.2">𝐓</ci><ci id="S4.SS3.p1.2.m2.1.1.2.3.cmml" xref="S4.SS3.p1.2.m2.1.1.2.3">𝑏</ci></apply><ci id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">\mathbf{T}_{b}^{c}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">bold_T start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT</annotation></semantics></math> stand for the ground truth and estimated pose respectively. The area-under-the-curve (AUC) value and mean ADD are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4.T2" title="TABLE II ‣ IV Experiments and Results ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag">II</span></a>. Benefiting from the advanced distribution-aware coordinate decoding method, CtRNet-X achieves higher AUC and lower mean errors compared to existing methods.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="564" id="S4.F5.g1" src="x5.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">Qualitative results on Panda manipulation dataset. The first row is rendered robot masks using ground-truth extrinsic calibration (green) and the second row is the rendered robot masks using the pose from CtRNet-X (blue). </span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.4.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.5.2">Experiment on Panda Manipulation Dataset</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">The DREAM-real dataset only includes images where the robot arm is fully visible. To better evaluate our performance in real-world robot manipulation scenarios, we collected the Panda manipulation dataset. This dataset contains scenarios where only certain parts of the robot are visible during manipulation, and the robot arm is sometimes in and out of the image frame.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">This dataset includes 60 video episodes: 30 robot-in-view episodes, where the visibility of the robot parts remains the same throughout the episode, and 30 robot-in-and-out episodes, where the visibility of the robot parts changes throughout the episode.
Each episode comes with synchronized robot joint angles and ground-truth camera-to-robot transformation. The camera-to-robot transformation is carefully calibrated using a checkerboard. To reduce calibration errors, we verify the calibration results by projecting the points at each link and overlaying the robot masks to ensure alignment with the images.
We compare our method with the original CtRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib7" title="">7</a>]</cite>, and report the ADD metric in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4.T3" title="TABLE III ‣ IV-E Experiment on DROID Dataset ‣ IV Experiments and Results ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag">III</span></a>. Additionally, we have provided the qualitative results in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4.F5" title="Figure 5 ‣ IV-C Experiment on DREAM-real Dataset ‣ IV Experiments and Results ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag">5</span></a>. Our method demonstrates significant improvements in partial-view robot pose estimation compared to the previous method. By leveraging temporal consistency through batch estimation, CtRNet-X further improves accuracy by a significant margin.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.4.1.1">IV-E</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS5.5.2">Experiment on DROID Dataset</span>
</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">In this section, we demonstrate that our method can be used to obtain accurate extrinsic calibration for large robot learning datasets.
DROID <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib5" title="">5</a>]</cite> is a large-scale in-the-wild robot manipulation dataset. We randomly sampled video episodes from the DROID and applied CtRNet-X to obtain the camera-to-robot transformation. We show some of the qualitative results in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#S4.F4" title="Figure 4 ‣ IV Experiments and Results ‣ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera"><span class="ltx_text ltx_ref_tag">4</span></a>, where we render the robot masks based on the estimated robot pose.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">To quantitatively evaluate our pose estimation performance on DROID, we use Segment Anything <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib42" title="">42</a>]</cite> to obtain the ground-truth robot masks and compute the Intersection over Union (IoU) for the rendered robot masks.
Due to labeling intensity for a hand-labeled ground-truth, we randomly selected 10 video episodes from DROID, which in total contain 3232 image frames, to label the ground-truth robot masks. The CtRNet-X achieves the average IoU of 0.8356.
We noticed that using the extrinsic information provided by the dataset, the average IoU of the rendered robot mask is 0.0186, demonstrating that the extrinsic calibration is prone to having errors which highlights the necessity for accurate extrinsic calibration using our method.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.6.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S4.T3.7.2" style="font-size:90%;">Qualitative results on Panda Manipulation Dataset. We report the overall mean and AUC of ADD with both single-frame and batch estimation.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.4" style="width:292.3pt;height:82.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.7pt,3.6pt) scale(0.92,0.92) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.4.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.4.4.5.1.1" rowspan="2"><span class="ltx_text" id="S4.T3.4.4.5.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.4.4.5.1.2">robot in view</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.4.4.5.1.3">robot in-and-out</th>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.1">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.2.2.2">Mean (m) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.2.m1.1a"><mo id="S4.T3.2.2.2.2.m1.1.1" stretchy="false" xref="S4.T3.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.3.3.3">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.3.m1.1.1" stretchy="false" xref="S4.T3.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.4.4.4.4">Mean <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.4.4.4.4.m1.1"><semantics id="S4.T3.4.4.4.4.m1.1a"><mo id="S4.T3.4.4.4.4.m1.1.1" stretchy="false" xref="S4.T3.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.m1.1b"><ci id="S4.T3.4.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.4.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.4.4.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.4.4.6.1.1">CtRNet (single frame)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.6.1.2">16.764</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.6.1.3">0.381</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.6.1.4">35.944</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.6.1.5">0.335</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.4.4.7.2.1">CtRNet-X (single frame)</th>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.7.2.2">60.317</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.7.2.3">0.059</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.7.2.4">59.828</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.7.2.5">0.056</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.4.4.8.3.1">CtRNet-X (batch)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.4.8.3.2"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.8.3.2.1">70.817</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.4.8.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.8.3.3.1">0.038</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.4.8.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.8.3.4.1">79.665</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.4.8.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.8.3.5.1">0.022</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We propose CtRNet-X, an end-to-end image-based robot pose estimation framework that can generalize to real-world robot manipulation scenarios. We employ the VLM, CLIP, for robot parts detection and dynamically select the keypoints of the visible robot parts. The robot pose is then estimated using a PnP solver with selected 2D and 3D keypoint correspondence.
We evaluate our method on the public robot pose dataset and self-collected manipulation dataset, demonstrating the superiority of our method in both fully and partially visible scenarios.
Admittedly, the performance of the framework would be limited by the visible robot parts since we only utilize the robot end-effector and robot base links for pose estimation. However, our approach can be extended to finer granularity by including more robot parts. In the future, we will extend our method to different robots, incorporate kinematic uncertainty <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10441v1#bib.bib13" title="">13</a>]</cite>, and investigate the performance in more complex environments.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Garrido-Jurado <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">et al.</span>, “Automatic generation and detection of highly reliable fiducial markers under occlusion,” <span class="ltx_text ltx_font_italic" id="bib.bib1.2.2">Pattern Recognition</span>, vol. 47, no. 6, pp. 2280–2292, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
E. Olson, “Apriltag: A robust and flexible visual fiducial system,” in <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">2011 IEEE international conference on robotics and automation</span>, pp. 3400–3407, IEEE, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. Fiala, “Artag, a fiducial marker system using digital techniques,” in <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)</span>, vol. 2, pp. 590–596, IEEE, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Brohan, <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">et al.</span>, “Open x-embodiment: Robotic learning datasets and rt-x models,” <span class="ltx_text ltx_font_italic" id="bib.bib4.2.2">arXiv preprint arXiv:2310.08864</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">et al.</span>, “Droid: A large-scale in-the-wild robot manipulation dataset,” <span class="ltx_text ltx_font_italic" id="bib.bib5.2.2">arXiv preprint arXiv:2403.12945</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Lu, F. Richter, and M. C. Yip, “Pose estimation for robot manipulators via keypoint optimization and sim-to-real transfer,” <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">IEEE Robotics and Automation Letters</span>, vol. 7, no. 2, pp. 4622–4629, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Lu, F. Richter, and M. C. Yip, “Markerless camera-to-robot pose estimation via self-supervised sim-to-real transfer,” in <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 21296–21306, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
T. E. Lee, J. Tremblay, T. To, J. Cheng, T. Mosier, O. Kroemer, D. Fox, and S. Birchfield, “Camera-to-robot pose estimation from a single image,” in <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">2020 IEEE International Conference on Robotics and Automation (ICRA)</span>, pp. 9426–9432, IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Lu, F. Richter, and M. C. Yip, “Pose estimation for robot manipulators via keypoint optimization and sim-to-real transfer,” <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE Robotics and Automation Letters</span>, vol. 7, no. 2, pp. 4622–4629, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. Hao, O. Özgüner, and M. C. Çavuşoğlu, “Vision-based surgical tool pose estimation for the da vinci® robotic surgical system,” in <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">2018 IEEE/RSJ international conference on intelligent robots and systems (IROS)</span>, pp. 1298–1305, IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y. Labbé, J. Carpentier, M. Aubry, and J. Sivic, “Single-view robot pose and joint angle estimation via render &amp; compare,” in <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 1654–1663, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, “Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection,” <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">The International journal of robotics research</span>, vol. 37, no. 4-5, pp. 421–436, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
F. Richter, J. Lu, R. K. Orosco, and M. C. Yip, “Robotic tool tracking under partially visible kinematic chain: A unified approach,” <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IEEE Transactions on Robotics</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
F. Zhang, X. Zhu, H. Dai, M. Ye, and C. Zhu, “Distribution-aware coordinate representation for human pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp. 7093–7102, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
F. C. Park and B. J. Martin, “Robot sensor calibration: solving AX= XB on the Euclidean group,” <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">IEEE Transactions on Robotics and Automation</span>, vol. 10, no. 5, pp. 717–721, 1994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
I. Fassi and G. Legnani, “Hand to sensor calibration: A geometrical interpretation of the matrix equation ax= xb,” <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Journal of Robotic Systems</span>, vol. 22, no. 9, pp. 497–506, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Ilonen and V. Kyrki, “Robust robot-camera calibration,” in <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">2011 15th International Conference on Advanced Robotics (ICAR)</span>, pp. 67–74, IEEE, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
R. Horaud and F. Dornaika, “Hand-eye calibration,” <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">The international journal of robotics research</span>, vol. 14, no. 3, pp. 195–210, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T. Schmidt, R. A. Newcombe, and D. Fox, “Dart: Dense articulated real-time tracking.,” in <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Robotics: Science and Systems</span>, vol. 2, pp. 1–9, Berkeley, CA, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
K. Pauwels, L. Rubio, and E. Ros, “Real-time model-based articulated object pose detection and tracking with variable rigidity constraints,” in <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span>, pp. 3994–4001, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
F. Michel, A. Krull, E. Brachmann, M. Y. Yang, S. Gumhold, and C. Rother, “Pose estimation of kinematic chain instances via object coordinate regression.,” in <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">BMVC</span>, pp. 181–1, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
K. Desingh, S. Lu, A. Opipari, and O. C. Jenkins, “Factored pose estimation of articulated objects using efficient nonparametric belief propagation,” in <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">2019 International Conference on Robotics and Automation (ICRA)</span>, pp. 7221–7227, IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Lambrecht and L. Kästner, “Towards the usage of synthetic data for marker-less pose estimation of articulated robots in rgb images,” in <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">2019 19th International Conference on Advanced Robotics (ICAR)</span>, pp. 240–247, IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
T. E. Lee, J. Tremblay, T. To, J. Cheng, T. Mosier, O. Kroemer, D. Fox, and S. Birchfield, “Camera-to-robot pose estimation from a single image,” in <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">2020 IEEE International Conference on Robotics and Automation (ICRA)</span>, pp. 9426–9432, IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Zuo, W. Qiu, L. Xie, F. Zhong, Y. Wang, and A. L. Yuille, “Craves: Controlling robotic arm with a vision-based economic system,” in <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 4214–4223, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y. Labbé, J. Carpentier, M. Aubry, and J. Sivic, “Single-view robot pose and joint angle estimation via render &amp; compare,” in <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 1654–1663, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Lu, F. Liu, C. Girerd, and M. C. Yip, “Image-based pose estimation and shape reconstruction for robot manipulators and soft, continuum robots via differentiable rendering,” in <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">2023 IEEE International Conference on Robotics and Automation (ICRA)</span>, pp. 560–567, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
L. Chen, Y. Qin, X. Zhou, and H. Su, “Easyhec: Accurate and automatic hand-eye calibration via differentiable rendering and space exploration,” <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">IEEE Robotics and Automation Letters</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pp. 770–778, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Parnami and M. Lee, “Learning from few examples: A summary of approaches to few-shot learning,” <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2203.04291</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">et al.</span>, “Learning transferable visual models from natural language supervision,” in <span class="ltx_text ltx_font_italic" id="bib.bib31.2.2">International conference on machine learning</span>, pp. 8748–8763, PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Learning to prompt for vision-language models,” <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">International Journal of Computer Vision</span>, vol. 130, no. 9, pp. 2337–2348, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Bulat and G. Tzimiropoulos, “Lasp: Text-to-text optimization for language-aware soft prompting of vision &amp; language models,” in <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 23232–23241, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
G. Chen, W. Yao, X. Song, X. Li, Y. Rao, and K. Zhang, “Plot: Prompt learning with optimal transport for vision-language models,” <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2210.01253</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y. Lu, J. Liu, Y. Zhang, Y. Liu, and X. Tian, “Prompt distribution learning,” in <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 5206–5215, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
H. Yao, R. Zhang, and C. Xu, “Visual-language prompt tuning with knowledge-guided context optimization,” in <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp. 6757–6767, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
B. Zhu, Y. Niu, Y. Han, Y. Wu, and H. Zhang, “Prompt-aligned gradient for prompt tuning,” in <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 15659–15669, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
E. B. Zaken, S. Ravfogel, and Y. Goldberg, “Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models,” <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2106.10199</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao, “Clip-adapter: Better vision-language models with feature adapters,” <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">International Journal of Computer Vision</span>, vol. 132, no. 2, pp. 581–595, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
B. Chen, A. Parra, J. Cao, N. Li, and T.-J. Chin, “End-to-end learnable geometric vision by backpropagating pnp optimization,” in <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 8100–8109, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">et al.</span>, “Segment anything,” <span class="ltx_text ltx_font_italic" id="bib.bib42.2.2">arXiv preprint arXiv:2304.02643</span>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 16 16:16:44 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
