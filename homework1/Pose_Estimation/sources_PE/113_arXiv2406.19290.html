<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Human Modelling and Pose Estimation Overview</title>
<!--Generated on Thu Jun 27 15:55:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.19290v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S1" title="In Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2" title="In Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS1" title="In II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">2D Single-Person Human Pose Estimation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS2" title="In II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">2D Multi-Person Human Pose Estimation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS2.SSS1" title="In II-B 2D Multi-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>1 </span>Top-down methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS2.SSS2" title="In II-B 2D Multi-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>2 </span>Bottom-up Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS2.SSS3" title="In II-B 2D Multi-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>3 </span>Top-down and bottom-up methods in videos</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS3" title="In II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">3D Single-Person Human Pose Estimation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS3.SSS1" title="In II-C 3D Single-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>1 </span>Skeleton-based methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS3.SSS2" title="In II-C 3D Single-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>2 </span>Model-based methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS4" title="In II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">3D Multi-Person Human Pose Estimation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS4.SSS1" title="In II-D 3D Multi-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span>1 </span>Skeleton-based methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.SS4.SSS2" title="In II-D 3D Multi-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span>2 </span>Model-based methods</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S3" title="In Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Datasets and Metrics</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S3.SS1" title="In III Datasets and Metrics ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Datasets</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S3.SS2" title="In III Datasets and Metrics ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Metrics</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S4" title="In Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">SOTA Methods Overview and Comparison</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S4.SS1" title="In IV SOTA Methods Overview and Comparison ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">2D Human Pose Estimation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S4.SS2" title="In IV SOTA Methods Overview and Comparison ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">3D Human Pose Estimation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S5" title="In Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Future Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S6" title="In Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Human Modelling and Pose Estimation Overview</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pawel Knap1
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
<br class="ltx_break"/>University of Southampton
<br class="ltx_break"/>1pmk1g20@soton.ac.uk
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Human modelling and pose estimation stands at the crossroads of Computer Vision, Computer Graphics, and Machine Learning. This paper presents a thorough investigation of this interdisciplinary field, examining various algorithms, methodologies, and practical applications. It explores the diverse range of sensor technologies relevant to this domain and delves into a wide array of application areas. Additionally, we discuss the challenges and advancements in 2D and 3D human modelling methodologies, along with popular datasets, metrics, and future research directions. The main contribution of this paper lies in its up-to-date comparison of state-of-the-art (SOTA) human pose estimation algorithms in both 2D and 3D domains. By providing this comprehensive overview, the paper aims to enhance understanding of 3D human modelling and pose estimation, offering insights into current SOTA achievements, challenges, and future prospects within the field.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Human modelling encompasses a range of techniques that include human pose estimation (HPE) and visualization of 3D human models. Situated at the intersection of Computer Vision (CV), Computer Graphics (CG), and Machine Learning (ML), this field integrates various methodologies to create accurate representations of human anatomy in 3D space. In our study, we delve into different algorithms employed for these tasks and conduct a comparative analysis of state-of-the-art (SOTA) solutions.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">HPE involves identifying the poses of human body parts and joints, usually in images or videos. Various sensors can be employed for this task, each offering distinct advantages and drawbacks. Monocular cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib4" title="">4</a>]</cite>, commonly utilized due to their affordability, are hindered by limitations such as occlusion and depth ambiguity. Camera arrays or multiple camera motion capture systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib5" title="">5</a>]</cite> mitigate some monocular camera limitations but are costlier and less versatile. RADAR systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib13" title="">13</a>]</cite> excel in occlusion handling and privacy preservation, yet encounter challenges due to sparse data input. Similarly, LIDAR sensors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib15" title="">15</a>]</cite> face sparse data issues while also being costly; however, they offer high-resolution output. Infrared (IR) sensors like Kinect <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib17" title="">17</a>]</cite> face difficulties outdoors due to sunlight interference. Wearable Motion Capture Systems offer another option for HPE but are constrained by costs and intrusiveness, restricting their applications. Moreover, there exist hybrid systems utilizing a combination of the aforementioned sensors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib20" title="">20</a>]</cite>. Therefore, our focus in this study predominantly centres on camera-based solutions, as the current research direction is to overcome their limitations, positioning them as SOTA solutions. Works involving RADAR, LIDAR, or IR-based approaches are comparatively less advanced.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Once the poses are identified, CG techniques are employed to visualize these real-world poses on a computer screen. Various representations of the human body, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">1</span></a>, can be used. These representations can be further manipulated, altering pose shape or appearance. Additionally, they can be animated, exemplifying their usefulness for the movie or gaming industry. Other application domains include virtual (VR), and augmented reality (AR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib22" title="">22</a>]</cite>, which currently stand as the most actively explored scientific research avenues for HPE.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="213" id="S1.F1.g1" src="extracted/5696266/Figures/human-pose-model.jpg" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Different representations of humans used in HPE system’s visualisations. Source <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib23" title="">23</a>]</cite></figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="221" id="S1.F2.g1" src="x1.png" width="1196"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Dichotomy of Human Pose Estimation algorithms.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">HPE can also be used in Human-Computer Interactions (HCI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib25" title="">25</a>]</cite>, for gesture control, improving user interaction with digital devices. In Robotics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib28" title="">28</a>]</cite>, HPE improves robots’ ability to intuitively interact with humans, particularly in human assistant roles. It also plays a crucial role in video surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib31" title="">31</a>]</cite>, aiding in the recognition of suspicious actions, and automotive industry for self-driving cars <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib4" title="">4</a>]</cite>. Furthermore, in sports or rehabilitation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib36" title="">36</a>]</cite>, HPE is utilized for analyzing movements to evaluate performance and enhance training techniques. Similarly, it can recognize posture defects for healthcare applications. Additionally, HPE is a key step towards the concept of digital twins <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib38" title="">38</a>]</cite>, which would be useful in designing personalized disease treatments or user-focused architecture.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The rest of the paper is organized as follows: Section 2 offers background information, while Section 3 reviews popular datasets and metrics. Section 4 delves into cutting-edge algorithms, comparing them, and outlining the problems they address, advantages, disadvantages, specifics, and target applications. Section 5 provides a comprehensive discussion of future research directions, and the paper concludes in Section 6.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Constrained by limited data and computational resources, early HPE research was concentrated on crafting features manually or optimizing deformable human body models. However focus changed due to the recent advancements in deep learning (DL) techniques, and in the availability of large-scale 2D/3D pose datasets such as COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib39" title="">39</a>]</cite>, MPII <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib40" title="">40</a>]</cite>, Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib5" title="">5</a>]</cite>, and 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib41" title="">41</a>]</cite>. DL techniques have partially addressed challenges like occlusion, close inter-person interactions, crowded scenes, complex postures, and small target persons. Neural networks (NN) automatically learn crucial features and capture non-linear information, offering advantages over previous methods based on hand-crafted features. However, they also have drawbacks, including sensitivity to small image changes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib42" title="">42</a>]</cite>, difficulties in generalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib43" title="">43</a>]</cite>, and issues with explainability and interoperability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib44" title="">44</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">There are diverse methods for representing the detected human body parts. These include keypoint-based representations in 2D or 3D, or 2D/3D heatmaps exhibiting high responses at keypoint locations and being suitable for regression by NN. Different methods use orientation maps like Part Affinity Fields (PAF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib45" title="">45</a>]</cite>, which utilize a 2D vector field of each pixel pointing from one limb to the other. This concept was further extended to 3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib46" title="">46</a>]</cite>. Another keypoint-based approach is 2D compositional human pose (CHP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib47" title="">47</a>]</cite>, which introduces a blend of bone and limb vectors and has been further developed into 3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib49" title="">49</a>]</cite>. Alternatively, richer information can be obtained using model-based representations. These are categorized into part-based volumetric models (planar models), representing limbs as geometric figures like cylinders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib50" title="">50</a>]</cite> or ellipsoids <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib51" title="">51</a>]</cite>, and detailed statistical 3D human body models like the skinned multi-person linear model (SMPL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib52" title="">52</a>]</cite>, encoding shape and pose into low-dimensional parameters.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Nowadays, human modelling and pose estimation algorithms can be categorized into 2D and 3D approaches, and further divided based on whether they focus on single or multiple subjects. In our study, we will explore algorithms from these four distinct groups as shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S1.F2" title="Figure 2 ‣ I Introduction ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">2D Single-Person Human Pose Estimation</span>
</h3>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="117" id="S2.F3.g1" src="x2.png" width="574"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Standard framework for estimating the pose of a single individual. Image taken from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib53" title="">53</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The typical architecture for single-person HPE depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.F3" title="Figure 3 ‣ II-A 2D Single-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">3</span></a> consists of a pose encoder and decoder. The encoder extracts features that are used by the decoder to determine human keypoints through regression or detection. While pre-trained models such as ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib54" title="">54</a>]</cite> are commonly employed as encoders, there is a growing interest in specialized pose-focused encoders like the hourglass network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib55" title="">55</a>]</cite> and PoseNAS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib56" title="">56</a>]</cite>. These architectures leverage techniques such as skip connection layers and neural architecture search (NAS) to enhance feature extraction for pose-related tasks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">However, the major focus is put on the design of the decoder. Regression models, such as DeepPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib57" title="">57</a>]</cite>, which is a first method using deep convolution neural networks (CNN), Iterative Error Feedback (IEF) network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib58" title="">58</a>]</cite> using self-correcting model, and Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib47" title="">47</a>]</cite> model utilizing body structure-aware compositional pose regression, have been milestones. However, regression models often struggle with complex poses due to the inherent non-linearity of directly mapping images to body coordinates.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">An alternative approach involves a detection-based decoder using heatmaps, which can be categorized into two main types: multi-stage architectures and structural body models. Structural body models leverage anatomical knowledge; for instance, Tompson et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib59" title="">59</a>]</cite> use a Markov Random Field to represent body joint distributions, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib60" title="">60</a>]</cite> employ conditional probabilities to learn location and spatial relationships of body part, and Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib61" title="">61</a>]</cite> utilize a structure-aware network. On the other hand, multi-stage architectures extract features for HPE at different levels. The well-known Hourglass network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib55" title="">55</a>]</cite> and its variants, such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib62" title="">62</a>]</cite>, which combines the Hourglass network with a feature pyramid, and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib63" title="">63</a>]</cite>, which introduces Hourglass Residual Units to increase the network’s receptive field, fall under this category. Other notable architectures include the Convolutional Pose Machine (CPM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib64" title="">64</a>]</cite>, which utilizes intermediate inputs and lacks an explicit graphical model.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Moreover, active research focuses on multi-task learning and pose refinement techniques. This includes joint 2D and 3D HPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib65" title="">65</a>]</cite> and iterative pose refinement modules <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib58" title="">58</a>]</cite>. Additionally, efforts to enhance speed and efficiency, particularly for video applications, involve lightweight networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib67" title="">67</a>]</cite>, network binarization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib68" title="">68</a>]</cite>, and model distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib70" title="">70</a>]</cite>. Using temporal information across frames in video processing can enhance HPE results. Approaches such as optical flow-based feature extraction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib72" title="">72</a>]</cite> and sequence models like recurrent networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib73" title="">73</a>]</cite> and LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib74" title="">74</a>]</cite> have shown promise in this domain.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">2D Multi-Person Human Pose Estimation</span>
</h3>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="327" id="S2.F4.g1" src="x3.png" width="574"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Standard frameworks for estimating poses of multiple people. Both top-down and bottom-up methods use encoder-decoder architecture. Image taken from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib53" title="">53</a>]</cite>.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="268" id="S2.F5.g1" src="x4.png" width="1196"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The workflow of the bottom-up approach utilized in OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib45" title="">45</a>]</cite>. The (b) Part Confidence Maps represent the heatmaps of body parts. Following the prediction of (c) Part Affinity Fields, (d) Bipartite Matching is executed to correlate body part candidates, culminating in the derivation of the (e) Parsing Results. Image source: <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib45" title="">45</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The multi-person HPE estimation is significantly more difficult than the single-person one because, in a multi-person scenario, an algorithm has to be able to detect an unknown number of people in sometimes crowded scenes, where inter-person or object occlusion happens often, and small targets occur. For this group of algorithms, we can distinguish two methods shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.F4" title="Figure 4 ‣ II-B 2D Multi-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">4</span></a>: bottom-up, where body parts of people are firstly detected and subsequently these parts are combined together for each person. And top-down, where each person is firstly localised and then in the bounding box of such person its body parts are detected, often for that purpose single-person HPE algorithms are used.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS1.4.1.1">II-B</span>1 </span>Top-down methods</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">However, single-person HPE algorithms are adjusted for the multi-person scenario to handle challenges such as truncation, occlusions, and small targets. Moreover, post-processing techniques like Non-Maximum-Suppression (NMS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib75" title="">75</a>]</cite> are applied to enhance keypoint detection after single-person HPE.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">One of the pioneering top-down approaches was proposed by Papandreou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib76" title="">76</a>]</cite>, which utilized a Faster RCNN detector and ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib54" title="">54</a>]</cite> to predict keypoint heatmaps. They also introduced keypoint-based NMS for improved localization. Xiao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib77" title="">77</a>]</cite> aimed to enhance spatial resolution by incorporating three deconvolution layers into a ResNet backbone. Another significant advancement was made by Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib78" title="">78</a>]</cite> with the Cascade Pyramid Network (CPN), which combines a global network with subsequent refinement to improve keypoint prediction. They also introduced an online hard keypoints mining (OHKM) loss to address challenging keypoints. Another approach for difficult keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib79" title="">79</a>]</cite> maintains high-resolution images throughout the network and integrates high-to-low resolution sub-networks, creating multi-resolution features. The two above models fall into the category of multi-stage or multi-branch fusion models, which provide superior detection accuracy due to increased complexity.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1">A common issue in top-down models is the occurrence of multiple individuals within a single bounding box. To address this, RMPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib80" title="">80</a>]</cite> employs parametric pose NMS to eliminate redundant people. Additionally, it introduces a symmetric spatial transformer for person identification and a pose-guided human proposal generator to enhance efficiency. Crowdpose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib81" title="">81</a>]</cite> adopts a different approach by utilizing a graph model to separate joint candidates. Additionally, the authors introduce a crowded HPE dataset in this paper, along with the Crowd Index, a metric designed to assess image crowding levels. Furthermore, similarly to single HPE, a multi-task approach can also be employed in top-down multi-person HPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib83" title="">83</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS2.4.1.1">II-B</span>2 </span>Bottom-up Methods</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">These methods can be categorized into two groups: those that detect and group keypoints for each person, and those that directly regress the position of each person’s keypoints. In the latter group, which is less popular and well-known, pixel-wise methods like CornerNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib84" title="">84</a>]</cite> and CenterNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib85" title="">85</a>]</cite> stand out as prominent examples.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">In the first group, the core challenge lies in accurately grouping keypoints belonging to the same person. DeepCut <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib2" title="">2</a>]</cite> addresses this by modelling grouping as integer linear programming (ILP), albeit with high computational costs. PAF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib45" title="">45</a>]</cite>, on the other hand, have emerged as SOTA solutions. These are 2D vector fields that encode the directional relationship between body parts. OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib45" title="">45</a>]</cite>, a real-time HPE algorithm, utilizes PAFs to connect confidence maps for each body part through a greedy algorithm as shown in the method framework in figure <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.F5" title="Figure 5 ‣ II-B 2D Multi-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">5</span></a>. PifPaf <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib4" title="">4</a>]</cite> extends this concept by using Part Intensity Field (PIF) for body part localization and PAF for association. The other idea is to group detected keypoints using embedding features or tags. Newell et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib86" title="">86</a>]</cite> generates detection heatmaps and embedding tags for each body keypoint. HigherHRnet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib87" title="">87</a>]</cite> improves upon HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib79" title="">79</a>]</cite> by aggregating HRNet features in a pyramid and utilizing associative embeddings. Additionally, multi-task algorithms like PersonLab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib88" title="">88</a>]</cite> can be used to simultaneously detect body keypoints heatmaps and human segmentation maps.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS3.4.1.1">II-B</span>3 </span>Top-down and bottom-up methods in videos</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Leveraging temporal information across consecutive frames can improve HPE performance. Development in this area was possible due to datasets like PoseTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib89" title="">89</a>]</cite>, offering multi-person in-the-wild images for HPE and tracking. In top-down methods, temporal data can be used in two ways. Tracking-by-detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib90" title="">90</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib91" title="">91</a>]</cite>, also called clip-based technique, tracks individuals’ bounding boxes across frames for subsequent single-person 2D video HPE based on cropped images. The other technique utilizes an optical flow pose similarity to associate poses across frames <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib77" title="">77</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<p class="ltx_p" id="S2.SS2.SSS3.p2.1">In bottom-up methods, the temporal pose association can be solved as a linear programming problem utilizing spatio-temporal graph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib92" title="">92</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib89" title="">89</a>]</cite>, extending image-based multi-person HPE solutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib45" title="">45</a>]</cite>. However, these methods are slow, hindering real-time applications. Therefore, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib93" title="">93</a>]</cite> utilizes optical flow for online inter-frame pose association and tracking. Furthermore, concepts such as PAF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib45" title="">45</a>]</cite>, and associative embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib86" title="">86</a>]</cite> can be extended to Temporal Flow Fields <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib94" title="">94</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib95" title="">95</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib96" title="">96</a>]</cite> and spatio-temporal embedding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib97" title="">97</a>]</cite> for keypoints matching between consecutive frames.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">3D Single-Person Human Pose Estimation</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Monocular 3D HPE encounters similar challenges as 2D HPE, compounded by specific issues such as the scarcity of in-the-wild 3D datasets and inherent depth ambiguity. Despite these challenges, 3D representation offers advantages, capturing not only 3D location but also detailed shape and body texture. These aspects are leveraged in model-based 3D HPE, distinguishing it from skeleton-based 3D HPE, which lacks detailed appearance information.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS1.4.1.1">II-C</span>1 </span>Skeleton-based methods</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">The skeleton-based 3D HPE can be divided into heatmap-based, 2D-3D lifting, and image features with 2D keypoints integration frameworks as shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.F6" title="Figure 6 ‣ II-C1 Skeleton-based methods ‣ II-C 3D Single-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">6</span></a>. In the first method, a CNN network maps each 3D keypoint in the image as a 3D Gaussian distribution in the heatmap. Then a postprocessing step finds a final keypoint location as a local maximum. For example, Pavlokas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib98" title="">98</a>]</cite> introduces coarse-fine architecture consisting of hourglass networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib55" title="">55</a>]</cite> to interactively get fine-grained keypoint location. VNect <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib99" title="">99</a>]</cite> is a real-time video method that fits the kinematic skeleton in the post-processing to achieve temporally stable pose prediction. On the other hand, an integral method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib100" title="">100</a>]</cite> can be used to directly get keypoints location from heatmaps.</p>
</div>
<figure class="ltx_figure" id="S2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="250" id="S2.F6.g1" src="x5.png" width="574"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Exemplary frameworks for 3D single-person, skeleton-based HPE. Image taken from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib53" title="">53</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.1">2D-3D lifting methods start with stable 2D HPE algorithms followed by a depth-predicting network, thus utilizing 2D pose as an intermediate representation for 3D pose. However, accurately lifting the pose poses challenges due to inherent ambiguity — a single 2D pose can correspond to multiple 3D poses, and vice versa. Martinez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib101" title="">101</a>]</cite> introduced a simple feedforward lifting NN that showed promising performance. Other methodologies, like those by Chen and Ramanan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib81" title="">81</a>]</cite> and Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib102" title="">102</a>]</cite>, utilize exemplar-based techniques involving extensive dictionaries and nearest-neighbour searches for optimal 3D pose determination. Due to a lack of extensive datasets, weakly-supervised or unsupervised algorithms are often employed. The former methods use augmented 3D data or unpaired 2D-3D data to grasp human body priors without explicit 2D-3D correspondences. Notable contributions in this category include Pavlakos et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib103" title="">103</a>]</cite> and Ronchi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib104" title="">104</a>]</cite>, who integrated ordinal depth information into their methodologies. Additionally, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib105" title="">105</a>]</cite> introduced a weakly-supervised adversarial method using kinematic chains.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p3">
<p class="ltx_p" id="S2.SS3.SSS1.p3.1">Unsupervised 2D-3D lifting algorithms operate without labelled 3D pose data, relying on large-scale 2D annotated datasets and geometric constraints to infer 3D pose information. Tekin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib106" title="">106</a>]</cite> proposed a structured prediction framework that jointly estimates 2D and 3D pose. In contrast, Tome et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib107" title="">107</a>]</cite> use a multi-stage CNN to refine the 2D pose using back projection, thus refining 3D estimations. Alternative lifting approaches include creating a library of corresponding 2D-3D poses by projecting 3D poses to 2D space using random cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib108" title="">108</a>]</cite>. Then, the predicted 2D pose is matched to a 2D pose from the library, and its 3D pair is obtained. However, integrating features extracted from the image with 2D keypoints simplifies 3D keypoints prediction by reducing depth ambiguity through contextual information. Thus, SemGCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib109" title="">109</a>]</cite> employs joint-level features along with keypoints to construct graph nodes, enabling a graph CNN to predict 3D pose based on this information.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p4">
<p class="ltx_p" id="S2.SS3.SSS1.p4.1">Due to the lack of 2D-3D datasets, other approaches utilize the multi-camera view and 2D pose dataset for supervision during training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib110" title="">110</a>]</cite>, or learn from synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib111" title="">111</a>]</cite> which is created by projecting SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib52" title="">52</a>]</cite> or SCAPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib112" title="">112</a>]</cite> 3D model onto 2D in-the-wild images. To further improve 3D skeleton model estimation, VideoPose3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib113" title="">113</a>]</cite> uses temporal convolution on 2D poses, thus tackling the problem of inherent depth ambiguity, where a single 2D pose may correspond to multiple 3D poses.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS2.4.1.1">II-C</span>2 </span>Model-based methods</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">These methods are currently more popular than skeleton-based approaches as they offer finer details about human appearance, which is valuable for VR and AR applications. However, human representation are often simplified by utilizing statistical 3D human models like SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib52" title="">52</a>]</cite> instead of complex mesh regression techniques, thus streamlining the entire process. The representative framework for SMPL-based HPE algorithm is shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S2.F7" title="Figure 7 ‣ II-C2 Model-based methods ‣ II-C 3D Single-Person Human Pose Estimation ‣ II Background ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="107" id="S2.F7.g1" src="x6.png" width="574"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The exemplary framework for 3D HPE based on SMPL human model. Image taken from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib53" title="">53</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.SSS2.p2">
<p class="ltx_p" id="S2.SS3.SSS2.p2.1">Efforts have been made to explore alternative representations. For instance, GraphCMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib114" title="">114</a>]</cite> uses a graph CNN to estimate SMPL parameters from nodes through regression, where each node represents a vertex of the SMPL template mesh and image feature vectors. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib115" title="">115</a>]</cite> maps CNN features to joints and vertices of the mesh model to regress them to 3D coordinates using a transformer-based network. Other approaches extend SMPL and incorporate representations for the entire body. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib116" title="">116</a>]</cite> integrates SMPL with a 3D hand model, while SMPL-X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib117" title="">117</a>]</cite> further integrates the FLAME head model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib118" title="">118</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib117" title="">117</a>]</cite> also introduce SMPLify-X, which iteratively fits SMPL-X to 2D keypoints of the body, face, and hands to generate the whole-body mesh.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p3">
<p class="ltx_p" id="S2.SS3.SSS2.p3.1">Due to the limited availability of datasets, unpaired 2D-3D motion capture (MoCap) datasets are used to supervise the rationality of estimated SMPL pose parameters in a generative adversarial manner <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib119" title="">119</a>]</cite>. Another approach to deal with insufficient data is to use videos to leverage temporal information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib120" title="">120</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib121" title="">121</a>]</cite>, along with generative-adversarial motion supervision. Another strategy involves concurrently utilizing 2D and 3D keypoints by iteratively improving the 3D mesh and measuring its projection error to 2D keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib122" title="">122</a>]</cite>. SPIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib123" title="">123</a>]</cite> utilizes this strategy to enhance the estimated outcomes within the training loop.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.4.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.5.2">3D Multi-Person Human Pose Estimation</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Multi-person methods can be categorized similarly to single-person methods. This means classification into skeleton and model-based approaches.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS4.SSS1.4.1.1">II-D</span>1 </span>Skeleton-based methods</h4>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<p class="ltx_p" id="S2.SS4.SSS1.p1.1">Similar to 2D multi-person HPE, the approaches can be classified into top-down and bottom-up methods. Top-down methods, such as LCR-net++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib124" title="">124</a>]</cite> and the work by Moon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib125" title="">125</a>]</cite>, utilize anchor-based detection. On the other hand, following the bottom-up paradigm, Zehta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib126" title="">126</a>]</cite> employ single-image volumetric heatmaps for 3D keypoint prediction and confidence scores to group limbs belonging to the same individual. To address common inter-person occlusion challenges, an approach known as Occlusion-Robust Pose Maps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib127" title="">127</a>]</cite> has been proposed. It incorporates redundant occlusion information into the part affinity maps.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS4.SSS2.4.1.1">II-D</span>2 </span>Model-based methods</h4>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.1">One of the most challenging tasks in model-based multi-person HPE is 3D mesh recovery. For instance, Zanfir et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib128" title="">128</a>]</cite> employ semantic segmentation and SMPL model fitting to tackle this challenge. In ROMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib129" title="">129</a>]</cite>, 2D heatmaps and mesh parameter maps are utilized for 2D body representation and 3D mesh localization. Building on this framework, Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib130" title="">130</a>]</cite> introduce the Bird’s-Eye-View approach to infer depth from monocular cameras and categorize humans by age to employ different representations.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p2">
<p class="ltx_p" id="S2.SS4.SSS2.p2.1">Moreover, video-based solutions integrating keypoint features, such as XNect <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib1" title="">1</a>]</cite>, provide real-time multi-person 3D human motion capture with just a single RGB camera. This algorithm effectively handles occlusion and introduces a novel CNN architecture featuring selective long and short-range skip connections for improved speed and accuracy. However, despite its effectiveness, XNect has limitations, including difficulty in handling neck occlusion, challenges in capturing extremely close interactions like hugging, and vulnerability of the identity tracker to drastic appearance changes and similar clothing.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Datasets and Metrics</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Datasets</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Datasets play a crucial role in fueling the advancement of deep learning methods for HPE. Their existence and accessibility enable the development, evaluation, and comparison of various algorithms. However, not all datasets are created equal; they vary significantly in terms of labels, annotations, and tasks they support.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In the realm of 2D HPE, standout datasets include the MPII Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib40" title="">40</a>]</cite> and the MSCOCO Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib39" title="">39</a>]</cite>. The MPII Dataset, sourced from YouTube videos, offers a diverse range of activities and annotations, including 16 2D keypoints, 3D torso keypoints, and occlusion labels. Similarly, the MSCOCO Dataset, compiled from webpage images, provides detailed annotations for 17 keypoints, along with bounding boxes and segmentation areas. Despite its focus on keypoint detection, MSCOCO supports additional tasks like object detection and panoptic segmentation. For multi-person HPE and tracking, researchers often rely on the PoseTrack Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib89" title="">89</a>]</cite>, known for its large-scale video-level annotations. This dataset has labels such as 15 2D keypoints, a person ID, and the head bounding box.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">On the 3D HPE front, datasets exhibit a wider range of annotation types. For instance, the Human3.6M Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib5" title="">5</a>]</cite>, a single-person benchmark featuring annotations in 2D, 3D, and mesh formats. It contains 3.6 million human poses across various scenarios, captured using a sophisticated sensor setup comprising RGB cameras, a time-of-flight sensor, and motion cameras. Other notable datasets like HumanEva <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib131" title="">131</a>]</cite>, MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib132" title="">132</a>]</cite>, and MoVi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib133" title="">133</a>]</cite> provide valuable poses of actors performing pre-defined actions, albeit with slight differences in action types and capture sensors across datasets. Such as 7 cameras for HumanEva, 14 cameras including 3 cameras with a top-down view for MPI-INF-3DHP, and IMU devices together with a motion capture camera for MoVi.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Notably, MoVi stands out for its inclusion of mesh annotations alongside traditional 2D and 3D keypoints, providing a richer dataset for analysis. Detailed characteristics of individuals, such as age, height, and BMI are also supplied by this dataset. For all aforementioned datasets SMPL parameters can be obtained via the MoSh++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib134" title="">134</a>]</cite>. Additionally, datasets like 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib135" title="">135</a>]</cite> contribute to better model generalization in real-world scenarios by providing annotations of in-the-wild activities, albeit with limitations in crowded scenes. 3DPW utilizes IMUs to gather 2D, 3D, and mesh annotations in this challenging environment. Other important datasets include Leeds Sports Pose (LSP) Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib136" title="">136</a>]</cite>, Frames Labeled in Cinema (FLIC) Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib137" title="">137</a>]</cite>, CrowdPose Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib81" title="">81</a>]</cite>, J-HMDB Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib138" title="">138</a>]</cite>, CMU Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib139" title="">139</a>]</cite>, AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib140" title="">140</a>]</cite>, and SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib111" title="">111</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">It’s essential to acknowledge the limitations of existing datasets, particularly their constrained environments and limited pose diversity, as many datasets primarily capture poses involving standing or walking. Models trained on such datasets may struggle with generalization, especially for complex poses like those of gymnasts. Moreover, research has shown <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib53" title="">53</a>]</cite> that actors in datasets tend to move their upper bodies more than lower limbs, impacting model generalization, particularly for lower body parts.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Metrics</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In 2D estimation, commonly used metrics include the Percentage of Correct Parts (PCP) or Keypoint (PCK), both employing an estimation threshold. PCK assesses each keypoint individually, while PCP focuses on the endpoints of a limb, ensuring they both fall within a threshold. Additionally, Average Precision (AP) and Recall (AR) are popular metrics for 2D estimation, typically evaluated at various Object Keypoint Similarity (OKS) values. These metrics are commonly presented individually for different OKS thresholds or summarized as the mean (mAP, and mAR respectively) of OKS ranging from 0.50 to 0.95 at intervals of 0.05. OKS addresses the limitation of Euclidean distance not generalizing well to different body shapes and sizes. It measures the distance between keypoints normalized by the person’s scale, with distinct normalization values per keypoint to control fall-off. Similar normalization can also be used for PCP and PCK thresholds.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">While the PCK metric is applicable in 3D HPE, the more widely used metric is the Mean Per Joint Position Error (MPJPE). MPJPE calculates the average Euclidean distance between ground truth and predictions, with poses aligned by the pelvis. Typically it is measured in millimeters. An alternative is the Procrustes Aligned MPJPE (PMPJPE), which mitigates the effects of translation, rotation, and scale through Procrustes alignment. Also known as the reconstruction error, PMPJPE places greater emphasis on reconstruction accuracy compared to MPJPE.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">SOTA Methods Overview and Comparison</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">2D Human Pose Estimation</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The SOTA methods for 2D and 3D HPE are presented in Tables <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S4.T1" title="TABLE I ‣ IV-A 2D Human Pose Estimation ‣ IV SOTA Methods Overview and Comparison ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">I</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S4.T2" title="TABLE II ‣ IV-B 3D Human Pose Estimation ‣ IV SOTA Methods Overview and Comparison ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">II</span></a>, respectively. In Table <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S4.T1" title="TABLE I ‣ IV-A 2D Human Pose Estimation ‣ IV SOTA Methods Overview and Comparison ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">I</span></a>, we compare the results of various 2D algorithms alongside their backbone architectures and input sizes. A notable observation from this table is the superior performance of top-down approaches over bottom-up approaches in multi-person HPE tasks. This discrepancy can be attributed to the inherent simplicity of top-down methodologies. However, recent trends suggest that bottom-up approaches may soon surpass their top-down counterparts. It is also noteworthy that tracking HPE algorithms exhibit inferior performance compared to those solely focusing on pose estimation. This phenomenon is observed in both top-down and bottom-up methods, respectively. The primary reason for this is the increased complexity associated with integrating tracking into pose estimation algorithms.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Evaluation of representative 2D Human Pose Estimation methods and their performance. Single-person HPE algorithms were assessed on the MPII Test Set <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib40" title="">40</a>]</cite>, while multi-person methods were evaluated on the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib39" title="">39</a>]</cite>. Additionally, multi-person HPE and tracking algorithms underwent evaluation on the PoseTrack 2017 Test Set <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib89" title="">89</a>]</cite>. Notably, PCK represents PCKh@0.5, a popular PCK variant where the matching threshold is set to 50% of the evaluated individual’s head length.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S4.T1.41">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.41.42.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.41.42.1.1">Method</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.41.42.1.2">Backbone</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.41.42.1.3">Input size</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.41.42.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.41.42.1.4.1">
<span class="ltx_p" id="S4.T1.41.42.1.4.1.1" style="width:14.2pt;">PCK</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.41.42.1.5">mAP</td>
</tr>
<tr class="ltx_tr" id="S4.T1.41.43.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" colspan="5" id="S4.T1.41.43.2.1">Single-person HPE algorithms</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.2">Tompson et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib59" title="">59</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.3">AlexNet</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1">320 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><times id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">×</annotation></semantics></math> 240</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.4.1">
<span class="ltx_p" id="S4.T1.1.1.4.1.1" style="width:14.2pt;">79.6</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.2.2.2">Carreira et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib58" title="">58</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.2.3">GoogleNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.2.1">224 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.2.2.1.m1.1"><semantics id="S4.T1.2.2.1.m1.1a"><mo id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><times id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.1.m1.1d">×</annotation></semantics></math> 224</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.2.4.1">
<span class="ltx_p" id="S4.T1.2.2.4.1.1" style="width:14.2pt;">81.3</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.2.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.3.3.2">Tompson et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib141" title="">141</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.3.3">AlexNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.3.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.3.3.1.m1.1"><semantics id="S4.T1.3.3.1.m1.1a"><mo id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><times id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.1.m1.1d">×</annotation></semantics></math> 256</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.3.3.4.1">
<span class="ltx_p" id="S4.T1.3.3.4.1.1" style="width:14.2pt;">82.0</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.3.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.4.4.2">Pishchulin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib2" title="">2</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.3">VGG</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.1">340 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.4.4.1.m1.1"><semantics id="S4.T1.4.4.1.m1.1a"><mo id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><times id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.1.m1.1d">×</annotation></semantics></math> 340</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.4.4.1">
<span class="ltx_p" id="S4.T1.4.4.4.1.1" style="width:14.2pt;">82.4</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.5.5.2">Gkioxary et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib73" title="">73</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.5.3">InceptionNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.5.1">299 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.5.5.1.m1.1"><semantics id="S4.T1.5.5.1.m1.1a"><mo id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><times id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.1.m1.1d">×</annotation></semantics></math> 299</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.5.5.4.1">
<span class="ltx_p" id="S4.T1.5.5.4.1.1" style="width:14.2pt;">86.1</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.5.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.6.6.2">Insafutdinov et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.6.3">ResNet-152</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.6.1">340 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.6.6.1.m1.1"><semantics id="S4.T1.6.6.1.m1.1a"><mo id="S4.T1.6.6.1.m1.1.1" xref="S4.T1.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.1.m1.1b"><times id="S4.T1.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.1.m1.1d">×</annotation></semantics></math> 340</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.6.6.4.1">
<span class="ltx_p" id="S4.T1.6.6.4.1.1" style="width:14.2pt;">88.5</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.6.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.7.7.2">Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib64" title="">64</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.7.3">CPM</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.7.1">368 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.7.7.1.m1.1"><semantics id="S4.T1.7.7.1.m1.1a"><mo id="S4.T1.7.7.1.m1.1.1" xref="S4.T1.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><times id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.7.1.m1.1d">×</annotation></semantics></math> 368</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.7.7.4.1">
<span class="ltx_p" id="S4.T1.7.7.4.1.1" style="width:14.2pt;">88.5</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.7.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.8.8.2">Newell et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib55" title="">55</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.8.8.3">Hourglass</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.8.8.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.8.8.1.m1.1"><semantics id="S4.T1.8.8.1.m1.1a"><mo id="S4.T1.8.8.1.m1.1.1" xref="S4.T1.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.1.m1.1b"><times id="S4.T1.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.8.1.m1.1d">×</annotation></semantics></math> 256</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.8.8.4.1">
<span class="ltx_p" id="S4.T1.8.8.4.1.1" style="width:14.2pt;">90.9</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.8.8.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.9.9.2">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib69" title="">69</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.9.9.3">Hourglass</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.9.9.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.9.9.1.m1.1"><semantics id="S4.T1.9.9.1.m1.1a"><mo id="S4.T1.9.9.1.m1.1.1" xref="S4.T1.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.1.m1.1b"><times id="S4.T1.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.9.1.m1.1d">×</annotation></semantics></math> 256</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.9.9.4.1">
<span class="ltx_p" id="S4.T1.9.9.4.1.1" style="width:14.2pt;">91.7</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.9.9.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.10.10.2">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib61" title="">61</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.3">En/Decoder</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.10.10.1.m1.1"><semantics id="S4.T1.10.10.1.m1.1a"><mo id="S4.T1.10.10.1.m1.1.1" xref="S4.T1.10.10.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.1.m1.1b"><times id="S4.T1.10.10.1.m1.1.1.cmml" xref="S4.T1.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.10.1.m1.1d">×</annotation></semantics></math> 256</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.10.10.4.1">
<span class="ltx_p" id="S4.T1.10.10.4.1.1" style="width:14.2pt;">91.9</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.11.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.11.11.2">Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib79" title="">79</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.11.11.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.11.11.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.11.11.1.m1.1"><semantics id="S4.T1.11.11.1.m1.1a"><mo id="S4.T1.11.11.1.m1.1.1" xref="S4.T1.11.11.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.1.m1.1b"><times id="S4.T1.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.11.1.m1.1d">×</annotation></semantics></math> 256</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.11.11.4.1">
<span class="ltx_p" id="S4.T1.11.11.4.1.1" style="width:14.2pt;">92.3</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.11.11.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.12.12.2">Tang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib142" title="">142</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.12.12.3">Hourglass</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.12.12.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.12.12.1.m1.1"><semantics id="S4.T1.12.12.1.m1.1a"><mo id="S4.T1.12.12.1.m1.1.1" xref="S4.T1.12.12.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.1.m1.1b"><times id="S4.T1.12.12.1.m1.1.1.cmml" xref="S4.T1.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.12.1.m1.1d">×</annotation></semantics></math> 256</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.12.12.4.1">
<span class="ltx_p" id="S4.T1.12.12.4.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.12.12.4.1.1.1">92.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.12.12.5">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.41.44.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T1.41.44.3.1">Multi-person bottom-up HPE algorithms</td>
</tr>
<tr class="ltx_tr" id="S4.T1.13.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.13.13.2">OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib45" title="">45</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.13.13.3">CMU-Net</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.13.13.1">368 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.13.13.1.m1.1"><semantics id="S4.T1.13.13.1.m1.1a"><mo id="S4.T1.13.13.1.m1.1.1" xref="S4.T1.13.13.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.1.m1.1b"><times id="S4.T1.13.13.1.m1.1.1.cmml" xref="S4.T1.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.13.13.1.m1.1d">×</annotation></semantics></math> 368</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.13.13.4.1">
<span class="ltx_p" id="S4.T1.13.13.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.13.13.5">61.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.14">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.14.14.2">Asso. Emb. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib86" title="">86</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.14.3">Hourglass</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.14.1">512 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.14.14.1.m1.1"><semantics id="S4.T1.14.14.1.m1.1a"><mo id="S4.T1.14.14.1.m1.1.1" xref="S4.T1.14.14.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.1.m1.1b"><times id="S4.T1.14.14.1.m1.1.1.cmml" xref="S4.T1.14.14.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.14.14.1.m1.1d">×</annotation></semantics></math> 512</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.14.14.4.1">
<span class="ltx_p" id="S4.T1.14.14.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.14.5">65.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.15.15">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.15.15.2">PifPaf <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib4" title="">4</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.15.15.3">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.15.15.1">401 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.15.15.1.m1.1"><semantics id="S4.T1.15.15.1.m1.1a"><mo id="S4.T1.15.15.1.m1.1.1" xref="S4.T1.15.15.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.1.m1.1b"><times id="S4.T1.15.15.1.m1.1.1.cmml" xref="S4.T1.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.15.15.1.m1.1d">×</annotation></semantics></math> 401</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.15.15.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.15.15.4.1">
<span class="ltx_p" id="S4.T1.15.15.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.15.15.5">66.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.16.16">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.16.16.2">PersonLab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib88" title="">88</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.16.16.3">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.16.16.1">801 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.16.16.1.m1.1"><semantics id="S4.T1.16.16.1.m1.1a"><mo id="S4.T1.16.16.1.m1.1.1" xref="S4.T1.16.16.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.1.m1.1b"><times id="S4.T1.16.16.1.m1.1.1.cmml" xref="S4.T1.16.16.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.16.16.1.m1.1d">×</annotation></semantics></math> 801</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.16.16.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.16.16.4.1">
<span class="ltx_p" id="S4.T1.16.16.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.16.16.5">68.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.17.17">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.17.17.2">HigherHRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib87" title="">87</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.17.17.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.17.17.1">640 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.17.17.1.m1.1"><semantics id="S4.T1.17.17.1.m1.1a"><mo id="S4.T1.17.17.1.m1.1.1" xref="S4.T1.17.17.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.17.17.1.m1.1b"><times id="S4.T1.17.17.1.m1.1.1.cmml" xref="S4.T1.17.17.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.17.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.17.17.1.m1.1d">×</annotation></semantics></math> 640</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.17.17.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.17.17.4.1">
<span class="ltx_p" id="S4.T1.17.17.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.17.17.5">70.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.18.18">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.18.18.2">DEKR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib143" title="">143</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.18.18.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.18.18.1">640 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.18.18.1.m1.1"><semantics id="S4.T1.18.18.1.m1.1a"><mo id="S4.T1.18.18.1.m1.1.1" xref="S4.T1.18.18.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.18.18.1.m1.1b"><times id="S4.T1.18.18.1.m1.1.1.cmml" xref="S4.T1.18.18.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.18.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.18.18.1.m1.1d">×</annotation></semantics></math> 640</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.18.18.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.18.18.4.1">
<span class="ltx_p" id="S4.T1.18.18.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.18.18.5">71.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.19">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.19.19.2">SIMPLE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib144" title="">144</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.19.19.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.19.19.1">512 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.19.19.1.m1.1"><semantics id="S4.T1.19.19.1.m1.1a"><mo id="S4.T1.19.19.1.m1.1.1" xref="S4.T1.19.19.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.19.19.1.m1.1b"><times id="S4.T1.19.19.1.m1.1.1.cmml" xref="S4.T1.19.19.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.19.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.19.19.1.m1.1d">×</annotation></semantics></math> 512</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.19.19.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.19.19.4.1">
<span class="ltx_p" id="S4.T1.19.19.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.19.19.5">71.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.20.20">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.20.20.2">CenterGroup <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib145" title="">145</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.20.20.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.20.20.1">512 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.20.20.1.m1.1"><semantics id="S4.T1.20.20.1.m1.1a"><mo id="S4.T1.20.20.1.m1.1.1" xref="S4.T1.20.20.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.20.20.1.m1.1b"><times id="S4.T1.20.20.1.m1.1.1.cmml" xref="S4.T1.20.20.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.20.20.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.20.20.1.m1.1d">×</annotation></semantics></math> 512</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.20.20.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.20.20.4.1">
<span class="ltx_p" id="S4.T1.20.20.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.20.20.5">71.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.21.21">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.21.21.2">SWAHR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib146" title="">146</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.21.21.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.21.21.1">640 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.21.21.1.m1.1"><semantics id="S4.T1.21.21.1.m1.1a"><mo id="S4.T1.21.21.1.m1.1.1" xref="S4.T1.21.21.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.21.21.1.m1.1b"><times id="S4.T1.21.21.1.m1.1.1.cmml" xref="S4.T1.21.21.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.21.21.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.21.21.1.m1.1d">×</annotation></semantics></math> 640</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.21.21.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.21.21.4.1">
<span class="ltx_p" id="S4.T1.21.21.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.21.21.5"><span class="ltx_text ltx_font_bold" id="S4.T1.21.21.5.1">72.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.41.45.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T1.41.45.4.1">Multi-person top-down HPE algorithms</td>
</tr>
<tr class="ltx_tr" id="S4.T1.22.22">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.22.22.2">G-RMI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib75" title="">75</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.22.22.3">ResNe</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.22.22.1">353 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.22.22.1.m1.1"><semantics id="S4.T1.22.22.1.m1.1a"><mo id="S4.T1.22.22.1.m1.1.1" xref="S4.T1.22.22.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.22.22.1.m1.1b"><times id="S4.T1.22.22.1.m1.1.1.cmml" xref="S4.T1.22.22.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.22.22.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.22.22.1.m1.1d">×</annotation></semantics></math> 257</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.22.22.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.22.22.4.1">
<span class="ltx_p" id="S4.T1.22.22.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.22.22.5">64.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.23.23">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.23.23.2">Integral Regre. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib100" title="">100</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.23.23.3">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.23.23.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.23.23.1.m1.1"><semantics id="S4.T1.23.23.1.m1.1a"><mo id="S4.T1.23.23.1.m1.1.1" xref="S4.T1.23.23.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.23.23.1.m1.1b"><times id="S4.T1.23.23.1.m1.1.1.cmml" xref="S4.T1.23.23.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.23.23.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.23.23.1.m1.1d">×</annotation></semantics></math> 256</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.23.23.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.23.23.4.1">
<span class="ltx_p" id="S4.T1.23.23.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.23.23.5">67.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.24.24">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.24.24.2">CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib78" title="">78</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.24.24.3">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.24.24.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.24.24.1.m1.1"><semantics id="S4.T1.24.24.1.m1.1a"><mo id="S4.T1.24.24.1.m1.1.1" xref="S4.T1.24.24.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.24.24.1.m1.1b"><times id="S4.T1.24.24.1.m1.1.1.cmml" xref="S4.T1.24.24.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.24.24.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.24.24.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.24.24.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.24.24.4.1">
<span class="ltx_p" id="S4.T1.24.24.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.24.24.5">72.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.25.25">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.25.25.2">RMPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib80" title="">80</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.25.25.3">PyraNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.25.25.1">320 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.25.25.1.m1.1"><semantics id="S4.T1.25.25.1.m1.1a"><mo id="S4.T1.25.25.1.m1.1.1" xref="S4.T1.25.25.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.25.25.1.m1.1b"><times id="S4.T1.25.25.1.m1.1.1.cmml" xref="S4.T1.25.25.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.25.25.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.25.25.1.m1.1d">×</annotation></semantics></math> 256</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.25.25.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.25.25.4.1">
<span class="ltx_p" id="S4.T1.25.25.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.25.25.5">72.3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.26.26">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.26.26.2">SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib77" title="">77</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.26.26.3">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.26.26.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.26.26.1.m1.1"><semantics id="S4.T1.26.26.1.m1.1a"><mo id="S4.T1.26.26.1.m1.1.1" xref="S4.T1.26.26.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.26.26.1.m1.1b"><times id="S4.T1.26.26.1.m1.1.1.cmml" xref="S4.T1.26.26.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.26.26.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.26.26.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.26.26.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.26.26.4.1">
<span class="ltx_p" id="S4.T1.26.26.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.26.26.5">73.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.27.27">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.27.27.2">MSPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib147" title="">147</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.27.27.3">MSPN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.27.27.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.27.27.1.m1.1"><semantics id="S4.T1.27.27.1.m1.1a"><mo id="S4.T1.27.27.1.m1.1.1" xref="S4.T1.27.27.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.27.27.1.m1.1b"><times id="S4.T1.27.27.1.m1.1.1.cmml" xref="S4.T1.27.27.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.27.27.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.27.27.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.27.27.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.27.27.4.1">
<span class="ltx_p" id="S4.T1.27.27.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.27.27.5">76.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.28.28">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.28.28.2">DARK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib148" title="">148</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.28.28.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.28.28.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.28.28.1.m1.1"><semantics id="S4.T1.28.28.1.m1.1a"><mo id="S4.T1.28.28.1.m1.1.1" xref="S4.T1.28.28.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.28.28.1.m1.1b"><times id="S4.T1.28.28.1.m1.1.1.cmml" xref="S4.T1.28.28.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.28.28.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.28.28.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.28.28.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.28.28.4.1">
<span class="ltx_p" id="S4.T1.28.28.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.28.28.5">76.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.29.29">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.29.29.2">UDP [64]</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.29.29.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.29.29.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.29.29.1.m1.1"><semantics id="S4.T1.29.29.1.m1.1a"><mo id="S4.T1.29.29.1.m1.1.1" xref="S4.T1.29.29.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.29.29.1.m1.1b"><times id="S4.T1.29.29.1.m1.1.1.cmml" xref="S4.T1.29.29.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.29.29.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.29.29.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.29.29.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.29.29.4.1">
<span class="ltx_p" id="S4.T1.29.29.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.29.29.5">76.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.30.30">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.30.30.2">PoseFix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib149" title="">149</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.30.30.3">HR+ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.30.30.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.30.30.1.m1.1"><semantics id="S4.T1.30.30.1.m1.1a"><mo id="S4.T1.30.30.1.m1.1.1" xref="S4.T1.30.30.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.30.30.1.m1.1b"><times id="S4.T1.30.30.1.m1.1.1.cmml" xref="S4.T1.30.30.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.30.30.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.30.30.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.30.30.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.30.30.4.1">
<span class="ltx_p" id="S4.T1.30.30.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.30.30.5">76.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.31.31">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.31.31.2">Graph-PCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib150" title="">150</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.31.31.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.31.31.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.31.31.1.m1.1"><semantics id="S4.T1.31.31.1.m1.1a"><mo id="S4.T1.31.31.1.m1.1.1" xref="S4.T1.31.31.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.31.31.1.m1.1b"><times id="S4.T1.31.31.1.m1.1.1.cmml" xref="S4.T1.31.31.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.31.31.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.31.31.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.31.31.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.31.31.4.1">
<span class="ltx_p" id="S4.T1.31.31.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.31.31.5">76.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.32.32">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.32.32.2">RSN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib151" title="">151</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.32.32.3">4-RSN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.32.32.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.32.32.1.m1.1"><semantics id="S4.T1.32.32.1.m1.1a"><mo id="S4.T1.32.32.1.m1.1.1" xref="S4.T1.32.32.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.32.32.1.m1.1b"><times id="S4.T1.32.32.1.m1.1.1.cmml" xref="S4.T1.32.32.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.32.32.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.32.32.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.32.32.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.32.32.4.1">
<span class="ltx_p" id="S4.T1.32.32.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.32.32.5"><span class="ltx_text ltx_font_bold" id="S4.T1.32.32.5.1">78.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.41.46.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T1.41.46.5.1">Bottom-down multi-person HPE and tracking algorithms</td>
</tr>
<tr class="ltx_tr" id="S4.T1.33.33">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.33.33.2">ArtTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib92" title="">92</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.33.33.3">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.33.33.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.33.33.1.m1.1"><semantics id="S4.T1.33.33.1.m1.1a"><mo id="S4.T1.33.33.1.m1.1.1" xref="S4.T1.33.33.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.33.33.1.m1.1b"><times id="S4.T1.33.33.1.m1.1.1.cmml" xref="S4.T1.33.33.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.33.33.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.33.33.1.m1.1d">×</annotation></semantics></math> 256</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.33.33.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.33.33.4.1">
<span class="ltx_p" id="S4.T1.33.33.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.33.33.5">59.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.34.34">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.34.34.2">PoseTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib89" title="">89</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.34.34.3">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.34.34.1">340 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.34.34.1.m1.1"><semantics id="S4.T1.34.34.1.m1.1a"><mo id="S4.T1.34.34.1.m1.1.1" xref="S4.T1.34.34.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.34.34.1.m1.1b"><times id="S4.T1.34.34.1.m1.1.1.cmml" xref="S4.T1.34.34.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.34.34.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.34.34.1.m1.1d">×</annotation></semantics></math> 340</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.34.34.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.34.34.4.1">
<span class="ltx_p" id="S4.T1.34.34.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.34.34.5">59.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.35.35">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.35.35.2">JointFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib96" title="">96</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.35.35.3">SVG</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.35.35.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.35.35.1.m1.1"><semantics id="S4.T1.35.35.1.m1.1a"><mo id="S4.T1.35.35.1.m1.1.1" xref="S4.T1.35.35.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.35.35.1.m1.1b"><times id="S4.T1.35.35.1.m1.1.1.cmml" xref="S4.T1.35.35.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.35.35.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.35.35.1.m1.1d">×</annotation></semantics></math> 192</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.35.35.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.35.35.4.1">
<span class="ltx_p" id="S4.T1.35.35.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.35.35.5">63.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.36.36">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.36.36.2">STAF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib94" title="">94</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.36.36.3">VGG</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.36.36.1">368 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.36.36.1.m1.1"><semantics id="S4.T1.36.36.1.m1.1a"><mo id="S4.T1.36.36.1.m1.1.1" xref="S4.T1.36.36.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.36.36.1.m1.1b"><times id="S4.T1.36.36.1.m1.1.1.cmml" xref="S4.T1.36.36.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.36.36.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.36.36.1.m1.1d">×</annotation></semantics></math> 368</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.36.36.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.36.36.4.1">
<span class="ltx_p" id="S4.T1.36.36.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.36.36.5"><span class="ltx_text ltx_font_bold" id="S4.T1.36.36.5.1">70.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.41.47.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T1.41.47.6.1">Top-down multi-person HPE and tracking algorithms</td>
</tr>
<tr class="ltx_tr" id="S4.T1.37.37">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.37.37.2">Detect-Track <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib90" title="">90</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.37.37.3">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.37.37.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.37.37.1.m1.1"><semantics id="S4.T1.37.37.1.m1.1a"><mo id="S4.T1.37.37.1.m1.1.1" xref="S4.T1.37.37.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.37.37.1.m1.1b"><times id="S4.T1.37.37.1.m1.1.1.cmml" xref="S4.T1.37.37.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.37.37.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.37.37.1.m1.1d">×</annotation></semantics></math> 256</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.37.37.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.37.37.4.1">
<span class="ltx_p" id="S4.T1.37.37.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.37.37.5">59.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.41.48.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.41.48.7.1">PoseFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib93" title="">93</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.41.48.7.2">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.41.48.7.3">-</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.41.48.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.41.48.7.4.1">
<span class="ltx_p" id="S4.T1.41.48.7.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.41.48.7.5">63.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.38.38">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.38.38.2">PGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib152" title="">152</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.38.38.3">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.38.38.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.38.38.1.m1.1"><semantics id="S4.T1.38.38.1.m1.1a"><mo id="S4.T1.38.38.1.m1.1.1" xref="S4.T1.38.38.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.38.38.1.m1.1b"><times id="S4.T1.38.38.1.m1.1.1.cmml" xref="S4.T1.38.38.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.38.38.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.38.38.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.38.38.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.38.38.4.1">
<span class="ltx_p" id="S4.T1.38.38.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.38.38.5">72.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.39.39">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.39.39.2">DetTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib91" title="">91</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.39.39.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.39.39.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.39.39.1.m1.1"><semantics id="S4.T1.39.39.1.m1.1a"><mo id="S4.T1.39.39.1.m1.1.1" xref="S4.T1.39.39.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.39.39.1.m1.1b"><times id="S4.T1.39.39.1.m1.1.1.cmml" xref="S4.T1.39.39.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.39.39.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.39.39.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.39.39.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.39.39.4.1">
<span class="ltx_p" id="S4.T1.39.39.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.39.39.5">74.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.40.40">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.40.40.2">FlowTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib77" title="">77</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.40.40.3">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.40.40.1">256 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.40.40.1.m1.1"><semantics id="S4.T1.40.40.1.m1.1a"><mo id="S4.T1.40.40.1.m1.1.1" xref="S4.T1.40.40.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.40.40.1.m1.1b"><times id="S4.T1.40.40.1.m1.1.1.cmml" xref="S4.T1.40.40.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.40.40.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.40.40.1.m1.1d">×</annotation></semantics></math> 192</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.40.40.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.40.40.4.1">
<span class="ltx_p" id="S4.T1.40.40.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.40.40.5">74.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.41.41">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" id="S4.T1.41.41.2">DCPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib153" title="">153</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.41.41.3">HRNet</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.41.41.1">384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.41.41.1.m1.1"><semantics id="S4.T1.41.41.1.m1.1a"><mo id="S4.T1.41.41.1.m1.1.1" xref="S4.T1.41.41.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.41.41.1.m1.1b"><times id="S4.T1.41.41.1.m1.1.1.cmml" xref="S4.T1.41.41.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.41.41.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.41.41.1.m1.1d">×</annotation></semantics></math> 288</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T1.41.41.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.41.41.4.1">
<span class="ltx_p" id="S4.T1.41.41.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.41.41.5"><span class="ltx_text ltx_font_bold" id="S4.T1.41.41.5.1">79.2</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Furthermore, it’s evident that pioneering papers that introduced innovative approaches, like OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib45" title="">45</a>]</cite> or Associative Embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib86" title="">86</a>]</cite> in multi-person bottom-up HPE, often achieve lower scores compared to more refined methods that build upon them. These refined methods represent the current SOTA solutions. Among them, two notable papers are CenterGroup <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib145" title="">145</a>]</cite> and SWAHR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib146" title="">146</a>]</cite>. CenterGroup introduces an attention-based approach for multi-person HPE, focusing on accurately grouping keypoints based on attention mechanisms centred around keypoint centres. By employing attention mechanisms, the model dynamically weighs the importance of different keypoints relative to their spatial relationships, facilitating robust keypoint grouping. Experimental results demonstrate that the proposed method outperforms existing approaches, particularly in scenarios with crowded scenes and occlusions. On the other hand, SWAHR achieves even better results by rethinking the heatmap regression method. This novel strategy leverages both high-resolution and low-resolution representations to capture detailed pose information effectively. The proposed method achieves superior accuracy while maintaining computational efficiency. However, SWAHR’s reliance on heatmap regression may introduce quantization errors during coordinate conversion.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">For top-down approaches, which typically offer higher accuracy but slower processing times compared to bottom-up methods, PoseFix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib149" title="">149</a>]</cite> stands out for its exceptional performance. It is a model-agnostic pose refinement network aimed at improving the accuracy of existing HPE models. Unlike model-specific approaches, PoseFix is designed to refine the predictions of any pose estimator by learning to correct errors and inconsistencies in the initial estimations. By using both global context and local information, PoseFix effectively refines pose predictions, enhancing the overall quality of the estimated human poses. Nonetheless, its effectiveness hinges on the quality of the initial estimations; inaccurate or erroneous inputs may impede its ability to deliver optimal refinements. Conversely, Graph-PCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib150" title="">150</a>]</cite> introduces a two-stage approach, leveraging a graph-based refinement stage after initial pose estimation, resulting in better-matched inputs for refinement. Thus it achieves even better results. However, the computational overhead of both methods might pose challenges, particularly in real-time applications, even despite their effectiveness.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Among the single-person HPE algorithms, Adversarial PoseNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib61" title="">61</a>]</cite> deserves some attention. It presents a novel CNN architecture that considers the structural relationships between body joints, resulting in more accurate pose estimations. Through adversarial training, the model generates realistic and diverse poses, enhancing its generalization performance. The network learns hierarchical features, capturing intricate pose details while maintaining robustness to scale variations and occlusions. However, its computational complexity might limit its applicability in resource-constrained or real-time settings. By the publication time, it was the SOTA model. However, further validation across diverse datasets and scenarios is necessary to fully understand its generalization and robustness. Later a better-performing method by Tang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib142" title="">142</a>]</cite> emerged. The authors demonstrate that incorporating part-specific features enhances the performance, particularly in scenarios with occlusions and complex backgrounds. By focusing on related body parts, the proposed approach improves the model’s ability to capture spatial dependencies and contextual information, leading to more accurate pose estimations.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">While increased complexity often hampers accuracy in HPE methods with tracking, DCPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib153" title="">153</a>]</cite> challenges this norm by achieving superior results compared to other multi-person HPE algorithms. Its two-stage top-down architecture, featuring a coarse pose estimations network followed by a refinement network, contributes to its success. On the other hand, STAF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib94" title="">94</a>]</cite> presents an intriguing bottom-up approach inspired by Openpose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib45" title="">45</a>]</cite>. It utilizes recurrent spatio-temporal affinity fields to model the associations between detections across frames. By iteratively refining the affinity fields using recurrent NN, the model effectively captures temporal dependencies while maintaining computational efficiency. While STAF demonstrates SOTA performance in accuracy and speed, its reliance on spatial information leads to sub-optimal initial accuracy, with optimal results achieved after longer frame sequences.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">3D Human Pose Estimation</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">On the contrary, Table <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#S4.T2" title="TABLE II ‣ IV-B 3D Human Pose Estimation ‣ IV SOTA Methods Overview and Comparison ‣ Human Modelling and Pose Estimation Overview"><span class="ltx_text ltx_ref_tag">II</span></a> illustrates that simpler 3D HPE techniques tend to outperform more intricate mesh-based 3D HPE methods. This observation is evident from the MPJPE and PMPJPE results on the Human3.6M dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib5" title="">5</a>]</cite>. However, mesh-based algorithms demonstrate promising results and generalization capabilities on challenging in-the-wild datasets like 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib135" title="">135</a>]</cite>. Considering these factors, along with the early stages of development for these models, substantial progress can be anticipated in this domain. Hence, these research directions hold significant promise for achieving notable advancements in addressing real-world challenges in HPE.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Evaluation of representative 3D Human Pose, and Mesh Estimation methods on Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib5" title="">5</a>]</cite>, HumanEva-I <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib131" title="">131</a>]</cite>, 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib135" title="">135</a>]</cite> datasets.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.1.1.1">
<span class="ltx_p" id="S4.T2.1.1.1.1.1.1" style="width:68.3pt;">Method</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t" colspan="2" id="S4.T2.1.1.1.2">Human3.6M</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.1.3.1">
<span class="ltx_p" id="S4.T2.1.1.1.3.1.1" style="width:42.7pt;">HumanEva-I</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t" colspan="2" id="S4.T2.1.1.1.4">3DPW</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.1.1">
<span class="ltx_p" id="S4.T2.1.2.2.1.1.1" style="width:68.3pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.2.1">
<span class="ltx_p" id="S4.T2.1.2.2.2.1.1" style="width:22.8pt;">MPJPE</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.3.1">
<span class="ltx_p" id="S4.T2.1.2.2.3.1.1" style="width:24.2pt;">PMPJPE</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.4.1">
<span class="ltx_p" id="S4.T2.1.2.2.4.1.1" style="width:42.7pt;">PMPJPE</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.5.1">
<span class="ltx_p" id="S4.T2.1.2.2.5.1.1" style="width:22.8pt;">MPJPE</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.2.2.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.6.1">
<span class="ltx_p" id="S4.T2.1.2.2.6.1.1" style="width:24.2pt;">PMPJPE</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_tt" colspan="6" id="S4.T2.1.3.3.1">3D HPE methods</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.1.1">
<span class="ltx_p" id="S4.T2.1.4.4.1.1.1" style="width:68.3pt;">Zhou et al. <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib154" title="">154</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.2.1">
<span class="ltx_p" id="S4.T2.1.4.4.2.1.1" style="width:22.8pt;">113.0</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.3.1">
<span class="ltx_p" id="S4.T2.1.4.4.3.1.1" style="width:24.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.4.1">
<span class="ltx_p" id="S4.T2.1.4.4.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.5.1">
<span class="ltx_p" id="S4.T2.1.4.4.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.4.4.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.6.1">
<span class="ltx_p" id="S4.T2.1.4.4.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.1.1">
<span class="ltx_p" id="S4.T2.1.5.5.1.1.1" style="width:68.3pt;">CHP <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib47" title="">47</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.2.1">
<span class="ltx_p" id="S4.T2.1.5.5.2.1.1" style="width:22.8pt;">92.4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.3.1">
<span class="ltx_p" id="S4.T2.1.5.5.3.1.1" style="width:24.2pt;">59.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.4.1">
<span class="ltx_p" id="S4.T2.1.5.5.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.5.1">
<span class="ltx_p" id="S4.T2.1.5.5.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.5.5.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.6.1">
<span class="ltx_p" id="S4.T2.1.5.5.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.1.1">
<span class="ltx_p" id="S4.T2.1.6.6.1.1.1" style="width:68.3pt;">Tome et al. <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib107" title="">107</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.2.1">
<span class="ltx_p" id="S4.T2.1.6.6.2.1.1" style="width:22.8pt;">88.4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.3.1">
<span class="ltx_p" id="S4.T2.1.6.6.3.1.1" style="width:24.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.4.1">
<span class="ltx_p" id="S4.T2.1.6.6.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.5.1">
<span class="ltx_p" id="S4.T2.1.6.6.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.6.6.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.6.1">
<span class="ltx_p" id="S4.T2.1.6.6.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.1.1">
<span class="ltx_p" id="S4.T2.1.7.7.1.1.1" style="width:68.3pt;">C2F <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib98" title="">98</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.2.1">
<span class="ltx_p" id="S4.T2.1.7.7.2.1.1" style="width:22.8pt;">71.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.3.1">
<span class="ltx_p" id="S4.T2.1.7.7.3.1.1" style="width:24.2pt;">51.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.4.1">
<span class="ltx_p" id="S4.T2.1.7.7.4.1.1" style="width:42.7pt;">25.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.5.1">
<span class="ltx_p" id="S4.T2.1.7.7.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.7.7.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.6.1">
<span class="ltx_p" id="S4.T2.1.7.7.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.8.1.1">
<span class="ltx_p" id="S4.T2.1.8.8.1.1.1" style="width:68.3pt;">IHP <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib100" title="">100</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.8.2.1">
<span class="ltx_p" id="S4.T2.1.8.8.2.1.1" style="width:22.8pt;">64.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.8.3.1">
<span class="ltx_p" id="S4.T2.1.8.8.3.1.1" style="width:24.2pt;">49.6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.8.4.1">
<span class="ltx_p" id="S4.T2.1.8.8.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.8.5.1">
<span class="ltx_p" id="S4.T2.1.8.8.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.8.8.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.8.6.1">
<span class="ltx_p" id="S4.T2.1.8.8.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.9.1.1">
<span class="ltx_p" id="S4.T2.1.9.9.1.1.1" style="width:68.3pt;">Martinez et al. <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib101" title="">101</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.9.2.1">
<span class="ltx_p" id="S4.T2.1.9.9.2.1.1" style="width:22.8pt;">62.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.9.3.1">
<span class="ltx_p" id="S4.T2.1.9.9.3.1.1" style="width:24.2pt;">47.7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.9.4.1">
<span class="ltx_p" id="S4.T2.1.9.9.4.1.1" style="width:42.7pt;">24.6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.9.9.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.9.5.1">
<span class="ltx_p" id="S4.T2.1.9.9.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.9.9.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.9.6.1">
<span class="ltx_p" id="S4.T2.1.9.9.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.10.10.1.1">
<span class="ltx_p" id="S4.T2.1.10.10.1.1.1" style="width:68.3pt;">SemGCN <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib109" title="">109</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.10.10.2.1">
<span class="ltx_p" id="S4.T2.1.10.10.2.1.1" style="width:22.8pt;">57.6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.10.10.3.1">
<span class="ltx_p" id="S4.T2.1.10.10.3.1.1" style="width:24.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.10.10.4.1">
<span class="ltx_p" id="S4.T2.1.10.10.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.10.10.5.1">
<span class="ltx_p" id="S4.T2.1.10.10.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.10.10.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.10.10.6.1">
<span class="ltx_p" id="S4.T2.1.10.10.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.11.11.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.11.11.1.1">
<span class="ltx_p" id="S4.T2.1.11.11.1.1.1" style="width:68.3pt;">Pavlakos et al. <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib103" title="">103</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.11.11.2.1">
<span class="ltx_p" id="S4.T2.1.11.11.2.1.1" style="width:22.8pt;">56.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.11.11.3.1">
<span class="ltx_p" id="S4.T2.1.11.11.3.1.1" style="width:24.2pt;">41.8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.11.11.4.1">
<span class="ltx_p" id="S4.T2.1.11.11.4.1.1" style="width:42.7pt;">18.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.11.11.5.1">
<span class="ltx_p" id="S4.T2.1.11.11.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.11.11.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.11.11.6.1">
<span class="ltx_p" id="S4.T2.1.11.11.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.12.12.1.1">
<span class="ltx_p" id="S4.T2.1.12.12.1.1.1" style="width:68.3pt;">3DMPPE <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib125" title="">125</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.12.12.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.12.12.2.1">
<span class="ltx_p" id="S4.T2.1.12.12.2.1.1" style="width:22.8pt;">54.4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.12.12.3.1">
<span class="ltx_p" id="S4.T2.1.12.12.3.1.1" style="width:24.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.12.12.4.1">
<span class="ltx_p" id="S4.T2.1.12.12.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.12.12.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.12.12.5.1">
<span class="ltx_p" id="S4.T2.1.12.12.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.12.12.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.12.12.6.1">
<span class="ltx_p" id="S4.T2.1.12.12.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.13.13.1.1">
<span class="ltx_p" id="S4.T2.1.13.13.1.1.1" style="width:68.3pt;">Luvizon et al. <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib65" title="">65</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.13.13.2.1">
<span class="ltx_p" id="S4.T2.1.13.13.2.1.1" style="width:22.8pt;">53.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.13.13.3.1">
<span class="ltx_p" id="S4.T2.1.13.13.3.1.1" style="width:24.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.13.13.4.1">
<span class="ltx_p" id="S4.T2.1.13.13.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.13.13.5.1">
<span class="ltx_p" id="S4.T2.1.13.13.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.13.13.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.13.13.6.1">
<span class="ltx_p" id="S4.T2.1.13.13.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.14.14">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.14.14.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.14.14.1.1">
<span class="ltx_p" id="S4.T2.1.14.14.1.1.1" style="width:68.3pt;">VideoPose3D <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib113" title="">113</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.14.14.2.1">
<span class="ltx_p" id="S4.T2.1.14.14.2.1.1" style="width:22.8pt;">46.8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.14.14.3.1">
<span class="ltx_p" id="S4.T2.1.14.14.3.1.1" style="width:24.2pt;">36.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.14.14.4.1">
<span class="ltx_p" id="S4.T2.1.14.14.4.1.1" style="width:42.7pt;">19.7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.14.14.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.14.14.5.1">
<span class="ltx_p" id="S4.T2.1.14.14.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.14.14.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.14.14.6.1">
<span class="ltx_p" id="S4.T2.1.14.14.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.15.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.15.15.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.15.15.1.1">
<span class="ltx_p" id="S4.T2.1.15.15.1.1.1" style="width:68.3pt;">Xu et al. <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib48" title="">48</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.15.15.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.15.15.2.1">
<span class="ltx_p" id="S4.T2.1.15.15.2.1.1" style="width:22.8pt;">45.6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.15.15.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.15.15.3.1">
<span class="ltx_p" id="S4.T2.1.15.15.3.1.1" style="width:24.2pt;">36.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.15.15.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.15.15.4.1">
<span class="ltx_p" id="S4.T2.1.15.15.4.1.1" style="width:42.7pt;">15.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.15.15.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.15.15.5.1">
<span class="ltx_p" id="S4.T2.1.15.15.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.15.15.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.15.15.6.1">
<span class="ltx_p" id="S4.T2.1.15.15.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.16.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.16.16.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.16.16.1.1">
<span class="ltx_p" id="S4.T2.1.16.16.1.1.1" style="width:68.3pt;">Liu et al. <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib155" title="">155</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.16.16.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.16.16.2.1">
<span class="ltx_p" id="S4.T2.1.16.16.2.1.1" style="width:22.8pt;">45.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.16.16.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.16.16.3.1">
<span class="ltx_p" id="S4.T2.1.16.16.3.1.1" style="width:24.2pt;">35.6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.16.16.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.16.16.4.1">
<span class="ltx_p" id="S4.T2.1.16.16.4.1.1" style="width:42.7pt;">15.4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.16.16.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.16.16.5.1">
<span class="ltx_p" id="S4.T2.1.16.16.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.16.16.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.16.16.6.1">
<span class="ltx_p" id="S4.T2.1.16.16.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.17.17">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.17.17.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.17.17.1.1">
<span class="ltx_p" id="S4.T2.1.17.17.1.1.1" style="width:68.3pt;">OANet <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib50" title="">50</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.17.17.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.17.17.2.1">
<span class="ltx_p" id="S4.T2.1.17.17.2.1.1" style="width:22.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.17.17.2.1.1.1">42.9</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.17.17.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.17.17.3.1">
<span class="ltx_p" id="S4.T2.1.17.17.3.1.1" style="width:24.2pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.17.17.3.1.1.1">32.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.17.17.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.17.17.4.1">
<span class="ltx_p" id="S4.T2.1.17.17.4.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.17.17.4.1.1.1">14.3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.17.17.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.17.17.5.1">
<span class="ltx_p" id="S4.T2.1.17.17.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.17.17.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.17.17.6.1">
<span class="ltx_p" id="S4.T2.1.17.17.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.18.18">
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t" colspan="6" id="S4.T2.1.18.18.1">3D human mesh estimation methods</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.19.19">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.19.19.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.19.19.1.1">
<span class="ltx_p" id="S4.T2.1.19.19.1.1.1" style="width:68.3pt;">SMPLify <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib122" title="">122</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.19.19.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.19.19.2.1">
<span class="ltx_p" id="S4.T2.1.19.19.2.1.1" style="width:22.8pt;">82.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.19.19.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.19.19.3.1">
<span class="ltx_p" id="S4.T2.1.19.19.3.1.1" style="width:24.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.19.19.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.19.19.4.1">
<span class="ltx_p" id="S4.T2.1.19.19.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.19.19.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.19.19.5.1">
<span class="ltx_p" id="S4.T2.1.19.19.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.19.19.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.19.19.6.1">
<span class="ltx_p" id="S4.T2.1.19.19.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.20.20">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.20.20.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.20.20.1.1">
<span class="ltx_p" id="S4.T2.1.20.20.1.1.1" style="width:68.3pt;">HMR <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib119" title="">119</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.20.20.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.20.20.2.1">
<span class="ltx_p" id="S4.T2.1.20.20.2.1.1" style="width:22.8pt;">87.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.20.20.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.20.20.3.1">
<span class="ltx_p" id="S4.T2.1.20.20.3.1.1" style="width:24.2pt;">58.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.20.20.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.20.20.4.1">
<span class="ltx_p" id="S4.T2.1.20.20.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.20.20.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.20.20.5.1">
<span class="ltx_p" id="S4.T2.1.20.20.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.20.20.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.20.20.6.1">
<span class="ltx_p" id="S4.T2.1.20.20.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.21.21">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.21.21.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.21.21.1.1">
<span class="ltx_p" id="S4.T2.1.21.21.1.1.1" style="width:68.3pt;">Human dynamics <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib121" title="">121</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.21.21.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.21.21.2.1">
<span class="ltx_p" id="S4.T2.1.21.21.2.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.21.21.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.21.21.3.1">
<span class="ltx_p" id="S4.T2.1.21.21.3.1.1" style="width:24.2pt;">56.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.21.21.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.21.21.4.1">
<span class="ltx_p" id="S4.T2.1.21.21.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.21.21.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.21.21.5.1">
<span class="ltx_p" id="S4.T2.1.21.21.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.21.21.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.21.21.6.1">
<span class="ltx_p" id="S4.T2.1.21.21.6.1.1" style="width:24.2pt;">72.6</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.22.22">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.22.22.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.22.22.1.1">
<span class="ltx_p" id="S4.T2.1.22.22.1.1.1" style="width:68.3pt;">GraphCMR <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib114" title="">114</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.22.22.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.22.22.2.1">
<span class="ltx_p" id="S4.T2.1.22.22.2.1.1" style="width:22.8pt;">71.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.22.22.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.22.22.3.1">
<span class="ltx_p" id="S4.T2.1.22.22.3.1.1" style="width:24.2pt;">50.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.22.22.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.22.22.4.1">
<span class="ltx_p" id="S4.T2.1.22.22.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.22.22.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.22.22.5.1">
<span class="ltx_p" id="S4.T2.1.22.22.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.22.22.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.22.22.6.1">
<span class="ltx_p" id="S4.T2.1.22.22.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.23.23">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.23.23.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.23.23.1.1">
<span class="ltx_p" id="S4.T2.1.23.23.1.1.1" style="width:68.3pt;">SPIN <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib123" title="">123</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.23.23.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.23.23.2.1">
<span class="ltx_p" id="S4.T2.1.23.23.2.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.23.23.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.23.23.3.1">
<span class="ltx_p" id="S4.T2.1.23.23.3.1.1" style="width:24.2pt;">41.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.23.23.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.23.23.4.1">
<span class="ltx_p" id="S4.T2.1.23.23.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.23.23.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.23.23.5.1">
<span class="ltx_p" id="S4.T2.1.23.23.5.1.1" style="width:22.8pt;">96.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.23.23.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.23.23.6.1">
<span class="ltx_p" id="S4.T2.1.23.23.6.1.1" style="width:24.2pt;">59.2</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.24.24">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.24.24.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.24.24.1.1">
<span class="ltx_p" id="S4.T2.1.24.24.1.1.1" style="width:68.3pt;">VIBE <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib120" title="">120</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.24.24.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.24.24.2.1">
<span class="ltx_p" id="S4.T2.1.24.24.2.1.1" style="width:22.8pt;">65.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.24.24.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.24.24.3.1">
<span class="ltx_p" id="S4.T2.1.24.24.3.1.1" style="width:24.2pt;">41.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.24.24.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.24.24.4.1">
<span class="ltx_p" id="S4.T2.1.24.24.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.24.24.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.24.24.5.1">
<span class="ltx_p" id="S4.T2.1.24.24.5.1.1" style="width:22.8pt;">93.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.24.24.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.24.24.6.1">
<span class="ltx_p" id="S4.T2.1.24.24.6.1.1" style="width:24.2pt;">56.5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.25.25">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.25.25.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.25.25.1.1">
<span class="ltx_p" id="S4.T2.1.25.25.1.1.1" style="width:68.3pt;">METRO <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib115" title="">115</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.25.25.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.25.25.2.1">
<span class="ltx_p" id="S4.T2.1.25.25.2.1.1" style="width:22.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.25.25.2.1.1.1">54.0</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.25.25.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.25.25.3.1">
<span class="ltx_p" id="S4.T2.1.25.25.3.1.1" style="width:24.2pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.25.25.3.1.1.1">36.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.25.25.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.25.25.4.1">
<span class="ltx_p" id="S4.T2.1.25.25.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.25.25.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.25.25.5.1">
<span class="ltx_p" id="S4.T2.1.25.25.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.25.25.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.25.25.6.1">
<span class="ltx_p" id="S4.T2.1.25.25.6.1.1" style="width:24.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.26.26">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S4.T2.1.26.26.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.26.26.1.1">
<span class="ltx_p" id="S4.T2.1.26.26.1.1.1" style="width:68.3pt;">SPEC <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib156" title="">156</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.26.26.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.26.26.2.1">
<span class="ltx_p" id="S4.T2.1.26.26.2.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.26.26.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.26.26.3.1">
<span class="ltx_p" id="S4.T2.1.26.26.3.1.1" style="width:24.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.26.26.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.26.26.4.1">
<span class="ltx_p" id="S4.T2.1.26.26.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.26.26.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.26.26.5.1">
<span class="ltx_p" id="S4.T2.1.26.26.5.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.1.26.26.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.26.26.6.1">
<span class="ltx_p" id="S4.T2.1.26.26.6.1.1" style="width:24.2pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.26.26.6.1.1.1">53.2</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.27.27">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r" id="S4.T2.1.27.27.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.27.27.1.1">
<span class="ltx_p" id="S4.T2.1.27.27.1.1.1" style="width:68.3pt;">ROMP <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib129" title="">129</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T2.1.27.27.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.27.27.2.1">
<span class="ltx_p" id="S4.T2.1.27.27.2.1.1" style="width:22.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T2.1.27.27.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.27.27.3.1">
<span class="ltx_p" id="S4.T2.1.27.27.3.1.1" style="width:24.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T2.1.27.27.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.27.27.4.1">
<span class="ltx_p" id="S4.T2.1.27.27.4.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T2.1.27.27.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.27.27.5.1">
<span class="ltx_p" id="S4.T2.1.27.27.5.1.1" style="width:22.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.27.27.5.1.1.1">85.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T2.1.27.27.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.27.27.6.1">
<span class="ltx_p" id="S4.T2.1.27.27.6.1.1" style="width:24.2pt;">53.3</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Some of the most interesting mesh methods are METRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib115" title="">115</a>]</cite>, SPEC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib156" title="">156</a>]</cite>, and ROMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib129" title="">129</a>]</cite>. METRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib115" title="">115</a>]</cite> a single-person 3D HPE method proposes an end-to-end framework for simultaneously reconstructing 3D human poses and meshes from single images using transformers. Unlike traditional approaches that rely on separate stages for pose and mesh estimation, this method integrates both tasks into a unified architecture. The framework leverages a transformer-based backbone to encode the input image and generate intermediate pose and mesh representations. These representations are then refined using transformer decoder layers to produce the final 3D pose and mesh outputs. By jointly optimizing both tasks in an end-to-end manner, the proposed method achieves the smallest MPJPE and PMPJPE errors on the Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib5" title="">5</a>]</cite> dataset among all 3D mesh-based approaches. An alternative approach, SPEC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib156" title="">156</a>]</cite>, is specifically designed for handling in-the-wild HPE. By integrating camera parameters like focal length, SPEC enhances the geometric coherence of pose predictions in challenging outdoor environments. This approach enables more robust pose estimation even in scenarios with varying camera viewpoints and distances. Furthermore, SPEC introduces a novel dataset and evaluation protocol, providing a benchmark for assessing human pose estimation methods in outdoor settings. The dataset encompasses images captured across diverse outdoor scenes, spanning streets, parks, and public areas, with varying occlusion levels, lighting conditions, and background complexities. Additionally, it provides annotated ground truth poses and corresponding camera parameters, facilitating method evaluation. SPEC’s evaluation framework includes conventional metrics like accuracy, precision, and recall, alongside new criteria assessing pose estimation robustness to changes in camera perspective and distance. While SPEC addresses some of the requirements for tailored datasets and metrics in mesh-based 3D HPE, the ultimate efficacy and community acceptance of their approach remains uncertain. Consequently, there remains substantial room for improvement and further discoveries in this field. Another compelling multi-person solution is ROMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib129" title="">129</a>]</cite>. This paper introduces a monocular, one-stage regression approach for estimating multiple 3D human body meshes from a single RGB image. The model leverages a regression-based architecture that directly predicts the 3D joint locations of each person in the image without necessitating intermediate steps or bounding boxes. The method entails predicting a body centre heatmap and a mesh parameter map, which intricately describe 3D body meshes at a pixel level, and subsequently extracting body mesh parameters via a body-centre-guided sampling process. Experimental findings on challenging multi-person benchmarks showcase ROMP’s superior performance over SOTA, particularly in crowded and occluded environments. It is achieved by the Collision-Aware Representation that adeptly tackles centre ambiguity in crowded scenarios. Notably, ROMP is the first real-time implementation of monocular multi-person 3D mesh regression. However, ROMP’s performance might degrade in novel environments or with significantly different conditions from its training data.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Some of the top-performing 3D HPE methods leverage temporal video data for refining predictions. One such method, OANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib50" title="">50</a>]</cite>, addresses occlusion challenges by integrating occlusion-aware modules and temporal convolutional networks into its architecture, thus leveraging both spatial and temporal information. Similar accuracy is achieved by Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib155" title="">155</a>]</cite>, as they propose to use attention mechanisms to exploit temporal contexts. The method utilizes a recurrent NN architecture augmented with attention mechanisms to capture temporal dependencies in sequential data. By dynamically weighting the importance of temporal contexts, the model enhances its ability to predict 3D human poses in real-time from streaming video inputs. However, there are also notable methods focusing on single images. For instance, Xu’s method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib48" title="">48</a>]</cite> enhances accuracy and robustness by incorporating deep kinematics analysis. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib48" title="">48</a>]</cite> integrates kinematic constraints into the network architecture to infer more plausible 3D human poses while considering anatomical constraints and joint relationships. This work extends the 2D CHP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib47" title="">47</a>]</cite> approach to 3D.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Future Work</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The field of 3D HPE and Modeling continues to grapple with several unresolved challenges. One such challenge is the estimation of complex postures, such as those exhibited by athletes like gymnasts. Addressing this challenge may involve creating novel datasets tailored to specific complex and rare postures or exploring the use of unsupervised or generative models.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Crowded scenes present another significant problem, particularly challenging for 3D reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib81" title="">81</a>]</cite>. In such scenarios, occlusion and interperson interactions occur often, complicating the estimation process. These scenarios, along with person-object interactions, pose another challenge for researchers. Although some progress has been made in modelling hand-object interactions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib157" title="">157</a>]</cite> and body interactions with specific objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib158" title="">158</a>]</cite>, we are still far from achieving systems accurately operating in complex real-world environments.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Realistically modelling the entire human body, including fine-grained details of hand movement and facial appearance, presents another formidable challenge. While frameworks for whole-body representation have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib117" title="">117</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib159" title="">159</a>]</cite>, they often fall short of capturing intricate face and hand details. Addressing this challenge may involve creating datasets tailored to fine-grained whole-body representation and developing novel unsupervised or weakly supervised models. Furthermore, cutting-edge computer graphics solutions like NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib160" title="">160</a>]</cite> and gaussian splatting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.19290v1#bib.bib161" title="">161</a>]</cite> offer promising avenues for modelling fine-grained facial details. Additionally, there is room for improvement in modelling facial expressions, emotional states, and dynamic movement of clothes. Success in this area could pave the way for virtual digital human applications such as real-time telepresence, virtual customer service, improved computer-generated scenes in movies, and virtual reality. These require combining detailed representations of the entire human body with accurate portrayals of appearance, gender, and personality. It also involves capturing nuances such as lip movement during speech, as well as the ability to communicate using language, emotions, body language, and facial expressions through digital humans.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Furthermore, developing better benchmarks for mesh-based 3D human body reconstruction is crucial. Currently, we primarily rely on skeleton-based metrics like MPJPE and PMPJPE, which do not fully account for the complexities of mesh representations, including appearance. Additionally, with the increased complexity of 3D mesh models, there is a pressing need for extensive datasets to improve model training. Finally, transitioning 3D mesh modelling from research to industry will require the development of user-friendly toolkits and programming interfaces.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In conclusion, we’ve covered the essential background for understanding human modelling and pose estimation in both 2D and 3D domains. We introduced popular datasets, metrics, and state-of-the-art algorithms, comparing their strengths and weaknesses. Furthermore, we explored potential future research directions, all aimed at advancing towards a common goal. Given that many current approaches are task-specific, there is a pressing need to prioritize the development of more general human representations, particularly in natural, real-world environments.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D. Mehta, O. Sotnychenko, F. Mueller, W. Xu, M. Elgharib, P. Fua, H.-P. Seidel, H. Rhodin, G. Pons-Moll, and C. Theobalt, “XNect: Real-time multi-person 3D motion capture with a single RGB camera,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">ACM Transactions on Graphics</em>, vol. 39, no. 4, July 2020. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://gvv.mpi-inf.mpg.de/projects/XNect/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. Gehler, and B. Schiele, “Deepcut: Joint subset partition and labeling for multi person pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">CVPR</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele, “Deepercut: A deeper, stronger, and faster multiperson pose estimation model,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proc. Eur. Conf. Comp. Vis.</em>, 2016, pp. 34–50.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Kreiss, L. Bertoni, and A. Alahi, “Pifpaf: Composite fields for human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 11 977–11 986.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 36, no. 7, pp. 1325–1339, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
F. Adib, C.-Y. Hsu, H. Mao, D. Katabi, and F. Durand, “Capturing the human figure through a wall,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ACM Transactions on Graphics</em>, vol. 34, no. 6, pp. 219:1–219:13, Nov. 2015. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://dl.acm.org/doi/10.1145/2816795.2818072</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Sengupta, F. Jin, and S. Cao, “NLP based Skeletal Pose Estimation using mmWave Radar Point-Cloud: A Simulation Approach,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2020 IEEE Radar Conference (RadarConf20)</em>, Sep. 2020, pp. 1–6, iSSN: 2375-5318. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ieeexplore.ieee.org/document/9266600</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Sengupta, F. Jin, R. Zhang, and S. Cao, “mm-Pose: Real-Time Human Skeletal Posture Estimation using mmWave Radars and CNNs,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE Sensors Journal</em>, vol. 20, no. 17, pp. 10 032–10 044, Sep. 2020, arXiv:1911.09592 [cs, eess, stat]. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/1911.09592</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. An and U. Y. Ogras, “Fast and Scalable Human Pose Estimation using mmWave Point Cloud,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 59th ACM/IEEE Design Automation Conference</em>, Jul. 2022, pp. 889–894, arXiv:2205.00097 [eess]. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/2205.00097</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
G. Li, Z. Zhang, H. Yang, J. Pan, D. Chen, and J. Zhang, “Capturing Human Pose Using mmWave Radar,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)</em>.   Austin, TX, USA: IEEE, Mar. 2020, pp. 1–6. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ieeexplore.ieee.org/document/9156151/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H. Xue, Y. Ju, C. Miao, Y. Wang, S. Wang, A. Zhang, and L. Su, “mmMesh: towards 3D real-time dynamic human mesh construction using millimeter-wave,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services</em>, ser. MobiSys ’21.   New York, NY, USA: Association for Computing Machinery, Jun. 2021, pp. 269–282. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://dl.acm.org/doi/10.1145/3458864.3467679</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Huang, H. Cheena, A. Thomas, and J. K. P. Tsoi, “Indoor Detection and Tracking of People Using mmWave Sensor,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Journal of Sensors</em>, vol. 2021, pp. 1–14, Feb. 2021. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.hindawi.com/journals/js/2021/6657709/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S.-P. Lee, N. P. Kini, W.-H. Peng, C.-W. Ma, and J.-N. Hwang, “HuPR: A Benchmark for Human Pose Estimation Using Millimeter Wave Radar,” Oct. 2022, arXiv:2210.12564 [cs]. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/2210.12564</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Furst, S. Gupta, R. Schuster, O. Wasenmuller, and D. Stricker, “HPERL: 3D Human Pose Estimation from RGB and LiDAR,” 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D. Ye, Y. Xie, W. Chen, Z. Zhou, and H. Foroosh, “LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network,” Jun. 2023, arXiv:2306.12525 [cs]. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/2306.12525</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Keskin and et al., “Real time hand pose estimation using depth sensors,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Consumer Depth Cameras for Computer Vision</em>, 2013, pp. 119–137.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
T.-L. Le, M.-Q. Nguyen, and T.-T.-M. Nguyen, “Human posture recognition using human skeleton provided by kinect,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">2013 International Conference on Computing, Management and Telecommunications (ComManTel)</em>, 2013, pp. 340–345.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Tamajo, A. Amin, T. Fisho, I. Klugman, E. Stoev, H. Lim, and H. Kim, “Real time 3d multi-person human pose estimation using an omnidirectional camera and mmwave radars,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">2023 International Conference on Engineering and Emerging Technologies (ICEET)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
P. Knap, P. Hardy, A. Tamajo, H. Lim, and H. Kim, “Real-time omnidirectional 3d multi-person human pose estimation with occlusion handling,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ACM SIGGRAPH European Conference on Visual Media Production</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
——, “Improving real-time omnidirectional 3d multi-person human pose estimation with people matching and unsupervised 2d-3d lifting,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">2024 International Conference on Electronics, Information, and Communication (ICEIC)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T. Anvari, K. Park, and G. Kim, “Upper body pose estimation using deep learning for a virtual reality avatar,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Applied Sciences</em>, vol. 13, no. 4, p. 2460, Feb 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://dx.doi.org/10.3390/app13042460</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S.-R. Ke, L. Zhu, J.-N. Hwang, H.-I. Pai, K.-M. Lan, and C.-P. Liao, “Real-time 3d human pose estimation from monocular view with applications to event detection and video gaming,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance</em>, 2010, pp. 489–496.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
C. Zheng, W. Wu, C. Chen, T. Yang, S. Zhu, J. Shen, N. Kehtarnavaz, and M. Shah, “Deep learning-based human pose estimation: A survey,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. Xing, Z. Ding, S. Jiang, X. Ma, K. Yang, C. Yang, X. Li, and F. Jiang, “Hand gesture recognition based on deep learning method,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">2018 IEEE Third International Conference on Data Science in Cyberspace (DSC)</em>, 2018, pp. 542–546.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
W. Li, H. Pu, and R. Wang, “Sign language recognition based on computer vision,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)</em>, 2021, pp. 919–922.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Vasileiadis, S. Malassiotis, D. Giakoumis, C.-S. Bouganis, and D. Tzovaras, “Robust human pose tracking for realistic service robot applications,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops</em>, Oct 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
C. Xu, X. Yu, Z. Wang, and L. Ou, “Multi-view human pose estimation in human-robot interaction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society</em>, 2020, pp. 4769–4775.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
C. Zimmermann, T. Welschehold, C. Dornhege, W. Burgard, and T. Brox, “3d human pose estimation in rgbd images for robotic task learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">2018 IEEE International Conference on Robotics and Automation (ICRA)</em>, 2018, pp. 1986–1992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Lamas, S. Tabik, A. C. Montes, F. Pérez-Hernández, J. García, R. Olmos, and F. Herrera, “Human pose estimation for mitigating false negatives in weapon detection in video-surveillance,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Neurocomputing</em>, vol. 489, pp. 488–503, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. Cormier, F. Röpke, T. Golda, and J. Beyerer, “Interactive labeling for human pose estimation in surveillance videos,” in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</em>, October 2021, pp. 1649–1658.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M. Cormier, A. Clepe, A. Specker, and J. Beyerer, “Where are we with human pose estimation in real-world surveillance?” in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops</em>, January 2022, pp. 591–601.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
C. Papaioannidis, I. Mademlis, and I. Pitas, “Fast cnn-based single-person 2d human pose estimation for autonomous systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">IEEE Transactions on Circuits and Systems for Video Technology</em>, vol. 33, no. 3, pp. 1262–1275, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Zanfir, M. Zanfir, A. Gorban, J. Ji, Y. Zhou, D. Anguelov, and C. Sminchisescu, “Hum3dil: Semi-supervised multi-modal 3d humanpose estimation for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of The 6th Conference on Robot Learning</em>, ser. Proceedings of Machine Learning Research, K. Liu, D. Kulic, and J. Ichnowski, Eds., vol. 205.   PMLR, 14–18 Dec 2023, pp. 1114–1124.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Ludwig, S. Scherer, M. Einfalt, and R. Lienhart, “Self-supervised learning for human pose estimation in sports,” in <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">2021 IEEE International Conference On Multimedia &amp; Expo Workshops (ICMEW)</em>, 2021, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. Wang, K. Qiu, H. Peng, J. Fu, and J. Zhu, “Ai coach: Deep human pose estimation and analysis for personalized athletic training assistance,” in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 27th ACM International Conference on Multimedia</em>, ser. MM ’19.   New York, NY, USA: Association for Computing Machinery, 2019, p. 374–382.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. Badiola-Bengoa and A. Mendez-Zorrilla, “A systematic review of the application of camera-based human pose estimation in the field of sport and physical exercise,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Sensors</em>, vol. 21, no. 18, p. 5996, Sep 2021. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://dx.doi.org/10.3390/s21185996</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
F. Xue, H. Guo, and W. Lu, “Digital twinning of construction objects: Lessons learned from pose estimation methods,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 37th Information Technology for Construction Conference (CIB W78), São Paulo, Brazil</em>, 2020, pp. 2–4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
R. Gámez Díaz, “Digital twin coaching for edge computing using deep learning based 2d pose estimation,” Ph.D. dissertation, Université d’Ottawa/University of Ottawa, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele, “2d human pose estimation: New benchmark and state of the art analysis,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</em>, 2014, pp. 3686–3693.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
T. Von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and G. Pons-Moll, “Recovering accurate 3d human pose in the wild using imus and a moving camera,” in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, 2018, pp. 601–617.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S. Zheng, Y. Song, T. Leung, and I. Goodfellow, “Improving the robustness of deep neural networks via stability training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the ieee conference on computer vision and pattern recognition</em>, 2016, pp. 4480–4488.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
K. Zhou, X. Han, N. Jiang, K. Jia, and J. Lu, “Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 2344–2353.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Y. Zhang, P. Tiňo, A. Leonardis, and K. Tang, “A survey on neural network interpretability,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">IEEE Transactions on Emerging Topics in Computational Intelligence</em>, vol. 5, no. 5, pp. 726–742, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Z. Cao, G. Hidalgo Martinez, T. Simon, S.-E. Wei, and Y. Sheikh, “Openpose: Realtime multi-person 2d pose estimation using part affinity fields,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, pp. 1–1, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
C. Luo, X. Chu, and A. Yuille, “Orinet: A fully convolutional network for 3d human pose estimation,” <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:1811.04989</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
X. Sun, J. Shang, S. Liang, and Y. Wei, “Compositional human pose regression,” in <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 2602–2611.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J. Xu, Z. Yu, B. Ni, J. Yang, X. Yang, and W. Zhang, “Deep kinematics analysis for monocular 3d human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the IEEE/CVF Conference on computer vision and Pattern recognition</em>, 2020, pp. 899–908.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
S. Li, L. Ke, K. Pratama, Y.-W. Tai, C.-K. Tang, and K.-T. Cheng, “Cascaded deep monocular 3d human pose estimation with evolutionary training data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 6173–6183.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Y. Cheng, B. Yang, B. Wang, W. Yan, and R. T. Tan, “Occlusion-aware networks for 3d human pose estimation in video,” in <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 723–732.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
M. Wang, F. Qiu, W. Liu, C. Qian, X. Zhou, and L. Ma, “Monocular human pose and shape reconstruction using part differentiable rendering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Computer Graphics Forum</em>, vol. 39, no. 7.   Wiley Online Library, 2020, pp. 351–362.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black, “SMPL: a skinned multi-person linear model,” <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">ACM Transactions on Graphics</em>, vol. 34, no. 6, pp. 248:1–248:16, Oct. 2015. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://dl.acm.org/doi/10.1145/2816795.2818013</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
W. Liu, Q. Bao, Y. Sun, and T. Mei, “Recent advances of monocular 2d and 3d human pose estimation: A deep learning perspective,” <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ACM Computing Surveys</em>, vol. 55, no. 4, pp. 1–41, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Computer Vision - 14th European Conference, ECCV 2016, Proceedings</em>, ser. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics).   Springer Verlag, 2016, pp. 483–499.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Q. Bao, W. Liu, J. Hong, L. Duan, and T. Mei, “Pose-native network architecture search for multi-person human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the 28th ACM International Conference on Multimedia</em>, 2020, pp. 592–600.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
A. Toshev and C. Szegedy, “Deeppose: Human pose estimation via deep neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2014, pp. 1653–1660.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik, “Human pose estimation with iterative error feedback,” in <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 4733–4742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
J. J. Tompson, A. Jain, Y. LeCun, and C. Bregler, “Joint training of a convolutional network and a graphical model for human pose estimation,” <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Advances in neural information processing systems</em>, vol. 27, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
X. Chen and A. L. Yuille, “Articulated pose estimation by a graphical model with image dependent pairwise relations,” <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Advances in neural information processing systems</em>, vol. 27, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Y. Chen, C. Shen, X.-S. Wei, L. Liu, and J. Yang, “Adversarial posenet: A structure-aware convolutional network for human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 1212–1221.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
W. Yang, S. Li, W. Ouyang, H. Li, and X. Wang, “Learning feature pyramids for human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 1281–1290.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and X. Wang, “Multi-context attention for human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 1831–1840.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, “Convolutional pose machines,” in <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">CVPR</em>, 2016, pp. 4724–4732.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
D. C. Luvizon, D. Picard, and H. Tabia, “2d/3d pose estimation and action recognition using multitask deep learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 5137–5146.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
U. Rafi, B. Leibe, J. Gall, and I. Kostrikov, “An efficient convolutional network for human pose estimation.” in <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">BMVC</em>, vol. 1, 2016, p. 2.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
B. Debnath, M. O’brien, M. Yamaguchi, and A. Behera, “Adapting mobilenets for mobile based upper body pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</em>.   IEEE, 2018, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
A. Bulat and G. Tzimiropoulos, “Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources,” in <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 3706–3714.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Z. Li, J. Ye, M. Song, Y. Huang, and Z. Pan, “Online knowledge distillation for efficient pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 11 740–11 750.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
F. Zhang, X. Zhu, and M. Ye, “Fast human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 3517–3526.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
T. Pfister, J. Charles, and A. Zisserman, “Flowing convnets for human pose estimation in videos,” in <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Proceedings of the IEEE international conference on computer vision</em>, 2015, pp. 1913–1921.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
J. Song, L. Wang, L. Van Gool, and O. Hilliges, “Thin-slicing network: A deep structured model for pose estimation in videos,” in <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 4220–4229.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
G. Gkioxari, A. Toshev, and N. Jaitly, “Chained predictions using convolutional neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14</em>.   Springer, 2016, pp. 728–743.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Y. Luo, J. Ren, Z. Wang, W. Sun, J. Pan, J. Liu, J. Pang, and L. Lin, “Lstm pose machines,” in <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 5207–5215.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler, and K. Murphy, “Towards accurate multi-person pose estimation in the wild,” in <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 4903–4911.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
——, “Towards accurate multi-person pose estimation in the wild,” in <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 4903–4911.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
B. Xiao, H. Wu, and Y. Wei, “Simple baselines for human pose estimation and tracking,” in <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, 2018, pp. 466–481.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun, “Cascaded pyramid network for multi-person pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 7103–7112.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution representation learning for human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 5693–5703.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu, “Rmpe: Regional multi-person pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 2334–2343.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
J. Li, C. Wang, H. Zhu, Y. Mao, H.-S. Fang, and C. Lu, “Crowdpose: Efficient crowded scenes pose estimation and a new benchmark,” in <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 10 863–10 872.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
X. Liang, K. Gong, X. Shen, and L. Lin, “Look into person: Joint body parsing &amp; pose estimation network and a new benchmark,” <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 41, no. 4, pp. 871–885, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
F. Xia, P. Wang, X. Chen, and A. L. Yuille, “Joint multi-person pose estimation and semantic part segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 6769–6778.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,” in <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, 2018, pp. 734–750.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
X. Zhou, V. Koltun, and P. Krähenbühl, “Tracking objects as points,” in <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">European conference on computer vision</em>.   Springer, 2020, pp. 474–490.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
A. Newell, Z. Huang, and J. Deng, “Associative embedding: End-to-end learning for joint detection and grouping,” <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang, “Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 5386–5395.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
G. Papandreou, T. Zhu, L.-C. Chen, S. Gidaris, J. Tompson, and K. Murphy, “Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model,” in <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, 2018, pp. 269–286.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan, J. Gall, and B. Schiele, “Posetrack: A benchmark for human pose estimation and tracking,” in <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 5167–5176.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
R. Girdhar, G. Gkioxari, L. Torresani, M. Paluri, and D. Tran, “Detect-and-track: Efficient pose estimation in videos,” in <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 350–359.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
M. Wang, J. Tighe, and D. Modolo, “Combining detection and tracking for human pose estimation in videos,” in <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020, pp. 11 088–11 096.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
E. Insafutdinov, M. Andriluka, L. Pishchulin, S. Tang, E. Levinkov, B. Andres, and B. Schiele, “Arttrack: Articulated multi-person tracking in the wild,” in <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">CVPR</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Y. Xiu, J. Li, H. Wang, Y. Fang, and C. Lu, “Pose flow: Efficient online pose tracking,” <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">arXiv preprint arXiv:1802.00977</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Y. Raaj, H. Idrees, G. Hidalgo, and Y. Sheikh, “Efficient online multi-person 2d pose tracking with recurrent spatio-temporal affinity fields,” in <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 4620–4628.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
M. Fabbri, F. Lanzi, S. Calderara, A. Palazzi, R. Vezzani, and R. Cucchiara, “Learning to detect and track visible and occluded body joints in a virtual world,” in <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, 2018, pp. 430–446.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
A. Doering, U. Iqbal, and J. Gall, “Jointflow: Temporal flow fields for multi person pose estimation.” in <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">BMVC</em>, 2018, p. 261.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
S. Jin, W. Liu, W. Ouyang, and C. Qian, “Multi-person articulated tracking with spatial and temporal embeddings,” in <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 5664–5673.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis, “Coarse-to-fine volumetric prediction for single-image 3d human pose,” in <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shafiei, H.-P. Seidel, W. Xu, D. Casas, and C. Theobalt, “Vnect: Real-time 3d human pose estimation with a single rgb camera,” <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">Acm transactions on graphics (tog)</em>, vol. 36, no. 4, pp. 1–14, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
X. Sun, B. Xiao, F. Wei, S. Liang, and Y. Wei, “Integral human pose regression,” in <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, 2018, pp. 529–545.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
J. Martinez, R. Hossain, J. Romero, and J. J. Little, “A simple yet effective baseline for 3D human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 2640–2649.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
J. Yang, L. Wan, W. Xu, and S. Wang, “3d human pose estimation from a single image via exemplar augmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Journal of Visual Communication and Image Representation</em>, vol. 59, pp. 371–379, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
G. Pavlakos, X. Zhou, and K. Daniilidis, “Ordinal depth supervision for 3d human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2018, pp. 7307–7316.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
M. R. Ronchi, O. Mac Aodha, R. Eng, and P. Perona, “It’s all relative: Monocular 3d human pose estimation from weakly supervised data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">British Machine Vision Conference 2018, BMVC 2018</em>, Northumbria University, Newcastle, UK, September 2018, p. 300.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
B. Wandt and B. Rosenhahn, “Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 06 2019, pp. 7774–7783.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
B. Tekin, S. N. Sinha, and P. Fua, “Structured prediction of 3d human pose with deep neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">BMVC</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
D. Tome, C. Russell, and L. Agapito, “Lifting from the deep: Convolutional 3d pose estimation from a single image,” in <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
C.-H. Chen and D. Ramanan, “3d human pose estimation= 2d pose estimation+ matching,” in <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 7035–7043.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. N. Metaxas, “Semantic graph convolutional networks for 3d human pose regression,” in <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 3425–3435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
U. Iqbal, P. Molchanov, and J. Kautz, “Weakly-supervised 3d human pose learning via multi-view images in the wild,” in <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 5243–5252.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
G. Varol, J. Romero, X. Martin, N. Mahmood, M. J. Black, I. Laptev, and C. Schmid, “Learning from synthetic humans,” in <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 109–117.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, and J. Davis, “Scape: shape completion and animation of people,” in <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">ACM SIGGRAPH 2005 Papers</em>, 2005, pp. 408–416.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli, “3d human pose estimation in video with temporal convolutions and semi-supervised training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 7753–7762.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
N. Kolotouros, G. Pavlakos, and K. Daniilidis, “Convolutional mesh regression for single-image human shape reconstruction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 4501–4510.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
K. Lin, L. Wang, and Z. Liu, “End-to-end human pose and mesh reconstruction with transformers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2021, pp. 1954–1963.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
J. Romero, D. Tzionas, and M. J. Black, “Embodied hands: Modeling and capturing hands and bodies together,” <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">arXiv preprint arXiv:2201.02610</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. Osman, D. Tzionas, and M. J. Black, “Expressive body capture: 3d hands, face, and body from a single image,” in <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 10 975–10 985.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero, “Learning a model of facial shape and expression from 4d scans.” <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">ACM Trans. Graph.</em>, vol. 36, no. 6, pp. 194–1, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik, “End-to-end recovery of human shape and pose,” in <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 7122–7131.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
M. Kocabas, N. Athanasiou, and M. J. Black, “Vibe: Video inference for human body pose and shape estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 5253–5263.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
A. Kanazawa, J. Y. Zhang, P. Felsen, and J. Malik, “Learning 3d human dynamics from video,” in <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 5614–5623.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and M. J. Black, “Keep it smpl: Automatic estimation of 3d human pose and shape from a single image,” in <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14</em>.   Springer, 2016, pp. 561–578.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
N. Kolotouros, G. Pavlakos, M. J. Black, and K. Daniilidis, “Learning to reconstruct 3d human pose and shape via model-fitting in the loop,” in <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 2252–2261.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
G. Rogez, P. Weinzaepfel, and C. Schmid, “Lcr-net++: Multi-person 2d and 3d pose detection in natural images,” <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 42, no. 5, pp. 1146–1161, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
G. Moon, J. Y. Chang, and K. M. Lee, “Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image,” in <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 10 133–10 142.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
A. Zanfir, E. Marinoiu, M. Zanfir, A.-I. Popa, and C. Sminchisescu, “Deep network for the integrated 3d sensing of multiple people in natural images,” <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">Advances in neural information processing systems</em>, vol. 31, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shafiei, H.-P. Seidel, W. Xu, D. Casas, and C. Theobalt, “Single shot multi-person 3d pose estimation from monocular rgb,” in <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">3DV</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
A. Zanfir, E. Marinoiu, and C. Sminchisescu, “Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints,” in <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2018, pp. 2148–2157.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Y. Sun, Q. Bao, W. Liu, Y. Fu, M. J. Black, and T. Mei, “Monocular, one-stage, regression of multiple 3d people,” in <em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 11 179–11 188.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Y. Sun, W. Liu, Q. Bao, Y. Fu, T. Mei, and M. J. Black, “Putting people in their place: Monocular regression of 3d people in depth,” in <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 13 243–13 252.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
A. Jain, J. Tompson, Y. LeCun, and C. Bregler, “Humaneva: Synchronized video and motion capture dataset for evaluation of articulated human motion,” <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">International Journal of Computer Vision</em>, vol. 87, no. 1-2, pp. 4–27, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu, and C. Theobalt, “Monocular 3d human pose estimation in the wild using improved cnn supervision,” in <em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">2017 international conference on 3D vision (3DV)</em>.   IEEE, 2017, pp. 506–516.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
S. Ghorbani, K. Mahdaviani, A. Thaler, K. Kording, D. J. Cook, G. Blohm, and N. F. Troje, “Movi: A large multipurpose motion and video dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">arXiv preprint arXiv:2003.01888</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black, “Amass: Archive of motion capture as surface shapes,” in <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 5442–5451.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
T. Von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and G. Pons-Moll, “Recovering accurate 3d human pose in the wild using imus and a moving camera,” in <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, 2018, pp. 601–617.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
S. Johnson and M. Everingham, “Clustered pose and nonlinear appearance models for human pose estimation.” in <em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">bmvc</em>, vol. 2, no. 4.   Aberystwyth, UK, 2010, p. 5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
B. Sapp and B. Taskar, “Modec: Multimodal decomposable models for human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2013, pp. 3674–3681.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, “Towards understanding action recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">Proceedings of the IEEE international conference on computer vision</em>, 2013, pp. 3192–3199.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
H. Joo, H. Liu, L. Tan, L. Gui, B. Nabbe, I. Matthews, T. Kanade, S. Nobuhara, and Y. Sheikh, “Panoptic studio: A massively multiview system for social motion capture,” in <em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">Proceedings of the IEEE International Conference on Computer Vision</em>, 2015, pp. 3334–3342.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black, “Amass: Archive of motion capture as surface shapes,” in <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 5442–5451.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
J. Tompson, R. Goroshin, A. Jain, Y. LeCun, and C. Bregler, “Efficient object localization using convolutional networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</em>, 2015, pp. 648–656.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
W. Tang and Y. Wu, “Does learning specific features for related parts help human pose estimation?” in <em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 1107–1116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Z. Geng, K. Sun, B. Xiao, Z. Zhang, and J. Wang, “Bottom-up human pose estimation via disentangled keypoint regression,” in <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2021, pp. 14 676–14 686.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
J. Zhang, Z. Zhu, J. Lu, J. Huang, G. Huang, and J. Zhou, “Simple: Single-network with mimicking and point learning for bottom-up human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 35, no. 4, 2021, pp. 3342–3350.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
G. Brasó, N. Kister, and L. Leal-Taixé, “The center of attention: Center-keypoint grouping via attention for multi-person pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 11 853–11 863.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
Z. Luo, Z. Wang, Y. Huang, L. Wang, T. Tan, and E. Zhou, “Rethinking the heatmap regression for bottom-up human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2021, pp. 13 264–13 273.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
W. Li, Z. Wang, B. Yin, Q. Peng, Y. Du, T. Xiao, G. Yu, H. Lu, Y. Wei, and J. Sun, “Rethinking on multi-stage networks for human pose estimation,” <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">arXiv preprint arXiv:1901.00148</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
F. Zhang, X. Zhu, H. Dai, M. Ye, and C. Zhu, “Distribution-aware coordinate representation for human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 7093–7102.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
G. Moon, J. Y. Chang, and K. M. Lee, “Posefix: Model-agnostic general human pose refinement network,” in <em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 7773–7781.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
J. Wang, X. Long, Y. Gao, E. Ding, and S. Wen, “Graph-pcnn: Two stage human pose estimation with graph pose refinement,” in <em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16</em>.   Springer, 2020, pp. 492–508.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
Y. Cai, Z. Wang, Z. Luo, B. Yin, A. Du, H. Wang, X. Zhang, X. Zhou, E. Zhou, and J. Sun, “Learning delicate local representations for multi-person pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16</em>.   Springer, 2020, pp. 455–472.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Q. Bao, W. Liu, Y. Cheng, B. Zhou, and T. Mei, “Pose-guided tracking-by-detection: Robust multi-person pose tracking,” <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">IEEE Transactions on Multimedia</em>, vol. 23, pp. 161–175, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Z. Liu, H. Chen, R. Feng, S. Wu, S. Ji, B. Yang, and X. Wang, “Deep dual consecutive network for human pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2021, pp. 525–534.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
X. Zhou and et al., “Sparseness meets deepness: 3D human pose estimation from monocular video,” in <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 4966–4975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
R. Liu, J. Shen, H. Wang, C. Chen, S.-c. Cheung, and V. Asari, “Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 5064–5073.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
M. Kocabas, C.-H. P. Huang, J. Tesch, L. Müller, O. Hilliges, and M. J. Black, “Spec: Seeing people in the wild with an estimated camera,” in <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 11 035–11 045.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
B. Tekin, F. Bogo, and M. Pollefeys, “H+ o: Unified egocentric recognition of 3d hand-object poses and interactions,” in <em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 4511–4520.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
M. Hassan, V. Choutas, D. Tzionas, and M. J. Black, “Resolving 3d human pose ambiguities with 3d scene constraints,” in <em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 2282–2292.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
D. Xiang, H. Joo, and Y. Sheikh, “Monocular total capture: Posing face, body, and hands in the wild,” in <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 10 965–10 974.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” <em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">Communications of the ACM</em>, vol. 65, no. 1, pp. 99–106, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” <em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">ACM Transactions on Graphics</em>, vol. 42, no. 4, 2023.

</span>
</li>
</ul>
</section>
<figure class="ltx_float biography" id="id1">
<table class="ltx_tabular" id="id1.1">
<tr class="ltx_tr" id="id1.1.1">
<td class="ltx_td" id="id1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="125" id="id1.1.1.1.g1" src="extracted/5696266/Figures/my_photo_small.jpg" width="89"/></td>
<td class="ltx_td" id="id1.1.1.2">
<span class="ltx_inline-block" id="id1.1.1.2.1">
<span class="ltx_p" id="id1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="id1.1.1.2.1.1.1">Pawel Knap</span>  is a PhD student at the University of Freiburg, Germany. He completed his MEng in Electronic Engineering with Artificial Intelligence at the University of Southampton. Pawel is the first author of two peer-reviewed conference papers. His academic interests primarily focus on computer vision, particularly using AI methods for medical image data analysis. For more about Pawel please visit: pawelknap.github.io</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jun 27 15:55:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
