<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>The Influence of Faulty Labels in Data Sets on Human Pose Estimation</title>
<!--Generated on Mon Sep  9 15:51:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.03887v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S1" title="In The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S2" title="In The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S2.SS1" title="In 2 Related Work ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Human Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S2.SS2" title="In 2 Related Work ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Training and Benchmarking Data sets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S2.SS3" title="In 2 Related Work ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Advancements in Keypoint Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S2.SS4" title="In 2 Related Work ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Data Quality in Machine Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S2.SS5" title="In 2 Related Work ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Label Noise in HPE</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3" title="In The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.SS1" title="In 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Set Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.SS2" title="In 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Set Errors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.SS3" title="In 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Automatic Label Noise Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.SS4" title="In 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Evaluation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.SS5" title="In 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Per Keypoint Distance for MPII and COCO</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.SS6" title="In 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Calibrating the Outlier Threshold</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4" title="In The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments and Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.SS1" title="In 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Label Error Detection Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.SS1.SSS0.Px1" title="In 4.1 Label Error Detection Evaluation ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title">Heuristics evaluation on training set:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.SS1.SSS0.Px2" title="In 4.1 Label Error Detection Evaluation ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title">Heuristics evaluation on validation set:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.SS2" title="In 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Impact of Cleaning Validation Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.SS2.SSS0.Px1" title="In 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title">Influence of omitting key points on accuracy:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.SS3" title="In 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Impact of Cleaning Training Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.SS4" title="In 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Impact of Hard Poses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.SS5" title="In 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Impact of Annotation Jitter</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S5" title="In The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#A1" title="In The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Influence of Faulty Labels in Data Sets on Human Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Arnold Schwarz 
<br class="ltx_break"/>Berliner Hochschule für Technik
<br class="ltx_break"/>Berlin, Germany 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.1.id1">arnold.schwarz@bht-berlin.de</span>
<br class="ltx_break"/>&amp;Levente Hernadi 
<br class="ltx_break"/>Berliner Hochschule für Technik
<br class="ltx_break"/>Berlin, Germany 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.2.id2">leventejanko.hernadi@bht-berlin.de</span>
<br class="ltx_break"/>&amp;Felix Bießmann 
<br class="ltx_break"/>Berliner Hochschule für Technik
<br class="ltx_break"/>Berlin, Germany 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.3.id3">felix.bießmann@bht-berlin.de</span>
<br class="ltx_break"/>&amp;Kristian Hildebrand
<br class="ltx_break"/>Berliner Hochschule für Technik
<br class="ltx_break"/>Berlin, Germany 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id6.4.id4">kristian.hildebrand@bht-berlin.de</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">In this study we provide empirical evidence demonstrating that the quality of training data impacts model performance in Human Pose Estimation (HPE). Inaccurate labels in widely used data sets, ranging from minor errors to severe mislabeling, can negatively influence learning and distort performance metrics. We perform an in-depth analysis of popular HPE data sets to show the extent and nature of label inaccuracies.
Our findings suggest that accounting for the impact of faulty labels will facilitate the development of more robust and accurate HPE models for a variety of real-world applications. We show improved performance with cleansed data.</p>
<p class="ltx_p" id="id2.2"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="id2.2.1">K</em><span class="ltx_text ltx_font_bold" id="id2.2.2">eywords</span> Human Pose Estimation  <math alttext="\cdot" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">⋅</annotation></semantics></math>
Data Sets  <math alttext="\cdot" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">⋅</annotation></semantics></math>
Data Quality</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Human Pose Estimation (HPE) has recently been used to analyse and make decisions at sporting events. These analyses of human pose in the centimetre range can be decisive. The uncertainties or the quality of the machine decision making are partly due to the underlying data quality and are usually not communicated to the analyst.
While HPE has a long history of previous work, current model-based approaches mostly rely on two datasets for training (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S3.F2" title="Figure 2 ‣ 3.1 Data Set Selection ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>). These datasets are labelled for 2D HPE, which forms the basis for 3D HPE in many newer approaches  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib5" title="">5</a>]</cite> and is crucial for further applications, including those mentioned above.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="153" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We present an error taxonomy specifically for keypoint label errors in HPE. Left: We categorize and summarize various types of localization errors. Right: We display a range of label errors, highlighting their diversity.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While such standardized benchmarks are a fundamental prerequisite to scientific progress, recent studies have also highlighted problems with this approach. For one, with the ever increasing pace of innovation in the field of Machine Learning (ML), models achieve very good results in benchmarks quickly and the impact of model improvements become difficult to measure when predictive performance saturates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib6" title="">6</a>]</cite>. This problem is aggravated by the fact that statistical significance of performance improvements in such benchmarks is often difficult to assess: The generalization error on the test set is typically reported as a point estimate, but it can exhibit variances often larger than the differences in predictive performance commonly reported at the top of benchmark leaderboards <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib7" title="">7</a>]</cite>. Hence improvements on benchmarks that do not account for the uncertainty in the generalization error estimates are sometimes difficult to evaluate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib8" title="">8</a>]</cite>.
Second, data quality issues in the training and test data even in well curated and carefully controlled benchmark data sets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib9" title="">9</a>]</cite> have been demonstrated to impact both model training and estimates of generalization performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib10" title="">10</a>]</cite>.
In this study we argue that the impact of these problems on the advances of the state of the art in HPE have been underrepresented in the literature so far. We provide empirical evidence for data quality problems in erroneous annotation data of commonly used data sets and we demonstrate their impact on model training and generalization performance.
These findings have implications for the interpretation of previously published results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Therefore we underline the importance to look at the training and evaluation data to ensure reliability in benchmarks and real-world scenarios. In this work we contribute the following to existing methods and research:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We provide a comprehensive analysis of labeling errors in commonly used HPE benchmark data sets, developing both a taxonomy of annotation errors and a simple yet effective heuristic to identify faulty and difficult annotations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We evaluate the proposed heuristic’s impact on data quality in established models leading public HPE benchmarks, discussing the effects of cleaned data and model improvements on popular HPE models.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Human Pose Estimation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">There have been several surveys looking into the current state of HPE technology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib14" title="">14</a>]</cite>. HPE is generally divided into whether the model regresses the body keypoints directly as pixel coordinates or whether it uses a probabilistic prediction by outputting keypoint coordinates as heatmaps, which both show notable results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib11" title="">11</a>]</cite>. Furthermore, there are two widely used paradigms 2D HPE falls into: the top-down- or the bottom-up approaches. The bottom-up approach initially identifies all visible body keypoints in an image, and subsequently matches these keypoints to the respective individuals. In contrast, the top-down method begins by detecting each person and their bounding boxes, followed by pinpointing the keypoints within each of these boxes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Training and Benchmarking Data sets</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">HPE models are typically trained and evaluated on specific data sets, with their performance assessed using a variety of benchmark metrics tailored to each data set. The de facto standard among these data sets are COCO<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cocodataset.org" title="">https://cocodataset.org</a></span></span></span> (Common Objects in Context) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib15" title="">15</a>]</cite> and MPII<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://human-pose.mpi-inf.mpg.de/" title="">http://human-pose.mpi-inf.mpg.de/</a></span></span></span> (Max Planck Institute for Informatics) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib16" title="">16</a>]</cite> due to their size (200.000 and 40.000 labeled poses for COCO and MPII respectively) and their variety in terms of image content. For performance measurement, different metrics are employed based on the data set. For instance, MPII commonly uses a variant of the Percentage of Correct Keypoints (PCK) metric, specifically PCKh@0.5. This metric measures keypoint detection accuracy, considering a keypoint correct if it is within a threshold distance (50% of the head segment length for PCKh@0.5) from the ground truth. COCO uses the Object Keypoint Similarity (OKS) score, a metric that considers the person’s scale and keypoint visibility in the image, and evaluates the similarity between predicted and actual keypoints, allowing for some labeling noise.
Since most models evaluate and train on COCO and MPII we analyze those in more detail in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S3.SS1" title="3.1 Data Set Selection ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">subsection 3.1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Advancements in Keypoint Detection</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">A notable advancement in pixel-wise regression and keypoint detection accuracy was achieved through HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib17" title="">17</a>]</cite>, which addresses information loss challenges by implementing an architecture consisting of parallel subnetworks with varying resolutions.
This makes HRNet a preferred backbone for state-of-the-art keypoint prediction models and is used throughout in this study. Other advances tackled the issue of accuracy loss introduced by image and annotation pre- and post-processing such as Unbiased Data Processing (UPD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib18" title="">18</a>]</cite> and Distribution-Aware Coordinate Representation (DARK) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib19" title="">19</a>]</cite>. Recent leaderboards on the COCO and MPII test-set often mention the following high performing technologies: Polarized Self-Attention (PSA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib20" title="">20</a>]</cite>, Residual Step Network (RSN)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib21" title="">21</a>]</cite> and ViTPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib22" title="">22</a>]</cite>. PSA uses the idea of Self-Attention layers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib23" title="">23</a>]</cite> coupled with the concept of polarized filtering.
RSN introduced intra-level feature fusion through dense connections in its <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">Residual Step Blocks</em> to refine feature representation at sequential convolution levels and VitPose utilized the popular transformer architecture for the keypoint detection task. In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S2.T1" title="Table 1 ‣ 2.3 Advancements in Keypoint Detection ‣ 2 Related Work ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a> we report scores for each method as they have been published in their respective work.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Reported results of state-of-the-art 2D human pose estimation methods for the OKS metric (COCO test-dev) and the PCKh@0.5 (MPII)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.1.2">AP (COCO)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.1.3">PCKh@0.5 (MPII)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.1.1">VitPose+/VitPose-G</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.1.2">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.2.1">81.1</span>%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.3.1">94.3</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S2.T1.1.3.2.1">HRNet + UDP + PSA</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.3.2.2">79.4%</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.3.2.3">-</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<td class="ltx_td ltx_align_left" id="S2.T1.1.4.3.1">RSN</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.4.3.2">79.2 %</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.4.3.3">93.0%</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<td class="ltx_td ltx_align_left" id="S2.T1.1.5.4.1">HRNet + DARK</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.5.4.2">76.2%</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.5.4.3">90.6 %</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T1.1.6.5.1">HRNet + UDP</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T1.1.6.5.2">76.5%</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T1.1.6.5.3">-</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Data Quality in Machine Learning</h3>
<div class="ltx_para ltx_noindent" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Data quality has been recognized as one of the most important hyperparameters in ML model development. While the majority of models assume stationarity of both the data distribution as well as the label distribution, this assumption is violated in most real world applications. Data errors, or more generally shifts in the data distribution, do occur often and have been studied actively <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib24" title="">24</a>]</cite>. In the ML community this research is often referred to <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.1.1">covariate shift</span> if the shift is attributed to the input features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib25" title="">25</a>]</cite> and <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.1.2">label shift</span>, if the shift is associated with the target variable <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib26" title="">26</a>]</cite>. Some sources of noise in training data can have positive, regularizing effects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib27" title="">27</a>]</cite>. This effect is leveraged in the augmentation techniques applied in computer vision to render the models invariant w.r.t. data shifts that do not change the semantic properties of an image. Other data quality problems have been demonstrated to have severe negative impact on generalization performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib29" title="">29</a>]</cite>. Consequently strategies to detect data quality problems in data sets are being investigated in various application domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib30" title="">30</a>]</cite>. A key challenge in this context remains automation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib31" title="">31</a>]</cite>. Several approaches were proposed to detect data set shifts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib34" title="">34</a>]</cite> or label shifts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib26" title="">26</a>]</cite>. Other approaches aim at generation of realistic errors to improve model robustness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib36" title="">36</a>]</cite>.
Our work is inspired by and complements recent findings that demonstrate how simple heuristics can be effective for removing label errors in computer vision benchmark data sets and significantly impact the generalization error estimates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib10" title="">10</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Label Noise in HPE</h3>
<div class="ltx_para ltx_noindent" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">In the particular application domain investigated in this study, 2D HPE, label noise has been discussed by several authors and is typically being reported as:</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p2">
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Missing keypoints for visible body parts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib37" title="">37</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Localisation errors of keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib9" title="">9</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Structure confusion (Left/Right, Arms/Legs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib38" title="">38</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1">Randomly annotated keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib38" title="">38</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i5.p1">
<p class="ltx_p" id="S2.I1.i5.p1.1">Missing or incomplete occlusion label <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib37" title="">37</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p3">
<p class="ltx_p" id="S2.SS5.p3.1">Most approaches focus on making the HPE models more robust in the presence of noisy labels, rather than cleaning data. Johnson et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib38" title="">38</a>]</cite> model keypoint localization errors produced by human workers as an isotropic Gaussian distribution of vertical and horizontal displacement. Structural errors are assumed to be uniformly distributed across the entire data set. Using a small subset of ’expert’ annotated data they define a learning task to steer faulty annotations closer to the ’expert’ truth.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p4">
<p class="ltx_p" id="S2.SS5.p4.1">Kato et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib37" title="">37</a>]</cite> adapt the concept of knowledge distillation by using a teacher model to improve insufficient ground truth labels. A student network is then trained using improved annotations to increase its performance. To handle missing, shift, and duplicate noise in point data Wan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib39" title="">39</a>]</cite> proposed a new loss function that takes uncertainty about labels into consideration.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p5">
<p class="ltx_p" id="S2.SS5.p5.1">Complementing this prior work we propose to investigate in more detail the types of errors and their impact on HPE model training and generalization error estimates. While previous methods accept the errors in the data set, we show their frequencies and suggest a method for detecting those outliers. By data cleansing and showing the effect on training and evaluation, we conclude the problems in effectiveness of existing models trained and evaluated on these data sets, their benchmark in general and its implications on real-world scenarios. We show a more detailed error taxonomy for keypoint label errors in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We determine and quantify the prevalence and type of errors in the data set using a systematic analysis. This involves a comprehensive examination of various data sets (<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.SS1" title="3.1 Data Set Selection ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">3.1</span></a>), leading to the development of a detailed error taxonomy (<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.SS2" title="3.2 Data Set Errors ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Utilizing this taxonomy, we then devise a strategy for detecting faulty labels, enabling us to identify and address errors across the entire data set (<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.SS3" title="3.3 Automatic Label Noise Detection ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">3.3</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.SS6" title="3.6 Calibrating the Outlier Threshold ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">3.6</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Set Selection</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="308" id="S3.F2.g1" src="x2.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The plot illustrates the frequency of usage of each data set for training or benchmarking purposes. Data was gathered from three HPE surveys, encompassing 40 distinct methods, and additional research for publications after 2023.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Due to the variety of different training and benchmarking data sets, we decided to reduce our selection to the most relevant data sets currently used for HPE. We evaluated how many unique models and methods used which data set based on data from three surveys <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib11" title="">11</a>]</cite> which included 40 different methods from 2018 to 2022. In order to include publications after 2023 we also considered additional research working with online databases. The results can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S3.F2" title="Figure 2 ‣ 3.1 Data Set Selection ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a>. The outcome of this evaluation indicates that COCO and MPII are the most commonly used data sets for training and testing. Many published papers rely on the training data they provide and on their evaluation tasks to assess the performance of their methods. The ground truth annotation data is often provided by a crowd of human annotators, for example the Amazon Mechanical Turk (AMT) platform<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib16" title="">16</a>]</cite>. As mentioned earlier by Johnson et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib38" title="">38</a>]</cite>, human crowd work on pose labelling is prone to errors. Some data sets like COCO model margins of human labelling noise into their evaluation metric <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib9" title="">9</a>]</cite>, however severe labelling mistakes not accounted by the metric might still influence training and validation results.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="182" id="S3.F3.g1" src="x3.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Left: The frequency of occurrence of an error class in our sample. Right: The frequency of error notes for keypoint classes (consider only false annotations, LR-swap and position errors).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Set Errors</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We define an extended error taxonomy for the data sets as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> and distinguish between two main error classes, localization errors and labeling errors. For localization errors, we distinguish different subclasses to describe specific types of wrong positioning of keypoints or bounding boxes. These errors are continuous, while labeling errors are discrete and describe different types of missing labels or errors in the data structure.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">In order to better annotate the error frequency, we have reduced the error taxonomy (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>) to 5 classes and summarized them:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Missing annotation: a keypoint is located on the image plane, is visible or not and is not labeled.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">False label: A keypoint is not on the image plane but is labeled.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Incorrect position: The position of the keypoint is clearly incorrect.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Left-right swap: Two keypoints and their assignment are swapped between left and right.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">Visibility error: The visibility flag of the key point is set incorrectly.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS2.p2.2">In a next step, we asked three people to label these five error classes in a sample of the MPII validation set (here we use the split from HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib17" title="">17</a>]</cite>) and the COCO data set. From the MPII validation set we randomly take 161 out of 4917 annotations and from the COCO validation set we take 163 out of 6352. This corresponds to a confidence level of 99% and a margin of error of 10%.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Our analysis reveals that both data sets contain approximately 2% positional and left-right swap errors, along with false annotations. In these cases, keypoints are incorrectly labeled, resulting in them not actually being present on the image plane (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>). As can be seen in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S3.F3" title="Figure 3 ‣ 3.1 Data Set Selection ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>, both data sets show a large error for <em class="ltx_emph ltx_font_italic" id="S3.SS2.p3.1.1">visibility flag labels</em>, especially the COCO data set. Although these errors do not affect current top-down approaches, since they are not considered during training and loss calculation, they can, however, influence bottom-up approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib40" title="">40</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">We should also mention that the results between the two data sets are not directly comparable, as our annotators may have been better trained to detect errors in the COCO data set, leading to a seemingly more critical approach to scoring.
We did not additionally evaluate the bounding box fitting in the manual evaluation process. However, a frequency of poor bounding box fitting was noted. If we only look at the raw annotation data, we can already see that in the MPII data set around 3.4% of the annotated keypoints are outside the ground truth bounding box. For the COCO data set it is around 3.8%. If the ground truth bounding is used in the evaluation, then it may be the case that the keypoint cannot be estimated at all, as the part is cut off in the top-down process.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Automatic Label Noise Detection</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.13">In order to detect data points with faulty labels <math alttext="y_{e}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">y</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑦</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">y_{e}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math> automatically with high confidence we develop heuristics to estimate <math alttext="p(y_{e}|\hat{y},y)" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.3"><semantics id="S3.SS3.p1.2.m2.3a"><mrow id="S3.SS3.p1.2.m2.3.3" xref="S3.SS3.p1.2.m2.3.3.cmml"><mi id="S3.SS3.p1.2.m2.3.3.3" xref="S3.SS3.p1.2.m2.3.3.3.cmml">p</mi><mo id="S3.SS3.p1.2.m2.3.3.2" xref="S3.SS3.p1.2.m2.3.3.2.cmml">⁢</mo><mrow id="S3.SS3.p1.2.m2.3.3.1.1" xref="S3.SS3.p1.2.m2.3.3.1.1.1.cmml"><mo id="S3.SS3.p1.2.m2.3.3.1.1.2" stretchy="false" xref="S3.SS3.p1.2.m2.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS3.p1.2.m2.3.3.1.1.1" xref="S3.SS3.p1.2.m2.3.3.1.1.1.cmml"><msub id="S3.SS3.p1.2.m2.3.3.1.1.1.2" xref="S3.SS3.p1.2.m2.3.3.1.1.1.2.cmml"><mi id="S3.SS3.p1.2.m2.3.3.1.1.1.2.2" xref="S3.SS3.p1.2.m2.3.3.1.1.1.2.2.cmml">y</mi><mi id="S3.SS3.p1.2.m2.3.3.1.1.1.2.3" xref="S3.SS3.p1.2.m2.3.3.1.1.1.2.3.cmml">e</mi></msub><mo fence="false" id="S3.SS3.p1.2.m2.3.3.1.1.1.1" xref="S3.SS3.p1.2.m2.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.SS3.p1.2.m2.3.3.1.1.1.3.2" xref="S3.SS3.p1.2.m2.3.3.1.1.1.3.1.cmml"><mover accent="true" id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">y</mi><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">^</mo></mover><mo id="S3.SS3.p1.2.m2.3.3.1.1.1.3.2.1" xref="S3.SS3.p1.2.m2.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.SS3.p1.2.m2.2.2" xref="S3.SS3.p1.2.m2.2.2.cmml">y</mi></mrow></mrow><mo id="S3.SS3.p1.2.m2.3.3.1.1.3" stretchy="false" xref="S3.SS3.p1.2.m2.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.3b"><apply id="S3.SS3.p1.2.m2.3.3.cmml" xref="S3.SS3.p1.2.m2.3.3"><times id="S3.SS3.p1.2.m2.3.3.2.cmml" xref="S3.SS3.p1.2.m2.3.3.2"></times><ci id="S3.SS3.p1.2.m2.3.3.3.cmml" xref="S3.SS3.p1.2.m2.3.3.3">𝑝</ci><apply id="S3.SS3.p1.2.m2.3.3.1.1.1.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1"><csymbol cd="latexml" id="S3.SS3.p1.2.m2.3.3.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1.1.1">conditional</csymbol><apply id="S3.SS3.p1.2.m2.3.3.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.3.3.1.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1.1.2">subscript</csymbol><ci id="S3.SS3.p1.2.m2.3.3.1.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1.1.2.2">𝑦</ci><ci id="S3.SS3.p1.2.m2.3.3.1.1.1.2.3.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1.1.2.3">𝑒</ci></apply><list id="S3.SS3.p1.2.m2.3.3.1.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.3.3.1.1.1.3.2"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><ci id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1">^</ci><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝑦</ci></apply><ci id="S3.SS3.p1.2.m2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2">𝑦</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.3c">p(y_{e}|\hat{y},y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.3d">italic_p ( italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT | over^ start_ARG italic_y end_ARG , italic_y )</annotation></semantics></math>, the probability that a label is wrong given the prediction of a HPE model <math alttext="\hat{y}\in\mathbb{R}^{2}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mover accent="true" id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2.2" xref="S3.SS3.p1.3.m3.1.1.2.2.cmml">y</mi><mo id="S3.SS3.p1.3.m3.1.1.2.1" xref="S3.SS3.p1.3.m3.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.3.2" xref="S3.SS3.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS3.p1.3.m3.1.1.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><in id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></in><apply id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2"><ci id="S3.SS3.p1.3.m3.1.1.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1.2.1">^</ci><ci id="S3.SS3.p1.3.m3.1.1.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2.2">𝑦</ci></apply><apply id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.2">ℝ</ci><cn id="S3.SS3.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS3.p1.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\hat{y}\in\mathbb{R}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">over^ start_ARG italic_y end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> and the (potentially faulty) annotation <math alttext="y\in\mathbb{R}^{2}" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">y</mi><mo id="S3.SS3.p1.4.m4.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.2" xref="S3.SS3.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS3.p1.4.m4.1.1.3.3" xref="S3.SS3.p1.4.m4.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><in id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1"></in><ci id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2">𝑦</ci><apply id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2">ℝ</ci><cn id="S3.SS3.p1.4.m4.1.1.3.3.cmml" type="integer" xref="S3.SS3.p1.4.m4.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">y\in\mathbb{R}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_y ∈ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. We assume that the majority of data points is correctly annotated and that a HPE model has learned to make accurate predictions. Large deviations between prediction <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><mover accent="true" id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml">y</mi><mo id="S3.SS3.p1.5.m5.1.1.1" xref="S3.SS3.p1.5.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><ci id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1">^</ci><ci id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> and annotations <math alttext="y" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><mi id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><ci id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">italic_y</annotation></semantics></math> hence indicate labeling errors.
We assume that the large deviations between prediction <math alttext="\hat{y}_{m}" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m7.1"><semantics id="S3.SS3.p1.7.m7.1a"><msub id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml"><mover accent="true" id="S3.SS3.p1.7.m7.1.1.2" xref="S3.SS3.p1.7.m7.1.1.2.cmml"><mi id="S3.SS3.p1.7.m7.1.1.2.2" xref="S3.SS3.p1.7.m7.1.1.2.2.cmml">y</mi><mo id="S3.SS3.p1.7.m7.1.1.2.1" xref="S3.SS3.p1.7.m7.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p1.7.m7.1.1.3" xref="S3.SS3.p1.7.m7.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><apply id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">subscript</csymbol><apply id="S3.SS3.p1.7.m7.1.1.2.cmml" xref="S3.SS3.p1.7.m7.1.1.2"><ci id="S3.SS3.p1.7.m7.1.1.2.1.cmml" xref="S3.SS3.p1.7.m7.1.1.2.1">^</ci><ci id="S3.SS3.p1.7.m7.1.1.2.2.cmml" xref="S3.SS3.p1.7.m7.1.1.2.2">𝑦</ci></apply><ci id="S3.SS3.p1.7.m7.1.1.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">\hat{y}_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m7.1d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> and annotations <math alttext="y" class="ltx_Math" display="inline" id="S3.SS3.p1.8.m8.1"><semantics id="S3.SS3.p1.8.m8.1a"><mi id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.1b"><ci id="S3.SS3.p1.8.m8.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.8.m8.1d">italic_y</annotation></semantics></math> are similar for each HPE model <math alttext="m\in\{1,\dots,M\}" class="ltx_Math" display="inline" id="S3.SS3.p1.9.m9.3"><semantics id="S3.SS3.p1.9.m9.3a"><mrow id="S3.SS3.p1.9.m9.3.4" xref="S3.SS3.p1.9.m9.3.4.cmml"><mi id="S3.SS3.p1.9.m9.3.4.2" xref="S3.SS3.p1.9.m9.3.4.2.cmml">m</mi><mo id="S3.SS3.p1.9.m9.3.4.1" xref="S3.SS3.p1.9.m9.3.4.1.cmml">∈</mo><mrow id="S3.SS3.p1.9.m9.3.4.3.2" xref="S3.SS3.p1.9.m9.3.4.3.1.cmml"><mo id="S3.SS3.p1.9.m9.3.4.3.2.1" stretchy="false" xref="S3.SS3.p1.9.m9.3.4.3.1.cmml">{</mo><mn id="S3.SS3.p1.9.m9.1.1" xref="S3.SS3.p1.9.m9.1.1.cmml">1</mn><mo id="S3.SS3.p1.9.m9.3.4.3.2.2" xref="S3.SS3.p1.9.m9.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p1.9.m9.2.2" mathvariant="normal" xref="S3.SS3.p1.9.m9.2.2.cmml">…</mi><mo id="S3.SS3.p1.9.m9.3.4.3.2.3" xref="S3.SS3.p1.9.m9.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p1.9.m9.3.3" xref="S3.SS3.p1.9.m9.3.3.cmml">M</mi><mo id="S3.SS3.p1.9.m9.3.4.3.2.4" stretchy="false" xref="S3.SS3.p1.9.m9.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m9.3b"><apply id="S3.SS3.p1.9.m9.3.4.cmml" xref="S3.SS3.p1.9.m9.3.4"><in id="S3.SS3.p1.9.m9.3.4.1.cmml" xref="S3.SS3.p1.9.m9.3.4.1"></in><ci id="S3.SS3.p1.9.m9.3.4.2.cmml" xref="S3.SS3.p1.9.m9.3.4.2">𝑚</ci><set id="S3.SS3.p1.9.m9.3.4.3.1.cmml" xref="S3.SS3.p1.9.m9.3.4.3.2"><cn id="S3.SS3.p1.9.m9.1.1.cmml" type="integer" xref="S3.SS3.p1.9.m9.1.1">1</cn><ci id="S3.SS3.p1.9.m9.2.2.cmml" xref="S3.SS3.p1.9.m9.2.2">…</ci><ci id="S3.SS3.p1.9.m9.3.3.cmml" xref="S3.SS3.p1.9.m9.3.3">𝑀</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m9.3c">m\in\{1,\dots,M\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.9.m9.3d">italic_m ∈ { 1 , … , italic_M }</annotation></semantics></math> described in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S3.SS4" title="3.4 Evaluation Models ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">subsection 3.4</span></a>. We use the distance <math alttext="\delta_{m}" class="ltx_Math" display="inline" id="S3.SS3.p1.10.m10.1"><semantics id="S3.SS3.p1.10.m10.1a"><msub id="S3.SS3.p1.10.m10.1.1" xref="S3.SS3.p1.10.m10.1.1.cmml"><mi id="S3.SS3.p1.10.m10.1.1.2" xref="S3.SS3.p1.10.m10.1.1.2.cmml">δ</mi><mi id="S3.SS3.p1.10.m10.1.1.3" xref="S3.SS3.p1.10.m10.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m10.1b"><apply id="S3.SS3.p1.10.m10.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.10.m10.1.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS3.p1.10.m10.1.1.2.cmml" xref="S3.SS3.p1.10.m10.1.1.2">𝛿</ci><ci id="S3.SS3.p1.10.m10.1.1.3.cmml" xref="S3.SS3.p1.10.m10.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m10.1c">\delta_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.10.m10.1d">italic_δ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> between the predictions <math alttext="\hat{y}_{m}" class="ltx_Math" display="inline" id="S3.SS3.p1.11.m11.1"><semantics id="S3.SS3.p1.11.m11.1a"><msub id="S3.SS3.p1.11.m11.1.1" xref="S3.SS3.p1.11.m11.1.1.cmml"><mover accent="true" id="S3.SS3.p1.11.m11.1.1.2" xref="S3.SS3.p1.11.m11.1.1.2.cmml"><mi id="S3.SS3.p1.11.m11.1.1.2.2" xref="S3.SS3.p1.11.m11.1.1.2.2.cmml">y</mi><mo id="S3.SS3.p1.11.m11.1.1.2.1" xref="S3.SS3.p1.11.m11.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p1.11.m11.1.1.3" xref="S3.SS3.p1.11.m11.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m11.1b"><apply id="S3.SS3.p1.11.m11.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.11.m11.1.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1">subscript</csymbol><apply id="S3.SS3.p1.11.m11.1.1.2.cmml" xref="S3.SS3.p1.11.m11.1.1.2"><ci id="S3.SS3.p1.11.m11.1.1.2.1.cmml" xref="S3.SS3.p1.11.m11.1.1.2.1">^</ci><ci id="S3.SS3.p1.11.m11.1.1.2.2.cmml" xref="S3.SS3.p1.11.m11.1.1.2.2">𝑦</ci></apply><ci id="S3.SS3.p1.11.m11.1.1.3.cmml" xref="S3.SS3.p1.11.m11.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m11.1c">\hat{y}_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.11.m11.1d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> of model <math alttext="m\in\{1,\dots,M\}" class="ltx_Math" display="inline" id="S3.SS3.p1.12.m12.3"><semantics id="S3.SS3.p1.12.m12.3a"><mrow id="S3.SS3.p1.12.m12.3.4" xref="S3.SS3.p1.12.m12.3.4.cmml"><mi id="S3.SS3.p1.12.m12.3.4.2" xref="S3.SS3.p1.12.m12.3.4.2.cmml">m</mi><mo id="S3.SS3.p1.12.m12.3.4.1" xref="S3.SS3.p1.12.m12.3.4.1.cmml">∈</mo><mrow id="S3.SS3.p1.12.m12.3.4.3.2" xref="S3.SS3.p1.12.m12.3.4.3.1.cmml"><mo id="S3.SS3.p1.12.m12.3.4.3.2.1" stretchy="false" xref="S3.SS3.p1.12.m12.3.4.3.1.cmml">{</mo><mn id="S3.SS3.p1.12.m12.1.1" xref="S3.SS3.p1.12.m12.1.1.cmml">1</mn><mo id="S3.SS3.p1.12.m12.3.4.3.2.2" xref="S3.SS3.p1.12.m12.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p1.12.m12.2.2" mathvariant="normal" xref="S3.SS3.p1.12.m12.2.2.cmml">…</mi><mo id="S3.SS3.p1.12.m12.3.4.3.2.3" xref="S3.SS3.p1.12.m12.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p1.12.m12.3.3" xref="S3.SS3.p1.12.m12.3.3.cmml">M</mi><mo id="S3.SS3.p1.12.m12.3.4.3.2.4" stretchy="false" xref="S3.SS3.p1.12.m12.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.12.m12.3b"><apply id="S3.SS3.p1.12.m12.3.4.cmml" xref="S3.SS3.p1.12.m12.3.4"><in id="S3.SS3.p1.12.m12.3.4.1.cmml" xref="S3.SS3.p1.12.m12.3.4.1"></in><ci id="S3.SS3.p1.12.m12.3.4.2.cmml" xref="S3.SS3.p1.12.m12.3.4.2">𝑚</ci><set id="S3.SS3.p1.12.m12.3.4.3.1.cmml" xref="S3.SS3.p1.12.m12.3.4.3.2"><cn id="S3.SS3.p1.12.m12.1.1.cmml" type="integer" xref="S3.SS3.p1.12.m12.1.1">1</cn><ci id="S3.SS3.p1.12.m12.2.2.cmml" xref="S3.SS3.p1.12.m12.2.2">…</ci><ci id="S3.SS3.p1.12.m12.3.3.cmml" xref="S3.SS3.p1.12.m12.3.3">𝑀</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.12.m12.3c">m\in\{1,\dots,M\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.12.m12.3d">italic_m ∈ { 1 , … , italic_M }</annotation></semantics></math> and the ground truth <math alttext="y" class="ltx_Math" display="inline" id="S3.SS3.p1.13.m13.1"><semantics id="S3.SS3.p1.13.m13.1a"><mi id="S3.SS3.p1.13.m13.1.1" xref="S3.SS3.p1.13.m13.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.13.m13.1b"><ci id="S3.SS3.p1.13.m13.1.1.cmml" xref="S3.SS3.p1.13.m13.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.13.m13.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.13.m13.1d">italic_y</annotation></semantics></math></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\delta_{m}=\hat{y}_{m}-y" class="ltx_Math" display="inline" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">δ</mi><mi id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml">m</mi></msub><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mover accent="true" id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.3.2.2.2.cmml">y</mi><mo id="S3.E1.m1.1.1.3.2.2.1" xref="S3.E1.m1.1.1.3.2.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">m</mi></msub><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">−</mo><mi id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">y</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">𝛿</ci><ci id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3">𝑚</ci></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><minus id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></minus><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><apply id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2"><ci id="S3.E1.m1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.2.2.1">^</ci><ci id="S3.E1.m1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2">𝑦</ci></apply><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">𝑚</ci></apply><ci id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle\delta_{m}=\hat{y}_{m}-y</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_δ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT - italic_y</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.2">to create a feature vector <math alttext="\Delta=[\delta_{1},\delta_{2},\ldots,\delta_{M}]" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.4"><semantics id="S3.SS3.p2.1.m1.4a"><mrow id="S3.SS3.p2.1.m1.4.4" xref="S3.SS3.p2.1.m1.4.4.cmml"><mi id="S3.SS3.p2.1.m1.4.4.5" mathvariant="normal" xref="S3.SS3.p2.1.m1.4.4.5.cmml">Δ</mi><mo id="S3.SS3.p2.1.m1.4.4.4" xref="S3.SS3.p2.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS3.p2.1.m1.4.4.3.3" xref="S3.SS3.p2.1.m1.4.4.3.4.cmml"><mo id="S3.SS3.p2.1.m1.4.4.3.3.4" stretchy="false" xref="S3.SS3.p2.1.m1.4.4.3.4.cmml">[</mo><msub id="S3.SS3.p2.1.m1.2.2.1.1.1" xref="S3.SS3.p2.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.2.2.1.1.1.2" xref="S3.SS3.p2.1.m1.2.2.1.1.1.2.cmml">δ</mi><mn id="S3.SS3.p2.1.m1.2.2.1.1.1.3" xref="S3.SS3.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p2.1.m1.4.4.3.3.5" xref="S3.SS3.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS3.p2.1.m1.3.3.2.2.2" xref="S3.SS3.p2.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS3.p2.1.m1.3.3.2.2.2.2" xref="S3.SS3.p2.1.m1.3.3.2.2.2.2.cmml">δ</mi><mn id="S3.SS3.p2.1.m1.3.3.2.2.2.3" xref="S3.SS3.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p2.1.m1.4.4.3.3.6" xref="S3.SS3.p2.1.m1.4.4.3.4.cmml">,</mo><mi id="S3.SS3.p2.1.m1.1.1" mathvariant="normal" xref="S3.SS3.p2.1.m1.1.1.cmml">…</mi><mo id="S3.SS3.p2.1.m1.4.4.3.3.7" xref="S3.SS3.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS3.p2.1.m1.4.4.3.3.3" xref="S3.SS3.p2.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS3.p2.1.m1.4.4.3.3.3.2" xref="S3.SS3.p2.1.m1.4.4.3.3.3.2.cmml">δ</mi><mi id="S3.SS3.p2.1.m1.4.4.3.3.3.3" xref="S3.SS3.p2.1.m1.4.4.3.3.3.3.cmml">M</mi></msub><mo id="S3.SS3.p2.1.m1.4.4.3.3.8" stretchy="false" xref="S3.SS3.p2.1.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.4b"><apply id="S3.SS3.p2.1.m1.4.4.cmml" xref="S3.SS3.p2.1.m1.4.4"><eq id="S3.SS3.p2.1.m1.4.4.4.cmml" xref="S3.SS3.p2.1.m1.4.4.4"></eq><ci id="S3.SS3.p2.1.m1.4.4.5.cmml" xref="S3.SS3.p2.1.m1.4.4.5">Δ</ci><list id="S3.SS3.p2.1.m1.4.4.3.4.cmml" xref="S3.SS3.p2.1.m1.4.4.3.3"><apply id="S3.SS3.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.2.2.1.1.1.2">𝛿</ci><cn id="S3.SS3.p2.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS3.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS3.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS3.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS3.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.3.3.2.2.2.2">𝛿</ci><cn id="S3.SS3.p2.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS3.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">…</ci><apply id="S3.SS3.p2.1.m1.4.4.3.3.3.cmml" xref="S3.SS3.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS3.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS3.p2.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS3.p2.1.m1.4.4.3.3.3.2">𝛿</ci><ci id="S3.SS3.p2.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS3.p2.1.m1.4.4.3.3.3.3">𝑀</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.4c">\Delta=[\delta_{1},\delta_{2},\ldots,\delta_{M}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.4d">roman_Δ = [ italic_δ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_δ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_δ start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ]</annotation></semantics></math>. Our heuristic aggregates the distances of single joints estimates into one score by modeling the distribution of deviations <math alttext="p(\Delta)" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.2" xref="S3.SS3.p2.2.m2.1.2.cmml"><mi id="S3.SS3.p2.2.m2.1.2.2" xref="S3.SS3.p2.2.m2.1.2.2.cmml">p</mi><mo id="S3.SS3.p2.2.m2.1.2.1" xref="S3.SS3.p2.2.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p2.2.m2.1.2.3.2" xref="S3.SS3.p2.2.m2.1.2.cmml"><mo id="S3.SS3.p2.2.m2.1.2.3.2.1" stretchy="false" xref="S3.SS3.p2.2.m2.1.2.cmml">(</mo><mi id="S3.SS3.p2.2.m2.1.1" mathvariant="normal" xref="S3.SS3.p2.2.m2.1.1.cmml">Δ</mi><mo id="S3.SS3.p2.2.m2.1.2.3.2.2" stretchy="false" xref="S3.SS3.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.2.cmml" xref="S3.SS3.p2.2.m2.1.2"><times id="S3.SS3.p2.2.m2.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.2.1"></times><ci id="S3.SS3.p2.2.m2.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.2.2">𝑝</ci><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">Δ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">p(\Delta)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_p ( roman_Δ )</annotation></semantics></math> across all models</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx2">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle p(y_{e}|\hat{y},y)=1-p(\Delta)," class="ltx_Math" display="inline" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1.1" xref="S3.E2.m1.4.4.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.3.cmml">p</mi><mo id="S3.E2.m1.4.4.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.4.4.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.2.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml">e</mi></msub><mo fence="false" id="S3.E2.m1.4.4.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml"><mover accent="true" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">y</mi><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">^</mo></mover><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">y</mi></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.2" xref="S3.E2.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml"><mn id="S3.E2.m1.4.4.1.1.3.2" xref="S3.E2.m1.4.4.1.1.3.2.cmml">1</mn><mo id="S3.E2.m1.4.4.1.1.3.1" xref="S3.E2.m1.4.4.1.1.3.1.cmml">−</mo><mrow id="S3.E2.m1.4.4.1.1.3.3" xref="S3.E2.m1.4.4.1.1.3.3.cmml"><mi id="S3.E2.m1.4.4.1.1.3.3.2" xref="S3.E2.m1.4.4.1.1.3.3.2.cmml">p</mi><mo id="S3.E2.m1.4.4.1.1.3.3.1" xref="S3.E2.m1.4.4.1.1.3.3.1.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.3.3.3.2" xref="S3.E2.m1.4.4.1.1.3.3.cmml"><mo id="S3.E2.m1.4.4.1.1.3.3.3.2.1" stretchy="false" xref="S3.E2.m1.4.4.1.1.3.3.cmml">(</mo><mi id="S3.E2.m1.3.3" mathvariant="normal" xref="S3.E2.m1.3.3.cmml">Δ</mi><mo id="S3.E2.m1.4.4.1.1.3.3.3.2.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1"><eq id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2"></eq><apply id="S3.E2.m1.4.4.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1"><times id="S3.E2.m1.4.4.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.2"></times><ci id="S3.E2.m1.4.4.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.3">𝑝</ci><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2">𝑦</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.3">𝑒</ci></apply><list id="S3.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><ci id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">^</ci><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">𝑦</ci></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑦</ci></list></apply></apply><apply id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3"><minus id="S3.E2.m1.4.4.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3.1"></minus><cn id="S3.E2.m1.4.4.1.1.3.2.cmml" type="integer" xref="S3.E2.m1.4.4.1.1.3.2">1</cn><apply id="S3.E2.m1.4.4.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.3.3"><times id="S3.E2.m1.4.4.1.1.3.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3.3.1"></times><ci id="S3.E2.m1.4.4.1.1.3.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.3.2">𝑝</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">Δ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\displaystyle p(y_{e}|\hat{y},y)=1-p(\Delta),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_p ( italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT | over^ start_ARG italic_y end_ARG , italic_y ) = 1 - italic_p ( roman_Δ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.3">For estimating <math alttext="p(\Delta)" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.1"><semantics id="S3.SS3.p4.1.m1.1a"><mrow id="S3.SS3.p4.1.m1.1.2" xref="S3.SS3.p4.1.m1.1.2.cmml"><mi id="S3.SS3.p4.1.m1.1.2.2" xref="S3.SS3.p4.1.m1.1.2.2.cmml">p</mi><mo id="S3.SS3.p4.1.m1.1.2.1" xref="S3.SS3.p4.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p4.1.m1.1.2.3.2" xref="S3.SS3.p4.1.m1.1.2.cmml"><mo id="S3.SS3.p4.1.m1.1.2.3.2.1" stretchy="false" xref="S3.SS3.p4.1.m1.1.2.cmml">(</mo><mi id="S3.SS3.p4.1.m1.1.1" mathvariant="normal" xref="S3.SS3.p4.1.m1.1.1.cmml">Δ</mi><mo id="S3.SS3.p4.1.m1.1.2.3.2.2" stretchy="false" xref="S3.SS3.p4.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.2"><times id="S3.SS3.p4.1.m1.1.2.1.cmml" xref="S3.SS3.p4.1.m1.1.2.1"></times><ci id="S3.SS3.p4.1.m1.1.2.2.cmml" xref="S3.SS3.p4.1.m1.1.2.2">𝑝</ci><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">Δ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">p(\Delta)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.1d">italic_p ( roman_Δ )</annotation></semantics></math> we use a well established non-parametric outlier detection method, an <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.3.1">Isolation Forest</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib41" title="">41</a>]</cite> as implemented in PyOD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib42" title="">42</a>]</cite>. We emphasize that the choice of the outlier detection method is not the key factor for the label noise detection proposed in this work. The relevant functionality is a non-parametric density estimator that approximates the distribution of deviations <math alttext="p(\Delta)" class="ltx_Math" display="inline" id="S3.SS3.p4.2.m2.1"><semantics id="S3.SS3.p4.2.m2.1a"><mrow id="S3.SS3.p4.2.m2.1.2" xref="S3.SS3.p4.2.m2.1.2.cmml"><mi id="S3.SS3.p4.2.m2.1.2.2" xref="S3.SS3.p4.2.m2.1.2.2.cmml">p</mi><mo id="S3.SS3.p4.2.m2.1.2.1" xref="S3.SS3.p4.2.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p4.2.m2.1.2.3.2" xref="S3.SS3.p4.2.m2.1.2.cmml"><mo id="S3.SS3.p4.2.m2.1.2.3.2.1" stretchy="false" xref="S3.SS3.p4.2.m2.1.2.cmml">(</mo><mi id="S3.SS3.p4.2.m2.1.1" mathvariant="normal" xref="S3.SS3.p4.2.m2.1.1.cmml">Δ</mi><mo id="S3.SS3.p4.2.m2.1.2.3.2.2" stretchy="false" xref="S3.SS3.p4.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.2.cmml" xref="S3.SS3.p4.2.m2.1.2"><times id="S3.SS3.p4.2.m2.1.2.1.cmml" xref="S3.SS3.p4.2.m2.1.2.1"></times><ci id="S3.SS3.p4.2.m2.1.2.2.cmml" xref="S3.SS3.p4.2.m2.1.2.2">𝑝</ci><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">Δ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">p(\Delta)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.2.m2.1d">italic_p ( roman_Δ )</annotation></semantics></math> given the multivariate feature vectors <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS3.p4.3.m3.1"><semantics id="S3.SS3.p4.3.m3.1a"><mi id="S3.SS3.p4.3.m3.1.1" mathvariant="normal" xref="S3.SS3.p4.3.m3.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.3.m3.1d">roman_Δ</annotation></semantics></math>. Indeed we find most other methods commonly used for outlier detection to work equally well.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation Models</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We select five different top-down approach models using the MMPose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib43" title="">43</a>]</cite> library to create and evaluate our label noise detection.
MMPose supports various 2D human pose estimation model architectures and data sets. Therefore, it enabled us to keep evaluation standardized and fair by using the configuration files for each model listed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.T2" title="Table 2 ‣ 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Table 2</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.T3" title="Table 3 ‣ 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Table 3</span></a>. To simplify the reproduction of results we used pre-trained model checkpoints provided by MMPose to generate the predictions for the outlier detection. The model performances on the original validation set for MPII and COCO, reported in this paper, are all consistent with scores reported by MMPose<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mmpose.readthedocs.io/en/latest/overview.html" title="">https://mmpose.readthedocs.io/en/latest/overview.html</a></span></span></span>. Deviations in the COCO results compared to the results listed online are due to the fact that ground truth bounding boxes were used for the COCO evaluation, to avoid an influence of the bounding box estimation errors on the metric evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">We ensured models were comparable in size and performance. Model selection was mostly kept consistent between the COCO and MPII data set. However, for models pre-trained on the COCO data set there was no Hourglass model available that shared the same input size (256x192) with the other models. For COCO, we therefore, exchanged Hourglass for ResNeSt to maintain higher unity between models.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Per Keypoint Distance for MPII and COCO</h3>
<div class="ltx_para ltx_noindent" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Our non-parametric outlier detection requires the distance errors per joint. In order to extract this information we made the following modifications to the MPII and COCO evaluation pipelines. For MPII we modified the MMPose evaluation script so that per keypoint prediction to ground truth distance were saved in addition to the mean score values.
For the COCO data set, we used a modified version of the original COCO evaluation code<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/cocodataset/cocoapi" title="">https://github.com/cocodataset/cocoapi</a></span></span></span>. For each model prediction we saved the OKS metric per joint and pose as well as the distance for prediction to ground truth. Note that we did not use the ’raw’ distance between prediction and ground truth but the modified version that takes the OKS <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_σ</annotation></semantics></math> values and the object scale into consideration.
We want to note here that a person in an image can have multiple predictions, which, during the OKS calculation, are filtered using different IoU (intersection over union) thresholds. A lower threshold comes with a higher recall, which means more poses are found. For the heuristic we determined to use the ’loose’ 0.5 IoU threshold to extract distance and OKS score per joint.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Calibrating the Outlier Threshold</h3>
<div class="ltx_para ltx_noindent" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">The outlier score threshold for discarding presumably faulty annotations was calibrated as follows.
We assume that model predictions are less reliable for keypoints for which there is no annotation, for instance because the body part was not on the image plane. HPE models will produce predictions for those poses, but usually these keypoints are not included in the evaluation of HPE models. Here we included these keypoints and computed outlier scores for all keypoints. In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S3.F4" title="Figure 4 ‣ 3.6 Calibrating the Outlier Threshold ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>a we show the distribution of all outlier scores for poses with and without keypoint annotations. In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S3.F4" title="Figure 4 ‣ 3.6 Calibrating the Outlier Threshold ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>b we show the outlier score distribution only for keypoints that do have an annotation. We assume that the erroneous annotations have similar characteristics to the non-annotated keypoints. Keypoints witout annotations form a distinct mode of the outlier score distribution in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S3.F4" title="Figure 4 ‣ 3.6 Calibrating the Outlier Threshold ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>a. We set the threshold for each data set individually such that outlier scores smaller than the mode of keypoints without annotations are detected as outliers. For the COCO data set the threshold was thus set to 0.75 and for MPII the threshold was set to 0.35.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="311" id="S3.F4.sf1.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>The score distribution of all possible keypoints, including the keypoints without annotation.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="311" id="S3.F4.sf2.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>The outlier scores only for the keypoints for which an annotation exists.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>We determine the threshold for faulty annotations using the histograms of outlier scores for poses on images with (a) and without (b) annotations. Including keypoints without annotations, for which predictions are assumed to be less reliable, indicate a clear second mode in the outlier score distribution (compare a vs b). We set the threshold for faulty annotations to exclude all annotations that are as faulty (according to our heuristic) as keypoint predictions without annotations.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Evaluation</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We conduct a series of experiments and analysis to evaluate the effectiveness of our heuristics. We investigate how refining the validation data set influences model outcomes and assess the effects of various models on the enhanced, label-noise-free data sets. Our comparison includes examining the effect of faulty labels on evaluation metrics for COCO and MPII. We address distinguishing ‘hard’ and ‘faulty’ cases by contrasting manually hand-cleaned data sets (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.SS2" title="4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">4.2</span></a>) with sets created by our heuristics. Additionally, we examine the influence of annotation errors on the the heatmaps (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.SS5" title="4.5 Impact of Annotation Jitter ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Label Error Detection Evaluation</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Heuristics evaluation on training set:</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">To evaluate the performance of the heuristic on the training data sets, we instructed two annotators to label 100 of the 4416 poses in the COCO training set and 100 of the 1102 poses in the MPII training data that were detected by our heuristic.
For the COCO data our heuristic detects faulty labels with an average precision of about 36% and an average recognition rate of 31%.
For the MPII data set, the heuristic detects faulty annotations with an average precision of about 41% and an average recognition rate of 25%.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Heuristics evaluation on validation set:</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">In order to evaluate the quality of our heuristic to detect faulty annotations in the validation data we instructed three annotators to identify faulty annotations manually on the validation data flagged as faulty by our heuristic. On validation data our heuristic detects faulty annotations with an average precision of about 16% and an average recall of 22% for the MPII validation data set and an average precision of about 15% and an average recall of 33% for the COCO validation data set.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p2.1">We assume that there are several reasons why our heuristics’ performance differs between the training set and the validation set. The models have much better results on the training set, than on the validation set, so separating between faulty and non-faulty keypoints should become easier. Also the annotators have different opinions on keypoint errors, concentration levels and set their focus on different keypoints/body parts. Moreover, the labeling task becomes tedious. Bazarevsky et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib44" title="">44</a>]</cite> show that when they let two annotators relabel a data set, they achieve an average PCK@0.2 of 97.2. This raises questions on the annotation performance in HPE tasks.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Impact of Cleaning Validation Data</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The results in the previous section demonstrate that the proposed heuristic reliably detects faulty labels. To evaluate the impact of erroneous labels on the HPE evaluation metrics in commonly used benchmarks, we first investigate the impact of cleaning the validation data sets. We list the results for the COCO data in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.T2" title="Table 2 ‣ 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Table 2</span></a> and the results for the MPII data in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.T3" title="Table 3 ‣ 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Table 3</span></a>. For all metrics we compare results on the original validation data (RAW), the validation data cleaned automatically with our proposed heuristic, here referred to as <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.1">auto cleaned</em> and the impact of a partial cleanup by a human annotator (HC). We used the annotations from the error frequency analysis (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S3.SS2" title="3.2 Data Set Errors ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">subsection 3.2</span></a>) to clean the data for the HC set.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Impact of cleaning on HPE metrics computed on COCO data. Compared are metrics obtained on the original data set (RAW), the manually cleaned set (HC) and the automatically cleaned data set (AC). Results obtained when cleaning training data (TC) are listed in the bottom row. Predictive performance is improved when training data is cleaned with the proposed heuristic. Cleaning validation data leads to slight improvements, too. The input size for all models listed was 256x192</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.1.1.1.1"></th>
<td class="ltx_td ltx_align_center" colspan="3" id="S4.T2.1.1.1.2">AP</td>
<td class="ltx_td ltx_align_center" colspan="3" id="S4.T2.1.1.1.3">AR</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.1.2.2.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.2.2.2">RAW</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.2.2.3">HC</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.2.2.4">AC</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.2.2.5">RAW</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.2.2.6">HC</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.2.2.7">AC</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T2.1.3.3.1">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib45" title="">45</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.3.3.2">73.6</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.3.3.3">73.6</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.3.3.4">74.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.3.3.5">76.6</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.3.3.6">76.7</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.3.3.7">77.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.4.1">ResNeSt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib46" title="">46</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.2">73.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.3">73.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.4">74.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.5">76.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.6">76.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.7">77.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.5.1">SE-ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib47" title="">47</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.2">74.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.3">74.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.4">75.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.5">77.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.6">77.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.7">77.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.6.1">SCNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib48" title="">48</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.2">74.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.3">74.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.4">75.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.5">77.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.6">77.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.7">78.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.7.7.1">HRNet_w32 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib17" title="">17</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.2">76.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.3">76.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.4">77.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.5">79.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.6">79.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.7">80.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_tt" id="S4.T2.1.8.8.1">HRNet_w32 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib17" title="">17</a>]</cite> TC</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T2.1.8.8.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.8.8.2.1">76.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T2.1.8.8.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.8.8.3.1">76.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T2.1.8.8.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.8.8.4.1">77.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T2.1.8.8.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.8.8.5.1">79.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T2.1.8.8.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.8.8.6.1">79.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T2.1.8.8.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.8.8.7.1">80.2</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Impact of cleaning on HPE metrics computed on MPII data set. Compared are metrics obtained on the original validation data set (RAW), the manually cleaned set (HC) and the automatically cleaned data set (AC). Results obtained when cleaning training data (TC) are listed in the bottom rows. Especially when removing faulty labels from the training data we observe improvements in predictive performance. The input size for all models was kept consistent to 256x256</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.1.1.1.1"></th>
<td class="ltx_td ltx_align_center" colspan="3" id="S4.T3.1.1.1.2">PCKh@0.5</td>
<td class="ltx_td ltx_align_center" colspan="3" id="S4.T3.1.1.1.3">PCKh@0.1</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.1.2.2.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.2.2.2">RAW</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.2.2.3">HC</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.2.2.4">AC</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.2.2.5">RAW</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.2.2.6">HC</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.2.2.7">AC</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T3.1.3.3.1">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib45" title="">45</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.3.3.2">88.2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.3.3.3">93.6</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.3.3.4">94.4</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.3.3.5">28.6</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.3.3.6">66.7</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.3.3.7">67.3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.4.4.1">SE-ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib46" title="">46</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4.2">88.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4.3">93.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4.4">94.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4.5">29.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4.6">66.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4.7">67.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.5.5.1">SCNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib48" title="">48</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5.2">88.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5.3">93.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5.4">94.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5.5">29.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5.6">67.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5.7">67.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.6.6.1">Hourglass52 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib49" title="">49</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.6.2">88.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.6.3">94.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.6.4">94.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.6.5">31.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.6.6">68.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.6.7">68.8</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.7.7.1">HRNet_w32 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib17" title="">17</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.7.2">90.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.7.3">94.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.7.4">95.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.7.5">33.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.7.6"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.7.6.1">69.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.7.7"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.7.7.1">70.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_tt" id="S4.T3.1.8.8.1">HRNet_w32 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib17" title="">17</a>]</cite> TC</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T3.1.8.8.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.8.8.2.1">90.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T3.1.8.8.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.8.8.3.1">94.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T3.1.8.8.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.8.8.4.1">95.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T3.1.8.8.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.8.8.5.1">33.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T3.1.8.8.6">69.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T3.1.8.8.7">70.2</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">For the MPII validation data set, we discard 469(1.1%) for auto clean and 161(0.4%) for hand clean and for COCO auto clean 377(0.6%) and hand clean 185(0.3%) keypoint annotations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Our results demonstrate that discarding faulty annotations from the evaluation data improves metrics across the board slightly. In some cases the improvements are substantial. For the models trained with the MPII data set, we can see (see additional material) a significant improvement in the results for the Hips, Knees and Ankles. These body parts seem to be more affected by errors in our studies (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S3.F3" title="Figure 3 ‣ 3.1 Data Set Selection ‣ 3 Method ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>). We can also achieve better results for the COCO data set and the benchmark metric with cleaned validation data sets. This time, however, the improvements are significantly smaller. This is especially true for the hand-adjusted validation data set. The modest improvements on COCO could be attributed to the use of a more stringent metric compared to PCKh and we cleaned a smaller subset for COCO. Nonetheless, we can also see a change in the leaderborad order for the SE-ResNet50 and SCNet50 model for the manually cleaned part.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Also, we observe a significant decrease in PCKh@0.5 score variance for the five models on the respective adjusted validation data set. For the raw data set variance was 0.38, which shrunk to 0.09 for the manually cleaned set and 0.08 for the automatically cleaned set. This suggests that the models for the MPII data set are likely to perform similarly well. No such significant decrease is observed for the COCO data set.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="277" id="S4.F5.sf1.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>PCKh@0.5 (MPII) - We achieve <math alttext="509.05\sigma" class="ltx_Math" display="inline" id="S4.F5.sf1.3.m1.1"><semantics id="S4.F5.sf1.3.m1.1b"><mrow id="S4.F5.sf1.3.m1.1.1" xref="S4.F5.sf1.3.m1.1.1.cmml"><mn id="S4.F5.sf1.3.m1.1.1.2" xref="S4.F5.sf1.3.m1.1.1.2.cmml">509.05</mn><mo id="S4.F5.sf1.3.m1.1.1.1" xref="S4.F5.sf1.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.F5.sf1.3.m1.1.1.3" xref="S4.F5.sf1.3.m1.1.1.3.cmml">σ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.sf1.3.m1.1c"><apply id="S4.F5.sf1.3.m1.1.1.cmml" xref="S4.F5.sf1.3.m1.1.1"><times id="S4.F5.sf1.3.m1.1.1.1.cmml" xref="S4.F5.sf1.3.m1.1.1.1"></times><cn id="S4.F5.sf1.3.m1.1.1.2.cmml" type="float" xref="S4.F5.sf1.3.m1.1.1.2">509.05</cn><ci id="S4.F5.sf1.3.m1.1.1.3.cmml" xref="S4.F5.sf1.3.m1.1.1.3">𝜎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.sf1.3.m1.1d">509.05\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.F5.sf1.3.m1.1e">509.05 italic_σ</annotation></semantics></math> for the manually cleaned sample and <math alttext="325.36\sigma" class="ltx_Math" display="inline" id="S4.F5.sf1.4.m2.1"><semantics id="S4.F5.sf1.4.m2.1b"><mrow id="S4.F5.sf1.4.m2.1.1" xref="S4.F5.sf1.4.m2.1.1.cmml"><mn id="S4.F5.sf1.4.m2.1.1.2" xref="S4.F5.sf1.4.m2.1.1.2.cmml">325.36</mn><mo id="S4.F5.sf1.4.m2.1.1.1" xref="S4.F5.sf1.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.F5.sf1.4.m2.1.1.3" xref="S4.F5.sf1.4.m2.1.1.3.cmml">σ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.sf1.4.m2.1c"><apply id="S4.F5.sf1.4.m2.1.1.cmml" xref="S4.F5.sf1.4.m2.1.1"><times id="S4.F5.sf1.4.m2.1.1.1.cmml" xref="S4.F5.sf1.4.m2.1.1.1"></times><cn id="S4.F5.sf1.4.m2.1.1.2.cmml" type="float" xref="S4.F5.sf1.4.m2.1.1.2">325.36</cn><ci id="S4.F5.sf1.4.m2.1.1.3.cmml" xref="S4.F5.sf1.4.m2.1.1.3">𝜎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.sf1.4.m2.1d">325.36\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.F5.sf1.4.m2.1e">325.36 italic_σ</annotation></semantics></math> for the automatically cleaned sample. Both results therefore show a significant influence on the metrics.
</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="277" id="S4.F5.sf2.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>mAP (COCO) - We achieve <math alttext="2.97\sigma" class="ltx_Math" display="inline" id="S4.F5.sf2.3.m1.1"><semantics id="S4.F5.sf2.3.m1.1b"><mrow id="S4.F5.sf2.3.m1.1.1" xref="S4.F5.sf2.3.m1.1.1.cmml"><mn id="S4.F5.sf2.3.m1.1.1.2" xref="S4.F5.sf2.3.m1.1.1.2.cmml">2.97</mn><mo id="S4.F5.sf2.3.m1.1.1.1" xref="S4.F5.sf2.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.F5.sf2.3.m1.1.1.3" xref="S4.F5.sf2.3.m1.1.1.3.cmml">σ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.sf2.3.m1.1c"><apply id="S4.F5.sf2.3.m1.1.1.cmml" xref="S4.F5.sf2.3.m1.1.1"><times id="S4.F5.sf2.3.m1.1.1.1.cmml" xref="S4.F5.sf2.3.m1.1.1.1"></times><cn id="S4.F5.sf2.3.m1.1.1.2.cmml" type="float" xref="S4.F5.sf2.3.m1.1.1.2">2.97</cn><ci id="S4.F5.sf2.3.m1.1.1.3.cmml" xref="S4.F5.sf2.3.m1.1.1.3">𝜎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.sf2.3.m1.1d">2.97\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.F5.sf2.3.m1.1e">2.97 italic_σ</annotation></semantics></math> for the manually cleaned sample and <math alttext="25.74\sigma" class="ltx_Math" display="inline" id="S4.F5.sf2.4.m2.1"><semantics id="S4.F5.sf2.4.m2.1b"><mrow id="S4.F5.sf2.4.m2.1.1" xref="S4.F5.sf2.4.m2.1.1.cmml"><mn id="S4.F5.sf2.4.m2.1.1.2" xref="S4.F5.sf2.4.m2.1.1.2.cmml">25.74</mn><mo id="S4.F5.sf2.4.m2.1.1.1" xref="S4.F5.sf2.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.F5.sf2.4.m2.1.1.3" xref="S4.F5.sf2.4.m2.1.1.3.cmml">σ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.sf2.4.m2.1c"><apply id="S4.F5.sf2.4.m2.1.1.cmml" xref="S4.F5.sf2.4.m2.1.1"><times id="S4.F5.sf2.4.m2.1.1.1.cmml" xref="S4.F5.sf2.4.m2.1.1.1"></times><cn id="S4.F5.sf2.4.m2.1.1.2.cmml" type="float" xref="S4.F5.sf2.4.m2.1.1.2">25.74</cn><ci id="S4.F5.sf2.4.m2.1.1.3.cmml" xref="S4.F5.sf2.4.m2.1.1.3">𝜎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.sf2.4.m2.1d">25.74\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.F5.sf2.4.m2.1e">25.74 italic_σ</annotation></semantics></math> for the automatically cleaned sample. Both results therefore show a significant influence on the metrics.
</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparing the accuracy impact of randomly removed keypoints across 1000 repetitions and in comparision hand-cleaned and auto-cleaned data sets.</figcaption>
</figure>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Influence of omitting key points on accuracy:</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">To measure the influence of cleaned validation data we illustrate the impact of excluding keypoints from the evaluation on metrics (PCKh and mAP) through HRNet and the corresponding validation set, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#S4.F5" title="Figure 5 ‣ 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">5</span></a>. We randomly discarded the same number of keypoints as in the manual and automatic cleanup for the respective data sets and repeated this 1000 times. We can see a distribution of the influence of removing the random keypoints. The mean of each distribution reflects the original score (gray lines - RAW-score- in the <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.F5" title="Figure 5 ‣ 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>). We can also see that we achieve a significant score improvement with the hand-cleaned sets (HC) and the automatically cleaned sets (AC) for the MPII and COCO data set.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Impact of Cleaning Training Data</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Extending previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib10" title="">10</a>]</cite> that only investigated the influence of cleaning on evaluation data, as we did in the previous section, we also investigated the impact of cleaning on the training data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.T2" title="Table 2 ‣ 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Table 2</span></a> (COCO) and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.T3" title="Table 3 ‣ 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Table 3</span></a> (MPII) we observe improved predictive performance when re-training the HRNet model on cleaned data.
<br class="ltx_break"/>This effect can be seen for the model trained on cleaned MPII data for almost all body parts except the hips. In the error frequency analysis reported above, annotations for hip keypoints were often affected by labeling errors. Furthermore, hip joints have been reported before by Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib50" title="">50</a>]</cite> as "hard" to predict keypoints for models since their appearance is not always structurally obvious and often in need of more context information. We assume that similar difficulties exist for human annotators to detect hip joints correctly and therefore ground truth positions scatter a lot.
Nevertheless, the overall result shows that faulty training data influences model performances.
<br class="ltx_break"/>Comparing the improvements between the two data sets COCO and MPII we find that improvements for MPII are larger. We assume that this is due to the larger training set of the COCO data set and the higher complexity in COCO.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Impact of Hard Poses</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">One explanation for the improvements in evaluation metrics after cleaning the data sets with the proposed heuristic could be that we are not (only) removing faulty annotations – but just those annotations for poses that are difficult. We investigated this hypothesis with additional experiments.
Therefore, we asked three annotators to categorize images into the following three categories:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">easy: A pose is relatively easy to annotate without any further assumptions</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">hard: A pose is time-consuming to annotate or assumptions have to be made</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">impossible: No pose can be credibly annotated.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.SS4.p1.2">For the annotations of the COCO data set, we draw the ground truth bounding box on the images. For the annotations of the MPII data set, we draw the center on the image and a quadratic bounding box based on the scale information in the data.
The task for the annotators is to consider only the body parts within the bounding boxes and on the image plane.</p>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="356" id="S4.F6.sf1.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Comparison of the evaluation of the annotators between the sample set of the respective normal data sets and the respective sets discarded by our heuristics.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="356" id="S4.F6.sf2.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Comparison of the evaluation of the annotators for the normal data set between manually discarded and non-discarded keypoints.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Evaluation of the poses of the different sets by three annotators in the categories easy, hard and impossible.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">The evaluation shows that our heuristic indeed has the tendency to discard hard poses (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.F6.sf1" title="6(a) ‣ Figure 6 ‣ 4.4 Impact of Hard Poses ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">6(a)</span></a>), but we can also see that hard poses are generally more prone to incorrect annotations, as shown in the <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.F6.sf2" title="6(b) ‣ Figure 6 ‣ 4.4 Impact of Hard Poses ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">6(b)</span></a>. Consequently, our heuristic tends to exclude more challenging poses, which happen to also be incorrectly annotated in many cases. Moreover, the evaluation indicates that it also omits annotations that are classified easy by human annotators, suggesting that poses easy for humans might be difficult for models.
As the results of the re-training on the automatically cleaned training sets
in the <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.T2" title="Table 2 ‣ 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Table 2</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.T3" title="Table 3 ‣ 4.2 Impact of Cleaning Validation Data ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Table 3</span></a> show, both models slightly increase their performance on all relevant metrics.
Even though our heuristic also partially penalizes hard poses, the effect on the models are small (0.1%). Assuming that the model does not estimate easier poses better now than before, the models do not seem to benefit from hard poses during training.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Impact of Annotation Jitter</h3>
<div class="ltx_para ltx_noindent" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Current approaches utilize heatmaps for the prediction of keypoints. The accuracy correlates with the variance of these heatmaps; lower variance implies higher precision, while higher variance decreases accuracy which is influenced by several factors. Our findings indicate that the variance inherent in heatmaps stems not only from the model and its hyperparameter but is also significantly influenced by the data quality. This variability is introduced by human annotators who tend to place keypoints slightly differently, an effect considered in the creation of the OKS (Object Keypoint Similarity) score.
To measure the impact of this annotation jitter, we employed the Human3.6M data set <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib52" title="">52</a>]</cite>, which utilizes ground truth data generated by marker-based motion capture, thereby ensuring minimal annotation jitter. We trained an HRNet model on a subset of the Human3.6M data set and introduced annotation jitter by adding a random normal distribution to the ground truth data. The perturbations were set at <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mi id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><ci id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">italic_σ</annotation></semantics></math> levels of 0.5%, 1%, and 2% of the bounding box diagonal.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="146" id="S4.F7.g1" src="x10.png" width="622"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Comparison between annotation jitter and compression ratio using the Human3.6M dataset and HRNet. <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.F7.2.m1.1"><semantics id="S4.F7.2.m1.1b"><mi id="S4.F7.2.m1.1.1" xref="S4.F7.2.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.F7.2.m1.1c"><ci id="S4.F7.2.m1.1.1.cmml" xref="S4.F7.2.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.2.m1.1d">\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.F7.2.m1.1e">italic_σ</annotation></semantics></math> is the percentage of the diagonal of the bounding box used for the random normal distribution added to the ground truth data.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">We evaluate the 4 models based on the validation set and compress the results of the models to determine a compression ratio. We use the compression ratio as an indicator of how noisy the output heatmaps are. As shown in  <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.03887v2#S4.F7" title="Figure 7 ‣ 4.5 Impact of Annotation Jitter ‣ 4 Experiments and Evaluation ‣ The Influence of Faulty Labels in Data Sets on Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>, the compression ratio increases with the <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.SS5.p2.1.m1.1"><semantics id="S4.SS5.p2.1.m1.1a"><mi id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><ci id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.1.m1.1d">italic_σ</annotation></semantics></math>. This shows that more jitter produces noisier heatmaps. This is particularly relevant as current research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib53" title="">53</a>]</cite> focuses on the quality and noise of the heatmaps without investigating the reasons for this.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We show that the two most used data sets in the field of human pose estimation contain a variety of faulty annotations. These erroneous annotations have an impact on model training and evaluation. This should motivate further efforts to reach a better understanding of the data sets, which are commonly used in public benchmarks, and that ultimately drive scientific progress. One contribution to achieve this could be a more careful documentation of both data set creation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib54" title="">54</a>]</cite> and model development <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03887v2#bib.bib55" title="">55</a>]</cite>. Our results suggest that there is a need to improve model evaluation guidelines by creating more sophisticated testing sets, which also account for data quality in benchmark tasks.
Only when we know the flaws in the data we use can we truly interpret what our models are doing and where improvements can be made. Specifically for the MPII data set and its latest leaderboard scores, we can assume that we have reached a level in this benchmark, where we can no longer achieve significantly better results. The COCO data set on the other hand, with its more robust metrics leaves more room for further improvement. Furthermore, we show that the performance of HPE models does not only depend on hard cases of poses.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This research is funded by the German Research Foundation (DFG) - Project number: 528483508 - FIP 12.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli.

</span>
<span class="ltx_bibblock">3d human pose estimation in video with temporal convolutions and semi-supervised training, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Shanshe Wang, Siwei Ma, and Wen Gao.

</span>
<span class="ltx_bibblock">P-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part V</span>, pages 461–478. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jinlu Zhang, Zhigang Tu, Jianyu Yang, Yujin Chen, and Junsong Yuan.

</span>
<span class="ltx_bibblock">Mixste: Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Yu Zhan, Fenghai Li, Renliang Weng, and Wongun Choi.

</span>
<span class="ltx_bibblock">Ray3d: Ray-based 3d human pose estimation for monocular absolute 3d localization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 13116–13125, June 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Yating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang.

</span>
<span class="ltx_bibblock">Recovering 3d human mesh from monocular images: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, 45(12):15406–15425, December 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Aarohi Srivastava et al.

</span>
<span class="ltx_bibblock">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models, June 2023.

</span>
<span class="ltx_bibblock">arXiv:2206.04615 [cs, stat].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, Samira Ebrahimi Kahou, Vincent Michalski, Tal Arbel, Chris Pal, Gael Varoquaux, and Pascal Vincent.

</span>
<span class="ltx_bibblock">Accounting for Variance in Machine Learning Benchmarks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of Machine Learning and Systems</span>, 3:747–769, March 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Peter Steinbach, Felicita Gernhardt, Mahnoor Tanveer, Steve Schmerler, and Sebastian Starke.

</span>
<span class="ltx_bibblock">Machine Learning State-of-the-Art with Uncertainties, April 2022.

</span>
<span class="ltx_bibblock">arXiv:2204.05173 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Matteo Ruggero Ronchi and Pietro Perona.

</span>
<span class="ltx_bibblock">Benchmarking and error diagnosis in multi-instance pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017</span>, pages 369–378, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Curtis G. Northcutt, Anish Athalye, and Jonas Mueller.

</span>
<span class="ltx_bibblock">Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">arXiv:2103.14749 [cs, stat]</span>, November 2021.

</span>
<span class="ltx_bibblock">arXiv: 2103.14749.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ce Zheng, Wenhan Wu, Chen Chen, Taojiannan Yang, Sijie Zhu, Ju Shen, Nasser Kehtarnavaz, and Mubarak Shah.

</span>
<span class="ltx_bibblock">Deep learning-based human pose estimation: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">ACM Comput. Surv.</span>, 56(1), aug 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Luke K. Topham, Wasiq Khan, Dhiya Al-Jumeily, and Abir Hussain.

</span>
<span class="ltx_bibblock">Human body pose estimation for gait identification: A comprehensive survey of datasets and models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">ACM Computing Surveys</span>, 55(6):1–42, dec 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Pranjal Kumar, Siddhartha Chauhan, and Lalit Kumar Awasthi.

</span>
<span class="ltx_bibblock">Human pose estimation using deep learning: review, methodologies, progress and future research directions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">International Journal of Multimedia Information Retrieval</span>, 11(4):489–521, Dec 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Haoming Chen, Runyang Feng, Sifan Wu, Hao Xu, Fengcheng Zhou, and Zhenguang Liu.

</span>
<span class="ltx_bibblock">2d human pose estimation: a survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Multimedia Syst.</span>, 29(5):3115–3138, nov 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: common objects in context.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">CoRR</span>, abs/1405.0312, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele.

</span>
<span class="ltx_bibblock">2d human pose estimation: New benchmark and state of the art analysis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, June 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.

</span>
<span class="ltx_bibblock">Deep high-resolution representation learning for human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Junjie Huang, Zheng Zhu, Feng Guo, and Guan Huang.

</span>
<span class="ltx_bibblock">The devil is in the details: Delving into unbiased data processing for human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, June 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu.

</span>
<span class="ltx_bibblock">Distribution-aware coordinate representation for human pose estimation.

</span>
<span class="ltx_bibblock">pages 7091–7100, 06 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Huajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang.

</span>
<span class="ltx_bibblock">Polarized self-attention: Towards high-quality pixel-wise mapping.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Neurocomput.</span>, 506(C):158–167, sep 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo, Binyi Yin, Angang Du, Haoqian Wang, Xinyu Zhou, Erjin Zhou, Xiangyu Zhang, and Jian Sun.

</span>
<span class="ltx_bibblock">Learning delicate local representations for multi-person pose estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">CoRR</span>, abs/2003.04030, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Vitpose: Simple vision transformer baselines for human pose estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">ArXiv</span>, abs/2204.12484, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Won Kim, Byoung-Ju Choi, Eui Hong, Soo-Kyung Kim, and Doheon Lee.

</span>
<span class="ltx_bibblock">A taxonomy of dirty data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Data Min. Knowl. Discov.</span>, 7:81–99, 01 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M Sugiyama and Motoaki Kawanabe, Motoaki.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Machine Learning in Non-Stationary Environments Introduction to Covariate Shift Adaptation</span>.

</span>
<span class="ltx_bibblock">MIT Press, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zachary C. Lipton, Yu Xiang Wang, and Alexander J. Smola.

</span>
<span class="ltx_bibblock">Detecting and correcting for label shift with black box predictors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">35th Int. Conf. Mach. Learn. ICML 2018</span>, 7:4887–4897, February 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Christopher M Bishop.

</span>
<span class="ltx_bibblock">Training with Noise is Equivalent to Tikhonov Regularization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Neural Computation</span>, 7(1):108–116, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Sebastian Schelter, Tammo Rukat, and Felix Biessmann.

</span>
<span class="ltx_bibblock">JENGA-A framework to study the impact of data errors on the predictions of machine learning models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">24th international conference on extending database technology (EDBT)</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Rashida Hasan and Cheehung Henry Chu.

</span>
<span class="ltx_bibblock">Noise in datasets: What are the impacts on classification performance?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">International Conference on Pattern Recognition Applications and Methods</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Ziawasch Abedjan, Xu Chu, Dong Deng, Raul Castro Fernandez, Ihab F. Ilyas, Mourad Ouzzani, Paolo Papotti, Michael Stonebraker, and Nan Tang.

</span>
<span class="ltx_bibblock">Detecting data errors: Where are we and what needs to be done?

</span>
<span class="ltx_bibblock">Number 12, pages 993–1004, 2016.

</span>
<span class="ltx_bibblock">ISSN: 21508097 Publication Title: Proc. VLDB Endow. Volume: 9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Felix Biessmann, Jacek Golebiowski, Tammo Rukat, Dustin Lange, and Philipp Schmidt.

</span>
<span class="ltx_bibblock">Automated data validation in machine learning systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Bulletin of the IEEE Computer Society Technical Committee on Data Engineering</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Sebastian Schelter, Felix Biessmann, Tim Januschowski, David Salinas, Stephan Seufert, and Gyuri Szarvas.

</span>
<span class="ltx_bibblock">On Challenges in Machine Learning Model Management.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Bull. IEEE Comput. Soc. Tech. Comm. Data Eng.</span>, pages 5–13, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Eric Breck, Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich.

</span>
<span class="ltx_bibblock">Data Validation for Machine Learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">SysML</span>, pages 1–14, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Stephan Rabanser, Stephan Günnemann, and Zachary C. Lipton.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Failing loudly: An empirical study of methods for detecting dataset shift</span>.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Görkem Algan and Ilkay Ulusoy.

</span>
<span class="ltx_bibblock">Label noise types and their effects on deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">CoRR</span>, abs/2003.10471, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Derek Chong, Jenny Hong, and Christopher D. Manning.

</span>
<span class="ltx_bibblock">Detecting label errors by using pre-trained language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Conference on Empirical Methods in Natural Language Processing</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Naoki Kato, Tianqi Li, Kohei Nishino, and Yusuke Uchida.

</span>
<span class="ltx_bibblock">Improving multi-person pose estimation using label correction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">ArXiv</span>, abs/1811.03331, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Sam Johnson and Mark Everingham.

</span>
<span class="ltx_bibblock">Learning effective human pose estimation from inaccurate annotation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">CVPR 2011</span>, pages 1465–1472, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Jia Wan, Qiangqiang Wu, and Antoni B. Chan.

</span>
<span class="ltx_bibblock">Modeling noisy annotations for point-wise supervision.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, pages 1–16, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Yu Cheng, Yihao Ai, Bo Wang, Xinchao Wang, and Robby T. Tan.

</span>
<span class="ltx_bibblock">Bottom-up 2d pose estimation via dual anatomical centers for small-scale persons.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Pattern Recognition</span>, 139:109403, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.

</span>
<span class="ltx_bibblock">Isolation forest.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">2008 Eighth IEEE International Conference on Data Mining</span>, pages 413–422, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yue Zhao, Zain Nasrullah, and Zheng Li.

</span>
<span class="ltx_bibblock">Pyod: A python toolbox for scalable outlier detection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Journal of Machine Learning Research</span>, 20(96):1–7, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
MMPose Contributors.

</span>
<span class="ltx_bibblock">Openmmlab pose estimation toolbox and benchmark.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/open-mmlab/mmpose" title="">https://github.com/open-mmlab/mmpose</a>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Valentin Bazarevsky, Ivan Grishchenko, Karthik Raveendran, Tyler Zhu, Fan Zhang, and Matthias Grundmann.

</span>
<span class="ltx_bibblock">Blazepose: On-device real-time body pose tracking.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">CoRR</span>, abs/2006.10204, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">pages 770–778, 06 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Muller, R. Manmatha, Mu Li, and Alexander Smola.

</span>
<span class="ltx_bibblock">Resnest: Split-attention networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2004.08955</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Jie Hu, Li Shen, and Gang Sun.

</span>
<span class="ltx_bibblock">Squeeze-and-excitation networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 7132–7141, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Changhu Wang, and Jiashi Feng.

</span>
<span class="ltx_bibblock">Improving convolutional networks with self-calibrated convolutions.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 10096–10105, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Alejandro Newell, Kaiyu Yang, and Jia Deng.

</span>
<span class="ltx_bibblock">Stacked hourglass networks for human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">European conference on computer vision</span>, pages 483–499. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun.

</span>
<span class="ltx_bibblock">Cascaded pyramid network for multi-person pose estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 7103–7112, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.

</span>
<span class="ltx_bibblock">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Cristian Sminchisescu Catalin Ionescu, Fuxin Li.

</span>
<span class="ltx_bibblock">Latent structured models for human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">International Conference on Computer Vision</span>, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Yanjie Li, Sen Yang, Peidong Liu, Shoukui Zhang, Yunxiao Wang, Zhicheng Wang, Wankou Yang, and Shu-Tao Xia.

</span>
<span class="ltx_bibblock">Simcc: A simple coordinate classification perspective for human pose estimation.

</span>
<span class="ltx_bibblock">In Shai Avidan, Gabriel Brostow, Moustapha Cissé, Giovanni Maria Farinella, and Tal Hassner, editors, <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">Computer Vision – ECCV 2022</span>, pages 89–106, Cham, 2022. Springer Nature Switzerland.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford.

</span>
<span class="ltx_bibblock">Datasheets for Datasets, December 2021.

</span>
<span class="ltx_bibblock">arXiv:1803.09010 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.

</span>
<span class="ltx_bibblock">Model Cards for Model Reporting.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Proceedings of the Conference on Fairness, Accountability, and Transparency</span>, pages 220–229, January 2019.

</span>
<span class="ltx_bibblock">arXiv:1810.03993 [cs].

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<figure class="ltx_table" id="A1.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of the results on the COCO validation data set when evaluated on the original data set (RAW), the manually cleaned set (HC) and the automatically cleaned data set (AC). Results obtained when cleaning training data
(TC) are listed in the right column.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="A1.T4.1.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="A1.T4.1.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T4.1.1.1.3">ResNet50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T4.1.1.1.4">ResNeSt</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T4.1.1.1.5">SE-ResNet50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T4.1.1.1.6">SCNet50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A1.T4.1.1.1.7">HRNet_w32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T4.1.1.1.8">HRNet TC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row ltx_border_tt" id="A1.T4.1.2.1.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.2.1.1.1">AP</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A1.T4.1.2.1.2">RAW</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.1.2.1.3">73.6</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.1.2.1.4">73.8</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.1.2.1.5">74.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.1.2.1.6">74.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T4.1.2.1.7">76.6</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.1.2.1.8"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.8.1">76.7</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.3.2.1">HC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.3.2.2">73.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.3.2.3">73.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.3.2.4">74.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.3.2.5">74.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.3.2.6">76.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.3.2.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.3.2.7.1">76.8</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.4.3.1">AC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.4.3.2">74.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.4.3.3">74.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.4.3.4">75.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.4.3.5">75.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.4.3.6">77.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.4.3.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.4.3.7.1">77.6</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row" id="A1.T4.1.5.4.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.5.4.1.1">AP .5</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.5.4.2">RAW</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.5.4.3">92.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.5.4.4">92.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.5.4.5">92.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.5.4.6">92.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.5.4.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.5.4.7.1">93.6</span></td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.5.4.8">93.6</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.6.5.1">HC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.6.5.2">92.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.6.5.3">92.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.6.5.4">92.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.6.5.5">92.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.6.5.6"><span class="ltx_text ltx_font_bold" id="A1.T4.1.6.5.6.1">93.6</span></td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.6.5.7">93.6</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.7.6.1">AC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.7.6.2">92.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.7.6.3">92.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.7.6.4">92.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.7.6.5">92.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.7.6.6"><span class="ltx_text ltx_font_bold" id="A1.T4.1.7.6.6.1">93.6</span></td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.7.6.7">93.6</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7">
<th class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row" id="A1.T4.1.8.7.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.8.7.1.1">AP .75</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.8.7.2">RAW</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.8.7.3">81.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.8.7.4">82.3</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.8.7.5">82.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.8.7.6">82.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.8.7.7">83.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.8.7.8"><span class="ltx_text ltx_font_bold" id="A1.T4.1.8.7.8.1">84.7</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.9.8.1">HC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.9.8.2">81.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.9.8.3">82.3</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.9.8.4">82.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.9.8.5">82.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.9.8.6">83.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.9.8.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.9.8.7.1">84.7</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.10.9.1">AC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.10.9.2">82.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.10.9.3">83.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.10.9.4">83.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.10.9.5">83.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.10.9.6">85.1</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.10.9.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.10.9.7.1">86.0</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10">
<th class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row" id="A1.T4.1.11.10.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.11.10.1.1">AP (M)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.11.10.2">RAW</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.11.10.3">70.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.11.10.4">70.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.11.10.5">71.9</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.11.10.6">72.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.11.10.7">73.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.11.10.8"><span class="ltx_text ltx_font_bold" id="A1.T4.1.11.10.8.1">74.0</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.12.11.1">HC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.12.11.2">70.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.12.11.3">70.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.12.11.4">72.0</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.12.11.5">72.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.12.11.6">73.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.12.11.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.12.11.7.1">74.1</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.13.12.1">AC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.13.12.2">71.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.13.12.3">71.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.13.12.4">73.1</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.13.12.5">73.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.13.12.6">74.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.13.12.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.13.12.7.1">75.0</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.14.13">
<th class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row" id="A1.T4.1.14.13.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.14.13.1.1">AP (L)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.14.13.2">RAW</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.14.13.3">78.2</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.14.13.4">78.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.14.13.5">78.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.14.13.6">78.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.14.13.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.14.13.7.1">81.2</span></td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.14.13.8">81.0</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.15.14.1">HC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.15.14.2">78.3</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.15.14.3">78.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.15.14.4">78.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.15.14.5">78.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.15.14.6"><span class="ltx_text ltx_font_bold" id="A1.T4.1.15.14.6.1">81.2</span></td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.15.14.7">81.1</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.16.15.1">AC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.16.15.2">78.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.16.15.3">79.0</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.16.15.4">79.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.16.15.5">79.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.16.15.6">81.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.16.15.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.16.15.7.1">81.9</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.17.16">
<th class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row" id="A1.T4.1.17.16.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.17.16.1.1">AR</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.17.16.2">RAW</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.17.16.3">76.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.17.16.4">76.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.17.16.5">77.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.17.16.6">77.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.17.16.7">79.3</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.17.16.8"><span class="ltx_text ltx_font_bold" id="A1.T4.1.17.16.8.1">79.4</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.18.17.1">HC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.18.17.2">76.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.18.17.3">76.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.18.17.4">77.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.18.17.5">77.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.18.17.6">79.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.18.17.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.18.17.7.1">79.5</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.19.18.1">AC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.19.18.2">77.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.19.18.3">77.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.19.18.4">78.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.19.18.5">78.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.19.18.6">80.2</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.19.18.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.19.18.7.1">80.2</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.20.19">
<th class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row" id="A1.T4.1.20.19.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.20.19.1.1">AR .5</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.20.19.2">RAW</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.20.19.3">93.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.20.19.4">93.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.20.19.5">94.0</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.20.19.6">93.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.20.19.7">94.3</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.20.19.8"><span class="ltx_text ltx_font_bold" id="A1.T4.1.20.19.8.1">94.4</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.21.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.21.20.1">HC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.21.20.2">93.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.21.20.3">93.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.21.20.4">94.0</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.21.20.5">93.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.21.20.6">94.3</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.21.20.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.21.20.7.1">94.4</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.22.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.22.21.1">AC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.22.21.2">93.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.22.21.3">93.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.22.21.4">94.0</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.22.21.5">93.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.22.21.6">94.3</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.22.21.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.22.21.7.1">94.5</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.23.22">
<th class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row" id="A1.T4.1.23.22.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.23.22.1.1">AR .75</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.23.22.2">RAW</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.23.22.3">83.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.23.22.4">84.0</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.23.22.5">84.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.23.22.6">84.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.23.22.7">85.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.23.22.8"><span class="ltx_text ltx_font_bold" id="A1.T4.1.23.22.8.1">86.1</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.24.23.1">HC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.24.23.2">83.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.24.23.3">84.1</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.24.23.4">84.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.24.23.5">84.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.24.23.6">85.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.24.23.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.24.23.7.1">86.1</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.25.24.1">AC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.25.24.2">84.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.25.24.3">85.2</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.25.24.4">85.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.25.24.5">85.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.25.24.6">86.8</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.25.24.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.25.24.7.1">87.2</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.26.25">
<th class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row" id="A1.T4.1.26.25.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.26.25.1.1">AR (M)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.26.25.2">RAW</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.26.25.3">73.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.26.25.4">73.3</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.26.25.5">74.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.26.25.6">74.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.26.25.7">76.1</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.26.25.8"><span class="ltx_text ltx_font_bold" id="A1.T4.1.26.25.8.1">76.4</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.27.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.27.26.1">HC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.27.26.2">73.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.27.26.3">73.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.27.26.4">74.7</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.27.26.5">74.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.27.26.6">76.2</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.27.26.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.27.26.7.1">76.5</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.28.27.1">AC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.28.27.2">74.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.28.27.3">74.3</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.28.27.4">75.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.28.27.5">75.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.28.27.6">77.0</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.28.27.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.28.27.7.1">77.3</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.29.28">
<th class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row ltx_border_bb" id="A1.T4.1.29.28.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.29.28.1.1">AR (L)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.29.28.2">RAW</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.29.28.3">81.5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.29.28.4">82.0</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.29.28.5">82.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.29.28.6">82.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.29.28.7"><span class="ltx_text ltx_font_bold" id="A1.T4.1.29.28.7.1">84.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.29.28.8">84.2</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.30.29">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.30.29.1">HC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.30.29.2">81.6</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.30.29.3">82.1</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.30.29.4">82.4</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.30.29.5">82.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.30.29.6"><span class="ltx_text ltx_font_bold" id="A1.T4.1.30.29.6.1">84.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.30.29.7">84.2</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.31.30">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T4.1.31.30.1">AC</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.1.31.30.2">82.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.1.31.30.3">82.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.1.31.30.4">83.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.1.31.30.5">82.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T4.1.31.30.6"><span class="ltx_text ltx_font_bold" id="A1.T4.1.31.30.6.1">85.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.1.31.30.7">84.8</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_table ltx_transformed_outer" id="A1.T5" style="width:145.0pt;height:673.7pt;vertical-align:-0.0pt;"><div class="ltx_transformed_inner" style="width:673.7pt;transform:translate(-264.35pt,-263.85pt) rotate(-90deg) ;"><figure>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of the results for the MPII data set of publicly available HPE models for individual body parts when evaluated on the original validation data set (RAW), the manually cleaned set (HC) and the automatically cleaned data set (AC) and the results of the model trained on cleaned data (TC). The head part was not considered any further, as the change there was less than 1% on average.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.1.1.1">
<th class="ltx_td ltx_th ltx_th_row" id="A1.T5.1.1.1.1"></th>
<td class="ltx_td ltx_align_center" colspan="3" id="A1.T5.1.1.1.2">Shoulder</td>
<td class="ltx_td ltx_align_center" colspan="3" id="A1.T5.1.1.1.3">Elbow</td>
<td class="ltx_td ltx_align_center" colspan="3" id="A1.T5.1.1.1.4">Wrist</td>
<td class="ltx_td ltx_align_center" colspan="3" id="A1.T5.1.1.1.5">Hip</td>
<td class="ltx_td ltx_align_center" colspan="3" id="A1.T5.1.1.1.6">Knee</td>
<td class="ltx_td ltx_align_center" colspan="3" id="A1.T5.1.1.1.7">Ankle</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.2.2.1">method</th>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.2">RAW</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.3">HC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.4">AC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.5">RAW</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.6">HC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.7">AC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.8">RAW</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.9">HC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.10">AC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.11">RAW</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.12">HC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.13">AC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.14">RAW</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.15">HC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.16">AC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.17">RAW</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.18">HC</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.2.2.19">AC</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A1.T5.1.3.3.1">ResNet50</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.2">95.3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.3">95.9</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.4">96.3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.5">88.7</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.6">93.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.7">94.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.8">83.3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.9">91.6</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.10">93.0</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.11">87.4</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.12">92.98</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.13">93.4</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.14">83.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.15">92.09</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.16">92.8</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.17">78.9</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.18">90.75</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.3.3.19">92.3</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.4.4.1">SE-ResNet50</th>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.2">95.3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.3">96.0</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.4">96.4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.5">88.6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.6">93.6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.7">94.4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.8">83.9</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.9">91.3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.10">92.7</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.11">87.1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.12">93.05</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.13">93.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.14">83.6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.15">92.48</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.16">93.3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.17">80.4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.18">91.65</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.4.19">93.1</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.5.5.1">SCNet50</th>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.2">95.4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.3">96.0</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.4">96.3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.5">88.6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.6">93.6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.7">94.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.8">84.0</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.9">91.8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.10">93.1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.11">88.1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.12">93.24</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.13">93.7</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.14">84.8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.15">92.82</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.16">93.7</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.17">80.6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.18">91.39</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.5.5.19">93.1</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.6.6.1">Hourglass52</th>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.2">95.4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.3">96.0</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.4">96.3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.5">89.3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.6">94.0</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.7">94.8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.8">84.0</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.9">91.8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.10">93.1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.11">87.9</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.12">92.94</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.13">93.3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.14">84.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.15">93.09</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.16">93.8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.17">80.6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.18">91.49</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.6.6.19">92.9</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.7.7.1">HRNet_w32</th>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.2">95.8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.3">96.4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.4">96.7</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.5">90.4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.6">94.4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.7">95.1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.8">85.8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.9">92.6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.10"><span class="ltx_text ltx_font_bold" id="A1.T5.1.7.7.10.1">93.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.11">89.2</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.12"><span class="ltx_text ltx_font_bold" id="A1.T5.1.7.7.12.1">93.81</span></td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.13"><span class="ltx_text ltx_font_bold" id="A1.T5.1.7.7.13.1">94.2</span></td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.14">86.3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.15">93.63</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.16"><span class="ltx_text ltx_font_bold" id="A1.T5.1.7.7.16.1">94.4</span></td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.17">82.3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.18">92.34</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.7.7.19">93.8</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.1">HRNet_w32 TC</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.2"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.2.1">96.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.3"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.3.1">96.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.4"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.4.1">96.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.5"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.5.1">90.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.6"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.6.1">94.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.7"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.7.1">95.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.8"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.8.1">86.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.9"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.9.1">92.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.10"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.10.1">93.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.11"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.11.1">89.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.12">93.67</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.13">94.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.14"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.14.1">86.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.15"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.15.1">93.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.16"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.16.1">94.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.17"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.17.1">82.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.18"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.18.1">92.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="A1.T5.1.8.8.19"><span class="ltx_text ltx_font_bold" id="A1.T5.1.8.8.19.1">94.1</span></td>
</tr>
</tbody>
</table>
</figure></div></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep  9 15:51:04 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
